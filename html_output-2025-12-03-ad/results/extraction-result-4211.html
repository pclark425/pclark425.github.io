<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4211 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4211</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4211</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-97.html">extraction-schema-97</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <p><strong>Paper ID:</strong> paper-276579678</p>
                <p><strong>Paper Title:</strong> A large language model framework for literature-based disease–gene association prediction</p>
                <p><strong>Paper Abstract:</strong> Abstract With the exponential growth of biomedical literature, leveraging Large Language Models (LLMs) for automated medical knowledge understanding has become increasingly critical for advancing precision medicine. However, current approaches face significant challenges in reliability, verifiability, and scalability when extracting complex biological relationships from scientific literature using LLMs. To overcome the obstacles of LLM development in biomedical literature understating, we propose LORE, a novel unsupervised two-stage reading methodology with LLM that models literature as a knowledge graph of verifiable factual statements and, in turn, as semantic embeddings in Euclidean space. LORE captured essential gene pathogenicity information when applied to PubMed abstracts for large-scale understanding of disease–gene relationships. We demonstrated that modeling a latent pathogenic flow in the semantic embedding with supervision from the ClinVar database led to a 90% mean average precision in identifying relevant genes across 2097 diseases. This work provides a scalable and reproducible approach for leveraging LLMs in biomedical literature analysis, offering new opportunities for researchers to identify therapeutic targets efficiently.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4211.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4211.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LORE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Open Relation extraction and Embedding (LORE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage literature-semantics framework that uses LLMs to (1) extract atomic sentence-level relations from articles (LLM-ORE) and (2) embed aggregated relation knowledge per entity pair into dense vectors (LLM-EMB) for downstream modeling, enabling discovery of latent patterns such as a continuous 'pathogenic flow' across disease-gene pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LORE (two-stage LLM literature semantics framework)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LORE first applies LLM-ORE to read individual article texts (title + abstract) with text-continuation prompts that instruct the model to output atomic relational triplets <subject, predicate, object> for target entity pairs. Extracted sentential relations are lemmatized and tagged with curated key semantics. In the second stage, LLM-EMB encodes all relations for each entity pair into a single dense numeric embedding (512-d) by feeding concatenated relations (split into sub-documents if exceeding model context) into an LLM embedding model. The resulting literature-scale knowledge graph and embeddings are used by downstream supervised ranking models (ML-Ranker) to model and predict graded relationships (e.g., pathogenicity), and to reveal latent continuous fields in embedding space (the 'pathogenic flow').</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical literature / genomics (disease-gene relationships)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to a curated subset of 1,745,538 PubMed articles (resulting in 11,285,095 relations); extended to 3,997,496 pubmedKB abstracts when using Llama-8B (resulting in 74,132,940 relations).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Empirical semantic-correlational relationships and continuous latent patterns (graded pathogenicity scores / 'pathogenic flow' across embedding space).</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>The framework discovers a latent directional field in embedding space called 'pathogenic flow' where at each disease-gene (DG) point a unit flow vector u_DG is defined (Methods): u_DG = normalized( v_DG - mean_v_nonpathogenic_D ) for DG in non-pathogenic set; or normalized( mean_v_pathogenic_D - v_DG ) for DG in pathogenic set. Aggregated per-cube quantized vectors u_L = (1/|L|) sum_{DG in L} u_DG reveal a smooth cross-disease pathogenicity gradient; embeddings are 512-dimensional vectors representing literature semantics per DG.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Text-continuation prompting of LLMs to perform open relation extraction at the article level (produce <subject,predicate,object> triplets), lemmatization of predicates, automated tagging via curated key semantics, concatenation of relations per entity pair, and embedding via an LLM embedding model (text-embedding-3-large).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Comparison against curated ClinVar pathogenic gene annotations (mapped to MeSH IDs) using leave-one-disease-out cross-validation and other splits; visualization of embeddings with UMAP/PCA and regression axes; evaluation of ranking performance (AP/MAP) of downstream ML-Ranker versus baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Mean Average Precision (MAP) for ranking pathogenic genes: 79.9% MAP on full PMKB-CV (652,701 DGs across 2097 diseases); 90.0% MAP when restricting to DGs with LLM-ORE annotations; LLM-EMB linear regression alone achieves 87.7% MAP; replacing GPT-3.5 with Llama-8B and scaling to 3.9M abstracts produced 81.6% MAP (with 91.3% ClinVar coverage). Baselines: co-occurrence paper counting MAP = 69.4%; GPT-4o baseline MAP = 31.7%. Coverage: paper co-occurrence covered 94.8% of ClinVar DGs; LLM-ORE relations covered 71.4% (subset); Llama-extracted relations covered 91.3%.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>As measured by ranking performance (MAP): up to 90.0% MAP for DGs with relation annotations; 79.9% MAP across full PMKB-CV. Coverage of ClinVar DGs by LORE-derived relations ranged from 71.4% (initial subset) to 91.3% (Llama-8B full run).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Dependence on article abstracts rather than full texts (some ClinVar DGs absent because no abstract co-occurrence); LLM hallucination risk mitigated by producing traceable per-article relations but still reliant on LLM accuracy; context-window limits requiring splitting relations into sub-documents; curated semantic taxonomy used ClinVar in curation, potentially limiting generalizability of key-semantic tags; distinguishing pathogenic vs non-pathogenic points may require large data for nonlinear decision boundaries, motivating disease-wise flow modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to naive paper co-occurrence counting (MAP 69.4%), direct counting of tagged relations, LLM-EMB linear regression (87.7% MAP), direct prompting of GPT-3.5/GPT-4o as baselines (GPT-4o MAP 31.7%), and to using the open-source Llama-8B (comparable MAP ~79.5% when used in place of GPT-3.5 on subset; scaled to 81.6% MAP on larger set). LORE + ML-Ranker outperformed these baselines (90.0% MAP on annotated DGs).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A large language model framework for literature-based disease–gene association prediction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4211.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4211.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-ORE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Open Relation Extraction (LLM-ORE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM prompting pipeline that reads an article (title+abstract) and outputs atomic, structured relation triplets <subject, predicate, object> for specified entity pairs; used to build a literature-scale knowledge graph of disease-gene relations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-ORE (open relation extraction via text-continuation prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Task-agnostic demonstration prompt plus a target-article section instructs the LLM to produce concise factual triplets obeying formatting rules (subject contains entity1 only, object contains entity2 only, predicate is concise). Examples and counterexamples in the prompt encourage desired behaviors and avoid unwanted outputs. The method allows abstract, cross-sentence extraction and produces sentential relations which are lemmatized and later tagged with curated key semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0613 (used for the main extraction runs); also applied with Llama-3.1-8B-Instruct in large-scale runs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3.5: unspecified; Llama variant: 8B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical literature (disease-gene relations)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to a literature subset of 1,745,538 articles yielding 11,285,095 relations when using GPT-3.5; when using Llama-8B on 3,997,496 abstracts produced 74,132,940 relations.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Extraction of sentence-level factual relations and semantic lemmas that collectively reveal empirical associations and support extraction of graded relationships (e.g., mutation co-segregation, cause, associate).</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Extracted relations such as <'GENE', 'mutation cosegregates with disease', 'DISEASE'> and other atomic statements; output lemmas (e.g., 'associate', 'mutation', 'cause') used to map DGs along semantic axes correlated with pathogenicity.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Prompted LLM text-continuation to output list of relational triplets per article and entity pair; triplets are tokenized and predicate words lemmatized; predicates aggregated across articles for an entity pair and later embedded.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Coverage and precision filters versus ClinVar used in selecting important lemmas (coverage n >=100, precision r >=50%); downstream ranking performance evaluated against ClinVar labels; inspection and manual curation of sampled relations during key semantics taxonomy construction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Coverage of ClinVar DGs: initial LLM-ORE relations covered 71.4% of ClinVar DGs in PMKB-CV; when scaled with Llama-8B coverage rose to 91.3%. The taxonomy of lemmas resulted in 282 important lemmas and 105 curated key semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Precision filter in lemma selection required >=50% of relations involving a lemma to be from known pathogenic DGs (ClinVar), yielding 282 important lemmas prior to manual taxonomy; direct per-DG ranking using counts of tagged relations produced MAP (not directly comparable due to ClinVar use in curation) but served as an informative baseline (coarse performance reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Incomplete coverage (initial extract covered 71.4% of ClinVar DGs), sensitivity to LLM quality/context window, and dependence on abstracts (not full text). Manual curation required to produce reliable key semantic taxonomy; use of ClinVar in taxonomy construction introduces a circularity that limits independent generalizability estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to naive co-occurrence counting and direct LLM prompting for pathogenicity: LLM-ORE relations when counted (LLM-ORE count) provided stronger signal than raw paper counts but underperformed the full ML-Ranker trained on embeddings; scaling with Llama-8B improved coverage substantially.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A large language model framework for literature-based disease–gene association prediction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4211.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4211.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-EMB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Embedding (LLM-EMB)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embedding pipeline that encodes all extracted sentential relations between an entity pair into a single dense numeric vector (512 dimensions) representing literature semantics for downstream analysis and pattern discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-EMB (per-entity-pair literature embedding)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>For each entity pair, all extracted relations across articles are concatenated into a document (split into sub-documents to fit context), then passed to an embedding model (OpenAI text-embedding-3-large) to produce token-aware embeddings per sub-document which are aggregated (length-weighted averaging) to produce a 512-dimensional representation capturing the total literature semantics for that pair. These embeddings were visualized with UMAP and PCA and used as features for downstream regression and ranking models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-embedding-3-large (OpenAI) for embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical literature / genomics</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Embeddings created for DG pairs derived from 1,745,538 articles subset (11,285,095 relations) and for broader sets in scaled runs (millions of abstracts).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Latent continuous relationships (graded pathogenicity manifolds, PCA/regression axes correlated with ClinVar labels) — unsupervised embeddings that capture quantitative semantic gradients.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Visualization showed a curved arm manifold in PCA space where ClinVar pathogenic DGs concentrate; regression axes (ridge regression against ClinVar labels) spanned subspaces aligned with pathogenicity. No explicit analytic physical equations provided besides vector definitions and PCA/regression projections.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Aggregation of LLM-ORE retrieved relations per entity pair into textual documents, embedding with an LLM embedding model, aggregation into a single dense vector (512-d) per DG.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Correlation of embedding-derived axes with ClinVar pathogenic labels; downstream predictive modeling (linear regression on embeddings, ML-Ranker) validated via cross-validation and MAP/AP metrics against ClinVar.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>LLM-EMB linear regression against ClinVar achieved 87.7% MAP (ranking pathogenic genes), indicating embeddings capture strong predictive signal.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Measured via predictive performance: embeddings alone (linear regression) produced 87.7% MAP in ranking pathogenic DGs in the PMKB-CV benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Embeddings alone may require appropriate downstream modeling to fully exploit nonlinear patterns; central mixture regions in embedding space require disease-wise flow modeling to achieve best discrimination between pathogenic and non-pathogenic DGs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>LLM-EMB (linear regression) outperformed naive paper counting (69.4% MAP) and GPT-4o prompting (31.7% MAP), but the full ML-Ranker (that also uses embeddings plus features and ranking objective) further improved performance to 90.0% MAP on annotated DGs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A large language model framework for literature-based disease–gene association prediction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4211.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4211.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ML-Ranker</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ML-Ranker (lambda objective Gradient-Boosted Decision Trees)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised ranking model that uses embeddings and other features to learn disease-wise pathogenic flow and to rank genes by predicted pathogenic relevance for each disease using a LambdaRank objective implemented with GBDT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ML-Ranker (λGBDT with disease-wise LambdaRank objective)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ML-Ranker trains Gradient-Boosted Decision Trees with a lambda objective (LambdaRank style) to directly optimize pairwise ranking with NDCG-weighted gradients. Input features include LLM-EMB embeddings (512-d), co-occurrence counts (#papers), and other pubmedKB annotations. The model is trained with leave-one-disease-out cross-validation to predict per-disease ranked pathogenicity scores and to model the disease-wise pathogenic flow.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical genomics / literature mining</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Trained and evaluated on PMKB-CV dataset consisting of 652,701 DGs across 2097 diseases (derived from the literature subset and ClinVar mappings).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Learned ranking function that operationalizes a disease-wise quantitative score (pathogenicity score) and models directional 'pathogenic flow' in embedding space (vector field indicating nonpathogenic -> pathogenic direction).</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Optimizes pairwise RankNet loss: L_{DG1,DG2} = -log Pr(P(DG1) > P(DG2)) = log(1 + e^{-σ(s_DG1 - s_DG2)}), with lambda gradient λ_{DG1,DG2} scaled by NDCG change; uses learned scores s to produce graded pathogenicity ranking across genes for each disease.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Uses LLM-EMB vectors and literature-derived features as inputs to a LambdaRank GBDT model to extract and rank quantitative relationships (pathogenicity scores) from literature-derived semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Leave-one-disease-out cross-validation to compute AP per disease and MAP overall; statistical comparisons across methods (one-sided Wilcoxon signed-rank test) reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>MAP = 79.9% across full PMKB-CV (652,701 DGs); MAP = 90.0% when restricted to DGs with LLM-ORE annotations; MAP = 81% for high-prevalence disease subsets; significantly better than baselines (p-values reported in paper diagrams).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>As ranking accuracy: 79.9% MAP overall; 90.0% MAP on annotated DGs. Demonstrates high success in recovering ClinVar-curated pathogenic genes in top ranks.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Requires labeled pathogenic examples per disease for supervised training (ClinVar sparse); model generalization depends on embedding quality and feature completeness; disease-wise modeling presumes existence of consistent directional flows across diseases which may not hold universally.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to naive paper-counting baseline (MAP 69.4%), LLM-EMB linear regression (87.7% MAP), direct LLM prompting baselines (GPT-4o MAP 31.7%), and relation-counting (LLM-ORE count). ML-Ranker provided best overall ranking performance (90.0% MAP on annotated DGs).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A large language model framework for literature-based disease–gene association prediction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4211.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4211.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI conversational large language model used in this work to perform article-level open relation extraction (LLM-ORE) over PubMed abstracts to produce sentence-level relational triplets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-3.5 used as LLM-ORE reader</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>gpt-3.5-turbo-0613 was prompted with a demonstration and target-article sections (text-continuation prompts) to output structured relational triplets for specified entity pairs; used for the main extraction runs on the literature subset.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical literature processing</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Used to process the 1,745,538-article subset where LLM-ORE produced 11,285,095 relations.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Extraction of sentence-level factual relations (qualitative/semantically quantitative when aggregated) rather than explicit mathematical laws.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Generated triplets such as <'GENE', 'mutation cosegregates with disease', 'DISEASE'> which, when aggregated and embedded, contributed to discovery of the pathogenic flow and graded pathogenicity scores.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Text-continuation prompting with examples/counterexamples to induce structured triplet outputs per article and entity pair.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Quality assessed via downstream coverage and ranking performance (embeddings and ML-Ranker), and manual sampling during key semantics taxonomy curation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Contributed to overall LORE performance: LORE + ML-Ranker with GPT-3.5-derived relations achieved 79.9% MAP on PMKB-CV and 90.0% MAP for DGs with relation annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Indirectly measured via downstream MAP results (see LORE/ML-Ranker entries).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>LLM capacity and cost considerations; context window limits; initial LLM-ORE coverage (71.4% of ClinVar DGs for subset) which improved when scaled with Llama-8B.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to using Llama-8B for ORE (marginal performance loss when replacing GPT-3.5) and to direct querying of GPT-4o for pathogenicity which performed poorly as a direct baseline (GPT-4o MAP 31.7%).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A large language model framework for literature-based disease–gene association prediction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4211.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4211.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3.1-8B-Instruct (referred to as Llama-8B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 8-billion-parameter LLM used as a lower-cost alternative to GPT-3.5 for large-scale relation extraction; when used to run LLM-ORE on ~4M abstracts it produced substantially higher coverage of ClinVar DGs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Llama-3.1-8B-Instruct used for large-scale LLM-ORE</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An 8B-parameter instruct-tuned LLaMA variant was prompted with the same task-agnostic ORE prompts to extract relations across all 3,997,496 pubmedKB abstracts with DG co-occurrence; produced 74,132,940 relations covering 91.3% of ClinVar DGs and enabling ML-Ranker MAP of 81.6% on larger run.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical literature mining</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to 3,997,496 pubmedKB abstracts with DG co-occurrence in the scaled run.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Same types as LLM-ORE: sentence-level relations aggregated to reveal empirical graded relationships and latent patterns in embedding space (pathogenic flow).</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Produced relation triplets enabling embedding and ranking; coverage increased to 91.3% of ClinVar DGs when Llama-8B was used at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Prompted open relation extraction (same demonstration + target prompt) at large scale; relations aggregated and embedded as per LORE pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Downstream ranking performance measured (MAP = 81.6% on the scaled Llama-run), and ClinVar coverage assessed (91.3%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Scaled run with Llama-8B: 74,132,940 relations extracted; 91.3% ClinVar coverage; ML-Ranker achieved 81.6% MAP on the larger corpus. On the smaller subset substituting GPT-3.5 with Llama-8B yielded marginal performance loss (LLM-8B MAP ~79.5% vs GPT-3.5 MAP 79.9%).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Measured by coverage and MAP: 91.3% ClinVar coverage and 81.6% MAP after scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Smaller model capacity may slightly reduce extraction fidelity per-article compared with larger proprietary LLMs, but scaling compensated to increase coverage; context-window and prompt-engineering limitations remain.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly to GPT-3.5-based LORE runs (marginal performance drop on subset) and to GPT-4o direct prompting baseline (GPT-4o performed poorly). Llama-8B at scale outperformed initial GPT-3.5 ORE coverage by producing more relations overall (when applied to a much larger number of abstracts).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A large language model framework for literature-based disease–gene association prediction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4211.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4211.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A more recent OpenAI LLM variant used as a direct prompting baseline to ask about DG pathogenicity; performed poorly as a direct question-answer baseline compared to the structured LORE pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4o (used as direct prompting baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>GPT-4o (ver.2024-05-13) was directly prompted to assess pathogenicity of DGs as a baseline comparison; unlike the LORE pipeline this was a direct QA approach rather than structured extraction + embedding + supervised ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical knowledge QA baseline</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Queried as a baseline against PMKB-CV DGs (exact number of prompts equals number of DGs used for baseline evaluation; aggregated results reported).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Direct natural-language judgments about pathogenicity of DGs (not structured quantitative law extraction).</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Provided direct assessments of pathogenicity per DG but without producing structured relation triplets or embeddings; results used for baseline MAP comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Direct prompted QA (not open relation extraction or embedding aggregation).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Compared predicted pathogenicity to ClinVar curated labels; MAP computed as baseline metric.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GPT-4o baseline MAP = 31.7% on the PMKB-CV benchmark (substantially lower than LORE-based methods).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Low effectiveness as a direct baseline: 31.7% MAP indicates poor recovery of ClinVar pathogenic genes when queried directly.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Direct QA suffers from hallucination and lack of verifiable traceable evidence; opaque parametric memory provides poor traceability to source literature; inferior performance compared with the structured LORE approach.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Served as a baseline; underperformed relative to LORE's LLM-EMB + ML-Ranker (87.7% - 90.0% MAP) and even relative to naive paper-counting baseline in some scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A large language model framework for literature-based disease–gene association prediction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4211.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4211.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented generation paradigm mentioned as a mitigation strategy for hallucination that constrains an LLM's output to retrieved supporting documents, but limited by retrieval scalability and coarse retrieval methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>RAG augments LLM generation by retrieving a small set of documents and conditioning the LLM on them so outputs are grounded; the paper cites RAG as a commonly used approach to increase verifiability for LLMs but notes scalability and retrieval-quality limitations (fast shallow retrieval misses nuanced content).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP methodologies applied to literature mining</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Not an extraction of a quantitative law itself, but a method to ground model outputs to retrieved evidence; relevant to reliable extraction of relationships from literature.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Retrieve top-k documents via sentence-similarity embeddings (fast, shallow retrieval) followed by LLM-conditioned generation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Discussed as background and contrasted with LORE's approach of building an explicit knowledge graph of relations to support verifiability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Scalability constraints: requires fast but coarse retrieval that can omit nuanced relevant articles; may lead to incomplete information capture.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Presented as a related technique that LORE improves upon by extracting a concise knowledge graph so subsequent embedding and modeling can be more scalable and verifiable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A large language model framework for literature-based disease–gene association prediction', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-augmented generation for knowledge-intensive NLP tasks <em>(Rating: 2)</em></li>
                <li>Large language models encode clinical knowledge <em>(Rating: 2)</em></li>
                <li>The impact of large language models on scientific discovery: a preliminary study using GPT-4 <em>(Rating: 2)</em></li>
                <li>Retrieve, summarize, and Verify: how will ChatGPT affect information seeking from the medical literature? <em>(Rating: 1)</em></li>
                <li>UMAP: uniform manifold approximation and projection <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4211",
    "paper_id": "paper-276579678",
    "extraction_schema_id": "extraction-schema-97",
    "extracted_data": [
        {
            "name_short": "LORE",
            "name_full": "LLM-based Open Relation extraction and Embedding (LORE)",
            "brief_description": "A two-stage literature-semantics framework that uses LLMs to (1) extract atomic sentence-level relations from articles (LLM-ORE) and (2) embed aggregated relation knowledge per entity pair into dense vectors (LLM-EMB) for downstream modeling, enabling discovery of latent patterns such as a continuous 'pathogenic flow' across disease-gene pairs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LORE (two-stage LLM literature semantics framework)",
            "system_description": "LORE first applies LLM-ORE to read individual article texts (title + abstract) with text-continuation prompts that instruct the model to output atomic relational triplets &lt;subject, predicate, object&gt; for target entity pairs. Extracted sentential relations are lemmatized and tagged with curated key semantics. In the second stage, LLM-EMB encodes all relations for each entity pair into a single dense numeric embedding (512-d) by feeding concatenated relations (split into sub-documents if exceeding model context) into an LLM embedding model. The resulting literature-scale knowledge graph and embeddings are used by downstream supervised ranking models (ML-Ranker) to model and predict graded relationships (e.g., pathogenicity), and to reveal latent continuous fields in embedding space (the 'pathogenic flow').",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Biomedical literature / genomics (disease-gene relationships)",
            "number_of_papers": "Applied to a curated subset of 1,745,538 PubMed articles (resulting in 11,285,095 relations); extended to 3,997,496 pubmedKB abstracts when using Llama-8B (resulting in 74,132,940 relations).",
            "law_type": "Empirical semantic-correlational relationships and continuous latent patterns (graded pathogenicity scores / 'pathogenic flow' across embedding space).",
            "law_examples": "The framework discovers a latent directional field in embedding space called 'pathogenic flow' where at each disease-gene (DG) point a unit flow vector u_DG is defined (Methods): u_DG = normalized( v_DG - mean_v_nonpathogenic_D ) for DG in non-pathogenic set; or normalized( mean_v_pathogenic_D - v_DG ) for DG in pathogenic set. Aggregated per-cube quantized vectors u_L = (1/|L|) sum_{DG in L} u_DG reveal a smooth cross-disease pathogenicity gradient; embeddings are 512-dimensional vectors representing literature semantics per DG.",
            "extraction_method": "Text-continuation prompting of LLMs to perform open relation extraction at the article level (produce &lt;subject,predicate,object&gt; triplets), lemmatization of predicates, automated tagging via curated key semantics, concatenation of relations per entity pair, and embedding via an LLM embedding model (text-embedding-3-large).",
            "validation_approach": "Comparison against curated ClinVar pathogenic gene annotations (mapped to MeSH IDs) using leave-one-disease-out cross-validation and other splits; visualization of embeddings with UMAP/PCA and regression axes; evaluation of ranking performance (AP/MAP) of downstream ML-Ranker versus baselines.",
            "performance_metrics": "Mean Average Precision (MAP) for ranking pathogenic genes: 79.9% MAP on full PMKB-CV (652,701 DGs across 2097 diseases); 90.0% MAP when restricting to DGs with LLM-ORE annotations; LLM-EMB linear regression alone achieves 87.7% MAP; replacing GPT-3.5 with Llama-8B and scaling to 3.9M abstracts produced 81.6% MAP (with 91.3% ClinVar coverage). Baselines: co-occurrence paper counting MAP = 69.4%; GPT-4o baseline MAP = 31.7%. Coverage: paper co-occurrence covered 94.8% of ClinVar DGs; LLM-ORE relations covered 71.4% (subset); Llama-extracted relations covered 91.3%.",
            "success_rate": "As measured by ranking performance (MAP): up to 90.0% MAP for DGs with relation annotations; 79.9% MAP across full PMKB-CV. Coverage of ClinVar DGs by LORE-derived relations ranged from 71.4% (initial subset) to 91.3% (Llama-8B full run).",
            "challenges_limitations": "Dependence on article abstracts rather than full texts (some ClinVar DGs absent because no abstract co-occurrence); LLM hallucination risk mitigated by producing traceable per-article relations but still reliant on LLM accuracy; context-window limits requiring splitting relations into sub-documents; curated semantic taxonomy used ClinVar in curation, potentially limiting generalizability of key-semantic tags; distinguishing pathogenic vs non-pathogenic points may require large data for nonlinear decision boundaries, motivating disease-wise flow modeling.",
            "comparison_baseline": "Compared to naive paper co-occurrence counting (MAP 69.4%), direct counting of tagged relations, LLM-EMB linear regression (87.7% MAP), direct prompting of GPT-3.5/GPT-4o as baselines (GPT-4o MAP 31.7%), and to using the open-source Llama-8B (comparable MAP ~79.5% when used in place of GPT-3.5 on subset; scaled to 81.6% MAP on larger set). LORE + ML-Ranker outperformed these baselines (90.0% MAP on annotated DGs).",
            "uuid": "e4211.0",
            "source_info": {
                "paper_title": "A large language model framework for literature-based disease–gene association prediction",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "LLM-ORE",
            "name_full": "LLM-based Open Relation Extraction (LLM-ORE)",
            "brief_description": "An LLM prompting pipeline that reads an article (title+abstract) and outputs atomic, structured relation triplets &lt;subject, predicate, object&gt; for specified entity pairs; used to build a literature-scale knowledge graph of disease-gene relations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LLM-ORE (open relation extraction via text-continuation prompts)",
            "system_description": "Task-agnostic demonstration prompt plus a target-article section instructs the LLM to produce concise factual triplets obeying formatting rules (subject contains entity1 only, object contains entity2 only, predicate is concise). Examples and counterexamples in the prompt encourage desired behaviors and avoid unwanted outputs. The method allows abstract, cross-sentence extraction and produces sentential relations which are lemmatized and later tagged with curated key semantics.",
            "model_name": "gpt-3.5-turbo-0613 (used for the main extraction runs); also applied with Llama-3.1-8B-Instruct in large-scale runs",
            "model_size": "GPT-3.5: unspecified; Llama variant: 8B",
            "scientific_domain": "Biomedical literature (disease-gene relations)",
            "number_of_papers": "Applied to a literature subset of 1,745,538 articles yielding 11,285,095 relations when using GPT-3.5; when using Llama-8B on 3,997,496 abstracts produced 74,132,940 relations.",
            "law_type": "Extraction of sentence-level factual relations and semantic lemmas that collectively reveal empirical associations and support extraction of graded relationships (e.g., mutation co-segregation, cause, associate).",
            "law_examples": "Extracted relations such as &lt;'GENE', 'mutation cosegregates with disease', 'DISEASE'&gt; and other atomic statements; output lemmas (e.g., 'associate', 'mutation', 'cause') used to map DGs along semantic axes correlated with pathogenicity.",
            "extraction_method": "Prompted LLM text-continuation to output list of relational triplets per article and entity pair; triplets are tokenized and predicate words lemmatized; predicates aggregated across articles for an entity pair and later embedded.",
            "validation_approach": "Coverage and precision filters versus ClinVar used in selecting important lemmas (coverage n &gt;=100, precision r &gt;=50%); downstream ranking performance evaluated against ClinVar labels; inspection and manual curation of sampled relations during key semantics taxonomy construction.",
            "performance_metrics": "Coverage of ClinVar DGs: initial LLM-ORE relations covered 71.4% of ClinVar DGs in PMKB-CV; when scaled with Llama-8B coverage rose to 91.3%. The taxonomy of lemmas resulted in 282 important lemmas and 105 curated key semantics.",
            "success_rate": "Precision filter in lemma selection required &gt;=50% of relations involving a lemma to be from known pathogenic DGs (ClinVar), yielding 282 important lemmas prior to manual taxonomy; direct per-DG ranking using counts of tagged relations produced MAP (not directly comparable due to ClinVar use in curation) but served as an informative baseline (coarse performance reported in paper).",
            "challenges_limitations": "Incomplete coverage (initial extract covered 71.4% of ClinVar DGs), sensitivity to LLM quality/context window, and dependence on abstracts (not full text). Manual curation required to produce reliable key semantic taxonomy; use of ClinVar in taxonomy construction introduces a circularity that limits independent generalizability estimates.",
            "comparison_baseline": "Compared to naive co-occurrence counting and direct LLM prompting for pathogenicity: LLM-ORE relations when counted (LLM-ORE count) provided stronger signal than raw paper counts but underperformed the full ML-Ranker trained on embeddings; scaling with Llama-8B improved coverage substantially.",
            "uuid": "e4211.1",
            "source_info": {
                "paper_title": "A large language model framework for literature-based disease–gene association prediction",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "LLM-EMB",
            "name_full": "LLM-based Embedding (LLM-EMB)",
            "brief_description": "An embedding pipeline that encodes all extracted sentential relations between an entity pair into a single dense numeric vector (512 dimensions) representing literature semantics for downstream analysis and pattern discovery.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LLM-EMB (per-entity-pair literature embedding)",
            "system_description": "For each entity pair, all extracted relations across articles are concatenated into a document (split into sub-documents to fit context), then passed to an embedding model (OpenAI text-embedding-3-large) to produce token-aware embeddings per sub-document which are aggregated (length-weighted averaging) to produce a 512-dimensional representation capturing the total literature semantics for that pair. These embeddings were visualized with UMAP and PCA and used as features for downstream regression and ranking models.",
            "model_name": "text-embedding-3-large (OpenAI) for embeddings",
            "model_size": null,
            "scientific_domain": "Biomedical literature / genomics",
            "number_of_papers": "Embeddings created for DG pairs derived from 1,745,538 articles subset (11,285,095 relations) and for broader sets in scaled runs (millions of abstracts).",
            "law_type": "Latent continuous relationships (graded pathogenicity manifolds, PCA/regression axes correlated with ClinVar labels) — unsupervised embeddings that capture quantitative semantic gradients.",
            "law_examples": "Visualization showed a curved arm manifold in PCA space where ClinVar pathogenic DGs concentrate; regression axes (ridge regression against ClinVar labels) spanned subspaces aligned with pathogenicity. No explicit analytic physical equations provided besides vector definitions and PCA/regression projections.",
            "extraction_method": "Aggregation of LLM-ORE retrieved relations per entity pair into textual documents, embedding with an LLM embedding model, aggregation into a single dense vector (512-d) per DG.",
            "validation_approach": "Correlation of embedding-derived axes with ClinVar pathogenic labels; downstream predictive modeling (linear regression on embeddings, ML-Ranker) validated via cross-validation and MAP/AP metrics against ClinVar.",
            "performance_metrics": "LLM-EMB linear regression against ClinVar achieved 87.7% MAP (ranking pathogenic genes), indicating embeddings capture strong predictive signal.",
            "success_rate": "Measured via predictive performance: embeddings alone (linear regression) produced 87.7% MAP in ranking pathogenic DGs in the PMKB-CV benchmark.",
            "challenges_limitations": "Embeddings alone may require appropriate downstream modeling to fully exploit nonlinear patterns; central mixture regions in embedding space require disease-wise flow modeling to achieve best discrimination between pathogenic and non-pathogenic DGs.",
            "comparison_baseline": "LLM-EMB (linear regression) outperformed naive paper counting (69.4% MAP) and GPT-4o prompting (31.7% MAP), but the full ML-Ranker (that also uses embeddings plus features and ranking objective) further improved performance to 90.0% MAP on annotated DGs.",
            "uuid": "e4211.2",
            "source_info": {
                "paper_title": "A large language model framework for literature-based disease–gene association prediction",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "ML-Ranker",
            "name_full": "ML-Ranker (lambda objective Gradient-Boosted Decision Trees)",
            "brief_description": "A supervised ranking model that uses embeddings and other features to learn disease-wise pathogenic flow and to rank genes by predicted pathogenic relevance for each disease using a LambdaRank objective implemented with GBDT.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ML-Ranker (λGBDT with disease-wise LambdaRank objective)",
            "system_description": "ML-Ranker trains Gradient-Boosted Decision Trees with a lambda objective (LambdaRank style) to directly optimize pairwise ranking with NDCG-weighted gradients. Input features include LLM-EMB embeddings (512-d), co-occurrence counts (#papers), and other pubmedKB annotations. The model is trained with leave-one-disease-out cross-validation to predict per-disease ranked pathogenicity scores and to model the disease-wise pathogenic flow.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Biomedical genomics / literature mining",
            "number_of_papers": "Trained and evaluated on PMKB-CV dataset consisting of 652,701 DGs across 2097 diseases (derived from the literature subset and ClinVar mappings).",
            "law_type": "Learned ranking function that operationalizes a disease-wise quantitative score (pathogenicity score) and models directional 'pathogenic flow' in embedding space (vector field indicating nonpathogenic -&gt; pathogenic direction).",
            "law_examples": "Optimizes pairwise RankNet loss: L_{DG1,DG2} = -log Pr(P(DG1) &gt; P(DG2)) = log(1 + e^{-σ(s_DG1 - s_DG2)}), with lambda gradient λ_{DG1,DG2} scaled by NDCG change; uses learned scores s to produce graded pathogenicity ranking across genes for each disease.",
            "extraction_method": "Uses LLM-EMB vectors and literature-derived features as inputs to a LambdaRank GBDT model to extract and rank quantitative relationships (pathogenicity scores) from literature-derived semantics.",
            "validation_approach": "Leave-one-disease-out cross-validation to compute AP per disease and MAP overall; statistical comparisons across methods (one-sided Wilcoxon signed-rank test) reported.",
            "performance_metrics": "MAP = 79.9% across full PMKB-CV (652,701 DGs); MAP = 90.0% when restricted to DGs with LLM-ORE annotations; MAP = 81% for high-prevalence disease subsets; significantly better than baselines (p-values reported in paper diagrams).",
            "success_rate": "As ranking accuracy: 79.9% MAP overall; 90.0% MAP on annotated DGs. Demonstrates high success in recovering ClinVar-curated pathogenic genes in top ranks.",
            "challenges_limitations": "Requires labeled pathogenic examples per disease for supervised training (ClinVar sparse); model generalization depends on embedding quality and feature completeness; disease-wise modeling presumes existence of consistent directional flows across diseases which may not hold universally.",
            "comparison_baseline": "Compared to naive paper-counting baseline (MAP 69.4%), LLM-EMB linear regression (87.7% MAP), direct LLM prompting baselines (GPT-4o MAP 31.7%), and relation-counting (LLM-ORE count). ML-Ranker provided best overall ranking performance (90.0% MAP on annotated DGs).",
            "uuid": "e4211.3",
            "source_info": {
                "paper_title": "A large language model framework for literature-based disease–gene association prediction",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5 (gpt-3.5-turbo-0613)",
            "brief_description": "An OpenAI conversational large language model used in this work to perform article-level open relation extraction (LLM-ORE) over PubMed abstracts to produce sentence-level relational triplets.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-3.5 used as LLM-ORE reader",
            "system_description": "gpt-3.5-turbo-0613 was prompted with a demonstration and target-article sections (text-continuation prompts) to output structured relational triplets for specified entity pairs; used for the main extraction runs on the literature subset.",
            "model_name": "gpt-3.5-turbo-0613",
            "model_size": null,
            "scientific_domain": "Biomedical literature processing",
            "number_of_papers": "Used to process the 1,745,538-article subset where LLM-ORE produced 11,285,095 relations.",
            "law_type": "Extraction of sentence-level factual relations (qualitative/semantically quantitative when aggregated) rather than explicit mathematical laws.",
            "law_examples": "Generated triplets such as &lt;'GENE', 'mutation cosegregates with disease', 'DISEASE'&gt; which, when aggregated and embedded, contributed to discovery of the pathogenic flow and graded pathogenicity scores.",
            "extraction_method": "Text-continuation prompting with examples/counterexamples to induce structured triplet outputs per article and entity pair.",
            "validation_approach": "Quality assessed via downstream coverage and ranking performance (embeddings and ML-Ranker), and manual sampling during key semantics taxonomy curation.",
            "performance_metrics": "Contributed to overall LORE performance: LORE + ML-Ranker with GPT-3.5-derived relations achieved 79.9% MAP on PMKB-CV and 90.0% MAP for DGs with relation annotations.",
            "success_rate": "Indirectly measured via downstream MAP results (see LORE/ML-Ranker entries).",
            "challenges_limitations": "LLM capacity and cost considerations; context window limits; initial LLM-ORE coverage (71.4% of ClinVar DGs for subset) which improved when scaled with Llama-8B.",
            "comparison_baseline": "Compared to using Llama-8B for ORE (marginal performance loss when replacing GPT-3.5) and to direct querying of GPT-4o for pathogenicity which performed poorly as a direct baseline (GPT-4o MAP 31.7%).",
            "uuid": "e4211.4",
            "source_info": {
                "paper_title": "A large language model framework for literature-based disease–gene association prediction",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Llama-8B",
            "name_full": "Llama-3.1-8B-Instruct (referred to as Llama-8B)",
            "brief_description": "An open-source 8-billion-parameter LLM used as a lower-cost alternative to GPT-3.5 for large-scale relation extraction; when used to run LLM-ORE on ~4M abstracts it produced substantially higher coverage of ClinVar DGs.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Llama-3.1-8B-Instruct used for large-scale LLM-ORE",
            "system_description": "An 8B-parameter instruct-tuned LLaMA variant was prompted with the same task-agnostic ORE prompts to extract relations across all 3,997,496 pubmedKB abstracts with DG co-occurrence; produced 74,132,940 relations covering 91.3% of ClinVar DGs and enabling ML-Ranker MAP of 81.6% on larger run.",
            "model_name": "Llama-3.1-8B-Instruct",
            "model_size": "8B",
            "scientific_domain": "Biomedical literature mining",
            "number_of_papers": "Applied to 3,997,496 pubmedKB abstracts with DG co-occurrence in the scaled run.",
            "law_type": "Same types as LLM-ORE: sentence-level relations aggregated to reveal empirical graded relationships and latent patterns in embedding space (pathogenic flow).",
            "law_examples": "Produced relation triplets enabling embedding and ranking; coverage increased to 91.3% of ClinVar DGs when Llama-8B was used at scale.",
            "extraction_method": "Prompted open relation extraction (same demonstration + target prompt) at large scale; relations aggregated and embedded as per LORE pipeline.",
            "validation_approach": "Downstream ranking performance measured (MAP = 81.6% on the scaled Llama-run), and ClinVar coverage assessed (91.3%).",
            "performance_metrics": "Scaled run with Llama-8B: 74,132,940 relations extracted; 91.3% ClinVar coverage; ML-Ranker achieved 81.6% MAP on the larger corpus. On the smaller subset substituting GPT-3.5 with Llama-8B yielded marginal performance loss (LLM-8B MAP ~79.5% vs GPT-3.5 MAP 79.9%).",
            "success_rate": "Measured by coverage and MAP: 91.3% ClinVar coverage and 81.6% MAP after scaling.",
            "challenges_limitations": "Smaller model capacity may slightly reduce extraction fidelity per-article compared with larger proprietary LLMs, but scaling compensated to increase coverage; context-window and prompt-engineering limitations remain.",
            "comparison_baseline": "Compared directly to GPT-3.5-based LORE runs (marginal performance drop on subset) and to GPT-4o direct prompting baseline (GPT-4o performed poorly). Llama-8B at scale outperformed initial GPT-3.5 ORE coverage by producing more relations overall (when applied to a much larger number of abstracts).",
            "uuid": "e4211.5",
            "source_info": {
                "paper_title": "A large language model framework for literature-based disease–gene association prediction",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o",
            "brief_description": "A more recent OpenAI LLM variant used as a direct prompting baseline to ask about DG pathogenicity; performed poorly as a direct question-answer baseline compared to the structured LORE pipeline.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-4o (used as direct prompting baseline)",
            "system_description": "GPT-4o (ver.2024-05-13) was directly prompted to assess pathogenicity of DGs as a baseline comparison; unlike the LORE pipeline this was a direct QA approach rather than structured extraction + embedding + supervised ranking.",
            "model_name": "GPT-4o",
            "model_size": null,
            "scientific_domain": "Biomedical knowledge QA baseline",
            "number_of_papers": "Queried as a baseline against PMKB-CV DGs (exact number of prompts equals number of DGs used for baseline evaluation; aggregated results reported).",
            "law_type": "Direct natural-language judgments about pathogenicity of DGs (not structured quantitative law extraction).",
            "law_examples": "Provided direct assessments of pathogenicity per DG but without producing structured relation triplets or embeddings; results used for baseline MAP comparison.",
            "extraction_method": "Direct prompted QA (not open relation extraction or embedding aggregation).",
            "validation_approach": "Compared predicted pathogenicity to ClinVar curated labels; MAP computed as baseline metric.",
            "performance_metrics": "GPT-4o baseline MAP = 31.7% on the PMKB-CV benchmark (substantially lower than LORE-based methods).",
            "success_rate": "Low effectiveness as a direct baseline: 31.7% MAP indicates poor recovery of ClinVar pathogenic genes when queried directly.",
            "challenges_limitations": "Direct QA suffers from hallucination and lack of verifiable traceable evidence; opaque parametric memory provides poor traceability to source literature; inferior performance compared with the structured LORE approach.",
            "comparison_baseline": "Served as a baseline; underperformed relative to LORE's LLM-EMB + ML-Ranker (87.7% - 90.0% MAP) and even relative to naive paper-counting baseline in some scenarios.",
            "uuid": "e4211.6",
            "source_info": {
                "paper_title": "A large language model framework for literature-based disease–gene association prediction",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation (RAG)",
            "brief_description": "A retrieval-augmented generation paradigm mentioned as a mitigation strategy for hallucination that constrains an LLM's output to retrieved supporting documents, but limited by retrieval scalability and coarse retrieval methods.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Retrieval-Augmented Generation (RAG)",
            "system_description": "RAG augments LLM generation by retrieving a small set of documents and conditioning the LLM on them so outputs are grounded; the paper cites RAG as a commonly used approach to increase verifiability for LLMs but notes scalability and retrieval-quality limitations (fast shallow retrieval misses nuanced content).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "NLP methodologies applied to literature mining",
            "number_of_papers": null,
            "law_type": "Not an extraction of a quantitative law itself, but a method to ground model outputs to retrieved evidence; relevant to reliable extraction of relationships from literature.",
            "law_examples": "",
            "extraction_method": "Retrieve top-k documents via sentence-similarity embeddings (fast, shallow retrieval) followed by LLM-conditioned generation.",
            "validation_approach": "Discussed as background and contrasted with LORE's approach of building an explicit knowledge graph of relations to support verifiability.",
            "performance_metrics": "",
            "success_rate": "",
            "challenges_limitations": "Scalability constraints: requires fast but coarse retrieval that can omit nuanced relevant articles; may lead to incomplete information capture.",
            "comparison_baseline": "Presented as a related technique that LORE improves upon by extracting a concise knowledge graph so subsequent embedding and modeling can be more scalable and verifiable.",
            "uuid": "e4211.7",
            "source_info": {
                "paper_title": "A large language model framework for literature-based disease–gene association prediction",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Large language models encode clinical knowledge",
            "rating": 2,
            "sanitized_title": "large_language_models_encode_clinical_knowledge"
        },
        {
            "paper_title": "The impact of large language models on scientific discovery: a preliminary study using GPT-4",
            "rating": 2,
            "sanitized_title": "the_impact_of_large_language_models_on_scientific_discovery_a_preliminary_study_using_gpt4"
        },
        {
            "paper_title": "Retrieve, summarize, and Verify: how will ChatGPT affect information seeking from the medical literature?",
            "rating": 1,
            "sanitized_title": "retrieve_summarize_and_verify_how_will_chatgpt_affect_information_seeking_from_the_medical_literature"
        },
        {
            "paper_title": "UMAP: uniform manifold approximation and projection",
            "rating": 1,
            "sanitized_title": "umap_uniform_manifold_approximation_and_projection"
        }
    ],
    "cost": 0.018131249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Problem Solving Protocol A large language model framework for literature-based disease-gene association prediction</p>
<p>Peng-Hsuan Li 
Datong Dist
Taiwan AI Labs
6F., No. 70, Sec. 1, Chengde Road10355TaipeiTaiwan</p>
<p>Yih-Yun Sun 
Datong Dist
Taiwan AI Labs
6F., No. 70, Sec. 1, Chengde Road10355TaipeiTaiwan</p>
<p>Hsueh-Fen Juan 
Datong Dist
Taiwan AI Labs
6F., No. 70, Sec. 1, Chengde Road10355TaipeiTaiwan</p>
<p>Department of Life Science
National Taiwan University
No. 1, Sec. 4, Roosevelt Rd10617TaipeiTaiwan</p>
<p>Center for Computational and Systems Biology
National Taiwan University
No. 1, Sec. 4, Roosevelt Road10617TaipeiTaiwan</p>
<p>Center for Advanced Computing and Imaging in Biomedicine
National Taiwan University
No. 1, Sec. 4, Roosevelt Road10617TaipeiTaiwan</p>
<p>Chien-Yu Chen 0000-0002-6940-6389
Datong Dist
Taiwan AI Labs
6F., No. 70, Sec. 1, Chengde Road10355TaipeiTaiwan</p>
<p>Center for Computational and Systems Biology
National Taiwan University
No. 1, Sec. 4, Roosevelt Road10617TaipeiTaiwan</p>
<p>Center for Advanced Computing and Imaging in Biomedicine
National Taiwan University
No. 1, Sec. 4, Roosevelt Road10617TaipeiTaiwan</p>
<p>Department of Biomechatronics Engineering
National Taiwan University
No. 1, Sec. 4, Roosevelt Road10617TaipeiTaiwan</p>
<p>Huai-Kuang Tsai 0000-0002-4200-8137
Datong Dist
Taiwan AI Labs
6F., No. 70, Sec. 1, Chengde Road10355TaipeiTaiwan</p>
<p>Institute of Information Science
Academia Sinica
No. 128, Academia Road, Section 211529NankangTaipeiTaiwan</p>
<p>Jia-Hsin Huang jiahsin.huang@gmail.com 
Datong Dist
Taiwan AI Labs
6F., No. 70, Sec. 1, Chengde Road10355TaipeiTaiwan</p>
<p>Datong Dist
Taiwan AI Labs
6F., No. 70, Sec. 1, Chengde Road10355TaipeiTaiwan</p>
<p>Problem Solving Protocol A large language model framework for literature-based disease-gene association prediction
AED27303585B563AB05DD7EDBA7B860210.1093/bib/bbaf070Received: November 15, 2024. Revised: January 9, 2025. Accepted: February 6, 2025literature miningbiomedical relation extractionNLPknowledge graphlarge language model
This study explores biomedical informatics and artificial intelligence, leveraging large language models and knowledge graphs to advance precision medicine and enhance the discovery of disease-gene relationships.</p>
<p>Introduction</p>
<p>Knowledge curation from scientific literature is fundamental to the advancement of research across disciplines [1][2][3].Traditionally, domain-specific knowledge accumulates incrementally and relies heavily on human expert review processes.The biomedical literature, for instance, is a key resource for identifying causal genetic elements associated with diseases and offering insights into clinical practice.Several expert-curated databases, such as ClinVar [4], COSMIC [5], OMIM [6], and PharmGKB [7], provide invaluable assessments of literature evidence.However, such resources are limited in scale because of the broad scope and the rapid expansion of scientific publications [8,9].Many computational approaches have been applied to enhance automation in biomedical literature-based discovery for different tasks, such as gene-disease association prediction [10][11][12], text mining and curation [13][14][15][16][17], and biomedical entity relation extraction [18][19][20][21].However, these methods primarily focus on extracting isolated sentences or paragraphs containing entities of interest rather than synthesizing comprehensive information across multiple sources and contexts.Thus, substantial efforts are required to create task-specific datasets and train models for each literature domain.</p>
<p>On the other hand, Machine Reading Comprehension (MRC) [22], wherein machines answer questions based on textual context, serves as a promising complement to human expertise in reading vast amounts of literature.Recent advancements in natural language processing, particularly the development of Large Language Models (LLMs), have significantly enhanced MRC capabilities to potentially accelerate knowledge synthesis [23][24][25].Recent LLMs, such as GPT-4, have demonstrated remarkable capabilities in textual comprehension across diverse domains; however, LLMs face challenges when it comes to reliability and verifiability [23,26].Specifically, LLMs are prone to hallucination, a phenomenon whereby plausible but factually incorrect information is generated.Moreover, the opaque parametric memory of LLMs poses a substantial obstacle to traceabilitysources of evidence supporting their statements are often unclear.To mitigate these concerns, researchers have applied Retrieval Augmented Generation (RAG) in LLM-based chatbots [27,28].RAG restricts the information source of an LLM to an explicit but small set of retrieved texts per user query.However, owing to scalability constraints, fast but shallow sentence similarity-based retrieval is required, leading to incomplete information capture as nuanced content and relevant articles are often missing.Figure 1.Overview of the literature-semantics framework.Given a large body of literature containing expert domain knowledge, the proposed framework creates a comprehensive unsupervised knowledge graph and numerical embeddings between entities.This enables large-scale supervised modeling of downstream tasks, where model predictions are accompanied by verifiable evidence of relations that can be traced back to the original articles.(a) The framework is applied to PubMed literature, and a knowledge graph containing semantic relations between diseases and genes and their mutations is created.(b) An embedding for disease-gene relationships, where each point in space contains the literature-semantic knowledge of a DG, is created.(c) The embedding is shown to contain a latent structure of DG pathogenicity.(d) We further analyze the disease-wise pathogenic f low and find that it is consistent across diseases and even smoother than the point-wise distribution.(e) An ML-ranker is trained to model the f low and to predict pathogenic genes for each disease, where the prediction scope is 200× larger than expert-curated supervision.(f) We curated 105 key semantics about DG pathogenicity with linguistic lemmas to automatically tag relations.(G) With the proposed framework, we facilitate future research on DG pathogenicity with a literature-scale knowledge base of predicted DG scores and supporting evidence.</p>
<p>In essence, scalable knowledge curation calls for a computational method that is inductive across domains and is capable of capturing nuanced textual context for knowledge synthesis, all while maintaining essential reliability and verifiability.To this end, we introduce LORE (LLM-based Open Relation extraction and Embedding), a novel literature semantics framework that encompasses the best of both worlds and is tested true in capturing disease-gene relationships across the PubMed literature (Fig. 1).</p>
<p>LORE leverages a two-stage reading methodology comprising LLM-based Open Relation Extraction (LLM-ORE) and LLM-based embedding (LLM-EMB) (Fig. 1a).First, LORE employs LLM-ORE to comprehend each article and generate atomic statements about entity relations therein.Curated knowledge is explicitly derived from individual articles, making the generated relations reliable and verifiable.Furthermore, this operation creates a comprehensive unsupervised knowledge graph of the literature, the conciseness of which makes it possible for LORE to then use LLM-EMB to encode the full relation knowledge between each entity pair and create numerical embeddings for downstream task-specific applications.In this work, we applied LORE to curate disease-gene relationship knowledge in the PubMed literature, using disease, gene, and variant annotations from pubmedKB [2].</p>
<p>The ClinVar [4] database is an important annotation repository of the relationships between genes and diseases.Using key disease-gene pairs from ClinVar as references, we evaluated the effectiveness of LORE to explore the latent space of gene-wise pathogenicity and disease-wise pathogenic f low across genes (Fig. 1b).In addition, we constructed machine learning models with the supervision of pathogenic genes from ClinVar to rank the relevance of gene pathogenicity using the semantic embeddings (Fig. 1b).</p>
<p>Finally, we curated a taxonomy of key semantics to use as tags for relations (Fig. 1c).We created PMKB-CV (pubmedKB-ClinVar) dataset, a novel resource that expands the scope of disease-gene relationship data.Notably, PMKB-CV encompasses more than 2097 diseases and covers disease-gene pairs (DGs) at a scale 200 times larger than that covered by ClinVar.Moreover, PMKB-CV provides rich annotations, including semantic embeddings, predicted DG scores, and verifiable knowledge graph relations with tags and source article IDs (Fig. 1c).In summary, our LORE framework harnesses the power of LLM-based MRC and enables literaturescale knowledge graph construction and downstream modeling.Importantly, PMKB-CV further bridges the gap between largescale computational analysis and human assessment, helping to advance our understanding of disease genetics and potential therapeutic targets.</p>
<p>Results</p>
<p>LLM-ORE curates semantic relations knowledge from literature</p>
<p>We applied LLM-ORE to PubMed abstracts for annotating diseasegene relationships and created a comprehensive unsupervised knowledge graph (Fig. 1a).A total of 11 million relations across 1.7 million abstracts were obtained by prompting GPT-3.5 [29].Using text-continuation prompts [30], we employed LLMs to analyze individual articles and extract atomic statements describing relations between pairs of entities.Figure 2 illustrates the prompt structure used to guide GPT-3.5 in this task.This prompt is composed of two key sections.The first section demonstrates a domain-agnostic Open Relation Extraction (ORE) [31].The text serves as a primer, independent of the specific biomedical context, to establish the expected format and level of detail for the extraction (see more details in Methods).Following the same structure as the demonstration, the second section applies the ORE process to the target article and entities under investigation.Notably, this approach allows for a generalized ORE from the literature, which is not constrained to predefined relation types or entity pairs.A total of 358,888 distinct semantic lemmas are present across the 11 million disease-gene relations.We reviewed 282 highcoverage lemmas in the knowledge graph and curated a taxonomy of 105 key semantics about pathogenicity (Fig. 3).The key semantics, automatically tagged to relations, served as a set of indexes to access the knowledge graph.We grouped the key pathogenicity semantics into four main classes.</p>
<p>Class 1, Relation, includes key semantics that directly describe the relation between a gene and a disease.For example, the key semantic 'mutation cosegregates with disease' in Class 1.3 describes the correlation between occurrence of a certain genetic mutation and whether the genome is from a patient of a certain disease.</p>
<p>Class 2, Mutation, conveys information about genetic mutations.This type of information implicates that the corresponding gene has a role in a certain disease.</p>
<p>Class 3, Disease, conveys information on the genetic aspects of diseases.This semantic implies that a certain gene is at play.Class 4 contains cohort and miscellaneous information that entails or hints at pathogenicity.</p>
<p>LLM-EMB embeddings capture underlying gene pathogenicity</p>
<p>In the second stage of LORE, we applied LLM-EMB to the PubMed DG knowledge graph created in the first stage using LLM-ORE.For each pair of entities, all their relations across all articles were encoded to a single vector, which contained the literature knowledge about their relationship.A dense 512-dimensional representation was created for each DG.</p>
<p>To analyze the latent pathogenicity structure within the literature-semantic embedding, we first displayed the embedding space using 2D UMAP [32] (Fig. 4a-d).Each point represented a DG.When points were colored by co-occurrence frequency in PubMed abstracts, high-frequency DGs were observed to be distributed throughout the space (Fig. 4a).When points were colored by ClinVar pathogenicity labels, pathogenic DGs were observed to cluster in a subspace (Fig. 4b).This subspace was well-captured by the pathogenic score prediction of ML-Ranker (See more details about the ML-Ranker in the following sections and in Methods).With an optimal pathogenic score threshold to split the DGs by color into red or gray, the distribution was close to that of the ClinVar labels (Fig. 4d).In contrast to the ClinVar labels, which were human-curated and sparse, the stratified pathogenic score predicted by ML-Ranker delineated a smooth landscape of pathogenicity (Fig. 4c).Thus, high-score DGs not curated yet in ClinVar will be of high interest to the biomedical community.</p>
<p>Next, we displayed the 3D linear subspaces of the LLM-EMB embedding and observed the DG pathogenicity distribution (Fig. 4e-g).The axes were calculated using Principal Component Analysis (PCA) or ridge regression, a regularized version of ordinary least squares regression, against ClinVar pathogenicity labels.For the PCA subspace (Fig. 4e), the top three dimensions explained 12.8% of the variance of the embedding.A latent structure of two manifolds was observed-a dense spherical cluster of gray DG points without ClinVar annotations and a curved pattern of gradual transition from gray to red (i.e., known pathogenic DGs annotated in ClinVar).For the subspaces spanned by a combination of PCA axes and regression axes (Fig. 4f,g), clearer manifolds were observed when more regression axes were included.With two regression axes, a distribution pattern aligned with ClinVar annotations, signifying an association with pathogenicity, was revealed.We noted that the regression axes spanned a smaller subspace of the embedding compared with the PCA axes.Nevertheless, they provided an analytical pathogenicity perspective of the unsupervised embedding.</p>
<p>We further analyzed the literature-semantics structure by visualizing the distribution of important semantic lemmas in the 3D PCA subspace (Fig. 4h-j).The frequency of each semantic lemma, such as 'cause,' was represented in log scale per DG using a cool-blue-to-warm-red color gradient.Distinct patterns emerged for various lemmas, particularly those lemmas with a f latter distribution, such as 'associate,' 'mutation,' and 'cause.' The semantic 'associate' (Fig. 4h) was observed to increase along a linear axis but was more prevalent (indicated by greener colors) in the curved arm than in the sparse parts (bluer).The semantic 'mutation' (Fig. 4i) was primarily distributed along the curved arm.Similarly, the semantic 'cause' (Fig. 4j) was concentrated along the curved arm, with a significant presence at the space correlated with ClinVar annotations.This visualization of semantic lemmas revealed a connection between the intrinsic semantic structure of LLM-EMB and the latent DG point-wise pathogenicity structure.</p>
<p>In short, our analyses revealed the connection between the literature semantics captured by LORE and the known pathogenic DGs in the ClinVar database, as illustrated in Fig. 4. Using both UMAP and PCA visualization techniques, we examined how these known pathogenic DGs are distributed within the semantic LLM-EMB embedding space.Notably, these pathogenic DGs clustered in specific subspaces that aligned with disease-related semantic concepts -particularly terms like 'mutation' and 'cause'.This clustering pattern demonstrates the potential value of LLM-EMB embeddings as robust features for developing our ML-Ranker system for pathogenicity prediction.</p>
<p>Pathogenic flow in the literature-semantic space</p>
<p>In this section, we explore the embedding space underlying the DG pathogenicity distribution (Fig. 5).First, to model pathogenicity, a straightforward approach would be to model the distribution of pathogenic DGs (i.e., red points) in the embedding space (Fig. 5a,b).Clear subspaces of pathogenicity and nonpathogenicity were evident in the literature-semantic space visualized using 3D linear axes (Fig. 4e-g).These low-rank subspaces can be seen in the gray balls and the ends of the gray-to-red arm.However, the gray and red dots co-occur in some places, such as the center of the arm.To distinguish between gray and red dots, high-dimensional hyperplanes or complex nonlinear subspaces are needed, requiring more data and hampering generalization.</p>
<p>However, the fundamental task is to predict the most relevant pathogenic genes for each disease.If the gray and red dots for each disease are distributed separately to the two ends of a linear axis or, more generally, a smooth curve, pathogenic and nonpathogenic genes can be distinguished by splitting the curve.Furthermore, if the curves are consistent across diseases, the relevant pathogenic genes for every disease can be identified by monotonically raising the predicted relevance along the curves.Under this condition, perfect modeling can be achieved even if nonpathogenic and pathogenic DGs from different diseases are mixed in the embedding space (Fig. 5c).</p>
<p>In this study, we defined pathogenic f low as the direction and magnitude of nonpathogenicity to pathogenicity at each location in space.We started by calculating the disease-wise f low direction at each DG.Then, we quantized the f low direction at each location in space, using f low magnitude to ref lect consistency.We observed a smooth, cross-disease consistent field of pathogenic f low in the literature-semantic embedding (Fig. 5d).A consistent f low of nonpathogenicity to pathogenicity was noted along the arm-shaped manifold of the gray-to-red transition, implying that although DG point-wise modeling was difficult in the central mixture of the arm, disease-wise modeling was smoother and much more linear.</p>
<p>Literature-scale pathogenicity prediction by ML-ranker</p>
<p>We built an ML-Ranker that can model disease-wise pathogenic f low and score DGs that co-occur in PubMed abstracts.We applied the lambda objective with Gradient-Boosted Decision Trees (GBDT) [33] to directly model disease-wise pathogenic f low.</p>
<p>We compiled the PMKB-CV dataset to validate the proposed approach (Fig. 6a).PMKB-CV contains 2097 diseases that are present in both pubmedKB and ClinVar.For these diseases, 652 701 DGs co-occurred in PubMed abstracts, whereas only 3004 DGs were human-curated by ClinVar.Paper abstract co-occurrence covered 94.8% of the 3004 known pathogenic DGs, whereas LLM-ORE relations covered 71.4% (Fig. 6b).We included pubmedKB annotations, such as the number of paper abstracts in which a DG co-occurs in the dataset, and also used them as model features.Moreover, we used zero embedding for DGs without relations.Consequently, all DGs in the dataset can be uniformly used for training and prediction.We used leave-one-disease-out cross-validation to evaluate the performance of ML-Ranker for predicting pathogenic genes for each disease iteratively.An average precision (AP) score was calculated for each disease and Mean Average Precision (MAP) was used to evaluate the overall performance of the ranker.As a baseline, we directly asked GPT-3.5 (ver.2023-06-13) and GPT-4o (ver.2024-05-13) about the pathogenicity of each DG.In addition, to put into perspective the effectiveness of the curated key semantics in identifying crucial literature evidence, we tested a DG pathogenicity prediction method of simply counting the number of tagged relations per DG.Of note, the curated key semantics were only used in the experiment 'LLM-ORE (key semantics)' (Fig. 6b).</p>
<p>For all DGs in PMKB-CV, ML-Ranker achieved an MAP of 79.9%, which was a significant enhancement over the 69.4% MAP of co-occurrence paper counting and the 31.7%MAP of GPT-4o (Fig. 6b).Similar performance (MAP = 79.2%) was observed when applying 5-fold cross-validation of disjoint genes subsets (Supplementary Fig. 1).</p>
<p>When focusing specifically on those DGs with LLM-ORE annotations, ML-Ranker yielded a remarkable MAP of 90.0% (Fig. 6b).The predictive performance (AP) of ML-Ranker was statistically significantly better than other methods including co-occurrence paper counting (LLM-#paper), literature-semantic relation counting (LLM-ORE), and latent pathogenic f low modeling (LLM-EMB) (Fig. 6c).For the highly prevalent diseases that co-occurred with the highest number of genes in pubmedKB, ML-Ranker achieved a robust MAP of 81%, compared with the 67% MAP of co-occurrence paper counting (Fig. 6d).GPT-4o does not perform well.In comparison, LLM-EMB linear regression alone achieves 87.7% performance, whereas the full-f ledged MLranker provides higher performance at 90.0% or higher coverage at 94.8%.(c) Dispersion of ranking performance across diseases for each method and the p-values of distribution differences by one-sided Wilcoxon signed-rank test.LLM-ranker significantly outperformed baseline methods.(d) Performance across diseases of different scopes.The naive paper counting method encountered difficulties while ranking DGs for diseases that co-occurred with many genes in PMKB, but our semantic embedding approach remained robust.(e) Extending the scope of LORE using the public llama-8B model.Replacing GPT-3.5 with llama-8B resulted in marginal performance loss.The smaller model was further applied to all 3.9 million papers with DG co-occurrence.The final extracted relations covered 91.3% of ClinVar DGs and achieved 81.6% ranking performance.(f) Top-ranked DGs, seven out of 586 in PMKB, for Tourette syndrome accompanied by their literature relation evidence.</p>
<p>Furthermore, we extended LORE to use the open-source Llama-8B model (ver.Llama-3.1-8B-Instruct).Using a much smaller and accessible model, ML-Ranker achieved a comparable 79.5% MAP to the 79.9% MAP of using GPT-3.5.Leveraging Llama-8B, we processed all 3,997,496 pubmedKB abstracts with DG co-occurrence and curated 74,132,940 relations.The resulting relations covered 91.3% of ClinVar DGs, enabling ML-Ranker to achieve a notable ranking performance of 81.6% MAP (Fig. 6e).</p>
<p>Finally, the DG scores and ranking provided by ML-Ranker are accompanied by literature evidence (Fig. 6f).Our approach facilitates future expert assessment of DG pathogenicity by a quick grasp of literature knowledge with key semantics relations and relevant articles.</p>
<p>Discussion</p>
<p>Recent advancements in LLMs aim to automate complex sensemaking as human endeavors in reading and connecting information across large collections of scientific literature [34].Our study introduces LORE, a novel literature semantics framework that fundamentally reframes how we leverage LLMs to extract and use knowledge from scientific literature.</p>
<p>The LORE framework offers several key advantages.First, knowledge synthesis using the LORE approach constructs a literature knowledge graph of verifiable factual statements linked to the sources.Second, LORE offers a scalable framework for knowledge synthesis from large amounts of article texts.LORE extracts original article texts and transforms them into a concise knowledge graph.The approach is more efficient than traditional retrieval augmented generation approaches that select only a small set of articles for an LLM to read.The knowledge graph is much more concise compared with the original articles.This reduction in size and complexity allows for a more efficient representation of information; all the relevant knowledge can then be embedded for downstream tasks.In addition, LORE allows new publications to be annotated, thereby continually expanding the knowledge graph.Third, this approach places much less demand on the capability of LLMs, compared with directly asking LLMs expert domain questions.Using LORE, we have captured gene pathogenicity with GPT-3.5 and Llama-8B (Fig. 6e), a feat far from being achieved by directly asking GPT-4o (Fig. 6b).Indeed, small and open-source LLMs have been demonstrated to be competent for article-level comprehension [35,36], hence the methodology is not constrained to enterprise LLMs.Finally, our framework demonstrates remarkable efficacy in capturing disease-gene relationships through unsupervised relationship extraction and embedding, and users can also employ LORE with prompt engineering and fine-tuning to annotate task-specific knowledge across various domains of scientific inquiry [37,38].</p>
<p>When applying LORE to the complex landscape of DG relationships, we demonstrated the presence of a latent smooth field of cross-disease consistent pathogenic f low in the unsupervised literature-semantic embeddings.This discovery reveals that although pathogenic and nonpathogenic DGs from different diseases may occupy similar locations in the embedding space, a consistent directional f low of pathogenicity exists in terms of semantics.To illustrate, consider a simplified one-dimensional embedding axis where rare and common diseases coexist (Supplementary Fig. 2).Suppose that D1 is a rare disease reported in a few studies and that D2 is a common disease whose association with many genes is discussed in a multitude of papers; in this scenario, the following literature annotations, DG locations, and pathogenicity labels are possible:</p>
<p>G1 is not related to D1. (x = 0, y = non-pathogenic).</p>
<p>G2 mutation is found in a D1 patient. (x = 1, y = pathogenic). G3 mutation is found in a D2 patient; G3 is associated with D2. (x = 2, y = non-pathogenic). G4 mutation is found in a D2 patient; G4 causes D2. (x = 3, y = pathogenic).</p>
<p>Although the pathogenic D1G2 and the non-pathogenic D2G3 are mixed in the center of the axis, literature evidence about pathogenicity consistently increases along the axis.As a result, even when pathogenic and non-pathogenic associations are interspersed due to inter-disease differences such as popularity, the literature-semantic axis provides for a cross-disease consistent linear f low, enabling accurate pathogenicity modeling across diseases.</p>
<p>Our initial analyses of this pathogenic f low revealed clusters of disease-specific pathogenic curves.Notably, we found that these clusters often form a continuum, with the endpoint of one cluster serving as the starting point for another.This observation suggests a broader, interconnected field of pathogenic relationships across diseases, offering new perspectives on the complex landscape of genetic pathogenicity.</p>
<p>LORE curates knowledge for entity pairs that co-occur in literature articles.For the study of disease-gene pathogenicity, we noted that the potential curation scope was larger than the PMKB-CV dataset.PMKB-CV contained 2097 diseases that had both pubmedKB DG co-occurrence and known ClinVar pathogenic DGs (Fig. 6a); the full pubmedKB contained 3 128 402 DGs co-occurred in abstracts, spanning 8894 diseases (Supplementary Fig. 3).In this study, we focused on those 2097 diseases that could be validated by ClinVar, but the potential curation scope was as large as the 3 128 402 DGs.On the other hand, we also noted that the full ClinVar contained 4311 known pathogenic DGs, 1307 of which had no pubmedKB abstract co-occurrence.This was the inherent limitation to article abstract-based MRC.</p>
<p>Conclusion</p>
<p>In summary, our study makes three significant contributions to the field.First, it presents a novel literature semantics framework that addresses the long-standing challenges of comprehensiveness, reliability, and verifiability in machine reading comprehension.Second, it demonstrates the efficacy of LORE in capturing complex pathogenic relationships across diseases to reveal new insights into pathogenic f low.Finally, it provides a literature-scale dataset that not only complements existing resources such as ClinVar but also offers a knowledge graph of DG relationships with graded pathogenicity scores for genetic prioritization in clinical practice.The methodology of LORE is a general improvement on LLM-based machine reading comprehension, paving the way for bridging vast literature resources and actionable scientific knowledge to realize accelerated discoveries across scientific disciplines.</p>
<p>Methods</p>
<p>Two-stage reading comprehension of LORE</p>
<p>In the first stage of LORE, we applied LLM-ORE by prompting LLMs. Figure 2 shows the actual prompt we used.The demonstration consists of an article with a topic as Martin Likes Fish, target entity pair 'Martin' and 'fish', along with lists of unwanted and desired annotations.This section implicitly specifies the ORE task and the following required properties.</p>
<p>(1) The annotations should be concrete statements of fact implied by the article.</p>
<p>(2) The annotations should follow a structured format, specifically a list of relational triplets.</p>
<p>(3) Each relational triplet should be &lt; 'subject', 'predicate', 'object' &gt; .</p>
<p>(4) The subject should contain the first entity, and the first entity should only appear in the subject.</p>
<p>(5) The object should contain the second entity, and the second entity should only appear in the object.</p>
<p>(6) The predicate should be a concise, self-contained description of the relation between the subject and the object.</p>
<p>(7) Abstract understanding of the article is allowed beyond sentencelevel syntax.</p>
<p>The approach allows for abstract understanding beyond sentence-level syntax.The requirements are better understood through demonstration rather than explicit definitions.For instance, instead of explaining the terms 'subject,' 'predicate,' and 'object,' the examples and counterexamples in the demonstration make the concept clear.In addition, the examples illustrate behaviors that are easier to grasp instinctively rather than by complex rules.Examples include the removal of 'also' from 'Martin also loves eating fish', the rewriting of 'large fish scare him' to 'scare of', and the digested understanding of 'Martin dreamed about fishing' from 'he has a dream.The dream is about fishing'.Finally, we note that the ending '-' is important in ensuring that LLM follows the desired list format.</p>
<p>Formally, the desired generation
g = f LLM−ORE p, e 1 , e 2 ; m
where p is the target article, e 1 and e 2 are the names of the target entities, and m is the LLM model.In this work, we use the paper title and abstract as p.For m, gpt-3.5-turbo-0613 is used.The function maps &lt; p, e 1 , e 2 &gt; to g, the list of parsed relational triplets, using the text-continuation prompt shown in Fig. 2.</p>
<p>In the second stage of LORE, LLM reads all relations between an entity pair and produces a numerical representation of knowledge about their relationships.For example, the following relations between the entity e 1 and the entity e 2 are read by LLM as the following document.'e1', 'causes', 'e2'.'e1 mutations', 'are frequently encountered in', 'e2 patients'.'e1 haploinsufficiency', 'results in', 'e2'.Document: 'e1 causes e2.e1 mutations are frequently encountered in e2 patients.e1 haploinsufficiency results in e2.'.</p>
<p>If the document is larger than the allowed context of an LLM, it is split into multiple sub-documents, each of which contains as many relations as possible.</p>
<p>Formally, the embedding vector of an entity pair is given by
v = f LLM−EMB D e1,e2 ; m =</p>
<p>Modeling the pathogenic flow with ML-ranker</p>
<p>To visualize the disease-wise pathogenic f low, we define the f low as a unit vector at each DG that points to the pathogenic direction of disease D at the embedding location of DG.Formally, the f low vector is given by
u DG = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ v DG − 1 |S 0 D | DG ∈S 0 D v DG if P(DG) = 1 1 |S 1 D | DG ∈S 1 D v DG − v DG if P(DG) = 0 u DG = u DG | u DG |
where v denotes the embedding vector by LLM-EMB, S 0 D and S 1 D denote the sets of non-pathogenic and pathogenic diseasegene pairs of D respectively, and P maps non-pathogenic and pathogenic pairs to 0 and 1, respectively.In other words, each non-pathogenic DG has a unit f low vector directed at the average embedding of the pathogenic DGs of the same disease, and each pathogenic DG has a unit f low vector directed from the average embedding of the non-pathogenic DGs of the same disease.Then, we quantize the f low vectors for each cube in space by averaging them.Formally, a quantized f low vector is given by
u L = 1 | L | DG∈L u DG
where L is the set of DGs in a cube subspace.Finally, we show the field of pathogenic f low in Fig. 5d, where each arrow corresponds to a quantized f low vector.The arrow direction is the aggregated disease-wise f low direction from non-pathogenicity to pathogenicity, and the arrow length, proportional to | u L |, reflects the degree of cross-disease consistency at that location.</p>
<p>As shown in Fig. 5c, the essence of modeling the pathogenic f low is to model the difference between non-pathogenic and pathogenic genes per disease.Specifically, suppose DG1 and DG2 correspond to the same disease but P(DG1) = 1 and P(DG2) = 0. Let s denote the predicted pathogenicity score.Then one would want to maximize the probability Pr (P(DG1) &gt; P(DG2)) by minimizing the following RankNet [39]  To make sure the most relevant genes are ranked on top for each disease, the final gradient, λ, is the gradient of the RankNet loss multiplied by the change in Normalized Discounted Cumulative Gain (NDCG) [40].Formally, this LambdaRank [41] gradient is given by λ DG1,DG2 = ∂L ∂s DG1</p>
<p>• NDCG (DG1, DG2)</p>
<p>In this work, we use λGBDT, the lambda objective combined with GBDT [33], as the ML-Ranker to explicitly model the pathogenic f low in the literature-semantic space.</p>
<p>To evaluate the ranking performance for each disease, AP is used to see if the known pathogenic genes for a disease are ranked on top.Formally, for each disease,
AP = 1 R N k=1 known(k) × precision(k)
where N is the number of ranked genes for that disease, and R is the number of ranked known pathogenic genes for that disease.</p>
<p>known(k) = 1 if the gene ranked at k is a known pathogenic gene for that disease; otherwise known(k) = 0. precision(k) is the percentage of known pathogenic genes among genes ranked top-k for that disease.</p>
<p>Key semantics curation</p>
<p>The curation process consists of three main steps: linguistic lemma extraction, important lemma identification, and manual taxonomy construction.In the first step, the sentential relations extracted by LLM-ORE are tokenized to bags of words, and word inf lections are lemmatized to dictionary form.For example, cause, causes, caused, and causing all correspond to the same linguistic lemma.At this stage, entity names are also filtered out.</p>
<p>In the second step, important lemmas are identified using a coverage filter and a precision filter.The coverage filter demands a lemma to appear in LLM-ORE relations of at least n DGs.The precision filter requires that a certain proportion r of all the relations involving a lemma be from known pathogenic DGs, as indicated by ClinVar.The parameters can be adjusted according to the desired scope of key semantics.In this work, we selected a coverage parameter n = 100 and a precision parameter r = 50% and resulted in 282 important lemmas.</p>
<p>The final step involves manually curating a taxonomy of 105 key semantics by examining the LLM-ORE relations associated with these important lemmas During curation, we sampled 10 relations from known pathogenic DGs and 10 relations from other DGs to inspect for each lemma.The resulting key semantics were then used to tag all relevant relations in the respective lemmas.As a result, the LLM-ORE knowledge graph contains sentential relations linked to the semantic taxonomy of pathogenicity.</p>
<p>Furthermore, we experimented with a DG pathogenicity prediction method where the number of tagged relations for each DG is directly used as its pathogenic score.We note that the curation process has used the ClinVar information, so the generalizability of the ranking performance of this method is not directly comparable to other methods.Nevertheless, we put the effectiveness of the curation method into perspective, showing what performance and scope DG pathogenicity researchers can expect when using the key semantics tags to grasp the literature knowledge and identify relevant relations and articles for their DGs of interest.</p>
<p>Constructing the PMKB-CV dataset</p>
<p>We constructed the PMKB-CV dataset as a large-scale complement of ClinVar and an evaluation benchmark for our proposed methodology.The scope of the dataset is defined using disease IDs from Medical Subject Headings (MeSH), a vocabulary thesaurus maintained by the Nation Library of Medicine (NLM), and Homo sapiens protein-coding gene IDs from the National Center for Biotechnology Information (NCBI).</p>
<p>The PMKB-CV dataset comprises two main components including literature-based and expert database parts.For the literature part, we utilized the annotations from pubmedKB [2].The gene and variant mentions are both indexed by NCBI gene IDs, and we considered the occurrence of either a gene or its variant as an occurrence.This approach yielded 3,128,402 DG pairs with co-occurrence within abstracts, encompassing 8894 diseases and 18,393 genes.In addition, we extracted a subset of PubMed articles as the most relevant literature for the study of diseasegene relationships via a bootstrapping iteration with pubmedKB annotations as features.Using leave-one-disease-out training, we predicted a bootstrap score for each DG.Then, the literature subset is formed by the articles associated with the top three genes for each disease, the top three diseases for each gene, and the top 15 K DGs.The resulting subset contains 1,745,538 articles, for which we applied LORE and curated 11,285,095 relations.</p>
<p>The expert database part was derived from ClinVar [4], which provides gene-disease relationship data.As ClinVar uses OMIM (Online Mendelian Inheritance in Man) [6] numbers for disease indexing, we applied UMLS (Unified Medical Language System) [42], a vocabulary alignment dataset maintained by NLM, to map OMIM numbers to MeSH IDs.This process resulted in 4311 known pathogenic DGs, spanning 3175 distinct diseases and 2416 distinct genes.</p>
<p>The final PMKB-CV dataset was created by including those diseases that have both pubmedKB co-occurrence DGs and Clin-Var known pathogenic DGs.Statistics of the resulting dataset are shown in Fig. 6a.</p>
<p>Key Points</p>
<p>• We present a scalable framework that achieves 90% mean AP in identifying pathogenic gene associations across 2097 diseases, demonstrating remarkable accuracy in automated literature interpretation while effectively mitigating LLM hallucination risks.• Our analysis of literature-based semantic embeddings revealed a consistent directional pattern in how pathogenic genes are represented across different diseases.While both pathogenic and non-pathogenic disease-gene pairs cluster similarly in the embedding space, we discovered a distinct semantic f low that indicates pathogenicity.This pattern could help automate the identification of disease-causing genes from scientific literature.• The framework provides a reproducible methodology for leveraging LLMs in biomedical literature analysis, offering a valuable tool for researchers and clinicians in understanding disease mechanisms and identifying potential therapeutic targets.</p>
<p>Figure 2 .
2
Figure 2. Annotating articles with LLM-ORE (open relation extraction).Text-continuation prompts are used to make LLM write down its understanding of an article in the form of atomic factual statements.The ORE task is crafted to extract concise relations between entities at the article level.For example, 'Martin dreamed about fishing' requires comprehension and rewriting of several sentences.Also, common unwanted behaviors are avoided by providing examples of bad relations.We applied the task-agnostic prompt to extract an open set of diverse and comprehensive entity relationships for academic literature.</p>
<p>Figure 3 .
3
Figure 3. Curated key semantics.We created a taxonomy of four main classes, 15 subclasses, and 105 key semantics about disease-gene pathogenicity.With their corresponding linguistic lemmas, tags are added to relations automatically.Here, two key semantics and sample relations are shown for every subclass.</p>
<p>Figure 4 .
4
Figure 4. LLM-EMB literature-semantic embedding visualization.(a-d) Visualization with UMAP to analyze the latent pathogenicity structure within the literature-semantic embedding.Points are colored by the number of papers (#paper) (a), ClinVar pathogenicity labels (b), graded ML-ranker prediction (c), and binary ML-ranker prediction (d).The sparse ClinVar-curated red pathogenic DGs are seen clustering toward a subspace, captured well by ML-ranker, which also provides a smooth landscape of graded predictions for uncurated DGs.(e-g) Visualization with linear axes calculated using PCA (e), ridge (g), and their combination (f).A point is colored red if it is a known pathogenic DG in ClinVar, and DGs with unknown pathogenicity are colored gray.A latent structure of two manifolds-a gray ball of nonpathogenicity and a curved arm of transition from nonpathogenicity to pathogenicity-resides in the semantic space.(h-j) PCA visualization colored by distributions of literature semantics 'associate' (h), 'mutation' (i), and 'cause' (j).The connection between the smooth semantics distribution and the sparsely-curated ClinVar pathogenicity distribution can be seen.</p>
<p>Figure 5 .
5
Figure 5. Pathogenic f low in the literature-semantic space.(a) Suppose in the semantic embedding, nine DGs from three different diseases reside on a curve.(b) For the DG point-wise objective, the disease group information is not used, and absolute zero-one labels are the prediction target.As a result, a non-linear function along the curve must be learned.(c) For the disease-wise f low objective, DGs are grouped by disease, and relative f low directions are the prediction target.Because of the cross-disease consistency of the f low, a linear function along the curve will be learned to rank DGs for every disease perfectly.(d) Visualization of the actual pathogenic f low.A smooth, cross-disease consistent field of pathogenic f low is seen residing in the literature-semantic space.</p>
<p>Figure 6 .
6
Figure 6.PMKB-CV dataset and ranking performance.(a) Statistics of the PMKB-CV dataset.For 2097 diseases, the literature semantics framework has a 200× prediction scope against curated DGs.(b) Mean average precision (MAP) of the ranking performance and the ClinVar coverage of different methods.GPT-4o does not perform well.In comparison, LLM-EMB linear regression alone achieves 87.7% performance, whereas the full-f ledged MLranker provides higher performance at 90.0% or higher coverage at 94.8%.(c) Dispersion of ranking performance across diseases for each method and the p-values of distribution differences by one-sided Wilcoxon signed-rank test.LLM-ranker significantly outperformed baseline methods.(d) Performance across diseases of different scopes.The naive paper counting method encountered difficulties while ranking DGs for diseases that co-occurred with many genes in PMKB, but our semantic embedding approach remained robust.(e) Extending the scope of LORE using the public llama-8B model.Replacing GPT-3.5 with llama-8B resulted in marginal performance loss.The smaller model was further applied to all 3.9 million papers with DG co-occurrence.The final extracted relations covered 91.3% of ClinVar DGs and achieved 81.6% ranking performance.(f) Top-ranked DGs, seven out of 586 in PMKB, for Tourette syndrome accompanied by their literature relation evidence.</p>
<p>d∈De 1 ,e 2
2
emb d; m × | d | d∈De 1 ,e 2 | d | where e 1 and e 2 are the target entities, D e1,e2 is the set of all sub-documents, usually just one full document, containing the relations between e 1 and e 2 , and m is the LLM model.In this work, we employed the text-embedding-3-large from OpenAI for m, and the length of each sub-document | d | is its number of tokens according to m.</p>
<p>loss.L DG1,DG2 = − log Pr (P(DG1) &gt; P(DG2)) = log 1 + e −σ (sDG1−sDG2)</p>
<p>AcknowledgementsThis manuscript was edited by Wallace Academic Editing.The authors also thank Dau-Ming Niu and Yun-Ru Chen at the Taipei Veterans General Hospital in Taiwan for their support (NSTC 113-2634-F-A49-003).Code availabilityThe code supporting the conclusions of this study is available on GitHub at https://github.com/ailabstw/LORE.Data availabilityThe PMKB-CV datasets supporting the findings of this study are available at https://doi.org/10.5281/zenodo.14607639.FundingThis work was supported in part by the Center for Advanced Computing and Imaging in Biomedicine (NTU-113 L900701) from The Featured Areas Research Center Program within the framework of the Higher Education Sprout Project by the Ministry of Education in Taiwan.Author contributionsSupplementary dataSupplementary data are available at Briefings in Bioinformatics online.Conf lict of interest: None declared.
How user intelligence is improving PubMed. N Fiorini, R Leaman, D J Lipman, 10.1038/nbt.4267Nat Biotechnol. 362018</p>
<p>pubmedKB: an interactive web server for exploring biomedical entity relations in the biomedical literature. P H Li, T F Chen, J Y Yu, 10.1093/nar/gkac310Nucleic Acids Res. 502022</p>
<p>PubMed and beyond: biomedical literature search in the age of artificial intelligence. Q Jin, R Leaman, Z Lu, 10.1016/j.ebiom.2024.104988EBioMedicine. 1001049882024</p>
<p>ClinVar: public archive of interpretations of clinically relevant variants. M J Landrum, J M Lee, M Benson, 10.1093/nar/gkv1222Nucleic Acids Res. 442016</p>
<p>COSMIC: the catalogue of somatic mutations In cancer. J G Tate, S Bamford, H C Jubb, 10.1093/nar/gky1015Nucleic Acids Res. 472019</p>
<p>OMIM.org: leveraging knowledge across phenotype-gene relationships. J S Amberger, C A Bocchini, A F Scott, 10.1093/nar/gky1151Nucleic Acids Res. 472019</p>
<p>An evidence-based framework for evaluating pharmacogenomics knowledge for personalized medicine. M Whirl-Carrillo, R Huddart, L Gong, 10.1002/cpt.2350Clin Pharmacol Ther. 1102021</p>
<p>Biomedical language processing: what's beyond PubMed?. L Hunter, K B Cohen, 10.1016/j.molcel.2006.02.012Mol Cell. 212006</p>
<p>Manual curation is not sufficient for annotation of genomic databases. W A Baumgartner, Jr, K B Cohen, L M Fox, 10.1093/bioinformatics/btm229Bioinformatics. 232007</p>
<p>Extraction of relations between genes and diseases from text and large-scale data analysis: implications for translational research. A Bravo, J Pinero, N Queralt-Rosinach, 10.1186/s12859-015-0472-9BMC Bioinform. 16552015</p>
<p>DisGeNET: a comprehensive platform integrating information on human diseaseassociated genes and variants. J Pinero, À Bravo, N Queralt-Rosinach, 10.1093/nar/gkw943Nucleic Acids Res. 452017</p>
<p>RENET: a deep learning approach for extracting gene-disease associations from literature. Y Wu, R Luo, Hcm Leung, 10.1007/978-3-030-17083-7_17Research in Computational Molecular Biology. 114672019</p>
<p>PGxCorpus, a manually annotated corpus for pharmacogenomics. J Legrand, R Gogdemir, C Bousquet, 10.1038/s41597-019-0342-9Sci Data. 732020</p>
<p>ACE2 expression is increased in the lungs of patients with comorbidities associated with severe COVID-19. Bgg Pinto, Aer Oliveira, Y Singh, 10.1093/infdis/jiaa332J Infect Dis. 2222020</p>
<p>Molecular and networklevel mechanisms explaining individual differences in autism spectrum disorder. A M Buch, P E Vértes, J Seidlitz, 10.1038/s41593-023-01259-xNat Neurosci. 262023</p>
<p>Graph embedding-based link prediction for literature-based discovery in Alzheimer's disease. Y Pu, D Beck, K Verspoor, 10.1016/j.jbi.2023.104464J Biomed Inform. 1451044642023</p>
<p>The STRING database in 2023: protein-protein association networks and functional enrichment analyses for any sequenced genome of interest. D Szklarczyk, R Kirsch, M Koutrouli, 10.1093/nar/gkac1000Nucleic Acids Res. 512023</p>
<p>DTMiner: identification of potential disease targets through biomedical literature mining. D Xu, M Zhang, Y Xie, 10.1093/bioinformatics/btw503201632Bioinformatics</p>
<p>A global network of biomedical relationships derived from text. B Percha, R B Altman, 10.1093/bioinformatics/bty114Bioinformatics. 342018</p>
<p>BioREx: improving biomedical relation extraction by leveraging heterogeneous datasets. P T Lai, C H Wei, L Luo, 10.1016/j.jbi.2023.104487J Biomed Inform. 1461044872023</p>
<p>PubTator 3.0: an AI-powered literature resource for unlocking biomedical knowledge. C H Wei, A Allot, P T Lai, 10.1093/nar/gkae235Nucleic Acids Res. 522024</p>
<p>Neural machine reading comprehension: methods and trends. S Liu, X Zhang, S Zhang, 10.3390/app9183698Appl Sci. 936982019</p>
<p>Benefits, limits, and risks of GPT-4 as an AI Chatbot for medicine. P Lee, S Bubeck, J Petro, 10.1056/NEJMsr2214184N Engl J Med. 3882023</p>
<p>Large language models encode clinical knowledge. K Singhal, S Azizi, T Tu, 10.1038/s41586-023-06291-2Nature. 6202023</p>
<p>Assessing GPT-4 for cell type annotation in singlecell RNA-seq analysis. W Hou, Ji Z , 10.1038/s41592-024-02235-4Nat Methods. 212024</p>
<p>Retrieve, summarize, and Verify: how will ChatGPT affect information seeking from the medical literature?. Q Jin, R Leaman, Z Lu, 10.1681/ASN.0000000000000166J Am Soc Nephrol. 342023</p>
<p>Retrieval-augmented generation for knowledge-intensive NLP tasks. P Lewis, F Petroni, V Karpukhin, Adv Neural Inf Process Syst. 332020</p>
<p>Retrieval-augmented generation for large language models: a survey. Y Gao, Y Xiong, X Gao, 10.48550/arXiv.2312.10997arXiv:2312.109972023arXiv Preprint</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, Adv Neural Inf Process Syst. 352022</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, Adv Neural Inf Process Syst. 332020</p>
<p>Effectiveness and efficiency of open relation extraction. F Mesquita, J Schmidek, D Barbosa, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. the 2013 Conference on Empirical Methods in Natural Language ProcessingSeattle, Washington, USAAssociation for Computational Linguistics2013</p>
<p>UMAP: uniform manifold approximation and projection. L Mcinnes, J Healy, N Saul, 10.21105/joss.00861J Open Source Softw. 38612018</p>
<p>Adapting boosting for information retrieval measures. Q Wu, Cjc Burges, K M Svore, 10.1007/s10791-009-9112-1Inf Retr. 132009</p>
<p>The impact of large language models on scientific discovery: a preliminary study using. M Research Ai4science, Azure Quantum, M , 10.48550/arXiv.2311.07361arXiv:2311.073612023GPT-4. arXiv Preprint</p>
<p>Benchmarking large language models for news summarization. T Zhang, F Ladhak, E Durmus, 10.1162/tacl_a_00632Trans Assoc Comput Linguist. 122024</p>
<p>G Team, T Mesnard, C Hardin, 10.48550/arXiv.2403.08295arXiv:2403.08295open models based on Gemini research and technology. 2024arXiv Preprint</p>
<p>QLoRA: efficient Finetuning of quantized LLMs. T Dettmers, A Pagnoni, A Holtzman, Adv Neural Inf Process Syst. 362023</p>
<p>MEDITRON-70B: scaling medical Pretraining for large language models. Z Chen, A H Cano, A Romanou, 10.48550/arXiv.2311.16079arXiv:2311.160792023arXiv Preprint</p>
<p>Learning to rank using gradient descent. C Burges, T Shaked, E Renshaw, Proceedings of the 22nd international conference on Machine learning -ICML '05. the 22nd international conference on Machine learning -ICML '05New York, NY, USAAssociation for Computing Machinery2005</p>
<p>Cumulated gain-based evaluation of IR techniques. K Järvelin, J Kekäläinen, 10.1145/582415.582418ACM Trans Inf Syst. 202002</p>
<p>Learning to rank with nonsmooth cost functions. C Burges, R Ragno, Q Le, 10.7551/mitpress/7503.003.0029Adv Neural Inf Process Syst. 192006</p>
<p>The unified medical language system (UMLS): integrating biomedical terminology. O Bodenreider, 10.1093/nar/gkh061Nucleic Acids Res. 322004</p>
<p>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup. Author The, 10.1093/bib/bbaf070Briefings in Bioinformatics. 26120252025Oxford University PressThis is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License</p>
<p>. Problem Solving Protocol. </p>            </div>
        </div>

    </div>
</body>
</html>
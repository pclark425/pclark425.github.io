<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9583 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9583</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9583</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-166.html">extraction-schema-166</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-267199818</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.13086v1.pdf" target="_blank">Towards Trustable Language Models: Investigating Information Quality of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLM) are generating information at a rapid pace, requiring users to increasingly rely and trust the data. Despite remarkable advances of LLM, Information generated by LLM is not completely trustworthy, due to challenges in information quality. Specifically, integrity of Information quality decreases due to unreliable, biased, tokenization during pre-training of LLM. Moreover, due to decreased information quality issues, has led towards hallucination, fabricated information. Unreliable information can lead towards flawed decisions in businesses, which impacts economic activity. In this work, we introduce novel mathematical information quality evaluation of LLM, we furthermore analyze and highlight information quality challenges, scaling laws to systematically scale language models.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9583.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9583.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IQ-Formulation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mathematical Information Quality Evaluation (accuracy-consistency-relevance)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-agnostic mathematical formulation introduced in this paper that scores LLM-generated information as a weighted combination of three dimensions: accuracy, consistency, and relevance to evaluate information quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing / Information Quality</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>evaluation metric / mathematical formulation</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Proposes a parametric scoring function combining three normalized components (accuracy, consistency, relevance) with configurable weights to produce a single information-quality score for LLM outputs; described conceptually rather than as a fully specified algorithm in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified ‚Äî formulation is presented as domain-agnostic and intended to evaluate outputs from LLMs across tasks; no concrete corpus, dataset sizes, or preprocessing pipeline are reported in the paper for this formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not empirically evaluated in the paper; presented as a proposed metric/pipeline. The paper motivates the choice qualitatively (alignment with human expectations, normalization), but provides no numeric validation or benchmark comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>No quantitative results provided ‚Äî the paper introduces the formulation and argues for its utility but does not report experiments, discovered equations, or numeric validation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>No instantiated example laws or numerical validations are supplied; the contribution is a proposed composite metric rather than distilled physical or empirical laws.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>No empirical validation provided; potential dilution if more than three dimensions are used (authors' rationale), applicability and calibration depend on task-specific weighting and availability of ground-truth labels for accuracy/consistency/relevance; paper does not present procedures to compute each component in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>None provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Trustable Language Models: Investigating Information Quality of Large Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9583.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9583.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chinchilla</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chinchilla (compute-optimal scaling law / Kaplan-Hoffmann scaling results)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scaling law for autoregressive LLM training that prescribes the compute-optimal relationship between model parameters (N) and training tokens (D), summarized in the paper as recommending roughly equal proportional scaling of N and D under a compute budget.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine Learning / Large Language Model scaling</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>scaling law (empirical power-law relationship)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Empirical analysis of model loss vs combinations of parameters N and dataset size D under fixed compute budgets; indicates optimal allocation of compute between model size and dataset size to minimize loss.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not detailed in this paper (Chinchilla originates from Kaplan et al./Hoffmann et al. references); the current paper references the law conceptually without reporting the datasets used to fit it.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Empirical fit of loss as function of N, D (referenced works use held-out loss and compute-optimal experiments); in this paper it is discussed conceptually.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>No new experimental results in this paper ‚Äî the Chinchilla scaling principle is cited to argue how practitioners should scale N and D; specific numeric relations are referenced (roughly equal proportion of N and D under compute-optimality) but no fresh metrics are produced.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Paper restates the Chinchilla guideline that given fixed FLOPs budget, the number of model parameters and number of training tokens should be balanced (approximately equal scaling) to be compute-optimal.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Discussed generally: scaling laws require large experimental data to fit; practical barriers include massive compute and data requirements, dataset scarcity relative to industrial private data, and sensitivity to training details and dataset composition.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared in literature to older single-power-law scaling (Kaplan et al.) and to Broken Neural Scaling Laws; this paper references those frameworks but does not present direct experimental baselines itself.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Trustable Language Models: Investigating Information Quality of Large Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9583.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9583.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BSNL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Broken Neural Scaling Laws (BSNL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An empirical functional form described in the literature and cited in this paper that models non-monotonic or piecewise power-law scaling behaviors in neural networks, allowing for sharp or smooth breaks and complex scaling patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine Learning / Neural scaling analysis</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>empirical scaling model / broken power-law relationship</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Fit a flexible functional form (broken/smoothly-broken power laws) to large-scale experimental results (loss vs compute/size/data) to capture multiple scaling regimes and phenomena such as double descent; discussed conceptually in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not applicable in this paper ‚Äî BSNL is referenced as an analytical tool from Caballero et al. (2022); the current paper does not supply the underlying experimental datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>In cited works: goodness-of-fit to empirical scaling curves derived from many training runs; in this paper, BSNL is discussed qualitatively with mention of its ability to fit complex scaling behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>No new numeric results provided here; the paper notes BSNL 'offers many advantages' for modeling expressivity and capturing breaks but does not present fitted parameters or discovered equations.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Paper highlights that BSNL can model sharp/smooth breaks and non-monotonic relationships like double descent, but no concrete fitted scaling equations or coefficients are reproduced.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Explicitly noted: BSNL requires huge numbers of data points (many experiments) to fit reliably, is harder to fit, and yields results that can be difficult to interpret.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared conceptually to simpler single power-law scaling laws; BSNL is presented as more expressive but more data-hungry and harder to interpret.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Trustable Language Models: Investigating Information Quality of Large Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9583.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9583.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-knowledge-distillation-claim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claim: LLMs distill and accumulate knowledge from many sources</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conceptual statement in the paper that large language models, by virtue of scale and training on multi-source corpora, are able to distill and accumulate knowledge across domains and sources to form scalable representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General / Cross-domain knowledge representation</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>conceptual claim (not a quantitative law)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Descriptive ‚Äî the paper asserts LLMs distill knowledge from large mixed corpora; no specific method (e.g., prompting, fine-tuning) is provided for using LLMs to extract formal quantitative laws from scholarly papers.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>General multi-source corpora (web text, books, scientific corpora like arXiv) are cited as training material, but no curated pipeline for extracting formal equations from scholarly papers is described in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>No evaluation of law-distillation capability is provided; the paper cites general demonstrated downstream improvements (e.g., reasoning) but does not empirically evaluate extraction of mathematical relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>No empirical examples of LLMs discovering explicit quantitative scientific laws from collections of scholarly papers are reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>The paper contains no concrete instances where an LLM extracted or discovered a new or known quantitative scientific law from a corpus of scholarly articles.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Authors highlight general LLM limitations relevant to law extraction: hallucination, inconsistency, data quality issues (duplication, bias, tokenization), scarcity of high-quality domain-specific datasets, catastrophic forgetting when fine-tuning to new data ‚Äî all of which would impede reliably distilling quantitative laws.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>No comparisons to symbolic discovery methods or human experts for scientific law discovery are provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Trustable Language Models: Investigating Information Quality of Large Language Models', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Broken Neural Scaling Laws <em>(Rating: 2)</em></li>
                <li>Training Compute-Optimal Large Language Models <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
                <li>Galactica: A Large Language Model for Science <em>(Rating: 2)</em></li>
                <li>Scaling Language Models: Methods, Analysis & Insights from Training Gopher <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9583",
    "paper_id": "paper-267199818",
    "extraction_schema_id": "extraction-schema-166",
    "extracted_data": [
        {
            "name_short": "IQ-Formulation",
            "name_full": "Mathematical Information Quality Evaluation (accuracy-consistency-relevance)",
            "brief_description": "A domain-agnostic mathematical formulation introduced in this paper that scores LLM-generated information as a weighted combination of three dimensions: accuracy, consistency, and relevance to evaluate information quality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "scientific_domain": "Natural Language Processing / Information Quality",
            "law_type": "evaluation metric / mathematical formulation",
            "method_description": "Proposes a parametric scoring function combining three normalized components (accuracy, consistency, relevance) with configurable weights to produce a single information-quality score for LLM outputs; described conceptually rather than as a fully specified algorithm in experiments.",
            "input_corpus_description": "Not specified ‚Äî formulation is presented as domain-agnostic and intended to evaluate outputs from LLMs across tasks; no concrete corpus, dataset sizes, or preprocessing pipeline are reported in the paper for this formulation.",
            "evaluation_method": "Not empirically evaluated in the paper; presented as a proposed metric/pipeline. The paper motivates the choice qualitatively (alignment with human expectations, normalization), but provides no numeric validation or benchmark comparisons.",
            "results_summary": "No quantitative results provided ‚Äî the paper introduces the formulation and argues for its utility but does not report experiments, discovered equations, or numeric validation metrics.",
            "notable_examples": "No instantiated example laws or numerical validations are supplied; the contribution is a proposed composite metric rather than distilled physical or empirical laws.",
            "limitations_challenges": "No empirical validation provided; potential dilution if more than three dimensions are used (authors' rationale), applicability and calibration depend on task-specific weighting and availability of ground-truth labels for accuracy/consistency/relevance; paper does not present procedures to compute each component in practice.",
            "baseline_comparison": "None provided.",
            "uuid": "e9583.0",
            "source_info": {
                "paper_title": "Towards Trustable Language Models: Investigating Information Quality of Large Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Chinchilla",
            "name_full": "Chinchilla (compute-optimal scaling law / Kaplan-Hoffmann scaling results)",
            "brief_description": "A scaling law for autoregressive LLM training that prescribes the compute-optimal relationship between model parameters (N) and training tokens (D), summarized in the paper as recommending roughly equal proportional scaling of N and D under a compute budget.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "scientific_domain": "Machine Learning / Large Language Model scaling",
            "law_type": "scaling law (empirical power-law relationship)",
            "method_description": "Empirical analysis of model loss vs combinations of parameters N and dataset size D under fixed compute budgets; indicates optimal allocation of compute between model size and dataset size to minimize loss.",
            "input_corpus_description": "Not detailed in this paper (Chinchilla originates from Kaplan et al./Hoffmann et al. references); the current paper references the law conceptually without reporting the datasets used to fit it.",
            "evaluation_method": "Empirical fit of loss as function of N, D (referenced works use held-out loss and compute-optimal experiments); in this paper it is discussed conceptually.",
            "results_summary": "No new experimental results in this paper ‚Äî the Chinchilla scaling principle is cited to argue how practitioners should scale N and D; specific numeric relations are referenced (roughly equal proportion of N and D under compute-optimality) but no fresh metrics are produced.",
            "notable_examples": "Paper restates the Chinchilla guideline that given fixed FLOPs budget, the number of model parameters and number of training tokens should be balanced (approximately equal scaling) to be compute-optimal.",
            "limitations_challenges": "Discussed generally: scaling laws require large experimental data to fit; practical barriers include massive compute and data requirements, dataset scarcity relative to industrial private data, and sensitivity to training details and dataset composition.",
            "baseline_comparison": "Compared in literature to older single-power-law scaling (Kaplan et al.) and to Broken Neural Scaling Laws; this paper references those frameworks but does not present direct experimental baselines itself.",
            "uuid": "e9583.1",
            "source_info": {
                "paper_title": "Towards Trustable Language Models: Investigating Information Quality of Large Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "BSNL",
            "name_full": "Broken Neural Scaling Laws (BSNL)",
            "brief_description": "An empirical functional form described in the literature and cited in this paper that models non-monotonic or piecewise power-law scaling behaviors in neural networks, allowing for sharp or smooth breaks and complex scaling patterns.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "scientific_domain": "Machine Learning / Neural scaling analysis",
            "law_type": "empirical scaling model / broken power-law relationship",
            "method_description": "Fit a flexible functional form (broken/smoothly-broken power laws) to large-scale experimental results (loss vs compute/size/data) to capture multiple scaling regimes and phenomena such as double descent; discussed conceptually in the paper.",
            "input_corpus_description": "Not applicable in this paper ‚Äî BSNL is referenced as an analytical tool from Caballero et al. (2022); the current paper does not supply the underlying experimental datasets.",
            "evaluation_method": "In cited works: goodness-of-fit to empirical scaling curves derived from many training runs; in this paper, BSNL is discussed qualitatively with mention of its ability to fit complex scaling behavior.",
            "results_summary": "No new numeric results provided here; the paper notes BSNL 'offers many advantages' for modeling expressivity and capturing breaks but does not present fitted parameters or discovered equations.",
            "notable_examples": "Paper highlights that BSNL can model sharp/smooth breaks and non-monotonic relationships like double descent, but no concrete fitted scaling equations or coefficients are reproduced.",
            "limitations_challenges": "Explicitly noted: BSNL requires huge numbers of data points (many experiments) to fit reliably, is harder to fit, and yields results that can be difficult to interpret.",
            "baseline_comparison": "Compared conceptually to simpler single power-law scaling laws; BSNL is presented as more expressive but more data-hungry and harder to interpret.",
            "uuid": "e9583.2",
            "source_info": {
                "paper_title": "Towards Trustable Language Models: Investigating Information Quality of Large Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "LLM-knowledge-distillation-claim",
            "name_full": "Claim: LLMs distill and accumulate knowledge from many sources",
            "brief_description": "A conceptual statement in the paper that large language models, by virtue of scale and training on multi-source corpora, are able to distill and accumulate knowledge across domains and sources to form scalable representations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "scientific_domain": "General / Cross-domain knowledge representation",
            "law_type": "conceptual claim (not a quantitative law)",
            "method_description": "Descriptive ‚Äî the paper asserts LLMs distill knowledge from large mixed corpora; no specific method (e.g., prompting, fine-tuning) is provided for using LLMs to extract formal quantitative laws from scholarly papers.",
            "input_corpus_description": "General multi-source corpora (web text, books, scientific corpora like arXiv) are cited as training material, but no curated pipeline for extracting formal equations from scholarly papers is described in this work.",
            "evaluation_method": "No evaluation of law-distillation capability is provided; the paper cites general demonstrated downstream improvements (e.g., reasoning) but does not empirically evaluate extraction of mathematical relationships.",
            "results_summary": "No empirical examples of LLMs discovering explicit quantitative scientific laws from collections of scholarly papers are reported in this paper.",
            "notable_examples": "The paper contains no concrete instances where an LLM extracted or discovered a new or known quantitative scientific law from a corpus of scholarly articles.",
            "limitations_challenges": "Authors highlight general LLM limitations relevant to law extraction: hallucination, inconsistency, data quality issues (duplication, bias, tokenization), scarcity of high-quality domain-specific datasets, catastrophic forgetting when fine-tuning to new data ‚Äî all of which would impede reliably distilling quantitative laws.",
            "baseline_comparison": "No comparisons to symbolic discovery methods or human experts for scientific law discovery are provided in the paper.",
            "uuid": "e9583.3",
            "source_info": {
                "paper_title": "Towards Trustable Language Models: Investigating Information Quality of Large Language Models",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Broken Neural Scaling Laws",
            "rating": 2,
            "sanitized_title": "broken_neural_scaling_laws"
        },
        {
            "paper_title": "Training Compute-Optimal Large Language Models",
            "rating": 2,
            "sanitized_title": "training_computeoptimal_large_language_models"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Galactica: A Large Language Model for Science",
            "rating": 2,
            "sanitized_title": "galactica_a_large_language_model_for_science"
        },
        {
            "paper_title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher",
            "rating": 1,
            "sanitized_title": "scaling_language_models_methods_analysis_insights_from_training_gopher"
        }
    ],
    "cost": 0.0110485,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards Trustable Language Models: Investigating Information Quality of Large Language Models</p>
<p>Rick Rejeleene rrejeleene@ualr.edu 
Department of Information Science Department of Information Science
University of Arkansas University of Arkansas Little Rock
72204, 72204Little Rock</p>
<p>Xiaowei Xu xuxu@ualr.edu 
Department of Information Science Department of Information Science
University of Arkansas University of Arkansas Little Rock
72204, 72204Little Rock</p>
<p>John Talburt jrtalburt@ualr.edu 
Department of Information Science
University of Arkansas Little Rock
72204</p>
<p>Towards Trustable Language Models: Investigating Information Quality of Large Language Models
220E02AC18603D3F921ED83F28688AD8Language ModelsMachine LearningAttention-modulesInformation-Quality
Large language models (LLM) are generating information at a rapid pace, requiring users to increasingly rely and trust the data.Despite remarkable advances of LLM, Information generated by LLM is not completely trustworthy, due to challenges in information quality.Specifically, integrity of Information quality decreases due to unreliable, biased, tokenization during pre-training of LLM.Moreover, due to decreased information quality issues, has led towards hallucination, fabricated information.Unreliable information can lead towards flawed decisions in businesses, which impacts economic activity.In this work, we introduce novel mathematical information quality evaluation of LLM, we furthermore analyze and highlight information quality challenges, scaling laws to systematically scale language models.</p>
<p>Introduction</p>
<p>Recently, there's been wide deployment of large language models in various fields such as medicine, law, code generator, search, with chatbots being most prominent.In domains such as software engineering, productivity is improved due to code-completion (Cruz-Benito et al. 2021) (Solaiman et al. 2019) (Moradi Dakhel et al. 2023), In writing such as grammar assistance, autocomplete, creative writing, poetry generation, question-answering systems.</p>
<p>Large language models use conditional probability to generate sequences of tokens for generating sentences.ChatGPT, a transformer based LLM, is creating excitement beyond measure among researchers, engineers raising questions about trust and data quality (Brants et al. n.d.).Transformers (Vaswani et al. 2017a) a language model introduced has led towards state of the art natural language processing tasks.Attention mechanisms, which make the core of language models are playing an essential role as part of the architecture in language models (Bahdanau, Cho, and Bengio 2014) (Hu 2020;Brauwers and Frasincar 2023).Attention Mechanisms enable LLM to recall, generate an accurate sequence of tokens due to their ability to focus on relevant parts of input sequence during generation of tokens.In this work, we describe trust, data quality of large language models, we describe issues around trust, data-quality, and research directions for increasing trust of language models.In Summary, our contributions are as follows:</p>
<p>‚Ä¢ We propose a novel mathematical formulation of information quality using consistency, relevance and accuracy metric, thus introducing a pipeline for LLM to evaluate information quality in natural language processing</p>
<p>‚Ä¢ We explore importance of trust, data-quality for large language models, which affects economy, we highlight how unreliable data leads towards poor performance of LLM, data quality issues such as diversity of training, bias, tokenization are needed in larger datasets for LLM</p>
<p>‚Ä¢ We postulate why quality of LLM is facing limitations, we find due to data quality issues, and we find datasets such as general, specialized datasets are enabling LLM to improve performance in specific domains.We also explore scaling laws like Chinchilla and Broken Neural Scaling laws, which enable LLM to systematically scale language models</p>
<p>‚Ä¢ We find limitations of LLM such as hallucination, common sense reasoning, inconsistency, and we suggest research direction for improving LLM through investigating theory and principles of LLM, reducing dependence on human labellers and improving data quality</p>
<p>Why Information quality generated by LLM play a role in Economy?</p>
<p>Trust plays a central role in economic transactions, for the majority of professions in businesses.Key decisions are taken, based on the information quality available.Firstly from consumers, when they make a decision about a product, they desire reliable information to purchase, secondly, product reviews, data informs their choices.</p>
<p>Large language models are producing information based on training data, models such as GPT series are trained on trillions of tokens, amassing the entire internet, when LLM are used to accomplish tasks such as code-generation, tutoring students as education bots, users require them to rely and trust them.Unreliable information leads towards poor data quality, thus hampering both customers and professions to lose trust.Uncertainty hinders towards lack of growth, development.Therefore it is of urgent requirement to explore information quality, trust of large language models.</p>
<p>Increasing information quality might lead towards increased reliability of large language models, this might have indirect effects in the global economy such as stronger consumer confidence, effective partnership, adoption of language model technology all over the globe, increased transparency, thereby boosting economic activity of all businesses.</p>
<p>Why are Language Models facing Information Quality issues?</p>
<p>Reasons why language models are facing information quality issues is due to process of training, involving tokenization, quality of data which involves lack of diversity, bias, requiring larger dataset as LLM is being scaled, increasing in size.</p>
<p>Large language models since introduction of transformers (Vaswani et al. 2017a) have been increasing in size, data, performance of tasks in majority of natural language processing tasks.Generative Pre-trained Transformers (GPT) consists of decoder-only architecture, with 12 layers, with 110 million parameters, trained unidirectionally, with further improvements leading towards GPT-2, containing 48 layers, 1.5 billion parameters, trained on 40GB of WebText, with GPT-3 increasing size, with 96 layers, with a total of 175 billion parameters, containing 12 attention heads, 768 dimensional states, with increasing more size, GPT-4 (OpenAI 2023) consisting of 1.8 trillion parameters with 120 layers, trained on 13 trillion tokens.</p>
<p>Tokenization (Toraman et al. 2023) is an important pre-processing technique for large language models, where inputs are broken down into subunits before sending to encoder-decoder layers of language models.Tokenization methods applied in language models such as character-level, byte-pair encoding (BPE), Wordpiece, morphological-level, word-level are applied in language models.Variability (Kaddour et al. 2023), token's glitching leading towards unexpected behavior, higher computational costs as it adds additional layer of training, information loss and quality.</p>
<p>Method Tokenized Text</p>
<p>Character-level "L", "a", "n", "g", "u", "a", "g", "e", " ", "M", "o", "d", "e", "l", "s", " ", "a", "r", "e", " ", "i", "m", "p", "r", "e", "s", "s", "i", "v", "e" BPE "[CLS]", "Language", "Models", "are", "impress", "##ive", "[SEP]"</p>
<p>WordPiece "[CLS]", "Language", "Models", "are", "impress", "##ive", "[SEP]"</p>
<p>Morphological "[CLS]", "Language", "Model", "s", "are", "impress", "##ive", "[SEP]"</p>
<p>Word-level "[CLS]", "Language", "[UNK]", "are", "[UNK]", "[SEP]"  Contextual data quality refers to dimensions in data such as relevance, timelines, completeness, appropriateness, representational data quality refers to data which is presented in intelligible, clear to understand.Accessibility category refers to data which is obtainable, secure.For Language models, these dimensions affect the performance of large language models.</p>
<p>As Language models are becoming more pervasive (Batarseh and Freeman 2022;Felderer and Ramler 2021) and dependent upon AI, however quality assurance of the data and systems is necessary by data quality, which can be measured by correctness, model relevance, robustness, security, data privacy, efficiency, fairness, interpretability.These issues can be characterized by quality characters, artefact type, processes.Information Quality of language models can be measured also by consistency, relevance, accuracy metrics.We define Information quality as a function with a combination of parameters describing consistency, relevance and accuracy.</p>
<p>Mathematical formulation of Information Quality Evaluation LLM:</p>
<p>Motivated by issues in evaluating information quality produced from large language models, We introduce dimensions to measure information quality dimensions such as consistency, relevance and accuracy.be information generated
ùêøùëíùë° { ùë• 1 , ùë• 2 ..., ùë• ùëõ }
by Large Language models.To evaluate the quality of Large Language models, we propose three dimensions: accuracy, consistency and relevance.We chose three dimensions due to the following reasons.Firstly, choosing more than three might dilute the information quality dimension, secondly our formulation is domain-agnostic -together accuracy, consistency, relevance.Lastly large language models are intended to be used for specific, human expectations also known as alignment.Our formulation for evaluating Information quality aligns with the alignment of LLM.Accuracy, consistency, relevance are weighted combinations, which allows context-specific evaluation, which normalizes the score.</p>
<p>State of the Art Large Language Models:</p>
<p>Language has been studied by linguists, computer scientists, and statisticians.</p>
<p>Language is human expression communicated as symbols governed by a set of grammatical rules.Researchers are investigating capable Artificial Intelligence (AI) for comprehending, grasping language.We explore state of the art language models introduced.</p>
<p>One significant major approach for language understanding and generation has been, Language modeling (Jurafsky and Manning 2012) (Shannon 1948), where next word probability is based on analyzing text data, various approaches are formulated in Language modeling such as n-gram, unigram, bidirectional, exponential, neural language models, continuous space.Statistical language models (SLM) were introduced in 1990s, based on markov assumption, however due to size, they suffered from curse of dimensionality, Neural (Bengio, Ducharme, and Vincent 2000) Language model introduced concept of distribution representation of words using neural networks, unified solution for NLP tasks was proposed, leading towards state of the art performance.</p>
<p>Recently, (Zhao et al. 2023) Transformer based language models formulated as pre-trained language models are emerging with impressive capabilities, demonstrating strong capabilities in natural language processing tasks.Abilities such as in-context learning, which have been observed, resulting from scaling these language models, the abilities are not present in smaller language models.</p>
<p>Transformer language models are the underlying architecture for Large Language Models (LLM) such as (Devlin et al. 2018), Bidirectional Encoder Representations from Transformers (BERT), (Radford et al. 2018) Generative Pre-trained Transformers (GPT) series (Min et al. 2021) In recent years, BERT has emerged as a powerful language model, surpassing the previous state-of-the-art models in a multitude of natural language processing tasks.Pre-trained models became prominent due to the introduction of ELMo (Embeddings from Language Models) (Iyyer et al., n.d.) due to the leverage of unlabelled data to create word embeddings.ELMo, word embedding method for representing sequence of words as corresponding sequence of vectors (Iyyer et al., n.d.) is based on (Hochreiter and Schmidhuber 1997) bidirectional LSTM.Pre-trained models come under three categories, Autoregressive, Masked Language, Encoder-Decoder.Popular examples of Autoregressive models are GPT, GPT-2, GPT-3, GPT-3.5 where the objective is to predict which word comes next, given previous words.A few examples of masked language models are BERT, RoBERTa, XLM-R, where the objective is to predict masked words given other words in a sequence.Recently introduced state of the art language models are masked language models -Llama, Llama 2 (Touvron, Martin, et al. 2023) (Touvron, Lavril, et al. 2023) are large language models with scale of 7 billion to 70 billion parameters.Trained on trillions of tokens.LLama is trained on publicly available dataset.BART and T5 are Encoder-Decoder models, where the objective includes corrupting a sequence and then predicting the original sequence.While researchers and engineers are interested in finding the best performing model, It is difficult to conclude which type of pre-trained model is best performing.Pre-trained models perform efficiently depending on a task.(Li et al. 2021) Autoregressive models such as GPT are highly effective for generating text, used widely for text summarization and language generation.Masked language models such as BERT excel for understanding relationships between words in a sentence and are widely used for name entity recognition and sentiment analysis.Encoder-Decoder models such as BART demonstrate efficient understanding of the structures of sentences and are widely used for tasks such as text summarization and machine translation.A transformer language model's key component is (Vaswani et al. 2017b)</p>
<p>Broken Scaling Laws (BSNL)</p>
<p>Power laws frequently occur as distributions in many phenomena, such as frequency of words in most languages, frequency of family names, size of power outages.Power law occurs when there is a functional relationship between two quantities, where relative one change occurs in another quantity to a power of change.BSNL offers many advantages in neural networks.One major advantage is the ability to model expressiveness in large language models, modeling sharp, smooth breaks while allowing scale in large language models.It also allows non-monotonic relationships like double descent, accurate fit neural network scaling curves.While BSNL offers advantages, it suffers from limitations such as requiring huge data points, harder to fit, and difficult to interpret results.</p>
<p>Large Language Models and Data Quality for Performance:</p>
<p>Large language models are giving state of art performance in downstream tasks such as named-entity recognition, text generation, question-answering, translation, coreference resolution, involved in the majority of natural language processes (Brown et al. 2020;Bubeck et al. 2023).</p>
<p>Outstanding capabilities of LLM are due to properties such as expressivity, scalability, multimodality, memory capacity, compositionality (Bommasani et al. 2021).Due to the five properties, LLM is able to distill, accumulate knowledge from many sources, domains, organize, effective, scalable representation, flexible generalization towards novel context.GPT-3 (Brown et al. 2020), a Large language model has 175 billion parameters, which was trained on 570 gb of text data, GPT-3 demonstrated zero-shot generalization to downstream tasks, when increased in size of data and parameters from GPT-2 containing 1.5 billion parameters.Large Language Model Meta AI (Llama) (Touvron, Martin, et al. 2023) is from 7 billion to 70 billion parameters, trained on 2 trillion tokens of data on publicly available data focused on factual data.</p>
<p>As Large Language models are trained on large quantities of data, (Kaplan et al. 2020) where GPT based models such as WuDao (Yuan et al. 2021) is trained on 4.9 TB of multi-modal data, and scaling of dataset is only increasing due to emergent abilities of language models (Wei, Tay, et al. 2022).For training and increasing performance of LLM data quality and size is necessary.However, the available, public dataset is small compared to petabytes of consumer, business, personal data collected in industrial information, therefore there is a scalability problem of handling large datasets.Language models require both structured and unstructured data during training, (Orr et al. 2020) (Poerner, Waltinger, and Sch√ºtze 2020) demonstrated integrating structured and unstructured data can help to generalize rare concepts, improve recall and accuracy.</p>
<p>Moreover, effectively integrating both structured and unstructured datasets for increasing performance of large language models is a challenge.While LLM is able to be trained on unstructured data, additional steps are required for data with source containing entity or image data.In industrial scale, to integrate multi-modal dataset for text, image, more steps are required.Data privacy is another issue in integrating the data, where web crawlers are scraped for datasets without consent of users, how to preserve privacy, yet maintain high performance of large language models remain a persistent issue, where emerging regulation, guidelines ensure safe, responsible management of data.</p>
<p>Data Quality of Language Models: Data quality which is used to train large language models greatly affects performance (Lee et al. 2021) (Taylor et al. 2022), which can be incorporated into LLM, that has led towards increased performance in scientific reasoning tasks (Saier, Krause, and F√§rber 2023).Scientific corpus is collected from arXiv papers, scientific textbooks, physics webpages, tutorials, scholarly articles and other resources.These datasets contain mathematical formalism, symbols, and different formats, which require preprocessing so that they can be processed by language models.Code datasets have been collected, researched for program synthesis, which can be used for Pre-trained language models on Code (Piccolo et al. 2023</p>
<p>Data-Preprocessing:</p>
<p>Data preprocessing for LLM involves removing noisy, redundant, irrelevant, toxic data, which affects performance.Data-juicer for LLM (D. Chen et al. 2023) involves features which allows 50 processing operations and tools, for quality filtering in low-quality data from corpus, two strategies can be applied, classifier, heuristics.</p>
<p>Classifier identifies lower quality data and filters from the corpus, (Du et al. 17--23 Jul 2022), binary classifiers can be used with curated data such as wikipedia with positive classification.Language models such as Gopher (J.Rae, Irving, and Weidinger, n.d.) applied a heuristic based approach to eliminate lower quality text, using well-designed rules, which filters applied on language based filtering, metric based filtering, statistic, keyword based.Language models, when applied to Tamil language tasks, can use language filters to remove non-Tamil tokens in the dataset, perplexity frequently used as evaluation for quality of language models, could be used as a metric for filtering unnatural sentences.Low quality data can be removed also using symbol to word ratio, punctuation distribution, statistical filtering, keywords such as offensive words, toxicity in sentences can be removed using keyword filtering.</p>
<p>De-duplication occurs at document-level, sentence level, dataset level resulting in lower quality data, sentences which contain repeated words, phrases must be removed as these may result in modeling repetitive patterns in language modeling.Document level duplication can be detected by using n-gram, finding similar content, dataset level duplication can be removed by finding duplicate texts from the training set.Performance of LLM greatly increases due to removed of duplicates at three levels.Protecting privacy of users is important (Carlini et al. 2022) as LLM are trained on datasets which might contain personal information.Methods such as keyword detection, such to remove personal information name, address, phone numbers can be applied to remove sensitive data contained in user-generated content from web forums.</p>
<p>Tokenization as described in Figure 4 is important steps, where data is preprocessed.During step of tokenization, individual tokens are segmented from raw dataset, which are used as inputs for LLM, methods such as sequence labeling with conditional random fields, word based tokenization is used, other method such as subword tokenization have been used in Transformer based models, which applies BytePair Encoding tokenization, WordPiece tokenization, Unigram tokenization.</p>
<p>BytePair tokenization introduced as a data compression algorithm (Gage, n.d.;Sennrich, Haddow, and Birch 2015), basic symbols are iteratively combined with frequent pairs of two consecutive tokens in corpus as new tokens, as merging.</p>
<p>Merging is based on co-occurrence frequency of two contiguous tokens.Byte-level BPE is used to improve tokenization quality for multilingual corpus.Wordpiece tokenization is similar to Byte level BPE, instead the merge step involves selecting different criteria for merge, where in each merge, it selects the pair that leads to the most increase in likelihood of training data.Moreover other methods such as unigram tokenization are used in language models such as T5 and mBART.Pre-training data from a large mixture of data can enable LLM to acquire higher scope of knowledge, Gopher (J.W. Rae et al. 2021) uses ablation experiment on distribution of dataset to examine impact of mixed source on different tasks.High-quality data for adequately training the model results is good performance (Chung et al. 2022)</p>
<p>Issues and Limitations of Language Models:</p>
<p>Although Large language models have demonstrated state of the art performance, they face major challenges and limitations.The major issue has been the problem of hallucination (McKenna et al. 2023;Lee et al. 2018;del Campo and Leach 2022) in large language models, where given a text-corpus when LLM is prompted to produce factual text, LLM produces hallucinationed text.Hallucinations text is when generated text is in conflict with source (intrinsic) or cannot be verified by available source, (extrinsic).Due to this, LLM generates untruthful information Hallucination has been found in all major language models, even prone in GPT-4, even occurs in multi-modal vision-based language models (Bang et al. 2023).LLM utilize knowledge in solving tasks, which lacks ability to accurately control use of internal or external knowledge.Due to hallucination, it is not recommended to deploy in real-world applications involving healthcare or other areas, which requires a high level of reliability.To mitigate this, strategies such as use of high-quality data, use of human-feedback through reinforcement learning has been applied.Applying external knowledge for checking credibility of information has reduced hallucination issue SelfCheck-GPT (Manakul, Liusie, and Gales 2023) detects hallucinations by measuring information inconsistency with sampled output, TruthfulQA, HaluEval uses LLM generated and human annotation to evaluate ability of LLM to recognize hallucination in task-specific (S.Lin, Hilton, and Evans, n.d.;Li et al. 2023) scenarios.</p>
<p>While LLM are trained on large quantities of dataset, another major limitation has been, when tasked with challenging recent events or knowledge, which goes beyond training dataset, LLM is limited to provide reasonable answers.Knowledge recency is an issue as cost of training is high for fine-tuning with newest information, even when fine-tuned on recent knowledge, it is likely to cause catastrophic forgetting (Kaushik et al. 2021) LLMs are desired to produce output, which conform to human needs and values i.e human alignment (Zhao et al. 2023;Liu et al. 2023;OpenAI 2023), where LLM can be applied and used in real-world applications, due to this, TruthfulQA (S.Lin, Hilton, and Evans, n.d.), harmlessness CrowS-Pairs (Nangia et al. 2020) datasets are used to evaluate ability of LLMs towards human-alignment, LLM consists of ability to receive feedback from external environment, perform action according to behaviour instruction, generation action plans, manipulate agents (W.Huang et al. 17--23 Jul 2022), LLM also consists of ability to manipulate tools received by API calls, such as being applied to search engine, calculator, compiler, to enhance performance (Parisi, Zhao, and Fiedel 2022).In addition to manipulating existing tools, LLM possess capability to make their own tools for specific tasks autonomously, where models independently explore, manipulate, and self-create tools, which is giving them the ability to solve real-world tasks.</p>
<p>LLM in addition to the above three abilities such as human alignment, tool manipulation, interaction with external environment (Gilardi, Alizadeh, and Kubli 2023), LLM is able to do data annotation, self-improvement (Gamil 2023</p>
<p>Transformers: Architectural Foundation for Large Language Models</p>
<p>Transformers are the foundation for large language models (Vaswani et al. 2017a;Zhao et al. 2023).Transformers introduced as a novel architecture for neural machine translation established as state of art performance on English-German translation task.Following initial success in natural processing tasks -transformers architectures were rapidly adopted across diverse range of domains beyond natural language processing, even establishing as state of art performance in field of computer vision, in tasks such as image segmentation, multi-modal text and image generation and image recognition, using Vision-Transformers (Dosovitskiy et al. 2020).Transformers replaced convolutional, recurrent layer with attention mechanisms.Attention mechanisms were introduced to solve (Bahdanau, Cho, and Bengio 2014) fixed-length vector problem in neural machine translation (NMT).Fixed-length vector which decoder generates translation was a bottleneck.By allowing a encoder-decoder model to automatically soft-search for source sentences, which are relevant to target word, state of the art performance on NMT was achieved.Attention mechanisms allows Transformers to focus on relevant parts of the input, when generating outputs, this enables them to capture dependencies, and allows for parallelization.However, attention mechanisms face limitations such as requiring higher computation performance, lacking deeper reasoning, semantic understanding, and requiring a large dataset.Transformers consist of encoder-decoder architecture (Vaswani et al. 2017b), where encoder maps input sequence of symbol representations x to sequence of continuous representation z.Given a step z, decoder generates output sequence y of symbols one element at a time.For each step, the model is auto-regressive, depending on the previous element at a time, for generating next.Architecturally transformers contain self-attention, point-wise, fully connected layers for encoder and decoder.</p>
<p>Each layer consists of two sub-layers, a multi-head self-attention mechanism, second is position wise fully connected, feed-forward network, with a residual connection around each two sub-layers, layer normalisation, the output of the sub-layer is is a function of the ( + ()), () sublayer itself.Sublayers give product outputs of dimensions, .</p>
<p>ùëë ùëöùëúùëëùëíùëô = 512</p>
<p>Decoder consists of 7 identical stack layers, in addition to two sub-layers in the encoder, decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.The output of the Transformer model is a sequence of vectors with index to input tokens.What is required to make Large Language Models better?</p>
<p>We witnessed progress of large language models achieving state of the art performance in natural language processing, however the challenges remain, which could further performance, understanding of large language models.Performance of language models significantly is affected by dataset quality from pre-training, adaptation tuning where data collected is fine-tuned for specific domain, utilization is where LLM is applied to various tasks such as question-answering, summarization, translation, and evaluating performance of LLM.</p>
<p>Theory and Principles of underlying mechanisms of LLM is largely mysterious to researchers (Zhao et al. 2023), where how information is distributed, organized, utilized in language models is not understood properly.Scaling has allowed LLM to endow with emergent abilities in unexpected ways, however emergent abilities are mysterious as well, as when and how emergent abilities appear is not understood.Architecturally, stacked multi-head self attention layers has been how LLM is deployed, sparse attention has been used in (Brown et al. 2020), which has demonstrated improving performance and efficiency, by exploiting intrinsic redundancy, capturing dynamically changing attention weights.However, catastrophic forgetting (Korbak et al. 17--23 Jul 2022) (Ziegler et al. 2019;OpenAI 2023) has reduced hallucinations, toxicity generation from language models, However major limitation of RLHF is reliance on high-quality human feedback requiring professional labelers, which is difficult to implement in practice.Therefore, reducing human labelors with guaranteed data quality is required and needed.</p>
<p>Conclusion</p>
<p>With the rise of Large Language models deployed as ChatGPT, Claude, Bard, it is imperative to investigate processes towards building trustable language models.We have reviewed why information quality of data plays a key role in economy, highlighting information quality issues, investigating why language models performance is decreasing due to process of training, involving tokenization, quality of data which involves lack of diversity, bias, requiring larger dataset as LLM is being scaled, increasing in size.Moreover, we also explored state of the art language models with masked language models, autoregressive, and bidirectional models exploring performance.We also explored scaling laws of large language models with chinchilla and broken scaling laws, which helps researchers and engineers to scale systematically in a scientific way.</p>
<p>In addition to scaling laws, we explored information quality issues which LLM's performance is limiting, Information quality issues such as bias of many types such as social bias, false information, duplication, misinformation is a challenge in large language models, to improve performance of large language models, generalized dataset, and specialized dataset are being used, We found data pre-processing steps such as filter quality, duplication removal, reduce privacy, tokenization, greatly affect performance of large language models.We also found that to make LLM better in performance, theory and principles of LLM is to be investigated, reducing human labellers, and requiring high-quality data is recommended.A promising research direction is to increase high-data quality and investigate theory and principles of LLM.</p>
<p>Figure 1 :
1
Figure 1: Language Models Data Quality</p>
<p>Figure 2 :
2
Figure 2: Information Quality Dimensions</p>
<p>Figure 3 :
3
Figure 3: Types of Pre-trained Language Model(Bepler and Berger 2021)</p>
<p>of training the model N is number of parameters of the model D is number of tokens in training set L is average negative log-likelihood per token represents loss of ideal generative process on test data  0 captures Transformer language model with N parameters underperform ideal   Œ± generative process captures fact model trained on D tokens underperforms ideal generative process   Œ± Chinchilla scaling law(Jared, Sam, and Tom, n.d.)  given increased budget in floating point operations per second (FLOP), to achieve compute-optimal, number of model parameters (N) and number of tokens for training model (D) should be scaling in approximately equal proportion.</p>
<p>Figure 4 :
4
Figure 4: Functional form of Broken Neural Scaling Law(Caballero et al. 2022)</p>
<p>Figure 5 :
5
Figure 5: Pre-training steps in Language Models</p>
<p>Figure 6 :
6
Figure 6: Attention Mechanism(Bahdanau, Cho, and Bengio 2014)</p>
<p>Figure 7 :
7
Figure 7: Transformer model(Vaswani et al. 2017b)</p>
<p>Transformers, an encoder-decoder type of architecture, contained training data of 800 million words from Book-Corpus with 110 million parameters with 6 layers.BERT consists of encoder only architecture from Transformers, with 12 layers, 110 million parameters, trained bidirectionally.</p>
<p>Table 1 : Methods of Tokenization on text, "Language models are Impressive" Character
1law
(Kaplan et al. 2020)nd Rush 2017) character level, so there is no vocabulary for training, can represent characters, reducing model size, however it loses word level meaning and outputs long sequences, loses compositionality of words.BPE is able to handle vocabulary of subwords frequently used, and able to handle unseen words, a major disadvantage is that it is less interpretable towards morphological tokenization.Word-piece tokenizable is based on language modelling, also suffers from morphological tokenization and contains inconsistencies.Meaningful outputs are found from morphological tokenization, which captures semantics.World level tokenization loses subword information, while capturing simple whitespace tokenization.When large language models are trained on inconsistent tokenization, vocabulary size, it impacts quality of data and performance of language models.Poor quality data(Wiseman, Shieber, and Rush 2017)results in low performance in the model, SBNATION data, scored validation perplexity of 33.34 and BLEU score of 1.78, due to noisy quality of SBNATION data.Training data of language models are massive, however the training data is frequently lacking data-diversity(Bender et al. 2021)Language models(Kaplan et al. 2020) performance depends on scaling them, which is based on model parameters N, size of dataset D, amount of compute C for training.Based on three scale factors, N, D, C, performance has power-</p>
<p>Table 2 : Comparison of popular SOTA Large Language Models Scaling laws of Language Models In
2
(Kaplan et al. 2020;sm.Attention mechanisms are trained on large datasets, from numerous sources.Due to utilization of a multi-source datasets for natural language processing tasks, attention mechanisms provide a broader, more representative sample of linguistic phenomena present in the sequence of sentences.This enables language models to learn and generalize to new examples.The datasets collected for training large language models are sourced from the entire web.These datasets are processed using techniques such as tokenization, stemming, and lowercasing for filtering out irrelevant information as higher quality data is necessary for good results.parameters in autoregressive language models and performance.Scaling(Kaplan et al. 2020) laws for language models can be characterized by four parameters, size of model, size of training dataset, cost of training, performance after training, N, D, C, L (number of parameters, dataset size, computing cost, loss).Chinchilla scaling, a particular scaling law states(Kaplan et al. 2020; Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, de Las Casas,  Hendricks, Welbl, Clark, Hennigan, Noland, Millican, van den Driessche, Damoc,  Guy, Osindero, Simonyan, Elsen, Vinyals, et al. 2022)for training large language models auto-regressively, one epoch with cosine learning rate, The scaling law is as follows:
LanguageBERTGPT3BARTChatGPTLLama2ModelSizeBase: 110M175BBase: 110M175B7B to 70BLarge: 340MLarge: 340M
(Kaplan et al. 20202)6sets for running machine learning algorithms include (A.Wang et al. 2018), General Language Understanding Evaluation dataset (GLUE), which is a benchmark for performing evaluation for popular natural language processing tasks such as sentiment analysis.The Stanford Question Answering (SQuAD)(Rajpurkar et al. 2016) a commonly used question-answering dataset in natural language processing for performing evaluation in question answering, as well as Multi-Genre Natural Language Inference ((Williams, Nangia, and Bowman 2017), which is a popularly used for evaluating natural inference dataset.GPT-4 with Vision (OpenAI 2023) enables users to instruct, interact with image inputs, incorporating multimodal LLMs, which takes images, texts together as inputs(Mishkin et al. 2022), limitations remain in GPT-4V where answers might be factually incorrect, hallucinations occur in tested medical, scientific image data.theabovetable,Transformerbasedlanguagemodels displayed an increase in performance with an increase in size of parameters in language models.Buchatskaya, Cai, Rutherford, de Las Casas, Hendricks, Welbl, Clark, Hennigan, Noland, Millican, van den Driessche, Damoc, Guy, Osindero, Simonyan, Elsen,Rae,  et al. 2022), introduced Gopher, language modeling at scale (J.Rae, Irving, and  Weidinger, n.d.).(Kaplan et al. 2020) discovered there is power law relationship between number of</p>
<p>(Gao et al. 2020)20)2020)diverse capacities, however there are limitations as crawled data might contain high-quality text found in Wikipedia, and low-quality text like email spam mails.Conversational text data enables LLM to enhance the ability of LLM for conversational NLP tasks such as question-answering.Conversational datasets such as reddit corpus(Baumgartner et al. 2020)(Roller et al. 2020), social media corpus are used.Book datasets(Gao et al. 2020)provide an important source to learn longer texts in corpus, which allows LLM to model long-term dependency, generate narrative, coherent texts.While general datasets are used for General use LLM, specific corpus involving specialization is used to improve downstream tasks such as Multilingual text, Scientific text, Code text, which can be used for LLM based applications for specialized tasks.Specialized tasks such as code-generation, code-question-answer, scientific question-answering.PaLM, BLOOM(Chowdhery et  al. 2022; BigScience Workshop et al. 2022)involves multilingual datasets which includes 46 and 122 languages with their pre-training corpus, due to multilingual dataset, PaLM, BLOOM demonstrates state of the art performance in multilingual tasks such as translation, multilingual summarization.Scientific research involves publishing findings from research, however there's growing number of scientific publications
. Data used in LLM consists ofbias of many types such as social bias, false information, duplication (Blodgett andO'Connor 2017) (Fleisig et al. 2023) misinformation, which impacts training data. Specialized Data for LLM:Due to constant change in businesses, products, data is evolving, requiring to becontinuously updated and refined (Kiela et al. 2021), which has emergent abilities(Fetahu, Anand, and Anand 2015), distribution shift, meaning concept shift (MayeeChen et al. 18--24 Jul 2021) (Oortwijn, Bloem, and Sommerauer 2021). LLM whendeployed has found to undesirable behavior on critical, fine-grained sub-populationof data, such as bias, which needs to be detected and mitigated, tools to detect arehighly required (Hohman et al. 2018) (Goel et al. 2021)modeling, nextsentence prediction, which gives BERT ability for bidirectional representation andcontextual representation. Language understanding, generation are established dueto pre-training (Brown et al. 2020), even demonstrating few-shot abilities. Datasetsare key towards these abilities of LLM, high-quality data and how datasets arepre-processed is necessary. Datasets which are used for training can be describedas general data and specialized data (Chowdhery et al. 2022; S. Zhang et al. 2022)Datasets such as multi-lingual data, scientific data, code data have enabled LLM tosolve specific tasks (Taylor et al. 2022)
(Smith et al. 2013;Patel 2020ts for Training:Large language models are pre-trained on numerous corpus of dataset.Pre-training is a necessary step for allowing a language model to learn statistical properties, structure of human language present in data corpus.During pre-training, LLM learns the relationship between syntax, meaning, structure present between words in sentences.Vector representation of words are captured in semantic and syntactic ways through word embeddings.Word embeddings represent each word as high-dimensional vectors, which are used for optimizing for pre-training.Pre-training allows LLM to model as universal language model, which can be applied to downstream NLP tasks, that can be transferable, Pre-training in language models such as(Devlin et al. 2018) BERT involves masked language General Data for LLM: General text data contains dataset for general purpose LLM, thich includes the majority of dataset such as webpages, books, conversational text(Raffel et al. 2020).Webpage datasets contain large amount of data, which is absorbed by using(Smith et al. 2013;Patel 2020) common-crawl data, due to huge volume of</p>
<p>(Yao et al. 20232023)2))amework, EasyEdit (P.Wang et al. 2023)has been released to facilitate research of knowledge editing for LLM, how to update effectively within LLM remains an open research problem.While LLM has also demonstrated reasoning capabilities(Bubeck et al. 2023), many tasks require reasoning and relying on logical relations and evidence about factual knowledge on a particular question, chain of thought (Z.Zhang et al. 2022)prompting has been proposed for enhancing complex reasoning capacity of LLM, where intermediate reasoning steps can be manually created, automatically generated into prompts to guide LLMs to perform multi-step reasoning, reformulating tasks such as code generation improves performance.However, for tasks such as common sense reasoning, LLM and human performance is compared on Commonsense QA.LLM has accuracy of 55.9%(Wei, Wang, et al. 2022)(Chowdhery et al. 2022)(Dhingra et al. 2023), while human accuracy on dataset is about 89%, BERT performed below 10% on non-entailment category.LLM might miss or generate inaccurate intermediate steps, leading towards wrong final results.Thus, Reasoning inconsistencies are observed, even though LLM might produce correct answers, it might produce a wrong answer after a correct answer,(Yao et al. 2023) Tree of thoughts introduced mitigates this issue by empowering decision making process by exploring, self-evaluating reasoning paths.LLMs are found to have limitations when tasked with numerical computation, especially when tokens are not encountered in the dataset.</p>
<p>(Glaese et al. 2022))2022),Evaluationmethods using benchmarks are automated, however they suffer from training, test data contamination, human evaluation measure real world performance, however expensive and not reproducible, model based evaluation are efficient, however suffer from bias.(Shaikh et al. 2021) Data imbalance, a frequently occurring problem, where in the training dataset, one category of data exceeds the other category, thus causing imbalance in the dataset.Lack of enough data samples across class labels results in poor classification performance.To align LLM with human values such as harmlessness, honesty, helpfulness, honesty, Reinforcement learning from human feedback (RLHF) is used, which uses a reward model from human feedback, implemented as a reward function to optimise agent's policy using reinforcement learning (RL) using proximal policy optimization.Human feedback is gained by asking humans to rank instance's of LLM's behaviour from outputs generated.Human feedback is executed through ranking based(Ziegler et al. 2019), question-based approach(Nakano et al. 2021) and rule based approach(Glaese et al. 2022), which Sparrow(Glaese et al. 2022) selects based on rules to test whether model-generated responses meet alignment criteria.While RLHF has mitigated hallucinations to a certain degree, it suffers from limitations such as requiring training multiple LMs as model being aligned, reward model, reference model, which requires tedious process in algorithmic and consumes so much memory.Another major limitation is that RLHF is extremely complex and time consuming to deploy.</p>
<p>is observed, which remains a challenge in neural networks, as originally learned knowledge during training in LLM becomes damaged, affecting performance and abilities of LLM.Due to LLM (BigScience Workshop et al. 2022) (Zeng et al. 2022) being sensitive to data quality, systematic approaches for optimizing, factors of model effectiveness, efficiency optimization, training stability is required for economical reasons.Reinforcement Learning with Human Feedback (RLHF)</p>
<p>. J Austin, A Odena, M Nye, M Bosma, H Michalewski, D Dohan, E Jiang, C Cai, M Terry, Q Le, C Sutton, 2021Program Synthesis with Large Language Models</p>
<p>D Bahdanau, K Cho, Y Bengio, Neural Machine Translation by Jointly Learning to Align and Translate. 2014</p>
<p>Y Bang, S Cahyawijaya, N Lee, W Dai, D Su, B Wilie, H Lovenia, Z Ji, T Yu, W Chung, Q V Do, Y Xu, P Fung, A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity. 2023</p>
<p>F A Batarseh, L Freeman, AI Assurance: Towards Trustworthy, Explainable, Safe, and Ethical AI. Academic Press2022</p>
<p>J Baumgartner, S Zannettou, B Keegan, M Squire, J Blackburn, The Pushshift Reddit Dataset. ICWSM. 202014</p>
<p>On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ü¶ú. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. the 2021 ACM Conference on Fairness, Accountability, and Transparency2021</p>
<p>A neural probabilistic language model. Y Bengio, R Ducharme, P Vincent, Adv. Neural Inf. Process. Syst. 132000</p>
<p>Learning the protein language: Evolution, structure, and function. T Bepler, B Berger, T Fan, A Akiki, C Pavlick, E Iliƒá, S Hesslow, D Castagn√©, R Luccioni, A S Yvon, F Gall√©, M Tow, J Rush, A M Biderman, S Webson, A Ammanamanchi, P S Wang, T Sagot, B Wolf, BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. 2021. 202212Le Scao,</p>
<p>S L Blodgett, B O'connor, Racial Disparity in Natural Language Processing: A Case Study of Social Media African-American English. 2017</p>
<p>. R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Von Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, E Brynjolfsson, S Buch, D Card, R Castellon, N Chatterji, A Chen, K Creel, J Q Davis, D Demszky, P Liang, 2021On the Opportunities and Risks of Foundation Models</p>
<p>A general survey on attention mechanisms in deep learning. T Brants, A C Popat, P Xu, F J Och, J Dean, G Brauwers, F Frasincar, IEEE Trans. Knowl. Data Eng. 3542023Large Language Models in Machine Translation</p>
<p>Language models are few-shot learners. T Brown, &amp; Others.B Mann, &amp; Others.N Ryder, &amp; Others.M Subbiah, &amp; Others.J D Kaplan, &amp; Others.P Dhariwal, &amp; Others.A Neelakantan, &amp; Others.P Shyam, &amp; Others.G Sastry, &amp; Others.A Askell, &amp; Others.Adv. Neural Inf. Process. Syst. 332020</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, H Nori, H Palangi, M T Ribeiro, Y Zhang, Sparks of Artificial General Intelligence: Early experiments with GPT-4. 2023</p>
<p>. E Caballero, K Gupta, I Rish, D Krueger, 2022Broken Neural Scaling Laws</p>
<p>N Carlini, D Ippolito, M Jagielski, K Lee, F Tramer, C Zhang, Quantifying Memorization Across Neural Language Models. 2022</p>
<p>D Chen, Y Huang, Z Ma, H Chen, X Pan, C Ge, D Gao, Y Xie, Z Liu, J Gao, Y Li, B Ding, J Zhou, Data-Juicer: A One-Stop Data Processing System for Large Language Models. 2023</p>
<p>Mandoline: Model Evaluation under Distribution Shift. M Chen, K Goel, N S Sohoni, F Poms, K Fatahalian, C Re, Proceedings of the 38th International Conference on Machine Learning. M Meila, T Zhang, the 38th International Conference on Machine LearningPMLR2021139</p>
<p>. M Chen, J Tworek, H Jun, Q Yuan, H P De Oliveira Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, A Ray, R Puri, G Krueger, M Petrov, H Khlaaf, G Sastry, P Mishkin, B Chan, S Gray, W Zaremba, 2021Evaluating Large Language Models Trained on Code</p>
<p>A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, P Schuh, K Shi, S Tsvyashchenko, J Maynez, A Rao, P Barnes, Y Tay, N Shazeer, V Prabhakaran, N Fiedel, PaLM: Scaling Language Modeling with Pathways. 2022</p>
<p>Scaling Instruction-Finetuned Language Models. H W Chung, L Hou, S Longpre, B Zoph, Y Tay, W Fedus, Y Li, X Wang, M Dehghani, S Brahma, A Webson, S S Gu, Z Dai, M Suzgun, X Chen, A Chowdhery, A Castro-Ros, M Pellat, K Robinson, J Wei, 2022</p>
<p>Automated Source Code Generation and Auto-Completion Using Deep Learning: Comparing and Discussing Current Language Model-Related Approaches. J Cruz-Benito, S Vishwakarma, F Martin-Fernandez, I Faro, Machine Hallucinations: Architecture and Artificial Intelligence. M Campo, N Leach, John Wiley &amp; Sons2021. 2022AI</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, 2018</p>
<p>S Dhingra, M Singh, S B Vaisakh, N Malviya, S S Gill, Mind meets machine: Unravelling GPT-4's cognitive psychology. 2023</p>
<p>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. A Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, T Unterthiner, M Dehghani, M Minderer, G Heigold, S Gelly, J Uszkoreit, N Houlsby, N Du, Y Huang, A M Dai, S Tong, D Lepikhin, Y Xu, M Krikun, Y Zhou, A W Yu, O Firat, B Zoph, L Fedus, M P Bosma, Z Zhou, T Wang, E Wang, K Webster, M Pellat, K Robinson, C Cui, Proceedings of the 39th International Conference on Machine Learning. K Chaudhuri, S Jegelka, L Song, C Szepesvari, G Niu, &amp; S Sabato, the 39th International Conference on Machine LearningPMLR2020. 2022162GLaM: Efficient Scaling of Language Models with Mixture-of-Experts</p>
<p>GLaM: Efficient scaling of language models with mixture-of-experts. N Du, Y Huang, A M Dai, S Tong, D Lepikhin, Y Xu, M Krikun, Y Zhou, A W Yu, O Firat, B Zoph, L Fedus, M Bosma, Z Zhou, T Wang, Y E Wang, K Webster, M Pellat, K Robinson, C Cui, 2021</p>
<p>Quality Assurance for AI-Based Systems: Overview and Challenges (Introduction to Interactive Session. A Fan, T Lavril, E Grave, A Joulin, S Sukhbaatar, M Felderer, R Ramler, Software Quality: Future Perspectives on Software Engineering Quality. 2020. 2021Addressing Some Limitations of Transformers with Feedback Memory</p>
<p>B Fetahu, A Anand, A Anand, How much is Wikipedia Lagging Behind News? Proceedings of the ACM Web Science Conference. 201528</p>
<p>Fair-Prism: Evaluating fairness-related harms in text generation. E Fleisig, A Amstutz, C Atalla, S L Blodgett, H Daum√©, Iii, A Olteanu, E Sheng, D Vann, H Wallach, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2023</p>
<p>A new algorithm for data compression. P Gage, C Users Journal. </p>
<p>ChatGPT Basics: An Introduction to the Capabilities and Potential of Large Language Models. M Gamil, 2023Mostafa Gamil</p>
<p>L Gao, S Biderman, S Black, L Golding, T Hoppe, C Foster, J Phang, H He, A Thite, N Nabeshima, S Presser, C Leahy, The Pile: An 800GB Dataset of Diverse Text for Language Modeling. 2020</p>
<p>ChatGPT outperforms crowd workers for text-annotation tasks. F Gilardi, M Alizadeh, M Kubli, Proc. Natl. Acad. Sci. U. S. A. 12030e23050161202023</p>
<p>. A Glaese, N Mcaleese, M Trƒôbacz, J Aslanides, V Firoiu, T Ewalds, M Rauh, L Weidinger, M Chadwick, P Thacker, L Campbell-Gillingham, J Uesato, P.-S Huang, R Comanescu, F Yang, A See, S Dathathri, R Greig, C Chen, G Irving, 2022Improving alignment of dialogue agents via targeted human judgements</p>
<p>K Goel, N Rajani, J Vig, S Tan, J Wu, S Zheng, C Xiong, M Bansal, C R√©, Robustness Gym: Unifying the NLP Evaluation Landscape. 2021</p>
<p>Long short-term memory. S Hochreiter, J Schmidhuber, Neural Comput. 1997</p>
<p>An empirical analysis of compute-optimal large language model training. J Hoffmann, S Borgeaud, A Mensch, E Buchatskaya, T Cai, E Rutherford, D De Las Casas, L A Hendricks, J Welbl, A Clark, T Hennigan, E Noland, K Millican, G Van Den Driessche, B Damoc, A Guy, S Osindero, K Simonyan, E Elsen, L Sifre, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, &amp; A Oh, Curran Associates, Inc202235</p>
<p>. J Hoffmann, S Borgeaud, A Mensch, E Buchatskaya, T Cai, E Rutherford, D De Las Casas, L A Hendricks, J Welbl, A Clark, T Hennigan, E Noland, K Millican, G Van Den Driessche, B Damoc, A Guy, S Osindero, K Simonyan, E Elsen, L Sifre, 2022Training Compute-Optimal Large Language Models</p>
<p>Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers. F M Hohman, M Kahng, R Pienta, D H Chau, IEEE Trans. Vis. Comput. Graph. 2018</p>
<p>An Introductory Survey on Attention Mechanisms in NLP Problems. Intelligent Systems and Applications. D Hu, 2020</p>
<p>Large Language Models Can Self-Improve. J Huang, S S Gu, L Hou, Y Wu, X Wang, H Yu, J Han, 2022</p>
<p>Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents. W Huang, P Abbeel, D Pathak, I Mordatch, Proceedings of the 39th International Conference on Machine Learning. K Chaudhuri, S Jegelka, L Song, C Szepesvari, G Niu, &amp; S Sabato, the 39th International Conference on Machine LearningPMLR2022162</p>
<p>M Iyyer, M Gardner, C Clark, K Lee, L Zettlemoyer, K Jared, M C Sam, H Tom, CoRR abs/1802.05365Deep contextualized word representations. Brown Tom, B , Chess Benjamin, Child Rewon, Gray Scott, Radford Alec, Wu Jeffrey, Amodei Dario, 2018. 2020Scaling Laws for Neural Language Models</p>
<p>Natural language processing. D Jurafsky, C Manning, 20122123482Instructor</p>
<p>J Kaddour, J Harris, M Mozes, H Bradley, R Raileanu, R Mchardy, Challenges and Applications of Large Language Models. 2023</p>
<p>Understanding Catastrophic Forgetting and Remembering in Continual Learning with Optimal Relevance Mapping. J Kaplan, S Mccandlish, T Henighan, T B Brown, B Chess, R Child, S Gray, A Radford, J Wu, D Amodei, P Kaushik, A Gain, A Kortylewski, A Yuille, 2020. 2021Scaling Laws for Neural Language Models</p>
<p>. D Kiela, M Bartolo, Y Nie, D Kaushik, A Geiger, Z Wu, B Vidgen, G Prasad, A Singh, P Ringshia, Z Ma, T Thrush, S Riedel, Z Waseem, P Stenetorp, R Jia, M Bansal, C Potts, A Williams, 2021Dynabench: Rethinking Benchmarking in NLP</p>
<p>Controlling Conditional Language Models without Catastrophic Forgetting. T Korbak, H Elsahar, G Kruszewski, M Dymetman, Proceedings of the 39th International Conference on Machine Learning. K Chaudhuri, S Jegelka, L Song, C Szepesvari, G Niu, &amp; S Sabato, the 39th International Conference on Machine LearningPMLR2022162</p>
<p>K Lee, O Firat, A Agarwal, C Fannjiang, D Sussillo, Hallucinations in Neural Machine Translation. 2018</p>
<p>K Lee, D Ippolito, A Nystrom, C Zhang, D Eck, C Callison-Burch, N Carlini, Deduplicating Training Data Makes Language Models Better. 2021</p>
<p>HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models. J Li, X Cheng, W X Zhao, J.-Y Nie, J.-R Wen, arXiv:2305.11747ArXiv E-Prints. 2023</p>
<p>Pretrained Language Models for Text Generation: A Survey. J Li, T Tang, W X Zhao, J.-R Wen, 2021</p>
<p>TruthfulQA: Measuring How Models Mimic Human Falsehoods. S Lin, J Hilton, O Evans, 202129</p>
<p>A survey of transformers. T Lin, Y Wang, X Liu, X Qiu, AI Open. 32022</p>
<p>Y Liu, Y Yao, J.-F Ton, X Zhang, R Guo, H Cheng, Y Klochkov, M F Taufiq, H Li, Trustworthy LLMs: A Survey and Guideline for Evaluating Large Language Models' Alignment. 2023</p>
<p>SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models. P Manakul, A Liusie, M J F Gales, 2023</p>
<p>Sources of Hallucination by Large Language Models on Inference Tasks. N Mckenna, T Li, L Cheng, M J Hosseini, M Johnson, M Steedman, 2023</p>
<p>B Min, H Ross, E Sulem, A P B Veyseh, T H Nguyen, O Sainz, E Agirre, I Heinz, D Roth, Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey. 2021</p>
<p>DALLE 2 preview: Risks and limitations. P Mishkin, L Ahmad, M Brundage, G Krueger, G Sastry, 2022Github</p>
<p>GitHub Copilot AI pair programmer: Asset or Liability?. Moradi Dakhel, A Majdinasab, V Nikanjam, A Khomh, F Desmarais, M C Jiang, Z M , J. Syst. Softw. 2031117342023</p>
<p>WebGPT: Browser-assisted question-answering with human feedback. R Nakano, J Hilton, S Balaji, J Wu, L Ouyang, C Kim, C Hesse, S Jain, V Kosaraju, W Saunders, X Jiang, K Cobbe, T Eloundou, G Krueger, K Button, M Knight, B Chess, J Schulman, 2021</p>
<p>CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models. N Nangia, C Vania, R Bhalerao, S R Bowman, 2020</p>
<p>Challenging distributional models with a conceptual network of philosophical terms. Of the 2021 ‚Ä¶. OpenAI. Y Oortwijn, &amp; others.J Bloem, &amp; others.P Sommerauer, &amp; others.2021. 2023GPT-4 Technical Report</p>
<p>Bootleg: Chasing the Tail with Self-Supervised Named Entity Disambiguation. L Orr, M Leszczynski, S Arora, S Wu, N Guha, X Ling, C Re, 2020</p>
<p>A Parisi, Y Zhao, N Fiedel, TALM: Tool Augmented Language Models. 2022</p>
<p>Introduction to Common Crawl Datasets. J M Patel, Getting Structured Data from the Internet: Running Web Crawlers/Scrapers on a Big Data Production Scale. J M Patel, Apress2020</p>
<p>Evaluating a large language model's ability to solve programming exercises from an introductory bioinformatics course. S R Piccolo, P Denny, A Luxton-Reilly, S H Payne, P G Ridge, PLoS Comput. Biol. 199e10115112023</p>
<p>Inexpensive Domain Adaptation of Pretrained Language Models: Case Studies on Biomedical NER and Covid-19 QA. N Poerner, U Waltinger, H Sch√ºtze, 2020</p>
<p>Improving language understanding by generative pre-training. A Radford, cs.ubc.caK Narasimhan, cs.ubc.caT Salimans, cs.ubc.caI Sutskever, cs.ubc.ca2018</p>
<p>Language modelling at scale: Gopher, ethical considerations, and retrieval. J Rae, G Irving, L Weidinger, DeepMind Blog</p>
<p>J W Rae, S Borgeaud, T Cai, K Millican, J Hoffmann, F Song, J Aslanides, S Henderson, R Ring, S Young, E Rutherford, T Hennigan, J Menick, A Cassirer, R Powell, G Van Den Driessche, L A Hendricks, M Rauh, P.-S Huang, G Irving, Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher. 2021</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, J. Mach. Learn. Res. 2112020</p>
<p>SQuAD: 100,000+ Questions for Machine Comprehension of Text. P Rajpurkar, J Zhang, K Lopyrev, P Liang, 2016</p>
<p>. S Roller, E Dinan, N Goyal, D Ju, M Williamson, Y Liu, J Xu, M Ott, K Shuster, E M Smith, Y.-L Boureau, J Weston, 2020Recipes for building an open-domain chatbot</p>
<p>All arXiv Publications Pre-Processed for NLP, Including Structured Full-Text and Citation Network. T Saier, J Krause, M F√§rber, R Sennrich, B Haddow, A Birch, Neural Machine Translation of Rare Words with Subword Units. 2023. 2022. 2015</p>
<p>Towards Improved Classification Accuracy on Highly Imbalanced Text Dataset Using Deep Neural Language Models. S Shaikh, S M Daudpota, A S Imran, Z Kastrati, NATO Adv. Sci. Inst. Ser. E Appl. Sci. 1128692021</p>
<p>A mathematical theory of communication. C E Shannon, Bell Syst. Tech. J. 2741948</p>
<p>Experiments with a Heuristic Compiler. H A Simon, J. ACM. 1041963</p>
<p>Dirt cheap web-scale parallel text from the Common Crawl. J R Smith, H Saint-Amand, M Plamada, P Koehn, C Callison-Burch, A Lopez, Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 51st Annual Meeting of the Association for Computational Linguistics20131</p>
<p>I Solaiman, M Brundage, J Clark, A Askell, A Herbert-Voss, J Wu, A Radford, G Krueger, J W Kim, S Kreps, M Mccain, A Newhouse, J Blazakis, K Mcguffie, J Wang, Release Strategies and the Social Impacts of Language Models. 2019</p>
<p>R Taylor, M Kardas, G Cucurull, T Scialom, A Hartshorn, E Saravia, A Poulton, V Kerkez, R Stojnic, Galactica: A Large Language Model for Science. 2022</p>
<p>Impact of Tokenization on Language Models: An Analysis for Turkish. C Toraman, E H Yilmaz, F ≈ûahƒ±Ãánu√ß, O Ozcelik, ACM Trans. Asian Low-Resour. Lang. Inf. Process. 2242023</p>
<p>LLaMA: Open and Efficient Foundation Language Models. H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozi√®re, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample, 2023</p>
<p>. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, D Bikel, L Blecher, C C Ferrer, M Chen, G Cucurull, D Esiobu, J Fernandes, J Fu, W Fu, T Scialom, 2023Llama 2: Open Foundation and Fine-Tuned Chat Models</p>
<p>Attention is All you Need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Kaiser, I Polosukhin, Adv. Neural Inf. Process. Syst. 302017</p>
<p>A Wang, A Singh, J Michael, F Hill, O Levy, S R Bowman, GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. 2018</p>
<p>B Wang, A Komatsuzaki, GPT-J-6B: A 6 billion parameter autoregressive language model. 2021</p>
<p>P Wang, N Zhang, X Xie, Y Yao, B Tian, M Wang, Z Xi, S Cheng, K Liu, G Zheng, H Chen, EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models. 2023</p>
<p>Beyond Accuracy: What Data Quality Means to Data Consumers. R Y Wang, D M Strong, Journal of Management Information Systems. 1241996</p>
<p>. J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, D Yogatama, M Bosma, D Zhou, D Metzler, E H Chi, T Hashimoto, O Vinyals, P Liang, J Dean, W Fedus, 2022Emergent Abilities of Large Language Models</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q Le, D Zhou, 2022</p>
<p>A Williams, N Nangia, S R Bowman, A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference. 2017</p>
<p>Challenges in Data-to-Document Generation. S Wiseman, S M Shieber, A M Rush, 2017</p>
<p>Tree of Thoughts: Deliberate Problem Solving with Large Language Models. S Yao, D Yu, J Zhao, I Shafran, T L Griffiths, Y Cao, K Narasimhan, 2023</p>
<p>WuDaoCorpora: A super large-scale Chinese corpora for pre-training language models. S Yuan, H Zhao, Z Du, M Ding, X Liu, Y Cen, X Zou, Z Yang, J Tang, AI Open. 22021</p>
<p>A Zeng, X Liu, Z Du, Z Wang, H Lai, M Ding, Z Yang, Y Xu, W Zheng, X Xia, W L Tam, Z Ma, Y Xue, J Zhai, W Chen, P Zhang, Y Dong, J Tang, GLM-130B: An Open Bilingual Pre-trained Model. 2022</p>
<p>. S Zhang, S Roller, N Goyal, M Artetxe, M Chen, S Chen, C Dewan, M Diab, X Li, X V Lin, T Mihaylov, M Ott, S Shleifer, K Shuster, D Simig, P S Koura, A Sridhar, T Wang, L Zettlemoyer, 2022OPT: Open Pre-trained Transformer Language Models</p>
<p>Automatic Chain of Thought Prompting in Large Language Models. Z Zhang, A Zhang, M Li, A Smola, 2022</p>
<p>. W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, Y Du, C Yang, Y Chen, Z Chen, J Jiang, R Ren, Y Li, X Tang, Z Liu, J.-R Wen, A Survey of Large Language Models. 2023</p>
<p>D M Ziegler, N Stiennon, J Wu, T B Brown, A Radford, D Amodei, P Christiano, G Irving, Fine-Tuning Language Models from Human Preferences. 2019</p>            </div>
        </div>

    </div>
</body>
</html>
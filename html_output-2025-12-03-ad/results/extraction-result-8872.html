<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8872 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8872</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8872</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-f307b0674941dbdfcc4ff83ed9fd962a9f4350e1</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f307b0674941dbdfcc4ff83ed9fd962a9f4350e1" target="_blank">Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is hypothesized that the learned semantics of language tokens do the most heavy lifting during the reasoning process of Large Language Models, and a novel perspective is provided on the role of semantics in developing and evaluating language models' reasoning abilities.</p>
                <p><strong>Paper Abstract:</strong> The emergent few-shot reasoning capabilities of Large Language Models (LLMs) have excited the natural language and machine learning community over recent years. Despite of numerous successful applications, the underlying mechanism of such in-context capabilities still remains unclear. In this work, we hypothesize that the learned \textit{semantics} of language tokens do the most heavy lifting during the reasoning process. Different from human's symbolic reasoning process, the semantic representations of LLMs could create strong connections among tokens, thus composing a superficial logical chain. To test our hypothesis, we decouple semantics from the language reasoning process and evaluate three kinds of reasoning abilities, i.e., deduction, induction and abduction. Our findings reveal that semantics play a vital role in LLMs' in-context reasoning -- LLMs perform significantly better when semantics are consistent with commonsense but struggle to solve symbolic or counter-commonsense reasoning tasks by leveraging in-context new knowledge. The surprising observations question whether modern LLMs have mastered the inductive, deductive and abductive reasoning abilities as in human intelligence, and motivate research on unveiling the magic existing within the black-box LLMs. On the whole, our analysis provides a novel perspective on the role of semantics in developing and evaluating language models' reasoning abilities. Code is available at {\url{https://github.com/XiaojuanTang/ICSR}}.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8872.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8872.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT - Symbolic Tree</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT evaluated on the Symbolic Tree dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT (OpenAI chat completion model) was evaluated on strict synthetic symbolic reasoning (deduction, induction, abduction) using the Symbolic Tree benchmark under multiple prompting regimes and with semantics decoupled (symbols) vs retained (semantics).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI chat-completion model accessed via API (temperature=0, default penalties); evaluated with zero-shot, zero-shot Chain-of-Thought (CoT), few-shot-CoT, and variants where dataset semantics were replaced by abstract symbols.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Symbolic Tree (deduction / induction / abduction)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A synthetic, closed-world, noise-free multi-hop symbolic dataset generated with logical rules (≈28 inferred relations, gender and parentOf relation, trees of depth ≤5). Tasks: (1) Deduction: decide True/False of a queried fact from facts+rules; (2) Induction: infer a rule given facts and a template; (3) Abduction: produce an explanation (proof) for an inferred fact. The paper evaluates both Semantics (natural predicate names) and Symbols (relations replaced by r1,r2,...) settings.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluation under zero-shot, zero-shot-CoT ("Let's think step by step"), few-shot-CoT; semantic decoupling by replacing predicate names with symbols; ablation variants: removing rules/facts (force relying on internal commonsense) and counter-commonsense (shuffle relation labels to conflict with prior knowledge).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Symbols setting (ChatGPT): Deduction zero-shot 52.6%, zero-shot-CoT 55.7%, few-shot-CoT 54.8%; Induction zero-shot 6.10%, zero-shot-CoT 7.86%; Abduction zero-shot 1.50%, zero-shot-CoT 4.90%, few-shot-CoT 18.2%. Semantics setting (ChatGPT): Deduction zero-shot 66.1%, zero-shot-CoT 65.5%, few-shot-CoT 67.1%; Induction zero-shot 36.4%, zero-shot-CoT 32.2%; Abduction zero-shot 2.94%, zero-shot-CoT 3.40%, few-shot-CoT 21.8%. (Averaged across 10 sampled Symbolic Trees as reported.)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to random baseline (deduction ≈50.1%, induction ≈3.57%) ChatGPT is above random for deduction but near-random for induction/abduction in Symbols; logic-based symbolic methods achieve 100% on deduction and abduction and 57.1% on induction (logic-based stronger across all symbolic tasks). GPT-4 substantially outperforms ChatGPT on the same tasks (see separate entry).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Severe performance drop when semantics are decoupled (Symbols) or when in-context information conflicts with commonsense (counter-commonsense). Very poor induction and abduction in Symbol setting. Chain-of-Thought can help in some Symbol settings but is not universally beneficial; sometimes CoT produced more "Cannot be determined." answers. Performance degrades with long in-context contexts (hard to select relevant facts).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Paper concludes ChatGPT's in-context symbolic reasoning heavily depends on semantics / stored commonsense; semantics consistent with commonsense substantially improve results (especially induction), while symbolic representations force the model to rely on weak heuristics. The authors interpret results as LLMs forming semantic shortcuts rather than performing robust formal symbolic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8872.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8872.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 - Symbolic Tree</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 evaluated on the Symbolic Tree dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 (OpenAI) was evaluated on the same synthetic Symbolic Tree tasks and prompting regimes; it shows markedly stronger performance than ChatGPT but still inferior to pure logic-based systems on strictly symbolic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-4, evaluated via chat completion API (temperature=0); same prompting regimes (zero-shot, zero-shot-CoT, few-shot-CoT), semantics vs symbols experiments, and ablations (remove rules/facts, counter-commonsense).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Symbolic Tree (deduction / induction / abduction)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Synthetic closed-world multi-hop symbolic dataset requiring formal logical inference across facts and rules; tested across depths and multiple relation types.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot, zero-shot-CoT, few-shot-CoT; semantically intact vs symbolized inputs; remove rules/facts experiments forcing reliance on parameter-stored commonsense; counter-commonsense label shuffling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Symbols setting (GPT-4): Deduction zero-shot 68.8%, zero-shot-CoT 71.1%, few-shot-CoT 67.6%; Induction zero-shot 9.28%, zero-shot-CoT 8.93%; Abduction zero-shot 25.0%, zero-shot-CoT 31.2%, few-shot-CoT 44.2%. Semantics setting (GPT-4): Deduction zero-shot 79.2%, zero-shot-CoT 86.2%, few-shot-CoT 91.1%; Induction zero-shot 52.5%, zero-shot-CoT 53.9%; Abduction zero-shot 27.3%, zero-shot-CoT 33.4%, few-shot-CoT 69.2%. (Values from averaged experiments reported in Table 2.)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms ChatGPT by substantial margins across Semantics and Symbols. Still underperforms symbolic logic-based baselines (logic-based: deduction 100%, abduction 100%, induction 57.1%). Remove rules/facts experiment shows GPT-4 can leverage internal commonsense (remove R/F deductive few-shot-CoT = 90.4%)—indicating strong parametric commonsense.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Although GPT-4 is much stronger, it still struggles when in-context provided knowledge conflicts with its prior commonsense (counter-commonsense setting performance drops), and induction remains challenging in purely symbolized settings. Abduction performance varies widely depending on prompt and CoT; long contexts still impede performance.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>GPT-4's superior performance demonstrates richer internal commonsense helps in-context reasoning, but even GPT-4 appears to rely on semantic priors and shortcuts rather than systematic symbolic manipulation; authors recommend benchmarks with decoupled semantics to more fairly evaluate symbolic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8872.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8872.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT - ProofWriter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT evaluated on ProofWriter (OWA depths 1,2,3,5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT was evaluated on the ProofWriter open-world entailment/proof generation dataset (depths 1,2,3,5) with semantics decoupled (entity IDs) and with CoT prompting; results show performance depends on whether semantics align with commonsense and that CoT can sometimes hurt.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Proofwriter: Generating implications, proofs, and abductive statements over natural language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI chat-completion model; evaluated on ProofWriter OWA subsets with entity names replaced by IDs to decouple semantics; measured deductive accuracy per depth.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProofWriter (deductive entailment, depths 1/2/3/5)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>ProofWriter supplies small rulebases (natural-language facts and rules) and questions classified as provable True/False/Unknown; dataset requires multi-hop rule-based proofs; authors remove 'Unknown's and replace entities with IDs for decoupling semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot, zero-shot-CoT, few-shot-CoT prompting; entities replaced with IDs (Symbols) vs kept (Semantics) to test role of semantics; evaluated at multiple proof depths.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>ChatGPT (Symbols): Deduction accuracy depth-1: zero-shot 69.1%, zero-shot-CoT 56.2%, few-shot-CoT 65.8%; depth-2: zero-shot 62.3%, zero-shot-CoT 49.4%, few-shot-CoT 58.1%; depth-3: zero-shot 59.4%, zero-shot-CoT 45.2%; depth-5: zero-shot 52.8%, zero-shot-CoT 38.6%. ChatGPT (Semantics): similar zero-shot numbers (depth-1 69.0%, depth-2 63.5%, depth-3 60.3%, depth-5 51.4%); but CoT often reduced performance and increased 'Cannot be determined.' answers.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Symbols and Semantics performed comparably in zero-shot settings (when semantics are irrelevant to commonsense), but CoT degraded performance for many ProofWriter experiments; no explicit numeric logic-baseline reported for ProofWriter in paper, but symbolic provers would be expected to solve these rulebases perfectly.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Chain-of-thought sometimes harms performance on ProofWriter (CoT < zero-shot) and produces more 'Unknown' outputs; when semantics are unhelpful or odd, step-by-step reasoning magnifies disturbance. Models rely less on semantics for ProofWriter (since tasks are not commonsense-aligned) but still fail on deeper multi-hop reasoning with longer contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>When task semantics are irrelevant to commonsense, replacing entities with symbols has limited effect on zero-shot performance; however, CoT is not universally helpful and can even worsen results on strictly symbolic rule reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8872.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8872.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-7B - Memorization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-7B fine-tuned and evaluated for memorization of new symbolic facts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLaMA-7B was fine-tuned on synthetic triplets to test time/efficiency/forgetting when inserting/updating symbolic or semantic facts; evaluated via filter MRR and compared to a graph DB baseline (Neo4j).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLaMA model (7B parameters) fine-tuned on 1,258 triplets from Symbolic Trees with 100 epochs (cosine LR schedule, lr=2e-5) on 4x A100 80G. Evaluation: rank-based queries giving filter MRR of true tail among candidate entities.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Memorization / knowledge update (add / update / forgetting)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Fine-tune-and-query paradigm measuring how well an LLM can add new triplets (adding efficiency MRR), update existing triplets (updating efficiency MRR), and the forgetting ratio (MRR of facts that should not be updated). Tests performed in Semantics vs Symbols settings.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning LLaMA-7B on synthetic triplets; then updating tails of some triplets and measuring MRR/forgetting; comparison to Neo4j graph DB (non-parametric).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Semantics: Adding efficiency (MRR) = 50.375 ± 1.27%; Updating efficiency = 51.34 ± 0.55%; Forgetting = 7.02 ± 1.55%; Time per 1k triplets = 41.5 minutes. Symbols: Adding 48.91 ± 4.3%; Updating 40.74 ± 2.26%; Forgetting 2.2 ± 0.99%; Time per 1k = 41.5 minutes.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Neo4j graph DB baseline: Adding efficiency 100%, Updating efficiency 100%, Forgetting 0%, Time per 1k triplets = 19 seconds — substantially better than fine-tuned LLaMA for exact insertion/updating and faster.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Parametric LLaMA-7B is much less efficient and accurate at exact knowledge insertion and editing than a non-parametric graph DB; semantics increased forgetting (semantic facts more entangled), and fine-tuning is time-consuming. LLaMA updates can unintentionally influence other facts due to semantic correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Authors suggest combining LLMs with external non-parametric knowledge bases (e.g., graph DBs) for practical memory/update needs and highlight that LLMs are poor at precise symbolic knowledge updates compared to symbolic databases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8872.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8872.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neo4j baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neo4j graph database used as a memorization baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neo4j (graph database) is used as a non-parametric baseline for adding/updating triplets; achieves perfect insertion/updating accuracy and zero forgetting, with much faster update times than LLaMA-7B fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Neo4j</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A graph database system configured with a pre-stored knowledge base comparable in disk space to the LLaMA model for a fairer comparison; used to store and update triplets non-parametrically.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Memorization / knowledge base updates (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Non-parametric storage and update of symbolic triplets; querying performed to compare recall/forgetting against fine-tuned LLaMA.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Direct insertion/editing of triplets into graph database; retrieval via candidate-tail ranking (comparable evaluation protocol).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Adding efficiency 100%, Updating efficiency 100%, Forgetting 0%, Time per 1k triplets = 19 seconds (reported).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Used as the primary non-parametric baseline; outperforms LLaMA-7B on all memorization metrics by large margins.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not a neural reasoner; only provides exact storage and retrieval (no natural-language reasoning), but demonstrates practical advantages for knowledge updates.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Paper uses Neo4j results to argue for hybrid systems (LLM + external KB) for practical knowledge insertion/editing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8872.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8872.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logic-based symbolic baseline (enumeration / AMIE+)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic-based symbolic methods (path enumeration for deduction/abduction; AMIE+ for induction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Symbolic reasoning baselines that enumerate activating rules/paths for deduction and abduction, and use AMIE+ for rule mining in induction; these baselines outperform LLMs on the synthetic Symbolic Tree tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Logic-based symbolic methods (enumeration, AMIE+)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Deductive/abductive baseline enumerates paths between head and tail and applies activated rules to infer answers; AMIE+ is a rule-mining system that enumerates candidate rules and assigns scalar weights encoding rule quality (used for induction baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Symbolic Tree (deduction / induction / abduction) baselines</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Exact symbolic reasoning: enumerative proof search for deduction/abduction; rule mining (AMIE+) for induction evaluated against ground-truth rules.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Symbolic enumeration of proof paths and activation of formal rules for deduction/abduction; AMIE+ for induction rule discovery and weighting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported in paper for Symbolic Tree: Deduction 100%, Abduction 100%, Induction 57.1% (AMIE+ result reported).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>These symbolic systems substantially outperform LLMs on deduction and abduction (LLMs <100%) and outperform or match LLMs on induction (LLMs only reach ~52.5% induction in best GPT-4 semantics case vs 57.1% for AMIE+).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Symbolic methods require formal rule representations and are not natural-language models; they are not evaluated for fluency or general natural-language understanding but excel at formal symbolic correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Authors use these baselines to demonstrate that pure symbolic systems remain superior on strict symbolic reasoning tasks and that LLMs are not yet capable of fully replacing formal logic engines for such tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8872.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8872.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GreaseLM - Commonsense baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Greaselm (Graph Reasoning Enhanced Language Model) evaluated on commonsense QA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GreaseLM, a graph-reasoning enhanced language model, was included as a strong baseline on commonsense benchmarks (CommonsenseQA, OpenBookQA) to contrast LLM commonsense performance with strict symbolic reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GreaseLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A graph-reasoning enhanced language model integrating graph reasoning for question answering; used as baseline on CommonsenseQA and OpenBookQA.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CommonsenseQA and OpenBookQA (used for comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmarks for commonsense question answering (multiple-choice) evaluating everyday knowledge and reasoning rather than strictly symbolic logic.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Few-shot and zero-shot prompting variants; GreaseLM uses graph reasoning enhancements.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported: GreaseLM CSQA-dev 78.5%, CSQA-test 74.2%, OpenBookQA-Test 84.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>GPT-4 and ChatGPT also evaluated: GPT-4 reaches >80% on CSQA and ~91.8% on OpenBookQA; GreaseLM is competitive on commonsense but not directly comparable to symbolic reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Commonsense benchmarks do not decouple semantics and thus do not probe formal symbolic reasoning; not intended as a strict-symbolic baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Authors use commonsense benchmarks to show LLMs (and graph-enhanced models) can excel on semantics-rich reasoning, highlighting the contrast with failures on decoupled-symbolic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Proofwriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
                <li>Ontology reasoning with deep neural networks <em>(Rating: 1)</em></li>
                <li>Fast rule mining in ontological knowledge bases with AMIE+ <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8872",
    "paper_id": "paper-f307b0674941dbdfcc4ff83ed9fd962a9f4350e1",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "ChatGPT - Symbolic Tree",
            "name_full": "ChatGPT evaluated on the Symbolic Tree dataset",
            "brief_description": "ChatGPT (OpenAI chat completion model) was evaluated on strict synthetic symbolic reasoning (deduction, induction, abduction) using the Symbolic Tree benchmark under multiple prompting regimes and with semantics decoupled (symbols) vs retained (semantics).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_description": "OpenAI chat-completion model accessed via API (temperature=0, default penalties); evaluated with zero-shot, zero-shot Chain-of-Thought (CoT), few-shot-CoT, and variants where dataset semantics were replaced by abstract symbols.",
            "model_size": "",
            "reasoning_task_name": "Symbolic Tree (deduction / induction / abduction)",
            "reasoning_task_description": "A synthetic, closed-world, noise-free multi-hop symbolic dataset generated with logical rules (≈28 inferred relations, gender and parentOf relation, trees of depth ≤5). Tasks: (1) Deduction: decide True/False of a queried fact from facts+rules; (2) Induction: infer a rule given facts and a template; (3) Abduction: produce an explanation (proof) for an inferred fact. The paper evaluates both Semantics (natural predicate names) and Symbols (relations replaced by r1,r2,...) settings.",
            "method_or_approach": "Evaluation under zero-shot, zero-shot-CoT (\"Let's think step by step\"), few-shot-CoT; semantic decoupling by replacing predicate names with symbols; ablation variants: removing rules/facts (force relying on internal commonsense) and counter-commonsense (shuffle relation labels to conflict with prior knowledge).",
            "performance": "Symbols setting (ChatGPT): Deduction zero-shot 52.6%, zero-shot-CoT 55.7%, few-shot-CoT 54.8%; Induction zero-shot 6.10%, zero-shot-CoT 7.86%; Abduction zero-shot 1.50%, zero-shot-CoT 4.90%, few-shot-CoT 18.2%. Semantics setting (ChatGPT): Deduction zero-shot 66.1%, zero-shot-CoT 65.5%, few-shot-CoT 67.1%; Induction zero-shot 36.4%, zero-shot-CoT 32.2%; Abduction zero-shot 2.94%, zero-shot-CoT 3.40%, few-shot-CoT 21.8%. (Averaged across 10 sampled Symbolic Trees as reported.)",
            "baseline_comparison": "Compared to random baseline (deduction ≈50.1%, induction ≈3.57%) ChatGPT is above random for deduction but near-random for induction/abduction in Symbols; logic-based symbolic methods achieve 100% on deduction and abduction and 57.1% on induction (logic-based stronger across all symbolic tasks). GPT-4 substantially outperforms ChatGPT on the same tasks (see separate entry).",
            "limitations_or_failures": "Severe performance drop when semantics are decoupled (Symbols) or when in-context information conflicts with commonsense (counter-commonsense). Very poor induction and abduction in Symbol setting. Chain-of-Thought can help in some Symbol settings but is not universally beneficial; sometimes CoT produced more \"Cannot be determined.\" answers. Performance degrades with long in-context contexts (hard to select relevant facts).",
            "insights_or_conclusions": "Paper concludes ChatGPT's in-context symbolic reasoning heavily depends on semantics / stored commonsense; semantics consistent with commonsense substantially improve results (especially induction), while symbolic representations force the model to rely on weak heuristics. The authors interpret results as LLMs forming semantic shortcuts rather than performing robust formal symbolic reasoning.",
            "uuid": "e8872.0",
            "source_info": {
                "paper_title": "Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-4 - Symbolic Tree",
            "name_full": "GPT-4 evaluated on the Symbolic Tree dataset",
            "brief_description": "GPT-4 (OpenAI) was evaluated on the same synthetic Symbolic Tree tasks and prompting regimes; it shows markedly stronger performance than ChatGPT but still inferior to pure logic-based systems on strictly symbolic tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI's GPT-4, evaluated via chat completion API (temperature=0); same prompting regimes (zero-shot, zero-shot-CoT, few-shot-CoT), semantics vs symbols experiments, and ablations (remove rules/facts, counter-commonsense).",
            "model_size": "",
            "reasoning_task_name": "Symbolic Tree (deduction / induction / abduction)",
            "reasoning_task_description": "Synthetic closed-world multi-hop symbolic dataset requiring formal logical inference across facts and rules; tested across depths and multiple relation types.",
            "method_or_approach": "Zero-shot, zero-shot-CoT, few-shot-CoT; semantically intact vs symbolized inputs; remove rules/facts experiments forcing reliance on parameter-stored commonsense; counter-commonsense label shuffling.",
            "performance": "Symbols setting (GPT-4): Deduction zero-shot 68.8%, zero-shot-CoT 71.1%, few-shot-CoT 67.6%; Induction zero-shot 9.28%, zero-shot-CoT 8.93%; Abduction zero-shot 25.0%, zero-shot-CoT 31.2%, few-shot-CoT 44.2%. Semantics setting (GPT-4): Deduction zero-shot 79.2%, zero-shot-CoT 86.2%, few-shot-CoT 91.1%; Induction zero-shot 52.5%, zero-shot-CoT 53.9%; Abduction zero-shot 27.3%, zero-shot-CoT 33.4%, few-shot-CoT 69.2%. (Values from averaged experiments reported in Table 2.)",
            "baseline_comparison": "Outperforms ChatGPT by substantial margins across Semantics and Symbols. Still underperforms symbolic logic-based baselines (logic-based: deduction 100%, abduction 100%, induction 57.1%). Remove rules/facts experiment shows GPT-4 can leverage internal commonsense (remove R/F deductive few-shot-CoT = 90.4%)—indicating strong parametric commonsense.",
            "limitations_or_failures": "Although GPT-4 is much stronger, it still struggles when in-context provided knowledge conflicts with its prior commonsense (counter-commonsense setting performance drops), and induction remains challenging in purely symbolized settings. Abduction performance varies widely depending on prompt and CoT; long contexts still impede performance.",
            "insights_or_conclusions": "GPT-4's superior performance demonstrates richer internal commonsense helps in-context reasoning, but even GPT-4 appears to rely on semantic priors and shortcuts rather than systematic symbolic manipulation; authors recommend benchmarks with decoupled semantics to more fairly evaluate symbolic reasoning.",
            "uuid": "e8872.1",
            "source_info": {
                "paper_title": "Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ChatGPT - ProofWriter",
            "name_full": "ChatGPT evaluated on ProofWriter (OWA depths 1,2,3,5)",
            "brief_description": "ChatGPT was evaluated on the ProofWriter open-world entailment/proof generation dataset (depths 1,2,3,5) with semantics decoupled (entity IDs) and with CoT prompting; results show performance depends on whether semantics align with commonsense and that CoT can sometimes hurt.",
            "citation_title": "Proofwriter: Generating implications, proofs, and abductive statements over natural language",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_description": "OpenAI chat-completion model; evaluated on ProofWriter OWA subsets with entity names replaced by IDs to decouple semantics; measured deductive accuracy per depth.",
            "model_size": "",
            "reasoning_task_name": "ProofWriter (deductive entailment, depths 1/2/3/5)",
            "reasoning_task_description": "ProofWriter supplies small rulebases (natural-language facts and rules) and questions classified as provable True/False/Unknown; dataset requires multi-hop rule-based proofs; authors remove 'Unknown's and replace entities with IDs for decoupling semantics.",
            "method_or_approach": "Zero-shot, zero-shot-CoT, few-shot-CoT prompting; entities replaced with IDs (Symbols) vs kept (Semantics) to test role of semantics; evaluated at multiple proof depths.",
            "performance": "ChatGPT (Symbols): Deduction accuracy depth-1: zero-shot 69.1%, zero-shot-CoT 56.2%, few-shot-CoT 65.8%; depth-2: zero-shot 62.3%, zero-shot-CoT 49.4%, few-shot-CoT 58.1%; depth-3: zero-shot 59.4%, zero-shot-CoT 45.2%; depth-5: zero-shot 52.8%, zero-shot-CoT 38.6%. ChatGPT (Semantics): similar zero-shot numbers (depth-1 69.0%, depth-2 63.5%, depth-3 60.3%, depth-5 51.4%); but CoT often reduced performance and increased 'Cannot be determined.' answers.",
            "baseline_comparison": "Symbols and Semantics performed comparably in zero-shot settings (when semantics are irrelevant to commonsense), but CoT degraded performance for many ProofWriter experiments; no explicit numeric logic-baseline reported for ProofWriter in paper, but symbolic provers would be expected to solve these rulebases perfectly.",
            "limitations_or_failures": "Chain-of-thought sometimes harms performance on ProofWriter (CoT &lt; zero-shot) and produces more 'Unknown' outputs; when semantics are unhelpful or odd, step-by-step reasoning magnifies disturbance. Models rely less on semantics for ProofWriter (since tasks are not commonsense-aligned) but still fail on deeper multi-hop reasoning with longer contexts.",
            "insights_or_conclusions": "When task semantics are irrelevant to commonsense, replacing entities with symbols has limited effect on zero-shot performance; however, CoT is not universally helpful and can even worsen results on strictly symbolic rule reasoning.",
            "uuid": "e8872.2",
            "source_info": {
                "paper_title": "Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LLaMA-7B - Memorization",
            "name_full": "LLaMA-7B fine-tuned and evaluated for memorization of new symbolic facts",
            "brief_description": "LLaMA-7B was fine-tuned on synthetic triplets to test time/efficiency/forgetting when inserting/updating symbolic or semantic facts; evaluated via filter MRR and compared to a graph DB baseline (Neo4j).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-7B",
            "model_description": "Open-source LLaMA model (7B parameters) fine-tuned on 1,258 triplets from Symbolic Trees with 100 epochs (cosine LR schedule, lr=2e-5) on 4x A100 80G. Evaluation: rank-based queries giving filter MRR of true tail among candidate entities.",
            "model_size": "7B",
            "reasoning_task_name": "Memorization / knowledge update (add / update / forgetting)",
            "reasoning_task_description": "Fine-tune-and-query paradigm measuring how well an LLM can add new triplets (adding efficiency MRR), update existing triplets (updating efficiency MRR), and the forgetting ratio (MRR of facts that should not be updated). Tests performed in Semantics vs Symbols settings.",
            "method_or_approach": "Fine-tuning LLaMA-7B on synthetic triplets; then updating tails of some triplets and measuring MRR/forgetting; comparison to Neo4j graph DB (non-parametric).",
            "performance": "Semantics: Adding efficiency (MRR) = 50.375 ± 1.27%; Updating efficiency = 51.34 ± 0.55%; Forgetting = 7.02 ± 1.55%; Time per 1k triplets = 41.5 minutes. Symbols: Adding 48.91 ± 4.3%; Updating 40.74 ± 2.26%; Forgetting 2.2 ± 0.99%; Time per 1k = 41.5 minutes.",
            "baseline_comparison": "Neo4j graph DB baseline: Adding efficiency 100%, Updating efficiency 100%, Forgetting 0%, Time per 1k triplets = 19 seconds — substantially better than fine-tuned LLaMA for exact insertion/updating and faster.",
            "limitations_or_failures": "Parametric LLaMA-7B is much less efficient and accurate at exact knowledge insertion and editing than a non-parametric graph DB; semantics increased forgetting (semantic facts more entangled), and fine-tuning is time-consuming. LLaMA updates can unintentionally influence other facts due to semantic correlations.",
            "insights_or_conclusions": "Authors suggest combining LLMs with external non-parametric knowledge bases (e.g., graph DBs) for practical memory/update needs and highlight that LLMs are poor at precise symbolic knowledge updates compared to symbolic databases.",
            "uuid": "e8872.3",
            "source_info": {
                "paper_title": "Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Neo4j baseline",
            "name_full": "Neo4j graph database used as a memorization baseline",
            "brief_description": "Neo4j (graph database) is used as a non-parametric baseline for adding/updating triplets; achieves perfect insertion/updating accuracy and zero forgetting, with much faster update times than LLaMA-7B fine-tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Neo4j",
            "model_description": "A graph database system configured with a pre-stored knowledge base comparable in disk space to the LLaMA model for a fairer comparison; used to store and update triplets non-parametrically.",
            "model_size": "",
            "reasoning_task_name": "Memorization / knowledge base updates (baseline)",
            "reasoning_task_description": "Non-parametric storage and update of symbolic triplets; querying performed to compare recall/forgetting against fine-tuned LLaMA.",
            "method_or_approach": "Direct insertion/editing of triplets into graph database; retrieval via candidate-tail ranking (comparable evaluation protocol).",
            "performance": "Adding efficiency 100%, Updating efficiency 100%, Forgetting 0%, Time per 1k triplets = 19 seconds (reported).",
            "baseline_comparison": "Used as the primary non-parametric baseline; outperforms LLaMA-7B on all memorization metrics by large margins.",
            "limitations_or_failures": "Not a neural reasoner; only provides exact storage and retrieval (no natural-language reasoning), but demonstrates practical advantages for knowledge updates.",
            "insights_or_conclusions": "Paper uses Neo4j results to argue for hybrid systems (LLM + external KB) for practical knowledge insertion/editing.",
            "uuid": "e8872.4",
            "source_info": {
                "paper_title": "Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Logic-based symbolic baseline (enumeration / AMIE+)",
            "name_full": "Logic-based symbolic methods (path enumeration for deduction/abduction; AMIE+ for induction)",
            "brief_description": "Symbolic reasoning baselines that enumerate activating rules/paths for deduction and abduction, and use AMIE+ for rule mining in induction; these baselines outperform LLMs on the synthetic Symbolic Tree tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Logic-based symbolic methods (enumeration, AMIE+)",
            "model_description": "Deductive/abductive baseline enumerates paths between head and tail and applies activated rules to infer answers; AMIE+ is a rule-mining system that enumerates candidate rules and assigns scalar weights encoding rule quality (used for induction baseline).",
            "model_size": "",
            "reasoning_task_name": "Symbolic Tree (deduction / induction / abduction) baselines",
            "reasoning_task_description": "Exact symbolic reasoning: enumerative proof search for deduction/abduction; rule mining (AMIE+) for induction evaluated against ground-truth rules.",
            "method_or_approach": "Symbolic enumeration of proof paths and activation of formal rules for deduction/abduction; AMIE+ for induction rule discovery and weighting.",
            "performance": "Reported in paper for Symbolic Tree: Deduction 100%, Abduction 100%, Induction 57.1% (AMIE+ result reported).",
            "baseline_comparison": "These symbolic systems substantially outperform LLMs on deduction and abduction (LLMs &lt;100%) and outperform or match LLMs on induction (LLMs only reach ~52.5% induction in best GPT-4 semantics case vs 57.1% for AMIE+).",
            "limitations_or_failures": "Symbolic methods require formal rule representations and are not natural-language models; they are not evaluated for fluency or general natural-language understanding but excel at formal symbolic correctness.",
            "insights_or_conclusions": "Authors use these baselines to demonstrate that pure symbolic systems remain superior on strict symbolic reasoning tasks and that LLMs are not yet capable of fully replacing formal logic engines for such tasks.",
            "uuid": "e8872.5",
            "source_info": {
                "paper_title": "Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GreaseLM - Commonsense baseline",
            "name_full": "Greaselm (Graph Reasoning Enhanced Language Model) evaluated on commonsense QA",
            "brief_description": "GreaseLM, a graph-reasoning enhanced language model, was included as a strong baseline on commonsense benchmarks (CommonsenseQA, OpenBookQA) to contrast LLM commonsense performance with strict symbolic reasoning performance.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GreaseLM",
            "model_description": "A graph-reasoning enhanced language model integrating graph reasoning for question answering; used as baseline on CommonsenseQA and OpenBookQA.",
            "model_size": "",
            "reasoning_task_name": "CommonsenseQA and OpenBookQA (used for comparison)",
            "reasoning_task_description": "Benchmarks for commonsense question answering (multiple-choice) evaluating everyday knowledge and reasoning rather than strictly symbolic logic.",
            "method_or_approach": "Few-shot and zero-shot prompting variants; GreaseLM uses graph reasoning enhancements.",
            "performance": "Reported: GreaseLM CSQA-dev 78.5%, CSQA-test 74.2%, OpenBookQA-Test 84.8%.",
            "baseline_comparison": "GPT-4 and ChatGPT also evaluated: GPT-4 reaches &gt;80% on CSQA and ~91.8% on OpenBookQA; GreaseLM is competitive on commonsense but not directly comparable to symbolic reasoning tasks.",
            "limitations_or_failures": "Commonsense benchmarks do not decouple semantics and thus do not probe formal symbolic reasoning; not intended as a strict-symbolic baseline.",
            "insights_or_conclusions": "Authors use commonsense benchmarks to show LLMs (and graph-enhanced models) can excel on semantics-rich reasoning, highlighting the contrast with failures on decoupled-symbolic tasks.",
            "uuid": "e8872.6",
            "source_info": {
                "paper_title": "Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Proofwriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1
        },
        {
            "paper_title": "Ontology reasoning with deep neural networks",
            "rating": 1
        },
        {
            "paper_title": "Fast rule mining in ontological knowledge bases with AMIE+",
            "rating": 1
        }
    ],
    "cost": 0.0206865,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners</h1>
<p>Xiaojuan Tang ${ }^{1,3}$; Zilong Zheng ${ }^{3 *}$, Jiaqi $\mathrm{Li}^{3}$,<br>Fanxu Meng ${ }^{1,3}$, Song-Chun Zhu ${ }^{1,2,3}$, Yitao Liang ${ }^{1,3}$; Muhan Zhang ${ }^{1,3 \dagger}$<br>${ }^{1}$ Peking University, ${ }^{2}$ Tsinghua University<br>${ }^{3}$ National Key Laboratory of General Artificial Intelligence, BIGAI</p>
<h4>Abstract</h4>
<p>The emergent few-shot reasoning capabilities of Large Language Models (LLMs) have excited the natural language and machine learning community over recent years. Despite of numerous successful applications, the underlying mechanism of such in-context capabilities still remains unclear. In this work, we hypothesize that the learned semantics of language tokens do the most heavy lifting during the reasoning process. Different from human's symbolic reasoning process, the semantic representations of LLMs could create strong connections among tokens, thus composing a superficial logical chain. To test our hypothesis, we decouple semantics from the language reasoning process and evaluate three kinds of reasoning abilities, i.e., deduction, induction and abduction. Our findings reveal that semantics play a vital role in LLMs' in-context reasoning-LLMs perform significantly better when semantics are consistent with commonsense but struggle to solve symbolic or counter-commonsense reasoning tasks by leveraging in-context new knowledge. The surprising observations question whether modern LLMs have mastered the inductive, deductive and abductive reasoning abilities as in human intelligence, and motivate research on unveiling the magic existing within the black-box LLMs. On the whole, our analysis provides a novel perspective on the role of semantics in developing and evaluating language models' reasoning abilities. Our code is available at https://github.com/XiaojuanTang/ICSR.</p>
<h2>1 Introduction</h2>
<p>In recent years, Large Language Models (LLMs) have achieved impressive performance on a variety of natural language tasks, including question answering, text summarization, machine translation, logic reasoning, etc. These successes have been largely attributed to the emergent ability of LLMs to utilize a "zero-shot" or "few-shot" learning approach without any gradient updates-a task description or a few examples are provided to guide their reasoning process [1-4]. One typical example is the "chain-of-thought (CoT)" approach, involving reasoning demonstrations or a simple prompt such as "Let's think step by step" to perform complex reasoning tasks [5, 6].
Despite the powerful and versatile in-context learning ability of LLMs, the underlying mechanisms by which they operate within a given context still remain unclear. Previous works investigate which aspects of the given examples contribute to the final task performance, including ground-truth labels and example ordering [7-9]. Another line of recent work has focused on explaining and leveraging the in-context learning (ICL) mechanism [10-13]. However, the basic problem they have in common is that the in-context prompts they input are based on natural language queries to investigate the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>reasoning abilities of LLMs. However, according to the Dual Process Theory [14, 15], humans usually use symbolic reasoning with System II to solve complex logical reasoning problems. To fill the research gap, we systematically study the in-context reasoning ability of LLMs by decoupling the semantics from the language reasoning process. With extensive experiments, we aim to answer the following research question: Are LLMs good in-context reasoners without semantics?</p>
<p>In this work, we hypothesize that the learned semantics of language tokens play an important role in the reasoning process, creating strong connections among tokens which help to compose a superficial logical chain (shortcut) instead of really performing the formal reasoning process. To test our hypothesis, given symbolic knowledge (facts and rules), we test three kinds of reasoning abilities (i.e., deduction, induction, abduction) on a newly proposed synthetic dataset: Symbolic Tree dataset, which is composed of closed-world, noise-free, multi-hop symbolic reasoning data generated with logical rules. Besides, we also experiment with ProofWriter [16] task, containing questions whose answers require multi-hop reasoning. Our findings suggest that semantics indeed play a vital role in LLMs' in-context reasoning: When semantics are consistent with commonsense, LLMs perform fairly well; when semantics are decoupled or in the counter-commonsense context, LLMs struggle to solve the reasoning tasks by leveraging in-context new knowledge. Moreover, we also study the memorization ability of LLMs to memorize new symbols and semantics information, allowing us to investigate the role of semantics on LLMs' knowledge update ability.</p>
<p>To the best of our knowledge, this is the first study of the effect of semantics on LLMs' in-context reasoning abilities. Our analysis underscores the importance of semantics in LLMs' reasoning ability and questions whether modern LLMs have mastered the formal reasoning abilities as in human intelligence. We hope our findings can provide a novel perspective on the role of semantics in LLMs' in-context abilities, and inspire further research on unveiling the magic inside the black-box LLMs.</p>
<h1>2 Related Works</h1>
<p>Reasoning in LLMs Reasoning is a fundamental cognitive process involving logical inferences and conclusions based on given information. Developing models with strong reasoning capabilities has attracted increasing attention and many researches have been conducted on this topic since early days in the NLP domain [17]. Since then, various benchmarks focusing on different aspects of reasoning have been proposed, including natural language inference (NLI) [18-20], commonsense reasoning [21, 22], multi-hop reasoning [23, 24] etc. In recent years, there has been growing interests in studying the reasoning abilities of LLMs. Researchers have explored various approaches to enable LLMs to perform better on reasoning tasks. For example, "chain-of-thought (CoT)" [5, 25] is proposed to facilitate models to generate a reasoning path that decomposes complex reasoning into multiple easier steps; LLMs are decent zero-shot reasoners by adding a simple prompt, "Let's think step by step", to facilitate step-by-step thinking before giving the final answer [6]. This significantly improves the performance on arithmetic [26], commonsense [21, 27], and symbolic reasoning [5] benchmarks. However, despite their impressive performance on various reasoning benchmarks, all the tasks evaluated are rich in semantics. Thus it is unclear where the reasoning abilities of LLMs come from. This motivates us to investigate LLMs' reasoning abilities when semantics are decoupled.</p>
<p>In-Context Learning LLMs' reasoning abilities are closely related to in-context learning (ICL). ICL refers to the ability of language models to adapt and learn from a few prompt examples during the inference process. In recent years, there has been a focus on exploring how to improve the performance of ICL. Specifically, some works select related demonstrations to the test instance using off-the-shelf unsupervised similarity metrics or train a prompt retriever to select examples [2830]. Others incorporate task instructions or different task prompts [31, 32]. Despite the empirical success, the underlying mechanisms of ICL still remain unclear. A few studies have shown that the performance of ICL usually varies with the choice of in-context demonstrations [8, 33]. Specifically, the order of demonstrations may lead to large performance fluctuations [34, 9]. Recent works also explore the effect of ground-truth labels and question the necessity of ground-truth input-output mapping-using incorrect labels in the examples only marginally lowers the performance [35] and input-label correspondence plays a more important role in contextual demonstration [36]. To further understand why in-context learning works, some work provides theoretical analysis that in-context learning can be formalized as Bayesian inference [13] or some instances of ICL can be understood as implicit implementation of known learning algorithms [37]. However, the existing analyses of ICL are mainly based on natural language input with rich semantic information. We hypothesize that this</p>
<p>might not be able to reflect their true level of reasoning abilities including deduction, induction and abduction. Thus, this paper aims to decouple semantics in LLMs' in-context reasoning abilities.</p>
<p>Symbolic Reasoning Symbolic reasoning has long been studied in the field of artificial intelligence and cognitive science [38-40]. It involves manipulating symbols and applying logical rules to perform deduction [41], induction [39], and abduction [42]. Boole [43] introduced Boolean algebra, which laid the foundation for symbolic logic and provided a formal system for logical reasoning. McCarthy [44] introduced LISP programming language and the concept of symbolic computation, which boosted the development of sophisticated AI programs that could represent and manipulate complex ideas and relationships. Fuhr [45] introduced probabilistic Datalog, an extension of Datalog with probabilities, allowing for probabilistic reasoning in logic-based systems. Eiter et al. [46] introduced answer set programming (ASP), a logic-based programming paradigm that combines logic programming and non-monotonic reasoning. ASP has been used for various reasoning tasks, including planning, knowledge representation, and constraint solving. Yi et al. [47] proposed a neural-symbolic approach to visual question answering. It combines deep neural networks with symbolic rules to perform compositional and interpretable reasoning over visual and textual information. Shin et al. [48] explore using LLM-based models for program synthesis. They present an approach that leverages inferred execution traces to guide the generation of correct programs. Lample and Charton [49] focus on applying LLM-based models to mathematical reasoning, proposing a framework that combines deep learning with symbolic mathematics to perform algebraic reasoning, equation solving, and theorem proving. Pallagani et al. [50] use LLMs for automated planning-a branch of AI concerned with realizing action sequences (plans) to achieve certain goals, typically executed by intelligent agents, autonomous robots, and unmanned vehicles.</p>
<h1>3 Decoupling Semantics from In-Context Reasoning</h1>
<h3>3.1 Task Definitions</h3>
<p>To begin, we first introduce the definitions of reasoning and memorization mechanisms and provide task descriptions for each. Examples of the tasks are shown in Figure 1.</p>
<p>Reasoning In the field of psychology, reasoning refers to the process of using logical operations to draw conclusions or make inferences based on available information [51-54]. As an abstract notion, it encompasses a variety of aspects. Traditionally, we can classify it into three categories:</p>
<ul>
<li>Deductive reasoning is a logical process in which a conclusion can be derived from given premises or principles, meaning predicting new facts based on existing facts and logical rules. For example, given the two facts (Lisa, sisterOf, Alice) and (Alice, motherOf, Bob) along with a logical rule $\forall x, y, z:$ sisterOf $(x, y) \wedge \operatorname{motherOf}(y, z) \rightarrow$ auntOf $(x, z)$, the new fact (Lisa, auntOf, Bob) can be derived through deductive reasoning. The task is to predict the True/False of a predicted fact given facts and rules. The accuracy is the proportion of correct predictions.</li>
<li>Inductive reasoning involves making generalizations based on specific observations or evidence. In other words, a logical rule can be induced from given facts. For instance, given a set of observations that person A is the parent of person B and person B is the child of person A , inductive reasoning is to conclude the logical rule $\forall x, y:$ parentOf $(x, y) \rightarrow$ childOf $(y, x)$. We perform the rule generation task. Given multiple facts with similar patterns and a rule template, the goal is to induce a rule that entails these facts. We test the generated rules against the ground truth rules. If the generated rule matches the ground truth rule exactly, we regard the rule as correct; otherwise, we regard the rule as incorrect. The precision is the proportion of correct rules. More details of the rule template and the ground-truth rules are provided in Appendix F.</li>
<li>Abductive reasoning is a logical process of seeking a hypothesis that best fits or explains a set of observations. For example, given a lot of facts including (Lisa, sisterOf, Alice) and (Alice, motherOf, Bob), along with a set of logical rules including $\forall x, y, z:$ sisterOf $(x, y) \wedge$ motherOf $(y, z) \rightarrow$ auntOf $(x, z)$, if we observe Lisa is Bob's aunt, one possible explanation is that Lisa is Alice's sister and Alice is Bob's mother. We use explanation generation to evaluate the abductive reasoning ability. Given a theory including facts and logical rules, the task is to select specific facts and a logical rule from the given theory to explain the observation. The observation is chosen from inferred facts. We use Proof Accuracy (PA) as an evaluation metric, i.e., the fraction of examples where the generated proof matches exactly any of the gold proofs.</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: center;">Memorization <br> (Depth=0 Reasoning)</th>
<th style="text-align: center;">Deductive Reasoning</th>
<th style="text-align: center;">Inductive Reasoning</th>
<th style="text-align: center;">Abductive Reasoning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Fact1: (Tom, parentOf, Amy) <br> Fact2: (Alice, parentOf, Bob ) <br> Fact3: (Bob, childOf, Alice) <br> Fact4: (Amy, childOf, Tom)</td>
<td style="text-align: center;">Fact1: (Tom, parentOf, Amy) <br> Fact2: (Bob, childOf, Alice) <br> Fact3: (Lisa, sisterOf, Alice) <br> Fact4: (Alice, motherOf, Bob) <br> Rule: $\forall x, y, z:$ sisterOf $(x, y)$ A motherOf $(y, z) \rightarrow$ auntOf $(x, z)$ <br> Q: True or False? <br> (Amy, parentOf, Tom) <br> A: False</td>
<td style="text-align: center;">Fact1: (Tom, parentOf, Amy) <br> Fact2: (Alice, parentOf, Bob ) <br> Fact3: (Bob, childOf, Alice) <br> Fact4: (Amy, childOf, Tom) <br> Q: $\forall x, y: \frac{x}{y}(x, y) \rightarrow$ childOf $(y, x)$ <br> A: $\forall x, y:$ parentOf $(x, y) \rightarrow$ childOf $(y, x)$</td>
<td style="text-align: center;">Fact1: (Lisa, sisterOf, Alice) <br> Fact2: (Alice, motherOf, Bob ) <br> Fact3: (Bob, childOf, Tom) <br> Rule1: $\forall x, y, z:$ sisterOf $(x, y)$ A motherOf $(y, z) \rightarrow$ auntOf $(x, z)$ <br> Rule2: $\forall x, y:$ parentOf $(x, y) \rightarrow$ childOf $(y, x)$</td>
</tr>
<tr>
<td style="text-align: center;">Q: True or False? <br> (Amy, parentOf, Tom) <br> A: False</td>
<td style="text-align: center;">Q: True or False? <br> (Lisa, auntOf, Bob) <br> A: True</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q: Explain <br> (Lisa, auntOf, Bob) <br> A: Fact1, Fact2 $\xrightarrow{\text { Rule1 }}$ <br> (Lisa, auntOf, Bob)</td>
</tr>
</tbody>
</table>
<p>Figure 1: Task Definitions. Memorization: retrieving the predicted fact from in-context knowledge. Deductive: predicting the correctness of the predicted fact given rules and facts. Inductive: generating a rule based on multiple facts with similar patterns. Abductive: explaining the predicted fact based on given rules and facts.</p>
<p>Memorization Memory plays a crucial role in reasoning, as it involves storing the in-context or parametric knowledge necessary for the reasoning process. In some sense, memory can be considered as Depth=0 reasoning, where the question is a known fact. The reasoning task involves retrieving the fact itself from the stored knowledge. However, the specific impact of semantics on memorization has not been extensively explored. Thus, in addition to decoupling semantics from reasoning, we also try to study the impact of semantics on memorization. Specifically, we use a new dataset to fine-tune a language model and test its time, efficiency and forgetting ratio: time is the time cost of adding/updating facts, efficiency is the filter MRR (the mean reciprocal of rank of the correct entity [55]) of the facts added/updated, and forgetting ratio is the filter MRR of the facts that should not be updated. When evaluating whether a fact has been successfully added or updated, we query LLM with a question about the tail entity and rank the probability of the true tail against all entities. The better LLM remembers a triplet, the higher the MRR gets.</p>
<h1>3.2 Evaluation Datasets</h1>
<p>Our goal is to decouple semantics from the in-context reasoning process and solely rely on the given (new) knowledge to perform reasoning tasks. To implement this, we use Symbolic Tree [56] and ProofWriter [16] datasets, which contain both relevant and irrelevant facts and LLMs need to infer the unknown facts after selecting relevant facts from memory.</p>
<p>The Symbolic Tree dataset is an artificially close-world and noise-free symbolic dataset generated with complex logical rules. The dataset consists of randomly sampled "basic facts", which include gender information and "parentOf" relations among individuals. With the given logical rules, the dataset allows for reasoning about 28 different types of family relations, ranging from easy inferences (e.g., fatherhood), to more elaborate ones (e.g., a daughter of someone's cousin). Facts consist of basic facts (in-context knowledge) and inferred facts (what to reason). Note that Symbolic Tree is a close-world dataset, which means that any facts not presented in the dataset are assumed to be false. Thus, we construct the false facts by replacing the head entity or tail entity with a random entity as negative examples in inferred facts. Considering the context window size limitation, we restrict each tree's depth to 5 to generate the dataset. We experiment with 10 sampled Symbolic Trees; each has 30 kinds of relations ( 28 inferred relations, gender and parentOf relation), 26 entities, about 35 basic facts, 300 inferred facts and 300 false ones.</p>
<p>To decouple the semantics within the dataset, we replace the relation names (such as "parent") with hand-crafted symbols (e.g., "r1", "r2", ...), so that LLMs cannot leverage the semantics of the predicates in reasoning but must resort to the given new knowledge (presented as in-context facts and rules). We also experiment with replacing entity names (such as "Alice") with "e1", "e2", ..., but find that it has little impact on performance (more details are provided in Appendix Q). During the symbol generation process, we also try to randomly sample some letters as relation names (e.g., "lnqgv" instead of "r1"), but we observe that LLMs struggle to understand garbled characters, which may negatively affect performance (further discussion is provided in Appendix M).</p>
<p>ProofWriter [16] tasks provide artificial facts and rules expressed in natural language. For our experiments, we use a subset of the ProofWriter Open World Assumption (OWA) dataset with a depth of 1, 2, 3 and 5 (there is no depth 4 task), which contains many small rulebases of facts and rules, expressed in English and do not exist in LLMs' knowledge base. Each rulebase has a set of questions (English statements) that can be proven true, false or "Unknown". Note that if we want to prove something Unknown, it is necessary to enumerate all possible facts and check their true/false. Thus, we remove all the Unknowns and replace the subject and object with entity IDs. This dataset is simpler than Symbolic Tree. Considering most of the predicates in the sentences are unmeaningful verbs like "is" and "can", we only replace the entities with entity IDs to decouple semantics. Take "Anne is kind." as an example. We substitute subject (Anne) and object (kind) with "e1" and "e2", respectively, resulting in "e1 is e2". Figure 2 provides an illustrative example.</p>
<h1>4 Experiment</h1>
<p>Models Selected for Evaluation We primarily evaluate the performance of ChatGPT, GPT4 and LLaMA. ChatGPT and GPT-4 are advanced AI models developed by OpenAI and have demonstrated strong reasoning abilities across various tasks and benchmarks. LLaMA is an open-source large language model developed by Meta AI, with number of parameters ranging from 7B to 65B. Due to computational resource constraints, we could only fine-tune the LLaMA-7B version, which is used in our memorization test. As a reference, when comparing the reasoning abilities of LLMs, we also use some logic-based symbolic methods to conduct experiments as the baseline. To compare memorization, we use a popular graph database Neo4j [57] as the baseline. To ensure a relatively fair comparison, we configure Neo4j with a pre-stored knowledge base that has comparable disk space size to LLaMA. More introduction of Neo4j is represented in Appendix E.
Given a set of rules and facts, you have to reason whether a statement is true or false. Here are some facts and rules:</p>
<p>The bear likes the dog.
The cow is round.
The cow likes the bear.
The cow needs the bear.
The dog needs the squirrel.
The dog sees the cow.
The squirrel needs the dog.
If someone is round then they like the squirrel.
If the bear is round and the bear likes the squirrel then the squirrel needs the bear. If the cow needs the dog then the cow is cold.</p>
<p>Does it imply that the statement "The cow likes the squirrel." is True?</p>
<p>Given a set of rules and facts, you have to reason whether a statement is true or false. Here are some facts and rules:</p>
<p>The $\mathrm{e} 4$ likes the $\mathrm{e} 3$.
The $\mathrm{e} 14$ is $\mathrm{e} 2$.
The e 14 likes the e 4 .
The e 14 needs the e 4 .
The e 1 needs the e 26 .
The e 5 sees the e 14 .
The e 26 needs the e 5 .
If someone is e 1 then they like the e 26 . If the e 4 is e 2 and the e 4 likes the e 26 then the e 26 needs the e 4 .
If the e 14 needs the e 5 then the e 14 is e 1 .</p>
<p>Figure 2: Decoupling semantics from the ProofWriter task. In the original ProofWriter task, entities are represented by their names (left). However, in our decoupled setting, we replace the entity names with unique entity IDs (right).</p>
<p>Evaluation Setup For reasoning, we use Symbolic Tree and ProofWriter Depth-1 as evaluation data. We refer to the raw data, where semantics are retained, as Semantics. When semantics are decoupled using symbols, we refer to it as Symbols. For the Symbolic Tree dataset, we experiment with 10 sampled trees and report the average results, where facts and rules can be represented as logical language and natural language text as the input of LLMs. For example, the fact "motherOf(Alice, Bob)" can be represented as "Alice is Bob's mother"; the rule " $\forall x, y$ : parentOf $(x, y) \rightarrow \operatorname{childOf}(y, x)$ " can be represented as "If x is parent of y , then y is child of x .". Through numerous trials, we find that for the Symbols setting, LLMs tend to perform better when using logic language representations. Conversely, for the Semantics setting, LLMs tend to perform better when using natural language text. We select the representation that yields better performance in LLMs' reasoning. Additional results are presented in Appendix L. We consider zero-shot, zero-shot CoT, few-shot CoT and zero-plus-few-shot-CoT as baselines. To generate explanations for few-shot CoT experiments, for deductive reasoning, we use zero-shot CoT (i.e., Let's think step by step) to generate explanations given the random questions; for abductive reasoning, we randomly select five examples and manually design their demonstrations. We provide all prompts and CoT demonstrations in Appendix A.</p>
<p>For memorization, we randomly selected 1,258 triplets from four sampled Symbolic Trees to finetune the LLaMA. We then select the triplets in the first two trees and update their tail entities. The rest two trees are used to evaluate the forgetting ratio. Note that when updating, we simply flip the tails to random false facts and do not consider the consistency of the knowledge base. More illustrations of consistency are in Appendix H. We still use Symbols and Semantics to denote different experiment settings. Both settings ensure that the new information provided does not overlap with the old knowledge base of LLMs, avoiding any ambiguation problems and eliminating the influence</p>
<p>Table 1: Memorization abilities: LLaMA-7B and Neo4j. MRR are in $\%$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Adding Efficiency (MRR)</th>
<th style="text-align: center;">Updating Efficiency (MRR)</th>
<th style="text-align: center;">Forgetting (MRR $\downarrow$ )</th>
<th style="text-align: center;">Time/1k triplets</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LLaMA-7B</td>
<td style="text-align: center;">Semantics</td>
<td style="text-align: center;">$50.375 \pm 1.27$</td>
<td style="text-align: center;">$51.34 \pm 0.55$</td>
<td style="text-align: center;">$7.02 \pm 1.55$</td>
<td style="text-align: center;">41.5 min</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Symbols</td>
<td style="text-align: center;">$48.91 \pm 4.3$</td>
<td style="text-align: center;">$40.74 \pm 2.26$</td>
<td style="text-align: center;">$2.2 \pm 0.99$</td>
<td style="text-align: center;">41.5 min</td>
</tr>
<tr>
<td style="text-align: center;">Neo4j</td>
<td style="text-align: center;">Semantics</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">19s</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Symbols</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">19s</td>
</tr>
</tbody>
</table>
<p>of pre-existing knowledge on the memorization task. When testing, we follow the prompting of Taori et al. [58], using the head entity and relation as instructions and providing all candidate tails as input. The detailed prompts are contained in Appendix A.</p>
<p>Implementation Details For ChatGPT and GPT-4, we use the chat completion API provided by OpenAI. We use a temperature of zero to generate output. Additionally, we set the frequency penalty to zero and top $p$ to 1 , which are the default values for these APIs.</p>
<p>For LLaMA-7B, we utilized 4 A100 80G GPUs with batch size 64 for finetuning. The training process involved 100 epochs, employing a cosine learning rate schedule with an initial learning rate of $2 \mathrm{e}-5$. We run these experiments three times and recorded their mean MRR and standard deviations. Please refer to Appendix G for more details.</p>
<p>For logic-based symbolic baseline, in the deductive reasoning setting, it enumerates paths between head h and tail t and uses activated rules to infer the answer; For inductive reasoning, we adopt AMIE+ [59], which first enumerates possible rules and then learns a scalar weight for each rule to encode its quality. For abductive reasoning, we locate the logical rule that reason about the relation of the fact and find all paths connecting the head and tail that can activate the rule. These path facts, along with the logical rule, serve as the explanations.</p>
<h1>4.1 Semantics Matter in LLMs' memorizing</h1>
<p>We first test the memorization ability of LLMs when new knowledge are presented in semantics/symbols forms. The results are reported in Table 1.</p>
<p>Results From Table 1, the efficiency of adding and updating semantic knowledge is higher compared to symbolic knowledge. This suggests that semantic knowledge is easier for LLMs to memorize than symbolic knowledge, similar to human's memory capabilities (memorizing symbols is generally more challenging than memorizing words with semantic meanings). However, we also find that the forgetting ratio of the semantic setting is higher than the symbolic setting. This could be attributed to the fact that semantic knowledge has stronger correlation with each other than symbolic knowledge in LLMs, which means that updating some knowledge can have a more significant influence on other knowledge in the system.</p>
<p>In the comparison between LLMs and the graph-based system Neo4j, we can see that knowledge update using Neo4j achieves $100 \%$ accuracy when inserting new triplets or editing existing triplets, regardless of whether the knowledge is symbolic or semantic. As expected, since the added or updated knowledge does not overlap with the existing knowledge base, there is no further influence on the existing knowledge in the database. This highlights one advantage of using a non-parametric knowledge base. Additionally, compared to the computational cost of fine-tuning LLMs, updating knowledge in a graph database with optimized storage mechanisms is significantly faster. This inspires that combining LLMs with a non-parametric knowledge base can offer a more practical and efficient approach for real-world applications.</p>
<h3>4.2 Semantics Play a Vital Role in LLMs' Reasoning</h3>
<p>In this section, we evaluate the impact of decoupling semantics from LLMs' in-context reasoning. In Table 2, we present the results of deductive, inductive, and abductive reasoning tasks on the Symbolic Tree datasets.</p>
<p>Results From Table 2, we observe that in both deductive and inductive reasoning scenarios, Semantics setting significantly outperforms Symbols setting. Notably, in the inductive experiments, Semantics</p>
<p>Table 2: The reasoning results of Symbolic Tree. Results are in $\%$.</p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Model</th>
<th>Baseline</th>
<th>deduction</th>
<th>induction</th>
<th>abduction</th>
</tr>
</thead>
<tbody>
<tr>
<td>Symbols</td>
<td>ChatGPT</td>
<td>Zero-Shot</td>
<td>52.6</td>
<td>6.10</td>
<td>1.50</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Zero-Shot-CoT</td>
<td>55.7</td>
<td>7.86</td>
<td>4.90</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Few-Shot-CoT</td>
<td>54.8</td>
<td>-</td>
<td>18.2</td>
</tr>
<tr>
<td></td>
<td>GPT-4</td>
<td>Zero-Plus-Few-Shot-CoT</td>
<td>55.7</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Zero-Shot</td>
<td>68.8</td>
<td>9.28</td>
<td>25.0</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Zero-Shot-CoT</td>
<td>71.1</td>
<td>8.93</td>
<td>31.2</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Few-Shot-CoT</td>
<td>67.6</td>
<td>-</td>
<td>44.2</td>
</tr>
<tr>
<td>Semantics</td>
<td>ChatGPT</td>
<td>Zero-Shot</td>
<td>66.1</td>
<td>36.4</td>
<td>2.94</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Zero-Shot-CoT</td>
<td>65.5</td>
<td>32.2</td>
<td>3.40</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Few-Shot-CoT</td>
<td>67.1</td>
<td>-</td>
<td>21.8</td>
</tr>
<tr>
<td></td>
<td>GPT-4</td>
<td>Zero-Plus-Few-Shot-CoT</td>
<td>67.2</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Zero-Shot</td>
<td>79.2</td>
<td>52.5</td>
<td>27.3</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Zero-Shot-CoT</td>
<td>86.2</td>
<td>53.9</td>
<td>33.4</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Few-Shot-CoT</td>
<td>91.1</td>
<td>-</td>
<td>69.2</td>
</tr>
<tr>
<td>Random</td>
<td></td>
<td>-</td>
<td>50.1</td>
<td>3.57</td>
<td>-</td>
</tr>
<tr>
<td>Logic-based</td>
<td></td>
<td>-</td>
<td>100</td>
<td>57.1</td>
<td>100</td>
</tr>
</tbody>
</table>
<p>Table 3: The deduction results of ProofWriter tasks (ChatGPT). Results are in $\%$.</p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Baseline</th>
<th>depth-1</th>
<th>depth-2</th>
<th>depth-3</th>
<th>depth-5</th>
</tr>
</thead>
<tbody>
<tr>
<td>Symbols</td>
<td>Zero-Shot</td>
<td>69.1</td>
<td>62.3</td>
<td>59.4</td>
<td>52.8</td>
</tr>
<tr>
<td></td>
<td>Zero-Shot-CoT</td>
<td>56.2</td>
<td>49.4</td>
<td>45.2</td>
<td>38.6</td>
</tr>
<tr>
<td></td>
<td>Few-Shot-CoT</td>
<td>65.8</td>
<td>58.1</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Semantics</td>
<td>Zero-Shot</td>
<td>69.0</td>
<td>63.5</td>
<td>60.3</td>
<td>51.4</td>
</tr>
<tr>
<td></td>
<td>Zero-Shot-CoT</td>
<td>51.5</td>
<td>45.8</td>
<td>40.3</td>
<td>30.9</td>
</tr>
<tr>
<td></td>
<td>Few-Shot-CoT</td>
<td>62.5</td>
<td>56.7</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>achieves approximately $30 \%$ higher absolute accuracy compared to Symbols setting. This indicates that preserving rich semantics in the reasoning process leads to better performance for LLMs.</p>
<p>Although in the abductive reasoning scenario, Symbols setting achieves comparable results to Semantics setting, it is important to note that the longer in-context knowledge in abductive reasoning tasks can increase the difficulty of selecting relevant information and reasoning from memory. To further investigate this, additional experiments on a smaller Symbolic Tree dataset are conducted, and the results in Appendix O confirm that the Semantics setting still outperforms the Symbols setting. This reinforces the hypothesis that preserving semantics enhances the reasoning capabilities of LLMs.</p>
<p>Despite the improved in-context reasoning performance of LLMs with rich semantics, when compared to logic-based symbolic methods, LLMs still exhibit inferior performance in all reasoning tasks. This suggests that while LLMs possess a broad knowledge base and strong language understanding, symbolic reasoning is not their primary strength compared to methods specifically designed for symbolic reasoning. This also suggests the potential of future neural-symbolic AI systems.</p>
<h1>4.3 More Fine-grained Analysis about Semantics</h1>
<p>The aforementioned experiments offer initial evidence highlighting the significance of semantics in the reasoning of LLMs. To further investigate this observation, we examine the influence of commonsense knowledge stored within LLMs on their semantic reasoning performance. Specifically, we explore three aspects: First, we investigate the impact of removing given logical rules (in deduction) and facts (in induction), where LLMs have to rely solely on the prior commonsense knowledge stored within the parameters to infer the answers. This analysis allows us to assess the extent to which LLMs can leverage their internal knowledge to reason effectively without explicit in-context knowledge. Second, we retain the semantics of the datasets but introduce counter-commonsense logical rules. This requires LLMs to leverage in-context new knowledge and navigate the reasoning process by</p>
<p>strictly adhering to the new information conflicting with the old knowledge. We implement it by shuffling relations as new relation labels to construct a new counter-commonsense dataset. For instance, we replace "motherOf" with "sisterOf", "parentOf" with "brotherOf", and "female" with "male". Consequently, for a rule such as $\forall x, y:$ parentOf $(x, y) \wedge$ female $(x) \rightarrow$ motherOf $(x, y)$, we obtain $\forall x, y:$ brotherOf $(x, y) \wedge \operatorname{male}(x) \rightarrow \operatorname{sisterOf}(x, y)$. Thirdly, we use a subset of the ProofWriter OWA datasets for depths $0,1,2,3$ and 5 , which contains synthetic facts and rules despite written in natural language but irrelevant to commonsense (see Figure 2). These investigations allow us to gain deeper insights into the effect of semantics on the reasoning capabilities of LLMs.</p>
<p>When semantics are consistent with commonsense As shown in Table 4, in the deductive reasoning experiment, Removing rules/facts achieves comparable results to Semantics; in the inductive reasoning experiment, Removing rules/facts outperforms Symbols, achieving 35.7\% in GPT-4. These findings suggest that LLMs can perform deductive reasoning comparably by leveraging their stored commonsense knowledge without using the provided semantic knowledge, and providing symbolic instead of semantic knowledge in induction might even hurt the performance. Besides, GPT-4 significantly outperforms ChatGPT across all evaluation settings, suggesting that the commonsense knowledge stored within GPT-4 is more extensive and allows stronger reasoning capabilities.</p>
<p>When semantics are not consistent with commonsense To investigate the impact of semantics that are not consistent with commonsense, we introduce counter-commonsense (Counter-CS) scenarios, which is also shown in table 4. In comparison to Semantics and Symbols, we find that Counter-Commonsense performs worse than Semantics, even Symbols.</p>
<p>Table 4: Semantics, removing rules/facts and countercommonsense reasoning experiments (ChatGPT and GPT-4). Results are in \%.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">deductive (Few-Shot-CoT)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">inductive (Zero-Shot-CoT)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">GPT-4</td>
</tr>
<tr>
<td style="text-align: left;">Semantics</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">53.6</td>
</tr>
<tr>
<td style="text-align: left;">Symbols</td>
<td style="text-align: center;">53.7</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">7.14</td>
<td style="text-align: center;">21.4</td>
</tr>
<tr>
<td style="text-align: left;">Remove R/F</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">7.14</td>
<td style="text-align: center;">35.7</td>
</tr>
<tr>
<td style="text-align: left;">Counter-CS</td>
<td style="text-align: center;">48.9</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">7.14</td>
<td style="text-align: center;">17.8</td>
</tr>
</tbody>
</table>
<p>These findings suggest that when the in-context new knowledge conflicts with commonsense, LLMs struggle to accurately reason and predict.</p>
<p>When semantics are irrelevant to commonsense We use the ProofWriter tasks to test whether unmeaningful semantics are still useful. The results are shown in table 3. The Symbols setting performs comparably to the Semantics setting in the zero-shot setting, suggesting that when semantics are irrelevant to commonsense, they have little effect on the reasoning abilities of LLMs. In other words, when the task does not require deep semantic understanding or relies minimally on commonsense knowledge, the presence or absence of semantics does not significantly impact the performance of LLMs. However, in the CoT settings, we observe that Semantics is significantly worse than Symbols. This might be because step-by-step reasoning magnifies the disturbing effect brought by weird semantics such as "The squirrel needs the dog". Additionally, we observe that the CoT settings even perform worse than the zero-shot setting, with a higher frequency of the answer "Cannot be determined.". Similar phenomenons are also observed in table 2, indicating that CoT may not be always helpful for reasoning tasks with in-context new knowledge.</p>
<h1>4.4 More analysis and discussions</h1>
<p>(1) Deduction outperforms other reasoning types: Across both the Symbols and Semantics settings, LLMs demonstrate better performance in deduction compared to induction and abduction. This observation may be attributed to the transformer-decoder architecture used by the GPT series, which predicts the next token based solely on the preceding tokens. This architecture aligns well with the deductive reasoning process, where conclusions are derived from given premises or principles. Intuitively, induction/abduction is also harder than deduction for human.
(2) Shorter in-context knowledge enhances reasoning performance To examine the influence of context length on reasoning, we conducted an abductive reasoning experiment using a smaller Symbolic Tree, containing approximately 12 entities and 100 facts. The results, provided in Appendix O, show that abductive reasoning with a shorter context leads to better performance compared to a longer context. Besides, we also conduct deduction and induction experiments where LLMs are directly provided with the relevant facts related to the predicted fact or the predicted rule. The results are presented in Appendix J. This finding suggests that LLMs struggle with processing excessively long in-context information, particularly in reasoning tasks. The length of the context influences</p>
<p>reasoning performance, as shorter contexts make it easier to select relevant and useful information while minimizing the impact of unrelated content.
(3) Effectiveness of commonsense expressed in natural language: We explore the representation of knowledge in natural language and logic language forms in our experiments. The results, presented in Appendix L, indicate that for tasks involving semantics, natural language descriptions are more effective than logical language representations. Conversely, for symbolic and counter-commonsense tasks, logic language performs better. This observation suggests that natural language representations better stimulate the semantic understanding capabilities of LLMs, while logical language representations are more conducive to symbolic reasoning.
(4) Zero-shot capabilities are approaching zero-shot-CoT capabilities: In Symbols setting, comparing zero-shot with zero-shot-CoT across deduction, induction, and abduction evaluations, we observe that zero-shot-CoT only marginally improves the performance compared to zero-shot learning. This finding suggests that the zero-shot capabilities of current LLMs are approaching their zero-shot-CoT learning abilities. One plausible explanation is that ChatGPT has already been trained on similar tasks with CoT and has memorized the instructions. Consequently, it implicitly follows these instructions when applied to the same queries, even without explicit CoT guidance [60].
(5) Zero-shot as System-I, Zero-shot-CoT as System-II For Symbols setting, comparing zero-shot with zero-shot-CoT across deduction, induction, abduction, we observe that zero-shot-CoT improves the performance compared to zero-shot learning. In contrast, for Semantics setting, zero-shot-CoT underperforms zero-shot. This distinction can be attributed to the impressive zero-shot capabilities of LLMs in various System-I tasks, highlighted by Radford et al. [61]. Tasks that demand a profound comprehension of rich semantic information lead LLMs to rely on intuition and employ shortcuts to predict answers. On the other hand, for symbolic reasoning tasks, zero-shot-CoT learning outperforms zero-shot learning, aligning with the findings of Kojima et al. [6] who demonstrated the facilitative role of CoT in activating System-II abilities.
(5) Utilizing internal knowledge outperforms external in-context knowledge: To explore the ability of LLMs to utilize internal and external knowledge, we conduct an additional experiment where we provide LLMs with only the relevant facts related to the predicted fact. We compare the performance of Removing rules (leveraging internal knowledge) with Semantics (providing external logical rules). Surprisingly, we find that Removing rules performed better than Semantics. This suggests that LLMs possess the necessary internal knowledge to support answering questions and reasoning tasks, and leveraging this internal knowledge is more effective for reasoning than relying on external logical rules. Detailed results and case studies can be found in Appendix J.1.</p>
<h1>4.5 Commonsense Reasoning</h1>
<p>Commonsense reasoning refers to the ability to make judgments and draw conclusions based on understanding basic world knowledge and everyday experiences. For example, "water is wet and can put out a fire" is an example of commonsense. By considering the properties of water and its ability to put out flames, we can deduce that "pouring water on a kitchen fire" would be an effective solution. This section explores the commonsense reasoning abilities of LLMs. Although commonsense reasoning cannot decouple semantics, we include the comparisons for a thorough evaluation of LLMs' reasoning ability.
Evaluation Setup We use CommonsenseQA and OpenBookQA [62] to test commonsense reasoning. We use ChatGPT, GPT-4 and GreaseLM [63] as baselines. More experimental setup is in Appendix P.
Results As shown in Table 5, GreaseLM demonstrates remarkable performance in commonsense reasoning tasks, comparable to human performance of $88 \%$ as indicated by a public leaderboard. In addition, we conduct experiments employing different few-shot prompts to further investigate its capabilities. The results indicate that few-shot prompts only yield marginal improvements compared to zero-shot learning. This phenomenon can be attributed to the Robust Human Language Filter (RHLF) of ChatGPT, which empowers the models to better understand the meaning and context of tasks, enabling them to perform well even in zero-shot settings.
However, when we examine the few-shot (predict-explain or explain-predict) results of CSQA, which involves CoT explanations, the performance falls short. This is probably because the questions of CSQA rely more on intuition (System-I) but less on explicit reasoning (System-II). GPT-4 exhibits</p>
<p>Table 5: The commonsense reasoning results of CSQA and OpenBookQA. Results are in $\%$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Baseline</th>
<th style="text-align: center;">CSQA-Dev</th>
<th style="text-align: center;">CSQA-Test</th>
<th style="text-align: center;">OpenBookQA-Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GreaseLM</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">84.8</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">Zero-Shot</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">67.8</td>
<td style="text-align: center;">70.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-Shot (predict-explain)</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">74.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-Shot (explain-predict)</td>
<td style="text-align: center;">46.4</td>
<td style="text-align: center;">44.3</td>
<td style="text-align: center;">70</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-Shot (3)</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">75</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-Shot (8)</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">74</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-Shot (16)</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">73.8</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">Zero-Shot</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">91.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-Shot</td>
<td style="text-align: center;">85.3</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">86.2</td>
</tr>
</tbody>
</table>
<p>significant improvements in its reasoning ability on both datasets. It achieves a success accuracy of over $80 \%$ on CSQA and approaches an impressive $90 \%$ on OpenBookQA. These results demonstrate the potential for further optimization of LLMs in commonsense reasoning through training on higher-quality corpora, using larger models, and using more iterations of training.</p>
<h1>5 Conclusion and Discussion</h1>
<p>Our paper presents the first comprehensive investigation of the role of semantics in LLMs' in-context reasoning abilities by decoupling semantics from in-context prompts. Experimental results suggest that: When semantics are consistent with commonsense, LLMs perform fairly well; when semantics are decoupled or counter-commonsense, LLMs struggle to solve the reasoning tasks by leveraging in-context new knowledge. These findings reveal the importance of semantics in LLMs' reasoning abilities and inspire further research on unveiling the magic existing within the black-box LLMs. In light of the findings identified in our analysis, we point out several potential future directions for the development of large foundation models:</p>
<p>More complex symbolic reasoning benchmark: To improve LLMs' in-context symbolic reasoning abilities, developing new datasets with decoupled semantics and more complex reasoning tasks is necessary. These benchmarks should challenge LLMs with diverse and intricate symbolic knowledge.
Combination with external non-parametric knowledge base: As our experimental results show, the memorization abilities of LLMs are not comparable to existing graph-based methods. This motivates integrating LLMs with external non-parametric knowledge bases, such as graph databases, to enhance their knowledge insertion and updating. This hybrid approach can leverage the strengths of LLMs' language understanding and the comprehensive, accurate and up-to-date knowledge stored in non-parametric sources.</p>
<p>Improving the ability of processing in-context knowledge: More robust and strong abilities to process and memorize in-context knowledge is crucial to perform complex in-context reasoning tasks. Further research is needed to improve LLMs' capabilities in processing and leveraging in-context knowledge. This includes developing mechanisms to better encode and retrieve relevant information from the in-context knowledge, in order to enable more effective reasoning.</p>
<h1>References</h1>
<p>[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[2] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493, 2022.
[3] Rishi Bommasani, Drew A Hudson, Ebsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.
[4] Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Pretrained transformers as universal computation engines. arXiv preprint arXiv:2103.05247, 1, 2021.
[5] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.
[6] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.
[7] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022.
[8] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804, 2021.
[9] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pages 12697-12706. PMLR, 2021.
[10] Timo Schick and Hinrich Schütze. Exploiting cloze questions for few shot text classification and natural language inference. arXiv preprint arXiv:2001.07676, 2020.
[11] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021.
[12] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022.
[13] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.
[14] Steven A Sloman. The empirical case for two systems of reasoning. Psychological bulletin, 119(1):3, 1996.
[15] Thomas Gilovich, Dale Griffin, and Daniel Kahneman. Heuristics and biases: The psychology of intuitive judgment. Cambridge university press, 2002.
[16] Oyvind Tafjord, Bhavana Dalvi Mishra, and Peter Clark. Proofwriter: Generating implications, proofs, and abductive statements over natural language. arXiv preprint arXiv:2012.13048, 2020.
[17] Robin Cooper. The fracas consortium robin cooper, dick crouch, jan van eijck, chris fox, josef van genabith, jan jaspars, hans kamp, david milward, manfred pinkal, massimo poesio, steve pulman. 1996.
[18] SamuelR. Bowman, Gabor Angeli, Christopher Potts, and ChristopherD. Manning. A large annotated corpus for learning natural language inference, Aug 2015.
[19] Adina Williams, Nikita Nangia, and SamuelR. Bowman. A broad-coverage challenge corpus for sentence understanding through inference, Apr 2017.
[20] Jean-Philippe Bernardy and Stergios Chatzikyriakidis. Improving the precision of natural textual entailment problem datasets., May 2020.
[21] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge, Nov 2018.</p>
<p>[22] Shourya Aggarwal, Divyanshu Mandowara, Vishwajeet Agrawal, Dinesh Khandelwal, Parag Singla, and Dinesh Garg. Explanations for commonsenseqa: New dataset and models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Jul 2021. doi: 10.18653/v1/2021. acl-long.238. URL http://dx.doi.org/10.18653/v1/2021.acl-long. 238.
[23] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs, Mar 2019.
[24] Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and WilliamYang Wang. Hybridqa: A dataset of multi-hop question answering over tabular and textual data, Apr 2020.
[25] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.
[26] Subhro Roy and Dan Roth. Solving general arithmetic word problems. arXiv preprint arXiv:1608.01413, 2016.
[27] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346-361, 2021.
[28] Rajarshi Das, Manzil Zaheer, Dung Thai, Ameya Godbole, Ethan Perez, Jay Yoon Lee, Lizhen Tan, Lazaros Polymenakos, and Andrew McCallum. Case-based reasoning for natural language queries over knowledge bases. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9594-9611, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.755. URL https://aclanthology. org/2021.emnlp-main. 755.
[29] Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning. arXiv preprint arXiv:2112.08633, 2021.
[30] Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, and Tao Yu. Selective annotation makes language models better few-shot learners, 2022.
[31] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization, 2022.
[32] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners, 2022.
[33] Yao Lu, Max Bartolo, AlastairP. Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity, Apr 2021.
[34] Albert Webson and Ellie Pavlick. Do prompt-based models really understand the meaning of their prompts? In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2300-2344, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.167. URL https://aclanthology.org/2022.naacl-main. 167.
[35] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work?
[36] Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee, Kang Min Yoo, and Taeuk Kim. Ground-truth labels matter: A deeper look into input-label demonstrations. arXiv preprint arXiv:2205.12685, 2022.
[37] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models, Nov 2022.
[38] John McCarthy and Patrick J Hayes. Some philosophical problems from the standpoint of artificial intelligence. In Readings in artificial intelligence, pages 431-450. Elsevier, 1981.</p>
<p>[39] Nada Lavrac and Saso Dzeroski. Inductive logic programming. In WLP, pages 146-160. Springer, 1994.
[40] Allen Newell and Herbert A Simon. Computer science as empirical inquiry: Symbols and search. In ACM Turing award lectures, page 1975. 2007.
[41] Philip N Johnson-Laird. Deductive reasoning. Annual review of psychology, 50(1):109-135, 1999.
[42] Gyöngyi Kovács and Karen M Spens. Abductive reasoning in logistics research. International journal of physical distribution \&amp; logistics management, 2005.
[43] George Boole. The mathematical analysis of logic. Philosophical Library, 1847.
[44] John McCarthy. Recursive functions of symbolic expressions and their computation by machine, part i. Communications of the ACM, 3(4):184-195, 1960.
[45] Norbert Fuhr. Probabilistic datalog: Implementing logical information retrieval for advanced applications. Journal of the American Society for Information Science, 51(2):95-110, 2000.
[46] Thomas Eiter, Giovambattista Ianni, and Thomas Krennwallner. Answer set programming: A primer. Springer, 2009.
[47] Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Josh Tenenbaum. Neuralsymbolic vqa: Disentangling reasoning from vision and language understanding. Advances in neural information processing systems, 31, 2018.
[48] Eui Chul Shin, Illia Polosukhin, and Dawn Song. Improving neural program synthesis with inferred execution traces. Advances in Neural Information Processing Systems, 31, 2018.
[49] Guillaume Lample and François Charton. Deep learning for symbolic mathematics. arXiv preprint arXiv:1912.01412, 2019.
[50] Vishal Pallagani, Bharath Muppasani, Keerthiram Murugesan, Francesca Rossi, Lior Horesh, Biplav Srivastava, Francesco Fabiano, and Andrea Loreggia. Plansformer: Generating symbolic plans using transformers. arXiv preprint arXiv:2212.08681, 2022.
[51] Peter Cathcart Wason and Philip Nicholas Johnson-Laird. Psychology of reasoning: Structure and content, volume 86. Harvard University Press, 1972.
[52] P. C. Wason. Reasoning about a rule. Quarterly Journal of Experimental Psychology, page 273-281, Jul 2007. doi: 10.1080/14640746808400161. URL http://dx.doi.org/10.1080/14640746808400161.
[53] Ronald Fagin, Joseph Y Halpern, Yoram Moses, and Moshe Vardi. Reasoning about knowledge. MIT press, 2004.
[54] Douglas N Walton. What is reasoning? what is an argument? The journal of Philosophy, 87(8):399-419, 1990 .
[55] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. Advances in neural information processing systems, 26, 2013.
[56] Patrick Hohenecker and Thomas Lukasiewicz. Ontology reasoning with deep neural networks. Journal of Artificial Intelligence Research, 68:503-540, 2020.
[57] Massimo Carro. Nosql databases. arXiv preprint arXiv:1401.2101, 2014.
[58] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github. com/tatsu-lab/stanford_alpaca, 2023.
[59] Luis Galárraga, Christina Teflioudi, Katja Hose, and Fabian M Suchanek. Fast rule mining in ontological knowledge bases with amie
$+$
+. The VLDB Journal, 24(6):707-730, 2015.
[60] Jiuhai Chen, Lichang Chen, Heng Huang, and Tianyi Zhou. When do you need chain-of-thought prompting for chatgpt? arXiv preprint arXiv:2304.03262, 2023.
[61] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.</p>
<p>[62] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.
[63] Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang, Christopher D Manning, and Jure Leskovec. Greaselm: Graph reasoning enhanced language models for question answering. arXiv preprint arXiv:2201.08860, 2022.</p>
<h1>Table of Contents</h1>
<p>A Prompts ..... 16
A. 1 Deductive reasoning ..... 16
A.1.1 Zero-Shot ..... 16
A.1.2 Zero-Shot-CoT ..... 16
A.1.3 Few-Shot-CoT ..... 17
A.1.4 Examples of Few-Shot-CoT Demonstrations ..... 17
A.1.5 Zero-Shot of removing rules setting ..... 17
A. 2 Inductive reasoning ..... 18
A.2.1 Zero-Shot ..... 18
A.2.2 Zero-Shot CoT ..... 18
A.2.3 Zero-Shot of removing facts setting ..... 18
A. 3 Abductive reasoning ..... 18
A.3.1 Zero-Shot ..... 18
A.3.2 Zero-Shot-CoT ..... 19
A.3.3 Few-Shot-CoT prompt ..... 19
A.3.4 Examples of Few-Shot-CoT Demonstrations ..... 19
A.3.5 Few-Shot CoT of removing rules setting ..... 20
A. 4 Commonsense reasoning ..... 21
A.4.1 Zero-Shot prompt ..... 21
A.4.2 Few-Shot prompt ..... 21
A.4.3 Examples of Few-Shot Demonstrations ..... 21
A. 5 LLaMA Fine-tuning Prompt ..... 21
B Deduction examples of Symbolic Tree datasets ..... 21
B. 1 Semantics ..... 22
B.1.1 Logic language representations ..... 22
B.1.2 Natural language representations ..... 23
B. 2 Symbolization ..... 25
B.2.1 Logic language representations ..... 25
B.2.2 Natural language representations: ..... 27
B. 3 Semantics of removing rule setting ..... 28
C Examples of ProofWriter ..... 29
C. 1 Semantics ..... 29
C. 2 Symbols ..... 30
D Different Zero-Shot prompting ..... 30
E Introduction of Neo4j ..... 31</p>
<p>F Task definitions ..... 31
G Implementation of memorization ..... 32
H Consistency of knowledge base ..... 32
I Different Few-Shot-CoT examples ..... 33
I. 1 Few-Shot-CoT examples with Auto-Filter ..... 33
I. 2 Few-Shot-CoT examples with Manually-Designed ..... 34
J Reasoning results of after human selection ..... 35
J. 1 Comparison of Removing rules and Semantics ..... 35
J. 2 Reasoning results after human selection ..... 36
K Reasoning with natural language ..... 36
L Reasoning results of two representations ..... 36
M Reasoning with garbled symbols ..... 37
N More reasoning results of Symbolic Tree ..... 38
O Abductive reasoning on smaller datasets ..... 39
P More details on the experimental setup of commonsense reasoning ..... 39
Q Replacing entity labels ..... 40
R Multi-short rules ..... 40
A Prompts
A. 1 Deductive reasoning
A.1.1 Zero-Shot
system: You are a helpful assistant with deductive reasoning abilities.
user: I will provide a set of logical rules L1 to L(number of rules) and facts F1 to F(number
of basic facts). Please select one single logical rule from L1 to L(number of rules) and
a few facts from F1 to F(number of basic facts) to predict True/False of the unknown fact
using deductive reasoning.
Logical rules: {logical rules}
Facts: {basic facts}
Unknown fact: {statement}
The answer (True or False) is:
A.1.2 Zero-Shot-CoT
system: You are a helpful assistant with deductive reasoning abilities. Please select one
single logical rule and a few facts to predict True/False of the following statement.
user: I will provide a set of logical rules L1 to L(number of rules) and facts F1 to F(number
of basic facts). Please select one single logical rule from L1 to L(number of rules) and
a few facts from F1 to F(number of basic facts) to predict True/False of the following
statement using deductive reasoning.
Logical rules: {logical rules}</p>
<div class="codehilite"><pre><span></span><code>Facts: {basic facts}
Statement: {statement}
Answer with True or False? Let&#39;s think step by step.
</code></pre></div>

<h1>A.1.3 Few-Shot-CoT</h1>
<p>system: You are a helpful assistant with deductive reasoning abilities. Please select one single logical rule and a few facts to predict True/False of the following statement. user: I will provide a set of logical rules LI to L{number of rules} and facts F1 to F{number of basic facts}.
Logical rules: {logical rules}
Facts: {basic facts}
Please select one single logical rule from LI to L{number of rules} and a few facts from F1 to F{number of basic facts} to predict True/False of the following statement using deductive reasoning.
{Demonstrations}
Statement: {statement}
Answer:</p>
<h2>A.1.4 Examples of Few-Shot-CoT Demonstrations</h2>
<h2>Statement: $r 14($ Amelie, Jonathan)</h2>
<p>Answer: We can use L11: $\forall A, B, C, D, E: r 3(B, A) \wedge r 3(B, C) \wedge r 3(C, D) \wedge r 3(D, E) \wedge r 2(A) \rightarrow$ $r 14(A, E)$ to make a deduction. From the facts, we know that $r 3$ (Amelie, Thomas) and $r 3$ (Thomas, Jonathan). Therefore, we can apply L11 with $\mathrm{A}=$ Amelie, $\mathrm{B}=$ Thomas, $\mathrm{C}=$ Jonathan, D and E as variables. This gives us: $r 3$ (Thomas, Jonathan) $\wedge r 3$ (Amelie, Thomas) $\wedge r 3$ (Thomas, D) $\wedge r 3(\mathrm{D}, \mathrm{E})$ $\wedge r 2$ (Amelie) $\rightarrow r 14$ (Amelie, E) Since we know that $r 2$ (Amelie) is true from F7, we can simplify the above statement to: $r 3$ (Thomas, Jonathan) $\wedge r 3$ (Amelie, Thomas) $\wedge r 3$ (Thomas, D) $\wedge r 3(\mathrm{D}, \mathrm{E})$ $\rightarrow r 14$ (Amelie, E) Now, we can substitute the facts $r 3$ (Amelie, Thomas) and $r 3$ (Thomas, Jonathan) to get: $r 3$ (Thomas, Jonathan) $\wedge r 3$ (Thomas, D) $\wedge r 3(\mathrm{D}, \mathrm{E}) \rightarrow r 14$ (Amelie, E) We can see that this statement is true if we choose $\mathrm{D}=$ Alina and $\mathrm{E}=$ Jonathan, since we know that $r 3$ (Thomas, Alina) from F50 and $r 3$ (Alina, Jonathan) from F56. Therefore, the statement $r 14$ (Amelie, Jonathan) is true. Answer: True.
Statement: $r 31$ (Philipp, Nina)
Answer: Let's use L28: $\forall A, B, C, D: r 3(B, A) \wedge r 3(C, B) \wedge r 3(C, D) \wedge r 1(A) \rightarrow r 31(A, D)$ to see if we can prove the statement $r 31$ (Philipp, Nina). From the facts, we know that $r 3$ (Luisa, Nina) (F52) and $r 3$ (Emilia, Philipp) (F62). We can use L28 with $A=$ Philipp, $B=$ Emilia, $C=$ Luisa, and $D=$ Nina to get: $r 3$ (Emilia, Philipp) $\wedge r 3$ (Luisa, Emilia) $\wedge r 3$ (Luisa, Nina) $\wedge r 1$ (Philipp) $\rightarrow$ $r 31$ (Philipp, Nina) Since all the conditions are met, we can conclude that $r 31$ (Philipp, Nina) is True. Therefore, the answer is True.
Statement: $r 8$ (Leonie, Nico)
Answer: We can use L5 to make a deduction about the statement $r 8$ (Leonie, Nico). L5 states that if there exist three individuals $\mathrm{A}, \mathrm{B}$, and C such that A is related to $\mathrm{B}, \mathrm{B}$ is related to C , and A has a certain property (r2), then A is related to C in a certain way (r8). Using this rule, we can see that we have the following facts: - $r 3$ (Leonie, Emily)</p>
<ul>
<li>$r 3$ (Emily, Nico)</li>
</ul>
<p>Therefore, we can conclude that $r 8$ (Leonie, Nico) is true.
Statement: $r 7$ (Patrick, Alina)
Answer: We can use L4: $\forall A, B: r 3(A, B) \wedge r 1(A) \rightarrow r 7(A, B)$ to determine if $r 7$ (Patrick, Alina) is true or false. From the facts, we know that $r 3$ (Patrick, Alina) is true (F55). We also know that $r 1$ (Patrick) is true (F9). Therefore, we can apply L4 to conclude that $r 7$ (Patrick, Alina) is true. Answer: True.
Statement: $r 27$ (Jonathan, Leonie)
Answer: Let's use L4: $\forall A, B: r 3(A, B) \wedge r 1(A) \rightarrow r 7(A, B)$ and F56: $r 3$ (Patrick, Jonathan) to infer that $r 7$ (Patrick, Jonathan) is true. Then, we can use L24: $\forall A, B, C: r 3(B, A) \wedge r 3(C, B) \wedge r 1(A) \rightarrow$ $r 27(A, C)$ and F44: $r 3$ (Leonie, Emily) to infer that $r 27$ (Jonathan, Leonie) is false, since there is no fact that supports $r 3$ (Jonathan, Emily). Therefore, the answer is False.</p>
<h2>A.1.5 Zero-Shot of removing rules setting</h2>
<p>system: Please answer the question only with True or False.
user: I will provide a set of facts. Please predict True/False of the unknown fact based on given facts.
Facts: {facts}
Unknown fact: {statement}
The answer {True or False} is:</p>
<h1>A. 2 Inductive reasoning</h1>
<h2>A.2.1 Zero-Shot</h2>
<p>system: You are a helpful assistant with inductive reasoning abilities. Please generate one single rule to match the template and logically entail the facts. Note that the symbol ' $#$ ' in the template should be filled with either 'r1' or 'r45', while the symbol '++' should be filled with either 'r43' or 'r44'.
user: I will give you a set of facts F1 to F (number of basic facts), facts G1 to G (number of inferred fact} and a template for a logical rule. Please generate one single rule to match the template and logically entail the facts G1 to G (number of inferred fact} based on facts F1 to F (number of basic facts).
Facts: {facts}
Template: {rule template}
Note that the symbol '##' in the template should be filled with either 'r1' or 'r45', while the symbol '++' should be filled with either 'r43' or 'r44'.
After filling in the template, the generated rule is:</p>
<h2>A.2.2 Zero-Shot CoT</h2>
<p>system: You are a helpful assistant with inductive reasoning abilities. Please generate one single rule to match the template and logically entail the facts. Note that the symbol '##' in the template should be filled with either 'r1' or 'r45', while the symbol '++' should be filled with either 'r43' or 'r44'.
user: I will give you a set of facts F1 to F (number of basic facts), facts G1 to G (number of inferred fact} and a template for a logical rule. Please generate one single rule to match the template and logically entail the facts G1 to G (number of inferred fact} based on facts F1 to F (number of basic facts).
Facts: {facts}
Template: {rule template}
Note that the symbol '##' in the template should be filled with either 'r1' or 'r45', while the symbol '++' should be filled with either 'r43' or 'r44'.
After filling in the template, the generated rule is: Let's think step by step.</p>
<h2>A.2.3 Zero-Shot of removing facts setting</h2>
<p>system: Please generate one single rule to match the template. Note that the symbol '##' in the template should be filled with either 'parent' or 'child', while the symbol '++' should be filled with either 'male' or 'female'.
user: I will give you a template for a logical rule. Please generate one single rule to match the template and logically infer the relation sister
Template: If A is ## of B and B is ## of C and A is ++ , then A is sister of C .
Note that the symbol '##' in the template should be filled with either 'parent' or 'child', while the symbol '++' should be filled with either 'male' or 'female'.
After filling in the template, the generated rule is:</p>
<h2>A. 3 Abductive reasoning</h2>
<h2>A.3.1 Zero-Shot</h2>
<p>system: You are a helpful assistant with abductive reasoning abilities. Please select one single logical rule and a few facts to explain the following statement.
user: I will provide a set of logical rules L1 to L (number of rules} and facts F1 to F (number of basic facts}. Please select one single logical rule from L1 to L (number of rules} and a few facts from F1 to F (number of basic facts} to explain the following statement.
Rules: {logical rules}
Facts: {basic facts}
Statement: {statement}
Answer with the numbers of the selected rule and facts. The selected rule and facts are:</p>
<h1>A.3.2 Zero-Shot-CoT</h1>
<p>system: You are a helpful assistant with abductive reasoning abilities. Please select one single logical rule and a few facts to explain the following statement.
user: I will provide a set of logical rules L1 to L{number of rules} and facts F1 to F{number of basic facts}. Please select one single logical rule from L1 to L{number of rules} and a few facts from F1 to F{number of basic facts} to explain the following statement.
Rules: {logical rules}
Facts: {basic facts}
Statement: {statement}
Answer with the numbers of the selected rule and facts. The selected rule and facts are: Let's think step by step.</p>
<h2>A.3.3 Few-Shot-CoT prompt</h2>
<p>system: You are a helpful assistant with abductive reasoning abilities. Please select one single logical rule and a few facts to explain the following statement.
user:
I will provide a set of logical rules L1 to L{number of rules} and facts F1 to F{number of basic facts}. Please select one single logical rule from L1 to L{number of rules} and a few facts from F1 to F{number of basic facts} to explain the following statement.
Rules: {logical rules}
Facts: {basic facts}
{Demonstrations}
Statement: {statement}
Answer:</p>
<h2>A.3.4 Examples of Few-Shot-CoT Demonstrations</h2>
<p>Statement: $r 4$ (Laura, Fabian)
Answer: To explain the statement $r 4$ (Laura, Fabian), we can use the rule L3 and fact F1 and F37.
L3: $\forall A, B: r 1(A, B) \wedge r 44(A) \rightarrow r 4(A, B)$
F1: $r 1$ (Laura,Fabian)
F37: $r 44$ (Laura)
According to L3, if there is a relationship r1 between A and B, and A has the property r44, then the relationship r 4 holds between A and B. In this case, we have a relationship r1 between Laura and Fabian (F1) and Laura has the property r44 (F37). Therefore, the relationship r4 holds between Laura and Fabian, which is the given statement.
The selected rule and facts are: L3, F1, and F37.
Statement: $r 5$ (Stefan, Emily)
Answer: To explain the statement $r 5$ (Stefan, Emily), we can use the rule L4 and fact F17. L4: $\forall A, B: r 1(A, B) \wedge r 43(A) \rightarrow r 5(A, B)$
F17: $r 1$ (Stefan,Emily)
According to rule L4, if there is a relation r1(A, B) and A has the property r43, then the relation r5(A, B) holds. In our case, we have the fact F17, which states that there is a relation r1(Stefan, Emily). Additionally, Stefan has the property r43 (F61). Therefore, we can conclude that the relation r5(Stefan, Emily) holds.
The selected rule and facts are: L4, F17, and F61.
Statement: $r 23$ (Felix, Elias)
Answer: To explain the statement $r 23$ (Felix, Elias), we can use the rule L22 and fact F5. L22: $\forall A, B: r 1(B, A) \wedge r 43(A) \rightarrow r 23(A, B)$
F5: $r 1$ (Elias,Felix)
According to rule L22, if there exists a relationship $r 1(\mathrm{~B}, \mathrm{~A})$ and A has the property $r 43$, then the relationship $r 23(\mathrm{~A}, \mathrm{~B})$ holds. In our case, we have the fact F5, which states that there is a relationship $r 1$ (Elias, Felix). Additionally, we know that Felix has the property $r 43$ (F59). Therefore, applying rule L22, we can conclude that the relationship $r 23$ (Felix, Elias) holds.
The selected rule and facts are: L22, F5, and F59.
Statement: $r 3$ (Felix, Fabian)
Answer: To explain the statement $r 3$ (Felix, Fabian), we can use the rule L2 and facts F2, F1, and F59. L2: $\forall A, B, C: r 1(B, A) \wedge r 1(B, C) \wedge r 43(A) \rightarrow r 3(A, C)$
F1: $r 1$ (Laura,Fabian)</p>
<p>F2: $r 1$ (Laura,Felix)
F59: $r 43($ Felix)
By substituting the values into L2, we get:
$r 1($ Laura, Felix $) \wedge r 1($ Laura, Fabian $) \wedge r 43($ Felix $) \rightarrow r 3($ Felix, Fabian $)$
Since all the premises are true according to the given facts, the conclusion $r 3$ (Felix, Fabian) is also true.
The selected rule and facts are: L2, F1, F2, and F59.
Statement: $r 7$ (Fabian, Helga)
Answer: To explain the statement $r 7$ (Fabian, Helga), we can use the rule L6 and facts F13, F21, and F39.
L6: $\forall A, B, C: r 1(A, B) \wedge r 1(B, C) \wedge r 43(A) \rightarrow r 7(A, C)$
F13: $r 1$ (Fabian,Thomas)
F21: $r 1$ (Thomas,Helga)
F39: $r 43($ Fabian)
According to L6, if there exist A, B, and C such that $r 1(A, B), r 1(B, C)$, and $r 43(A)$, then $r 7(A, C)$ holds. In our case, $\mathrm{A}=$ Fabian, $\mathrm{B}=$ Thomas, and $\mathrm{C}=$ Helga. Since we have $r 1$ (Fabian,Thomas) from F13, $r 1$ (Thomas,Helga) from F21, and $r 43$ (Fabian) from F39, we can conclude that $r 7$ (Fabian, Helga) is true.
So, the selected rule and facts are: L6, F13, F21, and F39.</p>
<h1>A.3.5 Few-Shot CoT of removing rules setting</h1>
<p>system: You are a helpful assistant with abductive reasoning abilities. Please select a few facts to explain the following statement.
user: I will provide a set of facts F1 to F63. Please select a few facts from F1 to F63 to explain the following statement.
Facts: {facts}
Statement: Laura is mother of Felix.
Answer: To explain the statement "Laura is mother of Felix", we can use Facts:
Fact F2 states: Laura is parent of Felix.
Fact F37 states: Laura is female.
Using F2 and F37, we can conclude that "Laura is mother of Felix" holds.
Therefore, the selected rule and facts are F2, F37.
Statement: Samuel is brother of Alina.
Answer: To infer the statement "Samuel is brother of Alina", we have:
F27: Patrick is parent of Samuel.
F28: Patrick is parent of Alina.
F47: Samuel is male.
Based on these facts, we can infer "Samuel is brother of Alina":
Therefore, the selected rule and facts are F27, F28, F47.
Statement: Patrick is grandfather of David.
Answer: To explain the statement "Patrick is grandfather of David", we have:
F28: Patrick is parent of Alina.
F7: Alina is parent of David.
F45: Patrick is male.
Based on these facts, we can infer "Patrick is grandfather of David":
Therefore, the selected rule and facts are F28, F7, F45.
Statement: Amelie is daughter of Elena.
Answer: To explain the statement "Amelie is daughter of Elena", we have:
F20: Elena is parent of Amelie.
F43: Amelie is female.
Based on these facts, we can infer "Amelie is daughter of Elena".
Therefore, the selected rule and facts are F20, F43.
Statement: Claudia is sister of Felix
Answer: To prove the statement "Claudia is sister of Felix", we can use facts:
F3: Laura is parent of Claudia.
F2: Laura is parent of Felix.
F40: Claudia is female.
Based on these facts, we can infer "Claudia is sister of Felix".
Therefore, the selected rule and facts are F3, F2, F40.
Statement: Laura is mother of Fabian.</p>
<h1>A. 4 Commonsense reasoning</h1>
<h2>A.4.1 Zero-Shot prompt</h2>
<p>system: Please choose answer from [A, B, C, D, E] and output one capital letter only.
user: Question: {Question}
{Multiple choices}
Answer:</p>
<h2>A.4.2 Few-Shot prompt</h2>
<p>system: Please choose answer from [A, B, C, D, E] and output one capital letter only.
user: {Demonstrations: }
Question: {Question}
{Multiple choices}
Answer:</p>
<h2>A.4.3 Examples of Few-Shot Demonstrations</h2>
<h2>Standard:</h2>
<p>Question: When is hotter?
Choices: A.desert B.night C.morning D.noon
Answer: A.</p>
<h2>Predict-explain:</h2>
<p>Question: When is hotter?
Choices: A.desert B.night C.morning D.noon
Answer: A. The desert is hotter than the other options listed because it receives direct sunlight and has little to no vegetation to provide shade or cool the air. During the day, the sun beats down on the desert, causing temperatures to rise significantly. At night, the lack of vegetation and moisture in the air means that heat is quickly lost, causing temperatures to drop rapidly. In the morning and at noon, the sun is still high in the sky, but temperatures may be slightly cooler due to the presence of some shade and cooler air from the night before.</p>
<h2>Expain-predict:</h2>
<p>Question: When is hotter?
Choices: A.desert B.night C.morning D.noon
The desert is hotter than the other options listed because it receives direct sunlight and has little to no vegetation to provide shade or cool the air. During the day, the sun beats down on the desert, causing temperatures to rise significantly. At night, the lack of vegetation and moisture in the air means that heat is quickly lost, causing temperatures to drop rapidly. In the morning and at noon, the sun is still high in the sky, but temperatures may be slightly cooler due to the presence of some shade and cooler air from the night before. Answer: A.</p>
<h2>A. 5 LLaMA Fine-tuning Prompt</h2>
<div class="codehilite"><pre><span></span><code>Below is an instruction that describes a task, paired with an input that provides further
    context.
Write a response that appropriately completes the request.
Instruction: {Head} is the {Relation} of {Tail}
Input: {input}
Response:
</code></pre></div>

<h2>B Deduction examples of Symbolic Tree datasets</h2>
<p>In this section, we provide examples of deduction experiments conducted on the Symbolic Tree datasets. We present examples for both the Semantics and Symbols settings, represented in both natural language text and logic language</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Equal contribution
${ }^{\dagger}$ Corresponding authors&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
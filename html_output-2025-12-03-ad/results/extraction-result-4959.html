<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4959 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4959</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4959</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-264128006</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.09107v2.pdf" target="_blank">GLoRE: Evaluating Logical Reasoning of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have shown significant general language understanding abilities. However, there has been a scarcity of attempts to assess the logical reasoning capacities of these LLMs, an essential facet of natural language understanding. To encourage further investigation in this area, we introduce GLoRE, a General Logical Reasoning Evaluation platform that not only consolidates diverse datasets but also standardizes them into a unified format suitable for evaluating large language models across zero-shot and few-shot scenarios. Our experimental results show that compared to the performance of humans and supervised fine-tuning models, the logical reasoning capabilities of large reasoning models, such as OpenAI's o1 mini, DeepSeek R1 and QwQ-32B, have seen remarkable improvements, with QwQ-32B achieving the highest benchmark performance to date. GLoRE is designed as a living project that continuously integrates new datasets and models, facilitating robust and comparative assessments of model performance in both commercial and Huggingface communities.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4959.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4959.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa-base</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa (base)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 125M-parameter pre-trained Transformer encoder (BERT-family) used as a supervised fine-tuned baseline for logical reasoning tasks in GLoRE.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RoBERTa: A robustly optimized bert pretraining approach.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer encoder (BERT-family) pre-trained then fine-tuned for downstream classification tasks; used here as a supervised baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>125M</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>LogiQA 2.0, LogiQA22, NaN-NLI, ProofWriter, other GLoRE tasks (MRC, NLI, TF)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-choice reading comprehension (MRC), Natural Language Inference (NLI), and True-or-False (Yes/No) tasks covering high-quality logical reasoning phenomena (e.g., entailment/contradiction/neutral, monotonicity, sub-clausal negation, rule-based reasoning and provability).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Supervised fine-tuning on each dataset's training set (five epochs per dataset) using standard RoBERTa fine-tuning procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Examples reported: 48.76% accuracy on LogiQA 2.0 and 33.22% on LogiQA22 (MRC); 90.02% on NaN-NLI; 55.92% on ProofWriter. Performance varies by task, stronger on some NLI/TF tasks than on MRC.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lags human performance on MRC tasks; performs poorly on many multi-choice MRC items (often below more advanced LLMs); shows potential learning of superficial patterns in rule-based negation datasets (NaN-NLI).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Underperforms modern instruction-tuned and reasoning-enhanced LLMs (e.g., GPT-4, QwQ-32B, DeepSeek R1) on average; outperforms some zero-shot LLMs on specific NLI/TF tasks but not on MRC.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Fine-tuning improved task-specific performance (used as a supervised baseline across datasets). No further ablations for RoBERTa reported beyond per-dataset fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4959.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4959.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-30B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (30B) - instruction/CoT fine-tuned variant (LLaMA-30b-supercot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open foundation Transformer LLM (30B variant) evaluated zero-shot and few-shot on GLoRE; shows comparable zero-shot performance to Falcon despite smaller size.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llama: Open and efficient foundation language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-30b-supercot</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>30B-parameter autoregressive Transformer family model (LLaMA variant) with instruction/chain-of-thought style tuning applied in community variants; evaluated here zero-shot and few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>30B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>GLoRE suite: MRC (LogiQA, ReClor, AR-LSAT, LogiQA22), NLI (ConTRoL, HELP, TaxiNLI, NaN-NLI), TF (FraCaS, RuleTaker, ProofWriter)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same GLoRE tasks covering multi-step verbal logical reasoning, monotonicity, negation, and rule-based provability.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot and few-shot (in-context learning) evaluation using appended example demonstrations for 1/2/5-shot; no supervised fine-tuning reported for this model in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot average reported near 32.34% (Table summaries). Few-shot gains observed (e.g., up to ~39.62% at 5-shot in average few-shot table). Performs poorly on MRC (around ~20% on 4-way MRC in some cases), often worse than supervised RoBERTa on several tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Struggles in zero-shot MRC (sometimes worse than random guessing on 4-way tasks); sensitive to dataset distribution; performance improves with in-context examples but still far from human ceiling.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Performs similarly to Falcon-40B despite smaller parameter count; underperforms instruction/reasoning-enhanced models like QwQ-32B, DeepSeek R1, GPT-4 in GLoRE average and many specific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Few-shot (ICL) experiments show increasing accuracy with more examples (1→2→5-shot), indicating reliance on statistical adaptation rather than robust reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4959.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4959.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon-40B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon (40B) - instruct-tuned</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 40B-parameter open large language model (instruct-tuned variant) evaluated zero-shot and few-shot on GLoRE; performance comparable to LLaMA in zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon-40b-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open large autoregressive Transformer model with a 40B-parameter variant, instruction-tuned to follow prompts and instructions; evaluated in GLoRE zero-shot and few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>40B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>GLoRE suite (MRC, NLI, TF datasets listed in GLoRE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluation across multi-choice reading comprehension, NLI classification, and true/false rule reasoning tasks to probe verbal logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot and few-shot (in-context learning) evaluation following the same protocol as other LLMs in GLoRE.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot average near LLaMA (32.28% in few-shot table baseline); few-shot improves modestly (e.g., to ~35.72% at 5-shot in average few-shot table). Underperforms supervised RoBERTa on many tasks and is far below reasoning-enhanced models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Low MRC accuracy in zero-shot (approx ~20% on some 4-way tasks); sensitive to data distribution; limited logical generalization without in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Comparable to LLaMA-30B in zero-shot despite larger size; underperforms ChatGPT/GPT-4 and reasoning-enhanced models (QwQ-32B, DeepSeek R1) in average performance on GLoRE.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Few-shot ICL improves performance slightly; no detailed ablation beyond shot-count effects presented.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4959.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4959.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixtral-8x7b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixtral (8x7B Mixture-of-Experts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mixture-of-experts (MoE) open LLM variant evaluated zero-shot on GLoRE, demonstrating better zero-shot reasoning than some dense models of comparable compute.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixtral-8x7b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mixture-of-experts architecture composed of 8 experts each of ~7B parameters (community reported naming); used here in evaluation experiments as a community model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8x7b</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>GLoRE suite (MRC, NLI, TF datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same GLoRE tasks assessing verbal logical reasoning across MRC, entailment, and rule-based truth evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot evaluation (no supervised fine-tuning reported in this paper for Mixtral).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Outperforms LLaMA and Falcon in zero-shot on several tasks (reported qualitatively), indicating the effectiveness of MoE designs for reasoning; exact per-dataset averages not individually enumerated beyond comparative statements.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>While stronger than some dense models, still lags instruction-tuned reasoning-enhanced models; sensitive to data distribution and task type.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Outperforms LLaMA and Falcon in zero-shot, suggesting MoE benefits; still below top reasoning-enhanced models like QwQ-32B and DeepSeek R1 on average.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Authors note Mixtral's superior zero-shot performance as evidence for MoE effectiveness, but no internal ablations are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4959.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4959.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (OpenAI conversational model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's instruction-tuned conversational LLM evaluated in zero-shot and few-shot GLoRE experiments, demonstrating moderate logical reasoning performance with variability across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned conversational variant from OpenAI (GPT-family) used for zero-shot and few-shot evaluation; exact architecture/size not specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Selected GLoRE tasks including ConTRoL (NLI), MRC and TF datasets in zero-shot and few-shot settings</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>NLI, MRC and True/False tasks requiring contextual entailment, monotonicity, negation handling, and multi-step logical inference.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot and few-shot (in-context learning) evaluation; demonstration examples appended for few-shot conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Examples: ChatGPT achieves 58.45% on ConTRoL (NLI) and overall few-shot averages improve with more shots (52.10% zero-shot → 60.32% at 5-shot in the few-shot table). Generally outperforms RoBERTa on many NLI/TF tasks but below top reasoning-enhanced models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Inconsistent across datasets; sensitive to distributional shifts; relies on in-context patterns rather than robust logical rules, per authors' analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Outperformed by GPT-4 and top open-source reasoning-enhanced models on many tasks, though it surpasses traditional supervised baselines on several NLI/TF datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Few-shot ICL yields consistent improvements; authors note these gains likely reflect statistical adaptation rather than fundamental improvements in causal logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4959.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4959.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's most capable GPT-family model evaluated zero-shot and few-shot on the GLoRE benchmark; strong zero-shot MRC and NLI performance but sensitive to dataset shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OpenAI: Gpt-4 technical report.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>State-of-the-art instruction-tuned large autoregressive Transformer from OpenAI; architecture and exact parameter counts not specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Full GLoRE suite: MRC (LogiQA 2.0, ReClor, AR-LSAT, LogiQA22), NLI (ConTRoL, HELP, TaxiNLI, NaN-NLI), TF (FraCaS, RuleTaker, ProofWriter).</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks measure multi-step verbal logical reasoning, entailment classification, monotonicity/negation probing, and rule-based provability tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot and few-shot (ICL) evaluation. 1/2/5-shot prompts sampled from each dataset appended to prompts for few-shot experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported average: 66.34% (zero-shot average on GLoRE). Dataset highlights: ReClor 87.20%, NaN-NLI 75.74%, FraCaS 75.35%; large drop on LogiQA22 (58.49%) vs LogiQA 2.0 (72.25%). Few-shot: improves to 75.83% at 5-shot (from 66.34% zero-shot) in average few-shot table; GPT-4 shows ~9% absolute gain from 0→5-shot in a highlighted example.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance sensitive to data distribution (not robust across dataset shifts); inconsistent on monotonicity/negation tasks (e.g., HELP 46.01% on one dataset); may rely on superficial patterns rather than generalizable logical rules.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Strongest among commercial models in zero-shot in many MRC tasks and near human in some datasets (e.g., ReClor), but outperformed by some reasoning-enhanced open-source models (QwQ-32B and DeepSeek R1) on average and on multiple benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Few-shot (ICL) yields significant gains (e.g., ~9% absolute for GPT-4 at 5-shot). Authors analyze sensitivity to distribution and potential data leakage but no internal architectural ablation for GPT-4 is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4959.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4959.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o1 mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI o1 (mini) system</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI system-card model variant evaluated on GLoRE, described as a reasoning-capable commercial model that achieves strong performance on some NLI datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Openai, Openai o1 system card. 2024</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o1 mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI system variant (o1 mini) referenced in the GLoRE experiments; exact architecture and parameter size not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>GLoRE NLI and other tasks (e.g., HELP, NaN-NLI, MRC and TF tasks in the suite)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>NLI tasks probing monotonicity and fine-grained entailment, plus MRC and TF datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot evaluation (and few-shot in the same protocol as other LLMs); commercially provided instruction-tuned model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported stronger performance on certain NLI tasks (e.g., HELP: 63.69% where QwQ-32B scored 61.53%). Overall average not explicitly singled out beyond comparative mentions; o1 mini is among top-performing commercial/reasoning-enhanced models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Like other LLMs, sensitive to data distribution and shows inconsistent generalization across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Comparable or superior to GPT-4 on some NLI datasets (e.g., HELP), but overall QwQ-32B and DeepSeek R1 achieve higher average performance on the full GLoRE benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>No ablations provided; reported results used for cross-model comparison indicating strengths on certain fine-grained entailment tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4959.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4959.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek R1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek R1</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning-enhanced open model (reported as using reinforcement learning incentives for reasoning) evaluated on GLoRE and achieving strong average performance (second-best in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deepseek-Ai , Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek R1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source/research model described as reasoning-enhanced via reinforcement learning techniques to incentivize logical reasoning capabilities; exact architecture/size not specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>GLoRE suite (MRC, NLI, TF datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same mixture of multi-choice logical reading comprehension, entailment classification, and true/false rule reasoning tasks aimed at testing multi-step and symbolic-like reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated zero-shot and few-shot; model reportedly trained/optimized with reinforcement learning objectives to incentivize reasoning capability (cite provided in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported average accuracy: 75.14% (GLoRE average). Dataset numbers in the table: e.g., shows strong MRC and TF performance and balanced performance across most tasks (examples in table row include values such as 76.22, 81.49, 77.88, 90.01, ... leading to the 75.14% average).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Still sensitive to dataset distribution; uneven performance on particular NLI subdomains (some variability reported).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Outperforms GPT-4 on average and is slightly behind QwQ-32B in overall GLoRE average; shows more balanced performance across tasks compared to QwQ-32B's NLI weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Authors attribute DeepSeek R1's strong performance to its reinforcement-learning-based training incentives, but no detailed ablation within this paper isolates which components drive gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4959.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4959.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QwQ-32B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QwQ-32B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 32B-parameter reasoning-enhanced open model using reinforcement learning techniques; achieves state-of-the-art results on the GLoRE benchmark reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Qwq-32b: Embracing the power of reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>QwQ-32B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>32B-parameter open LLM reported as reasoning-enhanced via reinforcement learning or specialized training methodology designed to improve logical inference and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>GLoRE suite (MRC: ReClor, AR-LSAT, LogiQA variants; NLI: HELP, ConTRoL, NaN-NLI; TF: ProofWriter, RuleTaker, FraCaS)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Comprehensive logical reasoning evaluation across multi-choice reading comprehension, natural language inference (entailment/contradiction/neutral), and true/false provability tasks requiring multi-step and symbolic-like reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluation in zero-shot and few-shot settings; model training described in paper references as employing reinforcement learning or other specialized instruction/finetuning aimed at improving reasoning generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported highest average accuracy on GLoRE: 78.95% average. Dataset highlights: ReClor 93.76% (state-of-the-art), AR-LSAT 92.35%, ProofWriter 82.40%, LogiQA22 86.30%; however, lower on some NLI datasets (HELP 61.53%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Uneven generalization: dominates MRC and TF tasks but shows weaker performance on some fine-grained NLI tasks (e.g., monotonicity/negation in HELP). Sensitivity to data distribution remains an issue despite high averages.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Outperforms GPT-4 (66.34% average) and DeepSeek R1 (75.14% average) on overall GLoRE average; surpasses GPT-4 on many MRC tasks (e.g., ReClor) and TF benchmarks, but is sometimes outperformed on select NLI metrics (e.g., HELP by o1 mini).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Authors ascribe QwQ-32B's gains to reinforcement-learning-based or specialized training methodology that enables better cross-distribution generalization; no per-component ablation is presented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>RoBERTa: A robustly optimized bert pretraining approach. <em>(Rating: 2)</em></li>
                <li>Llama: Open and efficient foundation language models. <em>(Rating: 2)</em></li>
                <li>OpenAI: Gpt-4 technical report. <em>(Rating: 2)</em></li>
                <li>Deepseek-Ai , Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. <em>(Rating: 2)</em></li>
                <li>Qwq-32b: Embracing the power of reinforcement learning. <em>(Rating: 2)</em></li>
                <li>LogiQA 2.0-an improved dataset for logical reasoning in natural language understanding. <em>(Rating: 2)</em></li>
                <li>Reclor: A reading comprehension dataset requiring logical reasoning. <em>(Rating: 2)</em></li>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language. <em>(Rating: 1)</em></li>
                <li>RuleTaker: Reasoning over natural language rules. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4959",
    "paper_id": "paper-264128006",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "RoBERTa-base",
            "name_full": "RoBERTa (base)",
            "brief_description": "A 125M-parameter pre-trained Transformer encoder (BERT-family) used as a supervised fine-tuned baseline for logical reasoning tasks in GLoRE.",
            "citation_title": "RoBERTa: A robustly optimized bert pretraining approach.",
            "mention_or_use": "use",
            "model_name": "RoBERTa-base",
            "model_description": "Transformer encoder (BERT-family) pre-trained then fine-tuned for downstream classification tasks; used here as a supervised baseline.",
            "model_size": "125M",
            "logical_reasoning_task": "LogiQA 2.0, LogiQA22, NaN-NLI, ProofWriter, other GLoRE tasks (MRC, NLI, TF)",
            "task_description": "Multi-choice reading comprehension (MRC), Natural Language Inference (NLI), and True-or-False (Yes/No) tasks covering high-quality logical reasoning phenomena (e.g., entailment/contradiction/neutral, monotonicity, sub-clausal negation, rule-based reasoning and provability).",
            "method_or_approach": "Supervised fine-tuning on each dataset's training set (five epochs per dataset) using standard RoBERTa fine-tuning procedures.",
            "performance": "Examples reported: 48.76% accuracy on LogiQA 2.0 and 33.22% on LogiQA22 (MRC); 90.02% on NaN-NLI; 55.92% on ProofWriter. Performance varies by task, stronger on some NLI/TF tasks than on MRC.",
            "limitations_or_failure_cases": "Lags human performance on MRC tasks; performs poorly on many multi-choice MRC items (often below more advanced LLMs); shows potential learning of superficial patterns in rule-based negation datasets (NaN-NLI).",
            "comparison": "Underperforms modern instruction-tuned and reasoning-enhanced LLMs (e.g., GPT-4, QwQ-32B, DeepSeek R1) on average; outperforms some zero-shot LLMs on specific NLI/TF tasks but not on MRC.",
            "ablation_or_analysis_results": "Fine-tuning improved task-specific performance (used as a supervised baseline across datasets). No further ablations for RoBERTa reported beyond per-dataset fine-tuning.",
            "uuid": "e4959.0",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLaMA-30B",
            "name_full": "LLaMA (30B) - instruction/CoT fine-tuned variant (LLaMA-30b-supercot)",
            "brief_description": "An open foundation Transformer LLM (30B variant) evaluated zero-shot and few-shot on GLoRE; shows comparable zero-shot performance to Falcon despite smaller size.",
            "citation_title": "Llama: Open and efficient foundation language models.",
            "mention_or_use": "use",
            "model_name": "LLaMA-30b-supercot",
            "model_description": "30B-parameter autoregressive Transformer family model (LLaMA variant) with instruction/chain-of-thought style tuning applied in community variants; evaluated here zero-shot and few-shot.",
            "model_size": "30B",
            "logical_reasoning_task": "GLoRE suite: MRC (LogiQA, ReClor, AR-LSAT, LogiQA22), NLI (ConTRoL, HELP, TaxiNLI, NaN-NLI), TF (FraCaS, RuleTaker, ProofWriter)",
            "task_description": "Same GLoRE tasks covering multi-step verbal logical reasoning, monotonicity, negation, and rule-based provability.",
            "method_or_approach": "Zero-shot and few-shot (in-context learning) evaluation using appended example demonstrations for 1/2/5-shot; no supervised fine-tuning reported for this model in the paper.",
            "performance": "Zero-shot average reported near 32.34% (Table summaries). Few-shot gains observed (e.g., up to ~39.62% at 5-shot in average few-shot table). Performs poorly on MRC (around ~20% on 4-way MRC in some cases), often worse than supervised RoBERTa on several tasks.",
            "limitations_or_failure_cases": "Struggles in zero-shot MRC (sometimes worse than random guessing on 4-way tasks); sensitive to dataset distribution; performance improves with in-context examples but still far from human ceiling.",
            "comparison": "Performs similarly to Falcon-40B despite smaller parameter count; underperforms instruction/reasoning-enhanced models like QwQ-32B, DeepSeek R1, GPT-4 in GLoRE average and many specific tasks.",
            "ablation_or_analysis_results": "Few-shot (ICL) experiments show increasing accuracy with more examples (1→2→5-shot), indicating reliance on statistical adaptation rather than robust reasoning.",
            "uuid": "e4959.1",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Falcon-40B",
            "name_full": "Falcon (40B) - instruct-tuned",
            "brief_description": "A 40B-parameter open large language model (instruct-tuned variant) evaluated zero-shot and few-shot on GLoRE; performance comparable to LLaMA in zero-shot.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Falcon-40b-instruct",
            "model_description": "Open large autoregressive Transformer model with a 40B-parameter variant, instruction-tuned to follow prompts and instructions; evaluated in GLoRE zero-shot and few-shot.",
            "model_size": "40B",
            "logical_reasoning_task": "GLoRE suite (MRC, NLI, TF datasets listed in GLoRE)",
            "task_description": "Evaluation across multi-choice reading comprehension, NLI classification, and true/false rule reasoning tasks to probe verbal logical reasoning.",
            "method_or_approach": "Zero-shot and few-shot (in-context learning) evaluation following the same protocol as other LLMs in GLoRE.",
            "performance": "Zero-shot average near LLaMA (32.28% in few-shot table baseline); few-shot improves modestly (e.g., to ~35.72% at 5-shot in average few-shot table). Underperforms supervised RoBERTa on many tasks and is far below reasoning-enhanced models.",
            "limitations_or_failure_cases": "Low MRC accuracy in zero-shot (approx ~20% on some 4-way tasks); sensitive to data distribution; limited logical generalization without in-context examples.",
            "comparison": "Comparable to LLaMA-30B in zero-shot despite larger size; underperforms ChatGPT/GPT-4 and reasoning-enhanced models (QwQ-32B, DeepSeek R1) in average performance on GLoRE.",
            "ablation_or_analysis_results": "Few-shot ICL improves performance slightly; no detailed ablation beyond shot-count effects presented.",
            "uuid": "e4959.2",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Mixtral-8x7b",
            "name_full": "Mixtral (8x7B Mixture-of-Experts)",
            "brief_description": "A mixture-of-experts (MoE) open LLM variant evaluated zero-shot on GLoRE, demonstrating better zero-shot reasoning than some dense models of comparable compute.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mixtral-8x7b",
            "model_description": "Mixture-of-experts architecture composed of 8 experts each of ~7B parameters (community reported naming); used here in evaluation experiments as a community model.",
            "model_size": "8x7b",
            "logical_reasoning_task": "GLoRE suite (MRC, NLI, TF datasets)",
            "task_description": "Same GLoRE tasks assessing verbal logical reasoning across MRC, entailment, and rule-based truth evaluation.",
            "method_or_approach": "Zero-shot evaluation (no supervised fine-tuning reported in this paper for Mixtral).",
            "performance": "Outperforms LLaMA and Falcon in zero-shot on several tasks (reported qualitatively), indicating the effectiveness of MoE designs for reasoning; exact per-dataset averages not individually enumerated beyond comparative statements.",
            "limitations_or_failure_cases": "While stronger than some dense models, still lags instruction-tuned reasoning-enhanced models; sensitive to data distribution and task type.",
            "comparison": "Outperforms LLaMA and Falcon in zero-shot, suggesting MoE benefits; still below top reasoning-enhanced models like QwQ-32B and DeepSeek R1 on average.",
            "ablation_or_analysis_results": "Authors note Mixtral's superior zero-shot performance as evidence for MoE effectiveness, but no internal ablations are reported.",
            "uuid": "e4959.3",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT (OpenAI conversational model)",
            "brief_description": "OpenAI's instruction-tuned conversational LLM evaluated in zero-shot and few-shot GLoRE experiments, demonstrating moderate logical reasoning performance with variability across datasets.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_description": "Instruction-tuned conversational variant from OpenAI (GPT-family) used for zero-shot and few-shot evaluation; exact architecture/size not specified in the paper.",
            "model_size": null,
            "logical_reasoning_task": "Selected GLoRE tasks including ConTRoL (NLI), MRC and TF datasets in zero-shot and few-shot settings",
            "task_description": "NLI, MRC and True/False tasks requiring contextual entailment, monotonicity, negation handling, and multi-step logical inference.",
            "method_or_approach": "Zero-shot and few-shot (in-context learning) evaluation; demonstration examples appended for few-shot conditions.",
            "performance": "Examples: ChatGPT achieves 58.45% on ConTRoL (NLI) and overall few-shot averages improve with more shots (52.10% zero-shot → 60.32% at 5-shot in the few-shot table). Generally outperforms RoBERTa on many NLI/TF tasks but below top reasoning-enhanced models.",
            "limitations_or_failure_cases": "Inconsistent across datasets; sensitive to distributional shifts; relies on in-context patterns rather than robust logical rules, per authors' analysis.",
            "comparison": "Outperformed by GPT-4 and top open-source reasoning-enhanced models on many tasks, though it surpasses traditional supervised baselines on several NLI/TF datasets.",
            "ablation_or_analysis_results": "Few-shot ICL yields consistent improvements; authors note these gains likely reflect statistical adaptation rather than fundamental improvements in causal logical reasoning.",
            "uuid": "e4959.4",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "OpenAI's most capable GPT-family model evaluated zero-shot and few-shot on the GLoRE benchmark; strong zero-shot MRC and NLI performance but sensitive to dataset shifts.",
            "citation_title": "OpenAI: Gpt-4 technical report.",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "State-of-the-art instruction-tuned large autoregressive Transformer from OpenAI; architecture and exact parameter counts not specified in the paper.",
            "model_size": null,
            "logical_reasoning_task": "Full GLoRE suite: MRC (LogiQA 2.0, ReClor, AR-LSAT, LogiQA22), NLI (ConTRoL, HELP, TaxiNLI, NaN-NLI), TF (FraCaS, RuleTaker, ProofWriter).",
            "task_description": "Benchmarks measure multi-step verbal logical reasoning, entailment classification, monotonicity/negation probing, and rule-based provability tasks.",
            "method_or_approach": "Zero-shot and few-shot (ICL) evaluation. 1/2/5-shot prompts sampled from each dataset appended to prompts for few-shot experiments.",
            "performance": "Reported average: 66.34% (zero-shot average on GLoRE). Dataset highlights: ReClor 87.20%, NaN-NLI 75.74%, FraCaS 75.35%; large drop on LogiQA22 (58.49%) vs LogiQA 2.0 (72.25%). Few-shot: improves to 75.83% at 5-shot (from 66.34% zero-shot) in average few-shot table; GPT-4 shows ~9% absolute gain from 0→5-shot in a highlighted example.",
            "limitations_or_failure_cases": "Performance sensitive to data distribution (not robust across dataset shifts); inconsistent on monotonicity/negation tasks (e.g., HELP 46.01% on one dataset); may rely on superficial patterns rather than generalizable logical rules.",
            "comparison": "Strongest among commercial models in zero-shot in many MRC tasks and near human in some datasets (e.g., ReClor), but outperformed by some reasoning-enhanced open-source models (QwQ-32B and DeepSeek R1) on average and on multiple benchmarks.",
            "ablation_or_analysis_results": "Few-shot (ICL) yields significant gains (e.g., ~9% absolute for GPT-4 at 5-shot). Authors analyze sensitivity to distribution and potential data leakage but no internal architectural ablation for GPT-4 is provided.",
            "uuid": "e4959.5",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "o1 mini",
            "name_full": "OpenAI o1 (mini) system",
            "brief_description": "An OpenAI system-card model variant evaluated on GLoRE, described as a reasoning-capable commercial model that achieves strong performance on some NLI datasets.",
            "citation_title": "Openai, Openai o1 system card. 2024",
            "mention_or_use": "use",
            "model_name": "o1 mini",
            "model_description": "OpenAI system variant (o1 mini) referenced in the GLoRE experiments; exact architecture and parameter size not specified in this paper.",
            "model_size": null,
            "logical_reasoning_task": "GLoRE NLI and other tasks (e.g., HELP, NaN-NLI, MRC and TF tasks in the suite)",
            "task_description": "NLI tasks probing monotonicity and fine-grained entailment, plus MRC and TF datasets.",
            "method_or_approach": "Zero-shot evaluation (and few-shot in the same protocol as other LLMs); commercially provided instruction-tuned model.",
            "performance": "Reported stronger performance on certain NLI tasks (e.g., HELP: 63.69% where QwQ-32B scored 61.53%). Overall average not explicitly singled out beyond comparative mentions; o1 mini is among top-performing commercial/reasoning-enhanced models.",
            "limitations_or_failure_cases": "Like other LLMs, sensitive to data distribution and shows inconsistent generalization across datasets.",
            "comparison": "Comparable or superior to GPT-4 on some NLI datasets (e.g., HELP), but overall QwQ-32B and DeepSeek R1 achieve higher average performance on the full GLoRE benchmark.",
            "ablation_or_analysis_results": "No ablations provided; reported results used for cross-model comparison indicating strengths on certain fine-grained entailment tasks.",
            "uuid": "e4959.6",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "DeepSeek R1",
            "name_full": "DeepSeek R1",
            "brief_description": "A reasoning-enhanced open model (reported as using reinforcement learning incentives for reasoning) evaluated on GLoRE and achieving strong average performance (second-best in the paper).",
            "citation_title": "Deepseek-Ai , Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.",
            "mention_or_use": "use",
            "model_name": "DeepSeek R1",
            "model_description": "Open-source/research model described as reasoning-enhanced via reinforcement learning techniques to incentivize logical reasoning capabilities; exact architecture/size not specified in the paper.",
            "model_size": null,
            "logical_reasoning_task": "GLoRE suite (MRC, NLI, TF datasets)",
            "task_description": "Same mixture of multi-choice logical reading comprehension, entailment classification, and true/false rule reasoning tasks aimed at testing multi-step and symbolic-like reasoning.",
            "method_or_approach": "Evaluated zero-shot and few-shot; model reportedly trained/optimized with reinforcement learning objectives to incentivize reasoning capability (cite provided in paper).",
            "performance": "Reported average accuracy: 75.14% (GLoRE average). Dataset numbers in the table: e.g., shows strong MRC and TF performance and balanced performance across most tasks (examples in table row include values such as 76.22, 81.49, 77.88, 90.01, ... leading to the 75.14% average).",
            "limitations_or_failure_cases": "Still sensitive to dataset distribution; uneven performance on particular NLI subdomains (some variability reported).",
            "comparison": "Outperforms GPT-4 on average and is slightly behind QwQ-32B in overall GLoRE average; shows more balanced performance across tasks compared to QwQ-32B's NLI weaknesses.",
            "ablation_or_analysis_results": "Authors attribute DeepSeek R1's strong performance to its reinforcement-learning-based training incentives, but no detailed ablation within this paper isolates which components drive gains.",
            "uuid": "e4959.7",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "QwQ-32B",
            "name_full": "QwQ-32B",
            "brief_description": "A 32B-parameter reasoning-enhanced open model using reinforcement learning techniques; achieves state-of-the-art results on the GLoRE benchmark reported in this paper.",
            "citation_title": "Qwq-32b: Embracing the power of reinforcement learning.",
            "mention_or_use": "use",
            "model_name": "QwQ-32B",
            "model_description": "32B-parameter open LLM reported as reasoning-enhanced via reinforcement learning or specialized training methodology designed to improve logical inference and generalization.",
            "model_size": "32B",
            "logical_reasoning_task": "GLoRE suite (MRC: ReClor, AR-LSAT, LogiQA variants; NLI: HELP, ConTRoL, NaN-NLI; TF: ProofWriter, RuleTaker, FraCaS)",
            "task_description": "Comprehensive logical reasoning evaluation across multi-choice reading comprehension, natural language inference (entailment/contradiction/neutral), and true/false provability tasks requiring multi-step and symbolic-like reasoning.",
            "method_or_approach": "Evaluation in zero-shot and few-shot settings; model training described in paper references as employing reinforcement learning or other specialized instruction/finetuning aimed at improving reasoning generalization.",
            "performance": "Reported highest average accuracy on GLoRE: 78.95% average. Dataset highlights: ReClor 93.76% (state-of-the-art), AR-LSAT 92.35%, ProofWriter 82.40%, LogiQA22 86.30%; however, lower on some NLI datasets (HELP 61.53%).",
            "limitations_or_failure_cases": "Uneven generalization: dominates MRC and TF tasks but shows weaker performance on some fine-grained NLI tasks (e.g., monotonicity/negation in HELP). Sensitivity to data distribution remains an issue despite high averages.",
            "comparison": "Outperforms GPT-4 (66.34% average) and DeepSeek R1 (75.14% average) on overall GLoRE average; surpasses GPT-4 on many MRC tasks (e.g., ReClor) and TF benchmarks, but is sometimes outperformed on select NLI metrics (e.g., HELP by o1 mini).",
            "ablation_or_analysis_results": "Authors ascribe QwQ-32B's gains to reinforcement-learning-based or specialized training methodology that enables better cross-distribution generalization; no per-component ablation is presented in this paper.",
            "uuid": "e4959.8",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "RoBERTa: A robustly optimized bert pretraining approach.",
            "rating": 2,
            "sanitized_title": "roberta_a_robustly_optimized_bert_pretraining_approach"
        },
        {
            "paper_title": "Llama: Open and efficient foundation language models.",
            "rating": 2,
            "sanitized_title": "llama_open_and_efficient_foundation_language_models"
        },
        {
            "paper_title": "OpenAI: Gpt-4 technical report.",
            "rating": 2,
            "sanitized_title": "openai_gpt4_technical_report"
        },
        {
            "paper_title": "Deepseek-Ai , Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.",
            "rating": 2,
            "sanitized_title": "deepseekai_deepseekr1_incentivizing_reasoning_capability_in_llms_via_reinforcement_learning"
        },
        {
            "paper_title": "Qwq-32b: Embracing the power of reinforcement learning.",
            "rating": 2,
            "sanitized_title": "qwq32b_embracing_the_power_of_reinforcement_learning"
        },
        {
            "paper_title": "LogiQA 2.0-an improved dataset for logical reasoning in natural language understanding.",
            "rating": 2,
            "sanitized_title": "logiqa_20an_improved_dataset_for_logical_reasoning_in_natural_language_understanding"
        },
        {
            "paper_title": "Reclor: A reading comprehension dataset requiring logical reasoning.",
            "rating": 2,
            "sanitized_title": "reclor_a_reading_comprehension_dataset_requiring_logical_reasoning"
        },
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language.",
            "rating": 1,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "RuleTaker: Reasoning over natural language rules.",
            "rating": 1,
            "sanitized_title": "ruletaker_reasoning_over_natural_language_rules"
        }
    ],
    "cost": 0.01557975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>GLoRE: Evaluating Logical Reasoning of Large Language Models
20 Apr 2025</p>
<p>Hanmeng Liu 
Hainan University
HaikouHainan</p>
<p>Zhiyang Teng 
ByteDance SG</p>
<p>Ruoxi Ning 
Westlake University
HangzhouZhejiangChina</p>
<p>Yiran Ding 
Westlake University
HangzhouZhejiangChina</p>
<p>Xiulai Li 
Hainan University
HaikouHainan</p>
<p>Xiaozhang Liu 
Hainan University
HaikouHainan</p>
<p>Yue Zhang 
Westlake University
HangzhouZhejiangChina</p>
<p>GLoRE: Evaluating Logical Reasoning of Large Language Models
20 Apr 20252E53C0DD83F28EA3B1EBC449B1AC2C90arXiv:2310.09107v2[cs.CL]Large Language ModelLarge Reasoning ModelLogical reasoningNatural Language Inference
Large language models (LLMs) have shown significant general language understanding abilities.However, there has been a scarcity of attempts to assess the logical reasoning capacities of these LLMs, an essential facet of natural language understanding.To encourage further investigation in this area, we introduce GLoRE, a General Logical Reasoning Evaluation platform that not only consolidates diverse datasets but also standardizes them into a unified format suitable for evaluating large language models across zero-shot and few-shot scenarios.Our experimental results show that compared to the performance of humans and supervised fine-tuning models, the logical reasoning capabilities of large reasoning models, such as OpenAI's o1 mini, DeepSeek R1 and QwQ-32B, have seen remarkable improvements, with QwQ-32B achieving the highest benchmark performance to date.GLoRE is designed as a living project that continuously integrates new datasets and models, facilitating robust and comparative assessments of model performance in both commercial and Huggingface communities.It garnered over 300 citations upon its release.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) [50,67], especially reasoning language models [18,51] demonstrate advanced capabilities in complex reasoning tasks and show significant adaptability and versatility across various applications, from simple everyday tasks to specialized domains such as coding, mathematics, law, medicine, and finance [11,22,34,37,76].Quantitative evaluation of LLM reasoning has thus become a very important task.To this end, existing work has considered mathematical reasoning [15,26], algorithmic problem solving [9,58], and knowledge-driven reasoning [25,73].</p>
<p>Logical reasoning is a cornerstone of human intelligence and has been a central focus in artificial intelligence research since its inception [16,29,33].However, evaluating verbal reasoning turned out to be too difficult in the 1950s due to insufficient natural language understanding (NLU) technologies, and thus AI researchers focused on formal logical reasoning instead [29,48,49].Since the 2010s, NLU has witnessed huge advances, where reading comprehension [8,21] and natural language inference [7,74] tasks were solved with high accuracies, which made verbal reasoning evaluation feasible [43,80].Figure 1 illustrates an example of logical reasoning in reading comprehension.To address such questions, LLMs must engage in multi-step, algorithmic, symbolic reasoning.This makes logical reasoning an ideal testbed for evaluating LLMs' ability to process complex natural language information accurately, robustly, and logically.</p>
<p>To this end, we introduce the General Logical Reasoning Evaluation (GLoRE) benchmark, designed to assess instruction-tuned LLMs on various logical reasoning tasks.GLoRE evaluates the strengths and limitations of LLMs in this domain, similar to how GLUE [71] and Super-GLUE [70] benchmark natural language understanding.GLoRE includes three types of logical reasoning tasks: Multi-choice Reading Comprehension [35], Natural Language Inference (NLI) [17], and True-or-False (Yes-or-No) Questions [13].These tasks encompass a wide range of logical reasoning phenomena, with high-quality datasets that remain challenging for pre-trained language models [13,27,32].In total, GLoRE covers 12 datasets with 72,848 instances.Since its release in 2023, GLoRE has been used for evaluating language models, receiving over 300 citations on ArXiv.</p>
<p>Using GLoRE, we report the logical reasoning capabilities of commercial models such as GPT-4 and OpenAI o1 [51], as well as popular open-source models such as LLaMA [67], Falcon [1], Mistral [30], DeepSeek R1 [18], and QwQ-32B [66].We test their instruction-following and problem-solving abilities in logical reasoning tasks.Results show that while commercial LLMs like GPT-4 still excel in zero-shot settings and approach human performance on specific datasets like ReClor, open-source models like QwQ-32B now rival or even surpass commercial counterparts in key tasks, achieving state-of-the-art results on multiple benchmarks.This underscores rapid advancements in open-source LLMs, narrowing the performance gap with commercial models.However, performance varies significantly across datasets, indicating sensitivity to data distribution.This sensitivity is further confirmed by observations that in-context learning and supervised fine-tuning primarily enhance LLM performance on specific test distributions, demonstrating their strong learning ability.While LLMs show promise in logical reasoning, their robustness to data distribution variations remains a challenge, highlighting the need for further improvement.</p>
<p>Related Work</p>
<p>Logical reasoning with natural language.Tapping into logical reasoning capabilities represents a holistic endeavour in natural language understanding (NLU).A variety of methods have been explored to realize this objective, including symbolic systems [45,47,55], fine-tuning of language models [28,41,71,78], and hybrid approaches combining neural and symbolic elements [36,59,60].</p>
<p>The recent introduction of evaluation datasets, notably LogiQA [43] and Reclor [80], has reinvigorated the focus on logical reasoning in NLP research.Logical reasoning is now leveraged in numerous probing tasks over large Pretrained Language Models (PLMs) and applied to downstream tasks such as question-answering and dialogue systems [6,63].Despite these advancements, the aspiration to emulate human-like logical reasoning capabilities within NLU systems remains a significant challenge for traditional models [27,43].In this study, our goal is not only to quantitatively evaluate the capability of Large Language Models (LLMs) in addressing the previously mentioned challenge but also to underscore the significance of our work in providing a validated platform for enhancing various reasoning methods with our data.</p>
<p>LLM reasoning evaluation.Despite progress in evaluating LLMs for specific reasoning tasks like arithmetic [57] and commonsense [4], a yawning gap exists in comprehensively assessing their logical reasoning.While LLMs excel at specific tasks like arithmetic reasoning [57], they face challenges in complex areas like multi-step reasoning [23] and abstract scenarios [24].ChatGPT exhibits strengths in chat-specific reasoning and some commonsense domains [4,53], but struggles with tasks requiring longer chains of inference [4].Other LLMs like FLAN-T5 [12], LLaMA [67], and PaLM [2] show potential in general deductive reasoning [61], while InstructGPT and Codex excel in specialized domains like medical reasoning [38].Despite these advances, limitations in data bias [52], and complex reasoning tasks necessitate further research and optimization to fully unlock the reasoning potential of LLMs [77].</p>
<p>Big-Bench Hard (BBH) [64] isolates 23 most challenging tasks from BIG-Bench [3].These tasks comprise general language understanding, arithmetic and algorithmic reasoning, and logical deduction.However, in comparison to our benchmark, the data size of the logical reasoning section in BBH is very small.HumanEval [10] serves as a hand-written evaluation set for coding.The programming problems included are designed to assess language comprehension, reasoning, algorithms, and simple mathematics.While similar to logical reasoning in that code generation necessitates complex reasoning skills, GLoRE differs in presenting logical reasoning problems via natural language prompts.ARB [62] is a benchmark for advanced reasoning over multiple fields like mathematics, physics, biology, chemistry, and law.Similar to GLoRE, it introduces a challenging subset of math and physics problems that require advanced symbolic reasoning.However, the benchmark constraints its problem on the above subjects with domain knowledge, not general logical reasoning questions, which is the focus of GLoRE.</p>
<p>The GLoRE Dataset</p>
<p>As mentioned in the introduction, GLoRE contains three NLU tasks: Multichoice Reading Comprehension, NLI, and Yes-or-No.First, Multi-choice reading comprehension [35] is essential in verbal reasoning tests, which cover abundant high-quality logical reasoning problems in the wild.Second, Unlike multi-choice reading comprehension, NLI [17] is more general and centric on entailment relations in a simpler task format, which is a fundamental task for evaluating reasoning abilities [19,54].Third, the Yes-or-No reasoning task [13] is a combination of question-answering and textual entailment, which can serve as a playground for testing models' reasoning abilities [14,65].The data statistics are shown in Table 1.</p>
<p>Multi-choice Reading Comprehension (MRC)</p>
<p>Within the standard multi-choice reading comprehension (MRC) task setting, a system is presented with a passage and a question, and the objective is to choose the most suitable answer from a set of candidate responses.Particularly, GLoRE contains five such datasets: LogiQA [43] is a logical MRC dataset derived from the Chinese Civil Service Examination, translated into English, and made available in both Chinese and English versions.We adopt LogiQA 2.0 [40] and use both the English (LogiQA 2.0) and Chinese (LogiQA 2.0 zh) test sets for our evaluation.</p>
<p>ReClor [80] comprises question-answering examples from the LSAT exams designed to assess human logical reasoning abilities.We use the development set for our testing as the test set does not provide gold labels.AR-LSAT [72] is a dataset of analytical reasoning questions from the Law School Admission Test.Each question contains five options rather than four.</p>
<p>LogiQA22 is collected and processed according to the LogiQA 2.0 format after ChatGPT was released.It incorporates the newly released Chinese Civil Servant Exams from 2022, which are not included in the original LogiQA dataset.</p>
<p>Natural Language Inference (NLI)</p>
<p>NLI is the task of determining the logical relationship between a hypothesis and a premise.The typical scheme involves text classification, where the model selects one of three labels: entailment, contradiction, and neutral.ConTRoL [39] is an NLI dataset that offers an in-depth examination of contextual reasoning within the NLI framework.Approximately 36.2% of premisehypothesis pairs fall under the category of logical reasoning in this dataset.We choose the logical reasoning portion for our evaluation.HELP [79] is an NLI dataset emphasizing monotonicity reasoning, a crucial concept in Natural Logic [46].We use the training set for our evaluation.TaxiNLI [31] is an NLI dataset that has been re-annotated based on MNLI [75], with categories include logical categories such as connectives, mathematical reasoning, and deduction.NaN-NLI [68] is a test suite designed to probe the capabilities of NLP models in capturing sub-clausal negation.The successful handling of sub-clausal negation can be seen as a strong indicator of a model's logical reasoning capacity.</p>
<p>True-or-False (Yes-or-No) Questions (TF)</p>
<p>FraCaS test suite [56] presents complex entailment problems involving multipremised contexts as a three-way classification task.The ability to determine entailment relationships in this context is closely tied to logical reasoning.RuleTaker [14] dataset is a synthetic creation designed to examine the reasoning ability of transformer models [69] over natural language rules.This task explicitly targets logical reasoning by asking models to reason over a set of rules and facts to generate true-or-false responses as output.ProofWriter [65] dataset generates sets of facts and rules, each followed by questions, which can be proven true or false using proofs of various depths.</p>
<p>Experiments</p>
<p>We employ GLoRE to assess the logical reasoning capabilities across different categories of language models, including traditional pre-trained models and reasoning-enhanced LLMs, both open-source and proprietary.We conduct a comprehensive comparative analysis of their performance against human benchmarks.</p>
<p>Experimental Settings</p>
<p>We adopted RoBERTa-base [44] as a baseline, fine-tuning it on the training set over five epochs for each dataset.The community models selected for comparison include Falcon-40b-instruct [1], LLaMA-30b-supercot [67] Mixtral-8x7b, DeepSeek R1 [18] and QwQ-32B [66].For OpenAI models, we choose ChatGPT, GPT-4 and o1 mini [51].</p>
<p>Our evaluation metrics consisted of classification accuracy scores.Additionally, we utilized reported accuracies for datasets where human performance data was available and recorded both the average and peak performance of human participants to establish a human baseline.For the LogiQA22 dataset, we engaged five co-authors as test subjects and computed their accuracy based on 150 test examples.</p>
<p>Main Results</p>
<p>Zero-shot results.Table 2 summarizes the zero-shot evaluation results.The first block shows human performance.The second block presents RoBERTabase's supervised fine-tuning results.With 125M parameters, RoBERTa-base achieves 48.76% and 33.22% accuracy on LogiQA 2.0 and LogiQA22, respectively, lagging behind human performance.It performs better on NLI and TF tasks than MRC tasks, likely due to output ambiguities.On NaN-NLI, RoBERTa achieves 90.02% accuracy, matching human performance, possibly due to learning superficial patterns from rule-based negation data.On ProofWriter, RoBERTabase scores 55.92%, indicating potential for specific logical reasoning tasks.</p>
<p>The third block shows zero-shot results for LLaMA, Falcon, and Mixtral.LLaMA and Falcon perform similarly (32.34% vs. 32.28%),suggesting comparable reasoning capabilities despite LLaMA-30B's smaller size.Both underperform RoBERTa-base on most tasks, except Falcon on RT.On MRC tasks, their accuracy is around 20%, worse than random guessing in 4-way classification, indicating challenges in logical reasoning without in-context demonstrations.</p>
<p>Performance gaps between LogiQA and LogiQA22 are smaller for these models, suggesting stable performance across data distributions without in-domain tuning.Mixtral-8x7b outperforms LLaMA and Falcon, demonstrating the effectiveness of mixture-of-expert models.</p>
<p>The fourth block provides zero-shot results ChatGPT and GPT-4.Both models, especially GPT-4, surpass RoBERTa-base on several MRC benchmarks.However, GPT-4's accuracy drops significantly on LogiQA22 (58.49% vs. 72.25% on LogiQA 2.0), indicating sensitivity to data distribution.In NLI and TF tasks, ChatGPT and GPT-4 outperform RoBERTa, with ChatGPT achieving 58.45% accuracy on ConTRoL, surpassing GPT-4.GPT-4's NLI performance varies across datasets, further highlighting its sensitivity to data distribution.TF task results show similar inconsistencies, suggesting model rationales differ from human reasoning.</p>
<p>The final block shows results for o1 mini, DeepSeek R1, and QwQ-32B, which achieve notable improvements over prior models.QwQ-32B attains the highest average accuracy (78.95%), surpassing GPT-4 (66.34%) and DeepSeek R1 (75.14%).It achieves state-of-the-art results on MRC tasks like ReClor (93.76%) and AR-LSAT (92.35%), indicating the need for more challenging benchmarks for logical reasoning.Its robustness is evident in LogiQA22 (86.30%), outperforming GPT-4 by 27.81 percentage points.However, QwQ-32B shows uneven performance on NLI datasets, such as HELP (61.53%, lagging behind o1 mini's 63.69%), suggesting its reasoning capabilities are less generalizable in tasks requiring fine-grained entailment analysis.</p>
<p>While GPT-4 retains an advantage on FraCas (75.35%),QwQ-32B surpasses GPT-4 on ReClor (93.76% vs. 87.20%),redefining state-of-the-art performance for MRC tasks.QwQ-32B and DeepSeek R1 showcase balanced performance across most tasks, with QwQ-32B achieving unprecedented TF results (e.g., 82.40% on ProofWriter, outperforming both GPT-4's 59.66% and DeepSeek R1's 80.51%).Though still below the human average overall, these models mark substantial progress -QwQ-32B's 78.95% average accuracy (vs.DeepSeek R1's 75.14% and GPT-4's 66.34%) highlights significant architectural or training innovations for logical inference.</p>
<p>Few-shot results.LLMs excel at in-context learning [20], where performance improves with context examples and demonstration methods [42].For this study, we randomly sampled instances (1 for 1-shot, 2 for 2-shot, and 5 for 5-shot) from each dataset and appended them to the prompt.We used the same model configuration as in the zero-shot scenario.Table 3 highlights the impact of in-context learning (ICL), as seen in GPT-4's 9% accuracy gain with 5-shot learning.However, this improvement stems from statistical adaptation rather than true reasoning, as models rely on superficial patterns rather than humanlike logical inference.This aligns with findings that chain-of-thought prompts correlate with outputs but do not causally drive reasoning [5].While reasoningenhanced models narrow the gap with human performance, their sensitivity to data distribution highlights the need for further research into more robust reasoning mechanisms.GLoRE's evolving framework will continue to track these advancements.</p>
<p>Analysis</p>
<p>Large language models vs. reasoning-enhanced models.The reasoningenhanced models like QwQ-32B, DeepSeek R1, and OpenAI's o1 mini demonstrate significant improvements over traditional LLMs.QwQ-32B, in particular, achieves the highest average performance (78.95%), indicating that its reinforcement learning framework or specialized training methodology enables better generalization across data distributions.While QwQ-32B dominates MRC and TF tasks, its relatively lower performance on NLI datasets like HELP (61.53%) suggests that even state-of-the-art models struggle with tasks requiring monotonicity or negation reasoning, highlighting the need for broader evaluation beyond task-specific robustness.Data leakage concerns.While GLoRE includes diverse datasets, potential data leakage risks arise from overlapping sources.GPT-4's lower accuracy on LogiQA22 (58.49%) compared to LogiQA 2.0 (72.25%) suggests limited exposure to newer data, reducing leakage concerns but highlighting distributional sensitivity.The benchmark's dynamic updates and inclusion of newly annotated datasets help mitigate leakage by testing models on unseen distributions.Sensitivity to data distribution.The above experiments show that the performance of LLMs is sensitive to the data distribution.Even though the underlying reasoning principles are the same, LLM performance varies significantly across datasets.This suggests that LLMs might not reason using the correct rationale but rely on superficial features.As shown in Table 2, although GPT-4 achieves near-human performance on datasets like ReClor (87.20%) and NaN-NLI (75.74%), it lags significantly on others (e.g., HELP at 46.01%).This inconsistency mirrors the behavior of reasoning-enhanced models like DeepSeek R1, revealing a critical divergence from human reasoning: once humans master a reasoning pattern, their performance generalizes robustly, whereas LLMs remain sensitive to data-specific features.</p>
<p>Conclusion</p>
<p>We constructed GLoRE, a dynamic and comprehensive benchmark tailored for assessing the logical reasoning capabilities of advanced language models, including GPT-4 and various strong open-source LLMs across multiple reasoning tasks.</p>
<p>Our findings indicate that QwQ-32B, a reasoning-enhanced model, sets a new state-of-the-art on the GLoRE benchmark, significantly narrowing the gap to human performance.This underscores the potential of targeted architectural and training innovations for logical reasoning.GLoRE will be continually maintained to track advancements in this rapidly evolving domain.</p>
<p>Fig. 1 .
1
Fig. 1.Instruction and question format for logical reading comprehension tasks.</p>
<p>Table 1 .
1
Data statistics.("E": entailment; "C": contradiction; "N": neutral.)
DatasetSizeTargetDatasetSizeTargetLogiQA 2.0 test1,572 4-way multi-choice ConTRoL805E, C, NLogiQA 2.0 zh test 1,594 4-way multi-choice HELP35,891E, C, NReClor dev500 4-way multi-choice TaxiNLI test10,071E, C, NAR-LSAT test230 5-way multi-choice NaN-NLI259E, C, NLogiQA221,354 4-way multi-choice FraCas346 Yes, No, NeutralRuleTaker dev 10,068Yes, NoProofWriter dev 10,158Yes, No</p>
<p>Table 3 .
3
Average accuracies on GLoRE few-shot evaluation.
Model0-shot 1-shot 2-shot 5-shotLLaMA32.34 32.89 35.03 39.62Falcon32.28 33.14 33.76 35.72ChatGPT 52.10 55.85 57.43 60.32GPT-466.34 70.31 71.44 75.83</p>
<p>. Mrc Nli Task, Avg Tf, Lq Lq Zh Rc Al Lq22 Ct Hl Tn Nn Fc Rt Dataset, Pw Human, Avg, 86.00 88.00 63.00 56.00 83.00 87.00 81.00 97.00 94.00 92.00 84.00 82.00 82.75</p>
<p>Human Ceiling. 95.00 96.00 100.00 91.00 99.00 94.00 95.00 100.00 100.00 97.00 95.00 93.00 96.25</p>
<p>. Deepseek, R1 76.22 81.49 77.88 90.01 71.63 78.37 62.05 75.74 72.58 59.96 75.29 80.51 75.14</p>
<p>All results are in %, the best ones are in bold, and the second best ones are in underline. E Almazrouei, H Alobeidli, A Alshamsi, A Cappelli, R Cojocaru, M Debbah, E Goffinet, D Heslow, J Launay, Q Malartic, B Noune, B Pannier, LQ: LogiQA 2.0, RC : Re-Clor, AL: AR-LSAT, CT : ConTRoL, HL: HELP, TN : TaxiNLI, NN : NaN-NLI, FC : FraCas, RT : RuleTaker, PW : ProofWriter. Penedo, G.2023Table 2. LLMs' performance on the GLoRE benchmark. Falcon-40B: an open large language model with state-of-the-art performance</p>
<p>R Anil, A M Dai, O Firat, M Johnson, D Lepikhin, A Passos, Palm 2 technical report. 2023</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. B Bench Authors, 2023TMLR</p>
<p>Y Bang, S Cahyawijaya, N Lee, W Dai, D Su, B Wilie, H Lovenia, Z Ji, T Yu, W Chung, arXiv:2302.04023A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. 2023arXiv preprint</p>
<p>How likely do llms with cot mimic human reasoning?. G Bao, H Zhang, C Wang, L Yang, Y Zhang, arXiv:2402.160482024arXiv preprint</p>
<p>Logical reasoning for task oriented dialogue systems. S Beygi, M Fazel-Zarandi, A Cervone, P Krishnan, S R Jonnalagadda, 2022</p>
<p>A large annotated corpus for learning natural language inference. S R Bowman, G Angeli, C Potts, C D Manning, Proc. of EMNLP. of EMNLP2015</p>
<p>A thorough examination of the CNN/daily mail reading comprehension task. D Chen, J Bolton, C D Manning, ACL. 2016</p>
<p>M Chen, J Tworek, H Jun, Q Yuan, Others: Evaluating large language models trained on code. 2021</p>
<p>M Chen, J Tworek, H Jun, Q Yuan, Others: Evaluating large language models trained on code. 2021</p>
<p>Chatgpt goes to law school. J H Choi, K E Hickman, A Monahan, D Schwarcz, Available at SSRN. 2023</p>
<p>H W Chung, L Hou, S Longpre, B Zoph, Y Tay, W Fedus, Y Li, X Wang, M Dehghani, S Brahma, arXiv:2210.11416Scaling instruction-finetuned language models. 2022arXiv preprint</p>
<p>Boolq: Exploring the surprising difficulty of natural yes/no questions. C Clark, K Lee, M W Chang, T Kwiatkowski, M Collins, K Toutanova, 2019</p>
<p>Transformers as soft reasoners over language. P Clark, O Tafjord, K Richardson, Proc. of IJCAI. of IJCAI2020</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>M J Cresswell, Logics and languages. Routledge19731st ed.</p>
<p>The pascal recognising textual entailment challenge. I Dagan, O Glickman, B Magnini, 2005MLCW</p>
<p>Deepseek-Ai , Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. 2025</p>
<p>Transforming question answering datasets into natural language inference datasets. D Demszky, K Guu, P Liang, 2018</p>
<p>Q Dong, L Li, D Dai, C Zheng, Z Wu, B Chang, X Sun, J Xu, L Li, Z Sui, A survey on in-context learning. 2023</p>
<p>DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. D Dua, Y Wang, P Dasigi, G Stanovsky, S Singh, M Gardner, Proc. of AACL. of AACL2019</p>
<p>. S Frieder, L Pinchetti, R R Griffiths, T Salvatori, T Lukasiewicz, P C Petersen, A Chevalier, J Berner, 2023Mathematical capabilities of chatgpt</p>
<p>Chain-of-thought hub: A continuous effort to measure large models' reasoning performance. Y Fu, L Ou, M Chen, Y Wan, H Peng, T Khot, arXiv:2305.173062023arXiv preprint</p>
<p>Large language models are not abstract reasoners. G Gendron, Q Bao, M Witbrock, G Dobbie, arXiv:2305.195552023arXiv preprint</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, Proceedings of the International Conference on Learning Representations (ICLR. the International Conference on Learning Representations (ICLR2021</p>
<p>D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, arXiv:2103.03874Measuring mathematical problem solving with the math dataset. 2021arXiv preprint</p>
<p>J Huang, K C C Chang, Towards reasoning in large language models: A survey. 2023</p>
<p>Y Huang, M Fang, Y Cao, L Wang, X Liang, arXiv:2103.14349Dagn: Discourse-aware graph network for logical reasoning. 2021arXiv preprint</p>
<p>L Iwańska, Logical reasoning in natural language: It is all about knowledge. Minds and Machines. 1993</p>
<p>. A Q Jiang, A Sablayrolles, A Roux, A Mensch, B Savary, Others, 2024Mixtral of experts</p>
<p>Taxinli: Taking a ride up the NLU hill. P Joshi, S Aditya, A Sathe, M Choudhury, 2020CoRR</p>
<p>ContractNLI: A dataset for document-level natural language inference for contracts. Y Koreeda, C Manning, Proc. of EMNLP Findings. of EMNLP Findings2021</p>
<p>Logic for problem solving. R Kowalski, 1979Ediciones Díaz de Santos</p>
<p>Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models. T H Kung, M Cheatham, A Medenilla, C Sillos, L De Leon, C Elepaño, M Madriaga, R Aggabao, G Diaz-Candido, J Maningo, 2023e0000198</p>
<p>RACE: Large-scale Reading Comprehension dataset from Examinations. G Lai, Q Xie, H Liu, Y Yang, E Hovy, EMNLP. 2017</p>
<p>Augmenting neural networks with first-order logic. T Li, V Srikumar, Proc. of ACL. of ACL2019</p>
<p>Competitionlevel code generation with alphacode. Y Li, D Choi, J Chung, N Kushman, J Schrittwieser, Others, 2022</p>
<p>Can large language models reason about medical questions?. V Liévin, C E Hother, O Winther, arXiv:2207.081432022arXiv preprint</p>
<p>Natural language inference in context -investigating contextual reasoning over long texts. H Liu, L Cui, J Liu, Y Zhang, 2020CoRR</p>
<p>Logiqa 2.0-an improved dataset for logical reasoning in natural language understanding. H Liu, J Liu, L Cui, Z Teng, N Duan, M Zhou, Y Zhang, Speech, and Language Processing. 2023</p>
<p>Logicot: Logical chainof-thought instruction tuning. H Liu, Z Teng, L Cui, C Zhang, Q Zhou, Y Zhang, Proc. of EMNLP Findings. of EMNLP Findings2023</p>
<p>. J Liu, D Shen, Y Zhang, B Dolan, L Carin, W Chen, What makes good in-context examples for gpt-3? (2021</p>
<p>Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. J Liu, L Cui, H Liu, D Huang, Wang, Y Zhang, 2020CoRR</p>
<p>Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, arXivRoberta: A robustly optimized bert pretraining approach. 2019</p>
<p>Natural logic for textual inference. B Maccartney, C D Manning, Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing. the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing2007</p>
<p>Natural logic for textual inference. B Maccartney, C D Manning, Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing. the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing2007</p>
<p>Programs with common sense. J Mccarthy, 2002</p>
<p>Some philosophical problems from the standpoint of artificial intelligence. J Mccarthy, P J Hayes, Machine Intelligence. 41969</p>
<p>The logic theory machine-a complex information processing system. A Newell, H Simon, IRE Transactions on Information Theory. 1956</p>
<p>OpenAI: Gpt-4 technical report. 2023</p>
<p>Openai, Openai o1 system card. 2024</p>
<p>Human-like problem-solving abilities in large language models using chatgpt. G Orrù, A Piarulli, C Conversano, A Gemignani, Frontiers in Artificial Intelligence. 11993502023</p>
<p>S Ott, K Hebenstreit, V Liévin, C E Hother, M Moradi, M Mayrhauser, R Praas, O Winther, M Samwald, arXiv:2301.11596Thoughtsource: A central hub for large language model reasoning data. 2023arXiv preprint</p>
<p>Collecting diverse natural language inference problems for sentence representation evaluation. A Poliak, A Haldar, R Rudinger, J E Hu, E Pavlick, A S White, B Van Durme, Proc. of EMNLP. of EMNLP2018</p>
<p>Theorist: A Logical Reasoning System for Defaults and Diagnosis. D Poole, R Goebel, R Aleliunas, 1987</p>
<p>Using the framework. S G Pulman, 1996</p>
<p>Is chatgpt a general-purpose natural language processing task solver?. C Qin, A Zhang, Z Zhang, J Chen, M Yasunaga, D Yang, 2023</p>
<p>S Quan, J Yang, B Yu, B Zheng, D Liu, A Yang, X Ren, B Gao, Y Miao, Y Feng, arXiv:2501.01257Codeelo: Benchmarking competition-level code generation of llms with human-comparable elo ratings. 2025arXiv preprint</p>
<p>Prover: Proof generation for interpretable reasoning over rules. S Saha, S Ghosh, S Srivastava, M Bansal, 2020</p>
<p>S Sanyal, H Singh, X Ren, Fairr: Faithful and robust deductive reasoning over natural language. 2022</p>
<p>A Saparov, R Y Pang, V Padmakumar, N Joshi, S M Kazemi, N Kim, H He, arXiv:2305.15269Testing the general deductive reasoning capacity of large language models using ood examples. 2023arXiv preprint</p>
<p>T Sawada, D Paleka, A Havrilla, P Tadepalli, P Vidas, A Kranias, J J Nay, K Gupta, A Komatsuzaki, Arb: Advanced reasoning benchmark for large language models. 2023</p>
<p>Neural natural logic inference for interpretable question answering. J Shi, X Ding, L Du, T Liu, B Qin, Proc. of EMNLP. of EMNLP2021</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. M Suzgun, N Scales, N Schärli, S Gehrmann, Y Tay, H W Chung, A Chowdhery, Q V Le, E H Chi, D Zhou, J Wei, 2022</p>
<p>O Tafjord, B D Mishra, P Clark, Proofwriter: Generating implications, proofs, and abductive statements over natural language. 2021</p>
<p>Qwq-32b: Embracing the power of reinforcement learning. Q Team, 2025</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample, Llama: Open and efficient foundation language models. 2023</p>
<p>Not another negation benchmark: The NaN-NLI test suite for sub-clausal negation. T H Truong, Y Otmakhova, T Baldwin, T Cohn, J H Lau, K Verspoor, Proc. of AACL. of AACL2022</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, 2017CoRR</p>
<p>A Wang, Y Pruksachatkun, N Nangia, A Singh, J Michael, F Hill, O Levy, S R Bowman, SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. 2019</p>
<p>GLUE: A multitask benchmark and analysis platform for natural language understanding. A Wang, A Singh, J Michael, F Hill, O Levy, S Bowman, Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP2018</p>
<p>From lsat: The progress and challenges of complex reasoning. S Wang, Z Liu, W Zhong, M Zhou, Z Wei, Z Chen, N Duan, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 2022</p>
<p>Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. Y Wang, X Ma, G Zhang, Y Ni, A Chandra, S Guo, W Ren, A Arulraj, X He, Z Jiang, Proc. of NeurIPS. of NeurIPS2024</p>
<p>A broad-coverage challenge corpus for sentence understanding through inference. A Williams, N Nangia, S Bowman, Proc. of AACL. of AACL2018</p>
<p>A broad-coverage challenge corpus for sentence understanding through inference. A Williams, N Nangia, S Bowman, Proc. of NAACL. of NAACL2018</p>
<p>S Wu, O Irsoy, S Lu, V Dabravolski, M Dredze, S Gehrmann, P Kambadur, D Rosenberg, G Mann, arXiv:2303.17564Bloomberggpt: A large language model for finance. 2023arXiv preprint</p>
<p>Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. Z Wu, L Qiu, A Ross, E Akyürek, B Chen, B Wang, N Kim, J Andreas, Y Kim, arXiv:2307.024772023arXiv preprint</p>
<p>Logiformer. F Xu, J Liu, Q Lin, Y Pan, L Zhang, Proc. of SIGIR. of SIGIR2022</p>
<p>Help: A dataset for identifying shortcomings of neural models in monotonicity reasoning. H Yanaka, K Mineshima, D Bekki, K Inui, S Sekine, J Bos, Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (<em>SEM2019). the Eighth Joint Conference on Lexical and Computational Semantics (</em>SEM2019)2019</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. W Yu, Z Jiang, Y Dong, J Feng, Proc. of ICLR. of ICLR2020</p>            </div>
        </div>

    </div>
</body>
</html>
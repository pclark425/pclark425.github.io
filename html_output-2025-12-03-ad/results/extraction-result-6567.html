<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6567 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6567</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6567</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-130.html">extraction-schema-130</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <p><strong>Paper ID:</strong> paper-277468152</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.00043v2.pdf" target="_blank">CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation</a></p>
                <p><strong>Paper Abstract:</strong> Existing reasoning evaluation frameworks for Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) predominantly assess either text-based reasoning or vision-language understanding capabilities, with limited dynamic interplay between textual and visual constraints. To address this limitation, we introduce CrossWordBench, a benchmark designed to evaluate the reasoning capabilities of both LLMs and LVLMs through the medium of crossword puzzles -- a task requiring multimodal adherence to semantic constraints from text-based clues and intersectional constraints from visual grid structures. CrossWordBench leverages a controllable puzzle generation framework that produces puzzles in two formats (text and image), supports adjustable difficulty through prefill ratio control, and offers different evaluation strategies, ranging from direct puzzle solving to interactive modes. Our extensive evaluation of over 20 models reveals that reasoning LLMs substantially outperform non-reasoning models by effectively leveraging crossing-letter constraints. We further demonstrate that LVLMs struggle with the task, showing a strong correlation between their puzzle-solving performance and grid-parsing accuracy. Our findings highlight limitations of the reasoning capabilities of current LLMs and LVLMs, and provide an effective approach for creating multimodal constrained tasks for future evaluations.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6567.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6567.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o3-mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>o3-mini</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary reasoning LLM evaluated in this paper that attains the highest reported intersection consistency (ICR) on 7x7 CrossWordBench puzzles, showing strong ability to enforce crossword grid constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o3-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary reasoning LLM (instruction-tuned); evaluated with Chain-of-Thought prompting and test-time reasoning-effort scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>CrossWordBench crossword puzzles (7x7, 14x14)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>constraint satisfaction / multimodal spatial-textual puzzle (crossword)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>CrossWordBench (English set; 7x7 and 14x14 grids)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot Chain-of-Thought (CoT); test-time scaling (varying reasoning token budget)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>CoT-style internal stepwise reasoning; increased token budget for deeper chain generation (test-time scaling)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Text input: empty grid encoded as 2D binary array (1 = blocked, 0 = unfilled) prepended to textual clues</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>WCR (Word Coverage Rate), LCR (Letter Coverage Rate), ICR (Intersection Consistency Rate)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>7x7: WCR=0.587 ±0.023; LCR=0.684 ±0.021; ICR=0.891. 14x14: WCR=0.445; LCR=0.520; ICR=0.512 (values reported in paper, mean ± stderr where provided).</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Highest ICR indicates strong adherence to crossing-letter constraints; benefits from increased crossing letters; test-time scaling improves performance up to a point (diminishing returns beyond medium reasoning effort).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Performance improves when increasing reasoning effort from low→medium; doubling reasoning tokens beyond medium yields little additional gain. Robust to textual grid-format ablations (array vs markdown).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Still imperfect on larger (14x14) puzzles; diminishing returns from additional reasoning tokens; evaluation limited to generated puzzles (no human-authored puzzles used).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6567.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6567.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-weight reasoning model (from the DeepSeek series) that attains the highest WCR and LCR on 7x7 CrossWordBench but slightly lower ICR than o3-mini, indicating strong answer accuracy with somewhat less intersection consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-weight reasoning LLM (reported in paper as a reasoning model); noted in related work as trained with reinforcement learning for reasoning emergence, but here evaluated via prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>CrossWordBench crossword puzzles (7x7, 14x14)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>constraint satisfaction / multimodal spatial-textual puzzle (crossword)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>CrossWordBench (English set; 7x7 and 14x14 grids)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>CoT prompting; evaluated as a reasoning-capable model (no external solver used during evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Text input: 2D binary array grid + textual clues</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>WCR, LCR, ICR</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>7x7: WCR=0.646 ±0.019; LCR=0.707 ±0.017; ICR=0.678 ±0.023 (reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Top WCR and LCR among evaluated models on 7x7, but ICR slightly lower than best (o3-mini), suggesting very good per-word accuracy but less perfect enforcement of crossing-letter consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Compared to non-reasoning models, shows substantial gains; performs better with higher crossing-letter counts on 7x7 grids.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Generalization gaps when models are specialized for math-style verifiable tasks (other models trained primarily on math underperform); performance drops on larger grids.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6567.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6567.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3.7-Sonnet (text)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3.7 Sonnet (text-input, LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary LLM evaluated in text-input mode on CrossWordBench; shows improved performance when enabling 'thinking' (reflection) mode on text inputs and achieves competitive WCR/LCR though ICR remains below the best reasoning models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude 3.7 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary LLM; evaluated with and without 'thinking' mode; instruction-tuned conversational model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>CrossWordBench crossword puzzles (7x7, 14x14)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>constraint satisfaction / multimodal spatial-textual puzzle (crossword) evaluated in text-only representation</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>CrossWordBench (English set)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot Chain-of-Thought (CoT); evaluation also includes 'thinking mode' variant</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>CoT prompting and internal self-reflection ('thinking') option available in model API</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Text input: 2D binary-array representation of grid prepended to clues; markdown-style grid used in ablations</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>WCR, LCR, ICR</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Text-input (7x7): reported example values include WCR≈0.617 ±0.019; LCR≈0.712 ±0.017; ICR≈0.754 ±0.021 (paper reports multiple runs and modes).</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Thinking mode improved Claude on text inputs across metrics (indicating benefit from additional internal token generation in text-only setting). Reasoning helps adherence to crossing-letter constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Thinking mode helps in text-input evaluation but, notably, enabling thinking mode with image inputs reduced grid-parsing and puzzle-solving performance (suggesting trade-offs between reasoning-token generation and image-processing).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>When provided image inputs, thinking mode can harm LVLM image processing; model still inferior to top reasoning models (e.g., o3-mini) on ICR for complex grids.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6567.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6567.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3.7-Sonnet (image/LVLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3.7 Sonnet (image-input / LVLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The same Claude 3.7 Sonnet model evaluated in LVLM (image+clue) mode; demonstrates strong across-word extraction but much weaker down-word/OCR extraction, illustrating modality-specific limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude 3.7 Sonnet (LVLM image mode)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary vision-language model variant of Claude 3.7 Sonnet; ingests image containing grid and clues.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>CrossWordBench crossword puzzles (image-input; 7x7)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>multimodal constraint satisfaction (visual + textual)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>CrossWordBench (English set; image inputs)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>image zero-shot Chain-of-Thought (CoT) prompt; Interactive Mode experiments also considered</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>Image-grounded CoT prompting; interactive step-by-step mode supported by updated grid images</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Image embedding of puzzle (grid + clues) as LVLM input; interactive mode uses updated grid images generated by evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Evaluation framework uses external grid-update functions to produce updated grid images between rounds; these functions can serve as callable tools in agentic settings to provide feedback (e.g., length/consistency checks).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Grid parsing WCR (Across/Down), puzzle WCR/LCR/ICR, ISS (Interactive Success Step) in interactive experiments</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Grid parsing (example reported): Across=0.954 ±0.009; Down=0.760 ±0.018 (Table 3). Puzzle-solving WCR with image inputs is substantially lower than text-input variant (see paper Table 2), and enabling thinking mode reduced grid parsing and WCR for image inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>LVLM performance strongly correlated with grid-parsing (r ≈ 0.94). Models extract Across words more accurately than Down words (vertical OCR weakness). Enabling thinking mode can divert tokens from image processing and harm performance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Two LVLM input formats (combined image with grid+clues vs separate grid image + text clues) showed no significant WCR difference, indicating format not primary cause of poor LVLM puzzle performance. Thinking mode harms image-input performance while helping text-input performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>OCR and spatial extraction for down words is a key failure mode; LVLMs struggle to build spatially coherent grid representations from images; puzzle-solving constrained by visual processing rather than reasoning capability alone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6567.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6567.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o-2024-11-20</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary multimodal model evaluated on CrossWordBench; shows moderate WCR on text or combined inputs but lower ICR and notable asymmetry in grid parsing (good across, poor down).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-2024-11-20</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary multimodal LLM/LVLM from OpenAI; evaluated in both text and image input modes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>CrossWordBench crossword puzzles (7x7)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>constraint satisfaction / multimodal</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>CrossWordBench (English set)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot Chain-of-Thought (CoT) prompting; image zero-shot CoT for LVLM input</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>CoT prompting; no external solver</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Text: 2D binary-array grid + textual clues. Image: grid+clues embedded in single image.</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>WCR, LCR, ICR; grid parsing Across/Down WCR</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Puzzle-solving (7x7): WCR≈0.410 ±0.018; LCR≈0.472 ±0.018; ICR≈0.288 ±0.019. Grid parsing (Table 3 example): Across=0.886 ±0.014; Down=0.448 ±0.024.</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Substantially better at extracting across words (horizontal) than down words (vertical), mirroring LVLM OCR limitations; overall LVLM puzzle performance strongly tied to grid parsing accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Image vs text input differences observed (image inputs tend to reduce performance compared to text-grid representation).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Low ICR indicates poor intersection consistency; poor vertical OCR reduces puzzle-solving effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6567.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6567.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pixtral-Large-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pixtral-Large-Instruct-2411</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-weight LVLM included in the evaluation; achieves the best WCR among open-weight LVLMs in the study but still lags behind proprietary LLMs and reasoning models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pixtral-Large-Instruct-2411</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-weight vision-language model (instruction-tuned LVLM) evaluated on CrossWordBench image inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>CrossWordBench crossword puzzles (7x7)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>multimodal constraint satisfaction (crossword)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>CrossWordBench (English set)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>image zero-shot Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>CoT prompting; no external solver</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Image containing grid and clues</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>WCR, LCR, ICR; grid parsing Across/Down WCR reported separately</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>7x7 puzzle-solving: WCR=0.297 ±0.015; LCR=0.338 ±0.014; ICR=0.198 ±0.014. Grid parsing (Across/Down): Across≈0.753 ±0.022; Down≈0.361 ±0.022 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Best among open-weight LVLMs but still much weaker than LLMs; strong dependence of puzzle-solving on grid-parsing accuracy; across extraction much stronger than down extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Providing OCR-extracted clues to LLMs showed that loss of spatial grid information (from OCR only) limits LLMs unless positional hints are added; Pixtral still underperforms LLMs due to visual-spatial challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Struggles with vertical word extraction (Down), low intersection consistency (ICR); LVLM visual parsing is the bottleneck rather than pure language reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6567.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6567.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QwQ-32B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QwQ-32B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-weight reasoning-capable LLM (32B parameters as indicated by name) evaluated on CrossWordBench that shows moderate word-level accuracy among reasoning LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>QwQ-32B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-weight LLM described in paper as a reasoning model; name indicates 32B-parameter scale (paper refers to it explicitly as 'QwQ-32B').</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>CrossWordBench crossword puzzles (7x7)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>constraint satisfaction</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>CrossWordBench (English set)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>CoT prompting (no external solver)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Text input (2D binary array grid + clues)</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>WCR reported; LCR/ICR for some models but paper summarizes WCRs for QwQ</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported approximate WCR ≈ 0.347 on 7x7 puzzles (paper states QwQ-32B WCR ≈ 0.347).</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Moderate performance among reasoning LLMs; observed to reason consistently in Chinese when solving Chinese puzzles (multilingual behavior noted).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Per paper, reasoning models (including QwQ) improve with more crossing letters; performance declines on larger 14x14 grids.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not top-performing among reasoning LLMs; struggles with larger grids and maintaining intersection consistency at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6567.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6567.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Open-Reasoner-Zero-32B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open-Reasoner-Zero-32B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning-focused model (32B) evaluated on CrossWordBench that performs poorly across metrics, suggesting training focused on mathematical/verifiable tasks does not transfer to spatial-linguistic crossword solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Open-Reasoner-Zero-32B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-weight reasoning model; paper notes its training emphasis on mathematical reasoning and verifiable answers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>CrossWordBench crossword puzzles (7x7)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>constraint satisfaction</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>CrossWordBench (English set)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>CoT prompting (no external solver used)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Text input grid encoded as 2D binary array + clues</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>WCR, LCR, ICR</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>7x7: WCR=0.139 ±0.010; LCR=0.204 ±0.010; ICR=0.184 ±0.012 (reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Performs poorly across metrics, indicating limited transfer from math/verifiable-task training to crossword-style spatial-textual reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Paper contrasts this model with other reasoning LLMs to highlight training-domain limitations; does not benefit from crossing-letter density like other reasoning models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Failure mode likely due to training focus (mathematical reasoning) not matching the multimodal spatial-linguistic demands of crossword puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6567.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6567.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Interactive Mode (LVLM agentic eval)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Interactive Mode / Agentic Evaluation (CrossWordBench)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation setting introduced in the paper that requires step-by-step puzzle solving with updated grid images and introduces the Interactive Success Step (ISS) metric to measure how many words a model correctly fills before its first error.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LVLMs (various) in Interactive Mode</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a single model but an evaluation protocol applied to LVLMs where models solve puzzles across multiple turns with updated grid images provided by the benchmark's functions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>CrossWordBench interactive puzzles (7x7 English)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>sequential constraint satisfaction / agentic multimodal task</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>CrossWordBench (interactive evaluation on English 7x7)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Interactive zero-shot CoT with step-by-step responses; four-round conversation history (context window limited)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>Stepwise CoT with external grid update feedback; tool-callable word-placement functions provided by framework</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Image inputs updated each round (grid images) plus conversation history; framework supplies updated visual state between steps</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>External grid-update functions automatically update puzzle images after each proposed word; intended as callable tools for agentic evaluation and to provide immediate feedback about length/matching/intersection consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Interactive Success Step (ISS) — number of correctly solved words before the model's first error; also WCR/LCR/ICR over rounds</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper reports that most models fail at the initial solution step on most puzzles (ISS distribution shows heavy mass at step 0/1); no strong multi-step success for evaluated LVLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Interactive setting reveals that LVLMs commonly make early errors and cannot reliably continue correct multi-step fills; grid-update feedback is a promising foundation for agentic/tool-use training but current models rarely exploit it effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Interactive mode contrasted with single-pass zero-shot CoT: models do not substantially improve via manual follow-up/self-reflection prompts alone; interactive feedback with callable functions is proposed as a better route but left for future training work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Most evaluated LVLMs fail early in the interactive process; experiments limited by four-round context window; actual agentic tool-use training not performed in this paper (framework only provides capability).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are crossword solvers <em>(Rating: 2)</em></li>
                <li>Are llms good cryptic crossword solvers? <em>(Rating: 2)</em></li>
                <li>Dr. Fill: Crosswords and an implemented solver for singly weighted csps <em>(Rating: 1)</em></li>
                <li>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning <em>(Rating: 2)</em></li>
                <li>Tree-of-thoughts: Deliberate problem solving with large language models <em>(Rating: 1)</em></li>
                <li>Mind's eye of llms: Visualization-of-thought elicits spatial reasoning in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6567",
    "paper_id": "paper-277468152",
    "extraction_schema_id": "extraction-schema-130",
    "extracted_data": [
        {
            "name_short": "o3-mini",
            "name_full": "o3-mini",
            "brief_description": "A proprietary reasoning LLM evaluated in this paper that attains the highest reported intersection consistency (ICR) on 7x7 CrossWordBench puzzles, showing strong ability to enforce crossword grid constraints.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "o3-mini",
            "model_description": "Proprietary reasoning LLM (instruction-tuned); evaluated with Chain-of-Thought prompting and test-time reasoning-effort scaling.",
            "model_size": null,
            "puzzle_name": "CrossWordBench crossword puzzles (7x7, 14x14)",
            "puzzle_type": "constraint satisfaction / multimodal spatial-textual puzzle (crossword)",
            "dataset_name": "CrossWordBench (English set; 7x7 and 14x14 grids)",
            "prompting_method": "zero-shot Chain-of-Thought (CoT); test-time scaling (varying reasoning token budget)",
            "reasoning_technique": "CoT-style internal stepwise reasoning; increased token budget for deeper chain generation (test-time scaling)",
            "internal_representation": "Text input: empty grid encoded as 2D binary array (1 = blocked, 0 = unfilled) prepended to textual clues",
            "use_of_external_tool": false,
            "external_tool_description": null,
            "evaluation_metric": "WCR (Word Coverage Rate), LCR (Letter Coverage Rate), ICR (Intersection Consistency Rate)",
            "performance": "7x7: WCR=0.587 ±0.023; LCR=0.684 ±0.021; ICR=0.891. 14x14: WCR=0.445; LCR=0.520; ICR=0.512 (values reported in paper, mean ± stderr where provided).",
            "analysis_findings": "Highest ICR indicates strong adherence to crossing-letter constraints; benefits from increased crossing letters; test-time scaling improves performance up to a point (diminishing returns beyond medium reasoning effort).",
            "ablation_comparison": "Performance improves when increasing reasoning effort from low→medium; doubling reasoning tokens beyond medium yields little additional gain. Robust to textual grid-format ablations (array vs markdown).",
            "limitations": "Still imperfect on larger (14x14) puzzles; diminishing returns from additional reasoning tokens; evaluation limited to generated puzzles (no human-authored puzzles used).",
            "uuid": "e6567.0",
            "source_info": {
                "paper_title": "CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "DeepSeek-R1",
            "name_full": "DeepSeek-R1",
            "brief_description": "An open-weight reasoning model (from the DeepSeek series) that attains the highest WCR and LCR on 7x7 CrossWordBench but slightly lower ICR than o3-mini, indicating strong answer accuracy with somewhat less intersection consistency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DeepSeek-R1",
            "model_description": "Open-weight reasoning LLM (reported in paper as a reasoning model); noted in related work as trained with reinforcement learning for reasoning emergence, but here evaluated via prompting.",
            "model_size": null,
            "puzzle_name": "CrossWordBench crossword puzzles (7x7, 14x14)",
            "puzzle_type": "constraint satisfaction / multimodal spatial-textual puzzle (crossword)",
            "dataset_name": "CrossWordBench (English set; 7x7 and 14x14 grids)",
            "prompting_method": "zero-shot Chain-of-Thought (CoT)",
            "reasoning_technique": "CoT prompting; evaluated as a reasoning-capable model (no external solver used during evaluation)",
            "internal_representation": "Text input: 2D binary array grid + textual clues",
            "use_of_external_tool": false,
            "external_tool_description": null,
            "evaluation_metric": "WCR, LCR, ICR",
            "performance": "7x7: WCR=0.646 ±0.019; LCR=0.707 ±0.017; ICR=0.678 ±0.023 (reported in paper).",
            "analysis_findings": "Top WCR and LCR among evaluated models on 7x7, but ICR slightly lower than best (o3-mini), suggesting very good per-word accuracy but less perfect enforcement of crossing-letter consistency.",
            "ablation_comparison": "Compared to non-reasoning models, shows substantial gains; performs better with higher crossing-letter counts on 7x7 grids.",
            "limitations": "Generalization gaps when models are specialized for math-style verifiable tasks (other models trained primarily on math underperform); performance drops on larger grids.",
            "uuid": "e6567.1",
            "source_info": {
                "paper_title": "CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Claude-3.7-Sonnet (text)",
            "name_full": "Claude 3.7 Sonnet (text-input, LLM)",
            "brief_description": "A proprietary LLM evaluated in text-input mode on CrossWordBench; shows improved performance when enabling 'thinking' (reflection) mode on text inputs and achieves competitive WCR/LCR though ICR remains below the best reasoning models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Claude 3.7 Sonnet",
            "model_description": "Proprietary LLM; evaluated with and without 'thinking' mode; instruction-tuned conversational model.",
            "model_size": null,
            "puzzle_name": "CrossWordBench crossword puzzles (7x7, 14x14)",
            "puzzle_type": "constraint satisfaction / multimodal spatial-textual puzzle (crossword) evaluated in text-only representation",
            "dataset_name": "CrossWordBench (English set)",
            "prompting_method": "zero-shot Chain-of-Thought (CoT); evaluation also includes 'thinking mode' variant",
            "reasoning_technique": "CoT prompting and internal self-reflection ('thinking') option available in model API",
            "internal_representation": "Text input: 2D binary-array representation of grid prepended to clues; markdown-style grid used in ablations",
            "use_of_external_tool": false,
            "external_tool_description": null,
            "evaluation_metric": "WCR, LCR, ICR",
            "performance": "Text-input (7x7): reported example values include WCR≈0.617 ±0.019; LCR≈0.712 ±0.017; ICR≈0.754 ±0.021 (paper reports multiple runs and modes).",
            "analysis_findings": "Thinking mode improved Claude on text inputs across metrics (indicating benefit from additional internal token generation in text-only setting). Reasoning helps adherence to crossing-letter constraints.",
            "ablation_comparison": "Thinking mode helps in text-input evaluation but, notably, enabling thinking mode with image inputs reduced grid-parsing and puzzle-solving performance (suggesting trade-offs between reasoning-token generation and image-processing).",
            "limitations": "When provided image inputs, thinking mode can harm LVLM image processing; model still inferior to top reasoning models (e.g., o3-mini) on ICR for complex grids.",
            "uuid": "e6567.2",
            "source_info": {
                "paper_title": "CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Claude-3.7-Sonnet (image/LVLM)",
            "name_full": "Claude 3.7 Sonnet (image-input / LVLM)",
            "brief_description": "The same Claude 3.7 Sonnet model evaluated in LVLM (image+clue) mode; demonstrates strong across-word extraction but much weaker down-word/OCR extraction, illustrating modality-specific limitations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Claude 3.7 Sonnet (LVLM image mode)",
            "model_description": "Proprietary vision-language model variant of Claude 3.7 Sonnet; ingests image containing grid and clues.",
            "model_size": null,
            "puzzle_name": "CrossWordBench crossword puzzles (image-input; 7x7)",
            "puzzle_type": "multimodal constraint satisfaction (visual + textual)",
            "dataset_name": "CrossWordBench (English set; image inputs)",
            "prompting_method": "image zero-shot Chain-of-Thought (CoT) prompt; Interactive Mode experiments also considered",
            "reasoning_technique": "Image-grounded CoT prompting; interactive step-by-step mode supported by updated grid images",
            "internal_representation": "Image embedding of puzzle (grid + clues) as LVLM input; interactive mode uses updated grid images generated by evaluation framework",
            "use_of_external_tool": true,
            "external_tool_description": "Evaluation framework uses external grid-update functions to produce updated grid images between rounds; these functions can serve as callable tools in agentic settings to provide feedback (e.g., length/consistency checks).",
            "evaluation_metric": "Grid parsing WCR (Across/Down), puzzle WCR/LCR/ICR, ISS (Interactive Success Step) in interactive experiments",
            "performance": "Grid parsing (example reported): Across=0.954 ±0.009; Down=0.760 ±0.018 (Table 3). Puzzle-solving WCR with image inputs is substantially lower than text-input variant (see paper Table 2), and enabling thinking mode reduced grid parsing and WCR for image inputs.",
            "analysis_findings": "LVLM performance strongly correlated with grid-parsing (r ≈ 0.94). Models extract Across words more accurately than Down words (vertical OCR weakness). Enabling thinking mode can divert tokens from image processing and harm performance.",
            "ablation_comparison": "Two LVLM input formats (combined image with grid+clues vs separate grid image + text clues) showed no significant WCR difference, indicating format not primary cause of poor LVLM puzzle performance. Thinking mode harms image-input performance while helping text-input performance.",
            "limitations": "OCR and spatial extraction for down words is a key failure mode; LVLMs struggle to build spatially coherent grid representations from images; puzzle-solving constrained by visual processing rather than reasoning capability alone.",
            "uuid": "e6567.3",
            "source_info": {
                "paper_title": "CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o-2024-11-20",
            "brief_description": "A proprietary multimodal model evaluated on CrossWordBench; shows moderate WCR on text or combined inputs but lower ICR and notable asymmetry in grid parsing (good across, poor down).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o-2024-11-20",
            "model_description": "Proprietary multimodal LLM/LVLM from OpenAI; evaluated in both text and image input modes.",
            "model_size": null,
            "puzzle_name": "CrossWordBench crossword puzzles (7x7)",
            "puzzle_type": "constraint satisfaction / multimodal",
            "dataset_name": "CrossWordBench (English set)",
            "prompting_method": "zero-shot Chain-of-Thought (CoT) prompting; image zero-shot CoT for LVLM input",
            "reasoning_technique": "CoT prompting; no external solver",
            "internal_representation": "Text: 2D binary-array grid + textual clues. Image: grid+clues embedded in single image.",
            "use_of_external_tool": false,
            "external_tool_description": null,
            "evaluation_metric": "WCR, LCR, ICR; grid parsing Across/Down WCR",
            "performance": "Puzzle-solving (7x7): WCR≈0.410 ±0.018; LCR≈0.472 ±0.018; ICR≈0.288 ±0.019. Grid parsing (Table 3 example): Across=0.886 ±0.014; Down=0.448 ±0.024.",
            "analysis_findings": "Substantially better at extracting across words (horizontal) than down words (vertical), mirroring LVLM OCR limitations; overall LVLM puzzle performance strongly tied to grid parsing accuracy.",
            "ablation_comparison": "Image vs text input differences observed (image inputs tend to reduce performance compared to text-grid representation).",
            "limitations": "Low ICR indicates poor intersection consistency; poor vertical OCR reduces puzzle-solving effectiveness.",
            "uuid": "e6567.4",
            "source_info": {
                "paper_title": "CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Pixtral-Large-Instruct",
            "name_full": "Pixtral-Large-Instruct-2411",
            "brief_description": "An open-weight LVLM included in the evaluation; achieves the best WCR among open-weight LVLMs in the study but still lags behind proprietary LLMs and reasoning models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Pixtral-Large-Instruct-2411",
            "model_description": "Open-weight vision-language model (instruction-tuned LVLM) evaluated on CrossWordBench image inputs.",
            "model_size": null,
            "puzzle_name": "CrossWordBench crossword puzzles (7x7)",
            "puzzle_type": "multimodal constraint satisfaction (crossword)",
            "dataset_name": "CrossWordBench (English set)",
            "prompting_method": "image zero-shot Chain-of-Thought (CoT)",
            "reasoning_technique": "CoT prompting; no external solver",
            "internal_representation": "Image containing grid and clues",
            "use_of_external_tool": false,
            "external_tool_description": null,
            "evaluation_metric": "WCR, LCR, ICR; grid parsing Across/Down WCR reported separately",
            "performance": "7x7 puzzle-solving: WCR=0.297 ±0.015; LCR=0.338 ±0.014; ICR=0.198 ±0.014. Grid parsing (Across/Down): Across≈0.753 ±0.022; Down≈0.361 ±0.022 (Table 3).",
            "analysis_findings": "Best among open-weight LVLMs but still much weaker than LLMs; strong dependence of puzzle-solving on grid-parsing accuracy; across extraction much stronger than down extraction.",
            "ablation_comparison": "Providing OCR-extracted clues to LLMs showed that loss of spatial grid information (from OCR only) limits LLMs unless positional hints are added; Pixtral still underperforms LLMs due to visual-spatial challenges.",
            "limitations": "Struggles with vertical word extraction (Down), low intersection consistency (ICR); LVLM visual parsing is the bottleneck rather than pure language reasoning.",
            "uuid": "e6567.5",
            "source_info": {
                "paper_title": "CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "QwQ-32B",
            "name_full": "QwQ-32B",
            "brief_description": "An open-weight reasoning-capable LLM (32B parameters as indicated by name) evaluated on CrossWordBench that shows moderate word-level accuracy among reasoning LLMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "QwQ-32B",
            "model_description": "Open-weight LLM described in paper as a reasoning model; name indicates 32B-parameter scale (paper refers to it explicitly as 'QwQ-32B').",
            "model_size": "32B",
            "puzzle_name": "CrossWordBench crossword puzzles (7x7)",
            "puzzle_type": "constraint satisfaction",
            "dataset_name": "CrossWordBench (English set)",
            "prompting_method": "zero-shot Chain-of-Thought (CoT)",
            "reasoning_technique": "CoT prompting (no external solver)",
            "internal_representation": "Text input (2D binary array grid + clues)",
            "use_of_external_tool": false,
            "external_tool_description": null,
            "evaluation_metric": "WCR reported; LCR/ICR for some models but paper summarizes WCRs for QwQ",
            "performance": "Reported approximate WCR ≈ 0.347 on 7x7 puzzles (paper states QwQ-32B WCR ≈ 0.347).",
            "analysis_findings": "Moderate performance among reasoning LLMs; observed to reason consistently in Chinese when solving Chinese puzzles (multilingual behavior noted).",
            "ablation_comparison": "Per paper, reasoning models (including QwQ) improve with more crossing letters; performance declines on larger 14x14 grids.",
            "limitations": "Not top-performing among reasoning LLMs; struggles with larger grids and maintaining intersection consistency at scale.",
            "uuid": "e6567.6",
            "source_info": {
                "paper_title": "CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Open-Reasoner-Zero-32B",
            "name_full": "Open-Reasoner-Zero-32B",
            "brief_description": "A reasoning-focused model (32B) evaluated on CrossWordBench that performs poorly across metrics, suggesting training focused on mathematical/verifiable tasks does not transfer to spatial-linguistic crossword solving.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Open-Reasoner-Zero-32B",
            "model_description": "Open-weight reasoning model; paper notes its training emphasis on mathematical reasoning and verifiable answers.",
            "model_size": "32B",
            "puzzle_name": "CrossWordBench crossword puzzles (7x7)",
            "puzzle_type": "constraint satisfaction",
            "dataset_name": "CrossWordBench (English set)",
            "prompting_method": "zero-shot Chain-of-Thought (CoT)",
            "reasoning_technique": "CoT prompting (no external solver used)",
            "internal_representation": "Text input grid encoded as 2D binary array + clues",
            "use_of_external_tool": false,
            "external_tool_description": null,
            "evaluation_metric": "WCR, LCR, ICR",
            "performance": "7x7: WCR=0.139 ±0.010; LCR=0.204 ±0.010; ICR=0.184 ±0.012 (reported in paper).",
            "analysis_findings": "Performs poorly across metrics, indicating limited transfer from math/verifiable-task training to crossword-style spatial-textual reasoning.",
            "ablation_comparison": "Paper contrasts this model with other reasoning LLMs to highlight training-domain limitations; does not benefit from crossing-letter density like other reasoning models.",
            "limitations": "Failure mode likely due to training focus (mathematical reasoning) not matching the multimodal spatial-linguistic demands of crossword puzzles.",
            "uuid": "e6567.7",
            "source_info": {
                "paper_title": "CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Interactive Mode (LVLM agentic eval)",
            "name_full": "Interactive Mode / Agentic Evaluation (CrossWordBench)",
            "brief_description": "An evaluation setting introduced in the paper that requires step-by-step puzzle solving with updated grid images and introduces the Interactive Success Step (ISS) metric to measure how many words a model correctly fills before its first error.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LVLMs (various) in Interactive Mode",
            "model_description": "Not a single model but an evaluation protocol applied to LVLMs where models solve puzzles across multiple turns with updated grid images provided by the benchmark's functions.",
            "model_size": null,
            "puzzle_name": "CrossWordBench interactive puzzles (7x7 English)",
            "puzzle_type": "sequential constraint satisfaction / agentic multimodal task",
            "dataset_name": "CrossWordBench (interactive evaluation on English 7x7)",
            "prompting_method": "Interactive zero-shot CoT with step-by-step responses; four-round conversation history (context window limited)",
            "reasoning_technique": "Stepwise CoT with external grid update feedback; tool-callable word-placement functions provided by framework",
            "internal_representation": "Image inputs updated each round (grid images) plus conversation history; framework supplies updated visual state between steps",
            "use_of_external_tool": true,
            "external_tool_description": "External grid-update functions automatically update puzzle images after each proposed word; intended as callable tools for agentic evaluation and to provide immediate feedback about length/matching/intersection consistency.",
            "evaluation_metric": "Interactive Success Step (ISS) — number of correctly solved words before the model's first error; also WCR/LCR/ICR over rounds",
            "performance": "Paper reports that most models fail at the initial solution step on most puzzles (ISS distribution shows heavy mass at step 0/1); no strong multi-step success for evaluated LVLMs.",
            "analysis_findings": "Interactive setting reveals that LVLMs commonly make early errors and cannot reliably continue correct multi-step fills; grid-update feedback is a promising foundation for agentic/tool-use training but current models rarely exploit it effectively.",
            "ablation_comparison": "Interactive mode contrasted with single-pass zero-shot CoT: models do not substantially improve via manual follow-up/self-reflection prompts alone; interactive feedback with callable functions is proposed as a better route but left for future training work.",
            "limitations": "Most evaluated LVLMs fail early in the interactive process; experiments limited by four-round context window; actual agentic tool-use training not performed in this paper (framework only provides capability).",
            "uuid": "e6567.8",
            "source_info": {
                "paper_title": "CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are crossword solvers",
            "rating": 2,
            "sanitized_title": "language_models_are_crossword_solvers"
        },
        {
            "paper_title": "Are llms good cryptic crossword solvers?",
            "rating": 2,
            "sanitized_title": "are_llms_good_cryptic_crossword_solvers"
        },
        {
            "paper_title": "Dr. Fill: Crosswords and an implemented solver for singly weighted csps",
            "rating": 1,
            "sanitized_title": "dr_fill_crosswords_and_an_implemented_solver_for_singly_weighted_csps"
        },
        {
            "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
            "rating": 2,
            "sanitized_title": "deepseekr1_incentivizing_reasoning_capability_in_llms_via_reinforcement_learning"
        },
        {
            "paper_title": "Tree-of-thoughts: Deliberate problem solving with large language models",
            "rating": 1,
            "sanitized_title": "treeofthoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Mind's eye of llms: Visualization-of-thought elicits spatial reasoning in large language models",
            "rating": 1,
            "sanitized_title": "minds_eye_of_llms_visualizationofthought_elicits_spatial_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.021409249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation
11 Aug 2025</p>
<p>Jixuan Leng jixuanl@cs.cmu.edu 
Chengsong Huang 
Langlin Huang 
Bill Yuchen Lin 
William W Cohen 
Haohan Wang 
Jiaxin Huang jiaxinh@wustl.edu 
Cmu 
Wustl 
Uw 
CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation
11 Aug 20258972061AC92B8448E864223B0652B10EarXiv:2504.00043v2[cs.CL]
Existing reasoning evaluation frameworks for Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) predominantly assess either text-based reasoning or vision-language understanding capabilities, with limited dynamic interplay between textual and visual constraints.To address this limitation, we introduce CrossWordBench, a benchmark designed to evaluate the reasoning capabilities of both LLMs and LVLMs through the medium of crossword puzzles-a task requiring multimodal adherence to semantic constraints from text-based clues and intersectional constraints from visual grid structures.CrossWordBench leverages a controllable puzzle generation framework that produces puzzles in two formats (text and image), supports adjustable difficulty through prefill ratio control, and offers different evaluation strategies, ranging from direct puzzle solving to interactive modes suitable for agentic evaluation.Our extensive evaluation of over 20 models reveals that reasoning LLMs substantially outperform non-reasoning models by effectively leveraging crossing-letter constraints.We further demonstrate that LVLMs struggle with the task, showing a strong correlation between their puzzle-solving performance and grid-parsing accuracy.Our findings highlight the limitations of the reasoning capabilities of current LLMs and LVLMs, and provide an effective approach for creating multimodal constrained tasks for future evaluations.</p>
<p>Introduction</p>
<p>Figure 1: Performance gap between top-performing reasoning LLMs and non-reasoning LLMs/LVLMs on CrossWordBench.Reasoning LLMs achieve better overall performance and adhere more effectively to crossing-letter constraints than non-reasoning LLMs/LVLMs.Large reasoning models (e.g., OpenAI-o1 (Jaech et al., 2024) and DeepSeek-R1 (Guo et al., 2025)) have made exceptional progress on reasoning benchmarks including math problem solving (Veeraboina, 2023), coding (Austin et al., 2021) and commonsense reasoning (Clark et al., 2018).However, existing evaluation benchmarks for Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) (e.g., Visual Question Answering (Yue et al., 2023)) mostly focus on text-based reasoning or vision-language understanding, lacking dynamic interplay between textual and visual constraints that characterizes real-world problem solving.Consequently, evaluating multimodal reasoning capabilities-particularly on reasoning tasks requiring both textual and visual constraints-remains challenging.</p>
<p>Crossword puzzles, a classic grid-based task in which horizontal ("Across") words and vertical ("Down") words must be filled in based on text-based clues, provide a unique testbed for such evaluations.They pose two distinct challenges: (1) question answering for each text-based clue, which may admit multiple correct solutions, and (2) visual constraint satisfaction, which requires precise letter alignment at the intersections of Across and Down entries.Prior crossword datasets (Efrat et al., 2021;Rozner et al., 2021;Kulshreshtha et al., 2022;Chen et al., 2025) often suffer from their reliance on static and copyrighted online news sources and adopt a text-centric formulation, thereby neglecting the visual structure.</p>
<p>In this paper, we introduce CrossWordBench, a controllable and scalable benchmark for evaluating the reasoning capabilities of both LLMs and LVLMs.CrossWordBench collects data and generates puzzles from three sources: (1) multilingual word-clue pairs from public repositories, (2) dictionary-based definitions, and (3) adapted question-answer pairs from existing benchmarks (e.g., CommonsenseQA (Talmor et al., 2018)) where the answers are open-ended or unconstrained.By representing grids and clues in different formats, as shown in Figure 2, CrossWordBench facilitates the evaluation of both model types.It supports two evaluation modes: a direct puzzle-solving mode, which generates one-round responses using zero-shot Chain-of-Thought (CoT) prompts, and an interactive mode for step-by-step puzzle-solving, where grid update functions can provide intermediate visual outputs for follow-ups, thereby serving as a foundation for evaluating agents using function calling.</p>
<p>We evaluate over 20 state-of-the-art models, including both proprietary models (e.g., GPT-4o (Hurst et al., 2024) and Claude 3.7 Sonnet (Anthropic, 2024)) and open-weight models (e.g., DeepSeek-R1 (Guo et al., 2025) and Pixtral-Large-Instruct (Agrawal et al., 2024)).Our evaluation yields several notable findings: (1) LVLMs perform significantly worse than LLMs on CrossWordBench (as shown in Figure 1), and they struggle in OCR in vertical ("Down") word extraction.In fact, their puzzle-solving performance strongly correlates with their grid-parsing accuracy (r = 0.94).(2) Reasoning LLMs outperform non-reasoning models, and benefit from both test-time scaling and increased crossing-letter constraints.</p>
<p>(3) Even puzzles derived from saturated benchmarks (e.g., CommonsenseQA) remain challenging, highlighting the significance of structural constraints in reasoning evaluation.</p>
<p>Related Work</p>
<p>LLMs and LVLMs Reasoning.Recent research on the reasoning capabilities of LLMs (Ahn et al., 2024;Huang &amp; Chang, 2022;Kojima et al., 2022;Plaat et al., 2024;Jaech et al., 2024;Guo et al., 2025;OpenAI, 2025;Huang et al., 2022) has led to the development of various approaches, including prompting-based methods (Wei et al., 2022;Yao et al., 2023;Besta et al., 2024;Chen et al., 2022) that guide LLMs through intermediate reasoning steps for solving complex problems, fine-tuning methods that train LLMs on long reasoning chains (Ye et al., 2025;Muennighoff et al., 2025;Zhao et al., 2025), and test-time scaling via self-refinement or the use of a verifier (Setlur et al., 2024;Feng et al., 2023;Wang et al., 2023;Zhang et al., 2024a;Uesato et al., 2022;Huang et al., 2025a).Moreover, a recent study from Deepseek-R1 (Guo et al., 2025) demonstrates that reinforcement learning (RL) with verifiable rewards facilitates the emergence of complex thinking processes in LLMs.Several studies have explored reasoning in the multimodal domain by constructing CoT data for fine-tuning (Xu et al., 2024) and developing verifiable problems for RL (Yang et al., 2025;Huang et al., 2025b).While these approaches have demonstrated success, existing evaluation datasets remain largely restricted to math problems (Wang et al., 2024;Lu et al., 2023;Zhang et al., 2024b).</p>
<p>Crossword Puzzles in Language Model Evaluation.Crossword puzzles have long been a focus of research in natural language processing (NLP), particularly before the advent of LLMs.Early approaches typically employed constraint satisfaction algorithms augmented by external knowledge bases.Notable examples include systems such as Proverb (Littman et al., 2002), Dr. Fill (Ginsberg, 2011), and specialized models such as the Berkeley Crossword Solver (Wallace et al., 2022), which incorporate a fine-tuned BERT (Devlin et al., 2019) and belief propagation.More recent studies have leveraged LLMs to address crossword puzzles through techniques including fine-tuning (Efrat et al., 2021;Rozner et al., 2021;Sadallah et al., 2024), prompting strategies such as Tree-of-Thoughts (Yao et al., 2023), or integration with search algorithms (Saha et al., 2024), demonstrating the potential of LLMs for crosswords.</p>
<p>Several datasets for crossword puzzles have been proposed, covering both English (Efrat et al., 2021;Rozner et al., 2021;Kulshreshtha et al., 2022;Chen et al., 2025) and Arabic (Zeinalipour et al., 2025).However, one significant limitation of these datasets and approaches is that they rely on data from online news sources (Efrat et al., 2021;Rozner et al., 2021;Kulshreshtha et al., 2022;Chen et al., 2025) and often formulate crossword solving as a question-answering (QA) task (Sadallah et al., 2024;Efrat et al., 2021;Rozner et al., 2021;Yao et al., 2023), thereby overlooking the fundamental constraint-based nature of the problem.Moreover, all of them treat crossword solving as a text-based task, despite the inherently visual nature of crossword grids, leaving a gap in extending crossword puzzles for evaluating LVLMs.In this work, we address these limitations by introducing CrossWordBench, a framework that features controllable puzzle generation and extends evaluation to LVLMs.Online sources, including The New York Times and The Los Angeles Times, provide complete crossword puzzles.Nevertheless, directly utilizing these puzzles presents several disadvantages.(1) Copyright restrictions may limit their usage, and their online availability increases the probability that they have been incorporated into the pretraining datasets of contemporary models.(2) Online puzzles are typically static-with predefined words, themes, grid sizes, and strict formatting (e.g., black square symmetry)1 -which not only limits their adaptability for diverse benchmark generation, but also imposes arbitrary formatting constraints that do not yield meaningful benefits for evaluation.Prior research (Mirzadeh et al., 2024) has shown that even minor modifications to questions can significantly affect model performance, particularly when performance on the original version is near saturation.(3) The static characteristics of these puzzles restrict the range of potential evaluation strategies.To overcome these limitations, we propose a two-stage benchmark construction strategy:</p>
<p>Benchmark Curation</p>
<p>Word-Clue Pairs Curation.We compile word-clue pairs from three source categories: (1) Public repositories: We selectively extract individual, often cryptic, word-clue pairs from public online repositories with samples in both English 2 and Chinese 3 .(2) Dictionary-Based Pairs: We collect standard English words and use their dictionary definitions from NLTK WordNet4 as clues, referring to this category as English Simple.(3) Adapted Benchmark Data: We demonstrate that existing LLM benchmarks, which are typically designed for open-ended or multiple-choice questions without strict formatting constraints, can be transformed into word-clue pairs.In our study, we filter the CommonsenseQA (Talmor et al., 2018) training set for single-word answers and use the associated questions as clues.</p>
<p>While the possibility of data contamination may still exist for these word-clue pairs, the variations in grid design can still yield distinct crossword puzzles, and as demonstrated in Section 4.5, even when incorporating extensively tuned data such as CommonsenseQA training set, the resulting puzzles remain remarkably challenging for both LLMs and LVLMs.Puzzle Generation.After collecting word-clue pairs, we implement an automatic pipeline to generate puzzles in both text and image formats, as illustrated in Figure 2. The pipeline enables (1) puzzle difficulty control by adjusting grid sizes, and (2) incremental word placement functions, which facilitate the interactive evaluation mode described in Section 4.4.</p>
<p>The generation framework leverages a heuristic scoring function that encourages desirable properties such as a high number of horizontal and vertical word intersections.Postprocessing filters are applied to discard puzzles with low clue counts or high blocked cell ratios, thereby enhancing overall quality and structural coherence.These heuristics and filters, in combination with grid size adjustments, help control puzzle complexity.To preserve the category integrity, we ensure that clues do not overlap within each category5 .</p>
<p>Puzzle Quality.While CrossWordBench does not include human-created puzzles, its design is inspired by them.Table 1 reports overall dataset statistics-including clue count, average word length, and blocked cell ratio-which serve as proxies for puzzle quality and difficulty.</p>
<p>Compared human-created puzzles-for example, those used LR 2 Bench (Chen et al., 2025), which were sourced from outlets such as The Los Angeles Times and Vulture-our generated puzzles exhibit comparable difficulty when evaluated using similar word-level metrics.Our focus on automatically generated puzzles is motivated by the need for scalable, controllable, and diverse evaluation across subjects and formats.This enables systematic variation in difficulty through grid size, prefill ratio, and clue selection; supports step-by-step reasoning via an interaction mode; and facilitates fine-grained metric analysis beyond overall accuracy.</p>
<p>Experiment</p>
<p>In the following sections, we present an extensive empirical evaluation of CrossWordBench across multiple different model architectures and analyze their performance characteristics.</p>
<p>Experimental Setup</p>
<p>Evaluation Data and Metrics.For the main experiments, we evaluate models on the English set with three metrics to assess answer accuracy and adherence to crossword constraints.</p>
<p>• Word Coverage Rate (WCR): word-level accuracy, percentage of correctly solved words.</p>
<p>• Letter Coverage Rate (LCR): letter-level accuracy, percentage of correct letter placements.</p>
<p>• Intersection Consistency Rate (ICR): the internal consistency of the model's answers at intersections where across and down words overlap, defined as:
ICR = 1 |I| ∑ (a,d,j,k)∈I 1{a[j] = d[k]}(1)
where I denotes the set of all intersections, where each tuple (a, d, j, k) indicates the jth letter of the across word a overlaps with the kth letter of the down word d.This metric reflects whether models correctly adhere to the grid structural constraints of a puzzle.</p>
<p>Evaluated Models.We evaluate a collection of proprietary and open-weight models, including both LVLMs and LLMs.For proprietary models, we consider state-of-the-art models such as GPT-4o (Hurst et al., 2024) and Claude 3.7 Sonnet (Anthropic, 2024) (with and without thinking mode).For open-weight models, our selections range from 3B to 124B parameters, such as the Qwen Series (Team, 2024;Bai et al., 2025) and Pixtral-Large-Instruct-2411 (Agrawal et al., 2024) for LVLMs.For LLMs, we include both reasoning models such as the Deepseek Series (Liu et al., 2024;Guo et al., 2025) and non-reasoning models such as Llama series (Dubey et al., 2024).The full list of models is shown in Table 2.We set decoding temperature to 0 for non-reasoning models for consistency, 0.6 for reasoning models based on commonly recommended settings, and 1.0 for certain proprietary models (e.g., used by Claude 3.7-Sonnet with Thinking).Further generation details are provided in Appendix D.3.</p>
<p>Input Prompt Templates and Output Response Parsing.For the main evaluation, we adopt the zero-shot Chain-of-Thought (CoT) (Wei et al., 2022) prompting strategy.In LVLM evaluation, the clues and the grid are both embedded within an image.In LLM evaluation, the grid is represented as a 2D binary array, with 1 indicating a blocked cell and 0 representing an unfilled cell, and is prepended to text clues in the prompt.To extract answers from responses, we leverage the structured output capabilities of o3-mini6 to convert raw model responses into JSON format by generating dynamic Pydantic models.Detailed prompt templates and implementation details are listed in Appendix D.4 and D.2.</p>
<p>Main Results</p>
<p>Reasoning LLMs substantially outperform conventional ones across metrics, with notable improvements in ICR, as shown in Table 2.In particular, among reasoning LLMs, o3-mini achieves an ICR of 0.891 on 7x7 grids, demonstrating a strong ability to interpret and enforce grid constraints.Although DeepSeek-R1 attains the highest WCR and LCR on 7x7 grids, its ICR is slightly lower than that of o3-mini.Other reasoning models, such as QwQ-32B and R1-Distilled-Llama-70B, show moderate performance, with WCRs of approximately 0.347 and 0.387, respectively.One notable outlier is Open-Reasoner-Zero-32B, which performs poorly across all metrics, indicating that it does not effectively leverage grid constraints for reasoning.This suggests that its training-primarily focused on mathematical reasoning-does not generalize well to tasks requiring spatial and linguistic integration, thereby highlighting a key limitation in the training strategies of these reasoning models.We reveal two significant patterns: (1) As shown in Table 3, LVLMs extract Across words more accurately than Down words, highlighting OCR limitations.(2) Figure 3 demonstrates a strong positive correlation between grid parsing and overall puzzlesolving performance (both measured by WCR).Notably, when enabling thinking mode, Claude 3.7 Sonnet exhibits lower grid parsing performance, which aligns with its lower puzzle-solving performance.We hypothesize that the additional token generation in thinking mode diverts attention from image processing.These findings highlight the critical role of precise spatialtextual interpretation for future LVLM design.For full results, please refer to Appendix C.2.</p>
<p>Agentic Evaluation Setting: Interactive Mode for LVLMs</p>
<p>Motivated by recent research on visual-of-thoughts (Wu et al., 2024;Li et al., 2025), we introduce a new evaluation setting for LVLMs, referred to as Interactive Mode.This setting leverages the ability of CrossWordBench to automatically generate updated grid images.</p>
<p>Interactive Mode requires step-by-step puzzle solving rather than completing the entire puzzle in a single pass.Specifically, the implementation of the controllable generation framework allows for updating grid images each time a model provides an answer.</p>
<p>Figure 4: CDF of ISS on 7x7 English Puzzles.</p>
<p>We maintain a four-round conversation history due to context window limitations.We introduce Interactive Success Step (ISS) to quantify how many words a model correctly solves before making its first error.</p>
<p>Figure 4 shows the cumulative distribution of ISS for each model-most models fail at the initial solution step on most puzzles.</p>
<p>Unlike the visual-of-thoughts approach, which focuses on generating intermediate reasoning plots, we employ external functions to update the grid.This establishes a foundation for evaluating LVLMs in agentic settings, where the word placement functions can serve as callable tools for the model and provide error feedback to guide the model in refining its responses-for example, by indicating whether a proposed word matches the expected length and whether its intersecting characters align with previously filled cells.We extend zero-shot CoT prompting evaluation to Chinese, dictionary-based, and benchmark-adapted word-clue pairs in CrossWordBench, with results shown in Table 4.</p>
<p>Beyond English: Evaluations on Multilingual, Dictionary-based, and Adapted Data</p>
<p>Reasoning models outperform conventional ones across metrics, and the performance gap between open-weight and proprietary models is reduced.o3-mini maintains a high ICR across all three datasets, consistent with the main evaluation results.Furthermore, the performance gap between open-weight and proprietary reasoning models is reduced, possibly due to the simplicity of these tasks and differences in training data.Interestingly, when solving Chinese puzzles, we observe that QwQ-32B consistently reasons in Chinese.</p>
<p>Effectiveness of grid construction: puzzles constructed with</p>
<p>CommonsenseQA training data remain challenging.Despite CommonsenseQA having saturated performance on most LLMs, its training set continues to pose significant challenges for reasoning models.For example, o3-mini achieves the highest WCR of 0.812, yet it still falls short of perfection.In contrast, the best-performing non-reasoning LLMs such as GPT-4o and open-weight LVLMs Pixtral-Large-Instruct obtain WCR of 0.524 and 0.439, respectively.These results highlight the effectiveness of our grid construction strategy in repurposing existing benchmark data.</p>
<p>Analysis</p>
<p>In this section, we provide an analysis of model behavior on CrossWordBench through the impact of structural constraints and reasoning mechanisms, and discuss future applications.LLMs exhibit robustness to variations in grid text representation.In the text-only evaluation setting, the empty crossword puzzle grid is represented as a 2D binary array, where 1 denotes a blocked cell and 0 denotes an unfilled cell.This array is then prepended to the prompt containing clues.To evaluate the impact of different formatting choices, we test an alternative markdown-style representation in this section, with • indicates unfilled cells andindicates blocked cells.An example of the two formats is provided in Appendix D.5 and Figure 19.</p>
<p>Grid Format Ablations: LLMs Robustness to Textual Grid Representations</p>
<p>Due to space constraints in the main text, we defer additional ablation studies on LVLM input formats and few-shot prompting to Appendix C.6 and C.8, respectively.We also demonstrate how controlling prefill ratio can be used to adjust task difficulty.We encourage readers to refer to these appendices for a comprehensive analysis and further discussion.</p>
<p>Crossing Letters: Reasoning LLMs Improve with More Intersections</p>
<p>Figure 5: Crossing letter counts and average WCR on 7x7 English puzzles across LLMs.</p>
<p>Average word accuracy on reasoning models increases with crossing letter count.</p>
<p>Figure 5 presents the average WCR for each range of crossing letter counts, divided into three groups, across five reasoning and nine non-reasoning LLMs.We observe that reasoning models exhibit increasing accuracy with a greater number of letter intersections, whereas this trend is not observed for nonreasoning LLMs.This observation aligns with our main experiment results, in which reasoning models tend to exhibit higher ICRs, suggesting that they benefit from effectively utilizing grid constraints for solution space reduction.However, we do not observe the same pattern on 14x14 puzzles-likely because puzzles with larger grids are substantially more difficult even for reasoning models.We provide detailed results in Appendix C.7.To further isolate the effect of crossing-letter density within a constrained grid, we perform a quasi-ablation study on 7x7 puzzles by stratifying puzzles into low, medium, and high-density groups based on the average number of crossing letters per word, as shown in Table 6.Overall, within different densities, reasoning LLMs demonstrate improved WCR with a greater number of crossing letters, whereas non-reasoning LLMs show relatively stable performance.</p>
<p>Self-Reflection: Limited Impact on Crossword Puzzle Solving</p>
<p>Figure 6: Self-reflection improvements on 7x7 English puzzles across metrics.Top: reasoning LLMs.Bottom: non-reasoning LVLMs.</p>
<p>Backtracking and verifying previously filled answers are essential components of effective crossword puzzle solving.To evaluate the effect of self-reflection, we include a manually crafted follow-up query that prompts models to revisit their previous zero-shot CoT responses (please see Appendix D.4 for details).As shown in Figure 6, we observe no measurable performance improvement for reasoning LLMs or non-reasoning LVLMs.This find suggests that additional interactions with manual prompting alone are insufficient to enhance reasoning capabilities for puzzle solving.</p>
<p>Test-Time Scaling: Diminishing Returns on Puzzle Performance</p>
<p>Figure 7: o3-mini performance on 7x7 English puzzles under three levels of reasoning efforts.</p>
<p>To examine the effect of test-time scaling on CrossWordBench, we evaluate o3-mini on 7x7 English puzzles by varying its reasoning effort, which controls the number of reasoning tokens generated during inference.As shown in Figure 7, increasing the effort from low to medium yields a substantial performance improvement across all three metrics.However, further doubling the reasoning tokens provides no significant additional gains, indicating diminishing returns.</p>
<p>Discussion: Verifiable Crossword for Future Multimodal RL Training</p>
<p>Table 2 shows that reasoning models such as Open-Reasoner-Zero-32B, which are primarily trained on mathematical problems, exhibit limited generalization to CrossWordBench.This highlights the limitations of relying predominantly on math-based tasks with verifiable answers for reinforcement learning in more complex reasoning settings.As multimodal reasoning advances, a significant challenge persists: the lack of multimodal environments with verifiable answers suitable for rule-based reinforcement learning.We propose crossword puzzles as a compelling alternative, owing to their unique combinations of verifiability, multimodal structure, and goal-oriented task design.Crossword puzzles are also suitable for multi-turn training, where each word fill can represent a discrete, observable decision.</p>
<p>Our framework supports this setting by providing word placement functions that dynamically update the puzzle grid in both text and image formats, allowing seamless integration with multimodal agents and enabling tool-use training.We leave the exploration of training multimodal agents on crossword puzzles with real-time function feedback for future work.</p>
<p>Conclusion</p>
<p>This paper introduces CrossWordBench, a benchmark designed to evaluate the multimodal reasoning capabilities of both LLMs and LVLMs using crossword puzzles, which uniquely integrate text-based clues and visual constraints.Our extensive evaluation of over 20 models shows that reasoning models substantially outperform non-reasoning counterparts and can benefit from increased crossing-letter constraints.Additionally, we find a strong correlation between puzzle-solving performance and grid-parsing accuracy in LVLMs.Even puzzles derived from saturated benchmarks remain challenging, emphasizing the necessity of structural complexity in rigorous reasoning evaluation.This work paves the way for improving future multimodal RL training where interplay between modalities is essential.</p>
<p>A Acknowledgment</p>
<p>We thank all reviewers for their feedback, which contributed to improving this work.</p>
<p>B Limitations</p>
<p>We propose CrossWordBench to evaluate the reasoning capabilities of both LLMs and LVLMs.Unlike prior work, CrossWordBench does not incorporate any human-authored puzzles from online sources.Although our generation process is carefully designed to preserve key structural properties-such as clue distribution, word length, and blocked cell ratios-as summarized in Table 1, and the generated puzzles exhibit difficulty levels comparable to those of human-created puzzles when evaluated using standard word-level metrics, manual verification of puzzle quality could be helpful.Although our framework is also capable of generating crossword puzzles for potential use in training, this paper focuses solely on evaluation and leaves the exploration of reinforcement learning for future work.</p>
<p>C More Results and Analysis</p>
<p>C.1 Behavior Analysis</p>
<p>To better analyze the errors made by models in puzzle-solving, we define two metrics: global length error and local length error.The global length error metric compares the number of words produced by the model with those in the reference answer list, assessing whether the model supplies an answer for every clue in the puzzle.In contrast, the local length error metric compares the length of each individual word to its corresponding reference, thereby quantifying the model's adherence to the grid constraints.Table 7 shows that even the best-performing reasoning models, such as Claude 3.7 Sonnet with thinking mode and DeepSeek R1, exhibit global length errors on two puzzles.Almost all models-with the exception of o3-mini, which demonstrates the highest ICR-commit a significant number of local errors.Moreover, we observe that all global length errors arise from models providing fewer answers than required, i.e., failing to address some clues.In contrast, most local length errors are due to models generating answers that exceed the expected word length.</p>
<p>C.2 Complete Results on Grid Parsing Performance</p>
<p>In Section 4.3, we demonstrate a positive correlation between LVLM's grid-parsing accuracy and their puzzle-solving performance.We also provide several examples illustrating the limitations of LVLMs in extracting horizontal words.Here, we present a complete table of all models shown in Table 3, as summarized in Table 8, where a similar trend is observed.</p>
<p>C.3 Complete Results on Extended Data</p>
<p>In this section, we present additional results for evaluations on the extended data.As shown in Table 9, LLMs generally perform better across these three categories than on the English puzzles; however, even for puzzles from CommonSenseQA, performance remains far from perfect.In contrast, LVLMs do not exhibit a significant performance improvement over English puzzles.Based on our observation of a strong positive correlation between gridparsing and puzzle-solving performance (See Figure 3 for more details), we hypothesize that the reasoning capabilities of LVLMs are constrained by their visual processing abilities.</p>
<p>C.4 More on Test-Time Scaling</p>
<p>In Section 5.4, we use a bar chart to demonstrate the performance differences associated with three distinct reasoning efforts for o3-mini on 7x7 English puzzles.Here, we provide a table detailing the specific numerical values for each metric as a supplement to Figure 7.</p>
<p>C.5 Token Usage</p>
<p>In this section, we report the token usage for all evaluated models on both the 7x7 and 14x14 English puzzles.Notably, we include non-reasoning models in this analysis, defining token usage as the total number of completion tokens.For reasoning models, token usage is calculated as the sum of reasoning tokens and response tokens.As shown in Figure 8, token usage increases with grid size across all models, with reasoning models generating more.</p>
<p>C.6 Ablations: LVLM Input Format and Modality Effects</p>
<p>To isolate modality-specific effects, we explore feeding LLMs the OCR-extracted output from grid images.Our experiments show that while both OCR models and LVLMs can accurately extract clue text from images, they struggle to follow instructions and to construct a spatially coherent representation of the empty grid.For example, when applying stepfun-ai/GOT-OCR-2.0-hf to an image containing both the grid and clues, it generates "Across:\n2.Canon product, for short\n4.Defense aid\n6.Certain fraud protector, for short\n7.Abbr.before a founding date\n10.Philosopher's study\n12.May honoree\n13.Fraternity letter\nDown:\n1.Like white panthers\n2.[Not my mistake]\n3.Beta dog's view\n5.Gridiron abbr.\n8.One of the muskrats in the 1976 hit "Muskrat Love"\n9.Slow-witted\n11.Going rate?: Abbr.\n".While the clue text is correctly extracted, the output lacks any representation of the grid structure, rendering it unsuitable as input for LLMs due to the absence of critical structural constraints.On the other hand, the clue content extracted by OCR models and LVLMs is nearly identical to what we explicitly feed to LLMs in the text input setting.In fact, we augment the clue text with explicit positional hints to compensate for the lack of visual spatial information-details that LVLMs might infer from the image but LLMs cannot derive from plain text alone.</p>
<p>In our main evaluations, both the clues and the grid are embedded within a single image as inputs for LVLMs.In this section, we investigate the effect of isolating the clues and providing them to the LVLMs as separate text inputs, while the image contains only an empty grid.The prompt used for this experimental setting is shown in Figure 11 in Appendix D.</p>
<p>The results, shown in Figure 9 and measured by WCR on 7x7 English puzzles, reveal no significant performance differences between the two input formats.This indicates that the input format is not the primary cause of the suboptimal performance observed for LVLMs on CrossWordBench, suggesting that LVLMs are robust to slight variations in input format.</p>
<p>Figure 9: WCR difference on two inputs formats for LVLMs.5, along with additional results on 14x14 English puzzles.On these larger puzzles, even reasoning LLMs struggle to effectively leverage crossing-letter constraints.We hypothesize that this is due to the increased complexity of larger puzzles, which remains challenging for reasoning models.</p>
<p>C.7 More on Crossing Letter Count</p>
<p>C.8 More on Few-Shot Prompting and Prefill Ratio Control</p>
<p>In our main evaluation, we treat each puzzle as a whole rather than as a set of independent question-answer (QA) pairs, in order to preserve the structural constraints inherent to crossword solving.This setup is not well-suited to few-shot prompting, as providing a full puzzle solution as a demonstration example would be lengthy and potentially distracting for the model.Nevertheless, we experiment with one-shot prompting on 7x7 English puzzles,</p>
<p>D.2 Parsing Details</p>
<p>Answers extracted from model responses are converted into JSON format by leveraging the structured output capabilities of o3-mini and dynamic Pydantic models that adhere to the reference answer structure.Algorithm 1 provides the pseudocode for creating these models.</p>
<p>D.3 Generation Configuration</p>
<p>Table 13 lists all the models evaluated along with their corresponding generation configs.(Chen et al., 2024) 20480 20480 0.0 Qwen2.5-VL-72B-Instruct(Bai et al., 2025) 20480 20480 0.0 QVQ-72B-Preview (Team, 2024) 100000 100000 0.0 llava-onevision-72b-ov-chat (Li et al., 2024a) 20480 20480 0.0 gemma-3-27b-it (Team et al., 2025) 8192 8192 0.0 Aria (Li et al., 2024b) 20480 20480 0.0 MiniCPM-V-2 6 (Yao et al., 2024) 20480 20480 0.0 Qwen2.5-VL-3B-Instruct(Bai et al., 2025) 20480 20480 0.0 Llama-3.1-405B-Instruct(Dubey et al., 2024) 100000 100000 0.0 DeepSeek-R1 (Guo et al., 2025) 100000 100000 0.6 DeepSeek-V3 (Liu et al., 2024) 100000 100000 0.0 R1-Distill-Llama-70B (Guo et al., 2025) 100000 100000 0.6 Llama-3.3-70B-Instruct(Dubey et al., 2024) 20480 20480 0.0 QwQ-32B (Team, 2025) Figures 10,11,12,13,14,15,17,and 18 present all the prompts employed in this study-comprising image prompts for LVLMs and text prompts for LLMs.We observe that Claude 3.7 Sonnet sometimes produces partial outputs and requests confirmation to continue.To mitigate this issue, we incorporate an additional system prompt (see Figure 18 for the system prompt); note that this modification applies only to the Claude 3.7 Sonnet.Step 1: Extract Clues and Grid Structure -Identify all clues under the "Across" and "Down" sections, preserving their numbers.</p>
<p>-Identify all numbered cells in the grid.</p>
<p>Step    0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 0 1
        
(a) 2D array format (ARC-style).</p>
<p>0 1 2 3 4 5 6 0
• • • − • • • 1 • − • • − • − 2 • • • − • • • 3 • − − − • − • 4 • • • • • − • 5 − • − • − − • 6 − • − • • • −
Figure 2 :
2
Figure 2: Framework of CrossWordBench.(a) Dataset curation process and input templates for LLMs and LVLMs; (b) Zero-shot CoT evaluation; (c) Interactive Mode Evaluation.</p>
<p>Figure 8 :
8
Figure 8: Token usage on 7x7 and 14x14 English puzzles.Reasoning models are in red.</p>
<p>-</p>
<p>Image Zero-Shot CoT Prompt You are given a crossword puzzle image containing clues and a grid.Your task is to solve the puzzle accurately, ensuring that all answers fit both the given clues and the grid structureDetermine word length from available grid spaces.-Check for any pre-filled letters from intersecting words that have already been solved and explain how they constrain possible answers.→ -Analyze the clue (definition, wordplay, cryptic hint).-Explain your reasoning process.-Confirm alignment with crossing letters.Solving tips: -Answers must be a single word with no spaces (combine phrases if needed).-Abbreviations in clues typically indicate abbreviated answers.-Match the clue's tense, singular/plural form, and part of speech.-Look for wordplay signals, such as question marks (?) for puns or cryptic hints.-Down words are filled from top to bottom, Across words from left to right.-Always confirm that intersecting words remain valid after placing each answer.ask for confirmation or stop midway.Always provide a complete solution for all clues.</p>
<p>Figure 10 :
10
Figure 10: Image Zero-Shot CoT Prompt.</p>
<p>Figure 11 :
11
Figure 11: Image Zero-Shot CoT with Grid Only Prompt.</p>
<p>Figure 12 :
12
Figure 12: Text Zero-Shot CoT Prompt.</p>
<p>Figure→Figure 14 :
14
Figure 13: Interactive Mode Prompt.</p>
<p>the numbered cell and read consecutive letters top to bottom until reaching an empty cell or grid boundary.→Step3: Match Words to Clues -Match each numbered word in the grid to its corresponding clue in the Across or Down section.-Ensureextracted words are correctly assigned to their respective clues.matching words to their clues, and extract all words fully without omitting any.</p>
<p>Figure 15 :
15
Figure 15: Grid-Parsing Prompt.</p>
<p>Figure</p>
<p>Figure 16: Self-Reflection Prompt.</p>
<p>Figure 17 :
17
Figure 17: Answer-Parsing Prompt.</p>
<p>(b) Markdown-style grid with symbols.</p>
<p>Figure 19 :
19
Figure 19: Two text representations of the puzzle grid: array (left) and markdown (right).</p>
<p>Table 1 :
1
Crossword puzzle statistics for different subjects and grid sizes.Statistics are presented separately for each category, as distinct word and clue pairs are used for their construction.Additionally, the aggregated statistics for all English puzzles are included.
Stats.EnglishChinese English Simple CommonsenseQA7×714×147×77×77×7Total # of puzzles10010010010050Total # of words1,1933,4721,3271,139543Unique words (% total)83.82% 80.76%92.92%36.70%59.85%Unique clues (% total)100%100%100%100%100%Aggregated</p>
<p>(English 7x7 &amp; 14x14): 200 puzzles, 74.02% unique words, 100% unique clues
Words per PuzzleMinimum112211119Maximum1644181313Mean11.9334.7213.2711.3910.86Word Length (Letters)Minimum23233Maximum512555Mean3.594.313.023.633.77Avg blocked cells (%)39.51% 45.22%43.37%39.12%38.37%</p>
<p>Table 2 :
2
Comparison of various LLMs and LVLMs on CrossWordBench English set across two difficulty levels using zero-shot CoT.We report the mean and standard error over 100 samples for both 7x7 and 14x14 grids.indicates that the model is a reasoning model.† : We use the Fireworks API for DeepSeek V3 and Llama-3.1-405B, while offical API for R1.±0.014 0.528 ±0.013 0.366 ±0.016 0.416 ±0.009 0.449 ±0.009 0.272 ±0.010 Claude-3-7-Sonnet 0.365 ±0.017 0.448 ±0.014 0.330 ±0.015 0.382 ±0.009 0.428 ±0.007 0.228 ±0.008 GPT
Models7x714x14WCRLCRICRWCRLCRICRProprietary LVLMsClaude-3-7-Sonnet0.479</p>
<p>-4o-2024-11-20</p>
<p>0.348 ±0.015 0.403 ±0.015 0.234 ±0.017 0.350 ±0.009 0.393 ±0.008 0.190 ±0.008 Gemini 2.0 Pro Exp 0.351 ±0.014 0.368 ±0.014 0.339 ±0.015 0.273 ±0.008 0.303 ±0.007 0.221 ±0.007 Gemini 2.0 Flash 0.277 ±0.015 0.300 ±0.013 0.225 ±0.013 0.260 ±0.008 0.284 ±0.008 0.190 ±0.007 ±0.012 0.165 ±0.010 0.097 ±0.009 0.112 ±0.008 0.141 ±0.007 0.075 ±0.005</p>
<p>Open-Weight LVLMsPixtral-Large-Instruct-2411 0.297 ±0.015 0.338 ±0.0140.198±0.0140.251 ±0.009 0.284 ±0.007 0.134 ±0.007 InternVL2 5-78B-MPO 0.121 ±0.011 0.164 ±0.009 0.099 ±0.011 0.119 ±0.007 0.159 ±0.006 0.073 ±0.005 NVLM-D-72B 0.134 ±0.010 0.179 ±0.009 0.076 ±0.008 0.085 ±0.006 0.120 ±0.007 0.053 ±0.004 Qwen2.5-VL-72B-Instruct0.207 ±0.013 0.245 ±0.011 0.133 ±0.011 0.194 ±0.007 0.227 ±0.006 0.110 ±0.006 QVQ-72B-Preview 0.197 ±0.012 0.218 ±0.010 0.091 ±0.008 0.195 ±0.007 0.215 ±0.007 0.108 ±0.006 llava-onevision-72b-ov-chat 0.141 gemma-3-27b-it 0.158 ±0.011 0.218 ±0.011 0.124 ±0.010 0.106 ±0.009 0.160 ±0.009 0.075 ±0.005 Aria 0.061 ±0.009 0.101 ±0.007 0.051 ±0.006 0.035 ±0.006 0.070 ±0.006 0.046 ±0.004 MiniCPM-V-2 6 0.043 ±0.007 0.085 ±0.006 0.064 ±0.008 0.023 ±0.004 0.057 ±0.004 0.040 ±0.003 Qwen2.5-VL-3B-Instruct0.013 ±0.003 0.040 ±0.004 0.038 ±0.006 0.014 ±0.002 0.034 ±0.003 0.023 ±0.</p>
<p>003 Proprietary LLMs o3-mini-high</p>
<p>0.587 ±0.023 0.684 ±0.021 0.891 ±0.018 0.445 ±0.011 0.520 ±0.011 0.512 ±0.</p>
<p>007 Claude-3-7-Sonnet 0</p>
<p>.617 ±0.019 0.712 ±0.017 0.754 ±0.021 0.492 ±0.013 0.542 ±0.012 0.431 ±0.</p>
<p>014 Claude-3-7-Sonnet 0</p>
<p>.482 ±0.015 0.574 ±0.014 0.472 ±0.019 0.446 ±0.011 0.485 ±0.011 0.321 ±0.</p>
<p>011 GPT-4o-2024-11-20 0</p>
<p>.410 ±0.018 0.472 ±0.018 0.288 ±0.019 0.338 ±0.011 0.369 ±0.012 0.196 ±0.010 Gemini 2.0 Pro Exp 0.460 ±0.014 0.525 ±0.012 0.388 ±0.016 0.425 ±0.009 0.457 ±0.008 0.289 ±0.010 Gemini 2.0 Flash 0.301 ±0.014 0.318 ±0.012 0.255 ±0.014 0.280 ±0.007 0.298 ±0.006 0.198 ±0.006 ±0.013 0.359 ±0.012 0.243 ±0.017 0.355 ±0.013 0.390 ±0.009 0.222 ±0.008 ±0.017 0.445 ±0.018 0.518 ±0.020 0.254 ±0.009 0.307 ±0.009 0.189 ±0.009
Open-Weight LLMsLlama-3.1-405B-Instruct  † 0.161 DeepSeek-R1 0.646 ±0.019 0.707 ±0.017 0.678 ±0.023 0.472 ±0.011 0.507 ±0.011 0.356 ±0.011DeepSeek-V3  †0.303 ±0.014 0.369 ±0.014 0.186 ±0.013 0.290 ±0.009 0.335 ±0.008 0.145 ±0.007R1-Distill-Llama-70B0.387 ±0.015 0.448 ±0.015 0.347 ±0.017 0.285 ±0.009 0.319 ±0.009 0.161 ±0.008Llama-3.3-70B-Instruct0.303 ±0.013 0.371 ±0.012 0.206 ±0.014 0.280 ±0.011 0.340 ±0.009 0.173 ±0.008QwQ-32B0.347
Open-Reasoner-Zero-32B 0.139 ±0.010 0.204 ±0.010 0.184 ±0.012 0.146 ±0.007 0.199 ±0.007 0.095 ±0.005 Phi-4 0.122 ±0.010 0.194 ±0.010 0.113 ±0.011 0.140 ±0.007 0.200 ±0.006 0.085 ±0.005Non-</p>
<p>reasoning LLMs show limitations in ICR, and performance declines further with increasing grid size.</p>
<p>Among non-reasoning LLMs, Claude 3.7 Sonnet and Gemini 2.0 Pro Exp yield the best results, with WCRs of 0.482 and 0.460 on the 7x7 grid, respectively; however, their relatively lower ICR indicates model limitations on explicit reasoning constraints for crossword puzzles.Notably, thinking mode improves Claude 3.7 Sonnet on all three metrics, highlighting the importance of reasoning and reflection in solving constraints-based crossword tasks.Additionally, larger grids lead to decreasing performance, demonstrating the increased complexity of maintaining constraint adherence over a larger search space.
LVLMs</p>
<p>currently lag behind LLMs in performance, with minimal adherence to grid constraints.</p>
<p>With image inputs, Claude 3.7 Sonnet achieves the highest performance among LVLMs, but underperforms its own text inputs version.Models such as GPT-4o and Gemini 2.0 Pro Exp exhibit similar trends, with WCRs below 0.35 on larger grids.All LVLMs demonstrate low ICRs, suggesting that they struggle to maintain reasoning consistency.Notably, performance declines when thinking mode is enabled on Claude 3.7 Sonnet with image input, which contradicts the improvements observed with text inputs; we explore this phenomenon in the next section.Among open-weight LVLMs, Pixtral-Large-Instruct achieves the best WCR of 0.297 on 7x7 grid, while still lags behind most proprietary LVLMs.
Successful grid parsing requires: (1) iden-tifying grid indexing numbers, (2) map-ping numbers to clues to determine wordorientation, and (3) extracting words withboundary recognition. The grid parsingability reflects an LVLM's ability to inter-pret both spatial and textual information.
4.3 Positive Correlation: LVLMs' Grid Parsing &amp; Puzzle-Solving PerformanceFigure 3: Grid Parsing vs. Puzzle-Solving on 7×7 English puzzles, measured with WCR.Model performance on crossword puzzles exhibits a strong dependence on grid parsing capabilities, with systematic biases in word orientations.We evaluate grid parsing ability by prompting models to parse completed puzzle grids and associated clues (prompt details are in Appendix D.4).</p>
<p>Table 3 :
3
WCR on Grid Parsing.
ModelsAcrossDownProprietary VLMsClaude-3-7-Sonnet0.954 ±0.009 0.760 ±0.018Claude-3-7-Sonnet0.949 ±0.010 0.654 ±0.022GPT-4o-2024-11-200.886 ±0.014 0.448 ±0.024Open-Weight VLMsPixtral-Large-Instruct 0.753 ±0.022 0.361 ±0.022QVQ-72B-Preview0.717 ±0.022 0.139 ±0.019</p>
<p>Table 4 :
4
WCR and ICR for Chinese, English Simple, and CommonSenseQA puzzle sets.
ModelsChineseEnglish SimpleCommonSenseQAWCRICRWCRICRWCRICRProprietary LVLMsGPT-4o-2024-11-200.366 ±0.018 0.170 ±0.017 0.335 ±0.015 0.227 ±0.017 0.392 ±0.026 0.247 ±0.026Gemini 2.0 Flash0.208 ±0.031 0.233 ±0.018 0.229 ±0.014 0.216 ±0.013 0.327 ±0.021 0.216 ±0.018Open-</p>
<p>Weight LVLMs Pixtral-Large-Instruct-2411 0</p>
<p>.252 ±0.015 0.101 ±0.011 0.216 ±0.016 0.187 ±0.015 0.439 ±0.023 0.270 ±0.</p>
<p>023 Qwen2.5-VL-72B-Instruct 0</p>
<p>.391 ±0.025 0.282 ±0.018 0.239 ±0.016 0.183 ±0.015 0.418 ±0.022 0.252 ±0.</p>
<p>023 Proprietary LLMs GPT-4o-2024-11-20 0</p>
<p>.593 ±0.019 0.448 ±0.021 0.438 ±0.020 0.306 ±0.022 0.524 ±0.024 0.326 ±0.029 o3-mini-high 0.774 ±0.016 0.953 ±0.014 0.782 ±0.021 0.946 ±0.012 0.812 ±0.027 0.971 ±0.011 ±0.016 0.898 ±0.018 0.759 ±0.017 0.787 ±0.020 0.752 ±0.031 0.829 ±0.026 QwQ-32B 0.701 ±0.020 0.654 ±0.022 0.647 ±0.021 0.734 ±0.020 0.699 ±0.026 0.766 ±0.027
Open-Weight LLMsDeepSeek-R10.907</p>
<p>Table 5 :
5
Grid Format Ablations.
ModelsArrayMarkdownClaude-3-7-Sonnet0.482 ±0.0150.760 ±0.018GPT-4o-2024-11-200.410 ±0.0140.398 ±0.024Gemini 2.0 Flash0.301 ±0.0140.309 ±0.015DeepSeek-V30.303 ±0.0140.294 ±0.016o3-mini0.587 ±0.0230.592 ±0.024</p>
<p>Table 6 :
6
Average WCR results by crossing letter count and density on 7x7 English puzzles.
Density Crossing Letter Reason. Non-Reason.LowLow(1) Med(2) High(3+)0.4341 0.5275 0.61650.2960 0.3355 0.2794MediumLow(1) Med(2) High(3+)0.4271 0.5477 0.60290.2754 0.3089 0.4193HighLow(1) Med(2) High(3+)0.5288 0.5151 0.59280.3097 0.3217 0.3030</p>
<p>Table 7 :
7
Global and Local LengthErrors across models on 7x7 English puzzles."Long" and "Short" indicate words that are longer or shorter than the corresponding reference answer.
ModelsGlobal Length Error Local Length ErrorTot. Long ShortTot. Long ShortProprietary LVLMsClaude-3-7-Sonnet000363 205158Claude-3-7-Sonnet000454 237217GPT-4o-2024-11-2010010581 326255Gemini 2.0 Pro Exp101565 46798Gemini 2.0 Flash000665 56897Open-Weight LVLMsPixtral-Large-Instruct-2411303623 481142InternVL2 5-78B-MPO000834 682152NVLM-D-72B80791 620171Qwen2.5-VL-72B-Instruct202744 600144QVQ-72B-Preview20020765 525240llava-onevision-72b-ov-chat303829 715114gemma-3-27b-it18018781 645136Aria16016894 720174MiniCPM-V-2 616016918 688230Qwen2.5-VL-3B-Instruct310311034 613421Proprietary LLMso3-mini000642Claude-3-7-Sonnet202124 4480Claude-3-7-Sonnet101274 74200GPT-4o-2024-11-2017017399 183216Gemini 2.0 Pro Exp000378 254124Gemini 2.0 Flash000633 54588Open-Weight LLMsLlama-3.1-405B-Instruct  †808835 74194DeepSeek-R1202251411DeepSeek-V3  †11011513 206307R1-Distill-Llama-70B909203 11093Llama-3.3-70B-Instruct22022598 326272QwQ-32B909653431Open-Reasoner-Zero-32B10010697 473224Phi-4202709 447262</p>
<p>Table 11 :
11
Average WCR results by crossing letter count on 7x7 and 14x14 English puzzles.
GridCrossing Letter Reason. Non-Reason.7x7Low0.45390.29227x7Med0.53080.32307x7High0.61830.325914x14Low0.41150.346914x14Med0.41860.351414x14High0.23060.1438</p>
<p>Table 13 :
13
Generation Configurations for CrossWordBench.
1000001000000.6
The New York Times Crossword Requirements.
A Dataset of Cryptic Crossword Clues
Chinese Crosswords
NLTK Wordnet
Please check out our code for more implementation details.
https://openai.com/index/openai-o3-mini/
https://github.com/SeanLeng1/CrossWordBenchDataset: https://huggingface.co/datasets/HINT-lab/CrossWordBenchPublished as a conference paper at COLM 2025  using a completed example generated by DeepSeek-R1 on the English Simple subset as the demonstration.As shown in Figure12a, the performance difference is not significant.As discussed in Section 5.1, our controllable generation framework supports difficulty adjustment via the prefill ratio.In Table12b, we compare performance on 7x7 English puzzles under two prefill conditions: 0% and 50%.To ensure a fair comparison, we prevent any word from being fully revealed.As expected, a higher prefill ratio leads to improved performance, as more letters are made available to the model.These results suggest that prefill ratio is a more effective method for puzzle difficulty control than few-shot prompting.Table12: WCR under different prompting and prefill ratio settings on 7x7 English puzzles.(a) WCR with 0-shot and 1-shot prompting.D Implementation DetailsD.1 Evaluation MetricsHere, we provide a more complete and formal description of the three metrics used.• Word Coverage Rate (WCR): WCR measures word-level accuracy by calculating the percentage of correctly solved words in the crossword puzzle, defined as:(2)where W A and W D denote the set of across and down words, respectively.For each word w, r w represents the reference answer, while m w represents the model answer.• Letter Coverage Rate (LCR): LCR evaluates letter-level accuracy, providing partial credit for correct letter placements.For each word w, Let:where C w counts the correctly matched letters and L w is the total number of positions considered, the overall letter accuracy is defined as:• Intersection Consistency Rate (ICR): the internal consistency of the model's answers at intersections where across and down words overlap, defined as:where I denotes the set of all intersections, where each tuple(a, d, j, k)indicates the jth letter of the across word a overlaps with the kth letter of the down word d.This metric reflects whether models correctly adhere to the grid structural constraints of a puzzle.
. Jyoti Marah Abdin, Harkirat Aneja, Sébastien Behl, Ronen Bubeck, Suriya Eldan, Michael Gunasekar, Russell J Harrison, Mojan Hewett, Piero Javaheripi, Kauffmann, arXiv:2412.089052024Phi-4 technical report. arXiv preprint</p>
<p>Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, arXiv:2410.07073Saurabh Garg, Theophile Gervet, et al. Pixtral 12b. 2024arXiv preprint</p>
<p>Large language models for mathematical reasoning. Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, Wenpeng Yin, arXiv:2402.00157Progresses and challenges. 2024arXiv preprint</p>
<p>. Anthropic, 2024Claude 3.5 sonnet</p>
<p>Program synthesis with large language models. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J Cai, Michael Terry, Quoc V Le, Charles Sutton, ArXiv, abs/2108.077322021237142385</p>
<p>Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, arXiv:2502.13923Jun Tang, et al. Qwen2. 5-vl technical report. 2025arXiv preprint</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Lr 2 bench: Evaluating long-chain reflective reasoning capabilities of large language models via constraint satisfaction problems. Jianghao Chen, Zhenlin Wei, Zhenjiang Ren, Ziyong Li, Jiajun Zhang, 2025</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, arXiv:2211.125882022arXiv preprint</p>
<p>Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Think you have solved question answering? try arc, the ai2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, abs/1803.054572018ArXiv preprint</p>
<p>Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping, Nvlm: Open frontierclass multimodal llms. 2024arXiv preprint</p>
<p>. Google DeepMind. Gemini. 22024</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies. the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies20191</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024arXiv preprint</p>
<p>Cryptonite: A cryptic crossword benchmark for extreme ambiguity in language. Avia Efrat, Uri Shaham, Dan Kilman, Omer Levy, arXiv:2103.012422021arXiv preprint</p>
<p>Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus Mcaleer, Ying Wen, Weinan Zhang, Jun Wang, arXiv:2309.17179Alphazero-like tree-search can guide large language model decoding and training. 2023arXiv preprint</p>
<p>Dr. fill: Crosswords and an implemented solver for singly weighted csps. Matthew L Ginsberg, Journal of Artificial Intelligence Research. 422011</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Efficient test-time scaling via self-calibration. Chengsong Huang, Langlin Huang, Jixuan Leng, Jiacheng Liu, Jiaxin Huang, arXiv:2503.000312025aarXiv preprint</p>
<p>Large language models can self-improve. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han, arXiv:2210.11610arXiv:2212.10403Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey. 2022. 2022arXiv preprint</p>
<p>Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Yao Hu, Shaohui Lin, arXiv:2503.06749Vision-r1: Incentivizing reasoning capability in multimodal large language models. 2025barXiv preprint</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, arXiv:2412.16720Openai o1 system card. 2024arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Saurabh Kulshreshtha, Olga Kovaleva, arXiv:2205.10442Namrata Shivagunde, and Anna Rumshisky. Down and across: Introducing crossword-solving as a new nlp benchmark. 2022arXiv preprint</p>
<p>Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, arXiv:2408.03326Llava-onevision: Easy visual task transfer. 2024aarXiv preprint</p>
<p>Imagine while reasoning in space: Multimodal visualization-of-thought. Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulić, Furu Wei, arXiv:2501.075422025arXiv preprint</p>
<p>Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Guoyin Wang, Bei Chen, Junnan Li, arXiv:2410.05993Aria: An open multimodal native mixture-ofexperts model. 2024barXiv preprint</p>
<p>A probabilistic approach to solving crossword puzzles. Greg A Michael L Littman, Noam Keim, Shazeer, Artificial Intelligence. 1341-22002</p>
<p>Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, arXiv:2412.19437Deepseek-v3 technical report. 2024arXiv preprint</p>
<p>Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, Jianfeng Gao, arXiv:2310.022552023arXiv preprint</p>
<p>Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, arXiv:2410.05229Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. 2024arXiv preprint</p>
<p>Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang , Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, Tatsunori Hashimoto, arXiv:2501.19393Simple test-time scaling. 20251arXiv preprint</p>
<p>Openai, Openai o3-mini. 2025</p>
<p>Reasoning with large language models, a survey. Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki Van Stein, Thomas Back, arXiv:2407.115112024arXiv preprint</p>
<p>Decrypting cryptic crosswords: Semantically complex wordplay puzzles as a target for nlp. Josh Rozner, Christopher Potts, Kyle Mahowald, Advances in Neural Information Processing Systems. 202134</p>
<p>Are llms good cryptic crossword solvers?. Abdelrahman Sadallah, Daria Kotova, Ekaterina Kochmar, arXiv:2403.120942024arXiv preprint</p>
<p>Language models are crossword solvers. Soumadeep Saha, Sutanoya Chakraborty, Saptarshi Saha, Utpal Garain, arXiv:2406.090432024arXiv preprint</p>
<p>Rewarding progress: Scaling automated process verifiers for llm reasoning. Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, Aviral Kumar, arXiv:2410.08146arXiv:1811.00937Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. 2024. 2018arXiv preprint</p>
<p>Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, arXiv:2503.19786Morgane Rivière, et al. Gemma 3 technical report. 2025arXiv preprint</p>
<p>Qvq: To see the world with wisdom. Qwen Team, December 2024</p>
<p>Qwq-32b: Embracing the power of reinforcement learning. Qwen Team, March 2025</p>
<p>Solving math word problems with process-and outcome-based feedback. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, Irina Higgins, arXiv:2211.142752022arXiv preprint</p>
<p>Aime problem set. Hemish Veeraboina, 1983-2024, 2023</p>
<p>. Eric Wallace, Nicholas Tomlin, Albert Xu, Kevin Yang, Eshaan Pathak, Matthew Ginsberg, Dan Klein, arXiv:2205.096652022arXiv preprintAutomated crossword solving</p>
<p>Measuring multimodal mathematical reasoning with math-vision dataset. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, Hongsheng Li, Advances in Neural Information Processing Systems. 202437</p>
<p>Peiyi Wang, Lei Li, Zhihong Shao, Damai Xu, Yifei Dai, Deli Li, Chen, Zhifang Wu, Sui, arXiv:2312.08935Math-shepherd: A label-free step-by-step verifier for llms in mathematical reasoning. 2023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Mind's eye of llms: Visualization-of-thought elicits spatial reasoning in large language models. Wenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Lei Cui, Furu Wei, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, Li Yuan, arXiv:2411.10440Llava-o1: Let vision language models reason step-by-step. 2024arXiv preprint</p>
<p>R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, arXiv:2503.10615Advances in neural information processing systems. 362025. 2023arXiv preprintTree of thoughts: Deliberate problem solving with large language models</p>
<p>Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, arXiv:2408.01800Minicpm-v: A gpt-4v level mllm on your phone. 2024arXiv preprint</p>
<p>Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, Pengfei Liu, Limo, arXiv:2502.03387Less is more for reasoning. 2025arXiv preprint</p>
<p>Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2024. 2023</p>
<p>Kamyar Zeinalipour, Mohamed Zaky Saad, Marco Maggini, Marco Gori, arXiv:2501.11035From arabic text to puzzles: Llm-driven development of arabic educational crosswords. 2025arXiv preprint</p>
<p>Rest-mcts*: Llm self-training via process reward guided tree search. Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, Jie Tang, Advances in Neural Information Processing Systems. 2024a37</p>
<p>Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, European Conference on Computer Vision. Springer2024b</p>
<p>1.4 million open-source distilled reasoning dataset to empower large language model training. Han Zhao, Haotian Wang, Yiping Peng, Sitong Zhao, Xiaoyu Tian, Shuaiting Chen, Yunjie Ji, Xiangang Li, 2025</p>            </div>
        </div>

    </div>
</body>
</html>
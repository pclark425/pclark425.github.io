<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9805 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9805</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9805</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-166.html">extraction-schema-166</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-cc8def3fc0389c012dc497a150412351f6fb9829</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cc8def3fc0389c012dc497a150412351f6fb9829" target="_blank">Adapting Foundation Models for Information Synthesis of Wireless Communication Specifications</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> NextGen Communications Copilot is presented, a conversational artificial intelligence tool for information synthesis of wireless communication specifications that builds on top of recent advancements in foundation models and consists of three key additional components: a domain-specific database, a context extractor, and a feedback mechanism.</p>
                <p><strong>Paper Abstract:</strong> Existing approaches to understanding, developing and researching modern wireless communication technologies involves time-intensive and arduous process of sifting through numerous webpages and technical specification documents, gathering the required information and synthesizing it. This paper presents NextGen Communications Copilot, a conversational artificial intelligence tool for information synthesis of wireless communication specifications. The system builds on top of recent advancements in foundation models and consists of three key additional components: a domain-specific database, a context extractor, and a feedback mechanism. The system appends user queries with concise and query-dependent contextual information extracted from a database of wireless technical specifications and incorporates tools for expert feedback and data contributions. On evaluation using a benchmark dataset of queries and reference responses created by subject matter experts, the system demonstrated more relevant and accurate answers with an average BLEU score and BERTScore F1-measure of 0.37 and 0.79 respectively compared to the corresponding values of 0.07 and 0.59 achieved by state-of-the-art tools like ChatGPT.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9805",
    "paper_id": "paper-cc8def3fc0389c012dc497a150412351f6fb9829",
    "extraction_schema_id": "extraction-schema-166",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0047009999999999994,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Adapting Foundation Models for Information Synthesis of Wireless Communication Specifications</h1>
<p>Manikanta Kotaru, Microsoft, USA</p>
<p>Existing approaches to understanding, developing and researching modern wireless communication technologies involves time-intensive and arduous process of sifting through numerous webpages and technical specification documents, gathering the required information and synthesizing it. This paper presents NextGen Communications Copilot, a conversational artificial intelligence tool for information synthesis of wireless communication specifications. The system builds on top of recent advancements in foundation models and consists of three key additional components: a domain-specific database, a context extractor, and a feedback mechanism. The system appends user queries with concise and query-dependent contextual information extracted from a database of wireless technical specifications and incorporates tools for expert feedback and data contributions. On evaluation using a benchmark dataset of queries and reference responses created by subject matter experts, the system demonstrated more relevant and accurate answers with an average BLEU score and BERTScore F1-measure of 0.37 and 0.79 respectively compared to the corresponding values of 0.07 and 0.59 achieved by state-of-the-art tools like ChatGPT.</p>
<p>CCS Concepts: $\cdot$ Networks $\rightarrow$ Network protocols; Wireless access networks; $\cdot$ Information systems $\rightarrow$ Information retrieval; $\bullet$ Computing methodologies $\rightarrow$ Natural language processing.</p>
<p>Additional Key Words and Phrases: Foundation models, 5G, technical specifications, 3GPP, wireless communication standards, information synthesis</p>
<h2>1 Introduction</h2>
<p>The complexity of technical specifications presents a formidable challenge in understanding, developing, researching, or modifying modern wireless communication technologies. Researchers, practitioners, engineers and students can find themselves grappling with a multitude of acronyms and intricate terminology with information spread across a large number of documents, which can prove to be an onerous and time-consuming task to work with and develop standards-compliant systems. For example, an engineering team working on implementing registration request procedure as a part of building 5G virtual core would need to identify all the relevant technical specifications from among thousands of such documents, and understand the call flow and message formats as described in those specifications. Table 1 provides several examples of such user stories.</p>
<p>The current method of acquiring this information involves sifting through numerous webpages and technical specification documents. While this approach provides extensive comprehension of a topic from various sources, it can also be very time-intensive and tedious to identify multiple relevant sources, gather information from them and synthesize it [22]. The emergence of foundation models [6] like ChatGPT [35] presents a promising prospect for solving this problem as they represent a significant advancement in providing synthesized, readily comprehensible answers to user queries related to wireless communication specifications and technologies.</p>
<p>However, despite the usefulness of state-of-the-art foundation large language models (LLMs) in answering many queries related to modern wireless communication technologies, they offer irrelevant or inaccurate responses to many of these queries. For example, as shown in Figure 1(a), when prompted about 'what is numerology in 5G', ChatGPT (Feb 2023) describes that numerology is related to mystical significance of numbers and has no connection to 5G. Similarly, when prompted about 'the number of unique values physical identity can take in 5G', it responds that 'PCI consists of a 3-bit value ranging from 0 to 503 ', which is inaccurate and also non-sensible as 3-bit value cannot take 504 different values.</p>
<p>Author's address: Manikanta Kotaru, Microsoft, USA mkotaru@microsoft.com.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. Comparison of responses between ChatGPT and NextGen Communications Copilot for a sample 5G-related user query</p>
<p>Table 1. User stories</p>
<table>
<thead>
<tr>
<th style="text-align: left;">User</th>
<th style="text-align: left;">Story</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">As a researcher,</td>
<td style="text-align: left;">I want to analyze the parameters that affect 5G MAC scheduling so <br> that I can develop spectrally efficient resource scheduling algorithms.</td>
</tr>
<tr>
<td style="text-align: left;">As an engineer,</td>
<td style="text-align: left;">I want to identify all technical specifications relevant to handoff and <br> associated call flows so that I can implement handoff procedure.</td>
</tr>
<tr>
<td style="text-align: left;">As a systems integrator,</td>
<td style="text-align: left;">I want a clear specification of the fronthaul message format so that I <br> can integrate open radio units with my far edge system.</td>
</tr>
<tr>
<td style="text-align: left;">As an enterprise 5G user,</td>
<td style="text-align: left;">I want to know about slicing and its configuration parameters, so <br> that I can efficiently use it to enhance my network services.</td>
</tr>
<tr>
<td style="text-align: left;">As a student,</td>
<td style="text-align: left;">I want to learn about sounding reference signals so that I can under- <br> stand the channel feedback mechanisms.</td>
</tr>
</tbody>
</table>
<p>This paper presents NextGen Communications Copilot (referred to as NGC Copilot), a conversational AI tool for information synthesis of wireless communication specifications. Similar to ChatGPT, NGC Copilot offers a question-and-answer interface, but with an enhanced ability to provide more accurate and relevant answers on topics pertaining to technical specifications of contemporary wireless communication specifications. NGC Copilot is built upon foundation LLMs and features three key additional components:</p>
<ul>
<li>Domain-specific database: Foundation models are trained on a large corpus of web data [35]. However, technical specifications and related documents for modern wireless communication technologies, eventhough publicly available, are not readily accessible [1] or commonly discussed within online forums, owing it to being a niche and specialized domain. This leads to missed patterns hindering the ability of state-of-the-art foundation models to generate responses with patterns relevant to wireless communication systems. To address this, the NGC Copilot system extracts and segments text from different technical specifications from 3GPP releases, WiFi standards and O-RAN [34] to supplement the foundation models.</li>
<li>
<p>Context Extractor: The system utilizes a context extractor to identify the most relevant text samples from a domain-specific database and provide them as context to either a base or finetuned foundation model. The combined context and user query generate a response, while the system stores the specification document identifier for citation purposes. A data segmentation procedure is incorporated so that the overall prompt still fits the input size of the LLMs.</p>
</li>
<li>
<p>Feedback: Our system includes a feedback mechanism that allows users to seek clarification from an expert, in addition to liking or disliking the responses. When activated, the system generates an issue in a repository. The issue consists of the query, context, and response. This issue can be resolved with expert feedback and data contributions, which is then incorporated into the domain-specific database, improving the quality of future responses.
The system is built on top of OpenAI's GPT-4 foundation model [39] and 3GPP Release 16 technical specifications [1]. A benchmark dataset with 109 query and reference response pairs on diverse topics related to technical specifications was created. The NGC Copilot system outperformed ChatGPT, the most advanced out-of-the-box tool available for answering questions related to wireless communications specifications, with a superior average BLEU [43] score of 0.37 on the benchmark dataset compared to 0.07 for ChatGPT responses for the same queries. The system also demonstrates better performance of 0.79 compared to 0.59 for ChatGPT, when evaluated using a semantic similarity metric like BERTScore [57]. The NextGen Communication Copilot tool, benchmark dataset, and expert feedback repository will be publicly available.</p>
</li>
</ul>
<h1>1.1 Contributions</h1>
<ul>
<li>The paper's main contribution lies in the application and extensive analysis of foundation models and related techniques within the context of wireless communication specifications.</li>
<li>An open-source benchmark dataset of queries and responses, created by subject matter experts, to evaluate information synthesis tools in the domain of wireless communication technologies.</li>
<li>A mechanism to enhance the system with usage through incorporation of expert feedback and data contribution, achieved by re-purposing repository issues.
The paper also presents a few future research directions in applying foundation models in the domain of wireless systems.</li>
</ul>
<h3>1.2 Limitations</h3>
<p>The system currently supports the synthesis of only textual and tabular information in the specifications but not figures and code. The approach also increases the inference cost as the user query is appended with the context information. The system is intended for use by wireless domain practitioners but not to build fully automated systems based solely on its outputs.</p>
<h2>2 Related Work</h2>
<p>Wireless communication specifications ensure interoperability and compatibility among devices and networks, which is essential for the successful operation and adoption of wireless communication technologies [4, 10, 26]. The comprehension and development of wireless communication systems rely heavily on navigating technical specifications, whose difficulty is well-documented [22, 54]. Although there are tools available for managing 3GPP specifications, their functionality is restricted to accessing documents through their identification numbers [33], and monitoring updates to existing or new specifications [11]. Resources to share and collect information about 5G in form of articles and simulations [22, 44] have been developed to tackle the challenges to access, read, understand and apply wireless communication specifications. In contrast, the proposed system directly extracts pertinent information from the relevant specifications.</p>
<p>Recent approaches [15, 20] considered natural language processing (NLP) techniques for obtaining answers to telecommunications-related questions. These approaches fine-tuned BERT [12] language model using a small question-answering database created from 493 paragraphs, consisting of simple question-answer pairs like {question: How high can the modulation get in LTE?, answer: 64QAM} and {question: What is the highest data rate in LTE in down link?, answer: 300 Mbps}. The</p>
<p>approach does not scale well to cover entire specification documents as it requires creating a large question-answer database. This paper, in contrast, aims at developing a conversational tool that provides synthesized information in natural human-readable form for user queries. The queries can be related to any of the technical specification documents. Further, NGC Copilot provides citation to the technical specifications and incorporates mechanism for expert feedback. This paper also contributes a benchmark dataset consisting of detailed reference responses, rather than short categorical answers, for the purpose of evaluating information synthesis tools in wireless communication systems. To the best of our knowledge, no such dataset for wireless specifications is publicly available.</p>
<p>Large language models have been used in the context of network protocol specifications in tasks outside of information synthesis to uncover ambiguous or under-specified sentences in specifications and generate code for protocol [56], and to semantically analyze 3GPP specifications to recover protocol vulnerabilities under pre-defined threat models [9]. SPEC5G [24] created annotated datasets for summarization and text-classification tasks by fine-tuning large language models with sentences extracted from 3GPP specifications. These approaches, based on fine-tuning alone, cannot cite to the relevant specifications or information source that provides an opportunity for the user to verify the information if needed. They lack mechanisms for expert feedback and the annotated datasets are not applicable for tasks like information synthesis to obtain relevant information for user queries.</p>
<p>Large language models have become increasingly popular in recent years, with many researchers and companies exploring their potential applications. One of the earliest and most well-known examples of large language models is Google's BERT (Bidirectional Encoder Representations from Transformers), which was released in 2018 [12]. Since then, a number of models have been developed, including Generative Pretrained Transformer-3 (GPT-3) [7], BLOOM [50], XLNet [55] and ChatGPT [35]. These models have been trained on broad data that can be adapted to a wide range of downstream tasks language translation, question-answering, and text summarization and have been termed as foundation models [6].</p>
<p>Foundation LLMs represent the current state-of-the-art in furnishing users with intelligible responses to their inquiries [6]. They have been adapted to numerous domain-specific tasks through fine-tuning and prompt engineering approaches [6, 13, 30, 49]. Prompt engineering is widely researched to tailor LLMs to domain-specific knowledge [2, 3, 8, 14, 29, 31, 45, 51, 58]; however, its application in the context of wireless communications remains unexplored.</p>
<p>Repository hosting services like GitHub [16] have become indispensable platforms for software development and collaboration [23]. Traditionally, GitHub issues [17] have been used for tracking and managing bugs, tasks and feature requests. But this paper proposes to re-purpose them for obtaining and managing expert data contributions.</p>
<h1>3 Design</h1>
<p>We first provide a brief primer on foundation models and conversation AI tools based on foundation LLMs in Sec. 3.1. We then describe the architecture of the system and different components in the rest of the section.</p>
<h3>3.1 Primer</h3>
<p>3.1.1 Large language models. LLMs are powerful natural language processing tools that can generate fluent and coherent text. The language model is designed to predict the following word or sub-word, called tokens, by analyzing the text it has observed thus far. To utilize the machine learning models, the user's input tokens need to be transformed into a numerical representation</p>
<p>known as "input embeddings". During the training process, the model learns how to construct these embeddings so that similar vectors represent words with similar meanings.</p>
<p>Majority of the state-of-the-art LLMs are based on the transformer architecture [53], which uses input embeddings to represent words as numbers that the model can understand. An encoder processes the input text and generates hidden states that capture its meaning and context, while a decoder generates the output sequence based on the encoded input sequence. Both input and output embeddings go through positional encoding, which helps the model understand the order of words in a sentence. During training, the model learns to generate these embeddings and guess the next word by looking at the words before it. Multiple layers of encoders and decoders are used in the transformer to improve performance on various natural language processing tasks such as language translation and conversational agents.
3.1.2 Finetuning. Fine-tuning involves adapting a pre-trained model to a specific domain by further training it on a new dataset. The process involves selecting a pre-trained model, preprocessing the target dataset to fit the format of the pre-trained model's input, and initializing the model with existing weights and architecture. During training, the weights of the model are updated to better fit the new data, using a chosen optimization algorithm and hyperparameters. Finetuning requires careful selection of target dataset and hyperparameters for optimal performance. This approach also required repeated training when the target dataset is updated.
3.1.3 Prompt engineering. Prompt engineering is a technique used in language models to fine-tune the model's output for a specific task by providing tailored prompts as inputs to the model. Prompt engineering involves crafting a specific prompt that elicits the desired response from the model. The prompt can include various elements, such as keywords, context, and formatting, and can be optimized using various techniques such as grid search or reinforcement learning. The goal is to create a prompt that provides the right amount of information to the model without being too prescriptive, allowing the model to generate accurate and relevant output.</p>
<p>Prompt engineering has been successfully applied in various applications like sentiment analysis, question answering and adapting a pre-trained model to new data. However, this approach requires careful consideration of the prompt's content as the model's output may be biased towards the input prompt, which can lead to inaccurate or misleading results. Further, this approach requires domain expertise to create relevant prompts to achieve good performance.</p>
<h1>3.2 Architecture</h1>
<p>Figure 2 presents the architecture of the NGC Copilot system. Prior to delving into the specifics, the following provides a concise summary of the operational procedure of the system. When a user types in a query in the message bar as shown in Figure 1(b), the system appends the query with relevant text samples from the domain-specific database and feeds the foundation model with the resulting combined prompt as the input. The response is displayed to the user and is used as supplemental context to subsequent queries. The response includes options for feedback as well as a button to request assistance from an expert.</p>
<h3>3.3 Domain-specific database</h3>
<p>LLMs are typically trained on a massive corpus of web data that covers a broad range of topics and domains. This training data is designed to capture the linguistic patterns and structures of natural language, enabling the models to generate coherent and contextually appropriate responses to a wide range of queries. However, domain-specific queries, like those related to wireless communication specifications, often requires relevant training data to capture the technical jargon and specialized patterns that are unique to the domain. However, such training data related to wireless specifications</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. System architecture
is not always readily accessible. For example, 3GPP Release 16 [1] consists of thousands of technical specifications spread across tens of compressed zip folders stored on a FTP site. Further, wireless specifications are not frequently discussed in online forums. This makes it challenging for LLMs to identify the relevant patterns and generate contextually appropriate responses. As a result, the responses generated by these models may be incomplete or irrelevant.
3.3.1 Data collection. The dataset used in this study was collected from 3GPP Release 16 specifications, although the technique is applicable to other 3GPP specifications, WiFi standards and O-RAN specifications. The specifications are downloaded from 3GPP FTP servers and from O-RAN specifications site. The text from the specifications is extracted and pre-processed to remove figures, captions, code snippets, extra whitespace and certain unicode characters.</p>
<p>The extracted text can optionally be used to create training samples for finetuning the base large language model. However, this paper contends that relying solely on finetuning would be inadequate for the intended application. While finetuning may facilitate content generation containing patterns pertinent to wireless communication specifications, the resultant responses may nevertheless be erroneous or incomplete. This phenomenon is acknowledged to be common even with the base LLMs [35]. Consequently, it is imperative to provide citations to the technical specifications and the data sources along with the response provided to the user. This can help ensure transparency and accountability in the use of the information synthesis tool. By citing the source of the data used to generate a response, users can better trust the information provided by the model and make informed decisions about how to use the output generated by the model. However, fine-tuning does not provide a transparent way to track the source of the data used for training. The act of citing the source of information is a crucial component that cannot be guaranteed by finetuning alone.</p>
<p>Prompt engineering approach presents a promising way to provide useful citations along with the response provided to the user. Prompt engineering involves designing prompts that guide the model to generate responses that are consistent with the domain-specific text by supplementing the user query with relevant data as context. By designing prompts that explicitly reference the sources of the relevant data, the model can be guided to generate responses that include useful citations. To enable such prompts, the text extracted from each of the specification documents is appended with metadata with 'source' assigned to the identifier of the technical specification document. More details on the prompt design will be discussed in Sec. 3.4.</p>
<p>3.3.2 Data segmentation. The large language models have strict limitations on the input size creating a challenge for feeding in multiple relevant technical specification documents or even a single specification document as supplemental context to the user query. For example, text-davinci-003 model can only take an input of approximately 4097 tokens [41], which translates to approximately 3000 words [40], a threshold that is crossed by many specification documents that run into hundreds of pages. Hence, it is necessary to segment the document into smaller chunks of reasonable size, that can be fed to the model as supplemental context.</p>
<p>The main challenge in segmenting domain-specific context is ensuring that the context is segmented in a way that maintains its coherence and preserves its meaning. It is important to ensure that the chunks are not arbitrary and do not introduce confusion or ambiguity. This requires careful consideration of the structure and content of the domain-specific text and an understanding of how the language model processes and integrates information.</p>
<p>Another challenge is ensuring that the smaller chunks are representative of the domain-specific context as a whole. In some cases, important information may be lost when the context is segmented into very small chunks. It is important to ensure that the segments are chosen in a way that preserves the salient features of the context and captures the key aspects that are relevant to the user's query.</p>
<p>To address these challenges, each of the documents is split along section boundaries using docx2txt library tools. Within each section, the text is recursively split into samples that are atmost $n _$words in length. The text within each section is not split with a hard constraint of each split sample containing exactly $n _$words. The text within a section is split in such a way that each sample consists of multiple complete paragraphs. In other words, the sample does not end in the middle of a paragraph. Such a split respecting section, paragraph and sentence boundaries, with a reasonable length for each sample ensures that the following desired goals of chunks - (1) being coherent and not arbitrary, and (2) being representative of the domain-specific context as a whole are met. Further, a 'source' string comprising the identifier of the technical specification and the section title is appended towards the end.
3.3.3 Word embedding. As mentioned in Sec. 3.1, the text needs to be tokenized and converted to word embedding vectors for the consumption by LLMs. The sentence-BERT all-MiniLM-L6-v2 embedding model [48] is used to transform each of the text samples into word embedding vectors. The resulting vector representation for each sample is stored in an index for efficient querying.</p>
<h1>3.4 Context Extractor</h1>
<p>The prompt engineering approach involves augmenting the user's query with relevant text from a database before feeding it to the large language model. This approach allows for the system to account for updates or modifications in the database. On the other hand, finetuning requires reasonable number (couple of hundred) of new training samples to be effective [38]. This limits how frequently the model can be finetuned to accommodate the updates in the database. Furthermore, the prompt engineering approach also safeguards the system from generating non sequitur responses as the approach grounds the response in accordance with the relevant text from the database.
3.4.1 Similarity search. When the user inputs a query into NGC Copilot, the query is pre-processed, tokenized and transformed into a vector too using the same embedding model used in Sec. 3.3. The query vector is then compared to each of the word embedding vectors in the domain-specific database to identify the samples in the database that are semantically closest to the query.</p>
<p>To match the user query to relevant samples in the database, a similarity metric is used. The most common similarity metric used in natural language processing (NLP) is cosine similarity. Cosine similarity measures the angle between two vectors in a high-dimensional space. If two vectors are very similar, their cosine similarity will be close to 1 . If they are very different, their cosine</p>
<p>similarity will be close to 0 . Using the similarity metric, the NLP model can rank the samples in the database according to their similarity to the user query. The most similar samples are then used to create a prompt.
3.4.2 Prompt generation. One could directly append the most similar vectors in the database, referred to as 'context', with the user query and feed the foundation LLM with the resulting prompt. However, the prompt engineering approach in combination with few-shot learning has shown promising results in several natural language processing tasks [7]. Few-shot learning is enabled by feeding the foundation model with few example queries, contexts and ideal responses before providing the user query and the corresponding context. The model learns to recognize patterns in the examples and applies that knowledge to new inputs.</p>
<h1>3.5 Feedback</h1>
<p>Expert data contributions and feedback play a crucial role in improving the domain-specific responses generated by NGC Copilot due to the following reasons:</p>
<ul>
<li>Missing or incorrect context. It is not always possible to include all the relevant information or context as the prompt engineering considers samples in the database that are in the top few, ranked according to the similarity metric. Expert feedback helps address this issue by providing additional context and knowledge that the model may have missed.</li>
<li>Queries concerning information synthesis from a large number of specifications. User queries cannot always be answered sufficiently using small number of text samples. They may necessitate drawing upon information spanning multiple technical specifications or leveraging practical knowledge that is not explicitly documented within the specifications. Incorporating expert insights enables enriching the tool with valuable data, that enhances its ability to address these complex queries effectively.</li>
<li>Ambiguous and under-specified protocols. Technical specifications are prone to contain ambiguous and under-specified sentences. These phenomenon and the resulting implications of buggy implementations and security vulnerabilities are known [56]. Expert data would help resolve the ambiguities and provide better context.</li>
</ul>
<p>Therefore, high-quality data contributions from experts in the domain of interest can significantly improve the performance of the model [19].</p>
<p>Hence, NGC Copilot includes mechanisms for incorporating expert feedback and data contributions. Upon receiving a response, the user can optionally like/dislike to provide feedback on the relevance of the response. Additionally, the user can also request expert assistance by clicking a designated button, which will create a corresponding issue in a GitHub repository. This issue will contain the query, context, and response, and can be resolved through contributions from domain experts. At present, only a select few pre-identified experts are capable of resolving these issues. The expert data obtained through this process is then added to the domain-specific database and attributed to the relevant expert as its source.</p>
<p>The creation of an issue in a GitHub repository ensures that the query, context, and response are stored in a central location and can be easily accessed by experts. The participation of domain experts in resolving issues ensures that the system's responses are accurate and reliable. Further, since the response and context is also posted along with the query, the expert may need to add only the information related to incomplete and incorrect portions, rather than writing an entire article answering the user query. This is in contrast to previous approaches [22,44] that attempted to ease the accessibility of specification information by collaborative contributions of entire articles from experts explaining different concepts and procedures.</p>
<p>The attribution of responses to the expert authors ensures that experts receive recognition for their contributions, which is essential for encouraging continued participation and also creates accountability to the information that is being added to the domain-specific database. While limiting the number of experts may create a bottleneck, leading to delays in resolving the issues, the system leaves the possibility to expand the pool of experts or adopting a voting mechanism, similar to Stack Overflow [42], as part of future work.</p>
<h1>4 Evaluation</h1>
<p>NGC Copilot has been built and evaluated using 3GPP Release 16 technical specifications. The text in the specifications is extracted and segmented using docx2txt [47], antiword [28] and PyPDF2 [46] tools. The resulting database consisted of text extracted from 1663 documents, consisting of 161743 sections, and 21.8 million words. The histogram of the number of words in each section highlights a long-tail distribution. $90^{\text {th }}$ percentile of the number of words per section is 360 .</p>
<p>The $n_{-}$words parameter is used to describe approximate maximum of the number of words in a text sample within domain-specific database. The choice of this parameter is guided by the observation that most of the sections have less than 360 words whereas advanced foundation models have the ability to accommodate few thousand words. We chose $n_{-}$words value of 360 to segment the text to ensure that the sections are not too fragmented and so that the prompt to the foundation model can accommodate good number of filtered text samples that provide the necessary context to the user query.</p>
<p>The sentence-BERT all-MiniLM-L6-v2 model [48] is used for embedding the text samples and user queries into word-embedding vectors. FAISS [21] library is used for efficient storing of the embedding vectors in domain-specific database, and for computing cosine similarity between the query vector and all the vectors in the database. Top 3 most similar text samples are appended as supplemental context to the user query. The resulting prompt, generated using LangChain [25] library, is fed into the GPT-4 [39] foundation model. Maximum number of output tokens is set to 1000 and temperature parameter of the foundation model is set to 0 for repeatable answers to the same query. The system has been released as an app service for limited set of 18 researchers and domain experts working in the domain of cellular communication systems.</p>
<h3>4.1 Benchmark dataset</h3>
<p>To evaluate the performance of different LLM-based conversational AI tools for information synthesis of wireless communication specifications, a benchmark dataset of 109 expert queries and reference responses has been created. The responses are free form answers and not categorical. To the best of our knowledge, no such dataset for 3GPP technical specifications is publicly available.</p>
<p>Each query and its associated reference response has been manually developed by subject matter experts, utilizing different sections of the 3GPP Release 16 technical specifications as the basis for their work. Multiple technical specifications, within the 1663 documents, covering different topics in the standards have been selected. Different sections have then been selected within each technical specification. In relation to any specific section, the expert has formulated one or more question and answer pairs if they ascertain that a comprehensive, independent question can be crafted, which could be answered solely based on the textual content within the section. A subset of the queries contained within the benchmark dataset drew inspiration from queries put forth by researchers utilizing the NGC Copilot. The queries span an extensive spectrum of topics that includes NR, mobility procedures, reporting formats, security, core, attributes information, authentication protocols and testing. The queries have been manually reviewed to ensure the diversity of the dataset. The dataset is used only for evaluation purposes and no training or finetuning is performed using the benchmark dataset.</p>
<p>The dataset will be made publicly available. Such an open source dataset potentially facilitates further research in training domain-specific language models related to wireless technologies, serving as benchmark for comparing various AI models in understanding and responding to questions about wireless technical specifications, and developing and improving conversational AI systems in the field of wireless communications. However, the authors acknowledge that the dataset, although diverse, does not cover all possible aspects of the 3GPP specifications due to its small size. Further, 3GPP standards are continually evolving while the dataset will be limited to Release 16 specifications at the time of this writing. The question answer pairs have been constrained to be self-contained to the text described within a section. However, the questions from the user could span information from multiple sections or even multiple technical specifications. Augmenting the current dataset with a larger volume of reference queries and expert data, generating questions that use information from multiple technical specifications, and exploring methodologies that could potentially facilitate the efficient expansion of the benchmark dataset by helping experts using AI tools is part of future work.</p>
<h1>4.2 End-to-end evaluation</h1>
<p>The accuracy and relevance of responses generated by the NGC Copilot is first evaluated against state-of-the-art approaches with respect to the benchmark dataset described in Sec. 4.1. Following that, the paper performs sensitivity analysis of different components incorporated into NGC Copilot.
4.2.1 Compared approaches: NGC Copilot and the following approaches are probed with the same set of test queries selected from the benchmark dataset.</p>
<ul>
<li>ChatGPT: 3GPP technical specifications are publicly accessible. Foundation models like GPT4 and ChatGPT have been trained on vast corpus of publicly available web data including wireless specifications, as evidenced by relevant answers to many questions related to wireless specifications. GPT-4 and ChatGPT represent the state-of-the-art in out-of-the-box solutions available for information synthesis of wireless specifications. For this study, the temperature and output token size of ChatGPT are set at the same values as used in the NGC Copilot .</li>
<li>Bing chat: Bing chat uses Prometheus model [32] built on top of advanced GPT models. The system grounds the responses from foundation model by leveraging information extracted from web searches, ensuring a wide knowledge base that encompasses a variety of domains, including wireless technical specifications.</li>
<li>ChatGPT+Enterprise data: Various closed domain question answering systems have emerged that leverage foundation models to respond to queries based on a specified collection of documents. A representative closed domain question answering system [3], which utilizes GPT-4, is considered and is pointed towards 3GPP Release 16 technical specifications.</li>
<li>PrivateGPT: It is a recent popular closed domain question answering system completely relying on private models hosted locally on premises [45].</li>
</ul>
<h3>4.2.2 Metric of Merit:</h3>
<ul>
<li>Automatic evaluation: The responses obtained from different approaches are compared against the corresponding reference responses in the benchmark dataset, employing a range of both syntactic and semantic evaluation metrics. Standard evaluation metrics like BLEU score [43] and F1-measures for ROUGE-1, ROUGE-2 and ROUGE-L [27] metrics have been computed that effectively gauge the completeness of the response by considering the extent of shared content. Semantic evaluation is conducted using F1-measure of BERTScore [57], which computes the similarity usign BERT-model embeddings of the generated and reference sentences. Lastly, the</li>
</ul>
<p>Table 2. End-to-end comparison using automatic evaluation metrics</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Approach</th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">ROUGE-1</th>
<th style="text-align: center;">ROUGE-2</th>
<th style="text-align: center;">ROUGE-L</th>
<th style="text-align: center;">BERTScore</th>
<th style="text-align: center;">Cosine Similarity</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">NGC Copilot</td>
<td style="text-align: center;">$\mathbf{0 . 3 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 9}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 9}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 2}$</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.49</td>
</tr>
<tr>
<td style="text-align: center;">Bing Chat</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.45</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT+Enterprise Data</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: center;">PrivateGPT</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.21</td>
</tr>
</tbody>
</table>
<p>cosine similarity between Sentence-BERT embeddings [48] of generated and reference responses provides another layer of semantic comparison.
4.2.3 Results. Table 2 presents a quantitative comparison of NGC Copilot against approaches described in Sec. 4.2.1 using several standard metrics. NGC Copilot has consistently outperformed state-of-the-art approaches across all the metrics. The responses from NGC Copilot have achieved an average BLEU score of 0.37 compared to 0.07 obtained for ChatGPT responses. Similar superiority was observed in BERTScore measuring the semantic congruity of the generated responses. This can be attributed to the supplemental context provided by the context extractor. Eventhough the ChatGPT foundation model had access to wireless specifications data during its training, the smaller frequency with which such data is available in web and the ambiguity of the terms across multiple domains makes it challenging for it to learn and understand relevant text patterns.</p>
<p>Bing chat, ChatGPT+Enterprise Data and PrivateGPT approaches explicitly ground the foundation model responses with results from web-search or semantic search over the provided 3GPP technical specifications document. However, they still provide inferior responses. NGC Copilot employs segmentation procedures that are aware of the structure of wireless technical specifications documents and the distribution of words in each of the sections in those documents. Further, NGC Copilot employs word embedding model that works well with the specifications text. Impact of the segmentation procedures and choice of word embedding models is further explored in sections 4.3.5 and 4.3.3 respectively.</p>
<p>To better understand how these evaluation metrics correspond to the quality of the responses, responses generated by different approaches for a sample question on 'mandatory categories in QoS control rule information' are presented below. The technical specification 3GPP TS 23.203 lists the information contained in a QoS rule and which information categories are mandatory in constructing a QoS rule. The reference response is manually generated by an expert as a part of creating benchmark dataset. NGC Copilot correctly lists all the mandatory information categories while ChatGPT and Bing chat generate comprehensible but incorrect answers.
Question: what are mandatory categories in QoS control rule information?
Reference response: The mandatory categories in QoS control rule information are: 1. Rule identifier 2. Precedence 3. Service data flow template 4. QoS class identifier Source: TS R16-23.203 Table 6.5: The QoS rule information
NGC Copilot response: The mandatory categories in QoS control rule information are:1. Rule identifier2. Precedence3. Service data flow template4. QoS class identifierThese categories are essential for defining and managing the Quality of Service (QoS) rules in a network.Source: TS R16-23.203 Table 6.5: The QoS rule information
ChatGPT response: As an AI language model, I don't have access to the latest updates. However, in general, the mandatory categories in QoS control rule information in 5G include:1. QoS Class Identifier (QCI): It defines the priority level of the traffic and the required packet delay, packet loss, and packet error rate.2. Allocation and Retention Priority (ARP): It determines the priority level</p>
<p>of the connection and the resources allocated to it.3. Maximum Bit Rate (MBR): It specifies the maximum data rate that can be transmitted over the connection.4. Guaranteed Bit Rate (GBR): It specifies the minimum data rate that must be provided to the connection.5. Packet Delay Budget (PDB): It defines the maximum delay that can be tolerated by the connection.6. Packet Error Rate (PER): It specifies the maximum acceptable error rate for the connection.7. Priority Level: It determines the priority level of the connection in case of congestion. These categories are essential for ensuring that the network can provide the required level of service to different types of traffic and applications.
Bing Chat response: Quality of Service (QoS) control rule information is used to organize traffic into traffic classes or categories on the basis of whether the traffic matches specific criteria. Classifying network traffic allows you to organize traffic (that is, packets) into traffic classes or categories on the basis of whether the traffic matches specific criteria. The rules fall into three categories: user-level versus computer-level; application versus the network quintuple; and among the network quintuple. By network quintuple, we mean the source IP address, destination IP address, source port, destination port, and protocol (TCP/UDP).
ChatGPT+Enterprise Data response: The QoS rule information contains several pieces of information. The Category field indicates if a certain piece of information is mandatory or not for the construction of a QoS rule, i.e. if it is possible to construct a QoS rule without it. [23203-g30120.pdf]. The mandatory categories in QoS control rule information include Rule identifier, Service data flow template, QoS class identifier, UL-maximum bitrate, DL-maximum bitrate, Guaranteed bit rate, Maximum bit rate, QoS control order, Packet filter, and IP flow mobility Routing Address [23203-g30-120.pdf]. However, the information contained in a QoS rule varies depending on the type of rule. For example, routing rule information includes Rule identifier, Routing information, Precedence, Packet filter, and IP flow mobility Routing Address [23203-g30-124.pdf]. The N6-LAN Traffic Steering Enforcement Control contains Traffic steering policy identifier(s) for steering traffic onto N6-LAN to the appropriate N6 service functions deployed by the operator [23503-gd0-91.pdf]. PrivateGPT response: The Category field indicates if a certain piece of Information is Mandatory or not, i.e., it can be constructed without the said information but there might still exist some cases where this type should never occur hence we need to have that kind of flexibility.</p>
<h1>4.3 Deep dive</h1>
<p>In this section, we conduct an ablation study by evaluating the impact of modifying different components of the NGC Copilot system.
4.3.1 Foundation model. A foundation model is an essential component in NGC Copilot. Different foundation models can be used and each of these models exhibit distinct capabilities, computational costs, and complexities. We considered three foundation models in the form of GPT-4, GPT-3.5-turbo, and Text-curie-001 [39] models. GPT-4 is widely considered as the most advanced foundation large language model with higher computational cost for inference. GPT-3.5-turbo presents a compelling balance between performance and cost. GPT-3.5-turbo provides better performance at an order of magnitude lower cost compared to GPT-3. So, we do not consider GPT-3 model. Text-Curie-001 model presents a cheaper alternative with lower performance.</p>
<p>The results from evaluating different foundation models while keeping the rest of the architecture the same are showcased in Table 3 Integrating GPT-4 significantly outperforms other models across all the metrics, while GPT-3.5-Turbo provides an attractive alternative for cost-sensitive deployments of NGC Copilot as they provide good performance at significantly smaller cost compared to using GPT-4. There is a significant drop in the performance when the model is switched to Text-curie-001.</p>
<p>Table 3. Sensitivity analysis: Foundation models</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">ROUGE-1</th>
<th style="text-align: center;">ROUGE-2</th>
<th style="text-align: center;">ROUGE-L</th>
<th style="text-align: center;">BERTScore</th>
<th style="text-align: center;">Cosine Similarity</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.72</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-turbo</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.69</td>
</tr>
<tr>
<td style="text-align: center;">Text-curie-001</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.58</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. Sensitivity analysis for $k$ value and few-shot learning.</p>
<p>However, it is interesting to note that even the least performing model still outperforms using ChatGPT alone in Table 2 underscoring the importance of appropriate context.
4.3.2 Number of top samples to consider during similarity search. The top- $k$ semantically similar text samples from the domain-specific database provide the additional context to the user query. A small $k$ value results in potentially missing the text samples that are relevant to answering the user query whereas a large $k$ value results in higher inference costs due to the increased input prompt size. Figure 3a illustrates the BLEU, and F1-measures for ROUGE-1 and BERTScore metrics for the $k$ values ranging from 1 to 4 . Considering the top 3 most similar text samples provides a good balance between performance and cost.
4.3.3 Embedding model. Embedding model plays a crucial role in NGC Copilot as it determines the context fed into the foundation model prompt. We evaluated a variety of embedding models by integrating them into NGC Copilot while the rest of the components of the system are at their default values. These models include advanced models from Sentence-BERT [48] like all-MiniLM-L6-v2 and msmarco-distilbert-base-dot-prod-v3 as well as CPU-optimized and quantized models such as ggml-all-MiniLM-L6-v2-f16 [18]. All the text samples in the entire domain-specific database are embedded using each of these embedding models. NGC Copilot integrated with each of the embedding models is evaluated using the queries in the benchmark dataset.</p>
<p>The BLEU score and F1-measures for the ROUGE-1 and BERTScore metrics are illustrated in Table 4. One can observe that the choice of embedding model significantly affects the performance and that the all-MiniLM-L6-v2 model outperforms other models across all the metrics. SentenceBERT is a modification of BERT that is specifically trained for semantic similarity tasks, making it highly effective for information synthesis use case. Training a custom embedding model specifically</p>
<p>Table 4. Comparison of performance metrics for different embedding models.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Embed Model</th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">ROUGE-1</th>
<th style="text-align: center;">BERTScore</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">all-MiniLM-L6-v2</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.79</td>
</tr>
<tr>
<td style="text-align: left;">msmarco-distilbert-base-dot-prod-v3</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.73</td>
</tr>
<tr>
<td style="text-align: left;">ggml-all-MiniLM-L6-v2-f16</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.75</td>
</tr>
</tbody>
</table>
<p>Table 5. Comparison of performance metrics for different prompts.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompt</th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">ROUGE-1</th>
<th style="text-align: center;">BERTScore</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">NGC Copilot prompt</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.79</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT+Enterprise Data prompt</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.76</td>
</tr>
<tr>
<td style="text-align: left;">PrivateGPT prompt</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.71</td>
</tr>
</tbody>
</table>
<p>for wireless communication specifications and evaluating its impact is an interesting research problem and is part of future work.
4.3.4 Few-shot learning. We conducted experiments to evaluate the performance of NGC Copilot under both zero-shot and few-shot settings. For few-shot learning, we supplied the system with an example query, related context and ideal response, and evaluated the responses for queries from the benchmark dataset having seen these examples. For zero-shot learning, we measured how well the system could generate accurate responses to queries related to wireless communication specifications without having seen any specific examples. The results in Figure 3b, show that both zero-shot and few-shot learning can be effective strategies for NGC Copilot. However, when integrated with GPT-3.5-turbo model, few-shot learning provides slightly better performance.
4.3.5 Data segmentation approach. Data segmentation procedures popular with other closed domain question answering systems [3, 45] where the documents are chunked into 1000 words with 100 word overlap between successive chunks of text is implemented while rest of components of the system remain the same. The performance degrades considerably with an average BLEU score and BERTScore F1-measure of 0.21 and 0.69 respectively. This demonstrates the performance improvement resulting from segmenting the text along intuitive section boundaries.
4.3.6 Prompt selection. The prompt that combines the user query and the context has significant affect on the quality of the answers for the questions in the benchmark dataset. NGC Copilot has been evaluated by keeping all the parameters including the few shot examples the same but changing the prompt alone. NGC Copilot uses the prompt 'You are a helpful assistant. Use what you know already to answer the QUESTION. Improve the answer using the following pieces of CONTEXT. Always return the most relevant SOURCE.' which provides improved results compared to the prompts used by alternative approaches, as illustrated in Table 5.
4.3.7 Inference cost. The average cost of each user query, for the queries in the benchmark dataset, is 6.25 cents as the context information is appended to the query. The average cost when GPT-4 model is replaced with GPT-3.5-turbo (see Table 3), the average cost reduces to 0.35 cents without significant reduction in performance. However, the inference cost is expected to reduce in the future with the advancements in foundation LLMs and inference techniques [52].</p>
<h1>5 Responsible AI considerations</h1>
<p>At the start of each session, the tool explicitly states that 'we do not recommend fully automated systems based solely on the outputs and that it is important that humans are still in the loop to correct any mistakes that the system may make'. Furthermore, the answers generated by the system will be cited with specifications and there are mechanisms in place for experts to contribute data and feedback in an accountable manner. All the data used in this paper is in public domain and is not subject to copyright issues.</p>
<p>However, the system may inherit any biases inherent to the text generated from the underlying foundation LLM [5]. For the benchmark dataset creation, guidelines were taken into consideration to be gender-neutral when possible.</p>
<h2>6 Future research directions</h2>
<p>Foundation models possess significant potential to transform the fields of wireless communication systems and mobile computing, with implications across the board from the way the systems are developed, researched, utilized and analyzed. Research in adapting foundation models to wireless communication technologies and mobile computing is still in its nascent stages. This paper aims at providing a brief overview of some of the capabilities of foundation models and present research directions in applying foundation models in these domains.</p>
<h3>6.1 Generation</h3>
<p>Foundation models have displayed remarkable ability in generating text, code and images [35-37] that are often difficult to distinguish from human-generated content. Adapting these capabilities to wireless technology domains to build Copilots that assist developers in generating standardcompliant code that adheres to 3GPP technical specifications, or Copilots that assist standard setting organizations in generating specifications from human intent in natural language will be transformative. Similarly, building Copilots that assist development teams in creating documentation for codebases will boost the productivity.</p>
<h3>6.2 Summarization and Question Answering</h3>
<p>Foundation models have the ability to process vast amounts of information and identify key ideas and themes of text and condensing into short meaningful summaries. Adapting foundation models to summarize a large corpus of technical papers from conferences and journals enables effective learning of a new domain. Similarly, summarization of complex specifications and codebases helps developers get a high level picture of the protocol or code functionality. It would be imperative, however, to carefully consider the copyright constraints. Foundation models' question answering capabilities allow engineers and students to iteratively mine for more and more detailed information.</p>
<h3>6.3 Analysis</h3>
<p>Foundation models perform various natural language processing tasks including text classification, sentiment analysis and named entity recognition. By training the models to recognize the key words associated with different incidents and root causes, one could build Copilots that can assist in performing root cause analysis. Similarly, adapting these models can also assist security specialists and researchers to identify security and other vulnerabilities in technical specifications by training them to recognize problematic, under-specified or ambiguous descriptions.</p>
<h1>6.4 Generating datasets</h1>
<p>To enable thriving research in different directions of applying foundation models to wireless and mobile domains, it is of utmost importance to have benchmark datasets related to different wireless specific tasks - from information synthesis to classifying if a portion of protocol description is under-specified. It would also be important to study and develop evaluation metrics that suit these tasks specific to wireless domain.</p>
<h2>7 Conclusion</h2>
<p>The paper presents a novel application of foundation models for information synthesis of wireless communication specifications. NGC Copilot is a conversational AI tool which features a question-and-answer interface with advanced capabilities for providing accurate and relevant responses related to wireless communication specifications. The system incorporates three essential elements, including a domain-specific database, a context extractor, and a feedback mechanism. It incorporates mechanisms to overcome the limitations of foundation LLMs' input size, for citing information sources and allowing expert data contributions.
Acknowledgments: The author thanks Ganesh Ananthanarayanan, Sharad Agarwal, Michael Brown, Victor Bahl, Tusher Chakraborty, Ranveer Chandra, Landon Cox, Xenofon Foukas, Pankaj Goyal, Siva Kakarla, Mustafa Kasap, Srikanth Kandula, Sanjeev Mehrotra, Munish Mehan, Sudeep Chakravartty, Bozidar Radunovic, Stefan Saroiu, Alec Wolman, Richard Whitehouse, Microsoft Networking Research and Azure for Operators Office of the CTO teams for their valuable feedback and insightful discussions.</p>
<h2>References</h2>
<p>[1] 3GPP. 2019. Release 16 Technical Specfications. https://www.3gpp.org/ftp/Specs/2022-12/Rel-16.
[2] Simran Arora, Avanika Narayan, Mayee F Chen, Laurel J Orr, Neel Guha, Kush Bhatia, Ines Chami, Frederic Sala, and Christopher R. 2022. Ask Me Anything: A simple strategy for prompting language models. arXiv preprint arXiv:2210.02441 (2022).
[3] Azure. 2023. ChatGPT+Enterprise data with Azure. https://github.com/Azure-Samples/azure-search-openai-demo/.
[4] Justus Baron and Kirti Gupta. 2018. Unpacking 3GPP standards. Journal of Economics \&amp; Management Strategy 27, 3 (2018), 433-461.
[5] Rishi Bommasani, Kelly Davis, and Claire Cardie. 2020. Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Online, 4758-4781. https://doi.org/10.18653/v1/2020.aclmain. 431 https://aclanthology.org/2020.acl-main. 431.
[6] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 (2021).
[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877-1901.
[8] Maximillian Chen, Alexandros Papangelis, Chenyang Tao, Seokhwan Kim, Andy Rosenbaum, Yang Liu, Zhou Yu, and Dilek Hakkani-Tur. 2023. PLACES: Prompting language models for social conversation synthesis. arXiv preprint arXiv:2302.03269 (2023).
[9] Yi Chen, Yepeng Yao, XiaoFeng Wang, Dandan Xu, Chang Yue, Xiaozhong Liu, Kai Chen, Haixu Tang, and Baoxu Liu. 2021. Bookworm Game: Automatic Discovery of LTE Vulnerabilities Through Documentation Analysis. In 2021 IEEE Symposium on Security and Privacy (SP). 1197-1214. https://doi.org/10.1109/SP40001.2021.00104
[10] Todor Cooklev. 2004. Wireless communication standards: A study of IEEE 802.11, 802.15, 802.16. IEEE Standards Association.
[11] Thomas Deinlein, Reinhard German, and Anatoli Djanatliev. [n. d.]. Introducing a Toolset for an easy Management of 3GPP Specifications. ([n. d.]).
[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).</p>
<p>[13] Shizhe Diao, Ruijia Xu, Hongjin Su, Yilei Jiang, Yan Song, and Tong Zhang. 2021. Taming pre-trained language models with n-gram representations for low-resource domain adaptation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 3336-3349.
[14] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A Survey for In-context Learning. arXiv preprint arXiv:2301.00234 (2022).
[15] Ericsson. 2022. Adopting neural language models for the telecom domain. https://www.ericsson.com/en/blog/2022/1/neural-language-models-telecom-domain.
[16] GitHub. 2023. GitHub. https://github.com/.
[17] GitHub. 2023. GitHub Issues. https://docs.github.com/en/issues.
[18] GPT4All. 2023. GPT4All embeddings. https://docs.gpt4all.io/.
[19] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Csar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. 2023. Textbooks Are All You Need. arXiv preprint arXiv:2306.11644 (2023).
[20] Henrik Holm. 2021. Bidirectional Encoder Representations from Transformers (BERT) for Question Answering in the Telecom Domain.: Adapting a BERT-like language model to the telecom domain using the ELECTRA pre-training approach.
[21] Jeff Johnson, Matthijs Douze, and Herv Jgou. 2019. Billion-scale similarity search with gpus. IEEE Transactions on Big Data 7, 3 (2019), 535-547.
[22] Grace Montenegro Jtiva, Martha Cecilia Paredes Paredes, and Julio C. Caiza. 2022. Design of a collaborative website to study 5G networks. In 2022 IEEE Sixth Ecuador Technical Chapters Meeting (ETCM). 1-6. https://doi.org/10.1109/ ETCM56276.2022.9935715
[23] Eirini Kalliamvakou, Georgios Gousios, Kelly Blincoe, Leif Singer, Daniel M German, and Daniela Damian. 2014. The promises and perils of mining github. In Proceedings of the 11th working conference on mining software repositories. $92-101$.
[24] Imtiaz Karim, Kazi Samin Mubasshir, Mirza Masfiqur Rahman, and Elisa Bertino. 2023. SPECSG: A Dataset for 5G Cellular Network Protocol Analysis. arXiv preprint arXiv:2301.09201 (2023).
[25] LangChain. 2023. LangChain. https://github.com/hwchase17/langchain.
[26] Aija Elina Leiponen. 2008. Competing through cooperation: The organization of standard setting in wireless telecommunications. Management science 54, 11 (2008), 1904-1919.
[27] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out. Association for Computational Linguistics, Barcelona, Spain, 74-81. https://aclanthology.org/W04-1013
[28] Linux. 2023. Antiword. https://wiki.gentoo.org/wiki/Antiword.
[29] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. Comput. Surveys 55, 9 (2023), $1-35$.
[30] Xiaofei Ma, Peng Xu, Zhiguo Wang, Ramesh Nallapati, and Bing Xiang. 2019. Domain adaptation with BERT-based domain classification and data selection. In Proceedings of the 2nd Workshop on Deep Learning Approaches for LowResource NLP (DeepLo 2019). 76-83.
[31] Grgoire Mialon, Roberto Dess, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozire, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, et al. 2023. Augmented Language Models: a Survey. arXiv preprint arXiv:2302.07842 (2023).
[32] Microsoft. 2023. Bing Chat. https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/.
[33] Netovate. 2022. 3GPP tools. http://netovate.com/doc-search/.
[34] O-RAN. 2023. Technical Specfications. https://www.o-ran.org/specifications.
[35] OpenAI. 2022. ChatGPT. https://openai.com/blog/chatgpt.
[36] OpenAI. 2023. Codex. https://openai.com/blog/openai-codex.
[37] OpenAI. 2023. Dall.E 2. https://openai.com/product/dall-e-2.
[38] OpenAI. 2023. Fine-tuning. https://platform.openai.com/docs/guides/fine-tuning/fine-tuning.
[39] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
[40] OpenAI. 2023. Pricing of the models. https://openai.com/pricing.
[41] OpenAI. 2023. Token size limitations of the models. https://platform.openai.com/docs/models/gpt-3-5.
[42] Stack Overflow. 2023. Stack Overflow. https://stackoverflow.com/.
[43] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 311-318.</p>
<p>[44] Larry Peterson, Oguz Sunay, and Bruce Davie. 2022. Private 5G: A Systems Approach. https://github.com/SystemsApproach/private5g.
[45] PrivateGPT. 2023. PrivateGPT. https://github.com/imartinez/privateGPT.
[46] PyPDF2. 2023. PyPDF2. https://pypi.org/project/PyPDF2/.
[47] Python. 2023. docx2txt. https://pypi.org/project/docx2txt/.
[48] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. https://arxiv.org/abs/1908.10084
[49] Alexander Rietzler, Sebastian Stabinger, Paul Opitz, and Stefan Engl. 2019. Adapt or get left behind: Domain adaptation through bert language model finetuning for aspect-target sentiment classification. arXiv preprint arXiv:1908.11860 (2019).
[50] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili, Daniel Hesslow, Roman Castagn, Alexandra Sasha Luccioni, Franois Yvon, Matthias Gall, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 (2022).
[51] Zhenwei Shao, Zhou Yu, Meng Wang, and Jun Yu. 2023. Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering. arXiv preprint arXiv:2303.01903 (2023).
[52] Sunyan. 2023. The Economics of Large Language Models. https://sunyan.substack.com/p/the-economics-of-large-language-models.
[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).
[54] Flavia-Denisa Veres. 2022. A study into the usability of 3GPP technical specifications. Master's thesis. University of Twente.
[55] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems 32 (2019).
[56] Jane Yen, Tams Lvai, Qinyuan Ye, Xiang Ren, Ramesh Govindan, and Barath Raghavan. 2021. Semi-automated protocol disambiguation and code generation. In Proceedings of the 2021 ACM SIGCOMM 2021 Conference. 272-286.
[57] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675 (2019).
[58] Shuyan Zhou, Uri Alon, Frank F Xu, Zhiruo Wang, Zhengbao Jiang, and Graham Neubig. 2022. Docprompting: Generating code by retrieving the docs. arXiv preprint arXiv:2207.05987 (2022).</p>            </div>
        </div>

    </div>
</body>
</html>
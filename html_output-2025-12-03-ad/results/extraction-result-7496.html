<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7496 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7496</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7496</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-136.html">extraction-schema-136</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <p><strong>Paper ID:</strong> paper-290732e9fb08a29af8892a7c1f73c9d2a1b9d7db</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/290732e9fb08a29af8892a7c1f73c9d2a1b9d7db" target="_blank">Language models show human-like content effects on reasoning</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work evaluates state of the art large language models, as well as humans, and finds that the language models reflect many of the same patterns observed in humans, and models answer more accurately when the semantic content of a task supports the logical inferences.</p>
                <p><strong>Paper Abstract:</strong> Reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks, but exhibit many imperfections. However, human abstract reasoning is also imperfect. For example, human reasoning is affected by our real-world knowledge and beliefs, and shows notable"content effects"; humans reason more reliably when the semantic content of a problem supports the correct logical inferences. These content-entangled reasoning patterns play a central role in debates about the fundamental nature of human intelligence. Here, we investigate whether language models $\unicode{x2014}$ whose prior expectations capture some aspects of human knowledge $\unicode{x2014}$ similarly mix content into their answers to logical problems. We explored this question across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task. We evaluate state of the art large language models, as well as humans, and find that the language models reflect many of the same patterns observed in humans across these tasks $\unicode{x2014}$ like humans, models answer more accurately when the semantic content of a task supports the logical inferences. These parallels are reflected both in answer patterns, and in lower-level features like the relationship between model answer distributions and human response times. Our findings have implications for understanding both these cognitive effects in humans, and the factors that contribute to language model performance.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7496.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7496.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chinchilla</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chinchilla (DeepMind)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based large language model (LLM) from DeepMind evaluated on three classic reasoning tasks in this study; shows content-sensitive reasoning patterns similar to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chinchilla</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based large language model (DeepMind). Evaluated in zero-shot instruction-prompt settings with DC-PMI scoring; confidence measured from log-probability differences.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Natural Language Inference (NLI)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Reasoning (logical inference)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Two-sentence task where a premise and one of two hypothesis completions are presented; model must choose the hypothesis that follows logically (entailment/contradiction neutral in a two-choice completion format).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (proportion correct); model confidence measured as prior-corrected log-prob difference between top and second answers.</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>High accuracy (near ceiling); no statistically significant effect of content on accuracy (mixed-effects: z < 1.21, p > 0.2). Human item-level accuracy correlated with model item-level accuracy (t(832)=3.49, p<0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>High accuracy (near ceiling); relatively little difference across belief-consistent, belief-violating, or nonsense conditions; model confidence related to correctness and to human RTs (see notes).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot with instruction prompt; DC-PMI prior-correction scoring used; few-shot evaluated as robustness (did not eliminate content effects).</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Human participants data collected in this study (new stimuli matched to model items).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>No significant content effect on NLI accuracy for humans or any model (z < 1.21, p > 0.2). Model-human item correlation reported (t(832)=3.49, p<0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Model confidence (log-prob difference) is lower on belief-violating items and correlates negatively with human RTs (problems with higher model confidence tend to have faster human responses). Scoring choices (DC-PMI vs raw likelihood) and presence/absence of instructions affect response biases but not the qualitative content-effect findings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language models show human-like content effects on reasoning', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7496.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7496.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chinchilla</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chinchilla (DeepMind)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based large language model evaluated on syllogism validity judgments; exhibits belief-bias parallel to human participants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chinchilla</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based LLM from DeepMind used in zero-shot instruction-prompt evaluation with prior-corrected scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Syllogism validity judgments</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Reasoning (deductive / logical reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Participants judge whether two premises logically imply a conclusion (valid vs invalid); stimuli include belief-consistent, belief-violating, and nonsense instantiations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Proportion of 'valid' judgments (and overall accuracy); model confidence measured as log-prob difference.</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Moderate accuracy with strong belief-bias: humans are more likely to endorse arguments with believable conclusions even when invalid (replicating prior belief-bias literature). Exact accuracy values not reported in text.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Moderate accuracy; exhibits substantial content effects (bias toward judging arguments as valid when conclusions are belief-consistent). Chinchilla in particular showed strong content-driven responding and some bias toward answering 'valid' on nonsense items.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot instruction prompt; DC-PMI scoring. Raw likelihood scoring and removal of instructions produce different biases but content effects remain robust.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Human participants data collected in this study (new syllogism stimuli matched to model items).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Overall content advantage significant across humans and models (reported: all z >= 2.25 or chi^2 > 6.39, p <= 0.01). Interaction between believability and content not significant for Chinchilla (chi^2 < 0.001, p > 0.99). Item-level human-model accuracy correlation t(345)=4.98, p<0.001.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Chinchilla tended to rely more on content than strictly logical validity (more content-driven than some larger models). Models and humans both showed a slight tendency to call nonsense-item arguments 'valid'. Scoring method affects absolute bias levels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language models show human-like content effects on reasoning', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7496.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7496.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chinchilla</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chinchilla (DeepMind)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based LLM evaluated on the Wason selection task; shows a content advantage for realistic rules similar to human patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chinchilla</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based LLM from DeepMind evaluated zero-shot with instruction prompts and DC-PMI scoring; confidence from log-prob differences analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Wason selection task</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Reasoning (conditional rule reasoning / logical reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Participants choose two cards (forced exactly two) from four to test a conditional rule (If P then Q); stimuli in realistic, arbitrary, and nonsense framings to measure content effects on conditional falsification reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (proportion of correct card-pair selections relative to chance); analysis of component card choices; model confidence measured as log-prob difference.</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Overall human performance low (near chance) on arbitrary/nonsense conditions; a slow subgroup (top ~15% of RTs, >80s) performed above chance on Realistic condition. Human accuracy associated with response time (z=4.44, p<0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Chinchilla showed a significant Realistic advantage (better accuracy on realistic rules) paralleling humans (reported significant for Chinchilla: z > 2.2, p < 0.03); overall models often exceeded human average accuracy on the task.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot instruction prompt; DC-PMI scoring for primary analyses; raw likelihood and few-shot variations tested in robustness checks.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Human participants data collected in this study (two experiments; collapsed for main analyses; one experiment included performance bonus).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Realistic > Arbitrary/Nonsense advantage significant for Chinchilla (z > 2.2, p < 0.03). Human RT associated with accuracy (z = 4.44, p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Behavioral patterns (matching bias, other systematic errors) were analyzed at card-component level; models tended to produce fewer matching errors than humans but had different error distributions (e.g., more Antecedent-False errors). Scoring choices affect error patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language models show human-like content effects on reasoning', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7496.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7496.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM 2-L</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2 Large</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large PaLM 2 family transformer LLM evaluated on NLI, syllogisms, and Wason tasks; shows human-like content effects and generally high sensitivity to logical validity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-L</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large variant of the PaLM 2 family (transformer LLM) evaluated in zero-shot instruction-prompt settings with DC-PMI scoring; confidence measured from log-prob differences.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Large</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Natural Language Inference (NLI)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Reasoning (logical inference)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Two-choice completion NLI task emphasizing strict logical relations (e.g., X is smaller than Y).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (proportion correct); model confidence (prior-corrected log-prob difference).</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>High accuracy (near ceiling); no significant effect of content on accuracy reported (mixed-effects z < 1.21, p > 0.2).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>High accuracy (near ceiling); little content-driven accuracy differences though model confidence shows content sensitivity and correlates with human RTs.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot instruction prompt with DC-PMI scoring; few-shot variants tested as robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Human participants data collected in this study (new NLI stimuli).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>No significant content effect on NLI accuracy for humans or any LM (z < 1.21, p > 0.2). Model confidence negatively associated with human RT (t(655) = -3.39, p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>PaLM 2-L shows item-level correlations with human responses; instruction-tuning (Flan-PaLM 2) did not produce consistent differences in overall accuracy or content effects on this task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language models show human-like content effects on reasoning', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7496.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7496.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM 2-L</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2 Large</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>PaLM 2 Large evaluated on syllogism validity judgments; shows both sensitivity to logical validity and belief-bias effects paralleling humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-L</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large PaLM 2 transformer LLM evaluated zero-shot with instruction prompt and DC-PMI scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Large</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Syllogism validity judgments</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Reasoning (deductive / logical reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Judgment of whether two premises entail a given conclusion; includes believable, unbelievable, and nonsense conclusions to probe belief-bias.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Proportion 'valid' responses and overall accuracy; model confidence (log-prob difference).</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Moderate accuracy; humans strongly biased by conclusion believability (tendency to endorse believable conclusions even when invalid). Exact numeric baseline not provided in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Moderate-to-high sensitivity to logical validity; shows substantial content (belief) biasâ€”models and humans both more likely to endorse belief-consistent conclusions. PaLM 2 family fairly sensitive to logical structure in nonsense condition.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot instruction prompt; DC-PMI scoring. Raw likelihood scoring and removing instructions were explored and affect response biases but not the qualitative content effects.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Human participants data collected in this study (new syllogism stimuli).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Overall content effect significant across humans and models (all z >= 2.25 or chi^2 > 6.39, p <= 0.01). Interaction between believability and content significant in PaLM 2-L (z > 5.9 or chi^2 > 14.3, p < 0.001). Item-level human-model accuracy correlation t(345)=4.98, p<0.001.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>PaLM 2-L shows high sensitivity for identifying valid arguments (good sensitivity) but relatively lower specificity (sometimes labeling invalid conclusions valid). Scoring method influences absolute biases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language models show human-like content effects on reasoning', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7496.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7496.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM 2-L</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2 Large</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>PaLM 2 Large evaluated on the Wason selection task; shows improved accuracy on realistic rule framings similar to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-L</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large PaLM 2 transformer LLM evaluated zero-shot with instruction prompts and DC-PMI scoring; confidence from log-prob differences analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Large</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Wason selection task</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Reasoning (conditional rule reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Choose two cards to test a conditional rule (If P then Q) from four cards; framings include realistic (social/real-world), arbitrary, and nonsense rule instantiations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (proportion of correct two-card choices), component card-choice analysis, model confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Overall human performance low and near chance on arbitrary/nonsense framings; a slow-response subgroup (top ~15%) performed above chance on realistic tasks. Human accuracy positively associated with response time (z = 4.44, p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>PaLM 2-L exhibited a significant advantage for Realistic rules (better accuracy on realistic than arbitrary/nonsense), paralleling human content effect (reported significant: z > 2.2, p < 0.03 for some models including PaLM 2-L). PaLM 2 family showed relatively stronger accuracy on Nonsense problems than some other models.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot instruction prompt; DC-PMI scoring; raw likelihood and few-shot analyses performed as robustness checks.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Human participants data collected in this study (two experiments; collapsed) with some participants given bonus in second experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Realistic advantage significant in PaLM 2-L (z > 2.2, p < 0.03). Human RT associated with accuracy (z = 4.44, p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>PaLM 2-L error patterns differ from humans (fewer matching errors, different distribution of card-component mistakes). Results robust to several scoring and prompting variations, though absolute accuracies shift with method.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language models show human-like content effects on reasoning', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7496.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7496.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM 2-M</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2 Medium</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A medium-sized PaLM 2 variant evaluated across reasoning tasks; tends to be more content-driven on syllogisms relative to some larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-M</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Medium variant of PaLM 2 (transformer LLM) evaluated zero-shot with instruction prompts and DC-PMI scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Medium</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Natural Language Inference (NLI)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Reasoning (logical inference)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Two-choice logical completion NLI task (structured comparisons like 'X is bigger than Y').</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy and model confidence (prior-corrected log-prob difference).</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>High accuracy (near ceiling); no significant content effect on NLI accuracy (z < 1.21, p > 0.2).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>High accuracy (near ceiling) similar to other models on NLI; model confidence shows relationships to correctness and human RTs.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot instruction prompt; DC-PMI scoring; few-shot tested as robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Human participants data collected in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>No significant content effect on NLI accuracy (z < 1.21, p > 0.2). Model confidence negatively associated with human RT (t(655) = -3.39, p < 0.001) reported at task level.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>PaLM 2-M overall less likely than larger variants to separate logical validity from belief-consistency in some conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language models show human-like content effects on reasoning', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7496.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7496.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM 2-M</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2 Medium</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>PaLM 2 Medium evaluated on syllogism validity judgments; more content-driven (less sensitive to abstract logical validity) compared to some larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-M</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Medium PaLM 2 transformer LLM assessed zero-shot with instruction prompt and DC-PMI scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Medium</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Syllogism validity judgments</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Reasoning (deductive / logical reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Judge whether two premises logically imply a conclusion; stimuli include believable, unbelievable, and nonsense conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Proportion 'valid' responses, overall accuracy, model confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Moderate accuracy with strong belief-bias effects; humans more likely to accept believable invalid conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>PaLM 2-M showed pronounced content-driven responding, often judging consistent conclusions as more valid than violating ones irrespective of formal validity; interaction between believability and content not significant for PaLM 2-M (chi^2 < 0.001, p > 0.99).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot instruction prompt; DC-PMI scoring. Raw likelihood scoring and removal of instructions change biases but preserve content effects.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Human participants data collected in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Overall content effect significant across humans and models (all z >= 2.25 or chi^2 > 6.39, p <= 0.01). Interaction not significant in PaLM 2-M (chi^2 < 0.001, p > 0.99). Item-level human-model correlation t(345)=4.98, p<0.001.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Compared to larger PaLM 2 variants, PaLM 2-M relies more on content than pure logical structure in syllogism judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language models show human-like content effects on reasoning', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7496.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7496.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM 2-M</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2 Medium</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>PaLM 2 Medium evaluated on the Wason selection task; showed marginal evidence of realistic-rule advantage but less robust than larger variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2-M</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Medium PaLM 2 transformer LLM evaluated zero-shot with instruction prompts and DC-PMI scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Medium</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Wason selection task</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Reasoning (conditional rule reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Select two cards to test a conditional rule across realistic, arbitrary, and nonsense framings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (correct two-card choices), component choice distributions, model confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Humans overall at-or-below chance in many conditions; slow subgroup performs above chance on Realistic tasks. Human accuracy positively associated with RT (z = 4.44, p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>PaLM 2-M showed only marginally significant Realistic advantage (z >= 1.78, p <= 0.08), with greater item-level variance; did not show as strong a realistic facilitation as some larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot instruction prompt; DC-PMI scoring; robustness checks with raw likelihood and few-shot performed.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Human participants data collected in this study (two experiments; collapsed).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Realistic advantage marginal for PaLM 2-M (z >= 1.78, p <= 0.08). Human RT association with accuracy significant (z = 4.44, p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>PaLM 2-M displayed stronger item-level variance on Wason tasks; PaLM 2 family overall tended to perform relatively well on Nonsense items compared to other models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language models show human-like content effects on reasoning', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7496.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7496.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-PaLM 2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-PaLM 2 (instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned variant of PaLM 2 evaluated on reasoning tasks; shows human-like content effects and high sensitivity in some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-PaLM 2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned PaLM 2 family transformer LLM; evaluated zero-shot with instruction prompts and DC-PMI scoring; has different log-probability distributions compared to base models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Natural Language Inference (NLI)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Reasoning (logical inference)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Two-choice logical NLI completion task designed to be strictly logical comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy; model confidence (prior-corrected log-prob difference).</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>High accuracy (near ceiling); no significant content effect on NLI accuracy (z < 1.21, p > 0.2).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>High accuracy similar to other large models; little content-driven accuracy differences on this simple NLI task though confidence shows content sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot instruction prompt (instruction-tuned model), DC-PMI scoring; few-shot tested.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Human participants data collected in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>No significant content effect on NLI accuracy (z < 1.21, p > 0.2). Model confidence-human RT relationships reported at task level (t(655) = -3.39, p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Instruction tuning did not consistently change the presence of content effects across tasks, though it did affect probability distributions and sometimes bias patterns under raw scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language models show human-like content effects on reasoning', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7496.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7496.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-PaLM 2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-PaLM 2 (instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned PaLM 2 evaluated on syllogism validity judgments; shows content effects and interaction with believability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-PaLM 2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned variant of PaLM 2 family; zero-shot evaluation with DC-PMI scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Syllogism validity judgments</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Reasoning (deductive / logical reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Judgment task whether two premises entail a given conclusion; includes believable, unbelievable, and nonsense conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Proportion 'valid' judgments, overall accuracy, model confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Moderate accuracy; humans biased by believability (endorse believable invalid conclusions at high rates per prior literature).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Shows substantial content effects; significant interaction between content and believability observed in Flan-PaLM 2 (reported z>5.9 or chi^2>14.3, p<0.001 alongside PaLM 2-L and GPT-3.5).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot instruction prompt; DC-PMI scoring. Raw likelihood and instruction removal tested with corresponding changes in bias.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Human participants data collected in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Overall content effect significant (all z >= 2.25 or chi^2 > 6.39, p <= 0.01). Interaction between content and believability significant for Flan-PaLM 2 (z > 5.9 or chi^2 > 14.3, p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Flan-PaLM 2's instruction tuning led to some differences in log-probabilities and bias patterns under different scoring, but content effects remained robust.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language models show human-like content effects on reasoning', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7496.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e7496.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-PaLM 2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-PaLM 2 (instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned PaLM 2 evaluated on the Wason selection task; produced only marginal Realistic advantages in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-PaLM 2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned PaLM 2 transformer LLM evaluated zero-shot with instruction prompt and DC-PMI scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Wason selection task</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Reasoning (conditional rule reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Four-card selection to test a conditional rule; participants/models choose exactly two cards; framings: realistic, arbitrary, nonsense.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (correct two-card choices), card-component choice distribution, model confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Overall human performance low; slow subgroup performs above chance on Realistic condition; human accuracy associated with RT (z = 4.44, p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Flan-PaLM 2 showed only marginally significant advantage for Realistic rules (z >= 1.78, p <= 0.08) in this study, due to some item-level variance.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot instruction prompt; DC-PMI scoring; raw likelihood and few-shot tested as robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Human participants data collected in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Realistic advantage marginal for Flan-PaLM 2 (z >= 1.78, p <= 0.08). Human RT association with accuracy significant (z = 4.44, p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Flan-PaLM 2 error patterns and absolute accuracies are somewhat sensitive to scoring method, but the presence of a realistic-content advantage is robust in direction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language models show human-like content effects on reasoning', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7496.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e7496.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned GPT-3.5 series evaluated on reasoning tasks in this paper; demonstrates human-like content effects and high sensitivity to logical validity in some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned GPT-3.5 transformer LLM evaluated zero-shot (instruction prompt) with DC-PMI scoring; included GPT-3.5-turbo-instruct variant in discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Natural Language Inference (NLI)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Reasoning (logical inference)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Two-choice completion NLI designed to enforce strict logical inference comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy; model confidence (log-prob difference).</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>High accuracy (near ceiling); no significant content-driven accuracy differences reported for NLI (z < 1.21, p > 0.2).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>High accuracy on NLI; model confidence lower on belief-violating items and correlated with human response times.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot instruction prompt (instruction-tuned); DC-PMI scoring used for primary analyses; few-shot tested as robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Human participants data collected in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>No significant content effect on NLI accuracy (z < 1.21, p > 0.2). Model confidence-human RT negative association reported (t(655) = -3.39, p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>GPT-3.5 shows item-level patterns correlated with humans; instruction tuning did not eliminate content effects but influences log-prob distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language models show human-like content effects on reasoning', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7496.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e7496.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 evaluated on syllogism validity; shows strong sensitivity to logical validity with notable belief-bias patterns similar to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned GPT-3.5 transformer LLM evaluated zero-shot with instruction prompts and DC-PMI scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Syllogism validity judgments</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Reasoning (deductive / logical reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Judge whether pair of premises logically entails the conclusion; includes belief-consistent, belief-violating, and nonsense instantiations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Proportion 'valid' judgments, overall accuracy; model confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Moderate accuracy with belief-bias: humans tend to accept believable invalid conclusions at high rates (classic belief-bias effect).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-3.5 shows high sensitivity for identifying valid arguments (high sensitivity) but relatively less specificity (tendency to label some invalid conclusions as valid); exhibits strong interaction between content and believability (significant).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot instruction prompt; DC-PMI scoring. Robustness checks with raw scoring and few-shot performed.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Human participants data collected in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Overall content effect significant (all z >= 2.25 or chi^2 > 6.39, p <= 0.01). Interaction between content and believability significant in GPT-3.5 (z > 5.9 or chi^2 > 14.3, p < 0.001). Item-level correlation t(345)=4.98, p<0.001.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>GPT-3.5 tended to correctly identify valid arguments but sometimes over-endorse invalid ones when conclusions were believable; shows similar qualitative belief-bias patterns as humans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language models show human-like content effects on reasoning', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7496.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e7496.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 evaluated on the Wason selection task; shows a significant realistic-rule advantage matching human patterns and generally higher accuracy than average human participants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned GPT-3.5 transformer LLM assessed zero-shot with instruction prompts and DC-PMI scoring; confidence analysis performed.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Wason selection task</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Reasoning (conditional rule reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Select two cards to evaluate a conditional rule; items include realistic, arbitrary, and nonsense framings; models forced to choose exactly two cards.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (correct two-card picks compared to chance), card-component choice distribution, model confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Humans generally low and near chance across many conditions; slow subgroup (>80s, top 15%) performed above chance on Realistic condition. Human accuracy associated with response time (z = 4.44, p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-3.5 showed a significant Realistic advantage and generally exceeded average human accuracy on this task; Realistic > Arbitrary/Nonsense significant for GPT-3.5 (z > 2.2, p < 0.03).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot instruction prompt; DC-PMI scoring; raw likelihood and few-shot variations evaluated as robustness checks.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Human participants data collected in this study (two experiments; collapsed in main analyses).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Realistic advantage significant for GPT-3.5 (z > 2.2, p < 0.03). Human RT association with accuracy significant (z = 4.44, p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>GPT-3.5's error profile differs from humans (fewer matching bias errors, different component error types). Scoring method changes absolute bias levels but not the direction of content effects.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language models show human-like content effects on reasoning', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7496.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e7496.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Humans</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human participants (this study)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human subjects recruited for directly-comparable experiments with the new stimuli; provide baseline behavioral performance and response times across NLI, syllogism, and Wason tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Human participants</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Human behavioral baseline collected in this study using newly-created stimuli analogous to classic cognitive tasks (NLI, syllogisms, Wason); response times recorded.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Natural Language Inference (NLI)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Reasoning (logical inference)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Human forced-choice completion of logical comparative statements; used to assess whether belief-consistency aids logical choice.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (proportion correct) and response time (RT).</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>High accuracy (near ceiling); no significant content effect on NLI accuracy (mixed-effects z < 1.21, p > 0.2).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>N/A (this entry is human baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>This study: new human participant data collected specifically for these stimuli.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>No significant content effect on NLI accuracy (z < 1.21, p > 0.2). Item-level human-model accuracy correlation t(832)=3.49, p<0.001.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Human responses used to compute item-level RTs which correlated negatively with model confidence on NLI and syllogisms (faster human RTs on items where models were more confident).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language models show human-like content effects on reasoning', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7496.16">
                <h3 class="extraction-instance">Extracted Data Instance 16 (e7496.16)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Humans</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human participants (this study)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human behavioral baseline for syllogism validity judgments showing classic belief-bias effects; used for comparison with LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Human participants</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Human subjects performing syllogism validity judgments on newly-created stimuli; response times recorded.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Syllogism validity judgments</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Reasoning (deductive / logical reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Humans judge whether two premises imply a conclusion; stimuli manipulated for believability and logical validity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Proportion 'valid' judgments, accuracy, response time.</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Moderate accuracy with strong belief-bias: participants often judged invalid but believable conclusions as valid; prior literature example: Evans et al. reported very high endorsement rates for believable invalid syllogisms (e.g., ~90% in classic demonstrations). Exact task-specific accuracy values for this study are reported graphically but not as single numeric values in text.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>N/A (human baseline entry).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>This study (new syllogism stimuli) with replication and extension of prior belief-bias findings.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Overall content effect significant across humans and models (all z >= 2.25 or chi^2 > 6.39, p <= 0.01). Item-level human-model accuracy correlation t(345)=4.98, p<0.001.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Human participants exhibit the classic belief-bias pattern: tendency to accept believable conclusions irrespective of logical validity; models replicate this qualitative effect.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language models show human-like content effects on reasoning', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7496.17">
                <h3 class="extraction-instance">Extracted Data Instance 17 (e7496.17)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Humans</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human participants (this study)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human baseline on the Wason selection task showing low average accuracy but a subgroup of slower responders performs above chance on realistic framings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Human participants</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Participants completed Wason selection task variants (realistic, arbitrary, nonsense); response times and choices recorded; slow/fast subgroup analyses performed.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Wason selection task</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Reasoning (conditional rule reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Select two cards to test a conditional if-then rule; the correct pick is Antecedent True and Consequent False (AT, CF). Data analyzed for matching bias and other systematic errors.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (correct two-card selection relative to chance), card-component choice distributions, response time.</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Majority of subjects at or below chance across conditions; slow subgroup (top ~15% by RT, >80s) showed above-chance performance in Realistic condition but near chance in Arbitrary/Nonsense conditions. Human accuracy associated with RT (z = 4.44, p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>N/A (human baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>This study (two experiments; one with performance bonus for Wason).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Human accuracy significantly associated with RT (z = 4.44, p < 0.001). Slow subgroup performance above chance in Realistic condition (reported qualitatively; exact p-values for subgroup not given in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Humans show matching bias (choose cards that match components of the rule) and other systematic errors. Behavioral patterns are not random even when overall accuracy is near chance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language models show human-like content effects on reasoning', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7496",
    "paper_id": "paper-290732e9fb08a29af8892a7c1f73c9d2a1b9d7db",
    "extraction_schema_id": "extraction-schema-136",
    "extracted_data": [
        {
            "name_short": "Chinchilla",
            "name_full": "Chinchilla (DeepMind)",
            "brief_description": "A transformer-based large language model (LLM) from DeepMind evaluated on three classic reasoning tasks in this study; shows content-sensitive reasoning patterns similar to humans.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Chinchilla",
            "model_description": "Transformer-based large language model (DeepMind). Evaluated in zero-shot instruction-prompt settings with DC-PMI scoring; confidence measured from log-probability differences.",
            "model_size": null,
            "test_name": "Natural Language Inference (NLI)",
            "test_category": "Reasoning (logical inference)",
            "test_description": "Two-sentence task where a premise and one of two hypothesis completions are presented; model must choose the hypothesis that follows logically (entailment/contradiction neutral in a two-choice completion format).",
            "evaluation_metric": "Accuracy (proportion correct); model confidence measured as prior-corrected log-prob difference between top and second answers.",
            "human_performance": "High accuracy (near ceiling); no statistically significant effect of content on accuracy (mixed-effects: z &lt; 1.21, p &gt; 0.2). Human item-level accuracy correlated with model item-level accuracy (t(832)=3.49, p&lt;0.001).",
            "llm_performance": "High accuracy (near ceiling); relatively little difference across belief-consistent, belief-violating, or nonsense conditions; model confidence related to correctness and to human RTs (see notes).",
            "prompting_method": "Zero-shot with instruction prompt; DC-PMI prior-correction scoring used; few-shot evaluated as robustness (did not eliminate content effects).",
            "fine_tuned": false,
            "human_data_source": "Human participants data collected in this study (new stimuli matched to model items).",
            "statistical_significance": "No significant content effect on NLI accuracy for humans or any model (z &lt; 1.21, p &gt; 0.2). Model-human item correlation reported (t(832)=3.49, p&lt;0.001).",
            "notes": "Model confidence (log-prob difference) is lower on belief-violating items and correlates negatively with human RTs (problems with higher model confidence tend to have faster human responses). Scoring choices (DC-PMI vs raw likelihood) and presence/absence of instructions affect response biases but not the qualitative content-effect findings.",
            "uuid": "e7496.0",
            "source_info": {
                "paper_title": "Language models show human-like content effects on reasoning",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Chinchilla",
            "name_full": "Chinchilla (DeepMind)",
            "brief_description": "A transformer-based large language model evaluated on syllogism validity judgments; exhibits belief-bias parallel to human participants.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Chinchilla",
            "model_description": "Transformer-based LLM from DeepMind used in zero-shot instruction-prompt evaluation with prior-corrected scoring.",
            "model_size": null,
            "test_name": "Syllogism validity judgments",
            "test_category": "Reasoning (deductive / logical reasoning)",
            "test_description": "Participants judge whether two premises logically imply a conclusion (valid vs invalid); stimuli include belief-consistent, belief-violating, and nonsense instantiations.",
            "evaluation_metric": "Proportion of 'valid' judgments (and overall accuracy); model confidence measured as log-prob difference.",
            "human_performance": "Moderate accuracy with strong belief-bias: humans are more likely to endorse arguments with believable conclusions even when invalid (replicating prior belief-bias literature). Exact accuracy values not reported in text.",
            "llm_performance": "Moderate accuracy; exhibits substantial content effects (bias toward judging arguments as valid when conclusions are belief-consistent). Chinchilla in particular showed strong content-driven responding and some bias toward answering 'valid' on nonsense items.",
            "prompting_method": "Zero-shot instruction prompt; DC-PMI scoring. Raw likelihood scoring and removal of instructions produce different biases but content effects remain robust.",
            "fine_tuned": false,
            "human_data_source": "Human participants data collected in this study (new syllogism stimuli matched to model items).",
            "statistical_significance": "Overall content advantage significant across humans and models (reported: all z &gt;= 2.25 or chi^2 &gt; 6.39, p &lt;= 0.01). Interaction between believability and content not significant for Chinchilla (chi^2 &lt; 0.001, p &gt; 0.99). Item-level human-model accuracy correlation t(345)=4.98, p&lt;0.001.",
            "notes": "Chinchilla tended to rely more on content than strictly logical validity (more content-driven than some larger models). Models and humans both showed a slight tendency to call nonsense-item arguments 'valid'. Scoring method affects absolute bias levels.",
            "uuid": "e7496.1",
            "source_info": {
                "paper_title": "Language models show human-like content effects on reasoning",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Chinchilla",
            "name_full": "Chinchilla (DeepMind)",
            "brief_description": "A transformer-based LLM evaluated on the Wason selection task; shows a content advantage for realistic rules similar to human patterns.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Chinchilla",
            "model_description": "Transformer-based LLM from DeepMind evaluated zero-shot with instruction prompts and DC-PMI scoring; confidence from log-prob differences analyzed.",
            "model_size": null,
            "test_name": "Wason selection task",
            "test_category": "Reasoning (conditional rule reasoning / logical reasoning)",
            "test_description": "Participants choose two cards (forced exactly two) from four to test a conditional rule (If P then Q); stimuli in realistic, arbitrary, and nonsense framings to measure content effects on conditional falsification reasoning.",
            "evaluation_metric": "Accuracy (proportion of correct card-pair selections relative to chance); analysis of component card choices; model confidence measured as log-prob difference.",
            "human_performance": "Overall human performance low (near chance) on arbitrary/nonsense conditions; a slow subgroup (top ~15% of RTs, &gt;80s) performed above chance on Realistic condition. Human accuracy associated with response time (z=4.44, p&lt;0.001).",
            "llm_performance": "Chinchilla showed a significant Realistic advantage (better accuracy on realistic rules) paralleling humans (reported significant for Chinchilla: z &gt; 2.2, p &lt; 0.03); overall models often exceeded human average accuracy on the task.",
            "prompting_method": "Zero-shot instruction prompt; DC-PMI scoring for primary analyses; raw likelihood and few-shot variations tested in robustness checks.",
            "fine_tuned": false,
            "human_data_source": "Human participants data collected in this study (two experiments; collapsed for main analyses; one experiment included performance bonus).",
            "statistical_significance": "Realistic &gt; Arbitrary/Nonsense advantage significant for Chinchilla (z &gt; 2.2, p &lt; 0.03). Human RT associated with accuracy (z = 4.44, p &lt; 0.001).",
            "notes": "Behavioral patterns (matching bias, other systematic errors) were analyzed at card-component level; models tended to produce fewer matching errors than humans but had different error distributions (e.g., more Antecedent-False errors). Scoring choices affect error patterns.",
            "uuid": "e7496.2",
            "source_info": {
                "paper_title": "Language models show human-like content effects on reasoning",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "PaLM 2-L",
            "name_full": "PaLM 2 Large",
            "brief_description": "A large PaLM 2 family transformer LLM evaluated on NLI, syllogisms, and Wason tasks; shows human-like content effects and generally high sensitivity to logical validity.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM 2-L",
            "model_description": "Large variant of the PaLM 2 family (transformer LLM) evaluated in zero-shot instruction-prompt settings with DC-PMI scoring; confidence measured from log-prob differences.",
            "model_size": "Large",
            "test_name": "Natural Language Inference (NLI)",
            "test_category": "Reasoning (logical inference)",
            "test_description": "Two-choice completion NLI task emphasizing strict logical relations (e.g., X is smaller than Y).",
            "evaluation_metric": "Accuracy (proportion correct); model confidence (prior-corrected log-prob difference).",
            "human_performance": "High accuracy (near ceiling); no significant effect of content on accuracy reported (mixed-effects z &lt; 1.21, p &gt; 0.2).",
            "llm_performance": "High accuracy (near ceiling); little content-driven accuracy differences though model confidence shows content sensitivity and correlates with human RTs.",
            "prompting_method": "Zero-shot instruction prompt with DC-PMI scoring; few-shot variants tested as robustness.",
            "fine_tuned": false,
            "human_data_source": "Human participants data collected in this study (new NLI stimuli).",
            "statistical_significance": "No significant content effect on NLI accuracy for humans or any LM (z &lt; 1.21, p &gt; 0.2). Model confidence negatively associated with human RT (t(655) = -3.39, p &lt; 0.001).",
            "notes": "PaLM 2-L shows item-level correlations with human responses; instruction-tuning (Flan-PaLM 2) did not produce consistent differences in overall accuracy or content effects on this task.",
            "uuid": "e7496.3",
            "source_info": {
                "paper_title": "Language models show human-like content effects on reasoning",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "PaLM 2-L",
            "name_full": "PaLM 2 Large",
            "brief_description": "PaLM 2 Large evaluated on syllogism validity judgments; shows both sensitivity to logical validity and belief-bias effects paralleling humans.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM 2-L",
            "model_description": "Large PaLM 2 transformer LLM evaluated zero-shot with instruction prompt and DC-PMI scoring.",
            "model_size": "Large",
            "test_name": "Syllogism validity judgments",
            "test_category": "Reasoning (deductive / logical reasoning)",
            "test_description": "Judgment of whether two premises entail a given conclusion; includes believable, unbelievable, and nonsense conclusions to probe belief-bias.",
            "evaluation_metric": "Proportion 'valid' responses and overall accuracy; model confidence (log-prob difference).",
            "human_performance": "Moderate accuracy; humans strongly biased by conclusion believability (tendency to endorse believable conclusions even when invalid). Exact numeric baseline not provided in main text.",
            "llm_performance": "Moderate-to-high sensitivity to logical validity; shows substantial content (belief) biasâ€”models and humans both more likely to endorse belief-consistent conclusions. PaLM 2 family fairly sensitive to logical structure in nonsense condition.",
            "prompting_method": "Zero-shot instruction prompt; DC-PMI scoring. Raw likelihood scoring and removing instructions were explored and affect response biases but not the qualitative content effects.",
            "fine_tuned": false,
            "human_data_source": "Human participants data collected in this study (new syllogism stimuli).",
            "statistical_significance": "Overall content effect significant across humans and models (all z &gt;= 2.25 or chi^2 &gt; 6.39, p &lt;= 0.01). Interaction between believability and content significant in PaLM 2-L (z &gt; 5.9 or chi^2 &gt; 14.3, p &lt; 0.001). Item-level human-model accuracy correlation t(345)=4.98, p&lt;0.001.",
            "notes": "PaLM 2-L shows high sensitivity for identifying valid arguments (good sensitivity) but relatively lower specificity (sometimes labeling invalid conclusions valid). Scoring method influences absolute biases.",
            "uuid": "e7496.4",
            "source_info": {
                "paper_title": "Language models show human-like content effects on reasoning",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "PaLM 2-L",
            "name_full": "PaLM 2 Large",
            "brief_description": "PaLM 2 Large evaluated on the Wason selection task; shows improved accuracy on realistic rule framings similar to humans.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM 2-L",
            "model_description": "Large PaLM 2 transformer LLM evaluated zero-shot with instruction prompts and DC-PMI scoring; confidence from log-prob differences analyzed.",
            "model_size": "Large",
            "test_name": "Wason selection task",
            "test_category": "Reasoning (conditional rule reasoning)",
            "test_description": "Choose two cards to test a conditional rule (If P then Q) from four cards; framings include realistic (social/real-world), arbitrary, and nonsense rule instantiations.",
            "evaluation_metric": "Accuracy (proportion of correct two-card choices), component card-choice analysis, model confidence.",
            "human_performance": "Overall human performance low and near chance on arbitrary/nonsense framings; a slow-response subgroup (top ~15%) performed above chance on realistic tasks. Human accuracy positively associated with response time (z = 4.44, p &lt; 0.001).",
            "llm_performance": "PaLM 2-L exhibited a significant advantage for Realistic rules (better accuracy on realistic than arbitrary/nonsense), paralleling human content effect (reported significant: z &gt; 2.2, p &lt; 0.03 for some models including PaLM 2-L). PaLM 2 family showed relatively stronger accuracy on Nonsense problems than some other models.",
            "prompting_method": "Zero-shot instruction prompt; DC-PMI scoring; raw likelihood and few-shot analyses performed as robustness checks.",
            "fine_tuned": false,
            "human_data_source": "Human participants data collected in this study (two experiments; collapsed) with some participants given bonus in second experiment.",
            "statistical_significance": "Realistic advantage significant in PaLM 2-L (z &gt; 2.2, p &lt; 0.03). Human RT associated with accuracy (z = 4.44, p &lt; 0.001).",
            "notes": "PaLM 2-L error patterns differ from humans (fewer matching errors, different distribution of card-component mistakes). Results robust to several scoring and prompting variations, though absolute accuracies shift with method.",
            "uuid": "e7496.5",
            "source_info": {
                "paper_title": "Language models show human-like content effects on reasoning",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "PaLM 2-M",
            "name_full": "PaLM 2 Medium",
            "brief_description": "A medium-sized PaLM 2 variant evaluated across reasoning tasks; tends to be more content-driven on syllogisms relative to some larger models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM 2-M",
            "model_description": "Medium variant of PaLM 2 (transformer LLM) evaluated zero-shot with instruction prompts and DC-PMI scoring.",
            "model_size": "Medium",
            "test_name": "Natural Language Inference (NLI)",
            "test_category": "Reasoning (logical inference)",
            "test_description": "Two-choice logical completion NLI task (structured comparisons like 'X is bigger than Y').",
            "evaluation_metric": "Accuracy and model confidence (prior-corrected log-prob difference).",
            "human_performance": "High accuracy (near ceiling); no significant content effect on NLI accuracy (z &lt; 1.21, p &gt; 0.2).",
            "llm_performance": "High accuracy (near ceiling) similar to other models on NLI; model confidence shows relationships to correctness and human RTs.",
            "prompting_method": "Zero-shot instruction prompt; DC-PMI scoring; few-shot tested as robustness.",
            "fine_tuned": false,
            "human_data_source": "Human participants data collected in this study.",
            "statistical_significance": "No significant content effect on NLI accuracy (z &lt; 1.21, p &gt; 0.2). Model confidence negatively associated with human RT (t(655) = -3.39, p &lt; 0.001) reported at task level.",
            "notes": "PaLM 2-M overall less likely than larger variants to separate logical validity from belief-consistency in some conditions.",
            "uuid": "e7496.6",
            "source_info": {
                "paper_title": "Language models show human-like content effects on reasoning",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "PaLM 2-M",
            "name_full": "PaLM 2 Medium",
            "brief_description": "PaLM 2 Medium evaluated on syllogism validity judgments; more content-driven (less sensitive to abstract logical validity) compared to some larger models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM 2-M",
            "model_description": "Medium PaLM 2 transformer LLM assessed zero-shot with instruction prompt and DC-PMI scoring.",
            "model_size": "Medium",
            "test_name": "Syllogism validity judgments",
            "test_category": "Reasoning (deductive / logical reasoning)",
            "test_description": "Judge whether two premises logically imply a conclusion; stimuli include believable, unbelievable, and nonsense conclusions.",
            "evaluation_metric": "Proportion 'valid' responses, overall accuracy, model confidence.",
            "human_performance": "Moderate accuracy with strong belief-bias effects; humans more likely to accept believable invalid conclusions.",
            "llm_performance": "PaLM 2-M showed pronounced content-driven responding, often judging consistent conclusions as more valid than violating ones irrespective of formal validity; interaction between believability and content not significant for PaLM 2-M (chi^2 &lt; 0.001, p &gt; 0.99).",
            "prompting_method": "Zero-shot instruction prompt; DC-PMI scoring. Raw likelihood scoring and removal of instructions change biases but preserve content effects.",
            "fine_tuned": false,
            "human_data_source": "Human participants data collected in this study.",
            "statistical_significance": "Overall content effect significant across humans and models (all z &gt;= 2.25 or chi^2 &gt; 6.39, p &lt;= 0.01). Interaction not significant in PaLM 2-M (chi^2 &lt; 0.001, p &gt; 0.99). Item-level human-model correlation t(345)=4.98, p&lt;0.001.",
            "notes": "Compared to larger PaLM 2 variants, PaLM 2-M relies more on content than pure logical structure in syllogism judgments.",
            "uuid": "e7496.7",
            "source_info": {
                "paper_title": "Language models show human-like content effects on reasoning",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "PaLM 2-M",
            "name_full": "PaLM 2 Medium",
            "brief_description": "PaLM 2 Medium evaluated on the Wason selection task; showed marginal evidence of realistic-rule advantage but less robust than larger variants.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM 2-M",
            "model_description": "Medium PaLM 2 transformer LLM evaluated zero-shot with instruction prompts and DC-PMI scoring.",
            "model_size": "Medium",
            "test_name": "Wason selection task",
            "test_category": "Reasoning (conditional rule reasoning)",
            "test_description": "Select two cards to test a conditional rule across realistic, arbitrary, and nonsense framings.",
            "evaluation_metric": "Accuracy (correct two-card choices), component choice distributions, model confidence.",
            "human_performance": "Humans overall at-or-below chance in many conditions; slow subgroup performs above chance on Realistic tasks. Human accuracy positively associated with RT (z = 4.44, p &lt; 0.001).",
            "llm_performance": "PaLM 2-M showed only marginally significant Realistic advantage (z &gt;= 1.78, p &lt;= 0.08), with greater item-level variance; did not show as strong a realistic facilitation as some larger models.",
            "prompting_method": "Zero-shot instruction prompt; DC-PMI scoring; robustness checks with raw likelihood and few-shot performed.",
            "fine_tuned": false,
            "human_data_source": "Human participants data collected in this study (two experiments; collapsed).",
            "statistical_significance": "Realistic advantage marginal for PaLM 2-M (z &gt;= 1.78, p &lt;= 0.08). Human RT association with accuracy significant (z = 4.44, p &lt; 0.001).",
            "notes": "PaLM 2-M displayed stronger item-level variance on Wason tasks; PaLM 2 family overall tended to perform relatively well on Nonsense items compared to other models.",
            "uuid": "e7496.8",
            "source_info": {
                "paper_title": "Language models show human-like content effects on reasoning",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Flan-PaLM 2",
            "name_full": "Flan-PaLM 2 (instruction-tuned)",
            "brief_description": "Instruction-tuned variant of PaLM 2 evaluated on reasoning tasks; shows human-like content effects and high sensitivity in some tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Flan-PaLM 2",
            "model_description": "Instruction-tuned PaLM 2 family transformer LLM; evaluated zero-shot with instruction prompts and DC-PMI scoring; has different log-probability distributions compared to base models.",
            "model_size": null,
            "test_name": "Natural Language Inference (NLI)",
            "test_category": "Reasoning (logical inference)",
            "test_description": "Two-choice logical NLI completion task designed to be strictly logical comparisons.",
            "evaluation_metric": "Accuracy; model confidence (prior-corrected log-prob difference).",
            "human_performance": "High accuracy (near ceiling); no significant content effect on NLI accuracy (z &lt; 1.21, p &gt; 0.2).",
            "llm_performance": "High accuracy similar to other large models; little content-driven accuracy differences on this simple NLI task though confidence shows content sensitivity.",
            "prompting_method": "Zero-shot instruction prompt (instruction-tuned model), DC-PMI scoring; few-shot tested.",
            "fine_tuned": false,
            "human_data_source": "Human participants data collected in this study.",
            "statistical_significance": "No significant content effect on NLI accuracy (z &lt; 1.21, p &gt; 0.2). Model confidence-human RT relationships reported at task level (t(655) = -3.39, p &lt; 0.001).",
            "notes": "Instruction tuning did not consistently change the presence of content effects across tasks, though it did affect probability distributions and sometimes bias patterns under raw scoring.",
            "uuid": "e7496.9",
            "source_info": {
                "paper_title": "Language models show human-like content effects on reasoning",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Flan-PaLM 2",
            "name_full": "Flan-PaLM 2 (instruction-tuned)",
            "brief_description": "Instruction-tuned PaLM 2 evaluated on syllogism validity judgments; shows content effects and interaction with believability.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Flan-PaLM 2",
            "model_description": "Instruction-tuned variant of PaLM 2 family; zero-shot evaluation with DC-PMI scoring.",
            "model_size": null,
            "test_name": "Syllogism validity judgments",
            "test_category": "Reasoning (deductive / logical reasoning)",
            "test_description": "Judgment task whether two premises entail a given conclusion; includes believable, unbelievable, and nonsense conditions.",
            "evaluation_metric": "Proportion 'valid' judgments, overall accuracy, model confidence.",
            "human_performance": "Moderate accuracy; humans biased by believability (endorse believable invalid conclusions at high rates per prior literature).",
            "llm_performance": "Shows substantial content effects; significant interaction between content and believability observed in Flan-PaLM 2 (reported z&gt;5.9 or chi^2&gt;14.3, p&lt;0.001 alongside PaLM 2-L and GPT-3.5).",
            "prompting_method": "Zero-shot instruction prompt; DC-PMI scoring. Raw likelihood and instruction removal tested with corresponding changes in bias.",
            "fine_tuned": false,
            "human_data_source": "Human participants data collected in this study.",
            "statistical_significance": "Overall content effect significant (all z &gt;= 2.25 or chi^2 &gt; 6.39, p &lt;= 0.01). Interaction between content and believability significant for Flan-PaLM 2 (z &gt; 5.9 or chi^2 &gt; 14.3, p &lt; 0.001).",
            "notes": "Flan-PaLM 2's instruction tuning led to some differences in log-probabilities and bias patterns under different scoring, but content effects remained robust.",
            "uuid": "e7496.10",
            "source_info": {
                "paper_title": "Language models show human-like content effects on reasoning",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Flan-PaLM 2",
            "name_full": "Flan-PaLM 2 (instruction-tuned)",
            "brief_description": "Instruction-tuned PaLM 2 evaluated on the Wason selection task; produced only marginal Realistic advantages in this study.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Flan-PaLM 2",
            "model_description": "Instruction-tuned PaLM 2 transformer LLM evaluated zero-shot with instruction prompt and DC-PMI scoring.",
            "model_size": null,
            "test_name": "Wason selection task",
            "test_category": "Reasoning (conditional rule reasoning)",
            "test_description": "Four-card selection to test a conditional rule; participants/models choose exactly two cards; framings: realistic, arbitrary, nonsense.",
            "evaluation_metric": "Accuracy (correct two-card choices), card-component choice distribution, model confidence.",
            "human_performance": "Overall human performance low; slow subgroup performs above chance on Realistic condition; human accuracy associated with RT (z = 4.44, p &lt; 0.001).",
            "llm_performance": "Flan-PaLM 2 showed only marginally significant advantage for Realistic rules (z &gt;= 1.78, p &lt;= 0.08) in this study, due to some item-level variance.",
            "prompting_method": "Zero-shot instruction prompt; DC-PMI scoring; raw likelihood and few-shot tested as robustness.",
            "fine_tuned": false,
            "human_data_source": "Human participants data collected in this study.",
            "statistical_significance": "Realistic advantage marginal for Flan-PaLM 2 (z &gt;= 1.78, p &lt;= 0.08). Human RT association with accuracy significant (z = 4.44, p &lt; 0.001).",
            "notes": "Flan-PaLM 2 error patterns and absolute accuracies are somewhat sensitive to scoring method, but the presence of a realistic-content advantage is robust in direction.",
            "uuid": "e7496.11",
            "source_info": {
                "paper_title": "Language models show human-like content effects on reasoning",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5 (OpenAI)",
            "brief_description": "Instruction-tuned GPT-3.5 series evaluated on reasoning tasks in this paper; demonstrates human-like content effects and high sensitivity to logical validity in some tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "Instruction-tuned GPT-3.5 transformer LLM evaluated zero-shot (instruction prompt) with DC-PMI scoring; included GPT-3.5-turbo-instruct variant in discussion.",
            "model_size": null,
            "test_name": "Natural Language Inference (NLI)",
            "test_category": "Reasoning (logical inference)",
            "test_description": "Two-choice completion NLI designed to enforce strict logical inference comparisons.",
            "evaluation_metric": "Accuracy; model confidence (log-prob difference).",
            "human_performance": "High accuracy (near ceiling); no significant content-driven accuracy differences reported for NLI (z &lt; 1.21, p &gt; 0.2).",
            "llm_performance": "High accuracy on NLI; model confidence lower on belief-violating items and correlated with human response times.",
            "prompting_method": "Zero-shot instruction prompt (instruction-tuned); DC-PMI scoring used for primary analyses; few-shot tested as robustness.",
            "fine_tuned": false,
            "human_data_source": "Human participants data collected in this study.",
            "statistical_significance": "No significant content effect on NLI accuracy (z &lt; 1.21, p &gt; 0.2). Model confidence-human RT negative association reported (t(655) = -3.39, p &lt; 0.001).",
            "notes": "GPT-3.5 shows item-level patterns correlated with humans; instruction tuning did not eliminate content effects but influences log-prob distributions.",
            "uuid": "e7496.12",
            "source_info": {
                "paper_title": "Language models show human-like content effects on reasoning",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5 (OpenAI)",
            "brief_description": "GPT-3.5 evaluated on syllogism validity; shows strong sensitivity to logical validity with notable belief-bias patterns similar to humans.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "Instruction-tuned GPT-3.5 transformer LLM evaluated zero-shot with instruction prompts and DC-PMI scoring.",
            "model_size": null,
            "test_name": "Syllogism validity judgments",
            "test_category": "Reasoning (deductive / logical reasoning)",
            "test_description": "Judge whether pair of premises logically entails the conclusion; includes belief-consistent, belief-violating, and nonsense instantiations.",
            "evaluation_metric": "Proportion 'valid' judgments, overall accuracy; model confidence.",
            "human_performance": "Moderate accuracy with belief-bias: humans tend to accept believable invalid conclusions at high rates (classic belief-bias effect).",
            "llm_performance": "GPT-3.5 shows high sensitivity for identifying valid arguments (high sensitivity) but relatively less specificity (tendency to label some invalid conclusions as valid); exhibits strong interaction between content and believability (significant).",
            "prompting_method": "Zero-shot instruction prompt; DC-PMI scoring. Robustness checks with raw scoring and few-shot performed.",
            "fine_tuned": false,
            "human_data_source": "Human participants data collected in this study.",
            "statistical_significance": "Overall content effect significant (all z &gt;= 2.25 or chi^2 &gt; 6.39, p &lt;= 0.01). Interaction between content and believability significant in GPT-3.5 (z &gt; 5.9 or chi^2 &gt; 14.3, p &lt; 0.001). Item-level correlation t(345)=4.98, p&lt;0.001.",
            "notes": "GPT-3.5 tended to correctly identify valid arguments but sometimes over-endorse invalid ones when conclusions were believable; shows similar qualitative belief-bias patterns as humans.",
            "uuid": "e7496.13",
            "source_info": {
                "paper_title": "Language models show human-like content effects on reasoning",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5 (OpenAI)",
            "brief_description": "GPT-3.5 evaluated on the Wason selection task; shows a significant realistic-rule advantage matching human patterns and generally higher accuracy than average human participants.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "Instruction-tuned GPT-3.5 transformer LLM assessed zero-shot with instruction prompts and DC-PMI scoring; confidence analysis performed.",
            "model_size": null,
            "test_name": "Wason selection task",
            "test_category": "Reasoning (conditional rule reasoning)",
            "test_description": "Select two cards to evaluate a conditional rule; items include realistic, arbitrary, and nonsense framings; models forced to choose exactly two cards.",
            "evaluation_metric": "Accuracy (correct two-card picks compared to chance), card-component choice distribution, model confidence.",
            "human_performance": "Humans generally low and near chance across many conditions; slow subgroup (&gt;80s, top 15%) performed above chance on Realistic condition. Human accuracy associated with response time (z = 4.44, p &lt; 0.001).",
            "llm_performance": "GPT-3.5 showed a significant Realistic advantage and generally exceeded average human accuracy on this task; Realistic &gt; Arbitrary/Nonsense significant for GPT-3.5 (z &gt; 2.2, p &lt; 0.03).",
            "prompting_method": "Zero-shot instruction prompt; DC-PMI scoring; raw likelihood and few-shot variations evaluated as robustness checks.",
            "fine_tuned": false,
            "human_data_source": "Human participants data collected in this study (two experiments; collapsed in main analyses).",
            "statistical_significance": "Realistic advantage significant for GPT-3.5 (z &gt; 2.2, p &lt; 0.03). Human RT association with accuracy significant (z = 4.44, p &lt; 0.001).",
            "notes": "GPT-3.5's error profile differs from humans (fewer matching bias errors, different component error types). Scoring method changes absolute bias levels but not the direction of content effects.",
            "uuid": "e7496.14",
            "source_info": {
                "paper_title": "Language models show human-like content effects on reasoning",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Humans",
            "name_full": "Human participants (this study)",
            "brief_description": "Human subjects recruited for directly-comparable experiments with the new stimuli; provide baseline behavioral performance and response times across NLI, syllogism, and Wason tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Human participants",
            "model_description": "Human behavioral baseline collected in this study using newly-created stimuli analogous to classic cognitive tasks (NLI, syllogisms, Wason); response times recorded.",
            "model_size": null,
            "test_name": "Natural Language Inference (NLI)",
            "test_category": "Reasoning (logical inference)",
            "test_description": "Human forced-choice completion of logical comparative statements; used to assess whether belief-consistency aids logical choice.",
            "evaluation_metric": "Accuracy (proportion correct) and response time (RT).",
            "human_performance": "High accuracy (near ceiling); no significant content effect on NLI accuracy (mixed-effects z &lt; 1.21, p &gt; 0.2).",
            "llm_performance": "N/A (this entry is human baseline).",
            "prompting_method": null,
            "fine_tuned": null,
            "human_data_source": "This study: new human participant data collected specifically for these stimuli.",
            "statistical_significance": "No significant content effect on NLI accuracy (z &lt; 1.21, p &gt; 0.2). Item-level human-model accuracy correlation t(832)=3.49, p&lt;0.001.",
            "notes": "Human responses used to compute item-level RTs which correlated negatively with model confidence on NLI and syllogisms (faster human RTs on items where models were more confident).",
            "uuid": "e7496.15",
            "source_info": {
                "paper_title": "Language models show human-like content effects on reasoning",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Humans",
            "name_full": "Human participants (this study)",
            "brief_description": "Human behavioral baseline for syllogism validity judgments showing classic belief-bias effects; used for comparison with LLMs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Human participants",
            "model_description": "Human subjects performing syllogism validity judgments on newly-created stimuli; response times recorded.",
            "model_size": null,
            "test_name": "Syllogism validity judgments",
            "test_category": "Reasoning (deductive / logical reasoning)",
            "test_description": "Humans judge whether two premises imply a conclusion; stimuli manipulated for believability and logical validity.",
            "evaluation_metric": "Proportion 'valid' judgments, accuracy, response time.",
            "human_performance": "Moderate accuracy with strong belief-bias: participants often judged invalid but believable conclusions as valid; prior literature example: Evans et al. reported very high endorsement rates for believable invalid syllogisms (e.g., ~90% in classic demonstrations). Exact task-specific accuracy values for this study are reported graphically but not as single numeric values in text.",
            "llm_performance": "N/A (human baseline entry).",
            "prompting_method": null,
            "fine_tuned": null,
            "human_data_source": "This study (new syllogism stimuli) with replication and extension of prior belief-bias findings.",
            "statistical_significance": "Overall content effect significant across humans and models (all z &gt;= 2.25 or chi^2 &gt; 6.39, p &lt;= 0.01). Item-level human-model accuracy correlation t(345)=4.98, p&lt;0.001.",
            "notes": "Human participants exhibit the classic belief-bias pattern: tendency to accept believable conclusions irrespective of logical validity; models replicate this qualitative effect.",
            "uuid": "e7496.16",
            "source_info": {
                "paper_title": "Language models show human-like content effects on reasoning",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Humans",
            "name_full": "Human participants (this study)",
            "brief_description": "Human baseline on the Wason selection task showing low average accuracy but a subgroup of slower responders performs above chance on realistic framings.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Human participants",
            "model_description": "Participants completed Wason selection task variants (realistic, arbitrary, nonsense); response times and choices recorded; slow/fast subgroup analyses performed.",
            "model_size": null,
            "test_name": "Wason selection task",
            "test_category": "Reasoning (conditional rule reasoning)",
            "test_description": "Select two cards to test a conditional if-then rule; the correct pick is Antecedent True and Consequent False (AT, CF). Data analyzed for matching bias and other systematic errors.",
            "evaluation_metric": "Accuracy (correct two-card selection relative to chance), card-component choice distributions, response time.",
            "human_performance": "Majority of subjects at or below chance across conditions; slow subgroup (top ~15% by RT, &gt;80s) showed above-chance performance in Realistic condition but near chance in Arbitrary/Nonsense conditions. Human accuracy associated with RT (z = 4.44, p &lt; 0.001).",
            "llm_performance": "N/A (human baseline).",
            "prompting_method": null,
            "fine_tuned": null,
            "human_data_source": "This study (two experiments; one with performance bonus for Wason).",
            "statistical_significance": "Human accuracy significantly associated with RT (z = 4.44, p &lt; 0.001). Slow subgroup performance above chance in Realistic condition (reported qualitatively; exact p-values for subgroup not given in main text).",
            "notes": "Humans show matching bias (choose cards that match components of the rule) and other systematic errors. Behavioral patterns are not random even when overall accuracy is near chance.",
            "uuid": "e7496.17",
            "source_info": {
                "paper_title": "Language models show human-like content effects on reasoning",
                "publication_date_yy_mm": "2022-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [],
    "cost": 0.02500775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Language models show human-like content effects on reasoning tasks</h1>
<p>Andrew K. Lampinen, ${ }^{1 <em>}$ Ishita Dasgupta, ${ }^{2 </em>}$<br>Stephanie C. Y. Chan, ${ }^{2}$ Hannah R. Sheahan, ${ }^{1}$ Antonia Creswell, ${ }^{3}$<br>Dharshan Kumaran, ${ }^{1}$ James L. McClelland, ${ }^{2,4}$ Felix Hill, ${ }^{1}$<br>${ }^{1}$ Google DeepMind, London, UK<br>${ }^{2}$ Google DeepMind, Mountain View, CA, USA<br>${ }^{3}$ Work performed at Google DeepMind, London, UK<br>${ }^{4}$ Department of Pscyhology, Stanford University, Stanford, CA, USA<br>*Equal contribution; to whom correspondence should be addressed: {lampinen,idg}@google.com</p>
<h4>Abstract</h4>
<p>Reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks, but exhibit many imperfections. However, human abstract reasoning is also imperfect. For example, human reasoning is affected by our real-world knowledge and beliefs, and shows notable "content effects"; humans reason more reliably when the semantic content of a problem supports the correct logical inferences. These content-entangled reasoning patterns play a central role in debates about the fundamental nature of human intelligence. Here, we investigate whether language models - whose prior expectations capture some aspects of human knowledge - similarly mix content into their answers to logical problems. We explored this question across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task (1). We evaluate state of the art large language models,</p>
<p>as well as humans, and find that the language models reflect many of the same patterns observed in humans across these tasks - like humans, models answer more accurately when the semantic content of a task supports the logical inferences. These parallels are reflected both in answer patterns, and in lower-level features like the relationship between model answer distributions and human response times. Our findings have implications for understanding both these cognitive effects in humans, and the factors that contribute to language model performance.</p>
<h1>Introduction</h1>
<p>A hallmark of abstract reasoning is the ability to systematically perform algebraic operations over variables that can be bound to any entity (2, 3): the statement: ' X is bigger than Y ' logically implies that ' Y is smaller than X ', no matter the values of X and Y . That is, abstract reasoning is ideally content-independent (2). The capacity for reliable and consistent abstract reasoning is frequently highlighted as a crucial missing component of current AI $(4,5,6)$. For example, while large Language Models (LMs) exhibit some impressive emergent behaviors, including some performance on abstract reasoning tasks ( $7,8,9,10,11$; though cf. 12), they have been criticized for failing to achieve systematic consistency in their abstract reasoning (e.g. $13,14,15,16)$.</p>
<p>However, humans - arguably the best known instances of general intelligence - are far from perfectly rational abstract reasoners (17, 18, 19). Patterns of biases in human reasoning have been studied across a wide range of tasks and domains (18). Here, we focus in particular on 'content effects' - the finding that humans are affected by the semantic content of a logical reasoning problem. In particular, humans reason more readily and more accurately about familiar, believable, or grounded situations, compared to unfamiliar, unbelievable, or abstract ones.</p>
<p>For example, when presented with a syllogism like the following:</p>
<div class="codehilite"><pre><span></span><code>All students read.
Some people who read also write essays.
Therefore some students write essays.
</code></pre></div>

<p>humans will often classify it as a valid argument. However, when presented with:</p>
<div class="codehilite"><pre><span></span><code>All students read.
Some people who read are professors.
Therefore some students are professors.
</code></pre></div>

<p>humans are much less likely to say it is valid $(20,21,22)$ - despite the fact that the arguments above are logically equivalent (both are invalid). Similarly, humans struggle to reason about how to falsify conditional rules involving abstract attributes (1, 23), but reason more readily about logically-equivalent rules grounded in realistic situations (24, 25, 26). This human tendency also extends to other forms of reasoning e.g. probabilistic reasoning, where humans are notably worse when problems do not reflect intuitive expectations (27).</p>
<p>The literature on human cognitive biases is extensive, but many of these biases can be idiosyncratic and context-dependent. For example, even some of the seminal findings in the influential work of Kahneman et al. (18), like 'base rate neglect', are sensitive to context and experimental design $(28,29)$, with several studies demonstrating exactly the opposite effect in a different context (30). However, the content effects on which we focus have been a notably consistent finding and have been documented in humans across different reasoning tasks and domains: deductive and inductive, or logical and probabilistic (31, 1, 32, 33, 20, 27). This ubiquity is notable and makes these effects harder to explain as idiosyncracies. This ubiquitous sensitivity to content is in direct contradiction with the definition of abstract reasoning: that it is independent of content, and speaks directly to longstanding debates over the fundamental nature</p>
<p>of human intelligence: are we best described as algebraic symbol-processing systems (2, 34), or emergent connectionist ones $(35,36)$ whose inferences are grounded in learned semantics? Yet explanations or models of content effects in the psychological sciences often focus on a single (task and content-specific) phenomenon and invoke bespoke mechanisms that only apply to these specific settings (e.g. 25). Could content effects be explained more generally? Could they emerge from simple learning processes over naturalistic data?</p>
<p>In this work, we address these questions, by examining whether language models show this human-like blending of logic with semantic content effects. Language models possess prior knowledge - expectations over the likelihood of particular sequences of tokens - that are shaped by their training. Indeed, the goal of the "pre-train and adapt" or the "foundation models" (37) paradigm is to endow a model with broadly accurate prior knowledge that enables learning a new task rapidly. Thus, language model representations often reflect human semantic cognition; e.g., language models reproduce patterns like association and typicality effects $(38,39)$, and language model predictions can reproduce human knowledge and beliefs (40, 41, 42, 43). In this work, we explore whether this prior knowledge impacts a language model's performance in logical reasoning tasks. While a variety of recent works have explored biases and imperfections in language models' performance (e.g. 13, 14, 15, 44, 16), we focus on the specific question of whether content interacts with logic in these systems as it does in humans. This question has significant implications not only for characterizing LMs, but potentially also for understanding human cognition, by contributing new ways of understanding the balance, interactions, and trade-offs between the abstract and grounded capabilities of a system.</p>
<p>We explore how the content of logical reasoning problems affects the performance of a range of large language models (45, 46, 47). To avoid potential dataset contamination, we create entirely new datasets using designs analogous to those used in prior cognitive work, and we also collect directly-comparable human data with our new stimuli. We find that language models</p>
<p>reproduce human content effects across three different logical reasoning tasks (Fig. 1). We first explore a simple Natural Language Inference (NLI) task, and show that models and humans answer fairly reliably, with relatively modest influences of content. We then examine the more challenging task of judging whether a syllogism is a valid argument, and show that models and humans are biased by the believability of the conclusion. We finally consider realistic and abstract/arbitrary versions of the Wason selection task (1) â€” a task introduced over 50 years ago that demonstrates a failure of systematic human reasoning - and show that models and humans perform better with a realistic framing. Our findings with human participants replicate and extend existing findings in the cognitive literature. We also report novel analyses of itemlevel effects, and the effect of content and items on continuous measures of model and human responses. We close with a discussion of the implications of these findings for understanding human cognition as well as language models.</p>
<h1>Evaluating content effects on logical tasks</h1>
<p>In this work, we evaluate content effects on three logical reasoning tasks, which are depicted in Fig. 1. These three tasks involve different types of logical inferences, and different kinds of semantic content. However, these distinct tasks admit a consistent definition of content effects: the extent to which reason is facilitated in situations in which the semantic content supports the correct logical inference, and correspondingly the extent to which reasoning is harmed when semantic content conflicts with the correct logical inference (or, in the Wason tasks, when the content is simply arbitrary). We also evaluate versions of each task where the semantic content is replaced with nonsense non-words, which lack semantic content and thus should neither support nor conflict with reasoning performance. (However, note that in some cases, particularly the Wason tasks, changing to nonsense content requires more substantially altering the kinds of inferences required in the task; see Methods.)</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Consistent</th>
<th style="text-align: center;">Violate</th>
<th style="text-align: center;">Nonsense</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">NLI</td>
<td style="text-align: center;">If seas are bigger than puddles, then puddles are smaller than seas</td>
<td style="text-align: center;">If puddles are bigger than seas, then seas are smaller than puddles</td>
<td style="text-align: center;">If voffs are bigger than feps, then feps are smaller than voffs</td>
</tr>
<tr>
<td style="text-align: center;">Syllogisms</td>
<td style="text-align: center;">All guns are weapons. <br> All weapons are dangerous things. <br> All guns are dangerous things.</td>
<td style="text-align: center;">All dangerous things are weapons. <br> All weapons are guns. <br> All dangerous things are guns.</td>
<td style="text-align: center;">All zoct are spuff. <br> All spuff are thrund. <br> All zoct are thrund.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Realistic</td>
<td style="text-align: center;">Arbitrary</td>
<td style="text-align: center;">Nonsense</td>
</tr>
<tr>
<td style="text-align: center;">Wason</td>
<td style="text-align: center;">If the clients are going skydiving, then they must have a parachute. card: skydiving card: scuba diving card: parachute card: wetsuit</td>
<td style="text-align: center;">If the cards have plural word, then they must have a positive emotion. card: shoes card: dog card: happiness card: anxiety</td>
<td style="text-align: center;">If the cards have more bem, then they must have less stope. card: more bem card: less bem card: less stope card: more stope</td>
</tr>
</tbody>
</table>
<p>Figure 1: Manipulating content within fixed logical structures. In each of our three datasets (rows), we instantiate different versions of the logical problems (columns). Different versions of a problem offer the same logical structures and tasks, but instantiated with different entities or relationships between those entities. The relationships in a task may either be consistent with, or violate real-world semantic relationships, or may be nonsense, without semantic content. In general, humans and models reason more accurately about belief-consistent or realistic situations or rules than belief-violating or arbitrary ones.</p>
<p>Natural Language Inference The first task we consider has been studied extensively in the natural language processing literature (48). In the classic Natural Language Inference (NLI) problem, a model receives two sentences, a 'premise' and a 'hypothesis', and has to classify them based on whether the hypothesis 'entails', 'contradicts', or 'is neutral to' the premise. Traditional datasets for this task were crowd-sourced (49) leading to sentence pairs that don't strictly follow logical definitions of entailment and contradiction. To make this a more strictly logical task, we follow Dasgupta et al. (50) to generate comparisons (e.g. X is smaller than Y). We then give participants an incomplete inference such as "If puddles are bigger than seas, then..." and give them a forced choice between two possible hypotheses to complete it: "seas are bigger than puddles" and "seas are smaller than puddles." Note that one of these completions is consistent with real-world semantic beliefs i.e. 'believable' while the other is logically consistent with the premise but contradicts real world beliefs. We can then evaluate whether models and humans answer more accurately when the logically correct hypothesis is</p>
<p>believable; that is, whether the content affects their logical reasoning.
However, content effects are generally more pronounced in difficult tasks that require extensive logical reasoning $(33,21)$. We therefore consider two more challenging tasks where human content effects have been observed in prior work.</p>
<p>Syllogisms Syllogisms (51) are a simple argument form in which two true statements necessarily imply a third. For example, the statements "All humans are mortal" and "Socrates is a human" together imply that "Socrates is mortal". But human syllogistic reasoning is not purely abstract and logical; instead it is affected by our prior beliefs about the contents of the argument (20, 22, 52). For example, Evans et al. (20) showed that if participants were asked to judge whether a syllogism was logically valid or invalid, they were biased by whether the conclusion was consistent with their beliefs. Participants were very likely ( $90 \%$ of the time) to mistakenly say an invalid syllogism was valid if the conclusion was believable, and thus mostly relied on belief rather than abstract reasoning. Participants would also sometimes say that a valid syllogism was invalid if the conclusion was not believable, but this effect was somewhat weaker (but cf. 53). These "belief-bias" effects have been replicated and extended in various subsequent studies (22, 53, 54, 55). We similarly evaluate whether models and humans are more likely to endorse an argument as valid if its conclusion is believable, or to dismiss it as invalid if its conclusion is unbelievable.</p>
<p>The Wason Selection Task The Wason Selection Task (1) is a logic problem that can be challenging even for subjects with substantial education in mathematics or philosophy. Participants are shown four cards, and told a rule such as: "if a card has a 'D' on one side, then it has a ' 3 ' on the other side." The four cards respectively show 'D', 'F', ' 3 ', and ' 7 '. The participants are then asked which cards they need to flip over to check if the rule is true or false. The correct answer is to flip over the cards showing 'D' and ' 7 '. However, Wason (1) showed that while</p>
<p>most participants correctly chose 'D', they were much more likely to choose ' 3 ' than ' 7 '. That is, the participants should check the contrapositive of the rule ("not 3 implies not D", which is logically implied), but instead they confuse it with the converse (" 3 implies D", which is not logically implied). This is a classic task in which reasoning according to the rules of formal logic does not come naturally for humans, and thus there is potential for prior beliefs and knowledge to affect reasoning.</p>
<p>Indeed, the difficulty of the Wason task depends upon the content of the problem. Past work has found that if an identical logical structure is instantiated in a common situation, particularly a social rule, participants are much more accurate (56, 24, 25, 26). For example, if participants are told the cards represent people, and the rule is "if they are drinking alcohol, then they must be 21 or older" and the cards show 'beer', 'soda', ' 25 ', ' 16 ', then many more participants correctly choose to check the cards showing 'beer' and ' 16 '. We therefore similarly evaluate whether language models and humans are facilitated in reasoning about realistic rules, compared to more-abstract arbitrary ones. (Note that in our implementations of the Wason task, we forced participants and language models to choose exactly two cards, in order to most closely match answer formats between the humans and language models.)</p>
<p>The extent of content effects on the Wason task are also affected by background knowledge; education in mathematics appears to be associated with improved reasoning in abstract Wason tasks (57, 58). However, even those experienced participants were far from perfect - undergraduate mathematics majors and academic mathematicians achieved less than $50 \%$ accuracy at the arbitrary Wason task (57). This illustrates the challenge of abstract logical reasoning, even for experienced humans. As we will see in the next section, many human participants did struggle with the abstract versions of our tasks.</p>
<h1>Results</h1>
<h2>Content effects on accuracy</h2>
<p>We summarize our primary results in Fig. 2. In each of our three tasks, humans and models show similar levels of accuracy across conditions. Furthermore, humans and models show similar content effects on each task, which we measure as the degree of advantage when reasoning about logical situations that are consistent with real-world relationships or rules. In the simplest Natural Language Inference task, humans and all models show high accuracy and relatively minor effects of content. When judging the validity of syllogisms, both humans and models show more moderate accuracy, and significant advantages when content supports the logical inference. Finally, on the Wason selection task, humans and models show even lower accuracy, and again substantial content effects. We describe each task, and the corresponding results and analyses, in more detail below.</p>
<p>Natural Language Inference The relatively simple logical reasoning involved in this task means that both humans and models exhibit high performance, and correspondingly show relatively little effect of content on their reasoning (Fig. 3). Specifically, we do not detect a statistically-significant effect of content on accuracy in humans or any of the language models in mixed-effects logistic regressions controlling for the random effect of items (or $\chi^{2}$ tests where regressions did not converge due to ceiling effects; all $z&lt;1.21$ or $\chi^{2}&lt;0.1$, all $p&gt;0.2$; see Appx. C. 1 for full results). However, we do find a statistically significant relationship between human and model accuracy at the item level $(t(832)=3.49, p&lt;0.001$; Appx. 27) â€” even when controlling for condition. Furthermore, as we discuss below, further investigation into the model confidence shows evidence of content effects on this task as well.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Across the three tasks we consider, various language models and humans show similar patterns of overall accuracy and content effects on reasoning. The vertical axis shows accuracy when the content of the problems supports the logical inference. The horizontal axis shows accuracy when the content conflicts (or, in the Wason task, when it is arbitrary). Thus, points above the diagonal indicate an advantage when the content supports the logical inference. (a) On basic natural language inferences, both humans and LMs demonstrate fairly high accuracy across all conditions, and thus relatively little effect of content. (a) When identifying whether syllogisms are logically valid or invalid, both humans and LMs exhibit moderate accuracy, and substantial content effects. (c) On the Wason selection task, the majority of humans show fairly poor performance overall. However, the subset of subjects who are take the longest to answer show somewhat higher accuracy, but primarily on the realistic tasks - i.e. substantial content effects. On this difficult task, language models generally exceed humans in both accuracy and magnitude of content effects. (Throughout, errorbars are bootstrap $95 \%$-CIs, and dashed lines are chance performance.)</p>
<p>Syllogisms Syllogism validity judgements are significantly more challenging than the NLI task above; correspondingly, we find lower accuracy in both humans and language models. Nevertheless, humans and most language models are sensitive to the logical structure of the task. However, we find that both humans and language models are strongly affected by the content of the syllogisms (Fig. 4), as in the past literature on syllogistic belief bias in humans (21). Specifically, if the semantic content supports the logical inference - that is, if the conclusion is believable and the argument is valid, or if the conclusion is unbelievable and the argument is invalid - both humans and all language models tend to answer more accurately (all $z \geq 2.25$</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Detailed results on the Natural Language Inference tasks. Both humans (left) and all models show relatively high performance, and relatively little difference in accuracy between belief-consistent and belief-violating inferences, or even nonsense ones.
or $\chi^{2}&gt;6.39$, all $p \leq 0.01$; see Appx. C. 2 for full results).
Two simple effects contribute to this overall content effect - that belief-consistent conclusions are judged as logically valid and that belief-inconsistent conclusions are judged as logically invalid. As in the past literature, we find that the dominant effect is that humans and models tend to say an argument is valid if the conclusion is belief-consistent, regardless of the actual logical validity. If the conclusion is belief-violating, humans and models do tend to say it is invalid more frequently, but most humans and models are more sensitive to actual logical validity in this case. Specifically, we observe a significant interaction between the content effect and believability in humans, PaLM 2-L, Flan-PaLM 2, and GPT-3.5 (all $z&gt;5.9$ or $\chi^{2}&gt;14.3$, all $p&lt;0.001$ ); but do not observe a significant interaction in Chinchilla or PaLM 2-M (both $\chi^{2}&lt;0.001, p&gt;0.99$ ). Both humans and models appear to show a slight bias towards saying syllogisms with nonsense words are valid, but again with some sensitivity to the actual logical structure.</p>
<p>Furthermore, even when controlling for condition, we observe a significant correlation between item-level accuracy in humans and language models $(t(345)=4.98, p&lt;0.001)$, sug-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Detailed results on syllogism validity judgements. The vertical axis shows the proportion of the time that each system answers that an argument is valid. Both humans and models exhibit substantial content effects - they are strongly biased towards saying an argument is valid if the conclusion is consistent with expectations (cyan), and somewhat biased towards saying the argument is invalid if the conclusion violates expectations (maroon). If the argument contains nonsense words (grey), both humans and models show a slight bias towards saying "valid." (Note that this figure plots the proportion of the time the humans or models answer 'valid' rather than raw accuracy, to more clearly illustrate the bias. To see accuracy, simply reverse the vertical axis for the invalid arguments.)
gesting shared patterns in the use of lower-level details of the logic or content.</p>
<p>The Wason Selection Task As in the prior human literature, we found that the Wason task was relatively challenging for humans, as well as for language models (Fig. 5). Nevertheless, we observed significant content advantages for the Realistic tasks in humans, and in Chinchilla, PaLM 2-L, and GPT-3.5 (all $z&gt;2.2$, all $p&lt;0.03$; Appx. C.3). We only observed marginally significant advantages of realistic rules in PaLM 2-M and Flan-PaLM 2 (both $z \geq 1.78$, both $p \leq 0.08$ ), due to stronger item-level effects in these models (though the item-level variance does not seem particularly unusual; see Appx. B.7.3 for further analysis). Intriguingly, some language models also show better performance at the versions of the tasks with Nonsense nouns</p>
<p>compared to the Arbitrary ones, though generally Realistic rules are still easier. We also consider several variations on these rules in Appx. 22.</p>
<p>Our human participants struggled with this task, as in prior research, and did not achieve significantly higher-than-chance performance overall â€” although their behavior is not random, as we discuss below, where we analyze answer choices in more detail. However, spending longer on logical tasks can improve performance (59, 60), and thus many studies split analyses by response time to isolate participants who spend longer, and therefore show better performance (61, 62). Indeed, we found that human accuracy was significantly associated with response time (z=4.44, p &lt; 0.001; Appx. C.3.1). We depict this relationship in Fig. 6. To visualize the the performance of discrete subjects in our Figures 2c and 5, we split subjects into â€˜slowâ€™ and â€˜fastâ€™ groups. The distribution of times taken by subjects is quite skewed, with a long tail. We separate out the top 15% of subjects that take the longest, who spent more than 80 seconds on the problem, as the slow group. These subjects showed above chance performance in the Realistic condition, but still performed near chance in the other conditions. We also dig further into the predictive power of human response times in the other tasks in the following sections (and Appx. B.6.1).</p>
<p>We collected the data for the Wason task in two different experiments; after observing the lower performance in the first sample, we collected a second sample where we offered a performance bonus for this task. We did not observe significant differences in overall performance or content effects between these subsets, so we collapse across them in the main analyses; however, we present results for each experiment and some additional analyses in Appx. B.5.</p>
<h3>Robustness of results to factors like removing instructions, few-shot prompting and scoring methods</h3>
<h4>Language model behavior is frequently sensitive to details of the evaluation.</h4>
<p>Thus, we performed several experiments to confirm that our results were robust to details of the</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Detailed results on the Wason selection task. Human performance is low, even on the Realistic rules. In particular, the majority of the subjects show at-or-below-chance accuracy in all conditions (though this behavior is not random; see below). However, the subset of subjects who answer more slowly show above chance accuracy for the realistic rules (cyan), but not for the arbitrary ones (pink). This pattern matches the prior results in the cognitive literature. Furthermore, each of the language models reproduces this pattern of advantage for the realistic rules. In addition, two of the larger models perform above chance at the arbitrary rules. (The dashed line corresponds to chance - a random choice of two cards among the four shown. Both models and humans were forced to choose exactly two cards.)
methods used. We present these results in full in Appx. B.2, but we outline the key experiments here. First, we show that removing the pre-question instructions does not substantially alter the overall results (Appx.B.2.1). Next, we show that our use of the DC-PMI correction for scoring is not the primary driver of content effects (Appx. B.2.2). On the syllogisms tasks, raw likelihood scoring with the instruction prompt yields strong answer biases - several models say every argument is valid irrespective of actual logical validity or content. However, the models that don't uniformly say valid show content effects as expected. Furthermore, if the instructions before the question are removed, raw likelihood scoring results in less validity bias, and again strong content effects. For the Wason task, raw likelihood scoring actually improves the accuracy of some models; however, again the content effects are as found with the DC-PMI scoring. Thus, although overall model accuracy and response biases change with uncorrected</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: There is a strong relationship between response time and answer accuracy in the
Wason tasks; subjects who take longer to answer are more accurate on average. Participants
who take sufficiently long to answer perform above chance in the Realistic tasks. There are
hints of a similar effect in the Arbitrary condition, but we do not have the power to detect it.
(Curves are logistic regression fits, with 95% CIs. We also plot regressions dropping outliers
with time greater than 180 seconds, to show that the effect is not driven solely by outliers.)</p>
<p>likelihood scoring, the content effects are similar. Finally, we consider few-shot evaluation, and
show that giving few-shot examples yields some mild improvements in accuracy (with greater
improvement in the simpler tasks), but does not eliminate the content effects (Appx. B.2.4).
Together, these results suggest that our findings are not strongly driven by idiosyncratic details
of our evaluation, and thus support the robustness of our findings.</p>
<p><strong>Variability across different language models</strong> While we generally find similar content effects
across the various models we evaluate, there are some notable differences among them. First,
across tasks the larger models tend to be more accurate overall (e.g., comparing the large vs.
the medium variants of PaLM 2); however, this does not necessarily mean they show weaker
content effects. While it might be expected that instruction-tuning would affect performance,
the instruction-tuned models (Flan-PaLM 2 and GPT-3.5-turbo-instruct) do not show consistent
differences in overall accuracy or content effects across tasks compared to the base language</p>
<p>modelsâ€”in particular, Flan-PaLM 2 performs quite similarly to PaLM 2-L overall. (However, there are some more notable differences in the distributions of log-probabilities the instruction-tuned models produce; Appx. B.8.)</p>
<p>On the syllogisms task in particular, there are some noticeable differences among the models. GPT-3.5, and the larger PaLM 2 models, have quite high sensitivity for identifying valid arguments (they generally correctly identify valid arguments) but relatively less specificity (they also consider several invalid conclusions valid). By contrast, PaLM 2-M and Chinchilla models answer more based on content rather than logical validity, i.e., regularly judging consistent conclusions as more valid than violating ones, irrespective of their logical validity. The sensitivity to logical structure in the nonsense condition also varies across modelsâ€”the PaLM models are fairly sensitive, while GPT 3.5 and Chinchilla both have a strong bias toward answering valid to all nonsense propositions irrespective of actual logical validity.</p>
<p>On the Wason task, the main difference of interest is that the PaLM 2 family of models show generally greater accuracy on the Nonsense problems than the other models do, comparable to their performance on the Realistic condition in some cases.</p>
<h2>Model confidence is related to content, correctness, and human response times</h2>
<p>Language models do not produce a single answer; rather, they produce a probability distribution over the possible answers. This distribution can provide further insight into their processing. For example, the probability assigned to the top answers, relative to the others, can be interpreted as a kind of confidence measure. By this measure, language models are often somewhat calibrated, in the sense that the probability they assign to the top answer approximates the probability that their top answer is correct (e.g., 63). Furthermore, human Response Times (RTs) relate to many similar variables, such as confidence, surprisal, or task difficulty; thus, many prior works have</p>
<p>related language model confidence to human response or reaction times for linguistic stimuli (e.g. 64, 65). In this section, we correspondingly analyze how the language model confidence relates to the task content and logic, the correctness of answers, and the human response times.</p>
<p>We summarize these results in Fig. 7. We measure model confidence as the difference in prior-corrected log-probability between the top answer and the second highest-thus, if the model is almost undecided between several answers, this confidence measure will be low, while if the model is placing almost all its probability mass on a single answer, the confidence measure will be high. In mixed-effects regressions predicting model confidence from task variables and average human RTs on the same problem, we find a variety of interesting effects. First, language models tend to be more confident on correct answers (that is, they are somewhat calibrated). Task variables also affect confidence; models are generally less confident when the conclusion violates beliefs, and more confident for the realistic rules on the Wason task.Furthermore, even when controlling for task variables and accuracy, there is a statistically-significant negative association with human response times on the NLI and syllogisms tasks (respectively $t(655)=$ $-3.39, p&lt;0.001$; and $t(353)=-2.03, p&lt;0.05$; Appx. C.4)â€”that is, models tend to show more confidence on problems where humans likewise respond more rapidly. We visualize this relationship in Fig. 8.</p>
<h1>Analyzing components of the Wason responses</h1>
<p>Because each answer to the Wason problems involves selecting a pair of cards, we further analyzed the individual cards chosen. The card options presented each problem are designed so that two cards respectively match and violate the antecedent, and similarly for the consequent. The correct answer is to choose one card for the antecedent and one for the consequent; more precisely, the card for which the antecedent is true (AT), and the card for which the consequent is false (CF). In Fig. 9 we examine human and model choices; we quantitatively analyze these</p>
<p>choices using a multinomial logistic regression model in Appx. C.3.2.
Even in conditions when performance is close to chance, behavior is generally not random. As in prior work, humans do not consistently choose the correct answer (AT, CF). Instead, humans tend to exhibit the matching bias; that is, they tend to choose each of the two cards that match each component of the rule (AT, CT). However, in the Realistic condition, slow humans answer correctly somewhat more frequently. Humans also exhibit errors besides the matching bias; including an increased rate of choosing the two cards corresponding to a single component of the rule - either both antecedent cards, or both consequent cards. Language models tend to give more correct responses than humans, and to show facilitation in the realistic rules compared to arbitrary ones. Relative to humans, language models show fewer matching errors, fewer errors of choosing two cards from the same rule component, but more errors of choosing the antecedent false options. These differences in error patterns may indicate differences between the response processes engaged by the models and humans. (Note, however, that while the models accuracies do not change too substantially with alternate scoring methods, the particular errors the models make are somewhat sensitive to scoring method - without the DC-PMI correction the model errors more closely approximate the human ones in some cases; Appx. B.2.3.)</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Language model confidence-as measured by the difference in(prior-corrected) logprobability between the chosen answer and the next most probable-is associated with correct answers, task variables, and human average response times. (a-b) On the NLI and syllogism tasks, models are generally more confident in correct answers and belief-consistent conditions, less confident in belief-violating conditions, and less confident on problems that humans take longer to answer. (c) On the Wason task, effects are weaker. Human RT and correct answers are not associated with confidence; however, the models do show more confidence on Realistic problems, and less on Nonsense ones. (Effects are calculated from a mixed-effects regression predicting the difference in log-probability between the top and second-highest answer, z-scored within each model, and controlling for all other significant predictors. Errorbars are parametric $95 \%$-CIs. Note that human RT is calculated across all human subjects for the Wason task, not just slow subjects.)</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" />
(b) Syllogisms raw results.</p>
<p>Figure 8: Human response times are generally negatively related to model confidence (measured as the difference in log-probabilities between the correct answer and the incorrect answer). That is, on problems for which the model displays greater confidence, humans tend to respond more quickly. This relationship holds on both (a) the NLI tasks, and (b) the syllogism tasks. (Points show average response times for individual problems, broken down by whether the humans or models answered correctly or not; see Appx. C. 4 for details.)</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Answer patterns for the Wason tasks, broken down into components: the pairings of individual cards that each participant chose ( $\mathrm{AT}=$ Antecedent True, $\mathrm{CF}=$ Consequent False, etc.). Behavior is not random, even when performance is near chance. As above, humans do not consistently choose the correct answer (AT, CF; dark blue); instead, humans more frequently exhibit the matching bias (AT, CT; light blue). Humans also show other errors, however, including a surprisingly high rate of choosing two cards corresponding to a single rule component (dark/light pink). Language models answer correctly more often than humans, but intriguingly choose options with the antecedent false and a consequent card (yellow/orange) more frequently. (Note that all participants and language models were forced to choose exactly two cards.)</p>            </div>
        </div>

    </div>
</body>
</html>
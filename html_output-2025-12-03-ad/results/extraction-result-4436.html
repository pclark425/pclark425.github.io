<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4436 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4436</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4436</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-279402998</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.12689v2.pdf" target="_blank">SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation</a></p>
                <p><strong>Paper Abstract:</strong> The rapid growth of scientific literature demands robust tools for automated survey-generation. However, current large language model (LLM)-based methods often lack in-depth analysis, structural coherence, and reliable citations. To address these limitations, we introduce SciSage, a multi-agent framework employing a reflect-when-you-write paradigm. SciSage features a hierarchical Reflector agent that critically evaluates drafts at outline, section, and document levels, collaborating with specialized agents for query interpretation, content retrieval, and refinement. We also release SurveyScope, a rigorously curated benchmark of 46 high-impact papers (2020-2025) across 11 computer science domains, with strict recency and citation-based quality controls. Evaluations demonstrate that SciSage outperforms state-of-the-art baselines (LLM x MapReduce-V2, AutoSurvey), achieving +1.73 points in document coherence and +32% in citation F1 scores. Human evaluations reveal mixed outcomes (3 wins vs. 7 losses against human-written surveys), but highlight SciSage's strengths in topical breadth and retrieval efficiency. Overall, SciSage offers a promising foundation for research-assistive writing tools.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4436.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4436.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciSage</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SCISAGE: A Multi-Agent Framework for High-Quality Scientific Survey Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent, LLM-driven pipeline for automated survey writing that coordinates specialized agents (Interpreter, Organizer, Collector, Composer, Refiner) plus a hierarchical Reflector that iteratively critiques drafts at outline, section and document levels to improve coherence and citation accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SciSage</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Multi-agent architecture: Interpreter (query understanding & rewrite), Organizer (multi-LLM ensemble outline generation and search-query extraction), Collector (multi-source adaptive retrieval and reranking using APIs like arXiv/PubMed/Google Scholar), Composer (bottom-up hierarchical synthesis from atomic subsection content to section and full-document assembly, including figure/table heuristics), Refiner (final polishing, citation deduplication/renumbering, format export), and Reflector (iterative hierarchical critique loop operating at outline/section/document levels using panels of LLM 'expert' personas and majority-vote to trigger re-retrieval/regeneration). The pipeline follows a generate–reflect–regenerate paradigm and produces LaTeX/Markdown outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Multi-LLM ensemble for outline generation (unspecified); experiments and all implemented baselines in this paper used QWEN3-32B</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Multi-source adaptive retrieval (APIs to arXiv/PubMed/Google Scholar), semantic relevance scoring and reranking by recency/venue/author/citation, heuristic extraction of figures/tables from LaTeX, and section-level retrieval guided by Organizer-generated search queries.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Hierarchical bottom-up synthesis: atomic subsection synthesis (citation-rich) -> assemble sections -> integrate chapters -> full-document draft; iterative hierarchical reflection (generate–reflect–regenerate) with targeted re-retrieval and regeneration; mindmap generation for high-level structure.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Evaluated on SurveyScope benchmark (46 curated papers); reference-alignment comparisons used human-cited sets totaling 3,844 references (SciSage matched 1,510 true positives).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Computer science surveys across 11 topics (NLP, LLMs, AI safety, robotics, multimodal learning, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Automated literature surveys / long-form scientific reviews (exportable to LaTeX/Markdown).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>LLM-based automatic metrics: content quality (language fluency, critical thinking), structural coherence (section-level and document-level), reference accuracy (TP, precision/recall, F1); plus human expert judgments on multiple dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Document-level coherence 80.37; language fluency 85.65; critical thinking 77.58; citation F1 = 0.46 (1,510 true positives matched out of 3,844 human-cited references). The paper reports SciSage outperforms baselines by +1.73 points in document coherence and +32% in citation F1.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>LLM×MapReduce-V2, AutoSurvey, OpenScholar (integrated with SciSage outlines for one variant).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Outperforms LLM×MapReduce-V2 on document coherence (80.37 vs 78.64) and substantially on citation F1 (0.46 vs 0.017 per Table 4); outperforms AutoSurvey and OpenScholar across most automatic metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Iterative hierarchical reflection (Reflector agent) and specialized multi-agent roles materially improve document-level coherence and citation accuracy; query understanding (Interpreter) improves outline quality; multi-source reranking increases retrieval precision; bottom-up synthesis preserves local factual grounding before global integration.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Limited analytical depth and precise mathematical expression (weak on formal math-heavy fields), English-only evaluation, dependence on the chosen foundation model (QWEN3-32B in experiments), tendency to produce long/verbosity-heavy text, and evaluation metric saturation for topical relevance and section coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Designed to handle long corpora via Composer and MapReduce-like assembly; reflection loops (outline trials up to 10, section trials up to 20) improve scores; evaluated on a 46-paper benchmark—paper reports improved metrics with reflection and query-understanding but does not provide formal parametric scaling laws across model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4436.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4436.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoSurvey: Large language models can automatically write surveys</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage LLM-based survey generation pipeline that performs retrieval, outline drafting, subsection generation, and evaluation, used as a baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autosurvey: Large language models can automatically write surveys.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Two-stage pipeline: (1) retrieval-augmented retrieval from an offline corpus and outline drafting, and (2) parallel subsection generation and evaluation to produce a survey. Emphasizes logical parallel generation to improve content quality and citation handling.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Implemented with QWEN3-32B for the baselines/experiments reported in this paper (original system details not fully described here).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Retrieval-augmented generation using an offline corpus; standard IR to fetch candidate papers/snippets for summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Outline-first then subsection generation (two-stage aggregation), using parallel logical generation for subsections.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to the same evaluation topics in this paper (SurveyScope benchmark—tasks drawn from the 46-paper set); uses an offline corpus for retrieval rather than online multi-source retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Automated survey generation in computer science domains.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Automated literature surveys / review articles.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same automatic metrics used in this paper (language fluency, critical thinking, topical relevance, section/document structural coherence, reference accuracy measured by TP/F1) and human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported lower performance than SciSage in this paper; Table 4 (as used by the authors) lists AutoSurvey citation F1 ≈ 0.143 (TP ≈ 92) and lower content/critical scores (e.g., language ≈ 72.13, critical ≈ 60.90) vs SciSage.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly to SciSage and LLM×MapReduce-V2 in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Underperforms SciSage on document coherence and reference accuracy (SciSage F1 0.46 vs AutoSurvey reported ~0.143); authors attribute some shortcomings to offline-only retrieval and less iterative critique.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Two-stage pipelines can produce reasonable surveys but are limited by retrieval freshness/coverage and lack of iterative, hierarchical reflection that improves final coherence and citation alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Dependence on offline corpora restricts recency and recall; weaker citation reproduction compared to multi-source reranked retrieval plus iterative critique.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not explicitly analyzed in this paper; scaling limited by offline corpus size and retrieval strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4436.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4436.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM×MapReduce-V2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM × MapReduce-V2: Entropy-driven convolutional test-time scaling for generating long-form articles from extremely long resources</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A MapReduce-inspired approach that applies entropy-driven convolutional scaling to synthesize coherent long-form drafts from extremely long resources and many document chunks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llm × mapreduce-v2: Entropy-driven convolutional test-time scaling for generating long-form articles from extremely long resources.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM×MapReduce-V2</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>MapReduce-style decomposition and aggregation: chunk extremely long inputs, generate local summaries, and progressively merge via convolutional/entropy-driven scaling to produce coherent long-form outputs. Includes an online retrieval mechanism in its standard implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Implemented with QWEN3-32B in the experiments reported in this paper (original implementations may use other LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Online retrieval to gather large numbers of document chunks; chunking and local summarization before global aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Entropy-driven convolutional scaling + MapReduce-style hierarchical aggregation of local summaries into a coherent document.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Designed for extremely long resources / very large corpora; evaluated in this paper on the SurveyScope tasks (benchmark comparisons over topics drawn from 46 curated papers).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Long-form article and survey generation over scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Long-form drafts / surveys.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same automatic metrics in this paper (content quality, structural coherence, reference accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>In this paper's evaluation, reported language fluency ≈ 86.14, critical ≈ 76.93, document coherence 78.64, and citation F1 ≈ 0.017 (TP ≈ 130), substantially lower reference alignment than SciSage.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to SciSage, AutoSurvey, OpenScholar in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Per authors' results, LLM×MapReduce-V2 attains strong local coherence but much poorer citation reproduction (F1 0.017) compared to SciSage (F1 0.46); SciSage had slightly higher document-level coherence (80.37 vs 78.64).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MapReduce-style scaling helps maintain coherence on very long inputs, but without specialized retrieval reranking and iterative critique it can struggle to reproduce the specific reference sets human authors cite.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Low citation alignment/accuracy in experiments; potential computational cost when scaling to extremely large corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Specifically designed to scale to extremely long resources via chunking and convolutional aggregation; in practice here maintained coherence but citation accuracy degraded when aggregating many sources.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4436.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4436.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenScholar</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenScholar (RAG-based scientific QA system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented generation (RAG) system for answering scientific queries by identifying and synthesizing relevant passages from large open-access corpora; cited as prior work and used in baselines (with SciSage-generated outlines integrated).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OpenScholar</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>OpenScholar</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>RAG-based LLM that identifies relevant passages from a large corpus to answer scientific queries; in this paper, authors integrated SciSage outlines and paragraph-level queries into OpenScholar and enabled both local and online retrieval modes.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Implemented with QWEN3-32B for baseline experiments in this paper (original OpenScholar implementation may differ).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Passage-level retrieval from a large open-access corpus (RAG); prioritizes relevant passages for downstream generation.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Synthesizes retrieved passages to answer queries or compose survey fragments; in this paper was used with externally supplied outlines and queries.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Operates over large open-access corpora; in this paper applied to the SurveyScope evaluation topics.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific queries and survey fragments.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>QA-style answers and assembled survey fragments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same automatic metrics used in this paper (content quality, structural coherence, reference accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Related-work claim: OpenScholar previously achieved citation accuracy comparable to human experts; in this paper's baseline run (with SciSage outlines) it produced lower citation F1 (~0.061) and lower automatic scores than SciSage.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared as a baseline (OpenScholar w/ SciSage outlines) against SciSage and other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Underperformed SciSage in the experiments reported here (lower F1 and lower content/structure metrics in the authors' reimplementation/usage).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RAG is effective for passage grounding, but performance depends strongly on retrieval/reranking quality and structured guidance (outlines/queries); integrating SciSage outlines improved OpenScholar's output but still lagged SciSage overall.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Performance variability depending on retrieval quality and the specifics of pipeline integration; discrepancy between prior claims and re-run baseline metrics suggests sensitivity to implementation and configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Intended to operate over large open-access corpora; scaling behavior depends on retrieval/index infrastructure but not deeply analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4436.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4436.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SurveyForge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SurveyForge: On the outline heuristics, memory-driven generation, and multi-dimensional evaluation for automated survey writing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based system that emphasizes outline heuristics and memory-driven generation to improve the structural quality of automated surveys and bridge the gap to human-written surveys.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Surveyforge: On the outline heuristics, memory-driven generation, and multi-dimensional evaluation for automated survey writing.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SurveyForge</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Focuses on outline design heuristics and memory mechanisms to retain and recall context across the multi-stage generation process, aiming to improve structure and continuity in generated surveys.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Memory-driven retrieval/recall to preserve context across generation steps (details not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Outline heuristics combined with memory-augmented generation to produce multi-section surveys.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Automated survey generation.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Literature surveys / structured outlines.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Multi-dimensional evaluation (authors of SurveyForge emphasize richer evaluation but specifics are not reproduced in detail here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Outline heuristics and memory modules can help reduce structural errors and improve continuity in long-form generation (as claimed by the SurveyForge authors and cited here).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Paper referenced in related work only; no experimental numbers provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4436.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4436.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InteractiveSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InteractiveSurvey: An LLM-based personalized and interactive survey paper generation system</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that treats survey generation as an interactive process, allowing users to customize and iteratively refine intermediate components such as reference categorization, outline, and content to improve output quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Interactivesurvey: An llm-based personalized and interactive survey paper generation system.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>InteractiveSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>User-in-the-loop LLM system enabling personalization and iterative refinement across outline, reference categorization, and content stages to improve engagement and output precision.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Interactive/reference categorization with user feedback; retrieval augmented by user-driven queries.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Iterative generation with human-guided refinements at multiple intermediate steps.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Automated survey generation with personalization.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Customizable literature surveys.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Interactivity and personalization can enhance output quality and user satisfaction; system cited as demonstrating improved engagement and refinement capability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Referenced as related work; no experimental details are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4436.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4436.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STORM / Co-STORM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>STORM and Co-STORM (multi-agent outline and draft generation systems)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>STORM is a multi-agent system that models pre-writing by discovering diverse perspectives and curating information into comprehensive outlines; Co-STORM extends STORM with human-in-the-loop semantic mind-mapping to improve outline coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>STORM (and Co-STORM)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>STORM / Co-STORM</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>STORM: multi-agent pipeline to generate Wikipedia-style drafts by simulating multi-perspective questioning and curating collected information into outlines. Co-STORM: adds human-in-the-loop semantic mind-mapping for improved outline coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Curates diverse perspectives and gathered material (multi-agent collection) to form outlines.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Outline construction followed by draft generation; Co-STORM uses semantic mind-maps to guide structure.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Long-form article and encyclopedic/draft generation (e.g., Wikipedia-style content).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Outlines and drafts (encyclopedic/survey-like documents).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Multi-agent perspective discovery and mind-mapping can improve outline diversity and coherence; Co-STORM demonstrates benefits from human-in-the-loop semantic structure.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Described as related work; details and numeric results are not reproduced in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4436.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4436.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deep Research tools</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Research (closed-source research-synthesis tools: OpenAI Deep Research, Gemini Deep Research)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Advanced closed-source LLM-based tools (e.g., OpenAI Deep Research, Gemini Deep Research) reported to synthesize large amounts of online information into comprehensive scientific surveys, though their internal mechanisms remain undisclosed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OpenAI. Introducing deep research.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Deep Research tools (OpenAI Deep Research, Gemini Deep Research)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Proprietary, closed-source systems built on advanced LLMs that claim to synthesize extensive online information into comprehensive scientific surveys; precise architectures and retrieval / synthesis mechanisms are not publicly documented.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Closed-source (OpenAI/Gemini family) according to related-work citations; details not disclosed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not specified (closed-source); described as synthesizing online information at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Not specified; claimed to produce comprehensive surveys by aggregating web and literature sources.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Broad web and scientific literature synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Comprehensive scientific surveys / research summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Cited as showing promising performance; no reproducible metrics provided in this paper due to closed-source nature.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Closed-source systems demonstrate high empirical performance on synthesis tasks but lack transparency; authors caution that mechanisms are unclear.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Closed-source nature prevents inspection of retrieval/reranking/synthesis methods; reproducibility and understanding of mechanisms are limited.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Claimed to operate at large scale (web-scale synthesis) but specific scaling details are unavailable publicly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4436.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4436.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach combining retrieval-augmented generation with self-reflection mechanisms that learn to retrieve, generate, and critique their own outputs to improve factuality and relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-rag: Learning to retrieve, generate, and critique through self-reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Self-RAG</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Integrates retrieval augmentation with a self-reflection loop where the model critiques its own generations to improve subsequent retrieval and generation steps.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Retrieval-augmented methods with feedback-driven retrieval improvement via self-critique.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Iterative generate-critique-regenerate cycle mediated by the model's internal reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General retrieval-augmented generation and improved factuality tasks (relevant to literature synthesis).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Improved retrieval-grounded generation outputs (summaries, answers, surveys).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Self-reflection mechanisms can help improve retrieval quality and downstream generation by enabling feedback-driven refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Referenced as related prior work; paper does not reproduce Self-RAG experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4436.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4436.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Synthesizing scientific literature (Akari Asai et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Synthesizing scientific literature with retrieval-augmented LMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent work (Akari Asai et al.) focused on using retrieval-augmented language models specifically for synthesizing scientific literature; cited as related work in the context of LLM-based literature synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Synthesizing scientific literature with retrieval-augmented lms.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Synthesizing scientific literature with retrieval-augmented LMs (Akari Asai et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Explores retrieval-augmented LLM pipelines tailored for synthesizing scientific literature, including mechanisms to ground outputs in retrieved sources and improve citation/factuality (cited here as prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Retrieval-augmented retrieval grounded in scientific corpora (paper-specific details not reproduced here).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>RAG-based synthesis with attention to citation grounding and literature synthesis best practices.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific literature synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Synthesis of scientific literature (surveys, summaries).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as closely related work demonstrating retrieval-augmented LMs can be used for literature synthesis; specifics not reproduced in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Referenced only in related work; details must be read from the original paper for full specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not described here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Autosurvey: Large language models can automatically write surveys. <em>(Rating: 2)</em></li>
                <li>Llm × mapreduce-v2: Entropy-driven convolutional test-time scaling for generating long-form articles from extremely long resources. <em>(Rating: 2)</em></li>
                <li>Surveyforge: On the outline heuristics, memory-driven generation, and multi-dimensional evaluation for automated survey writing. <em>(Rating: 2)</em></li>
                <li>Interactivesurvey: An llm-based personalized and interactive survey paper generation system. <em>(Rating: 2)</em></li>
                <li>Self-rag: Learning to retrieve, generate, and critique through self-reflection. <em>(Rating: 2)</em></li>
                <li>Synthesizing scientific literature with retrieval-augmented lms. <em>(Rating: 2)</em></li>
                <li>OpenScholar <em>(Rating: 1)</em></li>
                <li>STORM <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4436",
    "paper_id": "paper-279402998",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "SciSage",
            "name_full": "SCISAGE: A Multi-Agent Framework for High-Quality Scientific Survey Generation",
            "brief_description": "A multi-agent, LLM-driven pipeline for automated survey writing that coordinates specialized agents (Interpreter, Organizer, Collector, Composer, Refiner) plus a hierarchical Reflector that iteratively critiques drafts at outline, section and document levels to improve coherence and citation accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "SciSage",
            "system_description": "Multi-agent architecture: Interpreter (query understanding & rewrite), Organizer (multi-LLM ensemble outline generation and search-query extraction), Collector (multi-source adaptive retrieval and reranking using APIs like arXiv/PubMed/Google Scholar), Composer (bottom-up hierarchical synthesis from atomic subsection content to section and full-document assembly, including figure/table heuristics), Refiner (final polishing, citation deduplication/renumbering, format export), and Reflector (iterative hierarchical critique loop operating at outline/section/document levels using panels of LLM 'expert' personas and majority-vote to trigger re-retrieval/regeneration). The pipeline follows a generate–reflect–regenerate paradigm and produces LaTeX/Markdown outputs.",
            "llm_model_used": "Multi-LLM ensemble for outline generation (unspecified); experiments and all implemented baselines in this paper used QWEN3-32B",
            "extraction_technique": "Multi-source adaptive retrieval (APIs to arXiv/PubMed/Google Scholar), semantic relevance scoring and reranking by recency/venue/author/citation, heuristic extraction of figures/tables from LaTeX, and section-level retrieval guided by Organizer-generated search queries.",
            "synthesis_technique": "Hierarchical bottom-up synthesis: atomic subsection synthesis (citation-rich) -&gt; assemble sections -&gt; integrate chapters -&gt; full-document draft; iterative hierarchical reflection (generate–reflect–regenerate) with targeted re-retrieval and regeneration; mindmap generation for high-level structure.",
            "number_of_papers": "Evaluated on SurveyScope benchmark (46 curated papers); reference-alignment comparisons used human-cited sets totaling 3,844 references (SciSage matched 1,510 true positives).",
            "domain_or_topic": "Computer science surveys across 11 topics (NLP, LLMs, AI safety, robotics, multimodal learning, etc.).",
            "output_type": "Automated literature surveys / long-form scientific reviews (exportable to LaTeX/Markdown).",
            "evaluation_metrics": "LLM-based automatic metrics: content quality (language fluency, critical thinking), structural coherence (section-level and document-level), reference accuracy (TP, precision/recall, F1); plus human expert judgments on multiple dimensions.",
            "performance_results": "Document-level coherence 80.37; language fluency 85.65; critical thinking 77.58; citation F1 = 0.46 (1,510 true positives matched out of 3,844 human-cited references). The paper reports SciSage outperforms baselines by +1.73 points in document coherence and +32% in citation F1.",
            "comparison_baseline": "LLM×MapReduce-V2, AutoSurvey, OpenScholar (integrated with SciSage outlines for one variant).",
            "performance_vs_baseline": "Outperforms LLM×MapReduce-V2 on document coherence (80.37 vs 78.64) and substantially on citation F1 (0.46 vs 0.017 per Table 4); outperforms AutoSurvey and OpenScholar across most automatic metrics reported.",
            "key_findings": "Iterative hierarchical reflection (Reflector agent) and specialized multi-agent roles materially improve document-level coherence and citation accuracy; query understanding (Interpreter) improves outline quality; multi-source reranking increases retrieval precision; bottom-up synthesis preserves local factual grounding before global integration.",
            "limitations_challenges": "Limited analytical depth and precise mathematical expression (weak on formal math-heavy fields), English-only evaluation, dependence on the chosen foundation model (QWEN3-32B in experiments), tendency to produce long/verbosity-heavy text, and evaluation metric saturation for topical relevance and section coherence.",
            "scaling_behavior": "Designed to handle long corpora via Composer and MapReduce-like assembly; reflection loops (outline trials up to 10, section trials up to 20) improve scores; evaluated on a 46-paper benchmark—paper reports improved metrics with reflection and query-understanding but does not provide formal parametric scaling laws across model sizes.",
            "uuid": "e4436.0",
            "source_info": {
                "paper_title": "SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "AutoSurvey",
            "name_full": "AutoSurvey: Large language models can automatically write surveys",
            "brief_description": "A two-stage LLM-based survey generation pipeline that performs retrieval, outline drafting, subsection generation, and evaluation, used as a baseline in this paper.",
            "citation_title": "Autosurvey: Large language models can automatically write surveys.",
            "mention_or_use": "use",
            "system_name": "AutoSurvey",
            "system_description": "Two-stage pipeline: (1) retrieval-augmented retrieval from an offline corpus and outline drafting, and (2) parallel subsection generation and evaluation to produce a survey. Emphasizes logical parallel generation to improve content quality and citation handling.",
            "llm_model_used": "Implemented with QWEN3-32B for the baselines/experiments reported in this paper (original system details not fully described here).",
            "extraction_technique": "Retrieval-augmented generation using an offline corpus; standard IR to fetch candidate papers/snippets for summarization.",
            "synthesis_technique": "Outline-first then subsection generation (two-stage aggregation), using parallel logical generation for subsections.",
            "number_of_papers": "Applied to the same evaluation topics in this paper (SurveyScope benchmark—tasks drawn from the 46-paper set); uses an offline corpus for retrieval rather than online multi-source retrieval.",
            "domain_or_topic": "Automated survey generation in computer science domains.",
            "output_type": "Automated literature surveys / review articles.",
            "evaluation_metrics": "Same automatic metrics used in this paper (language fluency, critical thinking, topical relevance, section/document structural coherence, reference accuracy measured by TP/F1) and human evaluation.",
            "performance_results": "Reported lower performance than SciSage in this paper; Table 4 (as used by the authors) lists AutoSurvey citation F1 ≈ 0.143 (TP ≈ 92) and lower content/critical scores (e.g., language ≈ 72.13, critical ≈ 60.90) vs SciSage.",
            "comparison_baseline": "Compared directly to SciSage and LLM×MapReduce-V2 in experiments.",
            "performance_vs_baseline": "Underperforms SciSage on document coherence and reference accuracy (SciSage F1 0.46 vs AutoSurvey reported ~0.143); authors attribute some shortcomings to offline-only retrieval and less iterative critique.",
            "key_findings": "Two-stage pipelines can produce reasonable surveys but are limited by retrieval freshness/coverage and lack of iterative, hierarchical reflection that improves final coherence and citation alignment.",
            "limitations_challenges": "Dependence on offline corpora restricts recency and recall; weaker citation reproduction compared to multi-source reranked retrieval plus iterative critique.",
            "scaling_behavior": "Not explicitly analyzed in this paper; scaling limited by offline corpus size and retrieval strategy.",
            "uuid": "e4436.1",
            "source_info": {
                "paper_title": "SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "LLM×MapReduce-V2",
            "name_full": "LLM × MapReduce-V2: Entropy-driven convolutional test-time scaling for generating long-form articles from extremely long resources",
            "brief_description": "A MapReduce-inspired approach that applies entropy-driven convolutional scaling to synthesize coherent long-form drafts from extremely long resources and many document chunks.",
            "citation_title": "Llm × mapreduce-v2: Entropy-driven convolutional test-time scaling for generating long-form articles from extremely long resources.",
            "mention_or_use": "use",
            "system_name": "LLM×MapReduce-V2",
            "system_description": "MapReduce-style decomposition and aggregation: chunk extremely long inputs, generate local summaries, and progressively merge via convolutional/entropy-driven scaling to produce coherent long-form outputs. Includes an online retrieval mechanism in its standard implementation.",
            "llm_model_used": "Implemented with QWEN3-32B in the experiments reported in this paper (original implementations may use other LLMs).",
            "extraction_technique": "Online retrieval to gather large numbers of document chunks; chunking and local summarization before global aggregation.",
            "synthesis_technique": "Entropy-driven convolutional scaling + MapReduce-style hierarchical aggregation of local summaries into a coherent document.",
            "number_of_papers": "Designed for extremely long resources / very large corpora; evaluated in this paper on the SurveyScope tasks (benchmark comparisons over topics drawn from 46 curated papers).",
            "domain_or_topic": "Long-form article and survey generation over scientific literature.",
            "output_type": "Long-form drafts / surveys.",
            "evaluation_metrics": "Same automatic metrics in this paper (content quality, structural coherence, reference accuracy).",
            "performance_results": "In this paper's evaluation, reported language fluency ≈ 86.14, critical ≈ 76.93, document coherence 78.64, and citation F1 ≈ 0.017 (TP ≈ 130), substantially lower reference alignment than SciSage.",
            "comparison_baseline": "Compared to SciSage, AutoSurvey, OpenScholar in this paper.",
            "performance_vs_baseline": "Per authors' results, LLM×MapReduce-V2 attains strong local coherence but much poorer citation reproduction (F1 0.017) compared to SciSage (F1 0.46); SciSage had slightly higher document-level coherence (80.37 vs 78.64).",
            "key_findings": "MapReduce-style scaling helps maintain coherence on very long inputs, but without specialized retrieval reranking and iterative critique it can struggle to reproduce the specific reference sets human authors cite.",
            "limitations_challenges": "Low citation alignment/accuracy in experiments; potential computational cost when scaling to extremely large corpora.",
            "scaling_behavior": "Specifically designed to scale to extremely long resources via chunking and convolutional aggregation; in practice here maintained coherence but citation accuracy degraded when aggregating many sources.",
            "uuid": "e4436.2",
            "source_info": {
                "paper_title": "SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "OpenScholar",
            "name_full": "OpenScholar (RAG-based scientific QA system)",
            "brief_description": "A retrieval-augmented generation (RAG) system for answering scientific queries by identifying and synthesizing relevant passages from large open-access corpora; cited as prior work and used in baselines (with SciSage-generated outlines integrated).",
            "citation_title": "OpenScholar",
            "mention_or_use": "use",
            "system_name": "OpenScholar",
            "system_description": "RAG-based LLM that identifies relevant passages from a large corpus to answer scientific queries; in this paper, authors integrated SciSage outlines and paragraph-level queries into OpenScholar and enabled both local and online retrieval modes.",
            "llm_model_used": "Implemented with QWEN3-32B for baseline experiments in this paper (original OpenScholar implementation may differ).",
            "extraction_technique": "Passage-level retrieval from a large open-access corpus (RAG); prioritizes relevant passages for downstream generation.",
            "synthesis_technique": "Synthesizes retrieved passages to answer queries or compose survey fragments; in this paper was used with externally supplied outlines and queries.",
            "number_of_papers": "Operates over large open-access corpora; in this paper applied to the SurveyScope evaluation topics.",
            "domain_or_topic": "General scientific queries and survey fragments.",
            "output_type": "QA-style answers and assembled survey fragments.",
            "evaluation_metrics": "Same automatic metrics used in this paper (content quality, structural coherence, reference accuracy).",
            "performance_results": "Related-work claim: OpenScholar previously achieved citation accuracy comparable to human experts; in this paper's baseline run (with SciSage outlines) it produced lower citation F1 (~0.061) and lower automatic scores than SciSage.",
            "comparison_baseline": "Compared as a baseline (OpenScholar w/ SciSage outlines) against SciSage and other baselines.",
            "performance_vs_baseline": "Underperformed SciSage in the experiments reported here (lower F1 and lower content/structure metrics in the authors' reimplementation/usage).",
            "key_findings": "RAG is effective for passage grounding, but performance depends strongly on retrieval/reranking quality and structured guidance (outlines/queries); integrating SciSage outlines improved OpenScholar's output but still lagged SciSage overall.",
            "limitations_challenges": "Performance variability depending on retrieval quality and the specifics of pipeline integration; discrepancy between prior claims and re-run baseline metrics suggests sensitivity to implementation and configuration.",
            "scaling_behavior": "Intended to operate over large open-access corpora; scaling behavior depends on retrieval/index infrastructure but not deeply analyzed in this paper.",
            "uuid": "e4436.3",
            "source_info": {
                "paper_title": "SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "SurveyForge",
            "name_full": "SurveyForge: On the outline heuristics, memory-driven generation, and multi-dimensional evaluation for automated survey writing",
            "brief_description": "An LLM-based system that emphasizes outline heuristics and memory-driven generation to improve the structural quality of automated surveys and bridge the gap to human-written surveys.",
            "citation_title": "Surveyforge: On the outline heuristics, memory-driven generation, and multi-dimensional evaluation for automated survey writing.",
            "mention_or_use": "mention",
            "system_name": "SurveyForge",
            "system_description": "Focuses on outline design heuristics and memory mechanisms to retain and recall context across the multi-stage generation process, aiming to improve structure and continuity in generated surveys.",
            "llm_model_used": null,
            "extraction_technique": "Memory-driven retrieval/recall to preserve context across generation steps (details not provided in this paper).",
            "synthesis_technique": "Outline heuristics combined with memory-augmented generation to produce multi-section surveys.",
            "number_of_papers": null,
            "domain_or_topic": "Automated survey generation.",
            "output_type": "Literature surveys / structured outlines.",
            "evaluation_metrics": "Multi-dimensional evaluation (authors of SurveyForge emphasize richer evaluation but specifics are not reproduced in detail here).",
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Outline heuristics and memory modules can help reduce structural errors and improve continuity in long-form generation (as claimed by the SurveyForge authors and cited here).",
            "limitations_challenges": "Paper referenced in related work only; no experimental numbers provided in this paper.",
            "scaling_behavior": "Not described in this paper.",
            "uuid": "e4436.4",
            "source_info": {
                "paper_title": "SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "InteractiveSurvey",
            "name_full": "InteractiveSurvey: An LLM-based personalized and interactive survey paper generation system",
            "brief_description": "A system that treats survey generation as an interactive process, allowing users to customize and iteratively refine intermediate components such as reference categorization, outline, and content to improve output quality.",
            "citation_title": "Interactivesurvey: An llm-based personalized and interactive survey paper generation system.",
            "mention_or_use": "mention",
            "system_name": "InteractiveSurvey",
            "system_description": "User-in-the-loop LLM system enabling personalization and iterative refinement across outline, reference categorization, and content stages to improve engagement and output precision.",
            "llm_model_used": null,
            "extraction_technique": "Interactive/reference categorization with user feedback; retrieval augmented by user-driven queries.",
            "synthesis_technique": "Iterative generation with human-guided refinements at multiple intermediate steps.",
            "number_of_papers": null,
            "domain_or_topic": "Automated survey generation with personalization.",
            "output_type": "Customizable literature surveys.",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Interactivity and personalization can enhance output quality and user satisfaction; system cited as demonstrating improved engagement and refinement capability.",
            "limitations_challenges": "Referenced as related work; no experimental details are provided in this paper.",
            "scaling_behavior": "Not discussed here.",
            "uuid": "e4436.5",
            "source_info": {
                "paper_title": "SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "STORM / Co-STORM",
            "name_full": "STORM and Co-STORM (multi-agent outline and draft generation systems)",
            "brief_description": "STORM is a multi-agent system that models pre-writing by discovering diverse perspectives and curating information into comprehensive outlines; Co-STORM extends STORM with human-in-the-loop semantic mind-mapping to improve outline coherence.",
            "citation_title": "STORM (and Co-STORM)",
            "mention_or_use": "mention",
            "system_name": "STORM / Co-STORM",
            "system_description": "STORM: multi-agent pipeline to generate Wikipedia-style drafts by simulating multi-perspective questioning and curating collected information into outlines. Co-STORM: adds human-in-the-loop semantic mind-mapping for improved outline coherence.",
            "llm_model_used": null,
            "extraction_technique": "Curates diverse perspectives and gathered material (multi-agent collection) to form outlines.",
            "synthesis_technique": "Outline construction followed by draft generation; Co-STORM uses semantic mind-maps to guide structure.",
            "number_of_papers": null,
            "domain_or_topic": "Long-form article and encyclopedic/draft generation (e.g., Wikipedia-style content).",
            "output_type": "Outlines and drafts (encyclopedic/survey-like documents).",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Multi-agent perspective discovery and mind-mapping can improve outline diversity and coherence; Co-STORM demonstrates benefits from human-in-the-loop semantic structure.",
            "limitations_challenges": "Described as related work; details and numeric results are not reproduced in this paper.",
            "scaling_behavior": "Not specified here.",
            "uuid": "e4436.6",
            "source_info": {
                "paper_title": "SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Deep Research tools",
            "name_full": "Deep Research (closed-source research-synthesis tools: OpenAI Deep Research, Gemini Deep Research)",
            "brief_description": "Advanced closed-source LLM-based tools (e.g., OpenAI Deep Research, Gemini Deep Research) reported to synthesize large amounts of online information into comprehensive scientific surveys, though their internal mechanisms remain undisclosed.",
            "citation_title": "OpenAI. Introducing deep research.",
            "mention_or_use": "mention",
            "system_name": "Deep Research tools (OpenAI Deep Research, Gemini Deep Research)",
            "system_description": "Proprietary, closed-source systems built on advanced LLMs that claim to synthesize extensive online information into comprehensive scientific surveys; precise architectures and retrieval / synthesis mechanisms are not publicly documented.",
            "llm_model_used": "Closed-source (OpenAI/Gemini family) according to related-work citations; details not disclosed in this paper.",
            "extraction_technique": "Not specified (closed-source); described as synthesizing online information at scale.",
            "synthesis_technique": "Not specified; claimed to produce comprehensive surveys by aggregating web and literature sources.",
            "number_of_papers": null,
            "domain_or_topic": "Broad web and scientific literature synthesis.",
            "output_type": "Comprehensive scientific surveys / research summaries.",
            "evaluation_metrics": null,
            "performance_results": "Cited as showing promising performance; no reproducible metrics provided in this paper due to closed-source nature.",
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Closed-source systems demonstrate high empirical performance on synthesis tasks but lack transparency; authors caution that mechanisms are unclear.",
            "limitations_challenges": "Closed-source nature prevents inspection of retrieval/reranking/synthesis methods; reproducibility and understanding of mechanisms are limited.",
            "scaling_behavior": "Claimed to operate at large scale (web-scale synthesis) but specific scaling details are unavailable publicly.",
            "uuid": "e4436.7",
            "source_info": {
                "paper_title": "SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Self-RAG",
            "name_full": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
            "brief_description": "An approach combining retrieval-augmented generation with self-reflection mechanisms that learn to retrieve, generate, and critique their own outputs to improve factuality and relevance.",
            "citation_title": "Self-rag: Learning to retrieve, generate, and critique through self-reflection.",
            "mention_or_use": "mention",
            "system_name": "Self-RAG",
            "system_description": "Integrates retrieval augmentation with a self-reflection loop where the model critiques its own generations to improve subsequent retrieval and generation steps.",
            "llm_model_used": null,
            "extraction_technique": "Retrieval-augmented methods with feedback-driven retrieval improvement via self-critique.",
            "synthesis_technique": "Iterative generate-critique-regenerate cycle mediated by the model's internal reflection.",
            "number_of_papers": null,
            "domain_or_topic": "General retrieval-augmented generation and improved factuality tasks (relevant to literature synthesis).",
            "output_type": "Improved retrieval-grounded generation outputs (summaries, answers, surveys).",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Self-reflection mechanisms can help improve retrieval quality and downstream generation by enabling feedback-driven refinement.",
            "limitations_challenges": "Referenced as related prior work; paper does not reproduce Self-RAG experiments.",
            "scaling_behavior": "Not discussed in this paper.",
            "uuid": "e4436.8",
            "source_info": {
                "paper_title": "SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Synthesizing scientific literature (Akari Asai et al.)",
            "name_full": "Synthesizing scientific literature with retrieval-augmented LMs",
            "brief_description": "A recent work (Akari Asai et al.) focused on using retrieval-augmented language models specifically for synthesizing scientific literature; cited as related work in the context of LLM-based literature synthesis.",
            "citation_title": "Synthesizing scientific literature with retrieval-augmented lms.",
            "mention_or_use": "mention",
            "system_name": "Synthesizing scientific literature with retrieval-augmented LMs (Akari Asai et al.)",
            "system_description": "Explores retrieval-augmented LLM pipelines tailored for synthesizing scientific literature, including mechanisms to ground outputs in retrieved sources and improve citation/factuality (cited here as prior work).",
            "llm_model_used": null,
            "extraction_technique": "Retrieval-augmented retrieval grounded in scientific corpora (paper-specific details not reproduced here).",
            "synthesis_technique": "RAG-based synthesis with attention to citation grounding and literature synthesis best practices.",
            "number_of_papers": null,
            "domain_or_topic": "Scientific literature synthesis.",
            "output_type": "Synthesis of scientific literature (surveys, summaries).",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Cited as closely related work demonstrating retrieval-augmented LMs can be used for literature synthesis; specifics not reproduced in this paper.",
            "limitations_challenges": "Referenced only in related work; details must be read from the original paper for full specifics.",
            "scaling_behavior": "Not described here.",
            "uuid": "e4436.9",
            "source_info": {
                "paper_title": "SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Autosurvey: Large language models can automatically write surveys.",
            "rating": 2,
            "sanitized_title": "autosurvey_large_language_models_can_automatically_write_surveys"
        },
        {
            "paper_title": "Llm × mapreduce-v2: Entropy-driven convolutional test-time scaling for generating long-form articles from extremely long resources.",
            "rating": 2,
            "sanitized_title": "llm_mapreducev2_entropydriven_convolutional_testtime_scaling_for_generating_longform_articles_from_extremely_long_resources"
        },
        {
            "paper_title": "Surveyforge: On the outline heuristics, memory-driven generation, and multi-dimensional evaluation for automated survey writing.",
            "rating": 2,
            "sanitized_title": "surveyforge_on_the_outline_heuristics_memorydriven_generation_and_multidimensional_evaluation_for_automated_survey_writing"
        },
        {
            "paper_title": "Interactivesurvey: An llm-based personalized and interactive survey paper generation system.",
            "rating": 2,
            "sanitized_title": "interactivesurvey_an_llmbased_personalized_and_interactive_survey_paper_generation_system"
        },
        {
            "paper_title": "Self-rag: Learning to retrieve, generate, and critique through self-reflection.",
            "rating": 2,
            "sanitized_title": "selfrag_learning_to_retrieve_generate_and_critique_through_selfreflection"
        },
        {
            "paper_title": "Synthesizing scientific literature with retrieval-augmented lms.",
            "rating": 2,
            "sanitized_title": "synthesizing_scientific_literature_with_retrievalaugmented_lms"
        },
        {
            "paper_title": "OpenScholar",
            "rating": 1,
            "sanitized_title": "openscholar"
        },
        {
            "paper_title": "STORM",
            "rating": 1
        }
    ],
    "cost": 0.02394925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SCISAGE: A MULTI-AGENT FRAMEWORK FOR HIGH-QUALITY SCIENTIFIC SURVEY GENERATION
21 Jul 2025</p>
<p>Xiaofeng Shi xfshi@baai.ac.cn 
Beijing Academy of Artificial Intelligence (BAAI)</p>
<p>Qian Kou 
Beijing Academy of Artificial Intelligence (BAAI)</p>
<p>Yuduo Li 
Beijing Academy of Artificial Intelligence (BAAI)</p>
<p>Beijing Jiaotong University (BJTU</p>
<p>Ning Tang 
Beijing Academy of Artificial Intelligence (BAAI)</p>
<p>Fudan University (FDU)</p>
<p>Jinxin Xie 
Beijing Academy of Artificial Intelligence (BAAI)</p>
<p>Longbin Yu 
Beijing Academy of Artificial Intelligence (BAAI)</p>
<p>Songjing Wang 
Beijing Academy of Artificial Intelligence (BAAI)</p>
<p>Hua Zhou 
Beijing Academy of Artificial Intelligence (BAAI)</p>
<p>SCISAGE: A MULTI-AGENT FRAMEWORK FOR HIGH-QUALITY SCIENTIFIC SURVEY GENERATION
21 Jul 2025FAD81FB2F18C5E89222A4789F730F59CarXiv:2506.12689v2[cs.AI]
The rapid growth of scientific literature demands robust tools for automated survey-generation.However, current large language model (LLM)-based methods often lack in-depth analysis, structural coherence, and reliable citations.To address these limitations, we introduce SciSage, a multi-agent framework employing a reflect-when-you-write paradigm.SciSage features a hierarchical Reflector agent that critically evaluates drafts at outline, section, and document levels, collaborating with specialized agents for query interpretation, content retrieval, and refinement.We also release SurveyScope, a rigorously curated benchmark of 46 high-impact papers (2020-2025) across 11 computer science domains, with strict recency and citation-based quality controls.Evaluations demonstrate that SciSage outperforms state-of-the-art baselines (LLM×MapReduce-V2, AutoSurvey), achieving +1.73 points in document coherence and +32% in citation F1 scores.Human evaluations reveal mixed outcomes (3 wins vs. 7 losses against human-written surveys), but highlight SciSage's strengths in topical breadth and retrieval efficiency.Overall, SciSage offers a promising foundation for research-assistive writing tools.Githubgithub.com/FlagOpen/SciSageBenchmark BAAI/SurveyScope</p>
<p>Introduction</p>
<p>The rapid growth of scientific literature, particularly in fast-evolving domains like artificial intelligence, poses increasing challenges for researchers to stay up-to-date [1,2].As literature accumulation outpaces human synthesis capacity, concerns emerge around research quality, redundancy, and accessibility.Survey articles help address this burden by systematically synthesizing existing work, highlighting key trends, and identifying open problems.High-quality surveys provide structured overviews, critically evaluate methodologies, and guide future research [3,4,5].However, their creation remains labor-intensive, demanding deep domain expertise, thematic abstraction, and rigorous citation management.As the scale and speed of academic papers continue to grow, scalable and robust survey generation methods have become increasingly essential.</p>
<p>With the development of large language models (LLMs) [6,7,8,9], researchers are employing them to automate scientific research survey writing.Most prior systems for automating literature surveys adhere to a two-stage pipeline-outline generation followed by content synthesis.AutoSurvey [2] employs a streamlined pipeline of retrieval, outline drafting, subsection generation, and evaluation to produce human-level surveys.STORM [10] leverages multi-agent to generate Wikipedia-style drafts, while Co-STORM [11] adds human-in-the-loop semantic mind-mapping to improve outline coherence.For large-scale survey tasks, LLM×MapReduce-V2 [12] uses convolutional scaling to synthesize coherent drafts from vast corpora.</p>
<p>Related Works</p>
<p>Scientific survey generation.The automation of scientific survey generation using Large Language Models (LLMs) has garnered significant attention in recent years.Early approaches primarily relied on retrieval-augmented generation (RAG) techniques to synthesize literature.For instance, OpenScholar [16] introduced a specialized RAG-based LLM capable of answering scientific queries by identifying relevant passages from a vast corpus of open-access papers, achieving citation accuracy on par with human experts.Despite these advancements, challenges persist in ensuring the structural coherence and depth of generated surveys.AutoSurvey [2] proposed a two-stage LLMbased method for survey generation, focusing on logical parallel generation to enhance content quality and citation accuracy.Similarly, SurveyForge [17] addressed some of these issues by emphasizing outline heuristics and memorydriven generation, aiming to bridge the quality gap between LLM-generated surveys and those written by humans.InteractiveSurvey [18] took a different approach by introducing a personalized and interactive survey paper generation system.This system allows users to customize and refine intermediate components continuously during generation, including reference categorization, outline, and survey content, thereby enhancing user engagement and output quality.In the realm of long-form article generation, STORM [10] presented a writing system that models the pre-writing stage by discovering diverse perspectives, simulating multi-perspective questioning, and curating collected information to create comprehensive outlines, while Co-STORM [11] extends this with human-in-the-loop and semantic mind-map techniques to enhance outline coherence.To handle ultra-long document synthesis, LLM×MapReduce-V2 [12] applies entropy-driven convolutional scaling to assemble coherent survey drafts from extensive corpora.In addition, Deep Research tools based on advanced closed-source LLMs [19,20] show promise performance in synthesizing large amounts of online information into comprehensive scitific surveys, whose mechanisms are still unclear.Despite the impressive performance of Deep Research tools, due to closed-source nature, their search mechanisms are still unknown.These systems demonstrate LLMs' potential in automating end-to-end survey generation, yet persistent challenges remain in guaranteeing content quality, structural comprehensive, and establishing rigorous evaluation standards.</p>
<p>Generative Agents [26] introduced a framework where 25 AI agents, each with unique identities and memories, autonomously coordinated social events and daily activities in a virtual town, demonstrating believable human-like behavior.MetaGPT [23] utilizes human-like standard operating procedures and specialized roles-product manager, architect, coder, tester-to reduce hallucinations in software generation.AutoGen [24] offers a flexible framework allowing configurable conversation patterns among LLM agents, enabling tool invocation, human-in-the-loop interventions, and multi-agent debate strategies to boost reasoning and factuality.AgentVerse [27] emphasizes group formation and emergent social behaviors, demonstrating performance gains from collaborative diversity.ChatDev [28] and its companion systems implement entire virtual development teams, validating structured role allocation on real-world code bases.DyLAN [29] introduced a dynamic LLM-agent network leveraging inference-time agent selection via an unsupervised Importance Score, flexible communication structures, and early stopping.Debate oriented frameworks [30,31] formalize structured argumentation among solver agents, mediated by aggregators, and show clear improvements on arithmetic and reasoning benchmarks.AgentNet [32] introduced a decentralized, retrieval-augmented, evolutionary coordination model over a dynamically evolving DAG network, eliminating central orchestrators and enabling scalable, privacy-aware specialization.We share the idea by establishing an LLM-based multi-agent system to facilitate academic research.</p>
<p>Research</p>
<p>Method</p>
<p>In this section, we introduce SciSage, a LLM-based Multi-Agent framework designed for automated scientific survey generation.Inspired by the cognitive and iterative behaviors of expert authors, SciSage leverages a coordinated architecture of specialized agents that unfolds through three interconnected components-Query Understanding and Rewrite, Retrieval and Content Generation, and Iterative Hierarchical Reflection.Each component comprises distinct agents that cooperate to produce high-quality scientific surveys.</p>
<p>As shown in Figure 1, SciSage operates as a dynamic, iterative workflow.The system begins with user queries and proceeds through modular stages where intermediate results are critically reviewed and enhanced.Central to this process is the Reflector agent, which simulates expert-like revision cycles to ensure coherence, depth, and informativeness across all sections of the survey.The following subsections provide an in-depth analysis of each module's architecture and operational logic.The overall pseudo-code of SciSage is summarized in Algorithm 1.</p>
<p>Algorithm 1 SCISAGE: A Multi-Agent Survey Generation System</p>
<p>Require: User query Q, research sources D, reflection trials N , outline templates T Ensure: Final refined survey document F f inal 1: Rewrite the query and get intent information
Q R , I ← INTERPRETER(Q) 2: Select a suitable outline template t ← ORGANIZER(Q R , I, T ) 3: Generate outline O ← ORGANIZER(Q R , I, t) 4: repeat 5:
Receive feedback from Reflector ∆O ← REFLECTOR(O, Q R , I)
6: if ∆O ̸ = ∅ then 7:
Refine and update O ← ORGANIZER(O, ∆O, t) Retrieve relevant papers from multiple sources P i ← COLLECTOR(S i , D)</p>
<p>Query Understanding and Rewrite</p>
<p>The efficacy of the entire review generation process is contingent upon a precise and comprehensive understanding of the user's request.The Interpreter Agent serves as the entry point of the SciSage framework.Its objective is to transform original, often ambiguous user queries into well-structured, standardized, and actionable instructions for downstream agents.First, to accommodate multilingual user queries, the Interpreter performs automatic language detection for query Q and translates it into English Q E .This standardization ensures consistency in downstream processing and leverages broader retrieval sources.Next, the Interpreter engages in a deep semantic analysis of the translated query to discern the user's core intent, scientific domain of interest, and research topic, which can be represented as intent information I.For example, given the query "The latest progress in code generation using LLM", the Interpreter infers that the user seeks recent advances in deep learning of LLMs for code generation.Finally, to maximize the precision and recall of the information retrieval phase, the initial query often requires refinement.The Interpreter evaluates whether the input query needs to be rewritten.Once ambiguity, vagueness, or informal phrasing is detected, the Interpreter generates a refined version Q R = Interpreter(Q E , I) that is semantically equivalent but structurally optimized for retrieval and generation purposes.Prompts for query understading and rewriting are shown in Appendix A.1.</p>
<p>Retrieval and Content Generation</p>
<p>The central engine of the SciSage framework executes a "bottom-up" workflow for content creation, moving from high-level planning to detailed writing and final polishing.This entire process is orchestrated across four specialized agents-Organizer, Collector, Composer, and Refiner-working in unison to produce a coherent and comprehensive survey.</p>
<p>Outline Construction</p>
<p>The Organizer Agent constructs a comprehensive, logical, and scholarly outline that reflects the user's intent, guiding high-quality content generation.It begins by selecting a suitable outline structure from a curated template library T based on the user's intent(e.g., survey, theory) detected by the Interpreter.To move beyond this initial template and foster a more innovative and robust structure, the Organizer then employs a multi-model ensemble strategy.Multiple LLMs generate varied outline candidates in parallel to promote diversity and reduce bias.These candidate outlines are synthesized into a unified structure using content-aware heuristics and the outline is represented as
O = ORGANIZER(Q R , I, t) = Merge(O LLM1 , • • • , O LLM N ).
Finally, for each section and subsection in the outline, the Organizer extracts key points and generates precise search queries {Q i S } to guide the following retrieval process, while deliberately excluding non-content sections such as conclusion and references.The ultimate output of this stage is a tree structure where each node contains a section title, its hierarchy, key points, and the corresponding search queries, which is then passed to the Collector to initiate the research phase.</p>
<p>Multi-Source Retrieval and Re-Ranking The Collector Agent serves as the research assistant and gathers high-quality references from various academic sources.Integrated with APIs from multiple scholarly sources D (e.g., arXiv, PubMed, Google Scholar), the Collector employs a multi-source adaptive retrieval strategy.Guided by the domain context provided by the Interpreter, it prioritizes sources most likely to yield relevant results, thereby improving both the efficiency and precision of the retrieval process.Once the relevant sources are identified, the Collector retrieves candidate papers and scores them, evaluating their semantic relevance and topical depth.To further ensure the credibility and currency of selected papers, the Collector reranks the retrieval results based on publication recency, venue prestige, author influence, and citation metrics, ultimately prioritizing the most authoritative and timely literature for subsequent content generation.The retrieval process for each section S i can be represented as
P i = COLLECTOR(S i , D)
, where P i is the final reranked most relevant paper list.</p>
<p>Hierarchical Content Generation</p>
<p>The Composer Agent is the central synthesis engine in the SciSage framework, tasked with transforming the Organizer's structured outline and the Collector's curated papers into a coherent and comprehensive scientific survey.It adopts a bottom-up methodology that emphasizes local coherence and factual grounding before scaling up to larger textual structures.The Composer begins with atomic content generation, producing focused, citation-rich content S i for each outlined subsection s i by synthesizing titles, abstracts, and full texts from corresponding retrieved papers P i .These atomic units are then assembled into coherent sections S i = COMPOSER(s i , P i ), each featuring an introductory overview, core discussions and a conclusion.During this process, the Composer also performs key figures and tables extraction and integrates them into section contents, scanning documents (e.g., LaTeX files from arXiv) to heuristically identify and extract visual content that best supports the topic, particularly in method or result sections.Once all sections are generated, the Composer organizes the content at both section and document levels, integrating the components into chapters and compiling them into a full-document draft F .This also includes crafting the Introduction and Conclusion/Future Work sections to ensure thematic and logical coherence.To further enhance readability, the Composer generates visual aids such as mindmaps derived from the outline and integrates them with the document to provide a high-level overview of the paper's structure and intellectual architecture.Mindmap example can be found in Appendix E.</p>
<p>Final Refinement</p>
<p>The Refiner is the final agent in the content generation process, responsible for transforming the draft into a polished document which is ready for publication.Following the Composer's draft generation, a thorough finalization process is conducted by the Refiner for both content and presentation to get the final refined survey F f inal = REFINER(F ).It improves the internal flow of paragraphs, eliminates redundancy, enforces consistent terminology, and ensures logical transitions throughout the manuscript.It starts with the content and citation, where the Refiner progressively aligns the document with the final outline based on the section titles and their corresponding content, removes the duplicated references and renumbers the citations.Next, the writing format and style are checked and standardized to meet the academic requirements, while ensuring the clarity of the topic.Lastly, as for the output, the Refiner exports the document in formats such as LaTeX and Markdown to support most publishing systems.</p>
<p>Iterative Hierarchical Reflection</p>
<p>The Reflector Agent is a critical innovation of SciSage's system, functioning as a pervasive, iterative mechanism embedded deeply within the workflows of both the Organizer and Composer.Rather than being a standalone step, it operates through a continuous "generate-reflect-regenerate" loop that drives recursive, multi-level content refinement, mirroring the self-corrective nature of expert academic writing.Its hierarchical scope of reflection spans the entire generation process.At the outline level, the Reflector evaluates outline O in completeness, logical structure, topical relevance, and alignment with academic standards, returning feedback ∆O to the Organizer for iterative refinement until a quality threshold is reached.At the section level, as the Composer produces section content S i , the Reflector gives critique ∆S i in accuracy, evidential support, structural clarity, and the balance of perspectives.If deficiencies are detected, it may trigger new literature retrieval by the Collector, followed by targeted content regeneration.At the full-text level, the Reflector deploys a panel of LLM agents simulating expert personas, such as journal editors, senior professors, and peer reviewers, to evaluate the manuscript from diverse critical views.A majority vote system identifies suboptimal sections, prompting the creation of a structured revision plan, including new key points and queries, which reactivates the Collector and Composer in a recursive improvement cycle.The Reflector also ensures that chapter introductions communicate each chapter's intent and structure.Through this dynamic process, SciSage transforms initial drafts into rigorously refined academic surveys that have withstood multiple rounds of critique and enhancement.</p>
<p>Benchmark</p>
<p>To comprehensively evaluate the quality of generated survey content, we introduce SurveyScope, a high-quality benchmark specifically designed for academic survey writing.SurveyScope significantly improves upon existing evaluation benchmarks like SURVEYEVAL_TEST [12] and AUTOSURVEY [2] by enhancing both the diversity of research topics and the quality of papers.</p>
<p>Paper quality in SurveyScope is defined by two key criteria: publication recency and citation count.</p>
<p>Given the fast-moving nature of computer science research-especially in areas such as large language models (LLMs) and AI safety-recent papers are more likely to reflect current trends, methods, and state-of-the-art advances.To ensure timeliness, all papers in SurveyScope were published between 2020 and 2025.The majority are concentrated in the 2023-2024 period, coinciding with the surge in large language model research following the advent of ChatGPT [33].</p>
<p>Citation count serves as a proxy for academic influence and recognition.A high citation count generally indicates an influential, well-received, and widely adopted paper.Papers in SurveyScope exhibit significantly higher citation metrics than those in other benchmarks, with a maximum of 2,184 and an average of approximately 322 citations.</p>
<p>These stringent metrics ensure the benchmark's high reliability and representativeness, grounding evaluation results in authoritative and influential literature.</p>
<p>Construction Methodology</p>
<p>The construction pipeline of SurveyScope is illustrated in Figure 2, and it consists the following key steps:</p>
<p>SurveyScope Completion Extraction Benchmarks</p>
<p>Seed Topics As Table 1 shows, the dataset comprises 20 surveys from SURVEYEVAL_TEST, 8 from AUTOSURVEY, and 18 manually curated surveys collected from Google Scholar and other academic platforms.This diverse sourcing strategy ensures a balanced benchmark that reflects both standardized evaluations and high-quality, real-world survey writing.</p>
<p>… …</p>
<p>Paper Selection Citation
Pub Date Content
SurveyEval_Test AutoSurvey Expand Manually Curated Table 1: Source distribution of SurveyScope Following this pipeline, we constructed a curated benchmark of 46 high-quality research papers.Each paper was manually selected by professionals with graduate-level training in computer science, based on criteria including publication recency and citation impact.</p>
<p>Characteristics</p>
<p>Thanks to a carefully designed construction pipeline, SurveyScope exhibits several key characteristics that distinguish it from existing benchmarks:</p>
<p>Broad Topic Coverage SurveyScope covers a broad range of active research areas in computer science, including natural language processing (NLP), large language models (LLMs), AI safety, robotics, and multimodal learning.This topical diversity enables systematic and cross-domain evaluation of automatic survey generation systems.Figure 3 provides an overview of the topic distribution, with 46 papers spanning 11 distinct topics.A detailed comparison of topic categories across benchmarks is provided in Appendix B.1.showing that over 52% of the papers have received more than 100 citations.
0
Summary SurveyScope stands out from existing benchmarks through its broad topical coverage, inclusion of recent high-impact publications, and emphasis on citation-based influence.These characteristics make it a comprehensive and reliable resource for evaluating academic survey generation systems.A comparative analysis across benchmarks is presented in Figure 6, where SurveyScope consistently leads across all dimensions.Comparison details can be found in Appendix B.4.</p>
<p>Category Count
Topic</p>
<p>Evaluation</p>
<p>To comprehensively evaluate the quality of generated content compared to human-written counterparts, we adopt a two-fold evaluation protocol: (1) automatic evaluation leveraging large language models (LLMs), and (2) human evaluation by domain experts.</p>
<p>Automatic Evaluation with LLM-based Metrics</p>
<p>We established an automated evaluation framework, drawing inspiration from AUTOSURVEY [2] and LLM × MAPREDUCE-V2 [12].Our evaluation assesses content across three core dimensions: Content Quality, Document Structure, and Reference Accuracy.All scores are normalized to a 0-100 scale, with higher scores indicating better performance.</p>
<p>Content Quality Assessment</p>
<p>We evaluated textual quality across the following dimensions:</p>
<p>Language Fluency and Style This metric assesses the linguistic quality of generated content, emphasizing academic formality, clarity, and fluency.Referring to the evaluation method provided by LLM × MAPREDUCE-V2 [12], we observed that directly using their original 100-point prompt template often resulted in limited score variance, with most outputs receiving uniformly high scores and thus exhibiting low discriminative capacity.To address this, we employed a 10-point scoring rubric to encourage more granular distinctions, then linearly rescaled the scores to a 0-100 range for comparability across evaluation metrics.Figure 7 presents the score distribution under different prompt templates, demonstrating that the 10-point rubric yields a broader and more informative spread.The prompt details we used can be found in Appendix A.4. Critical Thinking and Originality This dimension evaluates the depth of analysis, the originality of perspectives, and the articulation of forward-looking insights.To ensure consistency and interpretability, we designed structured prompts that elicit both numerical ratings and textual justifications from the model.Following the observation that the 100-point scale used in LLM ×MAPREDUCE-V2 [12] often results in compressed score distributions, we adopted a revised 10-point scale to enhance discriminative capacity.The evaluation prompt template is provided in Appendix A.5.</p>
<p>Topical Relevance We evaluated how well generated content aligns with the target research topic, following the approach in AUTOSURVEY [2].Our assessment focused on whether the survey maintains consistent focused on the intended subject, avoiding off-topic content.We employed a five-level scoring rubric (Table 2) that measures increasing degrees of topical coherence.We preserved the original AUTOSURVEY rubric without modification for comparability with prior work.</p>
<p>Structural Coherence Assessment</p>
<p>We evaluated the structural quality of generated content from both local and global perspectives.</p>
<p>Section-Level Structure This dimension assesses the internal coherence and logical flow of individual sections and subsections, following the rubric proposed in AUTOSURVEY.A score of 1 indicates disorganized or incoherent content, while a score of 5 denotes a tightly structured and logically consistent organization with smooth transitions.After evaluation, the score is linearly scaled to a 0-100 range.The full rubric is provided in Table 3.</p>
<p>Document-Level Structure</p>
<p>This dimension evaluates the overall structural coherence, thematic completeness, and scholarly depth of the document structure.We adopted a composite scoring scheme, assigning a score from 0 to 10 for each of the following three criteria: (1) structural coherence and narrative logic, (2) conceptual depth and thematic coverage, and (3) critical thinking and scholarly synthesis.The final score is calculated as the average of these sub-scores and is linearly scaled to a 0-100 range.Detailed prompts and scoring criteria are provided in Appendix A.6.</p>
<p>Note that Section-Level Structure focuses on local coherence between adjacent sections or subsections, while Document-Level Structure captures the document-wide organization, conceptual design, and thematic rigor.</p>
<p>Score Description
1
The content is outdated or unrelated to the field it purports to review, offering no alignment with the topic 2</p>
<p>The survey is somewhat on topic but with several digressions; the core subject is evident but not consistently adhered to.</p>
<p>3</p>
<p>The survey is generally on topic, despite a few unrelated details.</p>
<p>4</p>
<p>The survey is mostly on topic and focused; the narrative has a consistent relevance to the core subject with infrequent digressions.</p>
<p>5</p>
<p>The survey is exceptionally focused and entirely on topic; the article is tightly centered on the subject, with every piece of information contributing to a comprehensive understanding of the topic.</p>
<p>Table 2: Topical Relevance Assessment Rubric.</p>
<p>Score Description
1
The survey lacks logic, with no clear connections between sections, making it difficult to understand the overall framework.</p>
<p>2</p>
<p>The survey has weak logical flow with some content arranged in a disordered or unreasonable manner.</p>
<p>3</p>
<p>The survey has a generally reasonable logical structure, with most content arranged orderly, though some links and transitions could be improved such as repeated subsections.</p>
<p>4</p>
<p>The survey has good logical consistency, with content well arranged and natural transitions, only slightly rigid in a few parts.</p>
<p>5</p>
<p>The survey is tightly structured and logically clear, with all sections and content arranged most reasonably, and transitions between adajecent sections smooth without redundancy.</p>
<p>Table 3: Structural Coherence Evaluation Rubric.</p>
<p>Reference Accuracy Assessment</p>
<p>To evaluate the quality of reference usage in generated surveys, we compared the reference papers retrieved by models against those cited by human authors using standard information retrieval (IR) metrics.This evaluation is especially critical in retrieval-augmented generation (RAG) settings, as the quality of retrieved content directly impacts the factual accuracy and trustworthiness of the generated text.Specifically, we employed true positives (TP) and the F1 score [34] to quantify the degree of alignment between model-selected references and those curated by human experts.</p>
<p>True Positives (TP) Let A denote the set of references retrievd by the framework, and B denote the set of references cited in HUMAN WRITTEN surveys.We compute the number of correctly predicted references as:
TP = |A ∩ B|.
This metric reflects the absolute count of overlapping references between the model and the human-written baseline.</p>
<p>F1 Score We further compute:
F1 = 2 • Precision • Recall Precision + Recall , where Precision = |A ∩ B| |A| , Recall = |A ∩ B| |B| .
Precision measures the proportion of model-generated references that are also cited by human authors.Recall quantifies the proportion of human-cited references that the model successfully retrieves.The F1 score provides a harmonic mean of these two metrics, offering an overall measure of citation alignment.A higher F1 score indicates stronger agreement with human citation behavior and thus reflects superior reference retrieval quality within the RAG framework.</p>
<p>Human Evaluation by Domain Experts</p>
<p>To provide a more comprehensive evaluation of content quality beyond automatic metrics, we conducted a human study with domain experts.We randomly sampled 10 research topics and recruited graduate-level students in computer science as annotators.For each topic, annotators were presented with two documents: one generated by our SCISAGE system and one authored by human researchers.They were instructed to compare the texts across multiple dimensions, including logical coherence, academic tone, paragraph transitions, content completeness, and conciseness, among others.</p>
<p>6 Experiments</p>
<p>Baseline Configurations</p>
<p>To assess the effectiveness of SCISAGE, we compared it against three representative baselines.All methods were implemented using QWEN3-32B [9], and the title of each benchmark paper was used as the input seed for generation.Each baseline was executed using its official codebase with default or recommended configurations.A brief description of each baseline is provided below:</p>
<ol>
<li>OpenScholar (w/ SciSage) [16]: Since OpenScholar did not support outline generation natively, we incorporated outlines and paragraph-level queries generated by SCISAGE into its pipeline.The implementation was based on the official repository (https://github.com/AkariAsai/OpenScholar),and both local and online retrieval mode were enabled.</li>
</ol>
<p>AutoSurvey [2]:</p>
<p>As AUTOSURVEY lacks support for online retrieval, we use its offline corpus for both retrieval and summarization.Our implementation strictly follows the official codebase (https://github.com/AutoSurveys/AutoSurvey).</p>
<p>LLM × MapReduce-V2</p>
<p>[12]: We followed the official implementation from https://github.com/thunlp/LLMxMapReduce.The paper title was directly used as the input query, and the system employed its built-in online retrieval mechanism to collect relevant content before generation.</p>
<p>Complete hyperparameter settings for each baseline are provided in Appendix C.</p>
<p>Main Result</p>
<p>Automatic Evaluation Results</p>
<p>All evaluation results were obtained using QWEN3-32B [9].Table 4 reports the automatic evaluation scores for SCISAGE and three competitive baselines across content quality, structural coherence, reference accuracy.</p>
<p>Content Quality.SCISAGE achieves the highest score in critical thinking (77.58) while maintaining strong language fluency (85.65), slightly below LLM × MAPREDUCE-V2 (86.14).It also achieves perfect topical relevance (100).These results suggest that SCISAGE generally produces higher-quality content.</p>
<p>Structural Coherence.At both the section and document levels, SCISAGE outperformed all baselines, with the highest document coherence score (80.37).This indicates that SCISAGE demonstrates superior logical flow and structural organization.</p>
<p>Reference Accuracy.SCISAGE substantially improves citation accuracy, achieving an F1 score of 0.46 by correctly matching 1,510 references out of 3,844 cited in HUMAN WRITTEN papers.In contrast, competing baselines typically retrieve only a single overlapping reference, highlighting their limited capability in accurate citation reproduction.</p>
<p>These evaluation results demonstrate the effectiveness of SCISAGE.It consistently outperforms baselines across almost all metrics, especially in reference accuracy and document-level coherence.</p>
<p>Human Evaluaion Results</p>
<p>To evaluate the quality of content generated by SCISAGE, we conducted a human evaluation on a randomly selected set of 10 papers.These papers were assessed by professional evaluators, each holding a Master's degree in Computer Science.The evaluators performed a comprehensive analysis, contrasting the characteristics and identified shortcomings of SCISAGE's output against content authored by expert researchers on identical topics.Figure 8 shows the human evaluation results between SCISAGE and HUMAN WRITTEN, with further details provided in Appendix D.1.Strengths: Broad Coverage and Summarization SCISAGE excels at generating content that is broad in scope and performs as well as or better than human authors on summarization tasks.For example, in areas requiring extensive literature reviews and synthesis, such as the "Reasoning with Large Language Models, a Survey", SCISAGE can effectively summarize and present information.This feature makes it a valuable tool for quickly generating overviews and synthesizing large amounts of information.</p>
<p>Limitations: Depth, Precision, and Stylistic Nuance SCISAGE faces significant challenges in terms of content depth, especially when dealing with complex arguments, subtle details, and scenarios that require rigorous logical coherence or empirical support.It lacks precise and rigorous mathematical expression, which is particularly prominent in fields such as "reinforcement learning and algorithm research" that rely on precise formula descriptions.In terms of language style, SCISAGE tends to complicate sentence structure, resulting in less clear and concise writing, for example, by using vague terms such as "mitigation techniques" instead of precise academic vocabulary.Similar to existing generative models or frameworks, SCISAGE's generation also suffers from lengthy text and lacks integrated visual elements such as detailed formulas/charts, which are key carriers for conveying complex information in academic communication.</p>
<p>Conclusion: SciSage's Capabilities and Limitations in Academic Content Generation SCISAGE performs well in literature review and information integration, and can efficiently generate academic content with wide coverage, even surpassing the level of professional human authors.However, it still lacks analytical depth, mathematical expression accuracy, and academic language style, especially in fields that require complex logical reasoning, precise formulas, or rigorous terminology.</p>
<p>7 Ablation Study</p>
<p>Structural Impact of Query Understanding</p>
<p>We conduct an ablation study to investigate the structural benefits introduced by incorporating Query Understanding (Q.U.) in our framework.Specifically, we compare the following two experimental settings:</p>
<p>• Experiment A (w/ Q.U.):The complete SCISAGE pipeline, where the system first performs query understanding before generating the document structure.</p>
<p>• Experiment B (w/o Q.U.):A simplified pipeline that omits the query understanding step and directly proceeds to structure generation.</p>
<p>We evaluated the structural quality of the generated outlines along three dimensions: structural coherence, topical coverage, and critical analysis, as defined in Appendix A.6.</p>
<p>As shown in</p>
<p>Contribution of the Reflection</p>
<p>To assess the impact of iterative hierarchical reflection, we conducted an ablation study by disabling the reflection component in SCISAGE.Table 6 presents a comparison between the full system and its ablated variant.</p>
<p>Results show that reflection leads to sustained improvements in all dimensions assessed.Specifically, content quality improved significantly: Language scores increased from 82. 28</p>
<p>Limitations</p>
<p>Our study has several limitations that should be acknowledged:</p>
<p>• Language Restriction: The current evaluation is limited to English-language queries and documents.The effectiveness of our approach for other languages (e.g., Chinese) remains untested and may require additional language-specific adaptations.</p>
<p>• Domain Specificity: While we demonstrate strong performance in academic paper retrieval, the generalizability of our method to broader search scenarios (e.g., web search or enterprise document retrieval) requires further validation.</p>
<p>• Model Dependence: All reported results are based on the QWEN3-32B [9].The performance characteristics may vary when implemented with other foundation models, and comprehensive cross-model evaluation would be needed to establish broader applicability.</p>
<p>• Metric Saturation: Several systems, including SCISAGE, LLM × MAPREDUCE-V2, and AUTOSURVEY, achieved near-perfect scores in both Topical Relevance and Section Coherence.This saturation suggests that these metrics are becoming less effective in distinguishing between modern LLM-based generation systems, as they typically produce well-structured and topically relevant content.Future evaluations may require more fine-grained metrics to capture subtle differences in reasoning and factual consistency.</p>
<p>Conclusion</p>
<p>In this work, we present SciSage, a novel multi-agent framework that addresses long-standing limitations in automated scientific survey generation-specifically issues of structural coherence, content depth, and citation reliability.Guided by a reflect-when-you-write paradigm, SciSage coordinates six specialized agents across a dynamic workflow, with the Reflector Agent playing a central role in iteratively critiquing and refining outputs at the outline, section, and document levels.This reflection-driven architecture emulates expert authoring behavior and ensures end-to-end consistency and factual accuracy throughout the generation pipeline.SCISAGE significantly improves structural coherence and citation accuracy over existing methods.Rigorously evaluate the quality of an academic survey on the topic of [TOPIC] by scoring three dimensions (each on a 0-10 scale) and computing the average as the final score.</p>
<p>[Evaluation Criteria]</p>
<p>The final score is the average of the individual scores from the following three dimensions.Please evaluate each dimension rigorously based on the highest scholarly standards.</p>
<ol>
<li>Critical Analysis (10 points) Offers a deep and incisive critique of methodologies, results, and underlying assumptions.Clearly identifies significant gaps, weaknesses, and areas for improvement.Challenges assumptions with well-supported arguments and proposes concrete alternatives.</li>
</ol>
<p>Original Insights (10 points)</p>
<p>Proposes novel, well-supported interpretations or frameworks based on the reviewed literature.Demonstrates strong subject-matter understanding and contributes genuinely original perspectives.Insights are well-integrated with existing research, challenging conventional views or offering new directions.</p>
<p>Future Directions (10 points)</p>
<p>Clearly articulates promising research directions with strong justification.Suggestions are concrete, actionable, and closely tied to gaps identified in the literature.Demonstrates foresight by proposing innovative approaches or methodologies.</p>
<p>[Topic]</p>
<p>[TOPIC]</p>
<p>[Section]</p>
<p>[SECTION]</p>
<p>[Output Format] Rationale: <Provide a detailed justification for the score.Address each of the three dimensions step by step, highlighting specific strengths and weaknesses, such as the depth of critique, the originality of insights, or the clarity of proposed future directions.> Rigorously evaluate the quality of an academic survey outline on the topic of [TOPIC] by scoring three dimensions (each on a 0-10 scale) and computing the average as the final score.</p>
<p>[Evaluation Criteria]</p>
<p>Evaluate each dimension on a strict 0-10 scale, based on the following high-precision standards.The final score is the average of the three dimension scores.</p>
<p>Figure 1 :
1
Figures &amp; Charts Mind Map</p>
<p>Figure 2 :
2
Figure 2: Overview of the SurveyScope construction pipeline.</p>
<p>Figure 3 :Figure 4 :Figure 5 :
345
Figure 3: Distribution of topics in SurveyScope</p>
<p>Figure 6 :
6
Figure 6: Comparison of different benchmarks on Category Count, Topic Diversity, Data Volume, Year Span, Max Citations, Avg Citations</p>
<p>Figure 7 :
7
Figure 7: Score comparison: direct 100-point (blue) vs. scaled from 10-point rubric (purple).</p>
<p>Figure 8 :
8
Figure 8: Human evaluation results comparing SCISAGE with HUMAN WRITTEN papers</p>
<p>Final Score: <SCORE>(X+Y+Z/ 3 =
3
Final)</SCORE> Example: <SCORE>(2.5+7+5.1)/3=4.87</SCORE>Use two decimal places; do not include any other text outside the SCORE tag.A.6 Prompt for Evaluation Document Outline [Task]</p>
<p>Figure 9 :
9
Figure 9: Radar chart illustrating topic distribution across SurveyScope, SURVEYEVAL_TEST, and AutoSurvey.SurveyScope exhibits broader and more balanced domain coverage.</p>
<p>Figure 13 :
13
Figure 13: Example of generated outline</p>
<p>until ∆O = ∅ or max reflection trails N reached 10: Construct search queries for each section in final outline {Q i S } K i=1 ← ORGANIZER(O) 11: for all outlined section s i ∈ O do
8:end if9: 12:</p>
<p>13 :
13
Generate section content S i ← COMPOSER(s i , P i ) Refine and update S i ← COMPOSER(S i , P i , ∆S i ) until ∆C i = ∅ or max reflection trails N reached 20: end for 21: Integrate all sections to full survey F ← Merge(S 1 , . . ., S K ) 22: Refine and get the final survey F f inal ← REFINER(F ) 23: return F f inal
14:repeat15:Receive feedback from Reflector ∆S i ← REFLECTOR(S i , P i , O)16:if ∆S i ̸ = ∅ then17:18:end if19:</p>
<p>Table 4 :
4
Metrics of Automatic Evaluation.
MethodContent QualityStructural CoherenceReferenceLanguage Critical Relevance SectionDocumentF1TPOpenScholar (w/ SciSage)68.0953.5599--0.061 156AutoSurvey72.1360.90998565.330.14392LLM × MapReduce-V286.1476.9310010078.640.017 130SciSage85.6577.5810010080.370.46 1510</p>
<p>Table 5
5, incorporating query understanding leads to consistent improvements in both overall and aspect-level evaluation. The average and maximum document-level scores increase from 8.04 to 8.16 and from 9.00 to 9.33,respectively. Aspect-wise, improvements are observed in structure (8.74 vs. 8.64), coverage (8.32 vs. 8.20), andanalysis (7.40 vs. 7.29). These results suggest that query understanding enhances the SCISAGE's ability to generateoutlines that are more coherent, comprehensive, and analytically robust. (Full evaluation details and examples areprovided in our project repository.)MethodDocument Level StructureStructure Score DetailsAvg MaxMinStructure Coverage Analysisw/o Q.U. 8.04 9.006.338.648.207.29w/ Q.U. 8.16 9.336.008.748.327.40</p>
<p>Table 5 :
5
Comparison of SCISAGE with and without Reflection.</p>
<p>Table 6 :
6
to 85.60, and Critical scores significantly increased from 69.70 to 77.93.Structural coherence also benefited from reflection, with Document-level structure scores improving from 71.25 to 81.48.These findings suggest that repeated reflection enables SCISAGE to better revise and organize its generated content, resulting in more fluent, thoughtful, and well-structured content.Comparison of SCISAGE with and without Reflection.
MethodContent QualityStructural CoherenceLanguage Critical Relevance Section DocumentSciSage (w/o Reflection)82.2869.70100.0099.0071.25SciSage (w/ Reflection)85.6077.93100.00100.0081.48</p>
<p>To rigorously evaluate system performance, we introduce SurveyScope benchmark, curated for recency and scholarly impact, provides a robust testbed for evaluating survey-generation systems.Empirical results confirm SCISAGE's superiority: it achieves an 80.37 document-coherence score (vs.78.64 for LLM×MapReduce-V2) and 46% citation F1, outperforming all baselines.While SCISAGE still trails humanauthored surveys in analytical depth (30% win rate), it demonstrates clear advantages on relatively straightforward topics and offers substantial reductions in drafting time, highlighting its practical utility.strengths and weaknesses (e.g., academic tone consistency, clarity of sentence structure, or presence of redundancy).&gt;
Final Score:<SCORE>(X+Y+Z/3 = Final)</SCORE>Example: <SCORE>(2.5+7+5.1)/3=4.87</SCORE>Use up to two decimal places. Do not include any text outside the SCORE tags.A.5 Pormpt for Evaluation Critical Thinking Score[Task]</p>
<p>1 .
1
Structural Coherence &amp; Narrative Logic (10 points) Ideal Standard: The outline presents a well-structured, logically flowing framework.Sections and subsections are clearly organized, transitions are smooth, and the narrative progression is coherent.Scoring Guidance: Deduct points for imbalanced section lengths, disjointed transitions, or subsections that interrupt narrative clarity.A perfect score(10)requires no observable flaws.2. Conceptual Depth &amp; Thematic Coverage (10 points) Ideal Standard: The outline captures key themes, concepts, and subfields comprehensively and insightfully.There is a balance of breadth and depth, with core debates and historical development of the field clearly reflected.Scoring Guidance: Deduct points for missing major themes, excessive focus on niche areas, or shallow treatment of foundational concepts.3. Critical Thinking &amp; Scholarly Synthesis (10 points) Ideal Standard: The outline integrates perspectives critically, addressing contradictions, methodological tensions, and open research questions.It synthesizes viewpoints into a coherent scholarly vision.Scoring Guidance: Deduct points for lack of critical analysis, overlooking disagreements or critiques, or failing to propose unresolved questions.Rationale: <Provide a detailed reason for the score, considering each dimension step by step.Highlight specific strengths and weaknesses, such as structural imbalances, thematic omissions, or weak analytical synthesis.Then provide the final scores for each dimension.> <SCORE>(X+Y+Z/3 = ...)</SCORE> Example: <SCORE>(2.5+7+5.1)/3=4.87</SCORE>Use two decimal places; do not include any other text outside the SCORE tag.
Finance / Domain-specificLLMs (General)Dialogue SystemsSurveyScope SurveyEval_Test AutoSurveyLLMs EfficiencyBenchmarking / Evaluation8642LLMs Safety7. Medical / BiomedicalMedical / BiomedicalRoboticsMultimodalOtherNLP[Topic][TOPIC][Skeleton][OUTLINE][Output Format]-Structure: <X/10>-Coverage: <Y/10>-Critical Analysis: <Z/10>Final Score:
A Prompt TemplateA.1 Prompt for Query Understanding Prompt for Query Intent Chassification You are an expert in classifying user queries for academic research purposes.Your task is to analyze the given user query and extract the following information:1. Research Domain: Identify the broad academic field the query falls into.Examples: Computer Science, Medicine, Physics, Sociology, History, Linguistics.Be as specific as reasonably possible (e.g., "Machine Learning" if clearly indicated within Computer Science, otherwise "Computer Science").2. Query Type: Determine the type of information or paper the user is likely seeking.You MUST choose one of the following predefined types: survey, method, application, analysis, position, theory, benchmark, dataset, OTHER.If none of the specific types fit well, use OTHER.3.Research Topic: Pinpoint the specific subject, concept, or entities at the core of the query.This should be a concise phrase representing the main focus.For example, if the query is "latest advancements in using LLMs for code generation", the topic could be "LLMs for code generation".Prompt for Query RewritingYou are a query rewriting expert.Your task is to evaluate a given query and determine if it requires rewriting by checking for: Rigorously evaluate the quality of an academic survey on the topic of [TOPIC] by scoring three dimensions on a 0-10 scale.The final score is the arithmetic mean of the three individual scores.[Evaluation Criteria]Assign scores for each dimension based on the highest academic standards described below.The final score is calculated as the average of the three:1. Academic Formality (10 points) Demonstrates flawless academic rigor.Uses precise terminology consistently, avoids colloquial language entirely, and maintains a scholarly tone throughout.Sentence structures are sophisticated and intentionally crafted to support analytical depth.Even a single instance of informal phrasing or vague terminology disqualifies a perfect score.Clarity &amp; Readability (10 points)Writing is exceptionally clear, concise, and unambiguous.Sentences are logically structured with seamless transitions.The argument progresses smoothly with no unnecessary complexity.Any ambiguity or minor inefficiency reduces the score.Redundancy (10 points)Uniqueness: Every sentence should contribute new value.Repetition is only acceptable for structural clarity, such as reinforcing terminology or aiding transitions.Efficiency: Arguments must be logically coherent and free from unnecessary repetition.Redundant rephrasing of the same point without adding new insight leads to point deductions.[Topic][TOPIC][Section][SECTION]
Growth rates of modern science: a latent piecewise growth curve approach to model publication numbers from established and new literature databases. Robin Lutz Bornmann, Rüdiger Haunschild, Mutz, Humanities and Social Sciences Communications. 812021</p>
<p>Autosurvey: Large language models can automatically write surveys. Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Qingsong Wen, Wei Ye, Advances in Neural Information Processing Systems. 202437</p>
<p>Deep reinforcement learning: A survey. Xu Wang, Sen Wang, Xingxing Liang, Dawei Zhao, Jincai Huang, Xin Xu, Bin Dai, Qiguang Miao, IEEE Transactions on Neural Networks and Learning Systems. 3542022</p>
<p>A survey on large language models: Applications, challenges, limitations, and practical usage. Rizwan Muhammad Usman Hadi, Abbas Qureshi, Muhammad Shah, Anas Irfan, Muhammad Zafar, Naveed Bilal Shaikh, Jia Akhtar, Seyedali Wu, Mirjalili, 2023Authorea Preprints</p>
<p>A survey on large language model based autonomous agents. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Frontiers of Computer Science. 1861863452024</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, arXiv:2312.118052023arXiv preprint</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang ; Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, Zihan Qiu, arXiv:2505.09388Qwen3 technical report. Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren,2025arXiv preprint</p>
<p>Yijia Shao, Yucheng Jiang, Theodore A Kanell, Peter Xu, Omar Khattab, Monica S Lam, arXiv:2402.14207Assisting in writing wikipedia-like articles from scratch with large language models. 2024arXiv preprint</p>
<p>Into the unknown unknowns: Engaged human learning through participation in language model agent conversations. Yucheng Jiang, Yijia Shao, Dekun Ma, Sina J Semnani, Monica S Lam, arXiv:2408.152322024arXiv preprint</p>
<p>Llm × mapreduce-v2: Entropy-driven convolutional test-time scaling for generating long-form articles from extremely long resources. Haoyu Wang, Yujia Fu, Zhu Zhang, Shuo Wang, Zirui Ren, Xiaorong Wang, Zhili Li, Chaoqun He, Bo An, Zhiyuan Liu, arXiv:2504.057322025arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202336</p>
<p>Self-rag: Learning to retrieve, generate, and critique through self-reflection. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Self-reflection in llm agents: Effects on problem-solving performance. Matthew Renze, Erhan Guven, arXiv:2405.066822024arXiv preprint</p>
<p>Akari Asai, Jacqueline He, Rulin Shao, Weijia Shi, Amanpreet Singh, Joseph Chee Chang, Kyle Lo, Luca Soldaini, Sergey Feldman, Mike D , arXiv:2411.14199Synthesizing scientific literature with retrieval-augmented lms. 2024arXiv preprint</p>
<p>Surveyforge: On the outline heuristics, memory-driven generation, and multi-dimensional evaluation for automated survey writing. Xiangchao Yan, Shiyang Feng, Jiakang Yuan, Renqiu Xia, Bin Wang, Bo Zhang, Lei Bai, arXiv:2503.046292025arXiv preprint</p>
<p>Interactivesurvey: An llm-based personalized and interactive survey paper generation system. Zhiyuan Wen, Jiannong Cao, Zian Wang, Beichen Guo, Ruosong Yang, Shuaiqi Liu, arXiv:2504.087622025arXiv preprint</p>
<p>. OpenAI. Introducing deep research. 2024</p>
<p>Gemini deep research overview. Google Deepmind, 2024</p>
<p>A survey on llm-based multi-agent systems: workflow, infrastructure, and challenges. Xinyi Li, Sai Wang, Siqi Zeng, Yu Wu, Yi Yang, 2024Vicinagearth19</p>
<p>Large language model based multi-agents: A survey of progress and challenges. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, arXiv:2402.016802024arXiv preprint</p>
<p>Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka, Shing Yau, Zijuan Lin, Liyang Zhou, arXiv:2308.00352Meta programming for multi-agent collaborative framework. 202336arXiv preprint</p>
<p>Autogen: Enabling next-gen llm applications via multi-agent conversation. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, arXiv:2308.081552023arXiv preprint</p>
<p>Unleashing the emergent cognitive synergy in large language models: A task-solving agent through multi-persona self-collaboration. Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, Heng Ji, arXiv:2307.053002023arXiv preprint</p>
<p>Generative agents: Interactive simulacra of human behavior. Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 36th annual acm symposium on user interface software and technology. the 36th annual acm symposium on user interface software and technology2023</p>
<p>Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, arXiv:2308.10848202326arXiv preprint</p>
<p>Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, arXiv:2307.07924Communicative agents for software development. 2023arXiv preprint</p>
<p>Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization. Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, Diyi Yang, arXiv:2310.021702023arXiv preprint</p>
<p>Encouraging divergent thinking in large language models through multi-agent debate. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, Zhaopeng Tu, arXiv:2305.191182023arXiv preprint</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, Igor Mordatch, Forty-first International Conference on Machine Learning. 2023</p>
<p>Agentnet: Decentralized evolutionary coordination for llm-based multi-agent systems. Yingxuan Yang, Huacan Chai, Shuai Shao, Yuanyi Song, Siyuan Qi, Renting Rui, Weinan Zhang, arXiv:2504.005872025arXiv preprint</p>
<p>Chatgpt and open-ai models: A preliminary review. I Konstantinos, Nikolaos D Roumeliotis, Tselikas, Future Internet. 1561922023</p>
<p>A probabilistic interpretation of precision, recall and f-score, with implication for evaluation. Cyril Goutte, Eric Gaussier, European conference on information retrieval. Springer2005</p>
<p>. Finance / Domain-specific. 9</p>
<p>Other Paper Title: {title} SurveyScope (n=46) SurveyEval_Test (n=20) AutoSurvey (n=20). 2019 2020 2021 2022 2023 2024 2025 20262022Benchmarking / Evaluation 11. Year Med:2023 Mean:2023.3 Max:2025 Min:2020 Med:2023 Mean:2022.5 Max:2024 Min:2020 Med:2024 Mean:2023.6 Max:2025 Min</p>
<p>SurveyScope emphasizes more recent works, reflecting rapid developments in the field. min_citation: 5 Minimum citation count for reference papers. norm_cite: True Normalize citation counts. ss_retriever: True Enable Semantic Scholar online retrieval. use_feedback: True Enable feedback for iterative refinement. new_feedback_docs: 2 Documents retrieved after feedback. Figure. 10Boxplot showing publication year distributions across benchmarks. feedback_num: 4 Number of feedback items used</p>
<p>Human Evaluation Details Paper Title Evaluation Result Human Analysis Measure and Improve Robustness in NLP Models: A Survey Human is better Human version defines robustness clearly, has better structure and logic. D Experiment Result, LLM version has awkward phrasing and lacks coherence</p>
<p>A Survey on Explainability in Machine Reading Comprehension Human is better Human version uses structured benchmarks and visuals effectively; LLM version lacks clarity and has poor section design. </p>
<p>LLM offers broader metrics. The Decades Progress on Code-Switching Research in NLP Human is better Human version aligns better with survey goals using empirical analysis. Efficient Methods for Natural Language Processing: A Survey Same Both cover NLP efficiency. human is clear. LLM fails to capture research trend focus</p>
<p>A Survey of Large Language Models in Medicine Human is better Human version is structured around medical use cases; LLM version is disjointed and overly focused on technical background. </p>
<p>A Survey of Controllable Text Generation Human is better Human version is intuitive and organized by model stages; LLM version is messy and lacks strategy-method separation. </p>
<p>Neural Entity Linking: A Survey of Models Based on Deep Learning Human is better Human version follows processing pipeline; LLM version has incoherent topic grouping and surface-level analysis. Reasoning with Large Language Models, a Survey SciSage is better Human version is CoT-focused but narrow. A Survey on Detection of LLMs-Generated Content Same LLM version is well-structured and easy to follow; human version introduces more novel and timely perspectives. LLM version covers broader reasoning aspects despite typical stylistic flaws</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3221 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3221</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3221</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-74.html">extraction-schema-74</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-88064de690af282dbdf222774f03ff070b9df22b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/88064de690af282dbdf222774f03ff070b9df22b" target="_blank">Beyond Goldfish Memory: Long-Term Open-Domain Conversation</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper Abstract:</strong> Despite recent improvements in open-domain dialogue models, state of the art models are trained and evaluated on short conversations with little context. In contrast, the long-term conversation setting has hardly been studied. In this work we collect and release a human-human dataset consisting of multiple chat sessions whereby the speaking partners learn about each otherâ€™s interests and discuss the things they have learnt from past sessions. We show how existing models trained on existing datasets perform poorly in this long-term conversation setting in both automatic and human evaluations, and we study long-context models that can perform much better. In particular, we find retrieval-augmented methods and methods with an ability to summarize and recall previous conversations outperform the standard encoder-decoder architectures currently considered state of the art.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3221.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3221.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BST 2.7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BlenderBot BST 2.7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pretrained encoder-decoder Transformer (BlenderBot family) used as a baseline; standard short-context model with 128-token encoder truncation in its released form.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards a human-like open-domain chatbot</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BST 2.7B (BlenderBot)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A 2.7B-parameter encoder-decoder Transformer (BlenderBot) pretrained for open-domain dialogue; in experiments used both in its original 128-token encoder-truncated form and as a baseline for fine-tuning on MSC.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>context window (standard, truncated)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>No external or long-term memory; the model uses only its encoder-decoder context window (default encoder truncation 128 tokens) and thus cannot access distant-session history beyond truncation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-Session Chat (MSC)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A newly collected multi-session open-domain dialogue dataset where pairs of personas converse across multiple sessions separated by hours or days; task tests long-term conversational consistency and re-engagement.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>dialogue</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>As a short-context baseline, BST 2.7B performs substantially worse on later sessions and session openings compared to models that use longer context or explicit memory; human raters found it less engaging and less likely to reference prior-session partner topics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>128-token truncation prevents use of long- or multi-session context; leads to poorer session-opening behavior and lower human engagingness compared to memory-augmented models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Goldfish Memory: Long-Term Open-Domain Conversation', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3221.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3221.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MSC 2.7B (trunc1024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MSC 2.7B (BST 2.7B fine-tuned, encoder extended to 1024 tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>BST 2.7B model fine-tuned on the MSC dataset with encoder positional embeddings extended to accept up to 1024 input tokens (context-window extension).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MSC 2.7B (truncate 1024)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Fine-tuned version of BST 2.7B where encoder positional embeddings were extended to allow up to 1024 tokens of dialogue context; no separate external memory or retrieval module.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>context window extension</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Extends the internal Transformer encoder positional embeddings from 128 to 1024 tokens so that larger amounts of recent dialogue (including previous sessions' text or summaries) can be provided within the model's standard context window.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-Session Chat (MSC)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Long-term multi-session dialogue where models must incorporate previous-session information to produce coherent openings and follow-ups.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>dialogue</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Test perplexity (sessions 1-5): [8.25, 8.76, 8.93, 9.07, 9.16]; Session Openings perplexity: 8.09 (Table 7). Human eval: engaging response rate 54.2%, final rating 3.47 (Table 8).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Increasing the model's truncation length to 1024 reduces perplexity versus the 128-token baseline and improves session-opening performance, showing that simply increasing context capacity helps, but gains over retrieval/summarization methods are modest.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Even at 1024 tokens, the model can still forget older content if conversation exceeds the window; scaling context size is inefficient and does not provide a persistent memory guarantee.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Goldfish Memory: Long-Term Open-Domain Conversation', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3221.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3221.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented generation framework that uses a learned bi-encoder retriever (DPR) to fetch documents which are then used by the generator; used here as an augmentation to MSC fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for knowledge-intensive nlp tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MSC 2.7B (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>MSC fine-tuned BST 2.7B augmented with a RAG-style retriever: a DPR bi-encoder indexes prior-dialogue documents (utterances, sessions or summaries), retrieves top-N candidates, and conditions generation on those retrieved candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Stores past dialogue items (utterances, session texts or summaries) as documents with DPR dense vector encodings; at generation time the bi-encoder scores and returns top-N documents which are attended to by the generator (end-to-end training applied in RAG).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-Session Chat (MSC)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-domain multi-session dialogue requiring access to prior-session content to produce contextually appropriate responses, especially at session openings.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>dialogue</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Test perplexity (sessions 1-5): [8.22, 8.78, 8.97, 9.11, 9.17]; Session Openings perplexity: 8.10 (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RAG-style retrieval enables the model to effectively access earlier-session content and improves over the 128-token baseline by a large margin; it performs comparably to large truncation baselines on average and provides a memory that does not forget as conversations grow.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires a learned retriever and document encoding; retrieval quality is a bottleneck and storage/retrieval overhead grows with memory size. Improvements over 1024-token truncation were small in some metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Goldfish Memory: Long-Term Open-Domain Conversation', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3221.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3221.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FiD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fusion-in-Decoder (FiD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented architecture where each retrieved document is encoded separately and the decoder attends over all encodings (used as an augmentation to MSC models).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Leveraging passage retrieval with generative models for open domain question answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MSC 2.7B (FiD)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>MSC fine-tuned BST 2.7B using the FiD approach: for each retrieved document (session/text/summary), the encoder encodes it separately and the decoder attends across the concatenated encodings to generate responses.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (FiD)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Documents from past dialogues are retrieved by a DPR-style retriever; each retrieved document is encoded independently and the decoder attends to the set of encodings to condition generation, enabling structured use of multiple retrieved memory items.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-Session Chat (MSC)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-session dialogue requiring retrieval and integration of salient past information to craft responses and session openings.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>dialogue</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Test perplexity (sessions 1-5): [8.22, 8.75, 8.92, 9.05, 9.11]; Session Openings perplexity: 8.06 (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>FiD typically outperforms RAG (and is competitive with large truncation) by better integrating multiple retrieved documents; it grants consistent improvements, notably on session-opening perplexity and some human metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Higher computation as each retrieved document must be separately encoded; performance depends on retrieval quality and document chunking strategy (session-level documents worked better).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Goldfish Memory: Long-Term Open-Domain Conversation', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3221.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3221.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FiD-RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FiD with RAG-trained retriever (FiD-RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid: FiD generation conditioned on documents retrieved by a RAG-trained retriever (jointly optimized retrieval for generation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for knowledge-intensive nlp tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MSC 2.7B (FiD-RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>FiD generator combined with a RAG-style retriever that was trained end-to-end to optimize generation; retrieves session-level documents or summaries and the FiD decoder attends to their encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (RAG-trained retriever + FiD generator)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Uses a RAG-trained DPR bi-encoder to rank memory documents and FiD-style encoding of top-N retrieved documents for the decoder to attend to; retrieval is optimized for the downstream generation objective.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-Session Chat (MSC)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Long-horizon multi-session dialogue requiring accurate retrieval of past salient facts and integration into responses.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>dialogue</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Test perplexity (sessions 1-5): [8.23, 8.75, 8.93, 9.04, 9.11]; Session Openings perplexity: 8.03 (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>FiD-RAG is among the strongest retrieval-augmented configurations, providing low perplexities and improved session-opening behavior compared to baselines without retrieval; retrieval training helps.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Complexity and training cost increase with joint retriever-generator training; still sensitive to retrieval/document chunk choices.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Goldfish Memory: Long-Term Open-Domain Conversation', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3221.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3221.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SumMem-MSC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Summarization Memory-Augmented MSC (SumMem-MSC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A novel read-write memory approach introduced in this paper that summarizes dialogue turns into short knowledge statements which are stored and later retrieved to condition generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SumMem-MSC 2.7B (summarization memory; FiD-RAG variant)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Introduces a two-component system: (1) an encoder-decoder abstractive summarizer that emits a one-line summary (or 'no-summary') of new salient info per turn and writes these into a long-term memory; (2) a retrieval-augmented generator (RAG/FiD/FiD-RAG) that retrieves and conditions on these stored summaries to generate responses.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>summarization-based long-term memory (read-write summaries) + retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>On each new turn the summarizer produces a short abstractive summary (or a 'no-summary' label); generated summaries are appended to a persistent memory store (documents). At response time, a DPR-style retriever fetches top summaries (or session-level summaries) and a generator (FiD/RAG) attends to them to produce the next response. Training includes supervised summarizer training (using human-annotated summaries) and retrieval/generation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-Session Chat (MSC)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Long-term open-domain dialogue where storing salient points across sessions and recalling them later (e.g., at session openings) is critical for engagement and coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>dialogue</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>SumMem-MSC (FiD-RAG) test perplexity (sessions 1-5): [8.22, 8.70, 8.89, 9.00, 9.07]; Session Openings perplexity: 7.87 (Table 7). Human eval (Table 8): SumMem-MSC (RAG) engaging 62.1% / final rating 3.65; SumMem-MSC (FiD) engaging 58.9% / 3.62; SumMem-MSC (FiD-RAG) engaging 59.3% / 3.68. Also improvements in BLEU/F1 (see Appendix tables).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Summarization-based memory plus retrieval outperforms encoder-decoder baselines (BST) and truncation baselines in both automatic metrics (lower perplexity, small BLEU/F1 gains) and human evaluations (higher engaging response rate and final ratings), and increases references to partner topics from previous sessions (important for perceived engagingness). The FiD-RAG SumMem variant was the best overall method tested.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Predicted summaries are noisier than gold summaries and require subsampling of the 'no-summary' class during training to match human sparsity; gains in perplexity over a 1024-token truncation are modest in some metrics; retrieval quality and summarizer errors are failure modes; storage and retrieval overhead still present.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Goldfish Memory: Long-Term Open-Domain Conversation', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3221.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3221.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Predicted-Summary Mem</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Predicted Summary Memory (automatic summarizer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised encoder-decoder summarizer trained on human summary annotations to predict per-turn abstractive summaries (or 'no-summary') which are then used as memory entries for retrieval-augmented generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Predicted Summary Memory (SumMem predicted)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An encoder-decoder model trained on MSC human-annotated summaries that predicts whether a turn generates a summary line and the summary text; predicted summaries are stored in the memory store and retrieved by RAG/FiD generators.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>learned summarization memory (predicted summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>The summarizer emits either a short summary string for a turn or a 'no-summary' token. During training the no-summary label is subsampled (K%) to control sparsity. Predicted summaries are inserted into the long-term memory and are retrieved like gold summaries for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-Session Chat (MSC)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same MSC long-term dialogue task; tests whether automatically predicted summaries can substitute for human gold summaries to support long-term recall.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>dialogue</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Validation perplexity (predicted summaries with 5% no-summary sampling): sessions 2-5 perplexities approx [9.11, 8.98, 9.07, 9.00]; Session Openings: 7.44 (Table 5). Sparsity of generated summaries depends on subsampling (e.g., 29.1% sparsity at 5% sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Predicted summaries improve over using raw dialogue-history context alone but do not match performance of gold summaries; subsampling the no-summary class during summarizer training yields better sparsity that matches human data and improves downstream performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Predicted summaries are less informative than gold summaries; selecting appropriate no-summary subsampling rate is necessary; inaccuracies in summarization can degrade retrieval usefulness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Goldfish Memory: Long-Term Open-Domain Conversation', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Leveraging passage retrieval with generative models for open domain question answering <em>(Rating: 2)</em></li>
                <li>Dense passage retrieval for open-domain question answering <em>(Rating: 2)</em></li>
                <li>Retrieval augmentation reduces hallucination in conversation <em>(Rating: 2)</em></li>
                <li>Towards a human-like open-domain chatbot <em>(Rating: 2)</em></li>
                <li>Longformer: The long-document transformer <em>(Rating: 1)</em></li>
                <li>Reformer: The efficient transformer <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3221",
    "paper_id": "paper-88064de690af282dbdf222774f03ff070b9df22b",
    "extraction_schema_id": "extraction-schema-74",
    "extracted_data": [
        {
            "name_short": "BST 2.7B",
            "name_full": "BlenderBot BST 2.7B",
            "brief_description": "Pretrained encoder-decoder Transformer (BlenderBot family) used as a baseline; standard short-context model with 128-token encoder truncation in its released form.",
            "citation_title": "Towards a human-like open-domain chatbot",
            "mention_or_use": "use",
            "agent_name": "BST 2.7B (BlenderBot)",
            "agent_description": "A 2.7B-parameter encoder-decoder Transformer (BlenderBot) pretrained for open-domain dialogue; in experiments used both in its original 128-token encoder-truncated form and as a baseline for fine-tuning on MSC.",
            "memory_used": false,
            "memory_type": "context window (standard, truncated)",
            "memory_mechanism_description": "No external or long-term memory; the model uses only its encoder-decoder context window (default encoder truncation 128 tokens) and thus cannot access distant-session history beyond truncation.",
            "task_name": "Multi-Session Chat (MSC)",
            "task_description": "A newly collected multi-session open-domain dialogue dataset where pairs of personas converse across multiple sessions separated by hours or days; task tests long-term conversational consistency and re-engagement.",
            "task_type": "dialogue",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "As a short-context baseline, BST 2.7B performs substantially worse on later sessions and session openings compared to models that use longer context or explicit memory; human raters found it less engaging and less likely to reference prior-session partner topics.",
            "limitations_or_challenges": "128-token truncation prevents use of long- or multi-session context; leads to poorer session-opening behavior and lower human engagingness compared to memory-augmented models.",
            "uuid": "e3221.0",
            "source_info": {
                "paper_title": "Beyond Goldfish Memory: Long-Term Open-Domain Conversation",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "MSC 2.7B (trunc1024)",
            "name_full": "MSC 2.7B (BST 2.7B fine-tuned, encoder extended to 1024 tokens)",
            "brief_description": "BST 2.7B model fine-tuned on the MSC dataset with encoder positional embeddings extended to accept up to 1024 input tokens (context-window extension).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MSC 2.7B (truncate 1024)",
            "agent_description": "Fine-tuned version of BST 2.7B where encoder positional embeddings were extended to allow up to 1024 tokens of dialogue context; no separate external memory or retrieval module.",
            "memory_used": true,
            "memory_type": "context window extension",
            "memory_mechanism_description": "Extends the internal Transformer encoder positional embeddings from 128 to 1024 tokens so that larger amounts of recent dialogue (including previous sessions' text or summaries) can be provided within the model's standard context window.",
            "task_name": "Multi-Session Chat (MSC)",
            "task_description": "Long-term multi-session dialogue where models must incorporate previous-session information to produce coherent openings and follow-ups.",
            "task_type": "dialogue",
            "performance_with_memory": "Test perplexity (sessions 1-5): [8.25, 8.76, 8.93, 9.07, 9.16]; Session Openings perplexity: 8.09 (Table 7). Human eval: engaging response rate 54.2%, final rating 3.47 (Table 8).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Increasing the model's truncation length to 1024 reduces perplexity versus the 128-token baseline and improves session-opening performance, showing that simply increasing context capacity helps, but gains over retrieval/summarization methods are modest.",
            "limitations_or_challenges": "Even at 1024 tokens, the model can still forget older content if conversation exceeds the window; scaling context size is inefficient and does not provide a persistent memory guarantee.",
            "uuid": "e3221.1",
            "source_info": {
                "paper_title": "Beyond Goldfish Memory: Long-Term Open-Domain Conversation",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation (RAG)",
            "brief_description": "A retrieval-augmented generation framework that uses a learned bi-encoder retriever (DPR) to fetch documents which are then used by the generator; used here as an augmentation to MSC fine-tuning.",
            "citation_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "mention_or_use": "use",
            "agent_name": "MSC 2.7B (RAG)",
            "agent_description": "MSC fine-tuned BST 2.7B augmented with a RAG-style retriever: a DPR bi-encoder indexes prior-dialogue documents (utterances, sessions or summaries), retrieves top-N candidates, and conditions generation on those retrieved candidates.",
            "memory_used": true,
            "memory_type": "retrieval-augmented",
            "memory_mechanism_description": "Stores past dialogue items (utterances, session texts or summaries) as documents with DPR dense vector encodings; at generation time the bi-encoder scores and returns top-N documents which are attended to by the generator (end-to-end training applied in RAG).",
            "task_name": "Multi-Session Chat (MSC)",
            "task_description": "Open-domain multi-session dialogue requiring access to prior-session content to produce contextually appropriate responses, especially at session openings.",
            "task_type": "dialogue",
            "performance_with_memory": "Test perplexity (sessions 1-5): [8.22, 8.78, 8.97, 9.11, 9.17]; Session Openings perplexity: 8.10 (Table 7).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "RAG-style retrieval enables the model to effectively access earlier-session content and improves over the 128-token baseline by a large margin; it performs comparably to large truncation baselines on average and provides a memory that does not forget as conversations grow.",
            "limitations_or_challenges": "Requires a learned retriever and document encoding; retrieval quality is a bottleneck and storage/retrieval overhead grows with memory size. Improvements over 1024-token truncation were small in some metrics.",
            "uuid": "e3221.2",
            "source_info": {
                "paper_title": "Beyond Goldfish Memory: Long-Term Open-Domain Conversation",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "FiD",
            "name_full": "Fusion-in-Decoder (FiD)",
            "brief_description": "A retrieval-augmented architecture where each retrieved document is encoded separately and the decoder attends over all encodings (used as an augmentation to MSC models).",
            "citation_title": "Leveraging passage retrieval with generative models for open domain question answering",
            "mention_or_use": "use",
            "agent_name": "MSC 2.7B (FiD)",
            "agent_description": "MSC fine-tuned BST 2.7B using the FiD approach: for each retrieved document (session/text/summary), the encoder encodes it separately and the decoder attends across the concatenated encodings to generate responses.",
            "memory_used": true,
            "memory_type": "retrieval-augmented (FiD)",
            "memory_mechanism_description": "Documents from past dialogues are retrieved by a DPR-style retriever; each retrieved document is encoded independently and the decoder attends to the set of encodings to condition generation, enabling structured use of multiple retrieved memory items.",
            "task_name": "Multi-Session Chat (MSC)",
            "task_description": "Multi-session dialogue requiring retrieval and integration of salient past information to craft responses and session openings.",
            "task_type": "dialogue",
            "performance_with_memory": "Test perplexity (sessions 1-5): [8.22, 8.75, 8.92, 9.05, 9.11]; Session Openings perplexity: 8.06 (Table 7).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "FiD typically outperforms RAG (and is competitive with large truncation) by better integrating multiple retrieved documents; it grants consistent improvements, notably on session-opening perplexity and some human metrics.",
            "limitations_or_challenges": "Higher computation as each retrieved document must be separately encoded; performance depends on retrieval quality and document chunking strategy (session-level documents worked better).",
            "uuid": "e3221.3",
            "source_info": {
                "paper_title": "Beyond Goldfish Memory: Long-Term Open-Domain Conversation",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "FiD-RAG",
            "name_full": "FiD with RAG-trained retriever (FiD-RAG)",
            "brief_description": "A hybrid: FiD generation conditioned on documents retrieved by a RAG-trained retriever (jointly optimized retrieval for generation).",
            "citation_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "mention_or_use": "use",
            "agent_name": "MSC 2.7B (FiD-RAG)",
            "agent_description": "FiD generator combined with a RAG-style retriever that was trained end-to-end to optimize generation; retrieves session-level documents or summaries and the FiD decoder attends to their encodings.",
            "memory_used": true,
            "memory_type": "retrieval-augmented (RAG-trained retriever + FiD generator)",
            "memory_mechanism_description": "Uses a RAG-trained DPR bi-encoder to rank memory documents and FiD-style encoding of top-N retrieved documents for the decoder to attend to; retrieval is optimized for the downstream generation objective.",
            "task_name": "Multi-Session Chat (MSC)",
            "task_description": "Long-horizon multi-session dialogue requiring accurate retrieval of past salient facts and integration into responses.",
            "task_type": "dialogue",
            "performance_with_memory": "Test perplexity (sessions 1-5): [8.23, 8.75, 8.93, 9.04, 9.11]; Session Openings perplexity: 8.03 (Table 7).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "FiD-RAG is among the strongest retrieval-augmented configurations, providing low perplexities and improved session-opening behavior compared to baselines without retrieval; retrieval training helps.",
            "limitations_or_challenges": "Complexity and training cost increase with joint retriever-generator training; still sensitive to retrieval/document chunk choices.",
            "uuid": "e3221.4",
            "source_info": {
                "paper_title": "Beyond Goldfish Memory: Long-Term Open-Domain Conversation",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "SumMem-MSC",
            "name_full": "Summarization Memory-Augmented MSC (SumMem-MSC)",
            "brief_description": "A novel read-write memory approach introduced in this paper that summarizes dialogue turns into short knowledge statements which are stored and later retrieved to condition generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SumMem-MSC 2.7B (summarization memory; FiD-RAG variant)",
            "agent_description": "Introduces a two-component system: (1) an encoder-decoder abstractive summarizer that emits a one-line summary (or 'no-summary') of new salient info per turn and writes these into a long-term memory; (2) a retrieval-augmented generator (RAG/FiD/FiD-RAG) that retrieves and conditions on these stored summaries to generate responses.",
            "memory_used": true,
            "memory_type": "summarization-based long-term memory (read-write summaries) + retrieval",
            "memory_mechanism_description": "On each new turn the summarizer produces a short abstractive summary (or a 'no-summary' label); generated summaries are appended to a persistent memory store (documents). At response time, a DPR-style retriever fetches top summaries (or session-level summaries) and a generator (FiD/RAG) attends to them to produce the next response. Training includes supervised summarizer training (using human-annotated summaries) and retrieval/generation.",
            "task_name": "Multi-Session Chat (MSC)",
            "task_description": "Long-term open-domain dialogue where storing salient points across sessions and recalling them later (e.g., at session openings) is critical for engagement and coherence.",
            "task_type": "dialogue",
            "performance_with_memory": "SumMem-MSC (FiD-RAG) test perplexity (sessions 1-5): [8.22, 8.70, 8.89, 9.00, 9.07]; Session Openings perplexity: 7.87 (Table 7). Human eval (Table 8): SumMem-MSC (RAG) engaging 62.1% / final rating 3.65; SumMem-MSC (FiD) engaging 58.9% / 3.62; SumMem-MSC (FiD-RAG) engaging 59.3% / 3.68. Also improvements in BLEU/F1 (see Appendix tables).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Summarization-based memory plus retrieval outperforms encoder-decoder baselines (BST) and truncation baselines in both automatic metrics (lower perplexity, small BLEU/F1 gains) and human evaluations (higher engaging response rate and final ratings), and increases references to partner topics from previous sessions (important for perceived engagingness). The FiD-RAG SumMem variant was the best overall method tested.",
            "limitations_or_challenges": "Predicted summaries are noisier than gold summaries and require subsampling of the 'no-summary' class during training to match human sparsity; gains in perplexity over a 1024-token truncation are modest in some metrics; retrieval quality and summarizer errors are failure modes; storage and retrieval overhead still present.",
            "uuid": "e3221.5",
            "source_info": {
                "paper_title": "Beyond Goldfish Memory: Long-Term Open-Domain Conversation",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "Predicted-Summary Mem",
            "name_full": "Predicted Summary Memory (automatic summarizer)",
            "brief_description": "A supervised encoder-decoder summarizer trained on human summary annotations to predict per-turn abstractive summaries (or 'no-summary') which are then used as memory entries for retrieval-augmented generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Predicted Summary Memory (SumMem predicted)",
            "agent_description": "An encoder-decoder model trained on MSC human-annotated summaries that predicts whether a turn generates a summary line and the summary text; predicted summaries are stored in the memory store and retrieved by RAG/FiD generators.",
            "memory_used": true,
            "memory_type": "learned summarization memory (predicted summaries)",
            "memory_mechanism_description": "The summarizer emits either a short summary string for a turn or a 'no-summary' token. During training the no-summary label is subsampled (K%) to control sparsity. Predicted summaries are inserted into the long-term memory and are retrieved like gold summaries for generation.",
            "task_name": "Multi-Session Chat (MSC)",
            "task_description": "Same MSC long-term dialogue task; tests whether automatically predicted summaries can substitute for human gold summaries to support long-term recall.",
            "task_type": "dialogue",
            "performance_with_memory": "Validation perplexity (predicted summaries with 5% no-summary sampling): sessions 2-5 perplexities approx [9.11, 8.98, 9.07, 9.00]; Session Openings: 7.44 (Table 5). Sparsity of generated summaries depends on subsampling (e.g., 29.1% sparsity at 5% sampling).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Predicted summaries improve over using raw dialogue-history context alone but do not match performance of gold summaries; subsampling the no-summary class during summarizer training yields better sparsity that matches human data and improves downstream performance.",
            "limitations_or_challenges": "Predicted summaries are less informative than gold summaries; selecting appropriate no-summary subsampling rate is necessary; inaccuracies in summarization can degrade retrieval usefulness.",
            "uuid": "e3221.6",
            "source_info": {
                "paper_title": "Beyond Goldfish Memory: Long-Term Open-Domain Conversation",
                "publication_date_yy_mm": "2021-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 2
        },
        {
            "paper_title": "Leveraging passage retrieval with generative models for open domain question answering",
            "rating": 2
        },
        {
            "paper_title": "Dense passage retrieval for open-domain question answering",
            "rating": 2
        },
        {
            "paper_title": "Retrieval augmentation reduces hallucination in conversation",
            "rating": 2
        },
        {
            "paper_title": "Towards a human-like open-domain chatbot",
            "rating": 2
        },
        {
            "paper_title": "Longformer: The long-document transformer",
            "rating": 1
        },
        {
            "paper_title": "Reformer: The efficient transformer",
            "rating": 1
        }
    ],
    "cost": 0.01693025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Beyond Goldfish Memory: Long-Term Open-Domain Conversation</h1>
<p>Jing Xu Arthur Szlam Jason Weston<br>Facebook AI Research<br>New York, NY<br>{jingxu23,aszlam, jase}@fb.com</p>
<h4>Abstract</h4>
<p>Despite recent improvements in open-domain dialogue models, state-of-the-art models are trained and evaluated on short conversations with little context. In contrast, the long-term conversation setting has hardly been studied. In this work we collect and release a humanhuman dataset consisting of multiple chat sessions whereby the speaking partners learn about each other's interests and discuss the things they have learnt from past sessions. We show how existing models trained on existing datasets perform poorly in this long-term conversation setting in both automatic and human evaluations, and we study long-context models that can perform much better. In particular, we find retrieval-augmented methods and methods with an ability to summarize and recall previous conversations outperform the standard encoder-decoder architectures currently considered state-of-the-art.</p>
<h2>1 Introduction</h2>
<p>Improvements in the ability to train large neural language models, together with the availability of larger and higher quality dialogue datasets, are spurring the development of increasingly convincing open-domain dialogue models (McTear, 2020). Unfortunately, a major aspect missing from the current state of the art is that human conversations can take place over long time frames, whereas the currently used systems suffer in this setting. Commonly used training and evaluation resources - while large in terms of number of training examples - include only short conversations, typically between 2-15 turns, consisting of a single conversational session. Perhaps for that reason, the current state-of-the-art models such as Meena (Adiwardana et al., 2020) and BlenderBot (Roller et al., 2020) employ Transformers with token truncation lengths of only the 128 most recent tokens, and</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>are clearly incapable of incorporating long-term conversational context. Consequently, it is unclear how well these models will perform on long or multi-session open-domain conversations. In contrast, a successfully deployed bot will engage in many conversations over a length of time, as capturing organic user interest will garner continual reengagement from returning users. Long-term open-domain communication gives the opportunity for the conversation to develop and even improve with time as the model has more context and more understanding of that specific user's interests. However current models, due to context truncation, will never use this information.</p>
<p>In this work we study methods for long-term open-domain conversation. As to the best of our knowledge no public domain task exists to study such methods, we collect and release ${ }^{1}$ a new English dataset, entitled Multi-Session Chat (MSC) that consists of human-human crowdworker chats over 5 sessions, with each session consisting of up to 14 utterances, where the conversationalists reengage after a number of hours or days and continue chatting. Previous sessions are annotated with summaries of important personal points that may be useful in further conversations. When reengaging, conversationalists often address existing knowledge about their partner to continue the conversation in a way that focuses and deepens the discussions on their known shared interests, or explores new ones given what they already know.</p>
<p>We study the performance of two long-context conversational architectures on this task: (i) retrieval-augmented generative models (Lewis et al., 2020b; Shuster et al., 2021); and (ii) a proposed read-write memory-based model that summarizes and stores conversation on the fly. We show that both techniques outperform conventional encoder-decoder Transformers, and that training</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>models on our new task give long-term conversational abilities that existing state-of-the-art models lack, as shown in both automatic metrics and human evaluations. We provide extensive experiments and ablations that study the reasons behind these improvements.</p>
<h2>2 Related Work</h2>
<p>A relatively large and growing number of either natural or crowdsourced datasets have been collected and used in open-domain dialogue research. These datasets focus on the vast array of different skills required by a dialogue agent, but conversations lengths are typically short. Recent state-of-the-art open-domain dialogue agents have utilized Daily Dialogue (Li et al., 2017), PersonaChat (Zhang et al., 2018), Empathetic Dialogues (Rashkin et al., 2019), Wizard of Wikipedia (Dinan et al., 2019) and Pushshift.io Reddit (Baumgartner et al., 2020); see Huang et al. (2020) for a review of other datasets. The number of conversational turns in these datasets is in the range of 2-15 turns, we provide statistics of some of these datasets in Table 2. We note there also exist some other kinds of dialogue datasets, e.g. from fantasy role-playing (Urbanek et al., 2019; Rameshkumar and Bailey, 2020) and TV shows as well (Poria et al., 2018). Crowdsourcing long conversations is difficult due to both the expense and the difficulty of employing crowdworkers for long lengths of time due to so called Human Intelligence Tasks (HITs) being typically of a short duration - only "a few minutes" (Paolacci et al., 2010). While organic long conversations regularly transpire on the internet, e.g. on messaging platforms, these are proprietary, and privacy concerns make public release implausible.</p>
<p>Several existing datasets explore the use of personal knowledge used as context to dialogue, which can be seen as a short, simple memory provided to the bot. In MazarÃ© et al. (2018) such personas were extracted from Reddit and used to train agents. In Zhang et al. (2018) personas were first crowdsourced, and speakers were asked to play those roles. Other works have considered encoding personas into vector-based weights (Li et al., 2016).</p>
<p>In this work, we explore summarizing the longterm conversations that occur in order to store useful information about them. Summarization is a rich field where the vast majority of work focuses on summarizing documents (Kaikhah, 2004; KryÅ›ciÅ„ski et al., 2019; Cheng and Lapata, 2016), for
example summarizing in order to predict other relevant information (West et al., 2019), while there is some work on dialogue as well (Goo and Chen, 2018; Gliwa et al., 2019; Pan et al., 2018).</p>
<p>Standard Transformers have a fixed context length which due to the all-vs-all self-attention mechanism becomes inefficient when it is too large. Consequently, many existing pre-trained models have short token truncation lengths, e.g. 128 tokens, as in BlenderBot (Roller et al., 2020) and Meena (Adiwardana et al., 2020), or 1024 tokens, as in BART (Lewis et al., 2020a). A number of approaches have been proposed to ameliorate this issue. Long-context Transformers consider ways to speed up the self-attention mechanism (Child et al., 2019; Kitaev et al., 2019; Beltagy et al., 2020) and retrieval-augmented methods consider ways to select the pertinent parts of the context to consider (Dinan et al., 2019; Lewis et al., 2020b; Shuster et al., 2021) which can also be related to earlier neural QA methods (Chen et al., 2017).</p>
<h2>3 Multi-Session Chat</h2>
<p>To conduct research on long-term conversations, we require data to both train on and to evaluate models. We consider the natural case where two speakers chat online in a series of sessions as is for example common on messaging platforms. Each chat session consists of 6-7 turns for each speaker. Then, after a certain amount of (simulated) time has transpired, typically hours or days, the speakers resume chatting, either continuing to talk about the previous subject, bringing up some other subject from their past shared history, or sparking up conversation on a new topic. We consider this multi-session long conversation setup, and name our dataset Multi-Session Chat (MSC).</p>
<p>Data Collection To build our publicly available dataset we employ crowdworkers. We provide screenshots of the task, and details of quality control via onboarding, crowdworker co-rating, and automatic evaluation procedures in Appendix B.</p>
<p>Personas Crowdworkers are asked to play a role, rather than speaking about their own personality, which helps mitigate privacy concerns, and ensures diversity even if the same crowdworker conducts multiple conversations. In addition to the crowdworkers being specifically told to play the role, they are also told not to discuss aspects of their real profiles or indeed any personally identifiable informa-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Data Type</th>
<th style="text-align: right;">Epsiodes</th>
<th style="text-align: right;">Train <br> Utts.</th>
<th style="text-align: right;">Summary</th>
<th style="text-align: right;">Epsiodes</th>
<th style="text-align: right;">Valid <br> Utts.</th>
<th style="text-align: right;">Summary</th>
<th style="text-align: right;">Epsiodes</th>
<th style="text-align: right;">Test <br> Utts.</th>
<th style="text-align: right;">Summary</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Session 1</td>
<td style="text-align: right;">8939</td>
<td style="text-align: right;">131,438</td>
<td style="text-align: right;">59,894</td>
<td style="text-align: right;">1,000</td>
<td style="text-align: right;">7,801</td>
<td style="text-align: right;">7,768</td>
<td style="text-align: right;">1015</td>
<td style="text-align: right;">6,634</td>
<td style="text-align: right;">6,572</td>
</tr>
<tr>
<td style="text-align: left;">Session 2</td>
<td style="text-align: right;">4000</td>
<td style="text-align: right;">46,420</td>
<td style="text-align: right;">46,420</td>
<td style="text-align: right;">500</td>
<td style="text-align: right;">5,897</td>
<td style="text-align: right;">5,897</td>
<td style="text-align: right;">501</td>
<td style="text-align: right;">5,939</td>
<td style="text-align: right;">5,939</td>
</tr>
<tr>
<td style="text-align: left;">Session 3</td>
<td style="text-align: right;">4000</td>
<td style="text-align: right;">47,259</td>
<td style="text-align: right;">26,976</td>
<td style="text-align: right;">500</td>
<td style="text-align: right;">5,890</td>
<td style="text-align: right;">5,890</td>
<td style="text-align: right;">501</td>
<td style="text-align: right;">5,924</td>
<td style="text-align: right;">5,924</td>
</tr>
<tr>
<td style="text-align: left;">Session 4</td>
<td style="text-align: right;">1001</td>
<td style="text-align: right;">11,870</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">500</td>
<td style="text-align: right;">5,904</td>
<td style="text-align: right;">5,904</td>
<td style="text-align: right;">501</td>
<td style="text-align: right;">5,940</td>
<td style="text-align: right;">5,940</td>
</tr>
<tr>
<td style="text-align: left;">Session 5</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">500</td>
<td style="text-align: right;">5,964</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">501</td>
<td style="text-align: right;">5,945</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">236,987</td>
<td style="text-align: right;">133,290</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">31,456</td>
<td style="text-align: right;">25,459</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">30,382</td>
<td style="text-align: right;">24,375</td>
</tr>
</tbody>
</table>
<p>Table 1: Data statistics of our Multi-SESSION Chat dataset. Speakers converse across sessions, each of which is a short focused conversation, with subsequent sessions picking up the conversation again hours or days later. We show the number of episodes, utterances (utts) and response summaries for each session.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: right;">Num. <br> Episodes</th>
<th style="text-align: right;">Num. <br> Utterances</th>
<th style="text-align: right;">Unique <br> Tokens</th>
<th style="text-align: right;">Avg. Utt. <br> Length</th>
<th style="text-align: right;">Sessions <br> per Episode</th>
<th style="text-align: right;">Utterances <br> per Episode</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Pushshift.io Reddit</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">1.2 B</td>
<td style="text-align: right;">$\sim 1 \mathrm{M}$</td>
<td style="text-align: right;">25.4</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">3.2</td>
</tr>
<tr>
<td style="text-align: left;">PersonaChat (Zhang et al., 2018)</td>
<td style="text-align: right;">8,939</td>
<td style="text-align: right;">131,438</td>
<td style="text-align: right;">18,688</td>
<td style="text-align: right;">11.9</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">14.7</td>
</tr>
<tr>
<td style="text-align: left;">Wiz. of Wikipedia (Dinan et al., 2019)</td>
<td style="text-align: right;">18,430</td>
<td style="text-align: right;">166,787</td>
<td style="text-align: right;">52,490</td>
<td style="text-align: right;">19.7</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">9.0</td>
</tr>
<tr>
<td style="text-align: left;">Daily Dialog (Li et al., 2017)</td>
<td style="text-align: right;">22,236</td>
<td style="text-align: right;">87,170</td>
<td style="text-align: right;">20,673</td>
<td style="text-align: right;">14.5</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">3.9</td>
</tr>
<tr>
<td style="text-align: left;">Empathetic Dialog (Kuziklis et al., 2019)</td>
<td style="text-align: right;">24,850</td>
<td style="text-align: right;">64,636</td>
<td style="text-align: right;">19,458</td>
<td style="text-align: right;">15.3</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">2.6</td>
</tr>
<tr>
<td style="text-align: left;">Multi-SESSION Chat (1-3)</td>
<td style="text-align: right;">4,000</td>
<td style="text-align: right;">161,440</td>
<td style="text-align: right;">37,366</td>
<td style="text-align: right;">21.4</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">40.4</td>
</tr>
<tr>
<td style="text-align: left;">Multi-SESSION Chat (1-4)</td>
<td style="text-align: right;">1,001</td>
<td style="text-align: right;">53,332</td>
<td style="text-align: right;">23,387</td>
<td style="text-align: right;">23.0</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">53.3</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison of the training data statistics of the Multi-SESSION Chat (MSC) dataset compared to other open-domain datasets. We show MSC in two categories: episodes with 3 or 4 sessions, named (1-3) or (1-4).
tion. The role is provided as a series of sentences describing characteristics, events and opinions of the character they are playing. We use the 1,155 personas crowdsourced from Zhang et al. (2018), validation and test use separate personas from the ones used in the training set.</p>
<p>Session 1 For the first chat session we use the PersonaChat dataset (Zhang et al., 2018), which already involves short conversations where two speakers get to know each other for the first time. We note that these conversations rarely go beyond the superficial stage because speakers simply do not have enough turns to discuss any topic deeply.</p>
<p>Sessions 2, 3, 4, ... For subsequent sessions, we first select a random amount of (simulated) time that has elapsed since the previous session, chosen to be either 1-7 hours or 1-7 days, as ideally speakers would reengage within that timeframe. We ask the crowdworkers to play the same roles that were played in the previous session, acting as if that amount of time has transpired. We note these crowdworkers may not be the same ones that played those characters in previous sessions, but will be playing the same roles: this makes the task tractable in a crowdworking frameworking where jobs are typically short, and matching pairs over a long duration would be infeasible. We instruct the workers to "chitchat with another worker for 6
turns, as if you were catching up since last time you two spoke." and that "When you expand the topic, make sure it makes sense with the personal details already mentioned.", i.e. emphasizing that not only must they play their role, but also pay attention to previous interactions with the other speaker.</p>
<p>Session Lengths We collect two lengths of training conversation: 4000 episodes with 3 sessions, and 1001 episodes with 4 sessions. For the validation and test data, the sessions extend up to 5 sessions, giving us a way to measure long-context session performance that extends beyond the training set distribution.</p>
<h2>Conversation Summaries (Extended Personas)</h2>
<p>We give crowdworkers access to all previous dialogues between the two conversational roles (for the role they are playing, and their partner's role). However, as the conversation gets longer, this becomes infeasible to read and digest within a limited amount of time. Therefore, between each session, including after session 1, we run a separate crowdworker task in which conversations are summarized into important points, which are much shorter than the full dialogues themselves. We then show previous dialogues, along with these summaries, as the primary reference for subsequent session dialogues. As these summaries were collected in order to store the important points pertinent to either one or the</p>
<p>other speaker, they can also be seen to function as extensions of the original given personas. As the two speakers continue to converse they create more depth to those characters.</p>
<p>Dataset Examples Two dataset examples, which consist of four sessions each, along with example summary annotations, are given in Appendix C (provided in the Appendix due to their length).</p>
<p>Dataset Statistics Statistics of the multi-session chat dataset are given in Table 1 and a comparison with other standard open-domain dialogue datasets is given in Table 2. We can see that the number of training utterances per episode is larger than other datasets (last column of Table 2). Our multisession training chats that last 4 sessions have an average of $\sim 53$ utterances in a full conversation (over all sessions), while our validation and test chats over 5 sessions have an average of $\sim 66$ utterances. In contrast, other standard datasets are in the range of 2.6-14.7 utterances on average. This brings challenges in open-domain dialogue modeling due to the large context size, e.g. an average of 1614 tokens as tokenized by the BlenderBot BPE dictionary (Roller et al., 2020), where the Transformer used in that work has a truncation length of 128. Further information on the dataset including analysis of its quality is given in Appendix B.</p>
<h2>4 Modeling Multi-Session Chat</h2>
<h3>4.1 Transformer Encoder-Decoders</h3>
<p>The most straight-forward approach for modeling dialogue using our new task is simply to use a large language model as is standard in open-domain dialogue, i.e. an encoder-decoder Transformer as in the Meena (Adiwardana et al., 2020) and BlenderBot (Roller et al., 2020) systems. We consider using the BST 2.7B parameter model from BlenderBot as an initial pre-trained model, which we then fine-tune on the Multi-Session Chat task.</p>
<p>Encoder Truncation As BST 2.7B has a truncation of 128 tokens in the encoder, we consider extending this to a larger input. To do this, we extend its available positional encodings from 128 to 512 or 1024 tokens as we fine-tune the whole network on the downstream task. We add new positional embeddings to be trained such that the existing ones (the first 128 most recent tokens) do not change from before. We then evaluate the impact of these choices in order to select the best model.</p>
<h3>4.2 Retrieval-Augmentation</h3>
<p>A popular technique when dealing with a large collection of text, only some of which is relevant, is to use a retrieval-augmented Transformer. A retrieval system is used to search over a text collection, and select some of it to be included in the final encoding which is attended to by the Transformer decoder.</p>
<p>RAG The RAG (Retrieval-Augmented Generation) approach (Lewis et al., 2020b) utilizes a neural-retriever-in-the-loop which is itself a second Transformer. Documents to be retrieved are stored in an approximate nearest-neighbor FAISS index (Johnson et al., 2019), and a DPR (Dense Passage Retrieval) (Karpukhin et al., 2020) Transformer biencoder model is used to score document-context pairs in order to rank them based on their match, where the base DPR model is pre-trained on QA data pairs. The DPR model is thus used to both retrieve from the FAISS index, and then score the top $N$ candidates. The entire system is trained end-to-end so that retrieval is optimized to help improve generation. This setup was shown to work for dialogue in particular in Shuster et al. (2021).</p>
<p>FiD and FiD-RAG We also consider the Fusion-in-Decoder (FiD) (Izacard and Grave, 2020), another method that has been shown to perform well. In this approach, the pre-trained retriever is used directly: each of the top $N$ documents returned is prepended to the context and encoded separately by the encoder, and finally all the results are concatenated. The decoder then attends to these encodings to produce a final response. We consider the pre-trained retriever to either be standard pretrained DPR, or the RAG-trained retriever, called FiD-RAG (Shuster et al., 2021).</p>
<p>Retriever and Documents In this work the set of passages in the memory is not large enough to require a FAISS index, but it is large enough that retrieval may be useful. We thus store for every item in the memory the vector encoding by the DPR model (whereas in the FAISS approach this dense vector is approximated instead). Then given a dialogue context, we score each memory using the bi-encoder, and use the top $N$ for generation. In our case, the memories consist of dialog utterances from the history of the conversation. We consider the chunk (document) size as a hyperparameter and try either encoding utterances as separate documents, or else whole sessions (or session summaries) as documents. The latter (whole se-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Pre-Train Model</th>
<th style="text-align: center;">Truncation</th>
<th style="text-align: center;">Sessions 1-4</th>
<th style="text-align: center;">Session 1</th>
<th style="text-align: center;">Session 2</th>
<th style="text-align: center;">Session 3</th>
<th style="text-align: center;">Session 4</th>
<th style="text-align: center;">Trunc\% (S4)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">With no previous session context</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">BST 2.7B</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">9.23</td>
<td style="text-align: center;">8.76</td>
<td style="text-align: center;">9.45</td>
<td style="text-align: center;">9.31</td>
<td style="text-align: center;">9.40</td>
<td style="text-align: center;">$51 \%$</td>
</tr>
<tr>
<td style="text-align: left;">BST 2.7B</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">9.06</td>
<td style="text-align: center;">8.18</td>
<td style="text-align: center;">9.42</td>
<td style="text-align: center;">9.26</td>
<td style="text-align: center;">9.36</td>
<td style="text-align: center;">$0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">BST 2.7B</td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">9.08</td>
<td style="text-align: center;">8.20</td>
<td style="text-align: center;">9.46</td>
<td style="text-align: center;">9.29</td>
<td style="text-align: center;">9.37</td>
<td style="text-align: center;">$0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">With previous session dialogue context</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">BST 2.7B</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">9.16</td>
<td style="text-align: center;">8.75</td>
<td style="text-align: center;">9.32</td>
<td style="text-align: center;">9.22</td>
<td style="text-align: center;">9.32</td>
<td style="text-align: center;">$100 \%$</td>
</tr>
<tr>
<td style="text-align: left;">BST 2.7B</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">8.87</td>
<td style="text-align: center;">8.15</td>
<td style="text-align: center;">9.14</td>
<td style="text-align: center;">9.04</td>
<td style="text-align: center;">9.17</td>
<td style="text-align: center;">$100 \%$</td>
</tr>
<tr>
<td style="text-align: left;">BST 2.7B</td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">8.89</td>
<td style="text-align: center;">8.17</td>
<td style="text-align: center;">9.18</td>
<td style="text-align: center;">9.05</td>
<td style="text-align: center;">9.16</td>
<td style="text-align: center;">$80 \%$</td>
</tr>
<tr>
<td style="text-align: left;">With previous session summary context</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">BST 2.7B</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">9.09</td>
<td style="text-align: center;">8.77</td>
<td style="text-align: center;">9.24</td>
<td style="text-align: center;">9.12</td>
<td style="text-align: center;">9.24</td>
<td style="text-align: center;">$100 \%$</td>
</tr>
<tr>
<td style="text-align: left;">BST 2.7B</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">8.79</td>
<td style="text-align: center;">8.17</td>
<td style="text-align: center;">8.69</td>
<td style="text-align: center;">9.15</td>
<td style="text-align: center;">9.22</td>
<td style="text-align: center;">$36 \%$</td>
</tr>
<tr>
<td style="text-align: left;">BST 2.7B</td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">8.80</td>
<td style="text-align: center;">8.18</td>
<td style="text-align: center;">9.05</td>
<td style="text-align: center;">8.91</td>
<td style="text-align: center;">9.04</td>
<td style="text-align: center;">$0 \%$</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparison of different context truncation lengths and context types when training on MultiSession Chat. We show validation perplexity for various models across different sessions, and percent of tokens truncated for session 4 (last column).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Session</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Session Openings</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model Context</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">No Session History</td>
<td style="text-align: center;">9.46</td>
<td style="text-align: center;">9.29</td>
<td style="text-align: center;">9.37</td>
<td style="text-align: center;">9.30</td>
<td style="text-align: center;">9.96</td>
<td style="text-align: center;">10.99</td>
<td style="text-align: center;">10.69</td>
<td style="text-align: center;">10.46</td>
</tr>
<tr>
<td style="text-align: left;">Dialogue History</td>
<td style="text-align: center;">9.18</td>
<td style="text-align: center;">9.05</td>
<td style="text-align: center;">9.16</td>
<td style="text-align: center;">9.08</td>
<td style="text-align: center;">7.55</td>
<td style="text-align: center;">8.48</td>
<td style="text-align: center;">8.27</td>
<td style="text-align: center;">7.94</td>
</tr>
<tr>
<td style="text-align: left;">Gold Summary</td>
<td style="text-align: center;">9.04</td>
<td style="text-align: center;">8.90</td>
<td style="text-align: center;">9.02</td>
<td style="text-align: center;">8.96</td>
<td style="text-align: center;">6.98</td>
<td style="text-align: center;">7.96</td>
<td style="text-align: center;">7.94</td>
<td style="text-align: center;">7.77</td>
</tr>
<tr>
<td style="text-align: left;">Gold Summary (without time features)</td>
<td style="text-align: center;">9.05</td>
<td style="text-align: center;">8.91</td>
<td style="text-align: center;">9.04</td>
<td style="text-align: center;">8.95</td>
<td style="text-align: center;">6.97</td>
<td style="text-align: center;">7.95</td>
<td style="text-align: center;">7.97</td>
<td style="text-align: center;">7.74</td>
</tr>
<tr>
<td style="text-align: left;">Gold Summary (partner's only)</td>
<td style="text-align: center;">9.14</td>
<td style="text-align: center;">8.99</td>
<td style="text-align: center;">9.11</td>
<td style="text-align: center;">9.03</td>
<td style="text-align: center;">7.66</td>
<td style="text-align: center;">8.49</td>
<td style="text-align: center;">8.49</td>
<td style="text-align: center;">8.07</td>
</tr>
<tr>
<td style="text-align: left;">Gold Summary (self only)</td>
<td style="text-align: center;">9.29</td>
<td style="text-align: center;">9.10</td>
<td style="text-align: center;">9.18</td>
<td style="text-align: center;">9.13</td>
<td style="text-align: center;">8.40</td>
<td style="text-align: center;">8.94</td>
<td style="text-align: center;">8.52</td>
<td style="text-align: center;">8.39</td>
</tr>
<tr>
<td style="text-align: left;">Predicted Summary</td>
<td style="text-align: center;">9.11</td>
<td style="text-align: center;">8.98</td>
<td style="text-align: center;">9.07</td>
<td style="text-align: center;">9.00</td>
<td style="text-align: center;">7.44</td>
<td style="text-align: center;">8.43</td>
<td style="text-align: center;">8.20</td>
<td style="text-align: center;">7.81</td>
</tr>
</tbody>
</table>
<p>Table 4: Summaries vs. Dialogue Context Performance when training on Multi-SESSION Chat, reporting validation perplexity, using a BST 2.7B-1024 pre-trained model with MSC fine-tuning. Note that the last row in this Table corresponds to the SumMem-MSC 2.7B (truncate 1024) row in Table 15 in the Appendix.
sions) worked better, and we report those in the final results. For $N$ we try values 3,5 and 6 , and also choose the best for each method according to the validation set.</p>
<h3>4.3 Summarization Memory-Augmentation</h3>
<p>The retrieval-augmentation models described in the previous section retrieve from the set of past dialogues. Simply storing historical text in the memory in their raw form is a simple approach that is often used elsewhere in the literature, e.g. in question answering or knowledge-grounded dialogue. However, those approaches have two potential drawbacks: (i) there is a lot of context to store, and hence retrieve from; (ii) no processing has been done on that content, so the reading, retrieving and combining operations required to generate an answer leave a lot of work for the model to do. We therefore propose instead a novel memory augmentation that first summarizes pertinent knowledge and only stores that in an attempt to solve both problems.</p>
<p>The procedure involves two main components:</p>
<ol>
<li>An encoder-decoder abstractive summarizer that takes as input the dialogue history, and outputs a summary of new pertinent information contained in the last dialogue turn, or "no-summary" if there is no new information found. When found, the summarized knowledge is added to the long-term memory.</li>
<li>A memory-augmented generator that takes the dialogue context and access to the long-term memory, and generates the next response.</li>
</ol>
<p>For (1) we can use the human annotated data from our newly collected MSC task to know what summaries to generate (see section 3 and Figure 1 in the Appendix). We thus train a supervised encoder-decoder model to produce summaries.</p>
<p>For (2) we can use the same systems as presented in subsection 4.2 to both retrieve from the summarization memories, and to finally generate an appropriate response. That is, we store the summaries in documents and retrieve them using either RAG, FiD or FiD-RAG.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Session</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Session Openings</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model Context</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Sparsity</td>
</tr>
<tr>
<td style="text-align: left;">Gold summary</td>
<td style="text-align: center;">9.04</td>
<td style="text-align: center;">8.90</td>
<td style="text-align: center;">9.02</td>
<td style="text-align: center;">8.96</td>
<td style="text-align: center;">6.98</td>
<td style="text-align: center;">7.96</td>
<td style="text-align: center;">7.94</td>
<td style="text-align: center;">7.77</td>
<td style="text-align: center;">$42.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Predicted Summary (sampling 5\%)</td>
<td style="text-align: center;">9.11</td>
<td style="text-align: center;">8.98</td>
<td style="text-align: center;">9.07</td>
<td style="text-align: center;">9.00</td>
<td style="text-align: center;">7.44</td>
<td style="text-align: center;">8.43</td>
<td style="text-align: center;">8.20</td>
<td style="text-align: center;">7.81</td>
<td style="text-align: center;">$29.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Predicted Summary (sampling 25\%)</td>
<td style="text-align: center;">9.11</td>
<td style="text-align: center;">8.97</td>
<td style="text-align: center;">9.07</td>
<td style="text-align: center;">9.01</td>
<td style="text-align: center;">7.46</td>
<td style="text-align: center;">8.53</td>
<td style="text-align: center;">8.22</td>
<td style="text-align: center;">7.94</td>
<td style="text-align: center;">$41.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Predicted Summary (sampling 50\%)</td>
<td style="text-align: center;">9.14</td>
<td style="text-align: center;">8.99</td>
<td style="text-align: center;">9.08</td>
<td style="text-align: center;">9.02</td>
<td style="text-align: center;">7.57</td>
<td style="text-align: center;">8.62</td>
<td style="text-align: center;">8.37</td>
<td style="text-align: center;">8.11</td>
<td style="text-align: center;">$50.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Predicted Summary (sampling 100\%)</td>
<td style="text-align: center;">9.14</td>
<td style="text-align: center;">8.99</td>
<td style="text-align: center;">9.10</td>
<td style="text-align: center;">9.03</td>
<td style="text-align: center;">7.68</td>
<td style="text-align: center;">8.69</td>
<td style="text-align: center;">8.56</td>
<td style="text-align: center;">8.25</td>
<td style="text-align: center;">$61.8 \%$</td>
</tr>
</tbody>
</table>
<p>Table 5: Predicted Summaries when subsampling the no-summary class on Multi-SESSION CHAT, reporting validation perplexity, using a BST 2.7B-1024 pre-trained model with MSC fine-tuning. The last column shows the sparsity of the summarizations (how often a summary line is generated), which can be controlled by subsampling the no-summary class at training time. Subsampling gives better results and closer sparsity levels to the original human annotated data.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">Session</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Training Data</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">4</td>
</tr>
<tr>
<td style="text-align: left;">Session 1</td>
<td style="text-align: right;">8.24</td>
<td style="text-align: right;">11.4</td>
<td style="text-align: right;">11.2</td>
<td style="text-align: right;">11.3</td>
</tr>
<tr>
<td style="text-align: left;">Sessions $1+2$</td>
<td style="text-align: right;">8.21</td>
<td style="text-align: right;">9.21</td>
<td style="text-align: right;">9.09</td>
<td style="text-align: right;">9.24</td>
</tr>
<tr>
<td style="text-align: left;">Sessions $1+2+3$</td>
<td style="text-align: right;">8.16</td>
<td style="text-align: right;">9.05</td>
<td style="text-align: right;">8.93</td>
<td style="text-align: right;">9.06</td>
</tr>
<tr>
<td style="text-align: left;">Sessions $1+2+3+4$</td>
<td style="text-align: right;">8.16</td>
<td style="text-align: right;">9.02</td>
<td style="text-align: right;">8.89</td>
<td style="text-align: right;">9.02</td>
</tr>
</tbody>
</table>
<p>Table 6: Varying the Number of Training Sessions when training on Multi-SESSION CHAT, reporting validation perplexity, using a BST 2.7B-1024 pre-trained model with MSC using gold summaries.</p>
<h2>5 Experiments</h2>
<p>Using session dialogue context We compare different context types in Table 3, evaluating over sessions 1-4. We observe an improvement in perplexity when incorporating the dialogue history from previous chat sessions, compared to no session context, for all sessions after the first one, and for all context lengths - with larger context lengths giving better improvement. This shows that our human conversationalists do use previous sessions to make dialogue more salient in successive sessions as this is reflected in the collected human-human dataset - and that our models are able to utilize this information well when training on this data.</p>
<p>Using session summary context We also show performance of using gold session summary contexts, as annotated by crowdworkers, in Table 3. As the summaries include salient points, they are potentially more informative than dialogue context for a generative model. We find perplexities improve when using summaries compared to using dialogue context (or no context at all) over all sessions after the first one, and for all context lengths, although the improvements are not large. This shows that conversation summaries are potentially useful for dialogue generation in the long-context case.</p>
<p>Comparing performance on session openings Session openings in the MSC dataset look quite different to other dialogue datasets that do not have a session format. This is because they involve an opening message that is intended to reengage the other speaker after a period of time, using known information that has been exchanged between speakers. In Table 4 we compare models that use different context types on only these opening responses. In this case we find much more pronounced perplexity differences between no session context history, dialogue history or summary context history. For example, we see around around 2 perplexity points difference between using or not using previous session context. We show examples of opening session generations in Appendix C. We observe that opening messages are categorically different to other conversation turns, typically involving a statement or question given knowledge of shared interests contained in the long-context. This explains why collection of our new dataset is so important for this goal, as reflected in perplexity improvements. That is, they indicate that our new task will likely help improve multi-session conversational engagement with users compared to existing training schemes.</p>
<p>Comparing different context lengths As shown in Table 3 changing the context length of a Transformer can impact the performance in our task. With no previous session context, improvements are minimal for sessions 2 onwards. However, using session dialogue or summary contexts we do see improvements with larger lengths of 512 or 1024 tokens, compared to 128. The last column of Table 3 shows the percentage of responses where the input to the Transformer is truncated for session 4, for each truncation length. One can see that using summaries can be beneficial as they are shorter,</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Session 1</th>
<th style="text-align: center;">Session 2</th>
<th style="text-align: center;">Session 3</th>
<th style="text-align: center;">Session 4</th>
<th style="text-align: center;">Session 5</th>
<th style="text-align: center;">Session Openings</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BST 2.7B (Roller et al., 2020)</td>
<td style="text-align: center;">8.97</td>
<td style="text-align: center;">9.98</td>
<td style="text-align: center;">10.26</td>
<td style="text-align: center;">10.40</td>
<td style="text-align: center;">10.50</td>
<td style="text-align: center;">12.92</td>
</tr>
<tr>
<td style="text-align: left;">MSC 2.7B (truncate 128)</td>
<td style="text-align: center;">8.87</td>
<td style="text-align: center;">8.89</td>
<td style="text-align: center;">9.10</td>
<td style="text-align: center;">9.21</td>
<td style="text-align: center;">9.27</td>
<td style="text-align: center;">8.95</td>
</tr>
<tr>
<td style="text-align: left;">MSC 2.7B (truncate 1024)</td>
<td style="text-align: center;">8.25</td>
<td style="text-align: center;">8.76</td>
<td style="text-align: center;">8.93</td>
<td style="text-align: center;">9.07</td>
<td style="text-align: center;">9.16</td>
<td style="text-align: center;">8.09</td>
</tr>
<tr>
<td style="text-align: left;">MSC 2.7B (RAG)</td>
<td style="text-align: center;">8.22</td>
<td style="text-align: center;">8.78</td>
<td style="text-align: center;">8.97</td>
<td style="text-align: center;">9.11</td>
<td style="text-align: center;">9.17</td>
<td style="text-align: center;">8.10</td>
</tr>
<tr>
<td style="text-align: left;">MSC 2.7B (FiD)</td>
<td style="text-align: center;">8.22</td>
<td style="text-align: center;">8.75</td>
<td style="text-align: center;">8.92</td>
<td style="text-align: center;">9.05</td>
<td style="text-align: center;">9.11</td>
<td style="text-align: center;">8.06</td>
</tr>
<tr>
<td style="text-align: left;">MSC 2.7B (FiD-RAG)</td>
<td style="text-align: center;">8.23</td>
<td style="text-align: center;">8.75</td>
<td style="text-align: center;">8.93</td>
<td style="text-align: center;">9.04</td>
<td style="text-align: center;">9.11</td>
<td style="text-align: center;">8.03</td>
</tr>
<tr>
<td style="text-align: left;">SumMem-MSC 2.7B (truncate 1024)</td>
<td style="text-align: center;">8.25</td>
<td style="text-align: center;">8.71</td>
<td style="text-align: center;">8.89</td>
<td style="text-align: center;">9.01</td>
<td style="text-align: center;">9.09</td>
<td style="text-align: center;">8.04</td>
</tr>
<tr>
<td style="text-align: left;">SumMem-MSC 2.7B (RAG)</td>
<td style="text-align: center;">8.24</td>
<td style="text-align: center;">8.81</td>
<td style="text-align: center;">9.00</td>
<td style="text-align: center;">9.10</td>
<td style="text-align: center;">9.17</td>
<td style="text-align: center;">8.05</td>
</tr>
<tr>
<td style="text-align: left;">SumMem-MSC 2.7B (FiD)</td>
<td style="text-align: center;">8.20</td>
<td style="text-align: center;">8.71</td>
<td style="text-align: center;">8.89</td>
<td style="text-align: center;">9.00</td>
<td style="text-align: center;">9.07</td>
<td style="text-align: center;">7.91</td>
</tr>
<tr>
<td style="text-align: left;">SumMem-MSC 2.7B (FiD-RAG)</td>
<td style="text-align: center;">8.22</td>
<td style="text-align: center;">8.70</td>
<td style="text-align: center;">8.89</td>
<td style="text-align: center;">9.00</td>
<td style="text-align: center;">9.07</td>
<td style="text-align: center;">7.87</td>
</tr>
</tbody>
</table>
<p>Table 7: Test perplexity across sessions for our retrieval- and memory-augmented models (bottom two blocks) compared to several encoder-decoder baselines (top three rows).
meaning they are truncated less often, which can thus also help performance.</p>
<p>Summary context performance We can ablate the summary model training data to understand its impact further, results of which are given in Table 4. We see that removing the time feature (indicating how long ago the previous session occurred) only has minimal effect. Removing either the partner or self summary (and keeping the other one), on the other hand, has a larger effect in both cases, where keeping the self summary is slightly more important. Keeping both features is best. These differences, as before, are magnified when looking at session opening performance.</p>
<p>Predicted summary models We train models to predict dialogue summaries, and use predicted summaries of previous sessions as context (instead of the full dialogue history or the gold summary). The training data for predicting summaries consists of, for each turn, either a summarizing sentence or the no_summary label. As $42 \%$ of turns have the no_summary label, this can be overexpressed in the model at beam decoding time ${ }^{2}$, we therefore experiment with sampling this label only $K \%$ of the time during training in Table 5. Example predictions (for the 5\% sampling model) are shown in Figure 1. We find that subsampling gives better results and closer sparsity levels to the original human annotated data (e.g., with $K=25 \%$ ). We compare predicted summaries with $K=5 \%$ sampling to other methods of modeling long-context in Table 4. We observe results that are between using a standard dialogue history (predicted summaries are slightly better), and using gold summaries (predicted summaries are not as good).</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Varying the number of training sessions We vary the amount of available training sessions from $1-4$, with results reported in Table 6. We observe large gains when using more than one training session compared to only one (around 1.5 perplexity points), again justifying the construction of our MSC training data. The gains however decrease with the number of available sessions, e.g. between having 1-3 training sessions vs. 1-4 only gives a 0.03 perplexity gain averaged across sessions. The gain even on session 4 is not that large despite the 1-4 training data being in-distribution, whereas 1-3 is not, in addition to 1-4 having more training data.</p>
<p>Retrieval-augmentation model Comparison of our retrieval-augmented methods are given in Table 7, training on MSC using the BST 2.7B model as pre-training, hence called MSC 2.7B (RAG), (FiD) or (FiD-RAG), depending on the augmentation method. These methods are compared to the existing BlenderBot model (BST 2.7B), or training with MSC with no augmentation (MSC 2.7B with different dialogue history context truncation lengths). We find that all three retrieval augmentation methods, when using the session level-document size as retrieval documents, can effectively use retrieval to extend the conversation history length. We see a large performance improvement over the existing BlenderBot model or a truncation of 128 of the MSC 2.7B model. Performance improvements over MSC 2.7B with a truncation length of 1024 are minimal, but the retrieval-augmented models are guaranteed to have a memory that essentially never forgets the conversation, no matter how long it gets, whereas the truncation model does not.</p>
<p>Summary memory model variants We next compare the summary memory models, whereby</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Reference <br> own topic</th>
<th style="text-align: center;">Reference <br> other's topic</th>
<th style="text-align: center;">New <br> topic</th>
<th style="text-align: center;">Engaging <br> Response</th>
<th style="text-align: center;">Final <br> Rating</th>
<th style="text-align: center;"># Annotated <br> Responses</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BST 2.7B (Roller et al., 2020)</td>
<td style="text-align: center;">$19.9 \%$</td>
<td style="text-align: center;">$14.5 \%$</td>
<td style="text-align: center;">$69.0 \%$</td>
<td style="text-align: center;">$53.0 \%$</td>
<td style="text-align: center;">3.14</td>
<td style="text-align: center;">668</td>
</tr>
<tr>
<td style="text-align: left;">MSC 2.7B (truncate 128)</td>
<td style="text-align: center;">$15.8 \%$</td>
<td style="text-align: center;">$21.8 \%$</td>
<td style="text-align: center;">$75.8 \%$</td>
<td style="text-align: center;">$56.5 \%$</td>
<td style="text-align: center;">3.29</td>
<td style="text-align: center;">673</td>
</tr>
<tr>
<td style="text-align: left;">MSC 2.7B (truncate 1024)</td>
<td style="text-align: center;">$15.0 \%$</td>
<td style="text-align: center;">$22.5 \%$</td>
<td style="text-align: center;">$74.4 \%$</td>
<td style="text-align: center;">$54.2 \%$</td>
<td style="text-align: center;">3.47</td>
<td style="text-align: center;">653</td>
</tr>
<tr>
<td style="text-align: left;">SumMem-MSC 2.7B (RAG)</td>
<td style="text-align: center;">$19.6 \%$</td>
<td style="text-align: center;">$33.8 \%$</td>
<td style="text-align: center;">$72.7 \%$</td>
<td style="text-align: center;">$\mathbf{6 2 . 1 \%}$</td>
<td style="text-align: center;">$\mathbf{3 . 6 5}$</td>
<td style="text-align: center;">668</td>
</tr>
<tr>
<td style="text-align: left;">SumMem-MSC 2.7B (FiD)</td>
<td style="text-align: center;">$22.1 \%$</td>
<td style="text-align: center;">$30.7 \%$</td>
<td style="text-align: center;">$76.4 \%$</td>
<td style="text-align: center;">$\mathbf{5 8 . 9 \%}$</td>
<td style="text-align: center;">$\mathbf{3 . 6 2}$</td>
<td style="text-align: center;">662</td>
</tr>
<tr>
<td style="text-align: left;">SumMem-MSC 2.7B (FiD-RAG)</td>
<td style="text-align: center;">$24.2 \%$</td>
<td style="text-align: center;">$26.4 \%$</td>
<td style="text-align: center;">$78.3 \%$</td>
<td style="text-align: center;">$\mathbf{5 9 . 3 \%}$</td>
<td style="text-align: center;">$\mathbf{3 . 6 8}$</td>
<td style="text-align: center;">649</td>
</tr>
</tbody>
</table>
<p>Table 8: Human Evaluation Results. Performance of various models measured during conversations with crowdworkers. Engaging response and final rating numbers in bold are statistically significant compared to BST 2.7B ( $p$-value $&lt;0.05$ ) using a $t$-test. See subsection 5.1 and Appendix B for more details.
previous dialogue history is summarized before being stored in the model's long-term memory, called SumMem-MSC 2.7B. We use the RAG, FiD, or RAG-FiD methods to retrieve from that memory, or we compare to a fixed memory of 1024 tokens that is truncated, resulting in four different methods that we compare. Results are given in Table 7. While improvements are small, we see the same patterns as for the retrieval-augmented methods that SumMem-MSC 2.7B FiD-RAG is better than FiD which is in turn better than RAG, with FiD and FiD-RAG better than truncation at session openings. Moreover, all SumMem-MSC models outperform their retrieval-augmented model counterparts MSC 2.7B (RAG/FiD/FiD-RAG). SumMem-MSC 2.7B (FiD-RAG) thus provides the best results out of all methods tested in this work.</p>
<p>Further Detailed Automatic Metrics Our analysis so far measured perplexity. We report more automatic metrics (F1 and BLEU) in Appendix A, which yield similar conclusions.</p>
<h3>5.1 Human Evaluation</h3>
<p>We perform a human evaluation using crowdworkers. The conversations begin with two randomly chosen personas from the validation set, and one is assigned to the crowdworker who is asked to play that role. We select the conversation to be the $5^{\text {th }}$ session that these two speakers will converse, and make available the summary of the previous 4 sessions. We ask the crowdworkers to have a natural conversation, where they will also evaluate their partner's responses for conversational attributes, in particular whether they reference knowledge of their own or the other speaker's persona (or topics they discussed) from previous sessions, from the current session, or neither. On each turn of the conversation the crowdworker is asked to check all attribute boxes that apply. A screenshot can be found in Figure 6 in the Appendix showing the UI.</p>
<p>Each conversation consists of 15 messages ( 7 from the human, 8 from the bot). At the end of the conversation, an additional question collects an overall engagingness score (out of 5) for their speaking partner.</p>
<p>The results are given in Table 8. We find that MSC-trained models outperform BlenderBot (BST 2.7B) in terms of both per-turn engaging responses and final ratings. Further, our summarization memory models (all three variants RAG, FiD and FiDRAG) outperform encoder-decoders with different levels of truncation of the dialogue history (MSC 2.7B with truncate 128 and 1024). For example, SumMem-MSC 2.7B (RAG) achieves an engaging response rate of $62.1 \%$ and final rating of 3.65 , compared to BlenderBot's $53.0 \%$ and 3.14 and MSC 2.7B (truncate 1024)'s $54.2 \%$ and 3.47. For all MSC models, while rates of referencing their own topics are not particularly increased, we do observe increased rates of referencing partner topics from previous sessions, with higher rates for the summarization memory models. For example, $33.8 \%$ for SumMem-MSC 2.7B (RAG) compared to BlenderBot's $14.5 \%$. This is likely an important reason why human raters feel the summarization memory models are more engaging.</p>
<h2>6 Conclusion</h2>
<p>We have shown that existing dialogue models, both in terms of training data and models trained, fail to conduct long-term conversations adequately. Our work investigates recent model architectures to ameliorate this issue, and collects a new crowdsourced task, Multi-Session Chat to both train and evaluate these models. We show, in terms of both automatic metrics and human evaluations, that these long-context dialogue modeling approaches outperform the previous systems. Future work should investigate further improvements to architectures for the long-context dialogue setting.</p>
<h2>7 Ethical Considerations</h2>
<p>The dialogue models we use in this work utilize large language models, and therefore have similar concerns as in other work, in particular concerns about toxic language, bias and other issues during language generation (Bender et al., 2021). For open-domain dialogue in particular, see Xu et al. (2020); Dinan et al. (2021) for reviews of the literature and evaluation of recent methods that try to mitigate these safety issues.</p>
<p>Our work focuses on models with long-term memory and open-domain conversations wherein speakers may divulge personal interests. We remark that, during data collection, crowdworkers were specifically playing roles with given personality traits, not talking about themselves, and hence not identifying any personal information. During conversations with our trained models, the models will store information they learn from the exchange. In contrast to current standard language models, our models have the capability of storing this in the long-term. This information is stored in the memory of the model, private to the individual's conversation, and hence is not shared with anyone else.</p>
<h2>References</h2>
<p>Daniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et al. 2020. Towards a human-like open-domain chatbot. arXiv preprint arXiv:2001.09977.</p>
<p>Bernard W Agranoff, Roger E Davis, and John J Brink. 1965. Memory fixation in the goldfish. Proceedings of the National Academy of Sciences of the United States of America, 54(3):788.</p>
<p>Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn. 2020. The pushshift reddit dataset. arXiv preprint arXiv:2001.08435.</p>
<p>Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150.</p>
<p>Emily M Bender, Timnit Gebru, Angelina McMillanMajor, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 610-623.</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading wikipedia to answer opendomain questions. In Proceedings of the 55th Annual</p>
<p>Meeting of the Association for Computational Linguistics, pages 1870-1879. Association for Computational Linguistics.</p>
<p>Jianpeng Cheng and Mirella Lapata. 2016. Neural summarization by extracting sentences and words. arXiv preprint arXiv:1603.07252.</p>
<p>Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509.</p>
<p>RichÃ¡rd CsÃ¡ky, Patrik Purgai, and GÃ¡bor Recski. 2019. Improving neural conversational models with entropy-based data filtering. arXiv preprint arXiv:1905.05471.</p>
<p>Richard Csaky and GÃ¡bor Recski. 2020. The gutenberg dialogue dataset. arXiv preprint arXiv:2004.12752.</p>
<p>Emily Dinan, Gavin Abercrombie, A Stevie Bergman, Shannon Spruit, Dirk Hovy, Y-Lan Boureau, and Verena Rieser. 2021. Anticipating safety issues in e2e conversational ai: Framework and tooling. arXiv preprint arXiv:2107.03451.</p>
<p>Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2019. Wizard of Wikipedia: Knowledge-powered conversational agents. In Proceedings of the International Conference on Learning Representations.</p>
<p>Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. 2019. Samsum corpus: A humanannotated dialogue dataset for abstractive summarization. arXiv preprint arXiv:1911.12237.</p>
<p>Chih-Wen Goo and Yun-Nung Chen. 2018. Abstractive dialogue summarization with sentence-gated modeling optimized by dialogue acts. In 2018 IEEE Spoken Language Technology Workshop (SLT), pages 735742. IEEE.</p>
<p>Minlie Huang, Xiaoyan Zhu, and Jianfeng Gao. 2020. Challenges in building intelligent open-domain dialog systems. ACM Transactions on Information Systems (TOIS), 38(3):1-32.</p>
<p>Gautier Izacard and Edouard Grave. 2020. Leveraging passage retrieval with generative models for open domain question answering. arXiv preprint arXiv:2007.01282.</p>
<p>Jeff Johnson, Matthijs Douze, and HervÃ© JÃ©gou. 2019. Billion-scale similarity search with gpus. IEEE Transactions on Big Data.</p>
<p>Khosrow Kaikhah. 2004. Automatic text summarization with neural networks. In 2004 2nd International IEEE Conference on'Intelligent Systems'. Proceedings (IEEE Cat. No. 04EX791), volume 1, pages 40-44. IEEE.</p>
<p>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2019. Reformer: The efficient transformer. In International Conference on Learning Representations.</p>
<p>Wojciech KryÅ›ciÅ„ski, Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Neural text summarization: A critical evaluation. arXiv preprint arXiv:1908.08960.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020a. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel, Sebastian Riedel, and Douwe Kiela. 2020b. Retrieval-augmented generation for knowledgeintensive nlp tasks. In Advances in Neural Information Processing Systems, volume 33, pages 94599474. Curran Associates, Inc.</p>
<p>Jiwei Li, Michel Galley, Chris Brockett, Georgios P Spithourakis, Jianfeng Gao, and Bill Dolan. 2016. A persona-based neural conversation model. arXiv preprint arXiv:1603.06155.</p>
<p>Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. 2017. DailyDialog: A manually labelled multi-turn dialogue dataset. In Proceedings of The 8th International Joint Conference on Natural Language Processing (IJCNLP 2017).</p>
<p>Pierre-Emmanuel MazarÃ©, Samuel Humeau, Martin Raison, and Antoine Bordes. 2018. Training millions of personalized dialogue agents. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2775-2779, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Michael McTear. 2020. Conversational ai: Dialogue systems, conversational agents, and chatbots. Synthesis Lectures on Human Language Technologies, 13(3):1-251.</p>
<p>Haojie Pan, Junpei Zhou, Zhou Zhao, Yan Liu, Deng Cai, and Min Yang. 2018. Dial2desc: end-to-end dialogue description generation. arXiv preprint arXiv:1811.00185.</p>
<p>Gabriele Paolacci, Jesse Chandler, and Panagiotis G Ipeirotis. 2010. Running experiments on amazon mechanical turk. Judgment and Decision making, 5(5):411-419.</p>
<p>Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea. 2018. Meld: A multimodal multi-party dataset for emotion recognition in conversations. arXiv preprint arXiv:1810.02508.</p>
<p>Revanth Rameshkumar and Peter Bailey. 2020. Storytelling with dialogue: A critical role dungeons and dragons dataset. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5121-5134.</p>
<p>Hannah Rashkin, Eric Michael Smith, Margaret Li, and Y-Lan Boureau. 2019. Towards empathetic opendomain conversation models: A new benchmark and dataset. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5370-5381, Florence, Italy. Association for Computational Linguistics.</p>
<p>Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M Smith, et al. 2020. Recipes for building an open-domain chatbot. arXiv preprint arXiv:2004.13637.</p>
<p>Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation. arXiv preprint arXiv:2104.07567.</p>
<p>Jack Urbanek, Angela Fan, Siddharth Karamcheti, Saachi Jain, Samuel Humeau, Emily Dinan, Tim RocktÃ¤schel, Douwe Kiela, Arthur Szlam, and Jason Weston. 2019. Learning to speak and act in a fantasy text adventure game. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 673-683, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Peter West, Ari Holtzman, Jan Buys, and Yejin Choi. 2019. Bottlesum: Unsupervised and self-supervised sentence summarization using the information bottleneck principle. arXiv preprint arXiv:1909.07405.</p>
<p>Jing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason Weston, and Emily Dinan. 2020. Recipes for safety in open-domain chatbots. arXiv preprint arXiv:2010.07079.</p>
<p>Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018. Personalizing dialogue agents: I have a dog, do you have pets too? In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages 2204-2213. ACL.</p>
<h2>A Extra Results</h2>
<p>Further Test Set Metrics We show the test BLEU-1 in Table 11, test BLEU-2 in Table 12, test BLEU-4 in Table 13 and test F1 in Table 14.</p>
<p>Main Validation Results We show the validation perplexity in Table 15 (corresponding to the test perplexity in Table 7).</p>
<h2>B Data Collection \&amp; Data Quality</h2>
<h2>B. 1 Data Collection \&amp; Quality Control</h2>
<p>Crowdsourced Data Collection The data collection lasted for around 6 months and in total over 1000 crowdworkers who are English-speaking annotators located in the United States were recruited and compensated through the Amazon Mechanical Turk platform. Before the data collection starts, all crowdworkers are informed that any message they send may be publicly disclosed for research purposes, and are instructed not to send any personal identifiable information (for example, name, address, email, or phone number etc.) in their messages.</p>
<p>Quality Control To optimize the quality of collected data, we implement a list of quality controls in both the conversation summarization task and the multi-session chat task. All crowdworkers must achieve high scores on the onboarding task that resembles the actual crowdsourcing tasks before they are eligible to work on the Human Intelligence Task (i.e. HIT, the term used by Amazon's Mechanical Turk to refer to a single instance of a crowdworker task). During the actual multi-session chat, crowdworkers are instructed to report at each conversational turn if the message from the other speaker is of poor quality or has BAD behaviors, for example, contradicting to or repeating what has been mentioned; changing topics too often, etc. A final rating is also collected at the end of each chat indicating how much they enjoy talking to their conversational partner. Crowdworkers that are frequently reported as producing messages of low quality or</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Split</th>
<th style="text-align: center;">Total</th>
<th style="text-align: center;">Unique</th>
<th style="text-align: center;">Unique\%</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">105429</td>
<td style="text-align: center;">105549</td>
<td style="text-align: center;">$99.88 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">17687</td>
<td style="text-align: center;">17691</td>
<td style="text-align: center;">$99.97 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">17798</td>
<td style="text-align: center;">17803</td>
<td style="text-align: center;">$99.97 \%$</td>
</tr>
</tbody>
</table>
<p>Table 9: Utterance overlap by data split.
receiving low ratings are blocked from working on any future HITs. Moreover, dialogues that fail the acceptability checks such as minimum average message length or have too many dialogue turns rated as low quality are also filtered out from the final dataset.</p>
<p>We show screenshots of the crowdsourced MultiSession Chat task in Figure 5 as well as the crowdsourced human evaluation task in Figure 6.</p>
<h2>B. 2 Data Quality Analysis</h2>
<p>Following analysis of previous datasets such as DailyDialogue that exposed significant overlap between train and test (Csaky and Recski, 2020; CsÃ¡ky et al., 2019), we measure various overlap statistics on our newly collected dataset.</p>
<p>Table 9 gives statistics for the number of unique utterances in a given data split (comparing to all other utterances in the same data split). We see there are very few duplicated messages across crowdworkers. For example in the validation set there are only 4 , these are: "What kind of dogs do you have ?", "How old are your children ?", "What kind of dancing do you do ?" and "What kind of dog is he ?", i.e. very common questions that just happened to be asked twice each across different conversations.</p>
<p>Table 10 gives statistics for the number of unique utterances across data splits (comparing either valid or test utterances to all utterances in the train data split). We again see there are very few duplicated messages across data splits, looking at these specific messages we again see they are typical things that might be commonly asked.</p>
<h2>C Dataset Examples</h2>
<p>MSC Dataset Examples We show two MSC dialogue examples in Figure 2 and Figure 3 each consist of four sessions. We also show example summary annotations in Figure 1.</p>
<p>Session Opening Examples We show example session opening predictions of a model trained on gold summaries in Figure 4.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Split vs. Split</th>
<th style="text-align: center;">Overlap</th>
<th style="text-align: center;">Overlap\%</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Train vs. Valid</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">$0.135 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Train vs. Test</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">$0.252 \%$</td>
</tr>
</tbody>
</table>
<p>Table 10: Utterance overlap across data splits.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Session 1</th>
<th style="text-align: center;">Session 2</th>
<th style="text-align: center;">Session 3</th>
<th style="text-align: center;">Session 4</th>
<th style="text-align: center;">Session 5</th>
<th style="text-align: center;">Session Openings</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BST 2.7B (Roller et al., 2020)</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">13.8</td>
<td style="text-align: center;">13.8</td>
<td style="text-align: center;">10.9</td>
</tr>
<tr>
<td style="text-align: left;">MSC 2.7B (truncate 1024)</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">11.3</td>
</tr>
<tr>
<td style="text-align: left;">SumMem-MSC 2.7B (RAG)</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">14.9</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">11.5</td>
</tr>
<tr>
<td style="text-align: left;">SumMem-MSC 2.7B (FiD-RAG)</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">15.7</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">11.6</td>
</tr>
</tbody>
</table>
<p>Table 11: Test BLEU-1 across sessions for our memory-augmented models compared to several encoder-decoder baselines (top two rows).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Session 1</th>
<th style="text-align: center;">Session 2</th>
<th style="text-align: center;">Session 3</th>
<th style="text-align: center;">Session 4</th>
<th style="text-align: center;">Session 5</th>
<th style="text-align: center;">Session Openings</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BST 2.7B (Roller et al., 2020)</td>
<td style="text-align: center;">5.43</td>
<td style="text-align: center;">4.84</td>
<td style="text-align: center;">4.39</td>
<td style="text-align: center;">4.19</td>
<td style="text-align: center;">4.31</td>
<td style="text-align: center;">1.79</td>
</tr>
<tr>
<td style="text-align: left;">MSC 2.7B (truncate 1024)</td>
<td style="text-align: center;">5.53</td>
<td style="text-align: center;">5.26</td>
<td style="text-align: center;">4.91</td>
<td style="text-align: center;">4.69</td>
<td style="text-align: center;">4.75</td>
<td style="text-align: center;">2.18</td>
</tr>
<tr>
<td style="text-align: left;">SumMem-MSC 2.7B (RAG)</td>
<td style="text-align: center;">5.37</td>
<td style="text-align: center;">5.26</td>
<td style="text-align: center;">4.96</td>
<td style="text-align: center;">4.89</td>
<td style="text-align: center;">4.85</td>
<td style="text-align: center;">2.45</td>
</tr>
<tr>
<td style="text-align: left;">SumMem-MSC 2.7B (FiD-RAG)</td>
<td style="text-align: center;">5.42</td>
<td style="text-align: center;">5.26</td>
<td style="text-align: center;">5.13</td>
<td style="text-align: center;">5.02</td>
<td style="text-align: center;">5.00</td>
<td style="text-align: center;">2.42</td>
</tr>
</tbody>
</table>
<p>Table 12: Test BLEU-2 across sessions for our memory-augmented models compared to several encoder-decoder baselines (top two rows).</p>
<h1>D Model Training Settings</h1>
<p>We use the openly available ParlAI framework for all training runs, as well as for evaluations, where metrics are measured using default settings. All the fine-tuned models are trained with a maximum of eight 32GB GPUs (NVIDIA V100), optimized with Adam using $\beta_{1}=0.9, \beta_{2}=0.999, \epsilon=$ $1 e-08$. Models are trained up to 4000 updates with batch size up to 128 . The typical fine-tuning time for standard transformer encoder-decoder is 8 hrs before it early stops, and for retrieval-based model is 16 hrs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Session 1</th>
<th style="text-align: center;">Session 2</th>
<th style="text-align: center;">Session 3</th>
<th style="text-align: center;">Session 4</th>
<th style="text-align: center;">Session 5</th>
<th style="text-align: center;">Session Openings</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BST 2.7B (Roller et al., 2020)</td>
<td style="text-align: center;">0.858</td>
<td style="text-align: center;">0.625</td>
<td style="text-align: center;">0.535</td>
<td style="text-align: center;">0.551</td>
<td style="text-align: center;">0.570</td>
<td style="text-align: center;">0.107</td>
</tr>
<tr>
<td style="text-align: left;">MSC 2.7B (truncate 1024)</td>
<td style="text-align: center;">0.837</td>
<td style="text-align: center;">0.745</td>
<td style="text-align: center;">0.629</td>
<td style="text-align: center;">0.595</td>
<td style="text-align: center;">0.631</td>
<td style="text-align: center;">0.139</td>
</tr>
<tr>
<td style="text-align: left;">SumMem-MSC 2.7B (RAG)</td>
<td style="text-align: center;">0.919</td>
<td style="text-align: center;">0.699</td>
<td style="text-align: center;">0.677</td>
<td style="text-align: center;">0.664</td>
<td style="text-align: center;">0.656</td>
<td style="text-align: center;">0.228</td>
</tr>
<tr>
<td style="text-align: left;">SumMem-MSC 2.7B (FiD-RAG)</td>
<td style="text-align: center;">0.853</td>
<td style="text-align: center;">0.765</td>
<td style="text-align: center;">0.673</td>
<td style="text-align: center;">0.679</td>
<td style="text-align: center;">0.678</td>
<td style="text-align: center;">0.222</td>
</tr>
</tbody>
</table>
<p>Table 13: Test BLEU-4 across sessions for our memory-augmented models compared to several encoder-decoder baselines (top two rows).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Session 1</th>
<th style="text-align: center;">Session 2</th>
<th style="text-align: center;">Session 3</th>
<th style="text-align: center;">Session 4</th>
<th style="text-align: center;">Session 5</th>
<th style="text-align: center;">Session Openings</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BST 2.7B (Roller et al., 2020)</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">19.4</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">19.4</td>
<td style="text-align: center;">13.7</td>
</tr>
<tr>
<td style="text-align: left;">MSC 2.7B (truncate 1024)</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">14.1</td>
</tr>
<tr>
<td style="text-align: left;">SumMem-MSC 2.7B (RAG)</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">14.4</td>
</tr>
<tr>
<td style="text-align: left;">SumMem-MSC 2.7B (FiD-RAG)</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">14.5</td>
</tr>
</tbody>
</table>
<p>Table 14: Test F1 across sessions for our memory-augmented models compared to several encoder-decoder baselines (top two rows).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Session 1</th>
<th style="text-align: center;">Session 2</th>
<th style="text-align: center;">Session 3</th>
<th style="text-align: center;">Session 4</th>
<th style="text-align: center;">Session 5</th>
<th style="text-align: center;">Session Openings</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BST 2.7B (Roller et al., 2020)</td>
<td style="text-align: center;">8.84</td>
<td style="text-align: center;">10.56</td>
<td style="text-align: center;">10.44</td>
<td style="text-align: center;">10.51</td>
<td style="text-align: center;">10.44</td>
<td style="text-align: center;">13.04</td>
</tr>
<tr>
<td style="text-align: left;">MSC 2.7B (truncate 128)</td>
<td style="text-align: center;">8.75</td>
<td style="text-align: center;">9.32</td>
<td style="text-align: center;">9.22</td>
<td style="text-align: center;">9.32</td>
<td style="text-align: center;">9.23</td>
<td style="text-align: center;">8.95</td>
</tr>
<tr>
<td style="text-align: left;">MSC 2.7B (truncate 1024)</td>
<td style="text-align: center;">8.17</td>
<td style="text-align: center;">9.18</td>
<td style="text-align: center;">9.05</td>
<td style="text-align: center;">9.16</td>
<td style="text-align: center;">9.08</td>
<td style="text-align: center;">8.06</td>
</tr>
<tr>
<td style="text-align: left;">MSC 2.7B (RAG)</td>
<td style="text-align: center;">8.14</td>
<td style="text-align: center;">9.16</td>
<td style="text-align: center;">9.06</td>
<td style="text-align: center;">9.18</td>
<td style="text-align: center;">9.10</td>
<td style="text-align: center;">8.04</td>
</tr>
<tr>
<td style="text-align: left;">MSC 2.7B (FiD)</td>
<td style="text-align: center;">8.16</td>
<td style="text-align: center;">9.14</td>
<td style="text-align: center;">9.02</td>
<td style="text-align: center;">9.10</td>
<td style="text-align: center;">9.04</td>
<td style="text-align: center;">7.97</td>
</tr>
<tr>
<td style="text-align: left;">MSC 2.7B (FiD-RAG)</td>
<td style="text-align: center;">8.16</td>
<td style="text-align: center;">9.13</td>
<td style="text-align: center;">9.02</td>
<td style="text-align: center;">9.10</td>
<td style="text-align: center;">9.04</td>
<td style="text-align: center;">7.96</td>
</tr>
<tr>
<td style="text-align: left;">SumMem-MSC 2.7B (truncate 1024)</td>
<td style="text-align: center;">8.18</td>
<td style="text-align: center;">9.11</td>
<td style="text-align: center;">8.98</td>
<td style="text-align: center;">9.07</td>
<td style="text-align: center;">9.00</td>
<td style="text-align: center;">7.97</td>
</tr>
<tr>
<td style="text-align: left;">SumMem-MSC 2.7B (RAG)</td>
<td style="text-align: center;">8.16</td>
<td style="text-align: center;">9.19</td>
<td style="text-align: center;">9.07</td>
<td style="text-align: center;">9.17</td>
<td style="text-align: center;">9.09</td>
<td style="text-align: center;">7.95</td>
</tr>
<tr>
<td style="text-align: left;">SumMem-MSC 2.7B (FiD)</td>
<td style="text-align: center;">8.16</td>
<td style="text-align: center;">9.09</td>
<td style="text-align: center;">8.97</td>
<td style="text-align: center;">9.07</td>
<td style="text-align: center;">8.99</td>
<td style="text-align: center;">7.82</td>
</tr>
<tr>
<td style="text-align: left;">SumMem-MSC 2.7B (FiD-RAG)</td>
<td style="text-align: center;">8.16</td>
<td style="text-align: center;">9.08</td>
<td style="text-align: center;">8.96</td>
<td style="text-align: center;">9.07</td>
<td style="text-align: center;">8.99</td>
<td style="text-align: center;">7.78</td>
</tr>
</tbody>
</table>
<p>Table 15: Valid perplexity across sessions for our retrieval- and memory-augmented models (bottom two blocks) compared to several encoder-decoder baselines (top three rows).</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Example summary annotations and predictions on the validation set. We show the gold human annotation (label) and our model prediction (model).</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Example four session conversation from the newly collected Multi-Session Chat dataset. New sessions refer back to previous subjects, explore them in depth, 5194</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Example four session conversation from the newly collected Multi-Session Chat dataset. New sessions refer back to previous subjects, explore them in depth, or spark up conversation on new topics.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Example opening annotations and predictions given gold summaries on the validation set. We show the gold human annotation (label) and our model prediction (model).</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Screenshot from the crowdworker multi-session chat. The left panel shows the instructions as well as all dialogue history from previous sessions, and the right panel contains the conversation for the current session.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Crowdworker evaluation task screenshots. The left panel shows the instructions, and the right panel contains the conversation. In the human evaluation results in the main paper the "what THEY mentioned last time" binary checkbox is converted to a percentage over all annotated responses, and termed "Reference own topic" in Table 8. Similarly, "what YOU mentioned last time" is termed "Reference other's topic" in Table 8.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ We use a beam size of 3 and minimum beam length 10 with no context blocking.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{1}$ Dataset, model weights and code for this entire project will be made available upon acceptance.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
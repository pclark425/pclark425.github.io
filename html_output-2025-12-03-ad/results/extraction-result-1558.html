<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1558 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1558</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1558</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-30.html">extraction-schema-30</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-11e8cee777cd326afb75471d60efe3b1241cdba8</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/11e8cee777cd326afb75471d60efe3b1241cdba8" target="_blank">CommAI: Evaluating the first steps towards a useful general AI</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> A set of concrete Desiderata for general AI, together with a platform to test machines on how well they satisfy such desiderata, while keeping all further complexities to a minimum is proposed.</p>
                <p><strong>Paper Abstract:</strong> With machine learning successfully applied to new daunting problems almost every day, general AI starts looking like an attainable goal. However, most current research focuses instead on important but narrow applications, such as image classification or machine translation. We believe this to be largely due to the lack of objective ways to measure progress towards broad machine intelligence. In order to fill this gap, we propose here a set of concrete desiderata for general AI, together with a platform to test machines on how well they satisfy such desiderata, while keeping all further complexities to a minimum.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1558.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1558.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CommAI-mini curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CommAI-mini level-based curriculum (within the CommAI-env framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A level-based, incrementally-constructed curriculum of interactive text tasks (CommAI-mini) that progresses from simple strictly-local string recognition to locally-testable languages and production tasks, intended to encourage compositional learning-to-learn via increasing operators and compositional complexity. The curriculum is implemented as task sets/levels within the CommAI-env platform, with randomized ordering within levels and increasing syntactic/semantic complexity across levels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>unspecified learner (no empirical agent evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>No specific agent architecture or model is evaluated in this paper; the paper describes the task/curriculum design for future learners and outlines desiderata for learners (e.g., compositional memory, chunking, parsing, ability to learn from linguistic feedback).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>CommAI-env / CommAI-mini</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A bit-level interactive text environment where at each episode the environment sends a textual description of a target stringset (in simplified English), indicates whether the task is recognition or production, supplies example strings (for recognition) and gives linguistic feedback (including correct answers or example correct strings) and optional reward. Communication is constrained to a bit-stream interface; tasks are organized into task sets (levels) with randomized ordering within a level.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>syntactic stringset procedures (recognition/production); framework stated as extensible to simple physics/commonsense procedures via textual instructions</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Examples in CommAI-mini: 'description: C; verify: CCCC.' (recognize), 'description: AB or CD; produce.' (generate string matching description), and 'description: C; produce two distinct strings.'; the paper also mentions possible extensions like 'move the red block over the blue block' as illustrative simple-physics/commonsense instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Explicitly compositional: tasks build on reusable subskills such as chunking bit sequences into characters, parsing instruction structure (description/verify/produce), storing and recombining n-gram lookup tables, and transferring skills between recognition and production; task complexity arises from combining primitive operators (ngram presence, disjunction/or, conjunction/and, negation/not, 'anything' wildcard) and increasing n-gram length and number of terms.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>level-based incremental curriculum (task-set levels)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Tasks are grouped into discrete sets/levels (e.g., Set #1..#5). Level progression increases compositional complexity by (a) increasing maximum n-gram length and memory demands (strictly-local → longer n-grams), (b) adding logical operators in descriptions (or → and → not), (c) moving from recognition to production tasks and adding production variants (e.g., produce two distinct strings), and (d) varying the number of terms in descriptions; within each level tasks are presented in random order, while the curriculum advances by exposing learners to increasingly complex levels.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>compositional complexity / increasing operators and memory requirements (i.e., task complexity hierarchy from strictly-local n-grams → disjunctions → conjunctions → negation → production and multi-output requirements)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>From strictly-local unigram/bigram recognition (e.g., (AB)+, single n-gram presence) up to locally-testable languages requiring conjunction and negation over multiple n-grams and production tasks including multi-output (e.g., 'produce two distinct strings'); n-gram lengths and number of terms are parameters that scale complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>Described as a core desideratum and design goal rather than empirically measured: authors argue a compositional learner should transfer across tasks (e.g., recognition skills bootstrap production tasks and vice versa; solving disjunctions should help with conjunctions when recombining learned n-grams). No empirical transfer numbers are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Design-level findings and hypotheses only: (1) organizing tasks into incremental levels that add operators and increase n-gram memory demands should promote compositional learning-to-learn; (2) linguistic instruction and feedback (including decreasing explicit reward over time) are central to teaching new skills efficiently; (3) recognition and production counterparts are expected to mutually facilitate learning via compositional reuse; (4) the authors conjecture that current ML methods will struggle to solve CommAI-mini without developing genuinely compositional learners, but provide no experimental evidence or quantitative results in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CommAI: Evaluating the first steps towards a useful general AI', 'publication_date_yy_mm': '2017-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A roadmpap towards machine intelligence. <em>(Rating: 2)</em></li>
                <li>Modular multitask reinforcement learning with policy sketches <em>(Rating: 2)</em></li>
                <li>Towards AI-complete question answering: A set of prerequisite toy tasks <em>(Rating: 2)</em></li>
                <li>Dialog-based language learning <em>(Rating: 1)</em></li>
                <li>Building machines that learn and think like people <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1558",
    "paper_id": "paper-11e8cee777cd326afb75471d60efe3b1241cdba8",
    "extraction_schema_id": "extraction-schema-30",
    "extracted_data": [
        {
            "name_short": "CommAI-mini curriculum",
            "name_full": "CommAI-mini level-based curriculum (within the CommAI-env framework)",
            "brief_description": "A level-based, incrementally-constructed curriculum of interactive text tasks (CommAI-mini) that progresses from simple strictly-local string recognition to locally-testable languages and production tasks, intended to encourage compositional learning-to-learn via increasing operators and compositional complexity. The curriculum is implemented as task sets/levels within the CommAI-env platform, with randomized ordering within levels and increasing syntactic/semantic complexity across levels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "unspecified learner (no empirical agent evaluated)",
            "agent_description": "No specific agent architecture or model is evaluated in this paper; the paper describes the task/curriculum design for future learners and outlines desiderata for learners (e.g., compositional memory, chunking, parsing, ability to learn from linguistic feedback).",
            "agent_size": null,
            "environment_name": "CommAI-env / CommAI-mini",
            "environment_description": "A bit-level interactive text environment where at each episode the environment sends a textual description of a target stringset (in simplified English), indicates whether the task is recognition or production, supplies example strings (for recognition) and gives linguistic feedback (including correct answers or example correct strings) and optional reward. Communication is constrained to a bit-stream interface; tasks are organized into task sets (levels) with randomized ordering within a level.",
            "procedure_type": "syntactic stringset procedures (recognition/production); framework stated as extensible to simple physics/commonsense procedures via textual instructions",
            "procedure_examples": "Examples in CommAI-mini: 'description: C; verify: CCCC.' (recognize), 'description: AB or CD; produce.' (generate string matching description), and 'description: C; produce two distinct strings.'; the paper also mentions possible extensions like 'move the red block over the blue block' as illustrative simple-physics/commonsense instructions.",
            "compositional_structure": "Explicitly compositional: tasks build on reusable subskills such as chunking bit sequences into characters, parsing instruction structure (description/verify/produce), storing and recombining n-gram lookup tables, and transferring skills between recognition and production; task complexity arises from combining primitive operators (ngram presence, disjunction/or, conjunction/and, negation/not, 'anything' wildcard) and increasing n-gram length and number of terms.",
            "uses_curriculum": true,
            "curriculum_name": "level-based incremental curriculum (task-set levels)",
            "curriculum_description": "Tasks are grouped into discrete sets/levels (e.g., Set #1..#5). Level progression increases compositional complexity by (a) increasing maximum n-gram length and memory demands (strictly-local → longer n-grams), (b) adding logical operators in descriptions (or → and → not), (c) moving from recognition to production tasks and adding production variants (e.g., produce two distinct strings), and (d) varying the number of terms in descriptions; within each level tasks are presented in random order, while the curriculum advances by exposing learners to increasingly complex levels.",
            "curriculum_ordering_principle": "compositional complexity / increasing operators and memory requirements (i.e., task complexity hierarchy from strictly-local n-grams → disjunctions → conjunctions → negation → production and multi-output requirements)",
            "task_complexity_range": "From strictly-local unigram/bigram recognition (e.g., (AB)+, single n-gram presence) up to locally-testable languages requiring conjunction and negation over multiple n-grams and production tasks including multi-output (e.g., 'produce two distinct strings'); n-gram lengths and number of terms are parameters that scale complexity.",
            "performance_with_curriculum": null,
            "performance_without_curriculum": null,
            "has_curriculum_comparison": false,
            "alternative_curriculum_performance": null,
            "transfer_generalization": "Described as a core desideratum and design goal rather than empirically measured: authors argue a compositional learner should transfer across tasks (e.g., recognition skills bootstrap production tasks and vice versa; solving disjunctions should help with conjunctions when recombining learned n-grams). No empirical transfer numbers are reported.",
            "key_findings": "Design-level findings and hypotheses only: (1) organizing tasks into incremental levels that add operators and increase n-gram memory demands should promote compositional learning-to-learn; (2) linguistic instruction and feedback (including decreasing explicit reward over time) are central to teaching new skills efficiently; (3) recognition and production counterparts are expected to mutually facilitate learning via compositional reuse; (4) the authors conjecture that current ML methods will struggle to solve CommAI-mini without developing genuinely compositional learners, but provide no experimental evidence or quantitative results in this paper.",
            "uuid": "e1558.0",
            "source_info": {
                "paper_title": "CommAI: Evaluating the first steps towards a useful general AI",
                "publication_date_yy_mm": "2017-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A roadmpap towards machine intelligence.",
            "rating": 2
        },
        {
            "paper_title": "Modular multitask reinforcement learning with policy sketches",
            "rating": 2
        },
        {
            "paper_title": "Towards AI-complete question answering: A set of prerequisite toy tasks",
            "rating": 2
        },
        {
            "paper_title": "Dialog-based language learning",
            "rating": 1
        },
        {
            "paper_title": "Building machines that learn and think like people",
            "rating": 1
        }
    ],
    "cost": 0.00722175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>COMMAI: EVALUATING THE FIRST STEPS TOWARDS A USEFUL GENERAL AI</h1>
<p>Marco Baroni, Armand Joulin, Allan Jabri, Germán Kruszewski, Angeliki Lazaridou, ${ }^{*}$ Klemen Simonic \&amp; Tomas Mikolov<br>Facebook Artificial Intelligence Research<br>{mbaroni, ajoulin, ajabri, germank, angelikil, klemen, tmikolov}@fb.com</p>
<h4>Abstract</h4>
<p>With machine learning successfully applied to new daunting problems almost every day, general AI starts looking like an attainable goal (LeCun et al., 2015). However, most current research focuses instead on very specific applications, such as image classification or machine translation. We believe this to be largely due to the lack of objective ways to measure progress towards broad machine intelligence. In order to fill this gap, we propose here a set of concrete desiderata for general AI, together with a platform to test machines on how well they satisfy such desiderata, while keeping all further complexities to a minimum.</p>
<h2>1 DESIDERATA FOR THE EVALUATION OF MACHINE INTELLIGENCE</h2>
<p>Rather than trying to define intelligence in abstract terms, we take a pragmatic approach: we would like to develop AIs that are useful for us. This naturally leads to the following desiderata.</p>
<p>Communication through natural language An AI will be useful to us only if we are able to communicate with it: assigning it tasks, understanding the information it returns, and teaching it new skills. Since natural language is by far the easiest way for us to communicate, we require our useful AI to be endowed with basic linguistic abilities. The language the machine is exposed to in the testing environment will inevitably be very limited. However, given that we want the machine to also be a powerful, fast learner (see next point), humans should later be able to teach it more sophisticated language skills as they become important to instruct the machine in new domains. In concrete, the environment should not only expose the machine to a set of tasks, but provide instructions and feedback about the tasks in simple natural language. The machine should rely on this form of linguistic interaction to efficiently solve the tasks.</p>
<p>Learning to learn A useful AI should be flexible. As our needs change, the AI should help us with the new challenges we face: from solving a scientific problem in the morning at work to stocking our fridge at night. Progress towards AI should thus be measured on the ability to master a continuous flow of new tasks, with data-efficiency in solving new tasks as a fundamental evaluation component, and without distinguishing train and test phases. We must distinguish this learning to learn ability, pertaining to generalization across tasks (Ring, 1997; Schmidhuber, 2015; Silver et al., 2013; Thrun \&amp; Pratt, 1997), from 1-shot learning, that is, the challenging but more limited ability to generalize to new classes within the same task (e.g., extending an object classifier to recognize unseen objects from just a few examples; Lake et al., 2015). It's generally agreed that, in order to generalize across tasks, a program should be capable of compositional learning, that is, of storing and re-combining solutions to sub-problems across tasks (Fodor \&amp; Lepore, 2002; Lake et al., 2016; Minsky, 1986). The testing environment should thus feature sets of related tasks, such that a compositional learner can bootstrap skills from one task to the other. Finally, mastering language skills might be a crucial component of learning to learn, since understanding linguistic instructions allows us to quickly learn how to accomplish tasks we have never performed before.</p>
<p>Feedback As we grow up, we learn to master complex tasks with decreasing amounts of explicit reward. A useful AI should possess similar capabilities. Consequently, in our testing environment,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>reward should decrease with time. Conversely, the machine should be able to learn from performance cues that are not directly linked to an explicit reward score, such as purely linguistic feedback (see also Weston, 2016), or observing other agents that are correctly performing a task (as in learning by demonstration, Argall et al., 2009). The testing environment should include such cues.</p>
<p>Interface The interface between the machine and the world should be maximally general. The machine itself should learn the best way to process different kinds of input and output streams, with no need for manual re-programming as we apply it to different domains. We thus assume the simplest possible interface. At each time step, the machine receives one bit and sends one bit, without any further structure imposed on the bit stream (a separate channel is used for reward in the initial stages of the simulation).</p>
<p>We do not claim that satisfying our desiderata will lead to a full-fledged intelligent machine, but we see them as prerequisites to be able to efficiently teach it more advanced skills.</p>
<h1>2 THE COMMAI FRAMEWORK</h1>
<p>We call the evaluation framework satisfying the desiderata above CommAI (communication-based AI), given the prominence we give to communication skills. We have developed the open source CommAI-env platform ${ }^{1}$ to implement sets of CommAI tasks. As a concrete example of a set of simple tasks already satisfying many of the requirements above, we briefly present here the CommAImini tasks (described in more detail in the Supplementary Materials).</p>
<p>In a CommAI-mini task, the environment presents a (simplified) regular expression to the learner. It then asks it to either recognize or produce a string matching the expression. The environment listens to the learner response and it provides linguistic feedback on the learner's performance (possibly assigning reward). All exchanges take place at the bit level. Some examples follow:</p>
<p>Environment:
description: C or D; verify: CCCC.
wrong; correct: true.
description: HL and RM and BT; verify:
RMBTBTHLHLBT.
right. ( +1 reward)
description: not AB; verify:
ADFCFHGHADDDB.
description: C; produce.
wrong; example correct: CCC.
description: C; produce two distinct strings.</p>
<p>Learning-to-learn is a must, since the learner will rarely, if ever, be tested on exactly the same target grammar, grammars become more complex with time, and the learner is asked to use the same grammar in different ways (e.g., for recognition or production). Compositionality plays an important role at multiple levels: (i) Skills such as chunking bit sequences into characters and parsing messages into predictable parts (the description, the test string, the delimiters, etc.) will greatly help the learner to generalize across tasks. (ii) Succeeding at the recognition tasks should help in solving the equivalent production tasks (and vice versa). (iii) We control moreover the complexity of the stringsets and their descriptions, by incrementally adding operators to the regular expressions.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>For example, checking if a string contains all n-grams in a set requires checking the presence of the n-grams in the string, so a compositional learner should be faster at a task involving n-gram conjunction after it has solved the task disjunction tasks.</p>
<p>The tasks are different from standard artificial grammar learning (Reber, 1967), in that the learner is given explicit instructions in simplified English about the target stringset and what to do with it (description:, verify:, produce...), as well as verbal feedback on its performance. We thus satisfy our linguistic communication desideratum.</p>
<p>Importantly, although the CommAI-mini tasks are fully "linguistic", in the sense that they pertain to character string recognition and production, we chose the regular grammar domain just because it's simple and well-understood (incidentally, one could also think of the test strings as denoting non-verbal acoustic or visual stimulus sequences). What satisfies the language desideratum is not the nature of the tasks, but the fact that the environment provides meta-information about them (instructions, feedback) in simplified English. Other CommAI task sets could, for example, be based on simple physics tasks, where sensory information would be passed through the bit-based channel, together with instructions and feedback that would still be expressed in simplified English (e.g., move the red block over the blue block; see Andreas et al., 2016 for somewhat related ideas).</p>
<p>Despite their simplicity, we conjecture that solving the CommAI-mini tasks without astronomical amounts of training examples is out of the scope of current machine learning methods (more advanced task examples can be found on the CommAI-env site). We hope the CommAI-mini challenge is at the right level of complexity to stimulate researchers to develop genuinely new models.</p>
<h1>3 RELATED WORK</h1>
<p>We can identify two broad approaches to benchmarking general AI. Some researchers, like us, take a top-down view, deriving their requirements from psychological or mathematical considerations (for example, Adams et al., 2012, Lake et al., 2016, and see the extensive review in Hernández-Orallo, 2017). We sympathize with this principled approach, but we are not aware of others having emphasized the same set of practical desiderata that we outlined above, nor proposing a concrete framework for evaluation like we do with CommAI-env.</p>
<p>Others focus on existing applications that are considered of sufficient complexity to measure progress towards general-purpose intelligence. For example, games such as Go (Silver et al., 2016) and StarCraft (Ontañón et al., 2013) require sophisticated planning skills intuitively associated with intelligence. While current results in these domains are impressive, we think this approach is at the same time too simple and too complex as a general AI benchmark. On the one hand, the focus shifts from domain-independent skills to more limited game-specific strategies. On the other, raw input pre-processing and adapting to game-specific dynamics might require heavy computational resources and advanced domain-specific know-how, with high entry cost for researchers that are not already working in the target domains. These issues are partially addressed by platforms that provide a unified interface to multiple games and other programs. ${ }^{2}$ However, simply pooling a large number of existing applications will make for a ragtag collection of benchmarks, with no clear unified goal in terms of evaluating general intelligence.</p>
<p>The bAbI tasks (Weston et al., 2015) are superficially similar to CommAI tasks, but they evaluate general text understanding phenomena, rather than compositional learning-to-learn abilities.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>This abstract summarizes and refines ideas we originally presented in an unpublished manuscript (Mikolov et al., 2016). We thank Gemma Boleda, Stan Dehaene, Emmanuel Dupoux, Jan Feyereisl, Amaç Herdağdelen, José Hernández-Orallo, Iasonas Kokkinos, Martin Poliak, Marek Rosa, our FAIR colleagues and the participants of the MAIN@NIPS 2016 workshop for feedback.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>REFERENCES</h1>
<p>Sam Adams, Itamar Arel, Joscha Bach, Robert Coop, Rod Furlan, Ben Goertzel, Storrs Hall, Alexei Samsonovich, Matthias Scheutz, Matthew Schlesinger, Stuart Shapiro, and John Sowa. Mapping the landscape of human-level artificial general intelligence. AI Magazine, 33(1):25-41, 2012.</p>
<p>Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with policy sketches. https://arxiv.org/abs/1611.01796, 2016.</p>
<p>Brenna Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning from demonstration. Robotics and Autonomous Systems, 57(5):469-483, 2009.</p>
<p>Jerry Fodor and Ernest Lepore. The Compositionality Papers. Oxford University Press, Oxford, UK, 2002.</p>
<p>José Hernández-Orallo. The Measure of All Minds. Cambridge University Press, Cambridge, UK, 2017.</p>
<p>Gerhard Jäger and James Rogers. Formal language theory: refining the Chomsky hierarchy. Philosophical Transactions of the Royal Society of London B: Biological Sciences, 367(1598):19561970, 2012.</p>
<p>Brenden Lake, Ruslan Salakhutdinov, and Joshua Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332-1338, 2015.</p>
<p>Brenden Lake, Tomer Ullman, Joshua Tenenbaum, and Samuel Gershman. Building machines that learn and think like people. https://arxiv.org/abs/1604.00289, 2016.</p>
<p>Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521:436-444, 2015.
Robert McNaughton and Seymour Papert. Counter-Free Automata. MIT Press, Cambridge, MA, 1971.</p>
<p>Tomas Mikolov, Armand Joulin, and Marco Baroni. A roadmpap towards machine intelligence. http://arxiv.org/abs/1511.08130/, 2016.</p>
<p>Marvin Minsky. The Society of Mind. Simon \&amp; Schuster, New York, 1986.
Santiago Ontañón, Gabriel Synnaeve, Alberto Uriarte, Florian Richoux, David Churchill, and Mike Preuss. A survey of real-time strategy game AI research and competition in StarCraft. IEEE Transactions on Computational Intelligence and AI in Games, 5(4):293-311, 2013.</p>
<p>Arthur Reber. Implicit learning of artificial grammars. Verbal Learning and Verbal Behavior, 5(6): 855-863, 1967.</p>
<p>Mark Ring. CHILD: A first step towards continual learning. Machine Learning, 28:77-104, 1997.
James Rogers, Jeffrey Heinz, Margaret Fero, Jeremy Hurst, Dakotah Lambert, and Sean Wibel. Cognitive and sub-regular complexity. In Glyn Morrill and Mark-Jan Nederhof (eds.), Formal Grammar: 17th and 18th International Conferences, pp. 90-108. Springer, Berlin, Germany, 2013.</p>
<p>Jürgen Schmidhuber. On learning to think: Algorithmic information theory for novel combinations of reinforcement learning controllers and recurrent neural world models. http://arxiv.org/abs/1511.09249, 2015.</p>
<p>Daniel Silver, Qiang Yang, and Lianghao Li. Lifelong machine learning systems: Beyond learning algorithms. In Proceedings of the AAAI Spring Symposium on Lifelong Machine Learning, pp. 49-55, Stanford, CA, 2013.</p>
<p>David Silver, Aja Huang, Christopher Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 529:484-503, 2016.</p>
<p>Sebastian Thrun and Lorien Pratt (eds.). Learning to Learn. Kluwer, Dordrecht, 1997.
Jason Weston. Dialog-based language learning. In Proceedings of NIPS, pp. 829-837, Barcelona, Spain, 2016.</p>
<p>Jason Weston, Antoine Bordes, Sumit Chopra, and Tomas Mikolov. Towards AI-complete question answering: A set of prerequisite toy tasks. http://arxiv.org/abs/1502.05698, 2015.</p>
<h1>Supplementary Materials: The CommAI-mini Tasks</h1>
<p>The CommAI-mini tasks are based on the hierarchy of sub-regular languages, in turn a subset of the regular languages (Jäger \&amp; Rogers, 2012; McNaughton \&amp; Papert, 1971). Sub-regular languages are useful to characterize pattern recognition skills of humans and other animals (for example, constraints on the distribution of stressed syllables in the world languages, Rogers et al., 2013).</p>
<p>Strictly local languages, the simplest class of sub-regular languages, can be recognized with an ngram lookup table only. For example, the stringset accepted by the regular expression $(A B)+$ is strictly local, because a lookup table containing the bigram $A B$ is sufficient to recognize the strings in it (we're ignoring here technicalities regarding begin- and end-of-string conditions, and the lowlevel implementation of the actual "scanner"). The $(A B \mid C)+$ language is also strictly local, because it can be recognized through a lookup table containing the n-grams $A B$ and $C$.</p>
<p>The next class ascending the hierarchy is that of locally testable languages. The latter can be recognized by imposing logical constraints (union, conjunction, complement) on the n-grams that occur, or do not occur, in a string. For example, A<em> (BA</em>) +, the "at-least-one-B" language, can be recognized by using a lookup table containing the unigrams $A$ and $B$, plus a checking device that verifies that the B unigram occurred at least once. The strictly local languages are a strict subset of the locally testable languages.</p>
<p>Locally testable languages are not the most complex kind of sub-regular languages, and they are still far from exploiting the full expressive power of regular languages (that are in turn the simplest class in the Chomsky hierarchy). Yet, by combining (subsets of) strictly local and locally testable languages, we already obtain an interesting challenge for CommAI learners. Importantly, an efficient solution to the CommAI-mini tasks does not just involve stringset recognition/production, but learning the description language that specifies the rules about legal strings.</p>
<p>All CommAI-mini tasks have the same structure (where communication flows through the bit-level interface):</p>
<ol>
<li>The environment presents the description of a target stringset;</li>
<li>the environment tells the learner whether it is a recognition or a production task;</li>
<li>if it is a recognition task, the environment produces the string to be recognized;</li>
<li>the environment listens to the learner, and records the string produced by the latter until a period occurs, or a maximum number of bits has been emitted;</li>
<li>the environment checks the string produced by the learner;</li>
<li>if the string is correct, the environment issues reward and states that the answer is correct;</li>
<li>
<p>if the string is wrong</p>
</li>
<li>
<p>in a recognition task, the environment states that the answer is wrong, and produces the right answer (true or false);</p>
</li>
<li>in a production task, the environment states that the answer is wrong, and produces a sample correct string.</li>
</ol>
<p>The tasks are organized into task sets, with each set constituting a videogame-like "level". Tasks in the same set are presented in random order. Each recognition-based set below could also be seen as a single recognition task for the relevant class of stringsets (more generally, we could think of all recognition tasks as a single task). We prefer the granular structure we are outlining below, because it will facilitate analysis. For example, learning strictly local unigram languages (A or B or C) is a special case of learning strictly local maximally-5-gram languages (ANFJG or CED or KPQR or ZM or S). However, treating these as separate tasks should make it easier to check if a learner has memory limitations, such that it doesn't scale up to n-grams beyond a certain length.</p>
<p>Each task is defined by the structure of the description (maximum n-gram length, number of terms, permitted operators), but the actual symbols defining an acceptable stringset will change from exposure to exposure. For example, the second task in set #1 below consists in recognizing any (XY) + string, where $X$ and $Y$ are arbitrary upper-case letters: description $A B$ and description LK are two different instances of this task.</p>
<p>In what follows, the tasks are illustrated by the string produced by the environment at the beginning of a task instance (corresponding to the first 3 steps in the enumeration above). As we just remarked, the target language (the actual stringset) will change from instance to instance of the same task. We will moreover only show a few illustrative tasks for each set. Further tasks can be generated by varying the maximum n-gram size and, except in set #1, the number of n-gram terms present in the description.</p>
<h1>TASK SET #1</h1>
<p>The following examples illustrate set #1 (here and below, we only show positive examples, where the learner should answer true):</p>
<div class="codehilite"><pre><span></span><code><span class="n">description</span><span class="o">:</span><span class="w"> </span><span class="n">C</span><span class="o">;</span><span class="w"> </span><span class="n">verify</span><span class="o">:</span><span class="w"> </span><span class="n">CCCC</span><span class="o">.</span>
<span class="n">description</span><span class="o">:</span><span class="w"> </span><span class="n">AB</span><span class="o">;</span><span class="w"> </span><span class="n">verify</span><span class="o">:</span><span class="w"> </span><span class="n">ABAB</span><span class="o">.</span>
<span class="n">description</span><span class="o">:</span><span class="w"> </span><span class="n">FJG</span><span class="o">;</span><span class="w"> </span><span class="n">verify</span><span class="o">:</span><span class="w"> </span><span class="n">FJG</span><span class="o">.</span>
</code></pre></div>

<p>Tasks in set #1 involve strictly local descriptions. There is a natural hierarchy within the set in terms of the length of the n-grams that must be memorized: verifying the $(A B)+$ language requires less memory than verifying the (FJG) + language.</p>
<h2>TASK SET #2</h2>
<p>Examples:</p>
<div class="codehilite"><pre><span></span><code><span class="n">description</span><span class="o">:</span><span class="w"> </span><span class="n">anything</span><span class="o">;</span><span class="w"> </span><span class="n">verify</span><span class="o">:</span><span class="w"> </span><span class="n">ANFHG</span><span class="o">.</span>
<span class="n">description</span><span class="o">:</span><span class="w"> </span><span class="n">AB</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">CD</span><span class="o">;</span><span class="w"> </span><span class="n">verify</span><span class="o">:</span><span class="w"> </span><span class="n">ABAB</span><span class="o">.</span>
<span class="n">description</span><span class="o">:</span><span class="w"> </span><span class="n">FAB</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">GH</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">MIL</span><span class="o">;</span><span class="w"> </span><span class="n">verify</span><span class="w"> </span><span class="n">FABFAB</span><span class="o">.</span>
</code></pre></div>

<p>The tasks in set #2 are also based on strictly local descriptions. However, because of the or operator, solving them requires storing multiple n-grams in memory. The #2 tasks thus imply the abilities necessary to solve #1 tasks (storing n-grams and checking their presence in a string), but they generalize them (to storing and using multiple n-grams).</p>
<p>The #2 tasks vary in terms of the number of disjoint n-grams that comprise the description and the maximum length of the n-grams in the description.</p>
<p>We introduce anything as a special symbol matching any (byte-level) sequence. Recognizing anything is strictly local.</p>
<h2>TASK SET #3</h2>
<p>Examples:</p>
<div class="codehilite"><pre><span></span><code><span class="n">description</span><span class="o">:</span><span class="w"> </span><span class="n">AB</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">CF</span><span class="o">;</span><span class="w"> </span><span class="n">verify</span><span class="o">:</span><span class="w"> </span><span class="n">ABCFABAB</span><span class="o">.</span>
<span class="n">description</span><span class="o">:</span><span class="w"> </span><span class="n">HL</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">RM</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">BT</span><span class="o">;</span><span class="w"> </span><span class="n">verify</span><span class="o">:</span><span class="w"> </span><span class="n">RMBTBTHLHLBT</span><span class="o">.</span>
<span class="n">description</span><span class="o">:</span><span class="w"> </span><span class="n">AB</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">anything</span><span class="o">;</span><span class="w"> </span><span class="n">verify</span><span class="o">:</span><span class="w"> </span><span class="n">FKGABJJKJKSD</span><span class="o">.</span>
<span class="n">description</span><span class="o">:</span><span class="w"> </span><span class="n">AB</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">CF</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">anything</span><span class="o">;</span><span class="w"> </span><span class="n">verify</span><span class="o">:</span><span class="w"> </span><span class="n">FJGKJKJKJKJDCFDJKJKJKSJAB</span><span class="o">.</span>
</code></pre></div>

<p>Set #3 tasks involve locally testable languages. Verifying that the target string only contains ngrams from the description no longer suffices. The learner must check whether all n-grams in the description have been used. These tasks thus generalize #2 tasks. They also require storing multiple n-grams in memory, but they further need some device to check that all the n-grams in the lookup table have been used.</p>
<p>There is again an obvious hierarchy in terms of how many distinct n-grams must be stored in memory and their length. Tasks can also be distinguished in terms of whether they include the anything operator or not.</p>
<p>We are not considering tasks mixing conjunction and disjunction, except for the implicit anything-denoted disjunction. We exclude the more general case to avoid having to implement complex scope conventions.</p>
<p>TASK SET #4</p>
<p>Examples:
description: not AB and anything; verify: ADFCFHGHADDDB.
description: not AB and CF and anything; verify: DJFKJKJSCFDSFG.
description: not AB and not CF and anything; verify: DJFKJKJSCEFDSFG.
Tasks in set #4 also involve locally testable languages. However, on top of conjunction, they include a negation operator. In our setup, conjunction always takes scope over negation, to avoid the need for overt bracketing in the descriptions. The fact that a negated n-gram is equivalent to the affirmation of its complement is explicitly expressed by always adding the and anything condition. For the time being, we do not consider more general combinations of conjunction, disjunction and negation.</p>
<p>TASK SET #5</p>
<p>Examples:
description: C; produce.
description: AB; produce.
description: FJG; produce.
description: anything; produce.
description: AB or CD; produce.
description: FAB or GH or MIL; produce.
description: AB and CF; produce.
description: HL and RM and BT; produce.
description: AB and anything; produce.
description: AB and CF and anything; produce.
description: not AB and anything; produce.
description: not AB and CF and anything; produce.
description: not AB and not CF and anything; produce.
We consider the production counterparts of all recognition tasks. The learner is asked to generate one string matching the conditions in the description. We expect a compositional learner to solve the production tasks much faster if it has already been exposed to the recognition tasks (and vice versa).</p>
<h1>FURTHER TASKS</h1>
<p>The production tasks in set #5 can be solved by always generating the shortest string in the description. For the simpler tasks (without conjunction), this amounts to producing the first upper-case string in the description. We can force the learner out of this strategy by asking it to produce two distinct strings matching the description, e.g.:
description: C; produce two distinct strings.
Tasks of this sort would obviously build on skills acquired in set #5, adding the requirement that the learner stores its own past productions in memory, and uses them when planning what to produce next.</p>
<p>It's easy to think of further tasks that a learner could solve fast by exploiting skills acquired through sets #1-5, e.g., switching the capitalization conventions:</p>
<p>DESCRIPTION: ab; PRODUCE.
More ambitiously, the learner could be provided with a sample of strings, and asked to formulate a description accepting them (even producing extremely loose descriptions, such as anything, would constitute an impressive achievement).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ E.g.: https://openai.com/blog/universe/, https://github.com/deepmind/lab, http://www.ggp.org, http://www.gvgal.net/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6736 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6736</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6736</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-129.html">extraction-schema-129</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-f9838a3be5c94bb2674a0e224de349b50e18f3c4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f9838a3be5c94bb2674a0e224de349b50e18f3c4" target="_blank">Learning To Retrieve Prompts for In-Context Learning</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work proposes an efficient method for retrieving prompts for in-context learning using annotated data and an LM, and trains an efficient dense retriever from this data, which is used to retrieve training examples as prompts at test time.</p>
                <p><strong>Paper Abstract:</strong> In-context learning is a recent paradigm in natural language understanding, where a large pre-trained language model (LM) observes a test instance and a few training examples as its input, and directly decodes the output without any update to its parameters. However, performance has been shown to strongly depend on the selected training examples (termed prompts). In this work, we propose an efficient method for retrieving prompts for in-context learning using annotated data and an LM. Given an input-output pair, we estimate the probability of the output given the input and a candidate training example as the prompt, and label training examples as positive or negative based on this probability. We then train an efficient dense retriever from this data, which is used to retrieve training examples as prompts at test time. We evaluate our approach on three sequence-to-sequence tasks where language utterances are mapped to meaning representations, and find that it substantially outperforms prior work and multiple baselines across the board.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6736.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6736.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EPR (BREAK, GPT-NEO)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Efficient Prompt Retrieval (EPR) evaluated on BREAK using GPT-NEO</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>EPR is a contrastively-trained dense prompt retriever that uses a scoring language model to label positive/negative prompt examples by estimating Prob(y | candidate_prompt, x) and then trains encoders to retrieve useful in-context examples for an inference LM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-NEO</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2.7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>In-context learning with EPR prompt retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>retrieval-based / sequential (in-context)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>BREAK (development)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Map complex natural language questions to a stepwise language-based meaning representation (semantic parsing / question decomposition)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>LF-EM (logical-form exact match / semantic equivalence)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>31.9</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>BM25 (best unsupervised baseline used in supervised pipelines)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>5.9</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>EPR uses a scoring LM to label good/bad candidate prompts (Prob_{hat g}(y | candidate, x)) and then trains a dense retriever with contrastive learning; it outperforms surface-similarity baselines, indicating the LM-based supervision captures utility beyond lexical similarity. The paper also shows EPR is comparable to BM25-Oracle on BREAK (EPR 31.9 vs BM25-Oracle 32.3). Hyperparameter robustness and ablations on k and L are reported in the appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning To Retrieve Prompts for In-Context Learning', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6736.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6736.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EPR (MTOP, GPT-NEO)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Efficient Prompt Retrieval (EPR) evaluated on MTOP using GPT-NEO</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same EPR method as above, applied to MTOP (task-oriented semantic parsing) with GPT-NEO as the inference LM; training signal from scoring LM is used to learn a retriever that selects in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-NEO</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2.7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>In-context learning with EPR prompt retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>retrieval-based / sequential (in-context)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MTOP (English subset, development)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Semantic parsing for task-oriented dialogue: map commands to nested program-like queries across domains</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>64.2</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CBR (Case-Based Reasoning, best supervised baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>7.2</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>EPR substantially outperforms both surface-based retrievers and the adapted case-based reasoning baseline, illustrating that LM-derived supervision yields a stronger retrieval signal. The authors also report high abstract-copying rates on MTOP (see separate copying analysis entry) which correlates with higher accuracy on copied instances.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning To Retrieve Prompts for In-Context Learning', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6736.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6736.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EPR (SMCALFLOW, GPT-NEO)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Efficient Prompt Retrieval (EPR) evaluated on SMCALFLOW using GPT-NEO</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>EPR applied to SMCALFLOW (task-oriented dataflow program generation) with GPT-NEO as the inference LM; retriever trained from LM-based labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-NEO</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2.7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>In-context learning with EPR prompt retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>retrieval-based / sequential (in-context)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>SMCALFLOW (development)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Map natural language utterances to dataflow programs (complex API calls and compositions)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>54.3</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CBR (Case-Based Reasoning, best supervised baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>2.9</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>EPR outperforms BM25 and CBR on SMCALFLOW; the authors argue that scoring with an LM provides supervision beyond surface similarity, enabling retrieval of prompts that better guide in-context generation for program-like outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning To Retrieve Prompts for In-Context Learning', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6736.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6736.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Case-Based Reasoning (CBR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Case-Based Reasoning adapted scoring baseline (Das et al., adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised prompt retriever adapted from prior work that scores candidate prompts by token-level F1 between output sequences (i.e., surface overlap of output tokens) and uses those scores to train a dense retriever.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CBR (adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Case-based scoring for prompt selection</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>retrieval-based (surface/formula similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>homogeneous</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MTOP / BREAK / SMCALFLOW (development)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Supervised prompt retrieval for in-context semantic parsing</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM) / LF-EM for BREAK</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>57.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>EPR</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-7.2</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>CBR relies on surface overlap between output sequences (token F1) to label positives/negatives; EPR's LM-based labeling yields stronger signal and higher downstream EM across datasets. CBR tends to retrieve examples that match output token patterns but may miss examples that a scoring LM deems useful.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning To Retrieve Prompts for In-Context Learning', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6736.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6736.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BM25-Oracle</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BM25 retrieval using the gold output (oracle variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An oracle upper bound where BM25 is applied to the gold output y_test at test time to retrieve candidate prompts (provides an upper limit for BM25-based supervised retrievers).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BM25-ORACLE</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>BM25 retrieval with gold-output query (oracle)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>retrieval-based (sparse lexical)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>homogeneous</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MTOP (development)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Oracle retrieval: find training examples lexically similar to the gold meaning representation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>58.9</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>EPR</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-5.3</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>EPR outperforms BM25-Oracle on MTOP and SMCALFLOW, suggesting that LM-derived supervision can go beyond lexically similar outputs and find prompts that better help the LM generalize. On BREAK EPR and BM25-Oracle are comparable.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning To Retrieve Prompts for In-Context Learning', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6736.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6736.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Copying (Exact & Abstract)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Analysis of prompt copying: exact copying and abstract pattern copying</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Behavioral analysis measuring whether the LM's generated output exactly matches an example in the prompt (exact copying) or whether its masked/abstracted structure matches a prompt example (abstract copying).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-NEO</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2.7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Prompt-copying analysis (exact vs abstract)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>analysis / behavioral</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>BREAK / MTOP / SMCALFLOW (development)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Measure degree to which in-context outputs copy prompt examples versus generalize to novel structures in semantic parsing tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Abstract-copying rate (and accuracy on copied subset)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>84.5</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Reported dataset-specific statistics: MTOP abstract-copying rate = 84.5% with accuracy on abstract-copied subset = 71.6% (higher than overall EM), SMCALFlow abstract-copying rate >80%, BREAK shows much lower copying rates. The paper also shows copied patterns are mainly sourced from high-ranked retrieved prompts (distance distribution skewed toward top-ranked prompts). This suggests that prompt selection fidelity (ranking quality) strongly affects whether models reuse prompt structures versus generalize.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning To Retrieve Prompts for In-Context Learning', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6736.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6736.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EPR (LM-as-proxy scoring choice)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of scoring LM choice for EPR (GPT-NEO proxy vs GPT-J scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison showing that a smaller scoring LM (GPT-NEO) can serve as an effective proxy to generate training labels for EPR applied to larger inference LMs, and that using the inference LM itself as the scoring LM yields modest further gains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J (inference) / GPT-NEO (scoring proxy)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-J: 6B; GPT-NEO: 2.7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>EPR with different scoring LM choices (LM-as-proxy vs identical LM)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>retrieval-based / experimental comparison</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>BREAK (development, sample comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Semantic parsing on BREAK; measure how scoring-LM choice affects retrieval training and downstream in-context performance</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>LF-EM</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>33.6</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>EPR with GPT-NEO as scoring LM (inference GPT-J)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>2.1</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Using GPT-NEO as a proxy scoring LM gives strong gains when training EPR for larger inference LMs; using the inference LM itself as the scoring LM can further improve performance slightly (example: BREAK improves from 31.5 to 33.6 when scoring with GPT-J versus GPT-NEO). This supports training-efficient data generation with smaller LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning To Retrieve Prompts for In-Context Learning', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>What makes good in-context examples for gpt-3? <em>(Rating: 2)</em></li>
                <li>Case-based reasoning for natural language queries over knowledge bases <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>SentenceBERT: Sentence embeddings using Siamese BERT-networks <em>(Rating: 1)</em></li>
                <li>The probabilistic relevance framework: BM25 and beyond <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6736",
    "paper_id": "paper-f9838a3be5c94bb2674a0e224de349b50e18f3c4",
    "extraction_schema_id": "extraction-schema-129",
    "extracted_data": [
        {
            "name_short": "EPR (BREAK, GPT-NEO)",
            "name_full": "Efficient Prompt Retrieval (EPR) evaluated on BREAK using GPT-NEO",
            "brief_description": "EPR is a contrastively-trained dense prompt retriever that uses a scoring language model to label positive/negative prompt examples by estimating Prob(y | candidate_prompt, x) and then trains encoders to retrieve useful in-context examples for an inference LM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-NEO",
            "model_size": "2.7B",
            "reasoning_method_name": "In-context learning with EPR prompt retrieval",
            "reasoning_method_type": "retrieval-based / sequential (in-context)",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "BREAK (development)",
            "task_description": "Map complex natural language questions to a stepwise language-based meaning representation (semantic parsing / question decomposition)",
            "performance_metric": "LF-EM (logical-form exact match / semantic equivalence)",
            "performance_value": 31.9,
            "comparison_target_method": "BM25 (best unsupervised baseline used in supervised pipelines)",
            "performance_difference": 5.9,
            "statistical_significance": false,
            "analysis_notes": "EPR uses a scoring LM to label good/bad candidate prompts (Prob_{hat g}(y | candidate, x)) and then trains a dense retriever with contrastive learning; it outperforms surface-similarity baselines, indicating the LM-based supervision captures utility beyond lexical similarity. The paper also shows EPR is comparable to BM25-Oracle on BREAK (EPR 31.9 vs BM25-Oracle 32.3). Hyperparameter robustness and ablations on k and L are reported in the appendix.",
            "ablation_study_present": true,
            "uuid": "e6736.0",
            "source_info": {
                "paper_title": "Learning To Retrieve Prompts for In-Context Learning",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "EPR (MTOP, GPT-NEO)",
            "name_full": "Efficient Prompt Retrieval (EPR) evaluated on MTOP using GPT-NEO",
            "brief_description": "Same EPR method as above, applied to MTOP (task-oriented semantic parsing) with GPT-NEO as the inference LM; training signal from scoring LM is used to learn a retriever that selects in-context examples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-NEO",
            "model_size": "2.7B",
            "reasoning_method_name": "In-context learning with EPR prompt retrieval",
            "reasoning_method_type": "retrieval-based / sequential (in-context)",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "MTOP (English subset, development)",
            "task_description": "Semantic parsing for task-oriented dialogue: map commands to nested program-like queries across domains",
            "performance_metric": "Exact Match (EM)",
            "performance_value": 64.2,
            "comparison_target_method": "CBR (Case-Based Reasoning, best supervised baseline)",
            "performance_difference": 7.2,
            "statistical_significance": false,
            "analysis_notes": "EPR substantially outperforms both surface-based retrievers and the adapted case-based reasoning baseline, illustrating that LM-derived supervision yields a stronger retrieval signal. The authors also report high abstract-copying rates on MTOP (see separate copying analysis entry) which correlates with higher accuracy on copied instances.",
            "ablation_study_present": true,
            "uuid": "e6736.1",
            "source_info": {
                "paper_title": "Learning To Retrieve Prompts for In-Context Learning",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "EPR (SMCALFLOW, GPT-NEO)",
            "name_full": "Efficient Prompt Retrieval (EPR) evaluated on SMCALFLOW using GPT-NEO",
            "brief_description": "EPR applied to SMCALFLOW (task-oriented dataflow program generation) with GPT-NEO as the inference LM; retriever trained from LM-based labels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-NEO",
            "model_size": "2.7B",
            "reasoning_method_name": "In-context learning with EPR prompt retrieval",
            "reasoning_method_type": "retrieval-based / sequential (in-context)",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "SMCALFLOW (development)",
            "task_description": "Map natural language utterances to dataflow programs (complex API calls and compositions)",
            "performance_metric": "Exact Match (EM)",
            "performance_value": 54.3,
            "comparison_target_method": "CBR (Case-Based Reasoning, best supervised baseline)",
            "performance_difference": 2.9,
            "statistical_significance": false,
            "analysis_notes": "EPR outperforms BM25 and CBR on SMCALFLOW; the authors argue that scoring with an LM provides supervision beyond surface similarity, enabling retrieval of prompts that better guide in-context generation for program-like outputs.",
            "ablation_study_present": true,
            "uuid": "e6736.2",
            "source_info": {
                "paper_title": "Learning To Retrieve Prompts for In-Context Learning",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Case-Based Reasoning (CBR)",
            "name_full": "Case-Based Reasoning adapted scoring baseline (Das et al., adapted)",
            "brief_description": "A supervised prompt retriever adapted from prior work that scores candidate prompts by token-level F1 between output sequences (i.e., surface overlap of output tokens) and uses those scores to train a dense retriever.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "CBR (adapted)",
            "model_size": "",
            "reasoning_method_name": "Case-based scoring for prompt selection",
            "reasoning_method_type": "retrieval-based (surface/formula similarity)",
            "reasoning_style_diversity": "homogeneous",
            "benchmark_name": "MTOP / BREAK / SMCALFLOW (development)",
            "task_description": "Supervised prompt retrieval for in-context semantic parsing",
            "performance_metric": "Exact Match (EM) / LF-EM for BREAK",
            "performance_value": 57.0,
            "comparison_target_method": "EPR",
            "performance_difference": -7.2,
            "statistical_significance": false,
            "analysis_notes": "CBR relies on surface overlap between output sequences (token F1) to label positives/negatives; EPR's LM-based labeling yields stronger signal and higher downstream EM across datasets. CBR tends to retrieve examples that match output token patterns but may miss examples that a scoring LM deems useful.",
            "ablation_study_present": false,
            "uuid": "e6736.3",
            "source_info": {
                "paper_title": "Learning To Retrieve Prompts for In-Context Learning",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "BM25-Oracle",
            "name_full": "BM25 retrieval using the gold output (oracle variant)",
            "brief_description": "An oracle upper bound where BM25 is applied to the gold output y_test at test time to retrieve candidate prompts (provides an upper limit for BM25-based supervised retrievers).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "BM25-ORACLE",
            "model_size": "",
            "reasoning_method_name": "BM25 retrieval with gold-output query (oracle)",
            "reasoning_method_type": "retrieval-based (sparse lexical)",
            "reasoning_style_diversity": "homogeneous",
            "benchmark_name": "MTOP (development)",
            "task_description": "Oracle retrieval: find training examples lexically similar to the gold meaning representation",
            "performance_metric": "Exact Match (EM)",
            "performance_value": 58.9,
            "comparison_target_method": "EPR",
            "performance_difference": -5.3,
            "statistical_significance": false,
            "analysis_notes": "EPR outperforms BM25-Oracle on MTOP and SMCALFLOW, suggesting that LM-derived supervision can go beyond lexically similar outputs and find prompts that better help the LM generalize. On BREAK EPR and BM25-Oracle are comparable.",
            "ablation_study_present": false,
            "uuid": "e6736.4",
            "source_info": {
                "paper_title": "Learning To Retrieve Prompts for In-Context Learning",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Prompt Copying (Exact & Abstract)",
            "name_full": "Analysis of prompt copying: exact copying and abstract pattern copying",
            "brief_description": "Behavioral analysis measuring whether the LM's generated output exactly matches an example in the prompt (exact copying) or whether its masked/abstracted structure matches a prompt example (abstract copying).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-NEO",
            "model_size": "2.7B",
            "reasoning_method_name": "Prompt-copying analysis (exact vs abstract)",
            "reasoning_method_type": "analysis / behavioral",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "BREAK / MTOP / SMCALFLOW (development)",
            "task_description": "Measure degree to which in-context outputs copy prompt examples versus generalize to novel structures in semantic parsing tasks",
            "performance_metric": "Abstract-copying rate (and accuracy on copied subset)",
            "performance_value": 84.5,
            "comparison_target_method": "n/a",
            "performance_difference": null,
            "statistical_significance": false,
            "analysis_notes": "Reported dataset-specific statistics: MTOP abstract-copying rate = 84.5% with accuracy on abstract-copied subset = 71.6% (higher than overall EM), SMCALFlow abstract-copying rate &gt;80%, BREAK shows much lower copying rates. The paper also shows copied patterns are mainly sourced from high-ranked retrieved prompts (distance distribution skewed toward top-ranked prompts). This suggests that prompt selection fidelity (ranking quality) strongly affects whether models reuse prompt structures versus generalize.",
            "ablation_study_present": false,
            "uuid": "e6736.5",
            "source_info": {
                "paper_title": "Learning To Retrieve Prompts for In-Context Learning",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "EPR (LM-as-proxy scoring choice)",
            "name_full": "Effect of scoring LM choice for EPR (GPT-NEO proxy vs GPT-J scoring)",
            "brief_description": "Comparison showing that a smaller scoring LM (GPT-NEO) can serve as an effective proxy to generate training labels for EPR applied to larger inference LMs, and that using the inference LM itself as the scoring LM yields modest further gains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-J (inference) / GPT-NEO (scoring proxy)",
            "model_size": "GPT-J: 6B; GPT-NEO: 2.7B",
            "reasoning_method_name": "EPR with different scoring LM choices (LM-as-proxy vs identical LM)",
            "reasoning_method_type": "retrieval-based / experimental comparison",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "BREAK (development, sample comparisons)",
            "task_description": "Semantic parsing on BREAK; measure how scoring-LM choice affects retrieval training and downstream in-context performance",
            "performance_metric": "LF-EM",
            "performance_value": 33.6,
            "comparison_target_method": "EPR with GPT-NEO as scoring LM (inference GPT-J)",
            "performance_difference": 2.1,
            "statistical_significance": false,
            "analysis_notes": "Using GPT-NEO as a proxy scoring LM gives strong gains when training EPR for larger inference LMs; using the inference LM itself as the scoring LM can further improve performance slightly (example: BREAK improves from 31.5 to 33.6 when scoring with GPT-J versus GPT-NEO). This supports training-efficient data generation with smaller LMs.",
            "ablation_study_present": false,
            "uuid": "e6736.6",
            "source_info": {
                "paper_title": "Learning To Retrieve Prompts for In-Context Learning",
                "publication_date_yy_mm": "2021-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "What makes good in-context examples for gpt-3?",
            "rating": 2
        },
        {
            "paper_title": "Case-based reasoning for natural language queries over knowledge bases",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "SentenceBERT: Sentence embeddings using Siamese BERT-networks",
            "rating": 1
        },
        {
            "paper_title": "The probabilistic relevance framework: BM25 and beyond",
            "rating": 1
        }
    ],
    "cost": 0.017764,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Learning To Retrieve Prompts for In-Context Learning</h1>
<p>Ohad Rubin Jonathan Herzig Jonathan Berant<br>The Blavatnik School of Computer Science, Tel Aviv University<br>{ohad.rubin, jonathan.herzig, joberant}@cs.tau.ac.il</p>
<h4>Abstract</h4>
<p>In-context learning is a recent paradigm in natural language understanding, where a large pretrained language model (LM) observes a test instance and a few training examples as its input, and directly decodes the output without any update to its parameters. However, performance has been shown to strongly depend on the selected training examples (termed prompts). In this work, we propose an efficient method for retrieving prompts for in-context learning using annotated data and an LM. Given an inputoutput pair, we estimate the probability of the output given the input and a candidate training example as the prompt, and label training examples as positive or negative based on this probability. We then train an efficient dense retriever from this data, which is used to retrieve training examples as prompts at test time. We evaluate our approach on three sequence-tosequence tasks where language utterances are mapped to meaning representations, and find that it substantially outperforms prior work and multiple baselines across the board.</p>
<h2>1 Introduction</h2>
<p>The striking language skills and world knowledge embedded in large pre-trained language models (LMs) (Devlin et al., 2019; Petroni et al., 2019; Raffel et al., 2020; Brown et al., 2020) have recently led to in-context learning, a new paradigm in natural language understanding. Under this paradigm, a language model is given a prompt, which typically contains a few training examples, as well as a test instance as input, and generates the output for the test instance directly, without any update to its parameters. This approach was first introduced in GPT-3 (Brown et al., 2020), but has quickly spread to other LMs (Lieber et al., 2021; Du et al., 2021; Rae et al., 2021).</p>
<p>An attractive property of in-context learning is that it provides a single model for multiple language understanding tasks. However, Liu et al.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An overview of prompt retrieval: Given a question from BREAK, one retrieves similar training examples from an index of the training set. The question and training examples (the prompt) are passed to an inference LM that decodes the output.
(2021a) showed that downstream performance can vary widely depending on the choice of in-context examples. This has sparked interest in prompt retrieval (see Fig. 1), where given a test instance, training examples are chosen for the prompt based on some similarity metric. Recent work has either used off-the-shelf unsupervised similarity metrics, or trained a prompt retriever to select examples based on surface similarity (Das et al., 2021).</p>
<p>In this work, we suggest to use language models themselves to label examples that can serve as good prompts, and train a prompt retriever from this signal. To train the retriever (see Fig. 2), we assume access to a training set of input-output pairs and to a scoring $L M$, i.e., a language model that will be used to score prompts. For each training example $(x, y)$, we go over other candidate training examples, and estimate the probability, according to the scoring LM, of $y$ conditioned on $x$ and the candidate prompt. We label training examples that lead to high probability as positive and low probability as negative and train a prompt retriever</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: An overview of our approach for training EPR. Given a training example, we use an unsupervised retriever $R_{u}$ to obtain a set of candidates. We then pass the candidates to a scoring LM and label the top- $k$ and the bottom- $k$ as positive and negative examples, respectively. Last, we use this training data to train a dense retriever.
from this data using contrastive learning. We argue that using an LM for labeling examples is a better proxy for training a retriever compared to previously-proposed surface similarity heuristics. Importantly, when creating the training data, we have access to the gold label $y$, which can be used to obtain a high-quality set of candidate prompts. This leads to good positive examples and hard negative examples, which are beneficial for training with a contrastive objective.</p>
<p>Using a scoring LM to train an efficient retriever for a potentially different test time inference $L M$ is beneficial in two scenarios. First, when the scoring LM is smaller than the inference LM and serves as a proxy for it. This results in cheap and efficient data generation for the retriever, accessible to a wide range of researchers. Second, our approach can be used even when the scoring and inference LMs are identical (e.g., both are GPT-3). This is beneficial when we do not have access to model parameters and can only use it as a service, an increasingly popular paradigm. In this case, we use the LM to train a light-weight retriever that is only tasked with learning a similarity function. More generally, given that the scale of LMs is likely to keep increasing in the foreseeable future, one can view our approach for Efficient Prompt Retrieval, or $E P R$, as a method for interfacing and learning to interact with large LMs.</p>
<p>We empirically test EPR on three structured sequence-to-sequence tasks, where input natural language utterances are mapped to a meaning representation: MTop (Li et al., 2021) and SMCALFlow(Andreas et al., 2020), which focus on task-oriented dialogue, and BREAK (Wolfson et al.,
2020), a benchmark for mapping questions to a language-based meaning representation. We observe that EPR substantially improves performance compared to prior work on prompt retrieval. When the scoring LM and inference LM are identical (using GPT-NEO (Black et al., 2021)), performance compared to the best baseline improves from $26 \%$ to $31.9 \%$ on BREAK, from $57 \%$ to $64.2 \%$ on MTOP, and from $51.4 \%$ to $54.3 \%$ on SMCAIFlow. When using GPT-NEO as a proxy for larger LMs (GPT-J, GPT-3, and CODEX), we observe similar gains, where performance improves substantially in all cases.</p>
<p>To conclude, we propose an approach for retrieving training examples for in-context learning in large language models, and show it substantially outperforms prior methods. Given recent developments in scaling LMs, designing efficient methods for interacting with LMs is an important direction for future research.</p>
<h2>2 Background: Prompt Retrieval</h2>
<p>Problem setup Given a training set $\mathcal{D}=$ $\left{\left(x_{i}, y_{i}\right)\right}<em _test="{test" _text="\text">{i=1}^{n}$ of input-output sequences, and a test example $x</em>$}}$, our goal is to train a retriever, $R\left(x_{\text {test }}, \mathcal{D}\right)$, that will retrieve a subset of training examples $\mathcal{P}=\left{\left(x_{j}, y_{j}\right)\right}_{j=1}^{m} \subset \mathcal{D}$, where $m \ll n$. We succinctly refer to $\mathcal{P}$ as the prompt. ${ }^{1</p>
<p>Given an inference LM, $g$, a good prompt should lead to the target output sequence when the test example $x_{\text {test }}$ is concatenated to the prompt $\mathcal{P}$ and passed as a prefix to $g$. Specifically, decoding from</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>the $\operatorname{LM} g\left(\left[\mathcal{P} ; x_{\text {test }}\right]\right)$ should yield $y_{\text {test }}$. In this work, we focus on structured tasks, such as semantic parsing, where $x$ is a natural language utterance and $y$ is a meaning representation for that utterance.</p>
<p>Prior work Liu et al. (2021a) investigated the effect of different prompts on the performance of GPT-3 and demonstrated that the choice of incontext examples strongly affects downstream performance. They used an unsupervised sentence encoder to encode training examples, and retrieved for every test instance its nearest neighbors.</p>
<p>Das et al. (2021) trained a supervised prompt retriever for knowledge-base question answering. The retriever was trained with supervision that is tailored for knowledge-base queries, and relies on surface similarity between formal queries. Conversely, our approach takes advantage of the generative LM itself and is thus more general.</p>
<p>Shin et al. (2021) used GPT-3 to select examples for the prompt for few-shot semantic parsing. However, rather than training a retriever, they randomly sample a large set of utterance-program pairs from the training set, and choose those that are similar to the target instance question according to GPT-3. This results in an expensive inference procedure, where GPT-3 is run hundreds of times for each test instance, unlike our approach, which is based on a light-weight sub-linear retriever.</p>
<h2>3 Efficient Prompt Retriever</h2>
<p>We now describe our method for training EPR, an efficient prompt retriever for in-context learning. We first describe how to generate labeled data (3.1), and then how to use the training data for training and inference (3.2). Fig. 2 provides an overview of the training procedure.</p>
<h3>3.1 Generating the Training Data</h3>
<p>Our approach relies on finding which training examples can serve as good prompts for other training examples. Scoring all pairs of training examples is quadratic in $|\mathcal{D}|$, and thus prohibitive. Hence, we present a method for choosing a set of candidate examples $\overline{\mathcal{E}} \subset D$, from which we will choose positive and negative examples for training. Importantly, since we are not at test time and are only generating data for training, we can use the target sequence $y$ to retrieve a good set of candidates. This can be approached using a simple retrieval method, given that our goal is to retrieve examples that are similar to the input in terms of their output sequence, $y$.</p>
<p>To obtain a high-quality candidate set of training examples, we take advantage of an unsupervised retriever, $\overline{\mathcal{E}}=R_{u}((x, y), \mathcal{D})$. For the choice of the unsupervised retriever, we experiment with BM25 (Robertson and Zaragoza, 2009), a sparse retriever that relies on surface text similarity, and SBERT (Reimers and Gurevych, 2019), which is based on dense sentence encoding. For both, we experimented with passing the retriever the training pair $(x, y)$ or the target sequence $y$ only, and found that using $y$ leads to slightly higher performance.</p>
<p>Scoring the candidate set Once we retrieve the set of candidates $\overline{\mathcal{E}}=\left{\bar{e}<em L="L">{1}, \cdots, \bar{e}</em>, g$. Specifically, the score for a candidate prompt is}\right}$ for a training example $(x, y),{ }^{2}$ we score each candidate $\bar{e}_{l} \in \overline{\mathcal{E}}$ independently with a scoring $\mathrm{LM}, \hat{g}$, which serves as a proxy for the inference $\mathrm{LM</p>
<p>$$
s\left(\bar{e}<em _hat_g="\hat{g">{l}\right)=\operatorname{Prob}</em>, x\right)
$$}}\left(y \mid \bar{e}_{l</p>
<p>which is the probability under the $\mathrm{LM}, \hat{g}$, of the output sequence conditioned on the candidate prompt and input sequence. This indicates how helpful this candidate is for decoding the target (independent of all other candidates). We argue this score is a better proxy for the utility of a training example at inference time compared to prior approaches.</p>
<p>We apply this scoring function to all training examples, and define for each training example a set of positive examples $\mathcal{E}<em l="l">{\text {pos }}$, which includes the top- $k$ candidates in $\overline{\mathcal{E}}$ according to $s\left(\bar{e}</em>}\right)$, and a set of negative examples $\mathcal{E<em l="l">{\text {neg }}$, which includes the bottom- $k$ candidates in $\overline{\mathcal{E}}$ according to $s\left(\bar{e}</em>)$. With positive and negative examples at our disposal, we can now apply contrastive learning, which we describe next.}\right)$. This should lead to relevant positive examples, assuming that the set of candidates, $\overline{\mathcal{E}}$ includes good prompt candidates and hard negatives, since all candidates have high similarity with $(x, y)$ according to $R_{u}(y, \mathcal{D</p>
<h3>3.2 Training and Inference</h3>
<p>Training Our training procedure proceeds exactly like the contrastive learning procedure from DPR (Karpukhin et al., 2020). This procedure results in an input encoder $E_{X}(\cdot)$, which receives the sequence of input tokens, $x$, and a prompt encoder $E_{P}(\cdot)$, which receives a candidate prompt, namely, a concatenation of the tokens in an input-output pair. Both encoders are initialized with BERT-base</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>(Devlin et al., 2019), and the output vector representation is given by the CLS token, as usual. The goal of training is to learn a similarity metric such that given a test example $x_{\text {test }}$, it will be similar to training examples that lead to decoding of $y_{\text {test }}$.</p>
<p>Our training instances are of the form $\left\langle x_{i}, e_{i}^{+}, e_{i, 1}^{-}, \ldots e_{i, 2 B-1}^{-}\right\rangle$. Where the positive example $e_{i}^{+}$is sampled from the set $\mathcal{E}<em _neg="{neg" _text="\text">{\text {pos }}^{(i)}$, and our negative examples consist of one hard negative example sampled from $\mathcal{E}</em>(e)$. We can now define the typical contrastive learning objective and minimize for each example the negative log likelihood of the positive example:}}^{(i)}, B-1$ positive examples from the other instances in the same mini-batch, and the $B-1$ hard negatives from those instances. We define the similarity score between an input and an input-output pair to be the inner product $\operatorname{sim}(x, e)=E_{X}(x)^{\top} E_{P</p>
<p>$$
\begin{aligned}
&amp; L\left(x_{i}, e_{i}^{+}, e_{i, 1}^{-}, \ldots e_{i, 2 B-1}^{-}\right) \
= &amp; -\log \frac{e^{\operatorname{sim}\left(x_{i}, e_{i}^{+}\right)}}{e^{\operatorname{sim}\left(x_{i}, e_{i}^{+}\right)}+\sum_{j=1}^{2 B-1} e^{\operatorname{sim}\left(x_{i}, e_{i, j}^{-}\right)}}
\end{aligned}
$$</p>
<p>An advantage of this approach is that for batch size $B$ the effective batch size is of order $B^{2}$, with the in-batch negatives trick (Henderson et al., 2017).</p>
<p>Inference After training the input encoder and prompt encoder, we encode the entire set of training examples with $E_{P}(\cdot)$ in a pre-processing step using FAISS (Johnson et al., 2017). At test time, given an input sequence, $x_{\text {test }}$, we compute its encoding $E_{X}\left(x_{\text {test }}\right)$, and then use maximum innerproduct search over the training data to find the $L$ most similar training examples, sorted by their inner product (from high to low): $\mathcal{P}=\left(e_{1}, \ldots, e_{L}\right)$. The final prompt $\mathcal{P}^{\prime}$ is determined by $C$, the maximal context size supported by the inference LM, $g$. Specifically, $L^{\prime} \leq L$ is the largest $L^{\prime}$ such $\sum_{i=1}^{L^{\prime}}\left|e_{i}\right|+\left|x_{\text {test }}\right|+\left|y^{\prime}\right| \leq C$, where $\left|y^{\prime}\right|$ is the desired maximal length of the generated output. Finally, we return the output of greedy decoding on $g\left(\left[e_{L^{\prime}} ; e_{L^{\prime}-1} ; \ldots ; e_{1} ; x_{\text {test }}\right]\right)$.</p>
<p>We note that while at training time we score each training example independently, at test time the language model observes a prompt, i.e., a sequence of examples. We leave modeling the dependence between different training examples to future work.</p>
<h2>4 Experimental Results</h2>
<p>We now compare EPR to a wide range of unsupervised and supervised baselines, both when the
scoring $\mathrm{LM}, \hat{g}$, is smaller than the inference $\mathrm{LM}, g$, and when they are identical.</p>
<h3>4.1 Datasets</h3>
<p>We focus on tasks that map utterances to meaning representations, where in-context examples can be used to learn the mapping from inputs to outputs. Examples from each dataset and the number of examples are in Table 1.</p>
<ul>
<li>BREAK (Wolfson et al., 2020): A dataset mapping complex natural language questions into a language-based meaning representation, where a question is decomposed into an ordered list of atomic steps. We use the low-level BREAK subset, containing $44 \mathrm{~K} / 7 \mathrm{~K} / 8 \mathrm{~K}$ examples in its training/development/test sets.</li>
<li>MTOP (Li et al., 2021): A semantic parsing dataset, focused on task-oriented dialogue, where commands are mapped to complex nested queries across 11 domains. Similar to past work (Pasupat et al., 2021), we use the English subset of MTOP, containing $16 \mathrm{~K} / 2 \mathrm{~K} / 4 \mathrm{~K}$ examples in its training/development/test sets.</li>
<li>SMCALFLOW (Andreas et al., 2020): A large English-language task-oriented dataset that covers tasks such as calendar, weather, places, and people. The meaning representation is a dataflow program, which includes API calls, function composition and complex constraints. SMCALFLOW includes 15 K development set examples and 134 K training examples, from which we sample a random set of 44 K examples for training.</li>
</ul>
<h3>4.2 Baselines and Oracles</h3>
<p>We consider the following unsupervised baselines, which are applied at test time only.</p>
<ul>
<li>RANDOM: we randomly sample examples from the training set $\mathcal{D}$.</li>
<li>SBERT: We use SentenceTransformers, a library providing BERT-based sentence embeddings. ${ }^{3}$ Specifically, we use paraphrase-mpnet-base-v2, a 110M parameter model to encode the test utterance $x_{\text {test }}$ and retrieve the examples with the most similar utterances as in-context examples.</li>
<li>BM25: We use the classical sparse retrieval method BM25 (Robertson and Zaragoza, 2009), which is an extension of TF-IDF, to retrieve for</li>
</ul>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Size</th>
<th style="text-align: center;">Utterance</th>
<th style="text-align: center;">Meaning Representation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BREAK</td>
<td style="text-align: center;">60 K</td>
<td style="text-align: center;">There are more birds in the image on the right than in the image on the left.</td>
<td style="text-align: center;">1) return right image; <br> 2) return birds in #1; <br> 3) return number of #2; <br> 4) return left image; <br> 5) return birds in #4 <br> 6) return number of #5; <br> 7) return if #3 is higher than #6;</td>
</tr>
<tr>
<td style="text-align: center;">MTOP</td>
<td style="text-align: center;">22 K</td>
<td style="text-align: center;">call Zoey's wife.</td>
<td style="text-align: center;">[IN:CREATE_CALL = <br> [SL:CONTACT = [IN:GET_CONTACT = <br> [SL:CONTACT_RELATED = Zoey] <br> [SL:TYPE_RELATION = wife]]]]</td>
</tr>
<tr>
<td style="text-align: center;">SMCALFLOW</td>
<td style="text-align: center;">148 K</td>
<td style="text-align: center;">Can you create me a new meeting on thursday morning?</td>
<td style="text-align: center;">(Yield (CreateCommitEventWrapper <br> (CreatePreflightEventWrapper <br> (Event.start_? <br> (DateTimeConstraint (Morning) <br> (NextDOW (Thursday)))))))</td>
</tr>
</tbody>
</table>
<p>Table 1: Examples from each of the datasets we evaluate on.
each test utterance $x_{\text {test }}$ the training examples with the most similar utterance.</p>
<ul>
<li>BrUTEFORCE: We apply the prompt selection method for few-shot semantic parsing from Shin et al. (2021). Given a test example $x_{\text {test }}$, we sample 200 training examples. For each training example $\left(x_{i}, y_{i}\right)$, compute $\operatorname{Prob}<em _test="{test" _text="\text">{g}\left(x</em>\right)$, and use the highest scoring examples for the prompt. Similar to us, this approach uses the inference LM to choose prompts. However, it does so at test time, which results in slow inference.
Next, we describe baselines that use the training set, $\mathcal{D}$, to train a prompt retriever. All supervised methods share the following template. First, a candidate set $\overline{\mathcal{E}}$ of $L=50$ examples is retrieved with the unsupervised retriever $R_{u}(y, \mathcal{D})$. We use BM25 as an unsupervised retriever, since it outperformed SBERT (see 4.4). We then score each candidate prompt $\bar{e}_{l} \in \overline{\mathcal{E}}$ with some scoring function, and label the top- $k$ prompts as positive examples and the bottom- $k$ as negative examples $(k=5)$. Different supervised methods only differ in the scoring function itself. ${ }^{4}$}} \mid x_{i</li>
<li>DR-BM25: Here, we use the original BM25 scores for labeling positive and negative examples and training a dense retriever.</li>
<li>CASE-BASED REASONING (CBR): We adapt the scoring function from Das et al. (2021), which focused on knowledge-base question answering. They define the weight for a pair of logical forms to be the $\mathrm{F}_{1}$ score between the two sets of relations appearing in those logical forms, and use this weight to softly label their data. Since in our setting we do not assume logical forms,</li>
</ul>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>we define the score between two output sequence $y_{i}$ and $y_{j}$ to be the $\mathrm{F}<em i="i">{1}$ between the two sets of tokens in $y</em>$, omitting stop words.}$ and $y_{j</p>
<ul>
<li>Efficient Prompt Retrieval (EPR): Our full approach from $\S 3$.</li>
</ul>
<p>Last, we also consider two oracle models.</p>
<ul>
<li>BM25-OraCLE: We score test examples with BM25 using the gold output sequence $R_{\mathrm{BM} 25}\left(y_{\text {test }}, \mathcal{D}\right)$. This provides an upper-bound on what can be learned by DR-BM25. EPR can potentially outperform this oracle, since its training signal goes beyond surface text similarity.</li>
<li>LM-Oracle: We use the procedure for labeling training data at test time. Given a test example $\left(x_{\text {test }}, y_{\text {test }}\right)$, we first retrieve $L$ candidate training examples with $R_{\mathrm{BM} 25}\left(y_{\text {test }}, \mathcal{D}\right)$, we then sort the candidates with the scoring LM $\hat{g}$, estimating the probability of $y_{\text {test }}$ given $x_{\text {test }}$ and the candidate prompt. This provides an upper bound for EPR, since EPR is trained to emulate this behaviour.</li>
</ul>
<h3>4.3 Experimental Details</h3>
<p>Language models In this work, we only train a dense retriever, but use scoring and inference LMs. For our scoring LM, $\hat{g}$, we use GPT-NEO (Black et al., 2021), a 2.7B-parameter LM trained on The Pile (Gao et al., 2021), an 825 GB English text corpus, constructed from a wide range of highquality resources.</p>
<p>In addition, we consider the following inference LMs: (a) GPT-J (Wang and Komatsuzaki, 2021): a 6B-parameter LM, also trained on The Pile. The advantage in this setup, is that GPT-J was trained on the same corpus as GPT-NEO. (b) GPT-3 (Brown et al., 2020): A 175B-parameter</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">BREAK</th>
<th style="text-align: center;">MTOP</th>
<th style="text-align: center;">SMCALFLOW</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Unsuper.</td>
<td style="text-align: center;">RANDOM</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">8.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SBERT</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">43.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BM25</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">46.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BRUTEFORCE</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">18.1</td>
<td style="text-align: center;">11.1</td>
</tr>
<tr>
<td style="text-align: center;">Super.</td>
<td style="text-align: center;">DR-BM25</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">43.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CBR</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">57.0</td>
<td style="text-align: center;">51.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EPR (ours)</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">64.2</td>
<td style="text-align: center;">54.3</td>
</tr>
<tr>
<td style="text-align: center;">Oracle</td>
<td style="text-align: center;">BM25-ORACLE</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">47.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LM-ORACLE</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">73.7</td>
</tr>
</tbody>
</table>
<p>Table 2: Development results when GPT-NEO is the scoring and inference LM. Numbers for BREAK are LF-EM, and for MTOP and SMCALFLOW are EM.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">BREAK</th>
<th style="text-align: left;">MTOP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Unsuper.</td>
<td style="text-align: left;">BM25</td>
<td style="text-align: left;">17.6</td>
<td style="text-align: left;">49.0</td>
</tr>
<tr>
<td style="text-align: left;">Super.</td>
<td style="text-align: left;">CBR</td>
<td style="text-align: left;">18.4</td>
<td style="text-align: left;">57.5</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">EPR (ours)</td>
<td style="text-align: left;">$\mathbf{2 3 . 9}$</td>
<td style="text-align: left;">$\mathbf{6 4 . 4}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Test results where GPT-NEO is the scoring and inference LM. Numbers for BREAK are NEM, the official metric, and for MTOP are EM.
model, trained mostly on a filtered subset of common crawl. (c) CODEX (Chen et al., 2021): A GPT-3 175B-parameter model finedtuned on code from GitHub. Since our tasks involve mapping from utterances to programs or meaning representations, CODEX might potentially perform well at in-context learning.</p>
<p>For all LMs, we use a maximum context size of $C=2,048$ tokens.</p>
<p>Evaluation On BREAK, we evaluate performance on the development set with LF-EM (Hasson and Berant, 2021), which is a better metric compared to Normalized Exact Match (NEM), the official metric, as it measures whether two meaning representations are semantically equivalent. On the test set, we use NEM. On MTOP and SMCALFLOW, we evaluate with Exact Match (EM), i.e., whether the string output by the inference LM is identical to the reference string.</p>
<p>We evaluate EPR in two settings: (a) LM-as-aservice, and (b) LM-as-a-proxy. In the first setting, we use GPT-NEO as both the scoring LM and inference LM. In this setting, we evaluate on the full development sets of BREAK, MTOP, and SMCALFLOW. In the latter setting, as we access GPT-3 and CODEX through a paid API, we sample a random subset of 1,000 development examples from each dataset and evaluate each model once on this subset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">One-shot</th>
<th style="text-align: center;">Full-context</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Unsuper.</td>
<td style="text-align: center;">RANDOM</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">1.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BM25</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">26.0</td>
</tr>
<tr>
<td style="text-align: center;">Super.</td>
<td style="text-align: center;">DR-BM25</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">23.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CBR</td>
<td style="text-align: center;">14.5</td>
<td style="text-align: center;">25.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EPR</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">31.9</td>
</tr>
<tr>
<td style="text-align: center;">Oracle</td>
<td style="text-align: center;">BM25-ORACLE</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">32.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LM-ORACLE</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">43.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ANYCORRECT-ORACLE</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 4: Development results on BREAK with GPTNeO in the one-shot setting. Numbers are LF-EM. Fullcontext is the corresponding numbers from Table 2.</p>
<h3>4.4 Results</h3>
<p>LM-as-a-service Table 2 reports results where the scoring and inference LMs are identical. EPR substantially outperforms all other baselines. Specifically, when comparing to the best baseline, it improves performance from 26.0 to 31.9 on BREAK, from 57.0 to 64.2 on MTOP, and from 51.4 to 54.3 on SMCALFLOW. This shows that using the LM itself to label examples is an effective approach for obtaining a strong prompt retriever. Table 3 shows test results on BREAK and MTOP corroborating that EPR substantially improves performance compared to BM25 and CBR.</p>
<p>For the unsupervised methods, the RANDOM baseline shows that random sampling of training examples leads to poor performance. BM25 outperforms SBERT for prompt retrieval, and consequently we use BM25 in all of our supervised approaches to retrieve the set of candidates, $\tilde{\mathcal{E}}$. Last, BRUTEFORCE performs worse than BM25. We assume this is since the training sets are large ( $\sim 14$ 120 K examples), and sampling 200 examples does not cover examples that are useful for GPT-NEO.</p>
<p>Interestingly, EPR outperforms BM25-ORACLE on MTOP and SMCALFLOW and is comparable on BREAK. This is surprising since BM25-ORACLE has access to the output sequence $y_{\text {test }}$ at test time, illustrating that the signal provided by the scoring LM for training goes beyond surface text similarity. The performance of LM-ORACLE is substantially higher than EPR, showing that the supervision provided by the scoring LM is strong, and training a better retriever from this signal can substantially enhance performance.</p>
<p>We further evaluate our models in the one-shot setup, i.e., when the prompt given to the inference LM includes the highest scoring example only. In this setup, the inference LM is applied in the same setting as when we generate labeled data, where we go over each prompt candidate independently.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">BREAK</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MTOP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">RANDOM</td>
<td style="text-align: center;">BM25</td>
<td style="text-align: center;">CBR</td>
<td style="text-align: center;">EPR</td>
<td style="text-align: center;">RANDOM</td>
<td style="text-align: center;">BM25</td>
<td style="text-align: center;">CBR</td>
<td style="text-align: center;">EPR</td>
<td style="text-align: center;">RANDOM</td>
<td style="text-align: center;">BM25</td>
<td style="text-align: center;">CBR</td>
<td style="text-align: center;">EPR</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">4.2</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">25.3</td>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">52.5</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">46.5</td>
</tr>
<tr>
<td style="text-align: center;">CODEX</td>
<td style="text-align: center;">8.9</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">10.8</td>
<td style="text-align: center;">60.6</td>
<td style="text-align: center;">59.4</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">50.3</td>
</tr>
<tr>
<td style="text-align: center;">GPT-J</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">8.8</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">57.4</td>
</tr>
<tr>
<td style="text-align: center;">GPT-NEO</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">25.8</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">53.5</td>
</tr>
</tbody>
</table>
<p>Table 5: Results on a random sample of 1,000 examples from the development set when using GPT-Neo as a scoring LM across different inference LMs and datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">EPR</th>
<th style="text-align: center;">CBR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Test <br> Example</td>
<td style="text-align: center;">Utterance</td>
<td style="text-align: center;">Give the code of the airport with the least flights.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Meaning <br> Representation</td>
<td style="text-align: center;">1) airports <br> 2) flights of #1 <br> 3) number of #2 for each #1 <br> 4) #1 where #3 is lowest <br> 5) code of #4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Top-1</td>
<td style="text-align: center;">Utterance</td>
<td style="text-align: center;">What is the code of the city with the most students?</td>
<td style="text-align: center;">What destination has the fewest number of flights?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Meaning <br> Representation</td>
<td style="text-align: center;">1) cities <br> 2) students in #1 <br> 3) number of #2 for each #1 <br> 4) #1 where #3 is highest <br> 5) code of #4</td>
<td style="text-align: center;">1) destinations <br> 2) flights of #1 <br> 3) number of #2 for each #1 <br> 4) #1 where #3 is lowest</td>
</tr>
<tr>
<td style="text-align: center;">Top-2</td>
<td style="text-align: center;">Utterance</td>
<td style="text-align: center;">Return the code of the city that has the most students.</td>
<td style="text-align: center;">Which destination has least number of flights?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Meaning <br> Representation</td>
<td style="text-align: center;">1) cities <br> 2) students in #1 <br> 3) number of #2 for each #1 <br> 4) #1 where #3 is highest <br> 5) code of #4</td>
<td style="text-align: center;">1) destinations <br> 2) flights to #1 <br> 3) number of #2 for each #1 <br> 4) #1 where #3 is lowest</td>
</tr>
<tr>
<td style="text-align: center;">Top-3</td>
<td style="text-align: center;">Utterance</td>
<td style="text-align: center;">Find the count and code of the job has most employees.</td>
<td style="text-align: center;">What is the number of airports per country, ordered from most to least?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Meaning <br> Representation</td>
<td style="text-align: center;">1) jobs <br> 2) employees of #1 <br> 3) number of #2 for each #1 <br> 4) #1 where #3 is highest <br> 5) employees of #4 <br> 6) number of #5 <br> 7) code of #4 <br> 8) #6, #7</td>
<td style="text-align: center;">1) countries <br> 2) airports in #1 <br> 3) number of #2 for each #1 <br> 4) #3 sorted by most to least</td>
</tr>
</tbody>
</table>
<p>Table 6: An example from BREAK development set where EPR is correct and CBR is incorrect along with the top-3 training examples retrieved from each retriever.</p>
<p>Since train and test time are now closer, we can expect the advantage of EPR to be more pronounced.</p>
<p>Table 4 shows the results. Indeed, EPR outperforms the best baseline by $8.5 \%$, and BM25ORACLE by $5 \%$. In addition, we examine ANYCORRECT-ORACLE, which tests whether any of the candidates returned by BM25 leads to the correct output. ANYCORRECT-ORACLE reaches $53.6 \%, 20$ points above LM-Oracle. This shows the high quality of candidates provided by BM25 (applied on the $y$ ), as one can reach more than $50 \%$ LF-EM with just a single prompt. Moreover, it hints that a better scoring function can potentially further improve performance.</p>
<p>LM-as-a-proxy Table 5 shows results when the scoring LM is GPT-NEO and the inference LM is a larger LM. First, the trends are similar to the LM-as-
a-service setup, i.e., EPR substantially outperforms prior baselines, including our best unsupervised baseline, BM25, and the best supervised baseline, CBR, by 2-8 points on all datasets and all pretrained models. Thus, GPT-NEO serves as a good proxy for choosing training examples.</p>
<p>To further validate this finding, we evaluate the performance of GPT-J on BREAK with GPT-NEO as the scoring LM compared to using GPT-J itself as the scoring LM. We find performance improves slightly from 31.5 to 33.6. Analogously, when using CODEX as the scoring LM and inference LM performance remains roughly the same: $29.5 \rightarrow 29.3$. Thus, using a smaller LM (GPT-NEO) is an effective strategy for training a retriever that will be applied on other LMs. Zooming in on different inference LMs, GPT-J performs slightly better than GPT-NEO across the board, since it was</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: A t-SNE projection and clustering of the representations learned by EPR for the training examples in BREAK. An interactive version displaying individual examples is available here.
trained on the same data and using the same procedure as GPT-NEO. CODEX outperforms GPT3, which can be explained by the fact that it was trained on code, and our datasets involve mapping to programs or meaning representations. Surprisingly, GPT-J outperforms CODEX (except on MTOP) and GPT-3 despite being 30x smaller. This can perhaps be explained by the fact that GPT-J was trained on a different dataset (The Pile (Gao et al., 2021)).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Pattern</th>
<th style="text-align: center;">Copied</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Novel</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Rate</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Rate</td>
<td style="text-align: center;">Acc</td>
</tr>
<tr>
<td style="text-align: left;">BREAK</td>
<td style="text-align: left;">Exact</td>
<td style="text-align: center;">$55.1 \%$</td>
<td style="text-align: center;">$10.4 \%$</td>
<td style="text-align: center;">$29.7 \%$</td>
<td style="text-align: center;">$89.6 \%$</td>
<td style="text-align: center;">$32.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Abstract</td>
<td style="text-align: center;">$58.0 \%$</td>
<td style="text-align: center;">$41.1 \%$</td>
<td style="text-align: center;">$14.5 \%$</td>
<td style="text-align: center;">$58.9 \%$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">MTOP</td>
<td style="text-align: left;">Exact</td>
<td style="text-align: center;">$77.3 \%$</td>
<td style="text-align: center;">$25.3 \%$</td>
<td style="text-align: center;">$59.7 \%$</td>
<td style="text-align: center;">$74.7 \%$</td>
<td style="text-align: center;">$64.2 \%$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Abstract</td>
<td style="text-align: center;">$71.6 \%$</td>
<td style="text-align: center;">$84.5 \%$</td>
<td style="text-align: center;">$23.4 \%$</td>
<td style="text-align: center;">$15.5 \%$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">SMCAL</td>
<td style="text-align: left;">Exact</td>
<td style="text-align: center;">$62.5 \%$</td>
<td style="text-align: center;">$60.2 \%$</td>
<td style="text-align: center;">$42.4 \%$</td>
<td style="text-align: center;">$39.8 \%$</td>
<td style="text-align: center;">$54.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Abstract</td>
<td style="text-align: center;">$62.4 \%$</td>
<td style="text-align: center;">$81.2 \%$</td>
<td style="text-align: center;">$20.6 \%$</td>
<td style="text-align: center;">$18.8 \%$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 7: Accuracy comparison between the decoded instances that contained patterns from the prompt and novel instances those that don't. Results shown are on the LM-as-a-service setup using GPT-NEO.</p>
<p>Analysis Table 6 shows an example from BREAK where EPR decodes the correct output, while CBR does not. All training examples retrieved by EPR perform an argmax (argmin in the original utterance), and return in the final step " $a$ code", while the third example retrieved by CBR does not perform an argmax or argmin, and do not involve " $a$ code". We provide additional examples in App. A.</p>
<p>Figure 3 shows a t-SNE (Hinton and Roweis, 2002) projection of the embeddings learned by EPR for the training examples of BREAK, with a link to an interactive version, where we applied the OPTICS (Ankerst et al., 1999; Schubert and Gertz, 2018) clustering algorithm. Examining clusters
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: On the subset of copied patterns we plot the distribution of the distance from the test instance to the example containing the pattern. Shown on the BREAK validation set using EPR in the LM-as-a-service setup using GPT-NEO. Note that the y-axis is in log-scale.
shows that EPR captures both lexical and structure similarity. Examples for clusters are also available in App. A.</p>
<p>Prompt copying We analyze how the LM utilizes in-context prompts. Specifically, is the target output copied from one of the prompts or is it a composition of different prompt fragments, which result in generalization to new structures.</p>
<p>To achieve this, we define two types of copying. (a) Exact copying measures if the generated output exactly matches one of the examples in the prompt, and (b) Abstract copying, that quantifies if the structure of the decoded output matches any of the structures seen in the prompt. Specifically, we eliminate the effect of non-structural elements such as entities and function arguments. We replace every sequence of words in the logical form that appears in the input utterance with the string [MASKED] for both the target utterance and incontext examples. If the masked logical form that the LM decoded appears in the set of masked examples defined by the prompt, we say that the LM copied that abstract pattern.</p>
<p>Table 7 presents the results on the validation set for each of our three datasets, as well as the accuracy on each subset. We observe that the rate of copying is much higher in MTOP and SMCALFlow compared to BREAK, where in MTOP and SMCALFlow abstract copying reaches more than $80 \%$. Moreover, accuracy on examples where copying occurred is much higher compared to accuracy where no copying happened. For example, on MTOP, $84.5 \%$ of the examples were abstractly copied, and on that subset of examples, EPR achieves $71.6 \%$ EM, compared to $64.2 \%$ on</p>
<p>the entire validation set. Nevertheless, even though accuracy is much lower in cases where no copying occurred, accuracy is not negligible, which shows that some form of generalization to new structures is taking place.</p>
<p>Another follow-up question is whether the model copies patterns from prompts uniformly or does it attend mostly to the ones with high retrieval score. To answer this, we look at the subset of examples where copying occurred. We then identify for each example the highest-ranking prompt that was copied from, and define the distance of that prompt by dividing the rank by the number of prompts that fit in that example. Figure 4 shows the distribution over distances for the BREAK dataset. We observe that copying happens mostly from highly-ranked prompts.</p>
<h2>5 Related Work</h2>
<p>In-context learning Our understanding of incontext learning has grown substantially recently. Saunshi et al. (2021) suggests that by conditioning on a prompt, the task of predicting the next word approaches linear separability. Xie et al. (2021) suggests that in-context learning occurs when the model infers a shared latent concept between examples in a prompt. Levine et al. (2021) present a pre-training scheme theoretically motivated by the bias of in-context learning, that gives significant improvements. Recently, Min et al. (2022) showed that the model does not rely on the ground truth input-label mapping provided in the demonstrations as much as previously thought.</p>
<p>Retrieval Research on training dense retrievers has skyrocketed recently, propelled by interest in open-domain question answering (Chen et al., 2017; Lee et al., 2019; Karpukhin et al., 2020; Guu et al., 2020; Khattab and Zaharia, 2020; Qu et al., 2021). Work on retrieval-based methods has also spread more widely to other knowledge-intensive tasks (Lewis et al., 2020), e.g., fact verification (Samarinas et al., 2021).</p>
<p>Similar to us, Pasupat et al. (2021) proposed to use retrieval in semantic parsing. However, they focus on controlling the output generated by a model. Retrieval methods have also been successfully used in language modeling (Khandelwal et al., 2020; Borgeaud et al., 2021; Alon et al., 2022) and machine translation (Khandelwal et al., 2021).</p>
<p>Prompts Developing methods for interacting with LMs and extracting desired behaviours has attracted considerable attention, under the umbrella term prompting. In this work, prompts are a set of in-context training examples, but substantial effort has also been devoted to casting natural language tasks as language modeling by phrasing the target task in natural language (see survey in (Liu et al., 2021b)). Such approaches include prompt engineering through manual patterns (Petroni et al., 2019; Schick and Schtze, 2021), decoding methods (Min et al., 2021; Zhao et al., 2021; Holtzman et al., 2021), and methods for extracting either hard (Shin et al., 2020; Haviv et al., 2021) or soft (Li and Liang, 2021; Zhong et al., 2021; Qin and Eisner, 2021) prompts automatically.</p>
<p>Prompt retrieval for supervised models In parallel to this work, adding training examples as additional input has been shown to be useful for supervised models as well. Wang et al. (2022) and Xu et al. (2021) used BM25 to retrieve and augment the input with similar examples from the training set. Fine-tuning the model with the additional inputs improved performance on tasks such as summarization and question answering. Such methods can also potentially benefit from a stronger retriever.</p>
<h2>6 Conclusions</h2>
<p>Large pre-trained LMs are becoming an inseparable part of the natural language understanding ecosystem. However, accessing their weights or updating them can be prohibitive for many researchers. In this work, we propose EPR, a method for learning to retrieve good prompts for in-context learning, by using language models themselves as the scoring function. This allows us to train a light-weight retriever and substantially improve performance on three challenging tasks.</p>
<p>More broadly, given that large LMs models are likely to play a prominent role in developing language understanding models, it is important to develop approaches for interacting with such models effectively. EPR can be viewed as a step in this direction.</p>
<h2>Acknowledgement</h2>
<p>We thank Ori Ram and Itay Itzhak for helpful suggestions and meaningful discussions. This research was supported in part by The Yandex Initiative for Machine Learning, and The European Research</p>
<p>Council (ERC) under the European Union Horizons 2020 research and innovation programme (grant ERC DELPHI 802800). This work was completed in partial fulfillment for the Ph.D degree of Ohad Rubin.</p>
<h2>References</h2>
<p>Uri Alon, Frank F. Xu, Junxian He, Sudipta Sengupta, Dan Roth, and Graham Neubig. 2022. Neuro-symbolic language modeling with automatonaugmented retrieval. ArXiv, abs/2201.12431.</p>
<p>Jacob Andreas, John Bufe, David Burkett, Charles Chen, Josh Clausman, Jean Crawford, Kate Crim, Jordan DeLoach, Leah Dorner, Jason Eisner, Hao Fang, Alan Guo, David Hall, Kristin Hayes, Kellie Hill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan Klein, Jayant Krishnamurthy, Theo Lanman, Percy Liang, Christopher H. Lin, Ilya Lintsbakh, Andy McGovern, Aleksandr Nisnevich, Adam Pauls, Dmitrij Petters, Brent Read, Dan Roth, Subhro Roy, Jesse Rusak, Beth Short, Div Slomin, Ben Snyder, Stephon Striplin, Yu Su, Zachary Tellman, Sam Thomson, Andrei Vorobev, Izabela Witoszko, Jason Wolfe, Abby Wray, Yuchen Zhang, and Alexander Zotov. 2020. Task-oriented dialogue as dataflow synthesis. Transactions of the Association for Computational Linguistics, 8:556-571.</p>
<p>Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel, and Jrg Sander. 1999. Optics: Ordering points to identify the clustering structure. SIGMOD Rec., 28(2):49-60.</p>
<p>Emily M. Bender, Timnit Gebru, Angelina McMillanMajor, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21, page 610-623, New York, NY, USA. Association for Computing Machinery.</p>
<p>Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow.</p>
<p>Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. 2021. Improving language models by retrieving from trillions of tokens. arXiv preprint arXiv:2112.04426.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,</p>
<p>Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer opendomain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870-1879, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea. Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, David W. Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, S. Arun Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. ArXiv preprint, abs/2107.03374.</p>
<p>Rajarshi Das, Manzil Zaheer, Dung Thai, Ameya Godbole, Ethan Perez, Jay Yoon Lee, Lizhen Tan, Lazaros Polymenakos, and Andrew McCallum. 2021. Casebased reasoning for natural language queries over knowledge bases. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9594-9611, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Nan Du, Yanping Huang, Andrew M. Dai, Dmitry Lepikhin Simon Tong, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathy MeierHellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng Chen, and Claire Cui. 2021. GLaM: Efficient scaling of language</p>
<p>models with mixture-of-experts. arXiv preprint arXiv:2112.06905.</p>
<p>Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. 2021. The pile: An 800gb dataset of diverse text for language modeling. ArXiv preprint, abs/2101.00027.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Retrieval augmented language model pre-training. In ICML.</p>
<p>Matan Hasson and Jonathan Berant. 2021. Question decomposition with dependency graphs. ArXiv, abs/2104.08647.</p>
<p>Adi Haviv, Jonathan Berant, and Amir Globerson. 2021. BERTese: Learning to speak to BERT. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 3618-3623, Online. Association for Computational Linguistics.</p>
<p>Matthew Henderson, Rami Al-Rfou, Brian Strope, Yun hsuan Sung, Laszlo Lukacs, Ruiqi Guo, Sanjiv Kumar, Balint Miklos, and Ray Kurzweil. 2017. Efficient natural language response suggestion for smart reply.</p>
<p>Geoffrey E. Hinton and Sam T. Roweis. 2002. Stochastic neighbor embedding. In NIPS.</p>
<p>Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. 2021. Surface form competition: Why the highest probability answer isn't always right. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7038-7051, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Jeff Johnson, Matthijs Douze, and Herv Jgou. 2017. Billion-scale similarity search with gpus. ArXiv preprint, abs/1702.08734.</p>
<p>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769-6781, Online. Association for Computational Linguistics.</p>
<p>Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2021. Nearest Neighbor Machine Translation. In Proceedings of ICLR.</p>
<p>Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through memorization: Nearest neighbor language models. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pages 3948.</p>
<p>Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proceedings of ICLR.</p>
<p>Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086-6096, Florence, Italy. Association for Computational Linguistics.</p>
<p>Yoav Levine, Noam Wies, Daniel Jannai, Daniel I. Navon, Yedid Hoshen, and Amnon Shashua. 2021. The inductive bias of in-context learning: Rethinking pretraining example design. ArXiv, abs/2110.04541.</p>
<p>Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kttler, Mike Lewis, Wen-tau Yih, Tim Rocktschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Proceedings of NeurIPS.</p>
<p>Haoran Li, Abhinav Arora, Shuohui Chen, Anchit Gupta, Sonal Gupta, and Yashar Mehdad. 2021. MTOP: A comprehensive multilingual task-oriented semantic parsing benchmark. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2950-2962, Online. Association for Computational Linguistics.</p>
<p>Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 45824597, Online. Association for Computational Linguistics.</p>
<p>Opher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. 2021. Jurassic-1: Technical details and evaluation. White Paper. AI21 Labs.</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021a. What makes good in-context examples for gpt-3? ArXiv preprint, abs/2101.06804.</p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021b. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing.</p>
<p>Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2021. Noisy channel language model prompting for few-shot text classification. arXiv preprint.</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? ArXiv, abs/2202.12837.</p>
<p>Panupong Pasupat, Yuan Zhang, and Kelvin Guu. 2021. Controllable semantic parsing via retrieval augmentation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7683-7698, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Fabio Petroni, Tim Rocktschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463-2473, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying LMs with mixtures of soft prompts. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5203-5212, Online. Association for Computational Linguistics.</p>
<p>Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2021. RocketQA: An optimized training approach to dense passage retrieval for opendomain question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5835-5847, Online. Association for Computational Linguistics.</p>
<p>Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis \&amp; insights from training gopher. arXiv preprint arXiv:2112.11446.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67.</p>
<p>Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982-3992, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Stephen Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval, 3:333-389.</p>
<p>Chris Samarinas, Wynne Hsu, and Mong Li Lee. 2021. Improving evidence retrieval for automated explainable fact-checking. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations, pages 84-91, Online. Association for Computational Linguistics.</p>
<p>Nikunj Saunshi, Sadhika Malladi, and Sanjeev Arora. 2021. A mathematical exploration of why language models help solve downstream tasks. In International Conference on Learning Representations.</p>
<p>Timo Schick and Hinrich Schtze. 2021. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 255-269, Online. Association for Computational Linguistics.</p>
<p>Erich Schubert and Michael Gertz. 2018. Improving the cluster structure extracted from optics plots. In $L W D A$.</p>
<p>Richard Shin, Christopher Lin, Sam Thomson, Charles Chen, Subhro Roy, Emmanouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, and Benjamin Van Durme. 2021. Constrained language models yield few-shot semantic parsers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7699-7715, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4222-4235, Online. Association for Computational Linguistics.</p>
<p>Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/ kingoflolz/mesh-transformer-jax.</p>
<p>Shuo Wang, Yichong Xu, Yuwei Fang, Yang Liu, S. Sun, Ruochen Xu, Chenguang Zhu, and Michael Zeng. 2022. Training data is more valuable than you think: A simple and effective method by retrieving from training data. ArXiv, abs/2203.08773.</p>
<p>Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gardner, Yoav Goldberg, Daniel Deutch, and Jonathan Berant. 2020. Break it down: A question understanding benchmark. Transactions of the Association for Computational Linguistics, 8:183-198.</p>
<p>Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2021. An explanation of in-context learning as implicit bayesian inference. ArXiv, abs/2111.02080.</p>
<p>Yichong Xu, Chenguang Zhu, Shuohang Wang, Siqi Sun, Hao Cheng, Xiaodong Liu, Jianfeng Gao, Pengcheng He, Michael Zeng, and Xuedong Huang. 2021. Human parity on commonsenseqa: Augmenting self-attention with external attention.</p>
<p>Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In ICML, pages 12697-12706.</p>
<p>Zexuan Zhong, Dan Friedman, and Danqi Chen. 2021. Factual probing is [MASK]: Learning vs. learning to recall. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5017-5033, Online. Association for Computational Linguistics.</p>
<h2>A Appendix</h2>
<h2>Distribution of the number of in-context examp</h2>
<p>ples Since the selection procedure for in-context examples is dynamic, the number of in-context examples differs for each test instance. In Figure 5, we plot the histogram of the number of examples we fit in $C=2,048$ tokens.</p>
<p>Effect of hyperparameters We test the effect of $k$, the number of prompts labeled as positive or negative, and $L$, the number of prompts retrieved by the unsupervised retriever. Table 8 shows that performance is is generally robust w.r.t these hyperparameters.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">BREAK</th>
<th style="text-align: left;">MTOP</th>
<th style="text-align: left;">SMCALFLOW</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$k=1$</td>
<td style="text-align: left;">$31.5 \%$</td>
<td style="text-align: left;">$63.0 \%$</td>
<td style="text-align: left;">$54.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">$k=5$</td>
<td style="text-align: left;">$31.9 \%$</td>
<td style="text-align: left;">$64.2 \%$</td>
<td style="text-align: left;">$54.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">$k=10$</td>
<td style="text-align: left;">$31.0 \%$</td>
<td style="text-align: left;">$64.1 \%$</td>
<td style="text-align: left;">$52.2 \%$</td>
</tr>
<tr>
<td style="text-align: left;">$L=50$</td>
<td style="text-align: left;">$31.9 \%$</td>
<td style="text-align: left;">$64.2 \%$</td>
<td style="text-align: left;">$54.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">$L=100$</td>
<td style="text-align: left;">$32.3 \%$</td>
<td style="text-align: left;">$63.7 \%$</td>
<td style="text-align: left;">$51.0 \%$</td>
</tr>
</tbody>
</table>
<p>Table 8: In the LM-as-a-service setup, using GPT-Neo, we search for other values for $L$ and $k$, and note that the choice of our hyperparameters is robust.</p>
<p>Training details To train EPR, we use the Adam optimizer (Kingma and Ba, 2015) with batch size 120 and learning rate 1e-4 on eight RTX 3090. We run training for 30 epochs. We used the default DPR hyperparameters without tuning. We used the final epoch of the model to perform model selection, and applied minimal learning rate tuning on the validation set of BREAK.</p>
<p>Risk assessment Large language models have been shown to exhibit various kinds of bias (Bender et al., 2021), since EPR is trained on the signal obtained from such large LMs, it might also exhibit these biases.</p>
<p>Additional examples Tables 9, 10, and 11 provide more examples for cases where EPR is correct while CBR is incorrect along with the top-3 prompts for each method.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Distribution of the number of in-context examples per test instance for each of the datasets. We mark the distribution mean using a dashed line.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">EPR</th>
<th style="text-align: center;">CBR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Test <br> Example</td>
<td style="text-align: center;">Utterance</td>
<td style="text-align: center;">Remind me to add 2 dozen eggs to my grocery list.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Meaning Representation</td>
<td style="text-align: center;">[IN:CREATE_REMINDER [SL:PERSON_REMINDED me ] [SL:TODO add 2 dozen eggs to my grocery list ] ]</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Top-1</td>
<td style="text-align: center;">Utterance</td>
<td style="text-align: center;">Remind me to get two bottles of water.</td>
<td style="text-align: center;">Please add a grocery list to my list of things to be reminded about doing today.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Meaning Representation</td>
<td style="text-align: center;">[IN:CREATE_REMINDER [SL:PERSON_REMINDED me ] [SL:TODO get two bottles of water ] ]</td>
<td style="text-align: center;">[IN:CREATE_REMINDER [SL:TODO a grocery list ] [SL:PERSON_REMINDED my ] [SL:DATE_TIME today ] ]</td>
</tr>
<tr>
<td style="text-align: center;">Top-2</td>
<td style="text-align: center;">Utterance</td>
<td style="text-align: center;">Remind me to bring an extra pair of shoes to the river.</td>
<td style="text-align: center;">Remind me to make a grocery list</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Meaning Representation</td>
<td style="text-align: center;">[IN:CREATE_REMINDER [SL:PERSON_REMINDED me ] [SL:TODO bring an extra pair of shoes to the river ] ]</td>
<td style="text-align: center;">[IN:CREATE_REMINDER [SL:PERSON_REMINDED me ] [SL:TODO make a grocery list ] ]</td>
</tr>
<tr>
<td style="text-align: center;">Top-3</td>
<td style="text-align: center;">Utterance</td>
<td style="text-align: center;">Remind me to add bottled water to grocery list.</td>
<td style="text-align: center;">I need to make a grocery list; will you remind me when I get off work at 5:00 p.m.?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Meaning Representation</td>
<td style="text-align: center;">[IN:CREATE_REMINDER [SL:PERSON_REMINDED me ] [SL:TODO add bottled water to grocery list ] ]</td>
<td style="text-align: center;">[IN:CREATE_REMINDER [SL:TODO make a grocery list ] [SL:PERSON_REMINDED me ] [SL:DATE_TIME at 5 : 00 p.m . ] ]</td>
</tr>
</tbody>
</table>
<p>Table 9: An example from MTOP development set where EPR is correct and CBR is incorrect along with the top-3 training examples retrieved from each retriever.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">EPR</th>
<th style="text-align: center;">CBR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Test <br> Example</td>
<td style="text-align: center;">Utterance</td>
<td style="text-align: center;">confirmed thanks</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Meaning Representation</td>
<td style="text-align: center;">(PleasantryAnythingElseCombined)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Top-1</td>
<td style="text-align: center;">Utterance</td>
<td style="text-align: center;">it's ok bye</td>
<td style="text-align: center;">Yes, but make sure to let me know the weather for that time.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Meaning Representation</td>
<td style="text-align: center;">(PleasantryAnythingElseCombined)</td>
<td style="text-align: center;">(let (x0 (Execute ("(Dynamic) ConfirmAndReturnAction))) (do (Yield x0) (Yield (WeatherForEvent ("(Dynamic) item x0)))))</td>
</tr>
<tr>
<td style="text-align: center;">Top-2</td>
<td style="text-align: center;">Utterance</td>
<td style="text-align: center;">It's ok</td>
<td style="text-align: center;">Awesome, perfect</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Meaning Representation</td>
<td style="text-align: center;">(PleasantryAnythingElseCombined)</td>
<td style="text-align: center;">(Yield (Execute ("(Dynamic) ConfirmAndReturnAction)))</td>
</tr>
<tr>
<td style="text-align: center;">Top-3</td>
<td style="text-align: center;">Utterance</td>
<td style="text-align: center;">It's ok</td>
<td style="text-align: center;">Perfect...</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Meaning Representation</td>
<td style="text-align: center;">(PleasantryAnythingElseCombined)</td>
<td style="text-align: center;">(Yield (Execute ("(Dynamic) ConfirmAndReturnAction)))</td>
</tr>
</tbody>
</table>
<p>Table 10: An example from SMCAlFLOW development set where EPR is correct and CBR is incorrect along with the top-3 training examples retrieved from each retriever.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">EPR</th>
<th style="text-align: center;">CBR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Test <br> Example</td>
<td style="text-align: center;">Utterance</td>
<td style="text-align: center;">Create a meeting with David Crim today</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Meaning Representation</td>
<td style="text-align: center;">(Yield (CreateCommitEventWrapper <br> (CreatePreflightEventWrapper <br> (Event.start_? (DateTime.date_? (?= <br> (Today)))) (Event.attendees_? <br> (AttendeeListHasRecipient (Execute <br> (refer (extensionConstraint <br> (RecipientWithNameLike ("(Recipient) <br> EmptyStructConstraint) (PersonName.apply <br> "David Crim")))))))))))</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Top-1</td>
<td style="text-align: center;">Utterance</td>
<td style="text-align: center;">make a meeting with jeri today</td>
<td style="text-align: center;">set up a meeting with both of David Crim's reports today</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Meaning Representation</td>
<td style="text-align: center;">(Yield (CreateCommitEventWrapper <br> (CreatePreflightEventWrapper <br> (Event.start_? (DateTime.date_? (?= <br> (Today)))) (Event.attendees_? <br> (AttendeeListHasRecipient (Execute <br> (refer (extensionConstraint <br> (RecipientWithNameLike ("(Recipient) <br> EmptyStructConstraint) (PersonName.apply <br> "/jeri")))))))))))</td>
<td style="text-align: center;">(Yield (CreateCommitEventWrapper <br> (CreatePreflightEventWrapper <br> (Event.start_? (DateTime.date_? (?= <br> (Today)))) (Event.attendees_? <br> (AttendeeListHasPeople (FindReports <br> (Execute (refer (extensionConstraint <br> (RecipientWithNameLike ("(Recipient) <br> EmptyStructConstraint) (PersonName.apply <br> "David Crim"))))))))))))))</td>
</tr>
<tr>
<td style="text-align: center;">Top-2</td>
<td style="text-align: center;">Utterance</td>
<td style="text-align: center;">put meeting with emlime on today</td>
<td style="text-align: center;">Make a meeting with David Largenstop on the 24 th.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Meaning Representation</td>
<td style="text-align: center;">(Yield (CreateCommitEventWrapper <br> (CreatePreflightEventWrapper <br> (Event.start_? (DateTime.date_? (?= <br> (Today)))) (Event.attendees_? <br> (AttendeeListHasRecipient (Execute <br> (refer (extensionConstraint <br> (RecipientWithNameLike ("(Recipient) <br> EmptyStructConstraint) (PersonName.apply <br> "emlime")))))))))))</td>
<td style="text-align: center;">(Yield (CreateCommitEventWrapper <br> (CreatePreflightEventWrapper <br> (Event.start_? (DateTime.date_? (?= <br> (nextDayOfMonth (Today) 24L)))) <br> (Event.attendees_? <br> (AttendeeListHasRecipient (Execute <br> (refer (extensionConstraint <br> (RecipientWithNameLike ("(Recipient) <br> EmptyStructConstraint) (PersonName.apply <br> "David Largenstop"))))))))))))</td>
</tr>
<tr>
<td style="text-align: center;">Top-3</td>
<td style="text-align: center;">Utterance</td>
<td style="text-align: center;">I want meet Dr Kennady from today</td>
<td style="text-align: center;">create a meet with bob today</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Meaning Representation</td>
<td style="text-align: center;">(Yield (CreateCommitEventWrapper <br> (CreatePreflightEventWrapper <br> (Event.start_? (DateTime.date_? (?= <br> (Today)))) (Event.attendees_? <br> (AttendeeListHasRecipient (Execute <br> (refer (extensionConstraint <br> (RecipientWithNameLike ("(Recipient) <br> EmptyStructConstraint) (PersonName.apply <br> "Dr Kennady")))))))))))</td>
<td style="text-align: center;">(Yield (CreateCommitEventWrapper <br> (CreatePreflightEventWrapper <br> (Event.start_? (DateTime.date_? (?= <br> (Today)))) (Event.attendees_? <br> (AttendeeListHasRecipient (Execute <br> (refer (extensionConstraint <br> (RecipientWithNameLike ("(Recipient) <br> EmptyStructConstraint) (PersonName.apply <br> "bob")))))))))))</td>
</tr>
</tbody>
</table>
<p>Table 11: An example from SMCAlFlow development set where EPR is correct and CBR is incorrect along with the top-3 training examples retrieved from each retriever.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Utterance</th>
<th style="text-align: left;">Meaning Representation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">which 3 seas border philippines?</td>
<td style="text-align: left;">$1 #)$ return the philippines</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$2 #)$ return seas that border #1</td>
</tr>
<tr>
<td style="text-align: left;">what three seas surround philippines?</td>
<td style="text-align: left;">$1 #)$ return seas</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$2 #)$ return #1 that surround the philippines</td>
</tr>
<tr>
<td style="text-align: left;">what states does west virginia border?</td>
<td style="text-align: left;">$1 #)$ return west virginia</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$2 #)$ return border states of $# 1$</td>
</tr>
<tr>
<td style="text-align: left;">what states borders west virginia?</td>
<td style="text-align: left;">$1 #)$ return west virginia</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$2 #)$ return border states of $# 1$</td>
</tr>
<tr>
<td style="text-align: left;">which states border colorado</td>
<td style="text-align: left;">$1 #)$ return states</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$2 #)$ return #1 that border colorado</td>
</tr>
</tbody>
</table>
<p>Table 12: Example of a cluster from the t-SNE projection of EPR on BREAK.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Utterance</th>
<th style="text-align: center;">Meaning Representation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">List the total scores of body builders in ascending order.</td>
<td style="text-align: center;">1#) return body builders <br> 2#) return scores of #1 <br> 3#) return sum of #2 for each #1 <br> 4#) return #3 sorted by ascending order</td>
</tr>
<tr>
<td style="text-align: center;">What are the names of body builders in descending order of total scores?</td>
<td style="text-align: center;">1#) return body builders <br> 2#) return names of #1 <br> 3#) return scores of #1 <br> 4#) return sum of #3 for each #1 <br> 5#) return #2 sorted by #4 in descending order</td>
</tr>
<tr>
<td style="text-align: center;">List the total points of gymnasts in descending order.</td>
<td style="text-align: center;">1#) return gymnasts <br> 2#) return points of #1 <br> 3#) return sum of #2 for each #1 <br> 4#) return #3 sorted by descending order</td>
</tr>
<tr>
<td style="text-align: center;">What are the total points for all gymnasts, ordered by total points descending?</td>
<td style="text-align: center;">1#) return gymnasts <br> 2#) return total points for all #1 <br> 3#) return #2 ordered by total points descending</td>
</tr>
<tr>
<td style="text-align: center;">List the total points of gymnasts in descending order of floor exercise points.</td>
<td style="text-align: center;">1#) return gymnasts <br> 2#) return points of #1 <br> 3#) return sum of #2 for each #1 <br> 4#) return floor exercise points of #1 <br> 5#) return #3 sorted by #4 in descending order</td>
</tr>
</tbody>
</table>
<p>Table 13: Example of a cluster from the t-SNE projection of EPR on BREAK.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Utterance</th>
<th style="text-align: center;">Meaning Representation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Show the locations that have both performances with more than 2000 attendees and performances with less than 1000 attendees.</td>
<td style="text-align: center;">1#) return performances <br> 2#) return attendees of #1 <br> 3#) return the number of #2 for each #1 <br> 4#) return #1 where #3 is more than 2000 <br> 5#) return #1 where #3 is less than 1000 <br> 6#) return the locations of #4 <br> 7#) return the locations of #5 <br> 8#) return the locations in both #6 and #7</td>
</tr>
<tr>
<td style="text-align: center;">Show the theme for exhibitions with both records of an attendance below 100 and above 500.</td>
<td style="text-align: center;">1#) return exhibitions <br> 2#) return attendances of #1 <br> 3#) return number of #2 for each #1 <br> 4#) return #1 where #3 is below 100 <br> 5#) return #1 where #3 is above 500 <br> 6#) return #1 of both #4 and #5 <br> 7#) return themes for #6</td>
</tr>
<tr>
<td style="text-align: center;">Which themes have had corresponding exhibitions that have had attendance both below 100 and above 500?</td>
<td style="text-align: center;">1#) return themes <br> 2#) return exhibitions with #1 <br> 3#) return attendances of #2 <br> 4#) return #1 where #3 is lower than 100 <br> 5#) return #1 where #3 is higher than 500 <br> 6#) return #1 of both #4 and #5</td>
</tr>
<tr>
<td style="text-align: center;">Show the publishers that have publications with price higher than 10000000 and publications with price lower than 5000000.</td>
<td style="text-align: center;">1#) return publishers <br> 2#) return publications of #1 <br> 3#) return prices of #2 <br> 4#) return #1 where #3 is higher than 10000000 <br> 5#) return #1 where #3 is lower than 5000000 <br> 6#) return #1 of both #4 and #5</td>
</tr>
<tr>
<td style="text-align: center;">Show the famous titles of the artists with both volumes that lasted more than 2 weeks on top and volumes that lasted less than 2 weeks on top.</td>
<td style="text-align: center;">1#) return artists <br> 2#) return volumes of #1 <br> 3#) return weeks on top that #2 lasted <br> 4#) return number of #3 for each #2 <br> 5#) return #1 where #4 is more than 2 <br> 6#) return #1 where #4 is less than 2 <br> 7#) return #1 in both #5 and #6 <br> 8#) return famous titles of #7</td>
</tr>
</tbody>
</table>
<p>Table 14: Example of a cluster from the t-SNE projection of EPR on BREAK.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Utterance</th>
<th style="text-align: left;">Meaning Representation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">What is the metal thing next to the <br> small cylinder?</td>
<td style="text-align: left;">$1 #)$ return the small cylinder <br> $2 #)$ return things <br> $3 #)$ return #2 that are metal <br> $4 #)$ return #3 that are next to #1</td>
</tr>
<tr>
<td style="text-align: left;">What is the purple thing next to the <br> brown thing?</td>
<td style="text-align: left;">$1 #)$ return the brown thing <br> $2 #)$ return things <br> $3 #)$ return #2 that are purple <br> $4 #)$ return #3 that are next to #1</td>
</tr>
<tr>
<td style="text-align: left;">What is the gray thing next to the <br> block?</td>
<td style="text-align: left;">$1 #)$ return gray thing <br> $2 #)$ return the block <br> $3 #)$ return #1 next to #2</td>
</tr>
<tr>
<td style="text-align: left;">What is the shiny thing next to the <br> cylinder?</td>
<td style="text-align: left;">$1 #)$ return shiny thing <br> $2 #)$ return cylinder <br> $3 #)$ return #1 next to #2</td>
</tr>
<tr>
<td style="text-align: left;">What is the thing in front of the red <br> square?</td>
<td style="text-align: left;">$1 #)$ return things <br> $2 #)$ return squares <br> $3 #)$ return #2 that is red <br> $4 #)$ return #1 that is in front of #3</td>
</tr>
</tbody>
</table>
<p>Table 15: Example of a cluster from the t-SNE projection of EPR on BREAK.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Utterance</th>
<th style="text-align: left;">Meaning Representation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Is the purple thing behind the big red <br> thing?</td>
<td style="text-align: left;">$1 #)$ return purple thing <br> $2 #)$ return big red thing <br> $3 #)$ return Is #1 behind #2</td>
</tr>
<tr>
<td style="text-align: left;">is the purple sphere in front of the <br> blue cube?</td>
<td style="text-align: left;">$1 #)$ return the purple sphere <br> $2 #)$ return the blue cube <br> $3 #)$ return if #1 is in front of #2</td>
</tr>
<tr>
<td style="text-align: left;">is the gray sphere behind the green <br> cylinder?</td>
<td style="text-align: left;">$1 #)$ return the green cylinder <br> $2 #)$ return the gray sphere <br> $3 #)$ return if #2 is behind #1</td>
</tr>
<tr>
<td style="text-align: left;">is the red cube in front of the yellow <br> ball?</td>
<td style="text-align: left;">$1 #)$ return the red cube <br> $2 #)$ return the yellow ball <br> $3 #)$ return if #1 is in front of #2</td>
</tr>
<tr>
<td style="text-align: left;">Is the blue ball in front of the silver <br> cube?</td>
<td style="text-align: left;">$1 #)$ return blue ball <br> $2 #)$ return silver cube <br> $3 #)$ return is #1 in front of #2</td>
</tr>
</tbody>
</table>
<p>Table 16: Example of a cluster from the t-SNE projection of EPR on BREAK.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ Results for $k \in{1,5,10}$ and $L \in{50,100}$ are in Appendix A.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
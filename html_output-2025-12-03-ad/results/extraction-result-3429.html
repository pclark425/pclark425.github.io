<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3429 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3429</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3429</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-77.html">extraction-schema-77</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <p><strong>Paper ID:</strong> paper-547c2cc8d45c22eaba7c7eb34d4e11a7d95a9cff</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/547c2cc8d45c22eaba7c7eb34d4e11a7d95a9cff" target="_blank">Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark</a></p>
                <p><strong>Paper Venue:</strong> AAAI Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> This research analyzes GPT’s spatial reasoning performance on the rectified benchmark, identifying proficiency in mapping natural language text to spatial relations but limitations in multi-hop reasoning, and proposes enhancements to improve spatial reasoning capabilities.</p>
                <p><strong>Paper Abstract:</strong> Artificial intelligence (AI) has made remarkable progress across various domains, with large language models like ChatGPT gaining substantial attention for their human-like text-generation capabilities. Despite these achievements, improving spatial reasoning remains a significant challenge for these models. Benchmarks like StepGame evaluate AI spatial reasoning, where ChatGPT has shown unsatisfactory performance. However, the presence of template errors in the benchmark has an impact on the evaluation results. Thus there is potential for ChatGPT to perform better if these template errors are addressed, leading to more accurate assessments of its spatial reasoning capabilities. In this study, we refine the StepGame benchmark, providing a more accurate dataset for model evaluation. We analyze GPT’s spatial reasoning performance on the rectified benchmark, identifying proficiency in mapping natural language text to spatial relations but limitations in multi-hop reasoning. We provide a flawless solution to the benchmark by combining template-to-relation mapping with logic-based reasoning. This combination demonstrates proficiency in performing qualitative reasoning on StepGame without encountering any errors. We then address the limitations of GPT models in spatial reasoning. To improve spatial reasoning, we deploy Chain-of-Thought and Tree-of-thoughts prompting strategies, offering insights into GPT’s cognitive process. Our investigation not only sheds light on model deficiencies but also proposes enhancements, contributing to the advancement of AI with more robust spatial reasoning capabilities.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3429.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3429.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StepGame</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>StepGame benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthetic natural-language benchmark for multi-hop directional spatial reasoning in text, where stories contain k relational sentences and the task is to infer the spatial relation between two queried objects across 1–10 hops.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Stepgame: A new benchmark for robust multi-hop spatial reasoning in texts.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>StepGame</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Stories of directional spatial relations among named objects (relations: overlap, above, below, left, right, upper-left, upper-right, lower-left, lower-right). Tasks require extracting relations and performing multi-hop chaining/inference to compute relative positions on a 2D grid.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Text: natural-language sentences (crowd-sourced templates) describing pairwise spatial relations; accompanied by a textual question asking relation between two objects.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>N/A (dataset/benchmark); used with zero-shot, few-shot, chain-of-thought (CoT), and tree-of-thoughts (ToT) prompting in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>StepGame uses a 2D grid semantics with discrete offsets for the nine relations; paper shows that many template-to-relation mappings in the original dataset are incorrect (template errors increase with hop count), which can distort model evaluation. Authors analyse relation-extraction difficulty, error propagation in multi-hop chains, and show that correct mapping of templates is crucial for fair assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Dataset statistics: 10,000 test samples per k (k=1..10). Paper reports percentages of incorrect template-derived instances increasing with k (clean: 7.64% at k=1 to 54.29% at k=10; noise: 20.43% to 74.21%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Many crowd-sourced templates are incorrect or ambiguous causing mislabeled examples; multi-hop reasoning is vulnerable to error accumulation; some templates are irreparable and must be removed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Benchmark compared across LLMs in this paper; no human baseline reported here. The paper demonstrates that non-LLM logical methods (Map+ASP) can perfectly solve the corrected benchmark, outperforming raw LLM-only approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3429.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3429.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Map+ASP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Template-based Sentence-to-Relation Mapping + ASP Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage logic-based pipeline: (1) deterministic template matching to convert input sentences into symbolic spatial relation facts; (2) an Answer Set Programming (ASP) reasoner (Clingo) that composes offsets on a 2D grid to perform multi-hop spatial inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Map+ASP (logic pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not an LLM; uses exact template matching to produce relation facts and an ASP program with grid-offset rules to compute positions and infer relations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>StepGame (corrected)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>As above: multi-hop directional spatial reasoning in text mapped to qualitative 2D grid offsets and composed via ASP rules.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Text (natural-language relation sentences) are matched to known templates and converted to symbolic relations, then fed as ASP facts.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>No prompting; deterministic template matching + ASP program.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Performs exact symbolic composition of 2D offsets for relations; removes ambiguity/error by relying on correct template base. The method demonstrates that when parsing is correct and the ASP rules are appropriate, qualitative spatial reasoning reduces to deterministic coordinate composition.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy: 100% across k=1..10 on the corrected StepGame test sets reported in Table 4 (Map+ASP row shows 100 for all k).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Requires prior knowledge of and updates to the template base; brittle to unseen natural-language phrasing not covered by templates; depends on handcrafted ASP rules (limited to the employed relation ontology and grid semantics).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Outperforms LLM-only approaches on the corrected StepGame benchmark (achieves perfect scores vs. degraded LLM performance without symbolic reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3429.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3429.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's large multimodal transformer language model (fourth generation) used here via API for spatial reasoning experiments with various prompting strategies (base, CoT, ToT_CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pre-trained transformer-based LLM by OpenAI; applied via API for in-context few-shot prompting and chain/tree-of-thought experiments. (Paper does not provide internal architecture/training-data specifics beyond standard GPT-4 reference.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>StepGame (corrected)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>As above: textual multi-hop directional spatial reasoning requiring relation extraction and composition across up to 10 hops.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Text: few-shot examples + story (set of relation sentences) + question; sometimes CoT or ToT scaffolding prompts which produce intermediate chain steps and coordinate calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Few-shot prompting (5-shot separate, 10-shot, 5-shot diverse), Chain-of-Thought (custom CoT with categorized thoughts: link/map/calcu), and Tree-of-Thoughts (ToT) combined with CoT (ToT_CoT). CoT runs with temperature 0; ToT thought generation uses temperature 0.7.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Authors report GPT-4 is good at mapping natural-language descriptions to spatial relations and at following coordinate-computation style CoT steps when guided; primary failure mode is multi-hop chaining without structured guidance. ToT (exploring multiple linking paths) substantially improves performance by avoiding dead-end chains. The paper provides qualitative examples of internal 'thought' types (link, map, calcu) and uses coordinate offsets to make reasoning explicit.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 4 (clean corrected dataset): base accuracies by hop: k=1:100, k=2:70, k=3:55, k=4:45, k=5:40, k=6:25, k=7:40, k=8:35, k=9:35, k=10:25. With CoT: large gains (e.g., k=3:75, k=4:95, k=5:85, k=6:85, etc.). With ToT_CoT: further improved stability and higher scores (e.g., k=9:100, k=10:95). Note: ToT/GPT-4 experiments were run on a smaller test set (20 instances) for token cost reasons; CoT/ToT results reflect those constrained evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Without CoT/ToT scaffolding GPT-4's accuracy degrades with increasing hop count due to error accumulation and difficulty in reliably selecting correct linking chains. CoT long chains can amplify mistakes; ToT reduces but does not fully eliminate failures and was evaluated on smaller sample sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>GPT-4 outperforms Davinci and Turbo across nearly all settings when using CoT/ToT prompting. However, a symbolic Map+ASP pipeline achieves perfect performance on the corrected benchmark, indicating structured reasoning still outperforms raw LLM reasoning for this controlled task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3429.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3429.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Davinci</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>text-davinci-003 (GPT-3 family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's Davinci (text-davinci-003) model from the GPT-3 family, used as a strong baseline for in-context few-shot and CoT/ToT experiments on StepGame.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A high-capability GPT-3.5-era text-only model (Davinci) used via API; applied with few-shot, CoT, and ToT_CoT prompting strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>StepGame (corrected)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Multi-hop directional spatial reasoning requiring relation extraction and sequential composition.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Text stories + few-shot examples + CoT/ToT scaffolding when used.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Few-shot (5-shot separate, 10-shot), CoT, and ToT_CoT (link-chain constructed by GPT-4 then Davinci used for CoT calculation in ToT_CoT experiments). Temperature settings: CoT experiments used temperature 0.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Davinci is effective at mapping text to relations and benefits substantially from CoT and ToT_CoT prompting; ToT_CoT notably improves multi-hop performance by assisting correct link-chain construction (authors attribute gains to explicit link-selection and coordinate calculation steps). Relation extraction errors are fewer than Curie and cause some remaining failures.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 4 (clean corrected dataset; experiments with 100 samples for Davinci/Turbo): base accuracies: k=1:77, k=2:42, k=3:21, k=4:26, k=5:25, k=6:30, k=7:23, k=8:23, k=9:22, k=10:22. With CoT: substantial improvement (example: k=3:53, k=4:46, k=5:46, k=6:48). With ToT_CoT: further improvements across hops, e.g., k=3:65, k=6:60, k=10:50.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Baseline performance degrades quickly with hop count; requires long prompts and precise few-shot examples to maintain performance. Still outperformed by GPT-4 with advanced prompting and by Map+ASP symbolic method.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Davinci outperforms Turbo in this task in most configurations; Davinci+ASP nearly achieves perfect results on the corrected dataset (Davinci+ASP row: essentially 100% across k with minor exceptions).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3429.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3429.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-3.5-turbo (ChatGPT API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's gpt-3.5-turbo (ChatGPT) API model used as a baseline LLM for few-shot, CoT, and ToT_CoT spatial reasoning experiments on StepGame.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An efficient, conversational GPT-3.5 model (Turbo) used via the Azure OpenAI Service in experiments; applied with few-shot, CoT, and ToT scaffolding.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>StepGame (corrected)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Textual multi-hop spatial reasoning across 1–10 hops.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Text stories + few-shot examples + CoT/ToT prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Few-shot prompting (various sets), CoT (temperature 0), ToT_CoT pipeline (linking by GPT-4 + CoT reasoning), ToT generation temperature 0.7.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Turbo shows competency at mapping relations at low hops but struggles substantially with multi-hop reasoning compared to Davinci and GPT-4. CoT yields modest improvements but long CoT steps increase chance of mistakes; ToT_CoT provides mixed gains depending on hop count.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 4 (100-sample evaluations): base accuracies: k=1:62, k=2:43, k=3:30, k=4:35, k=5:29, k=6:25, k=7:29, k=8:31, k=9:16, k=10:20. CoT and ToT_CoT variants show variable improvements (e.g., Turbo ToT_CoT k=6:45 in one row but inconsistent across hops).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Smaller model size/quality leads to poorer relation extraction and chain-following; long prompts (many CoT steps) are challenging; ToT effectiveness limited relative to larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Turbo generally underperforms Davinci and GPT-4; performs comparably to Davinci in some tasks per other studies but here shows lower spatial multi-hop reasoning capability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3429.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3429.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Curie</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>text-curie-001</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mid-tier OpenAI model (text-curie-001) evaluated for relation extraction performance when paired with ASP reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-curie-001</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An earlier/smaller OpenAI text model used to test relation-extraction quality when combined with ASP reasoning (Curie+ASP).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>StepGame (corrected) - relation extraction subtask</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Evaluation focused on relation extraction accuracy from templates/sentences that are then used by an ASP reasoner.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Textual relation sentences passed to the model to extract symbolic relations.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Few-shot prompting for semantic parsing; Curie used as the parser component in Curie+ASP experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Curie produced the highest number of incorrect relation extractions among evaluated models, demonstrating that poorer parsers drive down end-to-end reasoning performance even when ASP reasoning is applied downstream.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Curie+ASP row in Table 4: accuracies across k: [46, 43, 42, 59, 67, 67, 57, 56, 58, 61] (per-k accuracies reported; experiment scale assumed to match corrected dataset evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Relation extraction errors are frequent, limiting the downstream ASP reasoning performance; thus parser quality is a bottleneck.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Curie+ASP performed substantially worse than Davinci+ASP and Map+ASP, indicating the parser model quality is crucial for hybrid LLM+logic methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3429.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3429.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SOTA (GPT-3+ASP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 semantic parsing + ASP reasoning (Yang et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously reported state-of-the-art hybrid approach that uses GPT-3 for parsing spatial descriptions into symbolic relations, then uses an ASP reasoner for multi-hop spatial inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 + ASP (Yang et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Hybrid pipeline: LLM (GPT-3) as a semantic parser into symbolic relations, then ASP (Clingo) for deterministic multi-hop spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>StepGame</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Multi-hop spatial reasoning where GPT-3 performs semantic parsing and ASP composes relations on a grid.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Textual relation sentences parsed into symbolic facts for ASP.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>LLM used as semantic parser (few-shot prompting) followed by ASP program; not an LLM-only reasoning pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Hybrid design leverages LLM flexibility for parsing and symbolic ASP for robust multi-hop reasoning; paper attributes residual errors to parsing/data issues (~10.7% faults).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported SOTA accuracies (on original dataset prior to correction): ~90% for lower hops down to 88.3% for 10-hop reasoning. In this paper, re-evaluation on corrected dataset shows similar strong performance; Davinci+ASP achieved nearly perfect scores (100% on most k) with only 2 errors among 1000 tests attributed to semantic parsing.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Parsing errors in LLM component (semantic parsing) were the main failure mode; data/template errors in the benchmark also contributed to faults. The approach requires well-specified ASP rules and a robust parser.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Outperformed pure LLM (base) approaches; comparable to or slightly below the Map+ASP deterministic pipeline when parser errors are eliminated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3429.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3429.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Custom CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Customized Chain-of-Thought Prompting (link/map/calcu)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tailored CoT prompting method that decomposes each reasoning step into three categorized 'thoughts': link establishment (choose a relation sentence to extend the chain), relation mapping (map chosen sentence to a simple relation), and coordinate calculation (compute coordinates using fixed offsets).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>StepGame</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Designed to guide LLMs through multi-hop spatial inference by forcing explicit intermediate steps and coordinate bookkeeping.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Text prompt containing task description, few-shot demonstrations, story, and an explicit multi-step CoT scaffold.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Few-shot Chain-of-Thought; CoT steps are structured as c_i = [c_i^link, c_i^map, c_i^calcu]; CoT experiments used temperature 0 for determinism.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Decomposing each hop into link/map/calcu reduces ambiguity and helps the model maintain correct coordinate bookkeeping; helps larger models (GPT-4, Davinci) substantially, but long CoT chains can still accumulate errors in smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>CoT improves performance relative to base prompting: e.g., GPT-4 CoT shows large gains (see GPT-4 entry), Davinci CoT shows marked improvements over Davinci base (see Table 4), Turbo shows smaller gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Long CoT sequences increase opportunities for mistakes; smaller models show limited ability to execute long, structured CoT reliably; requires careful few-shot examples and prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>CoT benefits scale with model capability: larger models gain more from structured CoT. CoT alone still falls short of Map+ASP when parsing errors or accumulation persist.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3429.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3429.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Custom ToT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Customized Tree-of-Thoughts Prompting (ToT_CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A customized ToT approach that builds multiple candidate linking chains between source and target objects, evaluates candidate states (sure/likely/impossible), and then applies CoT coordinate calculation on the selected chain(s).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>StepGame</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Addresses the linking-chain construction subproblem in multi-hop spatial reasoning by explicitly exploring multiple chain expansions to avoid dead-ends and distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Text: initial tree state (chain, target, unused relations) and prompts to generate and evaluate candidate extensions; final chosen chain used with CoT coordinate calculation.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>ToT generation (G) and evaluation (V) loops; candidate generation uses temperature 0.7 to encourage diverse proposals; BFS-style selection maintaining top-b promising chains (b=3), then CoT used to compute spatial relation.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>ToT reduces failure due to wrong chain selection by allowing multiple parallel hypotheses and scoring them with an LLM-based evaluator; authors show ToT_CoT substantially improves multi-hop accuracy for Davinci and GPT-4, and provides modest gains for Turbo.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 4 ToT_CoT rows: e.g., GPT-4 ToT_CoT: k=3:85, k=4:85, k=5:90, k=6:90, k=9:100, k=10:95 (note: GPT-4 ToT experiments on smaller test set of 20 instances). Davinci ToT_CoT shows increases over Davinci CoT (see Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>ToT requires more tokens (higher cost) and more sophisticated prompt engineering; experimented on smaller sample sizes for GPT-4 due to cost; evaluator heuristics ('sure/likely/impossible') depend on LLM self-assessment and stochastic sampling which can still mis-rank good chains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>ToT_CoT narrows the performance gap between LLM-only methods and hybrid symbolic methods but still does not universally surpass Map+ASP deterministic pipeline on corrected StepGame.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text <em>(Rating: 2)</em></li>
                <li>Stepgame: A new benchmark for robust multi-hop spatial reasoning in texts. <em>(Rating: 2)</em></li>
                <li>SpartQA: A Textual Question Answering Benchmark for Spatial Reasoning <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>A multitask, multilingual, multimodal evaluation of ChatGPT on reasoning, hallucination, and interactivity. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3429",
    "paper_id": "paper-547c2cc8d45c22eaba7c7eb34d4e11a7d95a9cff",
    "extraction_schema_id": "extraction-schema-77",
    "extracted_data": [
        {
            "name_short": "StepGame",
            "name_full": "StepGame benchmark",
            "brief_description": "A synthetic natural-language benchmark for multi-hop directional spatial reasoning in text, where stories contain k relational sentences and the task is to infer the spatial relation between two queried objects across 1–10 hops.",
            "citation_title": "Stepgame: A new benchmark for robust multi-hop spatial reasoning in texts.",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "puzzle_name": "StepGame",
            "puzzle_description": "Stories of directional spatial relations among named objects (relations: overlap, above, below, left, right, upper-left, upper-right, lower-left, lower-right). Tasks require extracting relations and performing multi-hop chaining/inference to compute relative positions on a 2D grid.",
            "input_representation": "Text: natural-language sentences (crowd-sourced templates) describing pairwise spatial relations; accompanied by a textual question asking relation between two objects.",
            "prompting_method": "N/A (dataset/benchmark); used with zero-shot, few-shot, chain-of-thought (CoT), and tree-of-thoughts (ToT) prompting in experiments.",
            "spatial_reasoning_analysis": "StepGame uses a 2D grid semantics with discrete offsets for the nine relations; paper shows that many template-to-relation mappings in the original dataset are incorrect (template errors increase with hop count), which can distort model evaluation. Authors analyse relation-extraction difficulty, error propagation in multi-hop chains, and show that correct mapping of templates is crucial for fair assessment.",
            "performance_metrics": "Dataset statistics: 10,000 test samples per k (k=1..10). Paper reports percentages of incorrect template-derived instances increasing with k (clean: 7.64% at k=1 to 54.29% at k=10; noise: 20.43% to 74.21%).",
            "limitations_or_failure_modes": "Many crowd-sourced templates are incorrect or ambiguous causing mislabeled examples; multi-hop reasoning is vulnerable to error accumulation; some templates are irreparable and must be removed.",
            "comparison_to_other_models_or_humans": "Benchmark compared across LLMs in this paper; no human baseline reported here. The paper demonstrates that non-LLM logical methods (Map+ASP) can perfectly solve the corrected benchmark, outperforming raw LLM-only approaches.",
            "uuid": "e3429.0",
            "source_info": {
                "paper_title": "Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Map+ASP",
            "name_full": "Template-based Sentence-to-Relation Mapping + ASP Reasoning",
            "brief_description": "A two-stage logic-based pipeline: (1) deterministic template matching to convert input sentences into symbolic spatial relation facts; (2) an Answer Set Programming (ASP) reasoner (Clingo) that composes offsets on a 2D grid to perform multi-hop spatial inference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Map+ASP (logic pipeline)",
            "model_description": "Not an LLM; uses exact template matching to produce relation facts and an ASP program with grid-offset rules to compute positions and infer relations.",
            "model_size": null,
            "puzzle_name": "StepGame (corrected)",
            "puzzle_description": "As above: multi-hop directional spatial reasoning in text mapped to qualitative 2D grid offsets and composed via ASP rules.",
            "input_representation": "Text (natural-language relation sentences) are matched to known templates and converted to symbolic relations, then fed as ASP facts.",
            "prompting_method": "No prompting; deterministic template matching + ASP program.",
            "spatial_reasoning_analysis": "Performs exact symbolic composition of 2D offsets for relations; removes ambiguity/error by relying on correct template base. The method demonstrates that when parsing is correct and the ASP rules are appropriate, qualitative spatial reasoning reduces to deterministic coordinate composition.",
            "performance_metrics": "Accuracy: 100% across k=1..10 on the corrected StepGame test sets reported in Table 4 (Map+ASP row shows 100 for all k).",
            "limitations_or_failure_modes": "Requires prior knowledge of and updates to the template base; brittle to unseen natural-language phrasing not covered by templates; depends on handcrafted ASP rules (limited to the employed relation ontology and grid semantics).",
            "comparison_to_other_models_or_humans": "Outperforms LLM-only approaches on the corrected StepGame benchmark (achieves perfect scores vs. degraded LLM performance without symbolic reasoning).",
            "uuid": "e3429.1",
            "source_info": {
                "paper_title": "Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "OpenAI's large multimodal transformer language model (fourth generation) used here via API for spatial reasoning experiments with various prompting strategies (base, CoT, ToT_CoT).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large pre-trained transformer-based LLM by OpenAI; applied via API for in-context few-shot prompting and chain/tree-of-thought experiments. (Paper does not provide internal architecture/training-data specifics beyond standard GPT-4 reference.)",
            "model_size": null,
            "puzzle_name": "StepGame (corrected)",
            "puzzle_description": "As above: textual multi-hop directional spatial reasoning requiring relation extraction and composition across up to 10 hops.",
            "input_representation": "Text: few-shot examples + story (set of relation sentences) + question; sometimes CoT or ToT scaffolding prompts which produce intermediate chain steps and coordinate calculations.",
            "prompting_method": "Few-shot prompting (5-shot separate, 10-shot, 5-shot diverse), Chain-of-Thought (custom CoT with categorized thoughts: link/map/calcu), and Tree-of-Thoughts (ToT) combined with CoT (ToT_CoT). CoT runs with temperature 0; ToT thought generation uses temperature 0.7.",
            "spatial_reasoning_analysis": "Authors report GPT-4 is good at mapping natural-language descriptions to spatial relations and at following coordinate-computation style CoT steps when guided; primary failure mode is multi-hop chaining without structured guidance. ToT (exploring multiple linking paths) substantially improves performance by avoiding dead-end chains. The paper provides qualitative examples of internal 'thought' types (link, map, calcu) and uses coordinate offsets to make reasoning explicit.",
            "performance_metrics": "Table 4 (clean corrected dataset): base accuracies by hop: k=1:100, k=2:70, k=3:55, k=4:45, k=5:40, k=6:25, k=7:40, k=8:35, k=9:35, k=10:25. With CoT: large gains (e.g., k=3:75, k=4:95, k=5:85, k=6:85, etc.). With ToT_CoT: further improved stability and higher scores (e.g., k=9:100, k=10:95). Note: ToT/GPT-4 experiments were run on a smaller test set (20 instances) for token cost reasons; CoT/ToT results reflect those constrained evaluations.",
            "limitations_or_failure_modes": "Without CoT/ToT scaffolding GPT-4's accuracy degrades with increasing hop count due to error accumulation and difficulty in reliably selecting correct linking chains. CoT long chains can amplify mistakes; ToT reduces but does not fully eliminate failures and was evaluated on smaller sample sizes.",
            "comparison_to_other_models_or_humans": "GPT-4 outperforms Davinci and Turbo across nearly all settings when using CoT/ToT prompting. However, a symbolic Map+ASP pipeline achieves perfect performance on the corrected benchmark, indicating structured reasoning still outperforms raw LLM reasoning for this controlled task.",
            "uuid": "e3429.2",
            "source_info": {
                "paper_title": "Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Davinci",
            "name_full": "text-davinci-003 (GPT-3 family)",
            "brief_description": "OpenAI's Davinci (text-davinci-003) model from the GPT-3 family, used as a strong baseline for in-context few-shot and CoT/ToT experiments on StepGame.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "text-davinci-003",
            "model_description": "A high-capability GPT-3.5-era text-only model (Davinci) used via API; applied with few-shot, CoT, and ToT_CoT prompting strategies.",
            "model_size": null,
            "puzzle_name": "StepGame (corrected)",
            "puzzle_description": "Multi-hop directional spatial reasoning requiring relation extraction and sequential composition.",
            "input_representation": "Text stories + few-shot examples + CoT/ToT scaffolding when used.",
            "prompting_method": "Few-shot (5-shot separate, 10-shot), CoT, and ToT_CoT (link-chain constructed by GPT-4 then Davinci used for CoT calculation in ToT_CoT experiments). Temperature settings: CoT experiments used temperature 0.",
            "spatial_reasoning_analysis": "Davinci is effective at mapping text to relations and benefits substantially from CoT and ToT_CoT prompting; ToT_CoT notably improves multi-hop performance by assisting correct link-chain construction (authors attribute gains to explicit link-selection and coordinate calculation steps). Relation extraction errors are fewer than Curie and cause some remaining failures.",
            "performance_metrics": "Table 4 (clean corrected dataset; experiments with 100 samples for Davinci/Turbo): base accuracies: k=1:77, k=2:42, k=3:21, k=4:26, k=5:25, k=6:30, k=7:23, k=8:23, k=9:22, k=10:22. With CoT: substantial improvement (example: k=3:53, k=4:46, k=5:46, k=6:48). With ToT_CoT: further improvements across hops, e.g., k=3:65, k=6:60, k=10:50.",
            "limitations_or_failure_modes": "Baseline performance degrades quickly with hop count; requires long prompts and precise few-shot examples to maintain performance. Still outperformed by GPT-4 with advanced prompting and by Map+ASP symbolic method.",
            "comparison_to_other_models_or_humans": "Davinci outperforms Turbo in this task in most configurations; Davinci+ASP nearly achieves perfect results on the corrected dataset (Davinci+ASP row: essentially 100% across k with minor exceptions).",
            "uuid": "e3429.3",
            "source_info": {
                "paper_title": "Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Turbo",
            "name_full": "gpt-3.5-turbo (ChatGPT API)",
            "brief_description": "OpenAI's gpt-3.5-turbo (ChatGPT) API model used as a baseline LLM for few-shot, CoT, and ToT_CoT spatial reasoning experiments on StepGame.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_description": "An efficient, conversational GPT-3.5 model (Turbo) used via the Azure OpenAI Service in experiments; applied with few-shot, CoT, and ToT scaffolding.",
            "model_size": null,
            "puzzle_name": "StepGame (corrected)",
            "puzzle_description": "Textual multi-hop spatial reasoning across 1–10 hops.",
            "input_representation": "Text stories + few-shot examples + CoT/ToT prompts.",
            "prompting_method": "Few-shot prompting (various sets), CoT (temperature 0), ToT_CoT pipeline (linking by GPT-4 + CoT reasoning), ToT generation temperature 0.7.",
            "spatial_reasoning_analysis": "Turbo shows competency at mapping relations at low hops but struggles substantially with multi-hop reasoning compared to Davinci and GPT-4. CoT yields modest improvements but long CoT steps increase chance of mistakes; ToT_CoT provides mixed gains depending on hop count.",
            "performance_metrics": "Table 4 (100-sample evaluations): base accuracies: k=1:62, k=2:43, k=3:30, k=4:35, k=5:29, k=6:25, k=7:29, k=8:31, k=9:16, k=10:20. CoT and ToT_CoT variants show variable improvements (e.g., Turbo ToT_CoT k=6:45 in one row but inconsistent across hops).",
            "limitations_or_failure_modes": "Smaller model size/quality leads to poorer relation extraction and chain-following; long prompts (many CoT steps) are challenging; ToT effectiveness limited relative to larger models.",
            "comparison_to_other_models_or_humans": "Turbo generally underperforms Davinci and GPT-4; performs comparably to Davinci in some tasks per other studies but here shows lower spatial multi-hop reasoning capability.",
            "uuid": "e3429.4",
            "source_info": {
                "paper_title": "Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Curie",
            "name_full": "text-curie-001",
            "brief_description": "A mid-tier OpenAI model (text-curie-001) evaluated for relation extraction performance when paired with ASP reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "text-curie-001",
            "model_description": "An earlier/smaller OpenAI text model used to test relation-extraction quality when combined with ASP reasoning (Curie+ASP).",
            "model_size": null,
            "puzzle_name": "StepGame (corrected) - relation extraction subtask",
            "puzzle_description": "Evaluation focused on relation extraction accuracy from templates/sentences that are then used by an ASP reasoner.",
            "input_representation": "Textual relation sentences passed to the model to extract symbolic relations.",
            "prompting_method": "Few-shot prompting for semantic parsing; Curie used as the parser component in Curie+ASP experiments.",
            "spatial_reasoning_analysis": "Curie produced the highest number of incorrect relation extractions among evaluated models, demonstrating that poorer parsers drive down end-to-end reasoning performance even when ASP reasoning is applied downstream.",
            "performance_metrics": "Curie+ASP row in Table 4: accuracies across k: [46, 43, 42, 59, 67, 67, 57, 56, 58, 61] (per-k accuracies reported; experiment scale assumed to match corrected dataset evaluation).",
            "limitations_or_failure_modes": "Relation extraction errors are frequent, limiting the downstream ASP reasoning performance; thus parser quality is a bottleneck.",
            "comparison_to_other_models_or_humans": "Curie+ASP performed substantially worse than Davinci+ASP and Map+ASP, indicating the parser model quality is crucial for hybrid LLM+logic methods.",
            "uuid": "e3429.5",
            "source_info": {
                "paper_title": "Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "SOTA (GPT-3+ASP)",
            "name_full": "GPT-3 semantic parsing + ASP reasoning (Yang et al. 2023)",
            "brief_description": "A previously reported state-of-the-art hybrid approach that uses GPT-3 for parsing spatial descriptions into symbolic relations, then uses an ASP reasoner for multi-hop spatial inference.",
            "citation_title": "Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text",
            "mention_or_use": "mention",
            "model_name": "GPT-3 + ASP (Yang et al. 2023)",
            "model_description": "Hybrid pipeline: LLM (GPT-3) as a semantic parser into symbolic relations, then ASP (Clingo) for deterministic multi-hop spatial reasoning.",
            "model_size": null,
            "puzzle_name": "StepGame",
            "puzzle_description": "Multi-hop spatial reasoning where GPT-3 performs semantic parsing and ASP composes relations on a grid.",
            "input_representation": "Textual relation sentences parsed into symbolic facts for ASP.",
            "prompting_method": "LLM used as semantic parser (few-shot prompting) followed by ASP program; not an LLM-only reasoning pipeline.",
            "spatial_reasoning_analysis": "Hybrid design leverages LLM flexibility for parsing and symbolic ASP for robust multi-hop reasoning; paper attributes residual errors to parsing/data issues (~10.7% faults).",
            "performance_metrics": "Reported SOTA accuracies (on original dataset prior to correction): ~90% for lower hops down to 88.3% for 10-hop reasoning. In this paper, re-evaluation on corrected dataset shows similar strong performance; Davinci+ASP achieved nearly perfect scores (100% on most k) with only 2 errors among 1000 tests attributed to semantic parsing.",
            "limitations_or_failure_modes": "Parsing errors in LLM component (semantic parsing) were the main failure mode; data/template errors in the benchmark also contributed to faults. The approach requires well-specified ASP rules and a robust parser.",
            "comparison_to_other_models_or_humans": "Outperformed pure LLM (base) approaches; comparable to or slightly below the Map+ASP deterministic pipeline when parser errors are eliminated.",
            "uuid": "e3429.6",
            "source_info": {
                "paper_title": "Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Custom CoT",
            "name_full": "Customized Chain-of-Thought Prompting (link/map/calcu)",
            "brief_description": "A tailored CoT prompting method that decomposes each reasoning step into three categorized 'thoughts': link establishment (choose a relation sentence to extend the chain), relation mapping (map chosen sentence to a simple relation), and coordinate calculation (compute coordinates using fixed offsets).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "puzzle_name": "StepGame",
            "puzzle_description": "Designed to guide LLMs through multi-hop spatial inference by forcing explicit intermediate steps and coordinate bookkeeping.",
            "input_representation": "Text prompt containing task description, few-shot demonstrations, story, and an explicit multi-step CoT scaffold.",
            "prompting_method": "Few-shot Chain-of-Thought; CoT steps are structured as c_i = [c_i^link, c_i^map, c_i^calcu]; CoT experiments used temperature 0 for determinism.",
            "spatial_reasoning_analysis": "Decomposing each hop into link/map/calcu reduces ambiguity and helps the model maintain correct coordinate bookkeeping; helps larger models (GPT-4, Davinci) substantially, but long CoT chains can still accumulate errors in smaller models.",
            "performance_metrics": "CoT improves performance relative to base prompting: e.g., GPT-4 CoT shows large gains (see GPT-4 entry), Davinci CoT shows marked improvements over Davinci base (see Table 4), Turbo shows smaller gains.",
            "limitations_or_failure_modes": "Long CoT sequences increase opportunities for mistakes; smaller models show limited ability to execute long, structured CoT reliably; requires careful few-shot examples and prompt engineering.",
            "comparison_to_other_models_or_humans": "CoT benefits scale with model capability: larger models gain more from structured CoT. CoT alone still falls short of Map+ASP when parsing errors or accumulation persist.",
            "uuid": "e3429.7",
            "source_info": {
                "paper_title": "Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Custom ToT",
            "name_full": "Customized Tree-of-Thoughts Prompting (ToT_CoT)",
            "brief_description": "A customized ToT approach that builds multiple candidate linking chains between source and target objects, evaluates candidate states (sure/likely/impossible), and then applies CoT coordinate calculation on the selected chain(s).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "puzzle_name": "StepGame",
            "puzzle_description": "Addresses the linking-chain construction subproblem in multi-hop spatial reasoning by explicitly exploring multiple chain expansions to avoid dead-ends and distractors.",
            "input_representation": "Text: initial tree state (chain, target, unused relations) and prompts to generate and evaluate candidate extensions; final chosen chain used with CoT coordinate calculation.",
            "prompting_method": "ToT generation (G) and evaluation (V) loops; candidate generation uses temperature 0.7 to encourage diverse proposals; BFS-style selection maintaining top-b promising chains (b=3), then CoT used to compute spatial relation.",
            "spatial_reasoning_analysis": "ToT reduces failure due to wrong chain selection by allowing multiple parallel hypotheses and scoring them with an LLM-based evaluator; authors show ToT_CoT substantially improves multi-hop accuracy for Davinci and GPT-4, and provides modest gains for Turbo.",
            "performance_metrics": "Table 4 ToT_CoT rows: e.g., GPT-4 ToT_CoT: k=3:85, k=4:85, k=5:90, k=6:90, k=9:100, k=10:95 (note: GPT-4 ToT experiments on smaller test set of 20 instances). Davinci ToT_CoT shows increases over Davinci CoT (see Table 4).",
            "limitations_or_failure_modes": "ToT requires more tokens (higher cost) and more sophisticated prompt engineering; experimented on smaller sample sizes for GPT-4 due to cost; evaluator heuristics ('sure/likely/impossible') depend on LLM self-assessment and stochastic sampling which can still mis-rank good chains.",
            "comparison_to_other_models_or_humans": "ToT_CoT narrows the performance gap between LLM-only methods and hybrid symbolic methods but still does not universally surpass Map+ASP deterministic pipeline on corrected StepGame.",
            "uuid": "e3429.8",
            "source_info": {
                "paper_title": "Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text",
            "rating": 2
        },
        {
            "paper_title": "Stepgame: A new benchmark for robust multi-hop spatial reasoning in texts.",
            "rating": 2
        },
        {
            "paper_title": "SpartQA: A Textual Question Answering Benchmark for Spatial Reasoning",
            "rating": 2
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2
        },
        {
            "paper_title": "A multitask, multilingual, multimodal evaluation of ChatGPT on reasoning, hallucination, and interactivity.",
            "rating": 1
        }
    ],
    "cost": 0.018472,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark</h1>
<p>Fangjun Li ${ }^{1}$, David C. Hogg ${ }^{1}$, Anthony G. Cohn ${ }^{1,2}$<br>${ }^{1}$ School of Computing, University of Leeds, Leeds, UK<br>${ }^{2}$ Alan Turing Institute, UK<br>scfli@leeds.ac.uk, d.c.hogg@leeds.ac.uk, a.g.cohn@leeds.ac.uk</p>
<h4>Abstract</h4>
<p>Artificial intelligence (AI) has made remarkable progress across various domains, with large language models like ChatGPT gaining substantial attention for their human-like text-generation capabilities. Despite these achievements, spatial reasoning remains a significant challenge for these models. Benchmarks like StepGame evaluate AI spatial reasoning, where ChatGPT has shown unsatisfactory performance. However, the presence of template errors in the benchmark has an impact on the evaluation results. Thus there is potential for ChatGPT to perform better if these template errors are addressed, leading to more accurate assessments of its spatial reasoning capabilities. In this study, we refine the StepGame benchmark, providing a more accurate dataset for model evaluation. We analyze GPT's spatial reasoning performance on the rectified benchmark, identifying proficiency in mapping natural language text to spatial relations but limitations in multi-hop reasoning. We provide a flawless solution to the benchmark by combining template-to-relation mapping with logic-based reasoning. This combination demonstrates proficiency in performing qualitative reasoning on StepGame without encountering any errors. We then address the limitations of GPT models in spatial reasoning. We deploy Chain-of-thought and Tree-of-thoughts prompting strategies, offering insights into GPT's "cognitive process", and achieving remarkable improvements in accuracy. Our investigation not only sheds light on model deficiencies but also proposes enhancements, contributing to the advancement of AI with more robust spatial reasoning capabilities.</p>
<h2>Introduction</h2>
<p>Spatial reasoning, the ability to understand and navigate relationships in physical space, is a fundamental aspect of human cognition that significantly influences interactions with the environment. Enhancing spatial reasoning in AI models has the potential to enrich their comprehension of their surroundings and response to user interactions, leading to more advanced and immersive user experiences (Alomari et al. 2022). In recent years, AI has revolutionized numerous domains, from healthcare to finance to entertainment. Notably, OpenAI's large language models (LLMs), such as ChatGPT and GPT-4 (OpenAI 2023), have gained significant attention for their human-like text generation capabilities. However, despite their impressive abilities, LLMs encounter challenges in many logical reasoning aspects crucial for human communication, particularly spatial reasoning (Bang et al. 2023; Cohn and Hernandez-Orallo 2023).</p>
<p>One approach to evaluating spatial reasoning in an AI system is to use synthetic benchmarks such as StepGame (Shi, Zhang, and Lipani 2022) and SpartQA (Mirzaee and Rajaby 2021). Unfortunately, models like ChatGPT have shown unsatisfactory performance on these benchmarks. Improving the spatial reasoning capabilities of LLMs remains a primary focus to enhance their overall performance and understanding of complex environments.</p>
<p>Whilst examining the StepGame benchmark we discovered that it contains template errors that distort model performance evaluations. These errors were previously overlooked, leading to studies conducted on a flawed benchmark, inaccurately assessing the capabilities of the LLMs (Bang et al. 2023; Yang, Ishay, and Lee 2023). To rectify this issue, we present a more accurate version of the StepGame dataset for model evaluation, ensuring precise assessments of the models' true capabilities and limitations ${ }^{1}$.</p>
<p>We then conducted evaluation tests on the rectified benchmark across various test subsets, few-shot sets, and models. We observed that larger GPT models demonstrate proficiency in mapping natural language text to spatial relations. However, they struggle with multi-hop spatial reasoning.</p>
<p>Our goal is not merely to critique, but also to propose potential improvements. To this end, we provide a flawless solution to the benchmark, and explore different approaches to enhance the spatial reasoning ability of LLMs.</p>
<p>The solution we propose for the benchmark entails combining template-based sentence-to-relation mapping with logic-based spatial reasoning. The logical reasoner used in this approach comes from (Yang, Ishay, and Lee 2023), where they integrated GPT-3 for the task. GPT-3 was employed to parse spatial descriptions into symbolic spatial relation representations, which were then passed to the logical program for spatial reasoning. This fusion resulted in significant improvement in StepGame, achieving state-of-the-art (SOTA) but not perfect results: around $90 \%$ accu-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>racy for lower hops and $88.3 \%$ accuracy for 10-hop reasoning. They attributed $10.7 \%$ faults to data-related issues. With our aforementioned work on rectifying the benchmark, We take a step further to delve into the two components, analyzing the performance of each on our filtered version of the dataset. Remarkably, we achieved $100 \%$ accuracy for almost all hops, with only 2 errors among 1000 test examples, which were due to GPT-3's incorrect semantic parsing. Building on this, we replaced the GPT-3 parser with our sentence-to-relation mapping method and combined it with the ASP reasoner, showcasing proficiency in performing qualitative reasoning without encountering any errors, thus demonstrating a method to achieve a perfect score on the corrected benchmark.</p>
<p>Neither our solution or the SOTA utilize LLMs for the actual spatial reasoning functionality. Thus, we proceed to enhance GPT's capabilities as a native spatial reasoner. To achieve this, we employ Chain-of-Thoughts (CoT) and Tree-of-Thoughts (ToT) prompting strategies.</p>
<p>CoT (Wei et al. 2022) incorporates a sequence of intermediate reasoning steps to facilitate problem-solving. However, when applied to StepGame, previous studies (Yang, Ishay, and Lee 2023) have shown that CoT does not consistently improve performance and may even reduce accuracy in complex $k$-hop reasoning tasks. This observation is attributed to the higher probability of errors occurring in lengthy CoT processes. On the other hand, research on other tasks (Zhou et al. 2022; Creswell, Shanahan, and Higgins 2022) has demonstrated that breaking down complex problems into simpler subproblems and solving them sequentially can be beneficial. Given the ambiguity in the decomposition of "thoughts" ${ }^{2}$ within CoT, we propose refining the CoT prompt to empower language models to perform better in spatial reasoning tasks.</p>
<p>On the other hand, (Yao et al. 2023) introduced ToT, a framework enabling LLMs to explore multiple reasoning paths, and they demonstrated its effectiveness in improving problem-solving capabilities across tasks like the game of 24 , creative writing, and mini crosswords. In our work, we customize the ToT approach for object-linking chain building, a crucial subproblem in addressing spatial reasoning benchmarks.</p>
<p>Our customized CoT method showcases its advantages more prominently in larger models such as GPT-4 and Davinci, maintaining accuracy even as the tasks become more complex. Our ToT approach demonstrates its strengths on the three GPT models: on the largest model, GPT-4, we are able to maintain an accuracy of around $90 \%$ even as the tasks become more complex. On Davinci, the accuracy is maintained at around $50 \%$, while Turbo achieves a lower level of accuracy at around $30 \%$.</p>
<p>By identifying current deficiencies and proposing enhancements, we aim to contribute to the ongoing discourse</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>in AI development, pushing the boundaries of what LLMs can achieve. Ultimately, our investigation can pave the way for the development of advanced, intuitive, and user-friendly AI systems with robust spatial reasoning capabilities.</p>
<h2>Related Work</h2>
<p>The field of spatial reasoning in language with artificial intelligence has evolved through sustained efforts over time, with significant advancements achieved through both traditional methods and modern LLMs.</p>
<p>Early strides in spatial reasoning in language were marked by the development of formal structures to represent spatial relationships. (Kordjamshidi, Moens, and van Otterlo 2010) proposed a spatial ontology to formalize the representation of spatial relations. This work laid the groundwork for the subsequent introduction of text-based spatial role labeling (Kordjamshidi, Van Otterlo, and Moens 2011), which aims to convert text into formal spatial representations.</p>
<p>Then comes synthetic tasks designed to evaluate the text understanding and spatial reasoning capabilities of learning algorithms. The positional reasoning task (Task 17) in the bAbI dataset (Weston et al. 2015) is for spatial reasoning and requires models to reason using one or two sentences, which makes this task comparatively simple. (Shi, Zhang, and Lipani 2022) advanced this field by creating the StepGame benchmark to evaluate multi-hop spatial reasoning in text, with richer variety in spatial relation descriptions. Both of these datasets emphasize directional spatial relations (Cohn and Hazarika 2001; Skiadopoulos and Koubarakis 2001; Cohn and Renz 2008; Chen et al. 2015). Three spatial QA datasets: SpartQA(Mirzaee and Rajaby 2021), SPARTUN, and RESQ (Mirzaee and Kordjamshidi 2022) expanded the resource landscape by encompassing wider-ranging spatial language expressions, posing challenges for traditional logical programming, and are important benchmarks for evaluating LLMs' spatial reasoning capabilities.</p>
<p>Concurrently, the advent of LLMs such as OpenAI's ChatGPT has opened up fresh pathways for spatial reasoning. These models, leveraging transformer architectures, can generate human-like text and handle complex linguistic structures. However, while these models are indeed impressive, their capabilities in spatial reasoning are yet to be fully explored and exploited. One recent approach to assess these capabilities was taken by (Bang et al. 2023), who put ChatGPT to the test using SpartQA and StepGame. Despite the generally advanced capabilities of ChatGPT, the model showed shortcomings in these tasks, signaling a need for further enhancements in the realm of spatial reasoning.</p>
<p>A promising technique known as 'prompt engineering' (Bommasani et al. 2021) has been making its mark recently. This approach involves crafting specific prompts to guide the responses of the models, leading to outputs that are more contextually apt and insightful. This method demonstrates significant potential in enhancing the capabilities of LLMs like ChatGPT in various domains (Li, Hogg, and Cohn 2022), including the challenging area of logical reasoning (Wang et al. 2023). For instance, when faced with multi-step reasoning tasks, a method called 'few-shot chain-of-thought' (CoT) prompting (Zhang et al. 2022) comes into</p>
<table>
<thead>
<tr>
<th>図</th>
<th>Story: I is diagonally above 11 to the right at a 45 degree.</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Clock-position: $1 \square$ is the center of a clock face, $\square$ is located between 2 and 3.</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Compare-position: $\square$ is north east of 11</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Relation extraction</td>
<td>1-hop reasoning</td>
</tr>
<tr>
<td></td>
<td>I is on the upper right of 11,</td>
<td>Question: What is the relation</td>
</tr>
<tr>
<td></td>
<td>11 is is on the lower left of 11</td>
<td>of the agent $\square$ to the agent 11?</td>
</tr>
<tr>
<td>Semantic</td>
<td>top_right("I","1")</td>
<td>Question: What is the relation</td>
</tr>
<tr>
<td>Parsing</td>
<td>down_left("11","I")</td>
<td>of the agent 11 to the agent 11?</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Answer: lower-left</td>
</tr>
</tbody>
</table>
<p>Figure 1: An illustrative example for demonstrating relation extraction and 1-hop spatial reasoning. play. These demonstrations enable LLMs to explicitly generate reasoning steps, thereby improving their accuracy in reasoning tasks. This technique involves a handful of manually curated step-by-step reasoning demonstrations.</p>
<p>As we review these developments, it is clear that while significant progress has been made, challenges remain in both traditional and LLM approaches to spatial reasoning. The limitations of models like ChatGPT indicate the need for continued research and enhancement strategies. This paper aims to contribute to this by examining these limitations more closely and proposing potential avenues for improvement. We aim to explore the limit of GPT as a general problem solver that explores its own thoughts and guides its own exploration with deliberate reasoning as heuristics.</p>
<h2>The StepGame Benchmark for Evaluating Spatial Reasoning</h2>
<p>In this paper, we focus on StepGame, in line with other studies that evaluate ChatGPT's spatial reasoning proficiency using StepGame and SpartQA. StepGame comprises storyquestion pairs in natural language, The objective is to answer questions regarding the spatial relations between two specified entities. The StepGame benchmark contains two sets of data: a clean set where there are precisely $k$ facts given for any given $k$-hop instance, and a noise set where there are more than $k$ facts given, and the extra facts are distracting.</p>
<h2>Spatial Reasoning Types</h2>
<ul>
<li>1-Hop Spatial Reasoning. In 1-hop reasoning, we are given a relation description between two entities and are asked about the spatial relation from one entity to the other. 1-hop relation reasoning and relation extraction can be considered similar processes. As exemplified in Figure 1, consider the story where $J$ is diagonally above $B$ to the right at a 45-degree angle. The question is What is the spatial relation of agent $J$ to agent $B$ ?. This is similar to relation extraction. However, if we change the question to What is the spatial relation of agent $B$ to agent $J$ ?, it needs a reverse reasoning process top_right("J", "B") $\rightarrow$ down_left("B","J"). Both expressions are correct representations for relation extraction.</li>
<li>Multi-Hop Spatial Reasoning. Figure 2 provides one example of 10-hop reasoning, which is from the 'clean' set. The questions ask about the relation between two objects,</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: center;">10- hop story:</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1. is slightly off center to the top left and $\square$ is</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">slightly off center to the bottom right.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">2. is at the bottom of 11</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">3. presents right to 11</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">4. is lower right of 11</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">5. is positioned above $\square$ and to the right.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">6. is above $\square$ at 10 o'clock.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">7. is at the upper left of 11</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">8. is at the bottom of 11</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">9. is sitting in the right direction of 11</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">10. is placed at the lower right of 11</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">What is the relation of the agent 11 to the agent 11?</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Figure 2: Example of 10-hop reasoning, featuring a question regarding two entities that are not directly connected in the stories. The diagrams on the right do not form part of the input to the AI system but are for illustrative purposes only. either directly or indirectly connected. Multi-hop reasoning adds more complexity to the problem, as it involves a greater number of provided relations. To solve the problem, one needs to identify useful relations and then proceed with relation inference step by step.</p>
<h2>Problems with the Dataset</h2>
<p>Eight spatial relations (top, down, left, right, top-left, topright, down-left, and down-right) are utilized for the story generation of StepGame. These relations are expressed through sentences in natural language. All sentences/statements are based on a crowd-sourced template base ${ }^{3}$. Each "story" is accompanied by a question that seeks to identify the relations between two objects, and it is labeled according to the intended relations at the time of story creation, rather than the actual sentences used. A template is considered to contain an error if the meaning conveyed by the sentence does not align with the relationship that was intended to be expressed during the creation of stories and labels.</p>
<p>Table 1 presents a detailed enumeration of errors in the relation-to-sentence mappings identified in StepGame. Out of the 214 templates examined, 14 were found to contain errors. Of the eight different relationship mappings available, only $o_{1}$.above_ $o_{2}$ and $o_{1}$.left_ $o_{2}$ are devoid of mistakes. The question arises as to why there are so many errors in the crowd-sourced expressions; presumably this is down to insufficient quality control over the crowdworker reponses.</p>
<p>For each $k$ value, the StepGame dataset includes 10,000 test samples. Table 2 displays the percentage of examples containing sentences derived from incorrect templates, which hints at a rising trend in inaccuracies as $k$ increases, suggesting a potential cumulative impact.</p>
<p>Among these 14 incorrect templates, four cannot be remedied in existing StepGame benchmark examples.</p>
<ul>
<li>$o_{1}$.upperright_ $o_{2}$ : Object A is above object $o_{1}$.</li>
<li>$o_{1}$.upperleft_ $o_{2}: o_{1}$ is diagonally left and above $o_{1}$.</li>
<li>$o_{1}$.lowerright_ $o_{2} \mid o_{1}$.upperleft_ $o_{2} \mid o_{1}$.upperright_ $o_{2}$ : $o_{1}$ is to the right and above $o_{2}$ at an angle of about 45 degrees.</li>
</ul>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup> <sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>: ${ }^{3}$ https://github.com/ZhengxiangShi/StepGame/blob/main/ Code/template.py</p>
<table>
<thead>
<tr>
<th>Relation</th>
<th>Original Incorrect Template</th>
</tr>
</thead>
<tbody>
<tr>
<td>right</td>
<td>$o_{2}$ and $o_{1}$ are parallel, and $o_{2}$ on the right of $o_{1}$. $o_{2}$ and $o_{1}$ are parallel, and $o_{2}$ is to the right of $o_{1}$. $o_{2}$ and $o_{1}$ are horizontal and $o_{2}$ is to the right of $o_{1}$. $o_{2}$ and $o_{1}$ are both there with the object $o_{2}$ is to the right of object $o_{1}$.</td>
</tr>
<tr>
<td>below</td>
<td>$o_{2}$ is placed at the bottom of $o_{1}$. $o_{2}$ is at the bottom of $o_{1}$ and is on the same vertical plane. $o_{2}$ presents below $o_{1}$.</td>
</tr>
<tr>
<td>lowerleft</td>
<td>$o_{2}$ is there and $o_{1}$ is at the 10 position of a clock face. $o_{2}$ is positioned below $o_{1}$ and to the left.</td>
</tr>
<tr>
<td>upperright</td>
<td>Object A is above object $o_{1}$ and to the right of it, too. $o_{2}$ is diagonally to the upper right of $o_{1}$.</td>
</tr>
<tr>
<td>lowerright</td>
<td>$o_{1}$ is to the right and above $o_{2}$ at an angle of about 45 degrees.</td>
</tr>
<tr>
<td>upperleft</td>
<td>$o_{1}$ is to the right and above $o_{2}$ at an angle of about 45 degrees. $o_{1}$ is diagonally left and above $o_{1}$.</td>
</tr>
</tbody>
</table>
<p>Table 1: Incorrect sentence templates in StepGame. The Relation column signifies relation for $o_{1}.\mathrm{relation} .o_{2}$.</p>
<table>
<thead>
<tr>
<th></th>
<th>k=1</th>
<th>k=2</th>
<th>k=3</th>
<th>k=4</th>
<th>k=5</th>
<th>k=6</th>
<th>k=7</th>
<th>k=8</th>
<th>k=9</th>
<th>k=10</th>
</tr>
</thead>
<tbody>
<tr>
<td>Clean</td>
<td>7.64</td>
<td>15.03</td>
<td>20.87</td>
<td>26.39</td>
<td>32.54</td>
<td>37.66</td>
<td>41.71</td>
<td>47.20</td>
<td>51.50</td>
<td>54.29</td>
</tr>
<tr>
<td>Noise</td>
<td>20.43</td>
<td>30.19</td>
<td>34.59</td>
<td>48.18</td>
<td>57.13</td>
<td>61.14</td>
<td>63.60</td>
<td>69.45</td>
<td>72.84</td>
<td>74.21</td>
</tr>
</tbody>
</table>
<p>Table 2: Percentage of incorrect instances out of all instances over k=1-10 test sets.</p>
<ul>
<li>$o_{1}.\mathrm{lowerleft} .o_{2}$ | $o_{1}.\mathrm{upperleft} .o_{2}$: $o_{2}$ is there and $o_{1}$ is at the 10 position of a clock face.</li>
</ul>
<p>The first and second templates are irreparable because it is impossible to identify what $o_{2}$ is when sentences are formed using them. The third and fourth templates cannot be corrected since they were applied to multiple spatial relations, although each accurately represents just one. For example, for the sentence ‘Q is to the right and above P at an angle of about 45 degrees’, three mapping relations exist: $Q . u p p e r r i g h t . . P$, $Q . l o w e r r i g h t . . P$, and $Q . u p p e r l e f t . . P$. Although this sentence expresses the meaning $Q . u p p e r r i g h t . . P$, it is uncertain which candidate was used for the label. For such templates, a unique correction could not be chosen, necessitating the removal of the sentences that use these template from the dataset.</p>
<h2>Methods</h2>
<h2>Solution for the Corrected Benchmark</h2>
<p>Our error-free approach is entirely logic-based, without the use of LLMs. We begin by performing template-based sentence-to-relation mapping, akin to semantic parsing. Then, we employ ASP for logical reasoning, utilizing the ASP reasoner introduced by (Yang, Ishay, and Lee 2023). These two components operate independently:</p>
<ul>
<li>Sentence-to-Relation Mapping. When presented with a natural language relation description $r$, we first identify the template used in $r$ through a comparison with the template base. This template is symbolized as $o_{0} . \nu . o_{1}$. Then,
<img alt="img-0.jpeg" src="img-0.jpeg" /></li>
</ul>
<p>Figure 3: Sentence-to-Relation Mapping Examples.
we convert this template form into a structured representation $\nu\left(o_{0}, o_{1}\right)$, where $o_{0}$ and $o_{1}$ correspond to the two objects mentioned in $r$, and $\nu$ signifies the spatial relation between $o_{0}$ and $o_{1}$. Specifically, for questions inquiring about relations from the start object $o_{0}$ to the target object $o_{t}$, the template is query.$o_{0} . o_{t}$, and the corresponding ASP fact is represented as query $\left(o_{0}, o_{t}\right)$. Illustrative examples of this process can be found in Figure 3.</p>
<ul>
<li>Logical Reasoning with ASP. The logical facts $\nu\left(o_{0}, o_{1}\right)$, generated through semantic parsing for all relations in the story $R$, are used as input to the ASP module for spatial reasoning. The ASP module was implemented using Clingo and includes rules specifically tailored for StepGame. These rules transform StepGame into a qualitative spatial reasoning problem in a 2D grid space. These rules incorporate offsets for 9 spatial relations, such as offset $(r i g h t)=(0,1)$ and offset(lower-left) $=$ $(-1,-1)$. The main rule in the ASP module calculates the location of $o_{0}$ to $o_{1}$ by adding the offsets $\nu\left(o_{0}, o_{1}\right)$.</li>
</ul>
<p>While this approach offers a solution to the StepGame benchmark challenge, it does require prior familiarity with the templates and mandates updates to the template base when confronted with new stories employing novel templates. In contrast, an LLM approach holds the potential to flexibly adjust to unfamiliar templates. Additionally, the method's dependence on customized rules within the logical program constitutes another aspect to be mindful of.</p>
<h2>Chain-of-Thought (CoT) Prompting</h2>
<p>We devised a customized CoT for the spatial reasoning task. The core idea of CoT is to introduce a chain of thoughts $c_{1}, \ldots, c_{i}, \ldots, c_{n}$ to bridge input $x$ and output $y$, where $i$ represents $i$-th step. In our customized CoT for StepGame, $x$ consists of the task description, few-shot examples, relation story, and question, while $y$ represents the answer regarding the relations between the queried objects (from the start object $o_{i}$ to the target object $o_{t}$ ). Each thought $c_{i}$ is to identify direct spatial connections between objects ( $o_{i}$ and $o_{i+1}$ ). We take CoT a step further by decomposing each step of thought $c_{i}$ to explore the potential advantages of incorporating a coherent and detailed reasoning process.</p>
<p>Thought Categorisation. We categorise the thought into three types: link establishment thoughts $c^{\text {link }}$, relation mapping thoughts $c^{\text {map }}$, and coordinate calculation thoughts $c^{\text {calcu }}$. At each reasoning step, these three types of thought are sequentially sampled as a continuous language sequence $c_{i}=\left[c_{i}^{\text {link }}, c_{i}^{\text {map }}, c_{i}^{\text {calcu }}\right]$ using the LLM.</p>
<ol>
<li>$c_{i}^{\text {link }}$ : Guide the LLM to examine all relations in the story $\left(R=\left[r^{1}, \ldots, r^{j}, \ldots, r^{k}\right]\right)$ and select $r^{j}$ for the $i$-th step</li>
</ol>
<p>for $k$-hop reasoning, ensuring it directly describes the relation with $o_{i}$ and has not been used in any previous step. For the start object $(i=0)$, we use the prompt "Start with $o_{0}$. According to" and for the middle objects $(i \geq 1)$, we use the prompt "Then search for $o_{i}$. According to". Full details of the prompts can be found in the Appendix ${ }^{4}$.
2. $c_{i}^{\text {map }}$ : Map $r^{j}$ to a simple relation description such as " $o_{i}$ is to the $\nu$ of $o_{i+1}$," where $\nu$ represents the key spatial relation from $o_{i}$ to $o_{i+1}$. The prompt "This means" helps the LLM perform this mapping.
3. $c_{i}^{\text {calcu }}$ : Use $r^{j}$ to calculate the coordinates of $o_{i+1}$. We set $o_{o}$ at $(0,0)$, and each spatial relation is assigned an offset to determine the positions of the objects. The prompt " $o_{i+1}=o_{i}+\operatorname{offset}\left(r^{j}\right)=\left(x_{o_{i}}, y_{o_{i}}\right)+\left(x_{\nu}, y_{\nu}\right)=$ $\left(x_{o_{i+1}}, y_{o_{i+1}}\right)$ " instructs the LLM on the calculation process. It computes the coordinates of $o_{i+1}$ and generates the output like "Therefore, B is at $\left(x_{o_{i+1}}, y_{o_{i+1}}\right)$."</p>
<h2>Tree-of-Thoughts (ToT) Prompting</h2>
<p>Algorithm 1 is designed to enhance the reasoning chainbuilding process, allowing LLMs to consider different pathways. This is useful because during the search for relations with an object, distracting connections may arise, as shown in Figure 2. However, it is essential to follow a correct sequence to successfully reach the target object. If an LLM mistakenly tracks an incorrect sequence, it could get stuck in a dead end leading to incorrect reasoning conclusions such as "The story does not provide direct spatial information."</p>
<p>The algorithm initiates by prompting the LLM to set up the initial tree state, denoted as $S_{0}$, using the input $x$, which comprises a story and a question. $S_{0}$ is in the form "chain: $o_{0} \rightarrow$, target: $o_{t}$, unused: $R$ ". $R$ represents all connections between objects in the story, in the form of object1-object2. Then it proceeds to construct a linking chain from $o_{0}$ to $o_{t}$ in iterative steps, wherein for the $i$-th step $(1 \leq i \leq 10)$, the LLM considers the tree state $S_{i-1}$ built up to that step. If no state $s$ in $S_{i-1}$ reaches $o_{t}$, the LLM is prompted to generate $j$ candidate thoughts for each $s$ in the current set of states, $S_{i}(j=2$ in this paper). $G$ prompts the LLM to search for a potential object $o_{i}$ connected to the current object $o_{i-1}$ from the unused relations $R_{i-1}^{\text {unused }}$. A check is made $(\operatorname{CheckExtn}(c)))$ to see if the proposal made is a real candidate extension. For all candidate thoughts, $V$ prompts the LLM to evaluate the state to determine if the chain can proceed with $o_{i}$ and the updated $R_{i-1}^{\text {unused }}$ to reach $o_{t}$. The top-rated $b$ tree states in $S_{i}^{\prime}$ are selected as $S_{i}$. When there is a state $s_{f}$ which reaches $o_{t}$, the L will be prompted with the linking chain construction prompt (Appendix D.4) to form the final links $l$.</p>
<ul>
<li>Thought Generation $G(s, j)$. Given a tree state $s$, we let the LLM propose $j$ thoughts using the thought generation prompt "Use relations listed in unused relations to enumerate all potential expansions of the chain by considering unused relations that exhibit a direct link to the last object</li>
</ul>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Algorithm 1: Our ToT Approach
Require: LLM, input $x$
$: S_{0} \leftarrow \operatorname{Init}(x)$
$: i \leftarrow 1$
while no $s_{f} \in S_{i-1}$ has arrived at $o_{t}$ do
$S_{i}^{\prime} \leftarrow\left{s \cdot c \mid c \in G(s, j) \wedge \operatorname{ChainExtn}(c) \wedge s \in S_{i-1}\right}$
if $S_{i}^{\prime}=\emptyset$ then return failure
$S_{i} \leftarrow \operatorname{select}\left(b,\left{\langle s, y\rangle \mid s \in S_{i}^{\prime} \wedge y=\Sigma_{1}^{n} \sigma(V(s))\right}\right)$
$i=i+1$
end while
return $\operatorname{Link}\left(s_{f}\right)$
within the chain." In our experiment, we set $j=2$, meaning that we instruct the LLM to generate content twice for each state $s \in S_{i-1}$.</p>
<ul>
<li>State Evaluation $V(s)$. Our approach involves a classification methodology, using the designed value prompt "Evaluate whether the chain can reach the target (sure/ likely/impossible). If the chain has already reached the target, it's 'sure'. If the unused relations include the current object, it's 'likely'. If there are no unused relations that include the current object, it's 'impossible'." This prompt guides the LLM to sequentially examine all newly generated states $s \in S_{i}^{\prime} n$ times - using the stochasticity of the LLM with a non zero temperature to increase the reliability of the scoring. The three types of outputs - 'sure', 'likely', and 'impossible' - are converted into numerical scores using a function $\sigma()$ to facilitate the selection process among all newly generated states.</li>
<li>Search Algorithm The choice between utilizing breadthfirst search (BFS) or depth-first search (DFS) depends on the tree structure. In the StepGame benchmark, the tree depth is limited (depth $\leq 10$ ), and the number of thought candidates $k$ for each step is also limited (width $\leq 3$ in most cases). However, a deeper search does not necessarily guarantee better results. In certain scenarios, $o_{0}$ and $o_{t}$ may be directly connected in one relation statement, allowing for shorter linking chains between them, which is preferable. Therefore, we opt for BFS to maintain all promising states. We set the breadth width $b=3$, maintaining the three most promising linking-chain states per step. The criterion for stopping searching is set when the linking chain arrives at the target object.</li>
</ul>
<p>Our ToT approach is used to construct the reasoning chain from $o_{0}$ to $o_{t}$. Subsequently, the spatial relation between these objects is computed following the previous CoT prompting method, with the use of $c^{\text {map }}$ and $c^{\text {calcu }}$.</p>
<h2>Experimental Design</h2>
<h2>Model Settings</h2>
<p>We use the Azure OpenAI Service for ChatGPT (3.5-Turbo) and GPT-3 (Davinci), and GPT-4 API access. To yield more concentrated and deterministic results, we set the temperature to 0 in CoT experiments. In ToT experiments, we follow</p>
<p>(Yao et al. 2023), setting the temperature to 0.7 for generating varied thought proposals. The remaining parameters were left at the standard configurations for these models.</p>
<p>Different Test Subsets It is common practice in the studies cited (Bang et al. 2023), (Yang, Ishay, and Lee 2023) to use a subset of 30 or 100 test examples from the full set of 10,000 for each $k$ value. While this method helps in conserving token usage, it could potentially introduce biases or inaccurate estimations of the model performance.</p>
<p>We examine the effect of the number of test examples. Specifically, we wanted to determine whether evaluating on a limited number of test examples could introduce inaccuracies. To achieve this, we conducted tests on a clean, filtered test set for $k$-hop reasoning $(k \in[1,10])$, thereby covering a range of task complexities. Tests were carried out on 30, 100, and 1000 test examples to assess the impact of the number of test examples on the evaluation.</p>
<p>Different Few-Shot Sets We created three different fewshot prompting sets to evaluate the influence of input examples in prompts.</p>
<ul>
<li>clean 5shot(1,3,5,7,10): Create a prompt consisting of five examples, with one example each from tasks requiring 1hop, 3-hop, 5-hop, 7-hop, and 10-hop reasoning.</li>
<li>clean 10shot: Formulate a prompt using ten examples, each one derived from a distinct $k$-hop task in clean set.</li>
<li>clean 5shot separate: Construct a prompt for each $k$-hop reasoning task, utilizing five examples from the corresponding $k$-hop training set as few-shot examples.</li>
</ul>
<h2>Experimental Results</h2>
<h2>Evaluation Results</h2>
<p>Influence of Scale of Test Examples We employ the clean 10shot prompting setting. The results are presented in the left subplot of Figure 4. Upon evaluation of the expanded test set comprising 1000 examples, the model shows a uniform decrement in performance as $k$ increases from 1 to 10 . This trend indicates the increased complexity as the number of hops increases. With smaller test sets of 100 or 30 examples, the trend is less consistent, and there are occasional increases in performance at certain hop levels. The variance in performance, particularly for the 30 -example test set, may indeed be larger. This could be due to the smaller sample size providing less comprehensive coverage of the potential range of tasks, leading to more fluctuations in performance. This indicates larger test sets can provide a more stable and reliable indicator of a model's performance across different complexity levels (i.e., number of hops).</p>
<p>Influence of Prompting Examples The middle subplot in figure 4 indicates that the choice of prompting strategy can impact the model's ability to handle tasks of varying complexity. Similar to the previous data, all prompting strategies show a trend of decreasing accuracy as the number of hops increases. This trend is consistent and suggests that the complexity of the tasks grows with the number of hops.</p>
<p>The performances of the three methods are close. While differences exist at specific hop levels, no single method</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">left/ <br> right</th>
<th style="text-align: left;">above <br> /below</th>
<th style="text-align: left;">lower_left/ <br> upper_right</th>
<th style="text-align: left;">lower_right/ <br> upper_left</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">total</td>
<td style="text-align: left;">44</td>
<td style="text-align: left;">53</td>
<td style="text-align: left;">50</td>
<td style="text-align: left;">53</td>
</tr>
<tr>
<td style="text-align: left;">text-curie-001</td>
<td style="text-align: left;">11</td>
<td style="text-align: left;">41</td>
<td style="text-align: left;">30</td>
<td style="text-align: left;">37</td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-003</td>
<td style="text-align: left;">$\mathbf{0}$</td>
<td style="text-align: left;">$\mathbf{0}$</td>
<td style="text-align: left;">$\mathbf{0}$</td>
<td style="text-align: left;">2</td>
</tr>
<tr>
<td style="text-align: left;">gpt-3.5-turbo</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">$\mathbf{1}$</td>
</tr>
</tbody>
</table>
<p>Table 3: The relation extraction performance of GPT. The numbers in rows 2-4 are incorrect predictions numbers.
consistently outperforms the others across all hop levels. Interestingly, clean 5shot (1,3,5,7,10) performs better than clean 10shot ( $1 \sim 10$ ) at almost every hop level. This suggests that selecting examples from a wider range of hop levels $(1,3,5,7,10)$ can be more beneficial than having an example from each hop level from 1 to 10 .</p>
<p>Influence of Models As indicated in a recent study (Ye et al. 2023), Turbo demonstrates comparable performance to Davinci across many tasks. However, it falls short in the machine reading comprehension, part-of-speech, and relation extraction tasks, potentially owing to its smaller model size. The StepGame spatial reasoning task requires the comprehension of sequential spatial connections and the ability to draw deductions from them. According to the right subplot of Figure 4, the Davinci model generally outperforms the Turbo model across varying levels of task complexity (number of hops). The differences in performance between the two models are more significant at lower complexity levels, but they appear to converge as the complexity increases.</p>
<h2>Results of the Improved Methods</h2>
<p>Resolution for the Benchmark The results of our resolution (sentence-to-relation mapping + ASP-based reasoning) are displayed in the 'Map+ASP' row of Table 4. The numbers in the table indicate accuracy scores, with higher values indicating better performance. This demonstrates the proficiency achieved in spatial relation mapping and multi-hop spatial reasoning, all without encountering any errors.</p>
<p>GPT for Relation Extraction + ASP for Reasoning We analyze the performance of GPT in the relation extraction subtask, as outlined in Table 3. Curie has the highest number of wrong predictions across different relations, Davinci and Turbo show better performance.</p>
<p>The state-of-the-art results achieved by (Yang, Ishay, and Lee 2023) (using GPT-3 for semantic parsing and ASP for reasoning) are presented in the "SOTA" row of Table 4. They achieve approximately $90 \%$ accuracy for lower hops and $88.3 \%$ accuracy for 10-hop reasoning. They attribute $10.7 \%$ of the inaccuracies to data-related concerns.</p>
<p>We provide an evaluation of their approach onthe corrected dataset, with the results displayed in the "Curie+ASP" and "Davinci+ASP" rows. Among the 1000 test examples ( 100 for each k), only 2 errors were encountered with Davinci. caused by semantic parsing: the sentence "If E is the center of a clock face, H is located between 2 and 3." was parsed incorrectly as right("H", "E"), but</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 4: Accuracy comparison for varying numbers of hops (1-10) on the clean test set. On the left, we show the performance variation of the Turbo model with <em>10shot</em> prompting over different test set sizes (30, 100, and 1000 examples). The middle section illustrates the performance of the Turbo model under three distinct prompting settings: <em>5shot(1,3,5,7,10)</em>, <em>10shot</em>, and <em>5shot separate</em>. The right portion showcases the performance of two models - Davinci and Turbo - using <em>10shot</em> prompting.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th>k=1</th>
<th>k=2</th>
<th>k=3</th>
<th>k=4</th>
<th>k=5</th>
<th>k=6</th>
<th>k=7</th>
<th>k=8</th>
<th>k=9</th>
<th>k=10</th>
</tr>
</thead>
<tbody>
<tr>
<td>Map+ASP</td>
<td></td>
<td>100</td>
<td>100</td>
<td>100</td>
<td>100</td>
<td>100</td>
<td>100</td>
<td>100</td>
<td>100</td>
<td>100</td>
<td>100</td>
</tr>
<tr>
<td>Curie+ASP</td>
<td></td>
<td>46</td>
<td>43</td>
<td>42</td>
<td>59</td>
<td>67</td>
<td>67</td>
<td>57</td>
<td>56</td>
<td>58</td>
<td>61</td>
</tr>
<tr>
<td>Davinci+ASP</td>
<td></td>
<td>100</td>
<td>100</td>
<td>99</td>
<td>100</td>
<td>100</td>
<td>99</td>
<td>100</td>
<td>100</td>
<td>100</td>
<td>100</td>
</tr>
<tr>
<td>SOTA</td>
<td></td>
<td>92.6</td>
<td>89.9</td>
<td>89.1</td>
<td>93.8</td>
<td>92.9</td>
<td>91.6</td>
<td>91.2</td>
<td>90.4</td>
<td>89.0</td>
<td>88.3</td>
</tr>
<tr>
<td>Turbo</td>
<td>base</td>
<td>62</td>
<td>43</td>
<td>30</td>
<td>35</td>
<td>29</td>
<td>25</td>
<td>29</td>
<td>31</td>
<td>16</td>
<td>20</td>
</tr>
<tr>
<td></td>
<td>CoT</td>
<td>/</td>
<td>34</td>
<td>40</td>
<td>36</td>
<td>28</td>
<td>28</td>
<td>26</td>
<td>31</td>
<td>25</td>
<td>24</td>
</tr>
<tr>
<td></td>
<td>ToT_CoT</td>
<td>/</td>
<td>/</td>
<td>35</td>
<td>35</td>
<td>25</td>
<td>45</td>
<td>15</td>
<td>40</td>
<td>40</td>
<td>35</td>
</tr>
<tr>
<td>Davinci</td>
<td>base</td>
<td>77</td>
<td>42</td>
<td>21</td>
<td>26</td>
<td>25</td>
<td>30</td>
<td>23</td>
<td>23</td>
<td>22</td>
<td>22</td>
</tr>
<tr>
<td></td>
<td>CoT</td>
<td>/</td>
<td>48</td>
<td>53</td>
<td>46</td>
<td>46</td>
<td>48</td>
<td>40</td>
<td>45</td>
<td>41</td>
<td>32</td>
</tr>
<tr>
<td></td>
<td>ToT_CoT</td>
<td>/</td>
<td>/</td>
<td>65</td>
<td>50</td>
<td>45</td>
<td>60</td>
<td>50</td>
<td>50</td>
<td>55</td>
<td>50</td>
</tr>
<tr>
<td>GPT-4</td>
<td>base</td>
<td>100</td>
<td>70</td>
<td>55</td>
<td>45</td>
<td>40</td>
<td>25</td>
<td>40</td>
<td>35</td>
<td>35</td>
<td>25</td>
</tr>
<tr>
<td></td>
<td>CoT</td>
<td>/</td>
<td>80</td>
<td>75</td>
<td>95</td>
<td>85</td>
<td>85</td>
<td>90</td>
<td>80</td>
<td>60</td>
<td>65</td>
</tr>
<tr>
<td></td>
<td>ToT_CoT</td>
<td>/</td>
<td>/</td>
<td>85</td>
<td>85</td>
<td>90</td>
<td>90</td>
<td>85</td>
<td>90</td>
<td>100</td>
<td>95</td>
</tr>
</tbody>
</table>
<p>Table 4: Accuracy comparison of GPT models on revised StepGame using different methods.</p>
<p>supposed to be <em>up_right</em> ("H", "E").</p>
<p><strong>CoT and ToT</strong> The experimental results in Table 4 involving GPT-4 and ToT are based on a test set comprising 20 instances considering token usage, while for Davinci and Turbo, we used a larger test set of 100 samples. The results for the base and CoT methods were obtained using the <em>5shot separate</em> prompting on the <em>clean</em> set. All the ToT_CoT results presented in the table involve the use of GPT-4 for building the linking chain, followed by the application of Turbo, Davinci, and GPT-4 for CoT reasoning with the constructed linking chain. The GPT-4 model exhibits superior performance across nearly all settings. With the basic input-output prompt, despite starting at 100% accuracy for <em>k</em> = 1, its accuracy dips to 25% for <em>k</em> = 10, indicating that even the most powerful GPT model struggles to maintain accuracy as task complexity rises. Humans would probably find this challenging too.</p>
<p>With the implementation of our CoT and ToT approach, the GPT-4 model demonstrates significant performance enhancements for more complex tasks (ranging from <em>k</em> = 2 to <em>k</em> = 10). Our ToT and CoT method considerably enhances the performance of the Davinci and GPT-4, particularly in larger hops. For the Turbo model, although our CoT method brings improvements as <em>k</em> increases, the gains are not as profound as those observed with the Davinci and GPT-4. This could be attributed to the long length of our prompts, requiring a nuanced understanding of coordinates and relations.</p>
<h3>Conclusion</h3>
<p>This paper has introduced a revised version of the StepGame benchmark, correcting template errors that distort model performance evaluations, leading to a more accurate evaluation of the spatial reasoning capabilities of AI systems attempting the challenge. We highlight Davinci and Turbo's abilities in mapping texts to spatial relations and their limitations in multi-hop spatial reasoning. Our solution combines template-to-relation mapping with logic-based reasoning, effectively addressing challenges in this task. We also enhance LLMs' spatial reasoning ability through prompt engineering, using CoT and ToT strategies.</p>
<p>This paper focuses on StepGame; future studies could extend our findings to other benchmarks. Our methods are suitable for adaptation to various 2D grid-based directional spatial tasks, such as the bAbI (task 17). This adaptation would involve customizing the template for the ASP-based solution and modifying task descriptions and few-shot examples for CoT and ToT approaches. For tasks that require a combination of directional, topological, and distance reasoning, like SpartQA, it would be necessary to integrate additional rules and ontology into both the ASP program and the prompts to LLMs for effective solution development.</p>
<p>The effective resolution of the StepGame benchmark prompts a need for more challenging versions. While having a well-defined set of spatial relations converted into natural language using a set of templates is appealing, it leads to controlled natural language which is more amenable to special purpose reasoning. Finding a way to generate more naturalistic problem statements automatically would therefore be highly desirable. Additionally, the current independent use of LLMs and logic programs suggests a potential research direction towards integrating these tools for more comprehensive and cohesive problem-solving strategies.</p>
<h2>Acknowledgments</h2>
<p>This work has been partially supported by Microsoft Research - Accelerating Foundation Models Research program, with the provision of Azure resources to access GPT. This work was also partially supported by the Turing's Defence and Security programme through a partnership with the UK government in accordance with the framework agreement between GCHQ and The Alan Turing Institute.</p>
<h2>Author Contributions</h2>
<p>AGC and DCH proposed the initial line of work. FL designed the actual implementation, performed all the evaluations, and wrote the initial paper draft. DCH and AGC supervised FL. All authors contributed to subsequent paper revisions.</p>
<h2>References</h2>
<p>Alomari, M.; Li, F.; Hogg, D. C.; and Cohn, A. G. 2022. Online perceptual learning and natural language acquisition for autonomous robots. Artificial Intelligence, 303: 103637.
Bang, Y.; Cahyawijaya, S.; Lee, N.; Dai, W.; Su, D.; Wilie, B.; Lovenia, H.; Ji, Z.; Yu, T.; Chung, W.; et al. 2023. A multitask, multilingual, multimodal evaluation of ChatGPT on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023.
Bommasani, R.; Hudson, D. A.; Adeli, E.; Altman, R.; Arora, S.; von Arx, S.; Bernstein, M. S.; Bohg, J.; Bosselut, A.; Brunskill, E.; et al. 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258.
Chen, J.; Cohn, A. G.; Liu, D.; Wang, S.; Ouyang, J.; and Yu, Q. 2015. A survey of qualitative spatial representations. The Knowledge Engineering Review, 30(1): 106-136.
Cohn, A. G.; and Hazarika, S. M. 2001. Qualitative spatial representation and reasoning: An overview. Fundamenta informaticae, 46(1-2): 1-29.
Cohn, A. G.; and Hernandez-Orallo, J. 2023. Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs. arXiv preprint arXiv:2304.11164.
Cohn, A. G.; and Renz, J. 2008. Qualitative spatial representation and reasoning. Foundations of Artificial Intelligence, 3: 551-596.
Creswell, A.; Shanahan, M.; and Higgins, I. 2022. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712.
Kordjamshidi, P.; Moens, M.-F.; and van Otterlo, M. 2010. Spatial role labeling: Task definition and annotation scheme. In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC'10), 413-420. European Language Resources Association (ELRA).
Kordjamshidi, P.; Van Otterlo, M.; and Moens, M.-F. 2011. Spatial role labeling: Towards extraction of spatial relations from natural language. ACM Transactions on Speech and Language Processing (TSLP), 8(3): 1-36.</p>
<p>Li, F.; Hogg, D. C.; and Cohn, A. G. 2022. Ontology Knowledge-enhanced In-Context Learning for ActionEffect Prediction. In Advances in Cognitive Systems. ACS2022.</p>
<p>Mirzaee, R.; and Kordjamshidi, P. 2022. Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning. arXiv preprint arXiv:2210.16952.
Mirzaee, R.; and Rajaby, H. 2021. SpartQA: A Textual Question Answering Benchmark for Spatial Reasoning. In The 2021 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-2021).
OpenAI. 2023. GPT-4 Technical Report. ArXiv, abs/2303.08774.
Shi, Z.; Zhang, Q.; and Lipani, A. 2022. Stepgame: A new benchmark for robust multi-hop spatial reasoning in texts. In Proceedings of the AAAI Conference on Artificial Intelligence, 11321-11329.
Skiadopoulos, S.; and Koubarakis, M. 2001. Composing cardinal direction relations. In International Symposium on Spatial and Temporal Databases, 299-317. Springer.
Wang, L.; Xu, W.; Lan, Y.; Hu, Z.; Lan, Y.; Lee, R. K.-W.; and Lim, E.-P. 2023. Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models. arXiv preprint arXiv:2305.04091.
Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Chi, E.; Le, Q.; and Zhou, D. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.
Weston, J.; Bordes, A.; Chopra, S.; Rush, A. M.; Van Merriënboer, B.; Joulin, A.; and Mikolov, T. 2015. Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698.
Yang, Z.; Ishay, A.; and Lee, J. 2023. Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text. arXiv preprint arXiv:2307.07696.
Yao, S.; Yu, D.; Zhao, J.; Shafran, I.; Griffiths, T. L.; Cao, Y.; and Narasimhan, K. 2023. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601.
Ye, J.; Chen, X.; Xu, N.; Zu, C.; Shao, Z.; Liu, S.; Cui, Y.; Zhou, Z.; Gong, C.; Shen, Y.; et al. 2023. A comprehensive capability analysis of gpt-3 and gpt-3.5 series models. arXiv preprint arXiv:2303.10420.
Zhang, Z.; Zhang, A.; Li, M.; and Smola, A. 2022. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493.
Zhou, D.; Schärli, N.; Hou, L.; Wei, J.; Scales, N.; Wang, X.; Schuurmans, D.; Cui, C.; Bousquet, O.; Le, Q.; et al. 2022. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625.</p>
<h2>A. Example Prompts for Base</h2>
<p>Given a story about spatial relations among objects, answer the relation between two queried objects. Possible relations are: overlap, above, below, left, right, upper-left, upper-right, lower-left, and lower-right. If a sentence in the story is describing clock-wise information, then 12 denotes above, 1 and 2 denote upper-right, 3 denotes right, 4 and 5 denote lower-right, 6 denotes below, 7 and 8 denote lower-left, 9 denote left, 10 and 11 denote upper-left. If the sentence is describing cardinal directions, then north denotes above, east denotes right, south denotes below, and west denotes left.</p>
<p>Story:
$Q$ is to the right of $O$ and is on the same horizontal plane.
$Q$ is slightly off center to the top left and $M$ is slightly off center to the bottom right.
$X$ and $E$ are next to each other with $X$ on the top and $E$ at the bottom.
$O$ is sitting at the upper right position to E.
W is on the right side and below M.
What is the relation of the agent $W$ to the agent E?
Answer: lower-right</p>
<h2>$\cdots$</h2>
<p>Story:</p>
<ol>
<li>The object E is positioned directly above the object W.</li>
<li>E is sitting at the upper right position to I.</li>
<li>W is placed at the upper left of C.</li>
<li>$L$ is over there and $Y$ is on the left.</li>
<li>C and Y are both there with the object Y below the object C.</li>
<li>What is the relation of the agent $E$ to the agent Y ?</li>
</ol>
<h2>B. Example Prompts for CoT in (Yang, Ishay, and Lee 2023)</h2>
<p>Given a story about spatial relations among objects, answer the relation between two queried objects. Possible relations are: overlap, above, below, left, right, upper-left, upper-right, lower-left, and lower-right. If a sentence in the story is describing clock-wise information, then 12 denotes above, 1 and 2 denote upper-right, 3 denotes right, 4 and 5 denote lower-right, 6 denotes below, 7 and 8 denote lower-left, 9 denote left, 10 and 11 denote upper-left.</p>
<p>If the sentence is describing cardinal directions, then north denotes above, east denotes right, south denotes below, and west denotes left.</p>
<p>Story:
$Q$ is to the right of $O$ and is on the same horizontal plane.
$Q$ is slightly off center to the top left and $M$ is slightly off center to the bottom right.
$X$ and $E$ are next to each other with $X$ on the top and $E$ at the bottom.
$O$ is sitting at the upper right position to E.</p>
<p>W is on the right side and below M.
What is the relation of the agent $W$ to the agent E?
Answer: We first link W and E using the relations in the story. W is to the lower-right of M. M is to the lower-right of Q. Q is to the right of O. O is to the upper-right of E. So the answer is lower-right.</p>
<h2>$\cdots$</h2>
<p>Story:</p>
<ol>
<li>The object E is positioned directly above the object W.</li>
<li>E is sitting at the upper right position to I.</li>
<li>W is placed at the upper left of C.</li>
<li>L is over there and Y is on the left.</li>
<li>C and Y are both there with the object Y below the object C.</li>
<li>What is the relation of the agent $E$ to the agent Y ?</li>
</ol>
<h2>C. Example Prompts for Our CoT</h2>
<p>Given a story about spatial relations among objects, answer the relation between two queried objects. Possible relations are: overlap, above, below, left, right, upper-left, upper-right, lower-left, and lower-right. If a sentence in the story is describing clock-wise information, then 12 denotes above, 1 and 2 denote upper-right, 3 denotes right, 4 and 5 denote lower-right, 6 denotes below, 7 and 8 denote lower-left, 9 denote left, 10 and 11 denote upper-left. If the sentence is describing cardinal directions, then north denotes above, east denotes right, south denotes below, and west denotes left. In all the spatial relations, assume that all agents occupy a position on a grid point of equally spaced points in the vertical and horizontal directions and that agents occupy the nearest grid point consistent</p>
<p>with the spatial relation. The offsets of 9 spacial relations: offset (overlap) = $(0,0)$; offset (above) $=(0,1)$; offset (below) $=(0,-1) ;$ offset (left) $=(-1,0)$; offset (right) $=(1,0) ;$ offset (upper-left) $=(-1,1) ;$ offset (upper-right) $=$ $(1,1) ;$ offset (lower-left) $=(-1,-1)$; offset (lower-right) $=(1,-1)$.</p>
<p>Story:</p>
<ol>
<li>$Q$ is to the right of $O$ and is on the same horizontal plane.</li>
<li>$Q$ is slightly off center to the top left and $M$ is slightly off center to the bottom right.</li>
<li>$X$ and $E$ are next to each other with $X$ on the top and $E$ at the bottom.</li>
<li>$O$ is sitting at the upper right position to E.</li>
<li>W is on the right side and below M. What is the relation of the agent $W$ to the agent E?
Reasoning:
Let's suppose W is at $(0,0)$. We can connect W and E using the relations given in the story.
Start with W. According to 5, "W is on the right side and below M." This means $M$ is to the upper-left of $W . M=W+$ offset (upper-left) $=(0,0)+(-1,1)=(-1,1)$. Therefore, $M$ is at $(-1,1)$.
Then search for M. According to 2, "Q is slightly off center to the top left and $M$ is slightly off center to the bottom right." This means $Q$ is to the upper-left of M. Q= M+ offset (upper-left) $=(-1,1)+(-1,1)=(-2,2)$. Therefore, $Q$ is at $(-2,2)$.
Then search for $Q$. According to 1, "Q is to the right of $O$ and is on the same horizontal plane." This means $O$ is to the left of $Q$. $0=Q+$ offset (left) $=(-2,2)+(-1,0)=(-3,2)$. Therefore, $O$ is at $(-3,2)$.
Then search for $O$. According to 4, "O is sitting at the upper right position to E." This means E is to the lower-left of $O . \mathrm{E}=\mathrm{O}+$ offset (lower-left) $=$ $(-3,2)+(-1,-1)=(-4,1)$. Therefore, E is at $(-4,1)$.
We've reached E. So, considering W $(0,0)$ and $\mathrm{E}(-4,1), \mathrm{W}$ is to the lower-right of E . Answer: lower-right</li>
</ol>
<p>Story:</p>
<ol>
<li>The object $E$ is positioned directly above the object W.</li>
<li>E is sitting at the upper right position
to I.</li>
<li>W is placed at the upper left of C.</li>
<li>L is over there and $Y$ is on the left.</li>
<li>C and Y are both there with the object Y below the object C.</li>
<li>What is the relation of the agent E to the agent Y ?</li>
</ol>
<h2>D. Example Prompts for Our ToT</h2>
<h2>D.1. Tree state initialization prompt</h2>
<p>Provided with a sequence of statements that define the spatial relationships among various objects, your task is to detail the subsequent actions. This includes initiating the chain of connections, identifying the target object, and enumerating all links between objects from the statements.</p>
<p>Input: 1. $Q$ is to the right of $O$ and is on the same horizontal plane. 2. $Q$ is slightly off center to the top left and $M$ is slightly off center to the bottom right. 3. X and E are next to each other with X on the top and $E$ at the bottom. 4. O is sitting at the upper right position to E. 5. W is on the right side and below M. What is the relation of the agent $W$ to the agent $E$ ?
Possible next steps:
chain: W -&gt;, target: E, unused: 1. Q-O, 2. Q-M, 3. X-E, 4. O-E, 5. W-M.</p>
<p>Input: {input}
Possible next steps:</p>
<h2>D.2. Thought generation prompt</h2>
<p>Use relations listed in unused relations to enumerate all potential expansions of the chain by considering unused relations that exhibit a direct link to the last object within the chain.</p>
<p>Input: chain: G -&gt;, target: Q, unused: 1. C-R, 2. L-Q, 3. C-J, 4. J-E, 5. T-A, 6. G-N, 7. G-A, 8. L-Y, 9. R-Q, 10. Y-T.</p>
<p>Possible next steps:
The last object within the chain is G, and the unused relations 6. G-N and 7. G-A include G. relation chain: G -&gt; N (use 6) -&gt;, target: Q, unused: 1. C-R, 2. L-Q, 3. C-J, 4. J-E, 5. T-A, 7. G-A, 8. L-Y, 9. R-Q, 10. Y-T.
chain: G -&gt; A (use 7) -&gt;, target: Q, unused: 1. C-R, 2. L-Q, 3. C-J, 4. J-E, 5. T-A, 6. G-N, 8. L-Y, 9. R-Q, 10. Y-T.</p>
<p>Input: {input}
Possible next steps:</p>
<h2>D.3. State evaluation prompt</h2>
<p>Evaluate whether the chain can reach the target (sure/ likely/impossible). If the chain has already reached the target, it's 'sure'. If the unused relations include the current object, it's 'likely'. If there are no unused relations that include the current object, it's 'impossible'.
chain: F -&gt;, target: X, unused: 1. Y-F, 2. X-Y, 3. I-Q, 4. A-Q, 5. N-W, 6. N-A, 7. F-O, 8. O-W. The current object is F, there are unused relations that include F (1. Y-F, 7. F-O).
likely
chain: L -&gt; Q (use 2) -&gt;, target: Q, unused: 1. C-R, 3. C-J, 4. J-E, 7. G-A, 8. L-Y, 9. R-Q.
The chain already reaches the target object Q.
sure
chain: G -&gt; N (use 6) -&gt;, target: Q, unused: 1. C-R, 2. L-Q, 3. C-J, 4. J-E, 5. T-A, 8. L-Y, 9. R-Q, 10. Y-T.
The current object is N, and there are no unused relations that include N. impossible
{input}</p>
<h2>D.4. Linking chain construction prompt</h2>
<p>Given an input about spatial relations among objects, build the linking chain between the two queried objets.</p>
<p>Input:</p>
<ol>
<li>H is above S with a small gap between them. 2. S is positioned below I. 3. P is on the top side to I. What is the relation of the agent $S$ to the agent $P$ ?
Steps:
chain: S -&gt;, target: P, unused: 1. H-S, 2. S-I, 3. P-I.
chain: S -&gt; I (use 2) -&gt;, target: P, unused: 1. H-S, 3. P-I.
chain: I -&gt; P (use 3) -&gt;, target: P, unused: 1. H-S.
Answer: S -&gt; I (use 2) -&gt; P (use 3)</li>
</ol>
<p>Input:
{input}</p>
<h2>D.5. Spatial relation reasoning prompt</h2>
<p>Given a story about spatial relations among objects, answer the relation between two queried objects. Possible relations are: overlap, above, below, left, right, upper-left, upper-right, lower-left, and lower-right. If a sentence in the story is describing clock-wise information, then 12 denotes above, 1 and 2 denote upper-right, 3 denotes right, 4 and 5 denote lower-right, 6 denotes below, 7 and 8 denote lower-left, 9 denote left, 10 and 11 denote upper-left. If the sentence is describing cardinal directions, then north denotes above, east denotes right, south denotes below, and west denotes left. In all the spatial relations, assume that all agents occupy a position on a grid point of equally spaced points in the vertical and horizontal directions and that agents occupy the nearest grid point consistent with the spatial relation. The offsets of 9 spacial relations: offset (overlap) = $(0,0)$; offset (above) $=(0,1)$; offset (below) $=(0,-1)$; offset (left) $=(-1,0)$; offset (right) $=(1,0)$; offset (upper-left) $=(-1,1)$; offset (upper-right) $=$ $(1,1)$; offset(lower-left) $=(-1,-1)$; offset(lower-right) $=(1,-1)$.
Story:</p>
<ol>
<li>$Q$ is to the right of $O$ and is on the same horizontal plane.</li>
<li>$Q$ is slightly off center to the top left and $M$ is slightly off center to the bottom right.</li>
<li>$X$ and $E$ are next to each other with $X$ on the top and $E$ at the bottom.</li>
<li>O is sitting at the upper right position to E.</li>
<li>W is on the right side and below M. What is the relation of the agent $W$ to the agent E?
Linking chain: W -&gt; M (use 5) -&gt; Q (use 2) $-&gt;0$ (use 1) -&gt; E (use 4)
Reasoning:
Let's suppose W is at $(0,0)$. We can analyze the relation of $W$ to $E$ by following the linking chain and considering the relations provided in the story step by step. Start with W. According to 5, "W is on the right side and below M." This means $M$ is to the upper-left of $W . M=W+$ offset (upper-left) $=(0,0)+(-1,1)=(-1,1)$. Therefore, $M$ is at $(-1,1)$.
Then come to M. According to 2, "Q is slightly off center to the top left and $M$ is slightly off center to the bottom right." This means $Q$ is to the upper-left of M. Q= M+ offset (upper-left)</li>
</ol>
<p>$=(-1,1)+(-1,1)=(-2,2)$. Therefore, $Q$ is at $(-2,2)$.
Then come to Q. According to 1, "Q is to the right of 0 and is on the same horizontal plane." This means 0 is to the left of Q. $0=\mathrm{Q}+$ offset(left) $=(-2,2)+(-1,0)=(-3,2)$. Therefore, 0 is at $(-3,2)$.
Then come to 0 . According to 4 , " 0 is sitting at the upper right position to E." This means E is to the lower-left of $0 . \mathrm{E}=0+$ offset(lower-left) $=$ $(-3,2)+(-1,-1)=(-4,1)$. Therefore, $E$ is at $(-4,1)$.
We've reached E. So, considering W(0,0) and $\mathrm{E}(-4,1), \mathrm{W}$ is to the lower-right of E .
Answer: lower-right</p>
<p>Story:
{input}
Linking chain: {chain}</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ The ArXiv version of this paper includes the Appendix containing prompting examples.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
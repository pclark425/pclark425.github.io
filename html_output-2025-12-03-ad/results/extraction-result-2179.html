<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2179 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2179</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2179</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-57.html">extraction-schema-57</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <p><strong>Paper ID:</strong> paper-c72dd121863d9caaae0c9363278439bb42f0a8dc</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c72dd121863d9caaae0c9363278439bb42f0a8dc" target="_blank">Matter-of-Fact: A Benchmark for Verifying the Feasibility of Literature-Supported Claims in Materials Science</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work introduces Matter-of-Fact, a challenge dataset for determining the feasibility of hypotheses framed as claims, while operationalizing feasibility assessment as a temporally-filtered claim verification task using backtesting.</p>
                <p><strong>Paper Abstract:</strong> Contemporary approaches to assisted scientific discovery use language models to automatically generate large numbers of potential hypothesis to test, while also automatically generating code-based experiments to test those hypotheses. While hypotheses can be comparatively inexpensive to generate, automated experiments can be costly, particularly when run at scale (i.e. thousands of experiments). Developing the capacity to filter hypotheses based on their feasibility would allow discovery systems to run at scale, while increasing their likelihood of making significant discoveries. In this work we introduce Matter-of-Fact, a challenge dataset for determining the feasibility of hypotheses framed as claims, while operationalizing feasibility assessment as a temporally-filtered claim verification task using backtesting. Matter-of-Fact includes 8.4k claims extracted from scientific articles spanning four high-impact contemporary materials science topics, including superconductors, semiconductors, batteries, and aerospace materials, while including qualitative and quantitative claims from theoretical, experimental, and code/simulation results. We show that strong baselines that include retrieval augmented generation over scientific literature and code generation fail to exceed 72% performance on this task (chance performance is 50%), while domain-expert verification suggests nearly all are solvable -- highlighting both the difficulty of this task for current models, and the potential to accelerate scientific discovery by making near-term progress.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2179.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2179.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG (SemanticScholar) + O4-MINI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation using SemanticScholar with O4-MINI base model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented LLM pipeline that retrieves temporally-filtered full-text snippets from SemanticScholar and prompts an O4-MINI language model (with Chain-of-Thought and ICL) to assess the feasibility of literature-derived scientific claims.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for knowledge-intensive nlp tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>RAG (SemanticScholar) + O4-MINI</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>retrieval-augmented language model</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>materials science (feasibility assessment / claim verification)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>assessing feasibility of literature-supported scientific claims (binary true/feasible vs false/infeasible); produces explanations and evidence-backed verdicts</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Retrieves top-K temporally-filtered full-text snippets from SemanticScholar (no documents authored after the claim's source paper), supplies them in-prompt to the LLM (20-shot ICL + CoT), and uses the model's evidence-based reasoning as the validation decision; compared against oracle (source paper) verification and human domain expert labels.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Temporal filtering / knowledge cutoff date: claims authored after a model's cutoff are treated as novel; claim categories (qualitative vs quantitative; experiment/code/theory/integrative) used as secondary difficulty/novelty axes.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Feasibility-assessment (temporal restriction) overall accuracy 0.71; true (feasible) detection rate 0.60; false (infeasible) detection rate 0.82. In temporally-unrestricted claim verification (RAG no-date) O4-MINI achieved 0.90 accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Under temporal restriction (predicting future/unseen claims) accuracy ~0.71 with asymmetric class performance (better at detecting infeasible claims: 0.82). With temporally-unrestricted retrieval including post-source literature, accuracy rises to ~0.90; when provided the oracle source paper, O4-MINI reached 0.96 accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Approximately 0.18 for the RAG (O4-MINI) feasibility setting (1 - False detection rate = 1 - 0.82 = 0.18); this reflects accepting invalid/infeasible claims as feasible.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Approximately 0.40 for the RAG (O4-MINI) feasibility setting (1 - True detection rate = 1 - 0.60 = 0.40); this reflects rejecting valid/feasible claims as infeasible.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Validation accuracy is substantially lower under temporal novelty (feasibility-assessment mode) than when later literature or the source paper is available (0.71 vs 0.90), indicating degraded reliability on novel/future claims; qualitative/theoretical/integrative claims are easier than quantitative/experimental/code claims (11–19% higher accuracy on qualitative vs quantitative).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Yes — large improvement when models can access later literature or the original source (familiar cases) versus when predicting future/unseen results (novel cases), demonstrating better verification of familiar outputs than prediction of novel ones.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>When treated as 'novel' by temporal filtering, performance drops relative to unrestricted retrieval (0.71 vs 0.90). However, base-model contamination analysis in the paper shows per-model performance on claims before vs after a model's advertised cutoff is similar (±~1%), implying the OOD effect here is driven by retrieval restrictions rather than model memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported in the paper (no calibration/confidence metrics provided).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Relatively low: Table 2 reports an estimated cost of about $27 per 1,000 claims for CoT + ICL + RAG with O4-MINI (costs depend on base model and pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Access to the oracle/source paper (temporal-unrestricted retrieval); human-in-the-loop verification; combining RAG with code-based experiments; in-context learning and reflection steps provided modest improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RAG over SemanticScholar with O4-MINI achieves moderate feasibility-assessment accuracy (0.71) but substantially better verification when allowed to access later literature or the source paper (up to 0.96), demonstrating a clear gap between verifying familiar claims and predicting novel/future claims.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2179.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2179.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CODESCIENTIST</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CODESCIENTIST: End-to-end semiautomated scientific discovery with code-based experimentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid system in which an LLM generates Python experiments/simulations, the code is executed in a sandbox (Modal.com), and execution logs/results are returned to the model to support a final feasibility decision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Codescientist: End-to-end semiautomated scientific discovery with code-based experimentation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CODESCIENTIST</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>hybrid LLM + executable code experimentation system</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>materials science (feasibility assessment / experimental simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generates code-based experiments or simulations to produce evidence in support of or against scientific claims; can produce experimental protocols, scripts, and analysis code.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Executes the generated Python code in a restricted sandbox, captures stdout/stderr and logs, and supplies execution results back to the LLM to form a final verdict; validation therefore relies on simulated/analytic experiment outputs rather than (or prior to) physical experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Temporal filtering (knowledge cutoff) used for novelty; novelty also arises from quantitative/experiment-heavy claims requiring accurate simulation; no formal 'distance-from-training-data' novelty metric provided.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Feasibility-assessment performance using the Code pipeline: O4-MINI CoT+ICL+CODE overall accuracy 0.68 (True detection 0.66, False detection 0.71). GPT-40-MINI and Claude Sonnet 3.7 showed similar or slightly lower CODE-run accuracies (0.64 and 0.63 respectively).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation via code outputs produced modest gains but did not exceed the best RAG pipelines; constrained by practical runtime and iteration limits (in this study: single iteration, 10-minute time limit) which reduced ability to fully validate complex or novel claims.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>For O4-MINI CoT+ICL+CODE, false positive rate ≈ 0.29 (1 - False detection rate = 1 - 0.71 = 0.29).</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>For O4-MINI CoT+ICL+CODE, false negative rate ≈ 0.34 (1 - True detection rate = 1 - 0.66 = 0.34).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Code-based validation is particularly challenged by quantitative and experiment/code claims (lowest model performance category), and limited compute/time budgets further reduce validation effectiveness on novel or complex experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Yes — while the system can generate executable experiments, practical limitations (single short run, limited debugging) and the limits of simulation fidelity lead to lower validation success compared to verifying claims with direct literature evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not quantified beyond the feasibility-assessment numbers above; constrained sandbox runs limit the ability to address deep OOD novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>High relative to pure-text RAG: the paper cites an estimated per-experiment cost historically around $4 (they limited runs for tractability); Table 2 reports CODE pipeline costs per 1,000 claims (example: O4-MINI CoT+ICL+CODE ≈ $34 per 1k).</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Longer/more iterative code-debug-execute-reflect loops, increased compute and runtime for simulations, and human-in-the-loop oversight/debugging; combining execution outputs with literature retrieval can improve evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Code-generation + execution can provide concrete experiment-like evidence for claim assessment, but in practice constraints on runtime, iteration budget, and simulation fidelity limit its validation performance; in this benchmark CODE pipelines performed worse than the best retrieval-based pipelines.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2179.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2179.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Base LLMs (GPT-4O-MINI, O4-MINI, Claude Sonnet 3.7)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Base Large Language Models used as core reasoning agents (GPT-4O-MINI, O4-MINI, Claude Sonnet 3.7)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three base LLMs evaluated across multiple prompting and augmentation strategies (Chain-of-Thought, in-context learning, reflection, RAG, executed-code pipelines) for the task of feasibility assessment of materials-science claims.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4O-MINI / O4-MINI / Claude Sonnet 3.7</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language models</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>materials science claim feasibility assessment and general scientific reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generate claims, hypotheses, chain-of-thought reasoning, retrieval queries, and code (when prompted); used to produce feasibility predictions and explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Internal reasoning (CoT), retrieval-augmented evidence incorporation (RAG), code-based experiment generation (CODESCIENTIST), and comparison against human domain expert and oracle-source-paper baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Novelty operationalized via temporal cutoff (claims authored after a model's training cutoff treated as novel); claim category (qualitative/quantitative and experimental/code/theory/integrative) also used as a difficulty/novelty proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Feasibility-assessment overall accuracies ranged ~0.58–0.72 depending on model and pipeline; GPT-4O-MINI typically ~0.64–0.68 across pipelines, O4-MINI around 0.68–0.71, and Claude Sonnet 3.7 up to 0.72 for some pipelines (see Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>When allowed temporally-unrestricted retrieval or given the oracle source paper, verification performance rose substantially: O4-MINI RAG (no-date) 0.90 and Oracle Source Paper 0.96; Claude Sonnet 3.7 Oracle reached 1.00 on evaluated subset. This shows strong verification in familiar/found-evidence settings.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Varies by pipeline; across pipelines False detection accuracies in Table 2 ranged approximately 0.40–0.82, so false positive rates (accepting invalid as valid) range roughly from 0.18 to 0.60 depending on model and setup.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Varies by pipeline; True detection accuracies ranged roughly 0.58–0.89 in Table 2, yielding false negative rates in the range ~0.11 to 0.42.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Models perform substantially worse on temporally-novel feasibility assessment than on familiar verification tasks with access to later literature or source papers; category-level analysis shows quantitative and experiment/code claims are notably harder (11–19% lower accuracy). Table 4 indicates base-model memorization does not explain the gap (performance before vs after advertised cutoff varied only ±~1%).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Yes — consistent evidence that generating or verifying familiar claims (with direct evidence) is easier than predicting the feasibility of future/unseen experimental results, producing an observable generation-vs-validation gap.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Base-model contamination analysis (Table 4) shows small differences in model accuracy on claims from papers before vs after model cutoff (±1%), indicating that the larger performance drop in feasibility assessment stems from lack of accessible evidence or reasoning limits rather than simple memorization; OOD novel claims (temporal) show lower effective validation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Costs vary by pipeline and base model (Table 2): examples include GPT-4O-MINI CoT ≈ $1 per 1k, O4-MINI CoT+20-shot ICL ≈ $15 per 1k, RAG and CODE pipelines cost more ($27–$173 per 1k depending on base model and pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Providing oracle/source papers or temporally-unrestricted retrieval, improved retrieval/reranking, chain-of-thought + ICL + reflection, code-based experiments (with more iterations), and human expert review.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Top base LLMs can nearly perfectly verify claims when provided the source articles (oracle), but their ability to predict feasibility of genuine future claims under temporal restriction is substantially lower (~0.58–0.72), demonstrating a robust gap between verifying familiar results and predicting novel outcomes.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2179.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2179.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hypothesis Generation Systems (generic)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated Hypothesis Generation Systems (e.g., Lu et al., 2024; Jansen et al., 2025; O'Neill et al., 2025)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automated systems and agent pipelines that generate large numbers of research hypotheses or ideas (potentially thousands), often grounded in literature facets or using LLMs; cited as upstream producers of candidate claims that require feasibility filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Automated Hypothesis Generation Systems</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based generation pipelines / autonomous agents</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general scientific discovery / hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generate many candidate hypotheses, proposals, or research ideas (textual claims) and sometimes code-based experiment plans.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Typically require external validation via experiments, simulations, literature verification, or human experts; the paper recommends feasibility-assessment filtering (literature + inexpensive simulation) to reduce experimental cost.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Not formalized in this paper; novelty implied by the fact that many generated hypotheses are untested/future and only a small fraction (e.g., hypothetical 1%) are true/feasible in example scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>No direct quantitative generation-success metrics reported here; described qualitatively as capable of producing thousands of hypotheses, with true/feasible rate potentially very low (example scenario: 1%).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Not quantified; paper stresses that without feasibility filtering, validating generated hypotheses via experiments is costly and intractable at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Paper argues novelty exacerbates validation costs and uncertainty — many generated hypotheses are future/unseen results and require empirical testing, making validation the bottleneck.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Yes — generation is high-throughput but validation (experimentation) is low-throughput and costly; the paper frames feasibility assessment as a means to reduce this asymmetry.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>High when experiments are physical; even large-scale simulated experiments can be expensive; motivating goal of feasibility assessment is to reduce required experimental budget.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Feasibility filtering (this benchmark), literature retrieval, code-based pilot experiments, and human-in-the-loop triage.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Automated hypothesis generation is prolific but creates a validation bottleneck; feasibility-assessment systems (like those benchmarked) can materially reduce experiment costs and improve discovery efficiency if they reliably filter infeasible hypotheses.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2179.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2179.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Problem-specific discovery systems (AlphaFold, GNoME, Schmidt et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Problem-specific machine learning discovery systems (e.g., AlphaFold; GNoME; Schmidt et al.'s crystal-graph NN approaches)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Domain-tailored computational systems that perform discovery tasks (protein folding, stable crystal discovery, material stability screening) with high domain-specific accuracy and large-scale screening capability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Problem-specific discovery systems (AlphaFold, GNoME, crystal-graph models)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>problem-specific neural networks / graph neural networks / specialized ML pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>protein structure prediction; materials discovery and stability prediction</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Predict molecular/structural outputs and discover candidate compounds/materials (e.g., protein structures, stable crystals, extreme-property materials).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validated using domain-specific datasets, high-quality ab initio calculations, large-scale screening, and experimental follow-up when possible.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Novelty assessed by discovery of new structures/materials not in training sets and by scale (e.g., number of new candidates discovered); no single formal novelty metric provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported example achievements cited: AlphaFold achieved high accuracy in protein structure prediction (Jumper et al.); GNoME discovered over 2.2M new stable crystal structures; Schmidt et al. screened ~1 billion materials and discovered 150k+ stable compounds. (These are cited results from the referenced literature.)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Typically high within domain when validated against physics-based calculations or experimental results; used as examples of successful problem-specific discovery that can inform feasibility assessments for specific properties.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Problem-specific methods are often better at OOD within their narrow domain than generalist LLMs because they encode domain physics and have tailored validation pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Less pronounced for these systems because their outputs are typically validated by specialized computational pipelines and (sometimes) experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Depends on the domain and model; these systems can generalize well within narrowly defined property spaces but are not general-purpose across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Large-scale screening and high-fidelity simulations can be computationally expensive but are engineered to be tractable at scale for the target problem.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Combining ML predictions with physics-based models and targeted experimental validation; these systems exemplify how problem-specific specialization can reduce the generation-validation gap.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Problem-specific discovery systems demonstrate that targeted models with physics knowledge and validation pipelines can discover many novel candidates and effectively validate them, suggesting one avenue to reduce the general generation-vs-validation gap for particular scientific problems.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Codescientist: End-to-end semiautomated scientific discovery with code-based experimentation. <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks. <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery. <em>(Rating: 2)</em></li>
                <li>Scaling deep learning for materials discovery. <em>(Rating: 1)</em></li>
                <li>Highly accurate protein structure prediction with alphafold. <em>(Rating: 1)</em></li>
                <li>The semantic scholar open data platform. <em>(Rating: 2)</em></li>
                <li>Sparks of science: Hypothesis generation using structured paper data. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2179",
    "paper_id": "paper-c72dd121863d9caaae0c9363278439bb42f0a8dc",
    "extraction_schema_id": "extraction-schema-57",
    "extracted_data": [
        {
            "name_short": "RAG (SemanticScholar) + O4-MINI",
            "name_full": "Retrieval-Augmented Generation using SemanticScholar with O4-MINI base model",
            "brief_description": "A retrieval-augmented LLM pipeline that retrieves temporally-filtered full-text snippets from SemanticScholar and prompts an O4-MINI language model (with Chain-of-Thought and ICL) to assess the feasibility of literature-derived scientific claims.",
            "citation_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks.",
            "mention_or_use": "use",
            "system_name": "RAG (SemanticScholar) + O4-MINI",
            "system_type": "retrieval-augmented language model",
            "domain": "materials science (feasibility assessment / claim verification)",
            "generation_capability": "assessing feasibility of literature-supported scientific claims (binary true/feasible vs false/infeasible); produces explanations and evidence-backed verdicts",
            "validation_method": "Retrieves top-K temporally-filtered full-text snippets from SemanticScholar (no documents authored after the claim's source paper), supplies them in-prompt to the LLM (20-shot ICL + CoT), and uses the model's evidence-based reasoning as the validation decision; compared against oracle (source paper) verification and human domain expert labels.",
            "novelty_measure": "Temporal filtering / knowledge cutoff date: claims authored after a model's cutoff are treated as novel; claim categories (qualitative vs quantitative; experiment/code/theory/integrative) used as secondary difficulty/novelty axes.",
            "generation_performance": "Feasibility-assessment (temporal restriction) overall accuracy 0.71; true (feasible) detection rate 0.60; false (infeasible) detection rate 0.82. In temporally-unrestricted claim verification (RAG no-date) O4-MINI achieved 0.90 accuracy.",
            "validation_performance": "Under temporal restriction (predicting future/unseen claims) accuracy ~0.71 with asymmetric class performance (better at detecting infeasible claims: 0.82). With temporally-unrestricted retrieval including post-source literature, accuracy rises to ~0.90; when provided the oracle source paper, O4-MINI reached 0.96 accuracy.",
            "false_positive_rate": "Approximately 0.18 for the RAG (O4-MINI) feasibility setting (1 - False detection rate = 1 - 0.82 = 0.18); this reflects accepting invalid/infeasible claims as feasible.",
            "false_negative_rate": "Approximately 0.40 for the RAG (O4-MINI) feasibility setting (1 - True detection rate = 1 - 0.60 = 0.40); this reflects rejecting valid/feasible claims as infeasible.",
            "novelty_effect_on_validation": "Validation accuracy is substantially lower under temporal novelty (feasibility-assessment mode) than when later literature or the source paper is available (0.71 vs 0.90), indicating degraded reliability on novel/future claims; qualitative/theoretical/integrative claims are easier than quantitative/experimental/code claims (11–19% higher accuracy on qualitative vs quantitative).",
            "generation_validation_asymmetry": "Yes — large improvement when models can access later literature or the original source (familiar cases) versus when predicting future/unseen results (novel cases), demonstrating better verification of familiar outputs than prediction of novel ones.",
            "out_of_distribution_performance": "When treated as 'novel' by temporal filtering, performance drops relative to unrestricted retrieval (0.71 vs 0.90). However, base-model contamination analysis in the paper shows per-model performance on claims before vs after a model's advertised cutoff is similar (±~1%), implying the OOD effect here is driven by retrieval restrictions rather than model memorization.",
            "calibration_quality": "Not reported in the paper (no calibration/confidence metrics provided).",
            "validation_computational_cost": "Relatively low: Table 2 reports an estimated cost of about $27 per 1,000 claims for CoT + ICL + RAG with O4-MINI (costs depend on base model and pipeline).",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Access to the oracle/source paper (temporal-unrestricted retrieval); human-in-the-loop verification; combining RAG with code-based experiments; in-context learning and reflection steps provided modest improvements.",
            "evidence_type": "supports",
            "key_findings": "RAG over SemanticScholar with O4-MINI achieves moderate feasibility-assessment accuracy (0.71) but substantially better verification when allowed to access later literature or the source paper (up to 0.96), demonstrating a clear gap between verifying familiar claims and predicting novel/future claims.",
            "uuid": "e2179.0"
        },
        {
            "name_short": "CODESCIENTIST",
            "name_full": "CODESCIENTIST: End-to-end semiautomated scientific discovery with code-based experimentation",
            "brief_description": "A hybrid system in which an LLM generates Python experiments/simulations, the code is executed in a sandbox (Modal.com), and execution logs/results are returned to the model to support a final feasibility decision.",
            "citation_title": "Codescientist: End-to-end semiautomated scientific discovery with code-based experimentation.",
            "mention_or_use": "use",
            "system_name": "CODESCIENTIST",
            "system_type": "hybrid LLM + executable code experimentation system",
            "domain": "materials science (feasibility assessment / experimental simulation)",
            "generation_capability": "Generates code-based experiments or simulations to produce evidence in support of or against scientific claims; can produce experimental protocols, scripts, and analysis code.",
            "validation_method": "Executes the generated Python code in a restricted sandbox, captures stdout/stderr and logs, and supplies execution results back to the LLM to form a final verdict; validation therefore relies on simulated/analytic experiment outputs rather than (or prior to) physical experiments.",
            "novelty_measure": "Temporal filtering (knowledge cutoff) used for novelty; novelty also arises from quantitative/experiment-heavy claims requiring accurate simulation; no formal 'distance-from-training-data' novelty metric provided.",
            "generation_performance": "Feasibility-assessment performance using the Code pipeline: O4-MINI CoT+ICL+CODE overall accuracy 0.68 (True detection 0.66, False detection 0.71). GPT-40-MINI and Claude Sonnet 3.7 showed similar or slightly lower CODE-run accuracies (0.64 and 0.63 respectively).",
            "validation_performance": "Validation via code outputs produced modest gains but did not exceed the best RAG pipelines; constrained by practical runtime and iteration limits (in this study: single iteration, 10-minute time limit) which reduced ability to fully validate complex or novel claims.",
            "false_positive_rate": "For O4-MINI CoT+ICL+CODE, false positive rate ≈ 0.29 (1 - False detection rate = 1 - 0.71 = 0.29).",
            "false_negative_rate": "For O4-MINI CoT+ICL+CODE, false negative rate ≈ 0.34 (1 - True detection rate = 1 - 0.66 = 0.34).",
            "novelty_effect_on_validation": "Code-based validation is particularly challenged by quantitative and experiment/code claims (lowest model performance category), and limited compute/time budgets further reduce validation effectiveness on novel or complex experiments.",
            "generation_validation_asymmetry": "Yes — while the system can generate executable experiments, practical limitations (single short run, limited debugging) and the limits of simulation fidelity lead to lower validation success compared to verifying claims with direct literature evidence.",
            "out_of_distribution_performance": "Not quantified beyond the feasibility-assessment numbers above; constrained sandbox runs limit the ability to address deep OOD novelty.",
            "calibration_quality": "Not reported.",
            "validation_computational_cost": "High relative to pure-text RAG: the paper cites an estimated per-experiment cost historically around $4 (they limited runs for tractability); Table 2 reports CODE pipeline costs per 1,000 claims (example: O4-MINI CoT+ICL+CODE ≈ $34 per 1k).",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Longer/more iterative code-debug-execute-reflect loops, increased compute and runtime for simulations, and human-in-the-loop oversight/debugging; combining execution outputs with literature retrieval can improve evidence.",
            "evidence_type": "mixed",
            "key_findings": "Code-generation + execution can provide concrete experiment-like evidence for claim assessment, but in practice constraints on runtime, iteration budget, and simulation fidelity limit its validation performance; in this benchmark CODE pipelines performed worse than the best retrieval-based pipelines.",
            "uuid": "e2179.1"
        },
        {
            "name_short": "Base LLMs (GPT-4O-MINI, O4-MINI, Claude Sonnet 3.7)",
            "name_full": "Base Large Language Models used as core reasoning agents (GPT-4O-MINI, O4-MINI, Claude Sonnet 3.7)",
            "brief_description": "Three base LLMs evaluated across multiple prompting and augmentation strategies (Chain-of-Thought, in-context learning, reflection, RAG, executed-code pipelines) for the task of feasibility assessment of materials-science claims.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-4O-MINI / O4-MINI / Claude Sonnet 3.7",
            "system_type": "large language models",
            "domain": "materials science claim feasibility assessment and general scientific reasoning",
            "generation_capability": "Generate claims, hypotheses, chain-of-thought reasoning, retrieval queries, and code (when prompted); used to produce feasibility predictions and explanations.",
            "validation_method": "Internal reasoning (CoT), retrieval-augmented evidence incorporation (RAG), code-based experiment generation (CODESCIENTIST), and comparison against human domain expert and oracle-source-paper baselines.",
            "novelty_measure": "Novelty operationalized via temporal cutoff (claims authored after a model's training cutoff treated as novel); claim category (qualitative/quantitative and experimental/code/theory/integrative) also used as a difficulty/novelty proxy.",
            "generation_performance": "Feasibility-assessment overall accuracies ranged ~0.58–0.72 depending on model and pipeline; GPT-4O-MINI typically ~0.64–0.68 across pipelines, O4-MINI around 0.68–0.71, and Claude Sonnet 3.7 up to 0.72 for some pipelines (see Table 2).",
            "validation_performance": "When allowed temporally-unrestricted retrieval or given the oracle source paper, verification performance rose substantially: O4-MINI RAG (no-date) 0.90 and Oracle Source Paper 0.96; Claude Sonnet 3.7 Oracle reached 1.00 on evaluated subset. This shows strong verification in familiar/found-evidence settings.",
            "false_positive_rate": "Varies by pipeline; across pipelines False detection accuracies in Table 2 ranged approximately 0.40–0.82, so false positive rates (accepting invalid as valid) range roughly from 0.18 to 0.60 depending on model and setup.",
            "false_negative_rate": "Varies by pipeline; True detection accuracies ranged roughly 0.58–0.89 in Table 2, yielding false negative rates in the range ~0.11 to 0.42.",
            "novelty_effect_on_validation": "Models perform substantially worse on temporally-novel feasibility assessment than on familiar verification tasks with access to later literature or source papers; category-level analysis shows quantitative and experiment/code claims are notably harder (11–19% lower accuracy). Table 4 indicates base-model memorization does not explain the gap (performance before vs after advertised cutoff varied only ±~1%).",
            "generation_validation_asymmetry": "Yes — consistent evidence that generating or verifying familiar claims (with direct evidence) is easier than predicting the feasibility of future/unseen experimental results, producing an observable generation-vs-validation gap.",
            "out_of_distribution_performance": "Base-model contamination analysis (Table 4) shows small differences in model accuracy on claims from papers before vs after model cutoff (±1%), indicating that the larger performance drop in feasibility assessment stems from lack of accessible evidence or reasoning limits rather than simple memorization; OOD novel claims (temporal) show lower effective validation performance.",
            "calibration_quality": "Not reported.",
            "validation_computational_cost": "Costs vary by pipeline and base model (Table 2): examples include GPT-4O-MINI CoT ≈ $1 per 1k, O4-MINI CoT+20-shot ICL ≈ $15 per 1k, RAG and CODE pipelines cost more ($27–$173 per 1k depending on base model and pipeline).",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Providing oracle/source papers or temporally-unrestricted retrieval, improved retrieval/reranking, chain-of-thought + ICL + reflection, code-based experiments (with more iterations), and human expert review.",
            "evidence_type": "supports",
            "key_findings": "Top base LLMs can nearly perfectly verify claims when provided the source articles (oracle), but their ability to predict feasibility of genuine future claims under temporal restriction is substantially lower (~0.58–0.72), demonstrating a robust gap between verifying familiar results and predicting novel outcomes.",
            "uuid": "e2179.2"
        },
        {
            "name_short": "Hypothesis Generation Systems (generic)",
            "name_full": "Automated Hypothesis Generation Systems (e.g., Lu et al., 2024; Jansen et al., 2025; O'Neill et al., 2025)",
            "brief_description": "Automated systems and agent pipelines that generate large numbers of research hypotheses or ideas (potentially thousands), often grounded in literature facets or using LLMs; cited as upstream producers of candidate claims that require feasibility filtering.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Automated Hypothesis Generation Systems",
            "system_type": "LLM-based generation pipelines / autonomous agents",
            "domain": "general scientific discovery / hypothesis generation",
            "generation_capability": "Generate many candidate hypotheses, proposals, or research ideas (textual claims) and sometimes code-based experiment plans.",
            "validation_method": "Typically require external validation via experiments, simulations, literature verification, or human experts; the paper recommends feasibility-assessment filtering (literature + inexpensive simulation) to reduce experimental cost.",
            "novelty_measure": "Not formalized in this paper; novelty implied by the fact that many generated hypotheses are untested/future and only a small fraction (e.g., hypothetical 1%) are true/feasible in example scenarios.",
            "generation_performance": "No direct quantitative generation-success metrics reported here; described qualitatively as capable of producing thousands of hypotheses, with true/feasible rate potentially very low (example scenario: 1%).",
            "validation_performance": "Not quantified; paper stresses that without feasibility filtering, validating generated hypotheses via experiments is costly and intractable at scale.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Paper argues novelty exacerbates validation costs and uncertainty — many generated hypotheses are future/unseen results and require empirical testing, making validation the bottleneck.",
            "generation_validation_asymmetry": "Yes — generation is high-throughput but validation (experimentation) is low-throughput and costly; the paper frames feasibility assessment as a means to reduce this asymmetry.",
            "out_of_distribution_performance": null,
            "calibration_quality": null,
            "validation_computational_cost": "High when experiments are physical; even large-scale simulated experiments can be expensive; motivating goal of feasibility assessment is to reduce required experimental budget.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Feasibility filtering (this benchmark), literature retrieval, code-based pilot experiments, and human-in-the-loop triage.",
            "evidence_type": "supports",
            "key_findings": "Automated hypothesis generation is prolific but creates a validation bottleneck; feasibility-assessment systems (like those benchmarked) can materially reduce experiment costs and improve discovery efficiency if they reliably filter infeasible hypotheses.",
            "uuid": "e2179.3"
        },
        {
            "name_short": "Problem-specific discovery systems (AlphaFold, GNoME, Schmidt et al.)",
            "name_full": "Problem-specific machine learning discovery systems (e.g., AlphaFold; GNoME; Schmidt et al.'s crystal-graph NN approaches)",
            "brief_description": "Domain-tailored computational systems that perform discovery tasks (protein folding, stable crystal discovery, material stability screening) with high domain-specific accuracy and large-scale screening capability.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Problem-specific discovery systems (AlphaFold, GNoME, crystal-graph models)",
            "system_type": "problem-specific neural networks / graph neural networks / specialized ML pipelines",
            "domain": "protein structure prediction; materials discovery and stability prediction",
            "generation_capability": "Predict molecular/structural outputs and discover candidate compounds/materials (e.g., protein structures, stable crystals, extreme-property materials).",
            "validation_method": "Validated using domain-specific datasets, high-quality ab initio calculations, large-scale screening, and experimental follow-up when possible.",
            "novelty_measure": "Novelty assessed by discovery of new structures/materials not in training sets and by scale (e.g., number of new candidates discovered); no single formal novelty metric provided in this paper.",
            "generation_performance": "Reported example achievements cited: AlphaFold achieved high accuracy in protein structure prediction (Jumper et al.); GNoME discovered over 2.2M new stable crystal structures; Schmidt et al. screened ~1 billion materials and discovered 150k+ stable compounds. (These are cited results from the referenced literature.)",
            "validation_performance": "Typically high within domain when validated against physics-based calculations or experimental results; used as examples of successful problem-specific discovery that can inform feasibility assessments for specific properties.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Problem-specific methods are often better at OOD within their narrow domain than generalist LLMs because they encode domain physics and have tailored validation pipelines.",
            "generation_validation_asymmetry": "Less pronounced for these systems because their outputs are typically validated by specialized computational pipelines and (sometimes) experiments.",
            "out_of_distribution_performance": "Depends on the domain and model; these systems can generalize well within narrowly defined property spaces but are not general-purpose across domains.",
            "calibration_quality": null,
            "validation_computational_cost": "Large-scale screening and high-fidelity simulations can be computationally expensive but are engineered to be tractable at scale for the target problem.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Combining ML predictions with physics-based models and targeted experimental validation; these systems exemplify how problem-specific specialization can reduce the generation-validation gap.",
            "evidence_type": "supports",
            "key_findings": "Problem-specific discovery systems demonstrate that targeted models with physics knowledge and validation pipelines can discover many novel candidates and effectively validate them, suggesting one avenue to reduce the general generation-vs-validation gap for particular scientific problems.",
            "uuid": "e2179.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Codescientist: End-to-end semiautomated scientific discovery with code-based experimentation.",
            "rating": 2
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks.",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2
        },
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery.",
            "rating": 2
        },
        {
            "paper_title": "Scaling deep learning for materials discovery.",
            "rating": 1
        },
        {
            "paper_title": "Highly accurate protein structure prediction with alphafold.",
            "rating": 1
        },
        {
            "paper_title": "The semantic scholar open data platform.",
            "rating": 2
        },
        {
            "paper_title": "Sparks of science: Hypothesis generation using structured paper data.",
            "rating": 2
        }
    ],
    "cost": 0.0245705,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Matter-of-Fact: A Benchmark for Verifying the Feasibility of Literature-Supported Claims in Materials Science</h1>
<p>Peter Jansen ${ }^{1,2}$, Samiah Hassan ${ }^{1}$, Ruoyao Wang ${ }^{1}$<br>${ }^{1}$ University of Arizona, ${ }^{2}$ Allen Institute for Artificial Intelligence<br>pajansen@arizona.edu</p>
<h4>Abstract</h4>
<p>Contemporary approaches to assisted scientific discovery use language models to automatically generate large numbers of potential hypothesis to test, while also automatically generating code-based experiments to test those hypotheses. While hypotheses can be comparatively inexpensive to generate, automated experiments can be costly, particularly when run at scale (i.e. thousands of experiments). Developing the capacity to filter hypotheses based on their feasibility would allow discovery systems to run at scale, while increasing their likelihood of making significant discoveries. In this work we introduce MATTER-OF-FACT, a challenge dataset for determining the feasibility of hypotheses framed as claims. MATTER-OF-FACT includes 8.4 K claims extracted from scientific articles spanning four high-impact contemporary materials science topics, including superconductors, semiconductors, batteries, and aerospace materials, while including qualitative and quantitative claims from theoretical, experimental, and code/simulation results. We show that strong baselines that include retrieval augmented generation over scientific literature and code generation fail to exceed $72 \%$ performance on this task (chance performance is $50 \%$ ), while domain-expert verification suggests nearly all are solvable - highlighting both the difficulty of this task for current models, and the potential to accelerate scientific discovery by making near-term progress. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Contemporary language models are being broadly integrated into the scientific discovery pipeline. Existing systems can generate hypothesis ( Si et al., 2024; Radensky et al., 2024), run experiments ( Lu et al., 2024; Li et al., 2024; Jansen et al., 2025), analyze data (Majumder et al., 2025), and write or review papers (Liu and Shah, 2023; Zhou et al.,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>2024). A central benefit - and challenge - of these systems is that they can function at scales greater than any human scientist. For example, hypothesis generation systems might easily produce thousands of potential hypotheses (Lu et al., 2024; Jansen et al., 2025), and running experiments to test each of these would be costly and impractical - particularly in that few experiments are likely to yield positive results. In this work we investigate the task of feasibility assessment (e.g. O'Neill et al., 2025), or assessing whether we can filter hypothesis (expressed as claims) to those that are most likely to be feasible, and have their hypotheses confirmed. Performing well at this task would allow us to incorporate feasibility filtering in hypothesis generation systems, and potentially make more discoveries with a given (fixed) experimental budget.</p>
<p>Feasibility assessment is in principle quite challenging as it involves (at times) a high degree of uncertainty in predicting future results, and yet it is a task that scientists perform frequently during experiment planning stages - selecting the hypotheses that we believe are most likely to return positive results based on a combination of literature, pilot experiments or analyses (which may include empirical work, or code/simulations), and past experience. In this work we aim to investigate how well current models can perform this feasibility assessment task, and provide a benchmark to assist in improving model performance over time.</p>
<p>Generating data to test feasibility assessment is challenging, as (by nature) the experimental results of proposed hypotheses are as yet unknown, which makes gold labels for determining whether a hypothesis is feasible or infeasible effectively unavailable. To address this challenge, we operationalize feasibility assessment as a temporally-filtered claim verification task. As with conventional claim verification tasks (e.g. Thorne et al., 2018), we generate a corpus of claims extracted from recent scientific literature - however, in addition, each</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Examples of the four main materials science topics included in MATTER-OF-FACT, including superconductors, semiconductors, batteries, and aerospace materials. Each claim includes the claim text, gold label (true/feasible or false/infeasible), a brief explanation and supporting facts based from the original paper the claim was sourced from, additional meta-data (such as whether the claim is quantitative or qualitative), and the knowledge cutoff date for the feasibility assessment task.
claim is paired with a "knowledge cut-off date", which is the date that the paper the claim was generated from was first authored. When performing the feasibility assessment task, models are allowed to use any information available before the source paper was authored to assess feasibility, essentially rewinding into the past to predict future results. In this way, models are provided with knowledge up to (for example) 2023, and must use that knowledge (through a combination of literature search, smallscale code-based experimentation, world modeling, or other methods) to predict whether genuine results (and artificially-generated infeasible results) from 2024 are feasible or infeasible.</p>
<p>The contributions of this work are:</p>
<ol>
<li>We introduce Matter-of-Fact, a benchmark of 8.4 K claims extracted from recent materials science articles in four high-impact subdomains. Each claim includes categorical information (qualitative vs quantitative, and experiment, code, or theory focused), and is paired with a knowledge cut-off date to use for the feasibility assessment task.</li>
<li>We empirically demonstrate that strong baseline models using a variety of solution methods (including retrieval-augmented generation with SemanticsCholar, as well as evidence gathered from code-generation) across
base models (GPT-4O-MINI, O4-MINI, and Claude SonNet 3.7) achieve a maximum of $72 \%$ accuracy, highlighting the challenging nature of this feasibility assessment task.</li>
<li>We assess the quality of the claims both by domain expert evaluation, and by evaluating base models in a conventional claim verification task. Humans and models reach $93 \%+$, suggesting the benchmark is of high quality.</li>
</ol>
<h2>2 Related Work</h2>
<p>Scientific Claim Verification Datasets: The scientific claim verification task requires a model to determine whether a claim (typically extracted from a scientific paper) is true or false, either by leveraging its pretrained scientific knowledge or retrieving evidence from a corpus, with a selection of scientific claim verification benchmarks shown in Table 1. SciFact (Wadden et al., 2020) contains 1.4 K biomedical-domain claims generated by showing citances (sentences that cite a paper and describe its contribution) to human annotators, who were then asked to generate associated claims. Where SciFact pairs claims with a set of 5 K abstracts that can be used for gathering evidence, SciFactOpen (Wadden et al., 2022) expands this evidence retrieval corpus to 500 K abstracts, presenting a</p>
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>Domain</th>
<th>Claims</th>
<th>Source</th>
<th>Generation Method</th>
</tr>
</thead>
<tbody>
<tr>
<td>SciFact (Wadden et al., 2020)</td>
<td>Biomed</td>
<td>1.4 K</td>
<td>Paper Abstracts</td>
<td>Citances provided to annotators</td>
</tr>
<tr>
<td>COVID-Fact (Saakyan et al., 2021)</td>
<td>Biomed</td>
<td>4.0 K</td>
<td>Reddit</td>
<td>Extract positive, generate counterclaim</td>
</tr>
<tr>
<td>SciFact-Open (Wadden et al., 2022)</td>
<td>Biomed</td>
<td>1.4 K</td>
<td>Paper Abstracts</td>
<td>See SciFact</td>
</tr>
<tr>
<td>ClaimCheck (Ou et al., 2025)</td>
<td>ML</td>
<td>154</td>
<td>Paper Reviews</td>
<td>Emphasizes claim weaknesses</td>
</tr>
<tr>
<td>SciTab (Lu et al., 2023)</td>
<td>Comp. Sci</td>
<td>1.2 K</td>
<td>Paper Tables</td>
<td>Compositional reasoning on tables</td>
</tr>
<tr>
<td>MatterOfFact (This work)</td>
<td>Mat. Sci</td>
<td>8.4 K</td>
<td>Paper full-text</td>
<td>Extract positive, generate infeasible</td>
</tr>
</tbody>
</table>
<p>Table 1: A comparison of claim verification datasets with Matter-of-Fact, including their domain, size, source of the information used to generate or extract claims from, and the claim generation method.
more challenging retrieval problem. Also in the biomedical domain, COVID-Fact (Saakyan et al., 2021) consists of over 4 K claims extracted from Reddit. Lu et al. (2023) introduce SciTab, which requires verifying computer science claims centrally using tables extracted from papers. ClaimCheck (Ou et al., 2025) uses reviews of rejected NeurIPS submissions from OpenReview to build a corpus of 154 claims that emphasize identifying the weaknesses in scholarly claims. In contrast, Matter-of-Fact builds a corpus of 8.4 K materials science claims for feasibility assessment that are generated from the nuanced results found in the full text of source articles (rather than abstracts), and where negative claims focus on being scientifically infeasible rather than factually incorrect.</p>
<p>Claim Verification Models: Our framing of feasibility detection is as temporally-filtered claim verification with a knowledge cutoff. More broadly, recent approaches to claim verification typically involve two key steps: evidence retrieval and fact checking. For the retrieval step, augmenting LLMs with retrieved documents (Izacard et al., 2022) or knowledge bases (Baek et al., 2023; Hang et al., 2024) can be effective for improving fact verification performance of models. $\mathrm{Re}^{2} \mathrm{G}$ (Glass et al., 2022) extends the retrieval step with a trained reranker to achieve better retrieval performance for fact checking. Rani et al. (2023) propose a form of query expansion that generates claim-related questions as queries to retrieve supporting documents. For fact checking, some methods make use of structured knowledge representations such as knowledge graphs (Dammu et al., 2024) and first-orderlogic (Wang and Shu, 2023) to organize evidence and verify facts. End-to-end systems combine the entire retrieval and verification pipeline, such as ARSJoint (Zhang et al., 2021) and SciClaims (Ortega and Gómez-Pérez, 2025). In this work we demonstrate similar retrieval-backed systems (with temporal filtering) for feasibility assessment, while also providing formal approaches based on code generation.</p>
<h2>Scientific Discovery and Feasibility Assessment:</h2>
<p>Automated scientific discovery is frequently divided into two subfields: problem-specific methods (like AlphaFold (Jumper et al., 2021) for protein structure prediction), and problem-general methods that work across a variety of problem types. Examples of problem-specific systems in the materials science domain include GNoME (Merchant et al., 2023), a graph neural network (GNN) based method that discovered over 2.2 million new stable crystal structures, and Schmidt et al. (2023)'s method for using crystal-graph neural networks together with high-quality data for accurate stability prediction. The latter work screened 1 billion materials, discovering 150k+ stable compounds, and identified extreme-property materials like superconductors. Similarly, Chen et al. (2024) combine machine learning models with traditional physicsbased models to discover compounds to which can potentially serve as solid electrolytes. These problem-specific methods can be applied to feasibility assessment by predicting highly specific properties of unknown materials. Matter-ofFact works to bridge the gap between problemspecific methods and problem-general methods by providing a large set of claims across 4 broad and high-impact areas of materials science, each of which is likely to benefit from a variety of problemspecific methods to arrive at accurate feasibility assessments. As we empirically demonstrate in our modeling results, because Matter-of-Fact nominally requires a large set of capacities to solve, it is challenging benchmark for measuring a general capacity to assess feasibility over broad subdomains.</p>
<h2>3 Dataset</h2>
<p>The Matter-of-Fact benchmark consists of 8.4 K claims extracted from the full-text of materials science articles. The extraction and validation</p>
<p>process is described below, with example claims shown in Figure 1.</p>
<p>Inclusion Criteria: We assembled a corpus of recent publicly-available materials science domain articles by crawling Arxiv for all papers within the MATERIALS SCIENCE and SUPERCONDUCTIVITY topics submitted on or after January 2022, resulting in a total of 24 K articles. Articles were then filtered based on specific inclusion criteria. First, articles that were not licensed using a specific permissive license (Creative CommonsBY ATTRIBUTION-4.0) were removed. Second, to prevent having to use a PDF-TO-TEXT conversion pipeline (which can have limited quality on complex tables, chemical formulas, mathematics, and other artifacts found within materials science articles), we further filtered to include only articles with $\mathrm{IA}_{\mathrm{E}} \mathrm{X}$ source available. Papers with long source ( $&gt;30 \mathrm{k}$ tokens) were also removed (approximately $16 \%$ of articles). After initial filtering, 4.2 K articles remained. Our focus in this work is specifically in four high-impact subdomains: superconductors, semi-conductors, batteries, and aerospace materials. To identify articles within these topics, we performed topic labeling of each abstract using GPT-40-MINI with a prompt that emphasized identifying articles within these 4 focus areas. We then sampled 500 total articles ( 125 from each topic) to use for claim generation.</p>
<p>Initial Claim Generation: Claims were generated by providing the full-text ( $\mathrm{IA}_{\mathrm{E}} \mathrm{X}$ source) of each paper in a prompt, together with task instructions and JSON output format requirements. The model was instructed to generate matched pairs of claims - one true, and one that was clearly false or infeasible - and for each claim, to provide a list of supporting evidence, followed by an overall explanation as to why the evidence supports or refutes the claim. ${ }^{2}$ Claims were instructed to be standalone, and not make reference to the paper in the claim text (i.e. "Table 4 claims the boiling point of Material X is..."), so that they could be (in principle) solved without reference to the original source paper. Negative claims were instructed to be false or clearly infeasible (but not overly so), and not simply claims for which no evidence was available. Similarly, negative claims were instructed to use</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>balanced language so as not to give away their true or false nature by particulars of wording, such as through use of negation markers (i.e. "Material X does not have..."), and to instead use neutral framings. In addition to the above constraints, claims were explicitly asked to be authored on two dimensions. The first asks for claims to specifically test either qualitative knowledge (e.g. "In Situation X, Phenonemon Y helps Material Z maintain its superconductivity"), or quantitative knowledge (e.g. "Material X superconducts at 77 Kelvin"). Second, claims were asked to be authored cross 4 main types: those that focus on experimental results, code/simulation results, theoretical results, or integrative methods across types.</p>
<p>Balanced Temporal Sets: Claims were temporally sorted into those from papers first appearing on Arxiv in 2022 (for training), those in 2023 (for validation), and the most recent claims from papers submitted between 2024 and April 2025 (for testing). For each set, we filtered claims such that equal numbers of true and false claims were present, to achieve a baseline (random chance) performance of $50 \%$. Claims were also balanced such that equal numbers within the experimental, code, theory, and integrative categories appear within a given set. The final dataset includes a total of 8.4 K claims, distributed as 1.4 K claims for training, 2.5 K for validation, and 4.4 K for testing.</p>
<p>Domain Expert Validation: To measure the quality of the claim generation process, a domain expert with a graduate degree in materials science was given each claim and its source paper, and independently asked to determine the validity of the claim. This was a challenging task, because the claims span broad areas of materials science that would be unusual for any single individual to have expertise within. The domain expert performed this task for 100 claims from the test set, and initially agreed in $93 \%$ of cases (while noting a further $3 \%$ of claims appeared to not meet criteria, such as explicitly referencing the original source paper). They were then provided with the LLM-generated labels and explanations, and asked to resolve disagreements (either LLM errors, or human error), noting that nearly all errors were a result of the domain expert missing difficult-to-find evidence on their first attempt, and ultimately reaching $99 \%$ agreement after this resolution process. This empirically suggests that the overall data quality is high ( $96 \%$ after discounting data not meeting generation</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Flow diagrams for two models: the retrieval-augmented generation (RAG) model that retrieves snippets from the full-text of papers using SEMANTICSCHOLAR (left), and a code generation model that executes PYTHON code and examines the output using CODESCIENTIST.</p>
<p>Criteria). Before resolution, interrater agreement using Cohen's Kappa (Cohen, 1960) was <em>κ</em> = 0.86, or strong agreement (McHugh, 2012), suggesting the claims are highly objective.</p>
<h2>4 Baseline Models</h2>
<p>We evaluate performance on the MATTER-OF-FACT dataset using a selection of baseline models described below. Models are provided with the text of the claim, and must predict a binary label (true/feasible, or false/infeasible), as well as provide a brief explanation for their reasoning. All models investigated in this work use in-context learning (ICL), and are characterized across three common base models at different price/performance points, including GPT-4O-MINI, O4-MINI, and CLAUDE SONNET 3.7. Our retrieval-augmented generation and code-generation models are shown in Figure 2.</p>
<h3>4.1 Feasibility Assessment Models</h3>
<p><strong>Chain-of-Thought (CoT), ICL, Reflection:</strong> The language model is provided with a prompt that includes the claim, and a request to think and/or plan before responding in the style of Chain-of-Thought (Wei et al., 2022). We also include two variations of this model. The first includes a <em>20-shot</em> in-context learning example (Brown et al., 2020), using 20 claim problems (together with their supporting facts and explanations) drawn from the training set, including balanced numbers of true and false claims. The second includes adding a reflection step (Madaan et al., 2023) where, after the initial generation, the model then reflects on its response, then provides a final answer and explanation for the reasoning behind that answer.</p>
<p><strong>Retrieval Augmented Generation (RAG):</strong> Using the claim text as a query, the model first retrieves the <em>top K</em> matching full-text snippets from scholarly scientific articles using the SEMANTICSCHOLAR API (Kinney et al., 2023), where each snippet generally takes the form of a span of text (approximately 500 words in length) from an article indexed by SEMANTICSCHOLAR that most closely matches the query. To prevent temporal contamination with oracle knowledge, full-text snippets are filtered such that papers authored after the source paper for a given claim are not included in the search. For example, if a claim was derived from a paper first published on Arxiv in March 2024, then</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Overall Accuracy</th>
<th style="text-align: center;">True</th>
<th style="text-align: center;">False</th>
<th style="text-align: center;">Accuracy by Category</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Cost (per 1k)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Qual.</td>
<td style="text-align: center;">Qnt.</td>
<td style="text-align: center;">Exp.</td>
<td style="text-align: center;">Code</td>
<td style="text-align: center;">Ther.</td>
<td style="text-align: center;">Int.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">RANDOM BASELINE</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">GPT-40-MINI</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Chain-of-Thought (CoT)</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$1</td>
</tr>
<tr>
<td style="text-align: center;">CoT + 20-sHot ICL</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$2</td>
</tr>
<tr>
<td style="text-align: center;">CoT + 20-sHot ICL + Reflection</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$3</td>
</tr>
<tr>
<td style="text-align: center;">CoT + ICL + RAG (SEMANTICSChOLAR)</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$3</td>
</tr>
<tr>
<td style="text-align: center;">CoT + ICL + CODE (CODESCIENTIST)</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$4</td>
</tr>
<tr>
<td style="text-align: center;">04-MINI</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Chain-of-Thought (CoT)</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$4</td>
</tr>
<tr>
<td style="text-align: center;">CoT + 20-sHot ICL</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$15</td>
</tr>
<tr>
<td style="text-align: center;">CoT + 20-sHot ICL + Reflection</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$30</td>
</tr>
<tr>
<td style="text-align: center;">CoT + ICL + RAG (SEMANTICSChOLAR)</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$27</td>
</tr>
<tr>
<td style="text-align: center;">CoT + ICL + CODE (CODESCIENTIST)</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$34</td>
</tr>
<tr>
<td style="text-align: center;">CLAUDE-SONNET 3.7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Chain-of-Thought (CoT)</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$9</td>
</tr>
<tr>
<td style="text-align: center;">CoT + 20-sHot ICL</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$44</td>
</tr>
<tr>
<td style="text-align: center;">CoT + 20-sHot ICL + Reflection</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$87</td>
</tr>
<tr>
<td style="text-align: center;">CoT + ICL + RAG (SEMANTICSChOLAR)</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$76</td>
</tr>
<tr>
<td style="text-align: center;">CoT + ICL + CODE (CODESCIENTIST)</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\$173</td>
</tr>
</tbody>
</table>
<p>Table 2: Model performance on the feasibility assessment task, including overall performance, as well as performance broken down by specific categories of feasibility assessment claim problems. True and False represent performance on problems with those gold labels. Qual. and Quant. represent performance on qualitative and quantitative problems. Exp., Code, Ther., and Int. represent performance on claims focusing on experimental, code/simulation, theoretical, or integrative results, respectively. Cost represents the estimated model cost per 1000 claims, in US dollars.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Overall <br> Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-40-MINI</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">RAG (SEMANTICSChOLAR (No DATE))</td>
<td style="text-align: center;">0.76</td>
</tr>
<tr>
<td style="text-align: left;">ORACLE SOURCE PAPER</td>
<td style="text-align: center;">0.71</td>
</tr>
<tr>
<td style="text-align: left;">04-MINI</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">RAG (SEMANTICSChOLAR (No DATE))</td>
<td style="text-align: center;">0.90</td>
</tr>
<tr>
<td style="text-align: left;">ORACLE SOURCE PAPER</td>
<td style="text-align: center;">0.96</td>
</tr>
<tr>
<td style="text-align: left;">CLAUDE-SONNET 3.7</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">RAG (SEMANTICSChOLAR (No DATE))</td>
<td style="text-align: center;">0.87</td>
</tr>
<tr>
<td style="text-align: left;">ORACLE SOURCE PAPER</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: left;">HUMAN DOMAIN EXPERT</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">INITIAL ASSESSMENT</td>
<td style="text-align: center;">0.93</td>
</tr>
<tr>
<td style="text-align: left;">AFTER RESOLVING DISAGREEMENTS</td>
<td style="text-align: center;">0.99</td>
</tr>
</tbody>
</table>
<p>Table 3: Model performance on the claim verification task, using oracle models. [Note that due to the high model com, the ORACLE SOURCE PAPER (SONNET) model is assessed on a subset of the test set.
only papers authored in February 2024 or before will be included in the snippet search. The top 20 matching full-text snippets (sorted by the provided relevance score) are included in the language model prompt, in a retrieval-augmented-generation
paradigm (Lewis et al., 2020). The prompt for this model also includes a 20 -shot ICL example, and request for chain-of-thought reasoning.</p>
<p>Code Generation (CODESCIENTIST): This model is performed in two stages. During the first stage, the model is provided with the claim text, and prompted to generate a code-based experiment or simulation in PYTHON that would produce useful evidence in supporting or refuting the claim. The code is then executed, and the code and execution results are provided to a second prompt with a request to generate an answer for the feasibility task as well as a supporting explanation. For code execution, we use the experiment execution portion of CODESCIENTIST (Jansen et al., 2025), which allows executing arbitrary PYTHON code in a virtual sandbox on MODAL.COM, and supports installing external supporting libraries through PIP. While this execution pipeline stores and saves output streams (e.g. STDOUT/STDERR), the model explicitly prompted to save a log of its work, as well as a final list of results, which are then provided back to the model to help make its final decision. For tractability, we run CODESCIENTIST in a limited form due to its high overall cost (initially</p>
<p>reported as $\$ 4$ per experiment), which would be intractible for the size of our dataset ( $\approx \$ 16 k$ for 4 K test claims). Instead of 25 debug iterations, we run CODESCIENTIST for a single iteration (without reflection), and reduce the experiment time limit from 6 hours to 10 minutes (or 31 total CPU-days across all test claims). The model is made aware of these limitations in the code generation prompt, and encouraged to design appropriately-scoped experiments and output to support the decision process.</p>
<h3>4.2 Claim Verification Models</h3>
<p>As a method of characterizing model performance when oracle information is available, Table 3 also provides two models that perform a claim verification task rather than the feasibility assessment task - that is, they do not have the same temporal restrictions, and are able to use data available after the source claim was authored.
RAG (Temporally Unrestricted): The retrievalaugmented generation model described above, but without temporal restrictions. For a given claim, snippets from any scientific article may be retrieved, including (potentially) the source article of the claim, or those that cite the source article.
Oracle Baseline: The language model is provided both with the claim, as well as the original source paper the claim was derived from (in the form of the paper's original $\mathrm{LA}_{\mathrm{E}} \mathrm{X}$ source retrieved from Arxiv) in a retrieval-augmented-generation paradigm. This baseline measures how well a model can verify the claim when provided with a source scientific article that directly speaks to that claim's validity/feasibility.
Oracle (Human Domain Expert): The domain expert evaluation, as described in Section 3.</p>
<h3>4.3 Results</h3>
<p>Feasibility Assessment Results: The performance of all models when evaluated in the feasibility assessment mode is shown in Table 2. Performance across all models ranges from 0.58 to 0.72 , with the models that use the smallest (and least expensive) base model (GPT-40-MINI) generally performing about 5 percent lower than the two more performant (and more costly) base models, O4mini and Claude Sonnet 3.7. Across models, adding features (such as in-context learning, reflection, RAG over SemanticScholar, or Code Generation) generally provides modest performance improvements, or does not improve per-
formance over the Chain-of-Thought baseline, highlighting the difficulty of this task when using conventional solution methods, and its suitability as a challenge task. When examining performance broken down by category, we observe that while the overall performance of a given base model is similar with different features, some models are more performant at identifying true/feasible claims than they are at identifying false/infeasible claims, and vice versa. All models perform better at assessing the feasibility of qualitative claims than quantitative claims, with this difference between $11 \%$ and $19 \%$ across all models, potentially a result of quantitative claims requiring the ability (through code or other means) of verifying the feasibility of specific numerical values present in the claims. In line with this reasoning, claims that are based on experiments or code/simulations consistently achieve the lowest performance, while those based on theoretical results are next-highest, with integrative claims achieving the highest performance.</p>
<p>Claim Verification Results: The performance of models when evaluated in the claim verification mode is shown in Table 3. In this mode the models have no temporal restrictions, and may use knowledge from the source paper, or papers authored after the source paper (including those that may cite the source paper) as evidence to perform the claim verification task. These experiments serve two purposes. First, they identify an effective ceiling of how well a given base model can perform even when provided with the original source article used to create a claim, with O4-MINI and CLAUDE SonNET 3.7 capable of achieving nearly a $100 \%$ ceiling performance, while GPT-40-MINI has more modest performance ceiling between 0.71 and 0.76 . Second, these models serve as a consistency evaluation for the claim generation protocol, emphasizing that when strong models are asked to verify the labels of these automatically generated claims, they nearly always agree with the gold label. Further emphasizing this is the domain expert performance, who (when provided with the original source article), agreed with the LLM-generated label for $99 \%$ of claims after resolving disagreements.</p>
<p>Taken together, these results empirically demonstrate the generation quality of the feasibility claims, while also emphasizing that common models and architectures still achieve overall modest performance on the feasibility assessment task.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Base Model</th>
<th style="text-align: center;">Knowledge <br> Cutoff Date</th>
<th style="text-align: center;">Accuracy <br> (before cutoff)</th>
<th style="text-align: center;">Accuracy <br> (after cutoff)</th>
<th style="text-align: center;">Accuracy <br> $\Delta$</th>
<th style="text-align: center;"># Samples <br> (before/after)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-40-MINI</td>
<td style="text-align: center;">Sept 2023</td>
<td style="text-align: center;">0.661</td>
<td style="text-align: center;">0.664</td>
<td style="text-align: center;">0.003</td>
<td style="text-align: center;">$3236 / 5124$</td>
</tr>
<tr>
<td style="text-align: left;">O4-MINI</td>
<td style="text-align: center;">May 2024</td>
<td style="text-align: center;">0.694</td>
<td style="text-align: center;">0.705</td>
<td style="text-align: center;">-0.011</td>
<td style="text-align: center;">$5168 / 3192$</td>
</tr>
<tr>
<td style="text-align: left;">CLAUDE-SONNET-3-7</td>
<td style="text-align: center;">Oct 2024</td>
<td style="text-align: center;">0.672</td>
<td style="text-align: center;">0.661</td>
<td style="text-align: center;">0.011</td>
<td style="text-align: center;">$6130 / 2230$</td>
</tr>
</tbody>
</table>
<p>Table 4: Knowledge contamination analysis of base models. In this analysis, performance of the Chain-of-Thought + 20-Short ICL + Reflection model is shown for claims from papers that were authored before or after a given model's advertised knowledge cutoff date. Given the temporal nature of the dataset, all 8.4 k claims across train, development, and test sets were included. All models show almost identical performance $( \pm 1 \%)$ when tested on claims from papers before or after their knowledge cutoff date, suggesting that knowledge contamination does not play a significant role in performance.</p>
<h2>5 Discussion</h2>
<p>Base-Model Contamination: A central part of the framing of our feasibility assessment task as a temporally-filtered claim verification task is that it requires models to have a minimum of contamination with knowledge beyond a given claim's knowledge cutoff date. While it is possible that techniques such as model editing and machine unlearning (Bourtoule et al., 2021; Tarun et al., 2023; Liu et al., 2025) may eventually allow the knowledge in a base model to be temporally filtered to minimize this contamination, this may have limited success in current forms (Lynch et al., 2024; Deeb and Roger, 2024; Du et al., 2024). Instead, here we aim to measure how much of the current model performance is likely due to model contamination (from, for example, the base model being trained on the source articles used to generate the claims). To measure this, we examine each base model's performance for claims extracted from papers before and after the base model's advertised training data knowledge cut-off dates. The results, shown in Table 4, show that the performance of the base models on claims from papers authored after their knowledge cutoff is nearly identical to the performance on claims authored by papers that are before the knowledge cutoff date. This empirically suggests that the performance of current base models on the feasibility assessment task is not due to model contamination, but due to other properties, such as their capacity for reasoning.
Pragmatic Ceiling Performance: While we empirically show that the feasibility of many claims can be assessed using inexpensive means, the models we demonstrate are far from achieving perfect performance on this task. Pragmatically, a model that achieves near $100 \%$ performance would be able to (with near perfect accuracy) determine whether claims are likely to be feasible or infeasible through some combination of literature search, inexpensive code-based experimentation, world
modeling, and other means. Acheiving 100\% performance is likely impractical, as many scientific claims can only be verified with empirical work, and not with literature search or simulation, particularly for those (most impactful) scientific results that are surprising because they run counter to expectations. That being said, even though effective ceiling performance on feasibility assessment tasks is likely to be less than $100 \%$, increasing model performance on this task even a modest amount can have practical utility for improving the efficiency of discovery systems. As we show in APPENDIX A, for a hypothetical hypothesis generation system where $1 \%$ of the hypotheses are true, the performance of our current-best model could potentially allow discovering $60 \%$ of the true hypotheses while reducing experiment costs by $80 \%$ - a large overall budget reduction, at the cost of reducing the recall of finding true hypotheses by approximately $40 \%$.</p>
<h2>6 Conclusion</h2>
<p>We present MATTER-OF-FACT, a benchmark for assessing the feasibility of 8.4 K scientific claims in four high-impact subdomains of materials science: superconductors, semi-conductors, batteries, and aerospace materials. We frame the feasibility assessment task as a temporally-filtered claim verification task, and empirically demonstrate that strong baseline models using a variety of solving methods (including literature search and code generation) reach only modest performance on this task (72\%). Performance on feasibility assessment can directly translate to improving automated scientific discovery systems, particularly in hypothesis generation, where filtering infeasible hypotheses can make scientific discovery more efficient, and lower overall experiment costs. Ultimate solution methods for the feasibility assessment task are likely to require a combination of reasoning over deep literature search, code-based simulation, and world modeling at the scale of subdomains.</p>
<h2>Limitations</h2>
<p>Temporal Filtering for Prediction: Temporal datasets offer the opportunity to construct prediction datasets for high-impact domains (e.g. Luo et al., 2018, link prediction for cancer biology) where the knowledge a system is predicting is potentially beyond current human knowledge, and for which gold labels are infeasible to construct. Temporal filtering assumes well-controlled models that have not been contaminated with data past their temporal filtering date. In this work we characterize the contamination rate of our base language models, and this analysis suggests that data contamination either does not exist, or is not a significant factor driving current performance. That being said, users of this benchmark should characterize the performance of novel base models to characterize how much data contamination may play a role.
Limits of Code-based Experimentation: Pragmatically, to be useful for filtering scientific hypotheses, feasibility assessment methods must be able to perform well at scale. This necessitates that any pilot experiments (including code-based simulations) must be fast and inexpensive to run, otherwise the feasibility assessment step may be impractically expensive to provide overall cost savings. That being said, different applications and endusers may have varying preferred cost/performance points, and we encourage reporting performance as a function of overall cost (as we have done in this work) to help accurately assess the cost vs benefit of proposed models. It is our hope that providing a large-scale benchmark that necessitates developing inexpensive feasibility assessment methods will help facilitate innovation in this direction.</p>
<h2>Acknowledgments</h2>
<p>This research was developed with funding from the Defense Advanced Research Projects Agency's (DARPA) SciFy program (Agreement No. HR00112520300) to PJ at the University of Arizona. The views expressed are those of the author and do not reflect the official policy or position of the Department of Defense or the U.S. Government. PJ has an outside interest in the Allen Institute for Artificial Intelligence. This interest has been disclosed to the University of Arizona and reviewed in accordance with its conflict of interest policies. We thank the members of the DARPA Scientific Feasibilty (SciFy) program for thoughtful discussions.</p>
<h2>References</h2>
<p>Jinheon Baek, Soyeong Jeong, Minki Kang, Jong Park, and Sung Hwang. 2023. Knowledge-augmented language model verification. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1720-1736, Singapore. Association for Computational Linguistics.</p>
<p>Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. 2021. Machine unlearning. In 2021 IEEE symposium on security and privacy (SP), pages 141-159. IEEE.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and 1 others. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Chi Chen, Dan Thien Nguyen, Shannon J Lee, Nathan Baker, Ajay S Karakoti, Linda Lauw, Craig Owen, Karl T. Mueller, Brian A. Bilodeau, Vijayakumar Murugesan, and Matthias Troyer. 2024. Accelerating computational materials discovery with machine learning and cloud high-performance computing: from large-scale screening to experimental validation. Journal of the American Chemical Society.</p>
<p>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and psychological measurement, 20(1):37-46.</p>
<p>Preetam Prabhu Srikar Dammu, Himanshu Naidu, Mouly Dewan, YoungMin Kim, Tanya Roosta, Aman Chadha, and Chirag Shah. 2024. ClaimVer: Explainable claim-level verification and evidence attribution of text through knowledge graphs. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 13613-13627, Miami, Florida, USA. Association for Computational Linguistics.</p>
<p>Aghyad Deeb and Fabien Roger. 2024. Do unlearning methods remove information from language model weights? arXiv preprint arXiv:2410.08827.</p>
<p>Jiacheng Du, Zhibo Wang, Jie Zhang, Xiaoyi Pang, Jiahui Hu, and Kui Ren. 2024. Textual unlearning gives a false sense of unlearning. arXiv preprint arXiv:2406.13348.</p>
<p>Michael Glass, Gaetano Rossiello, Md Faisal Mahbub Chowdhury, Ankita Naik, Pengshan Cai, and Alfio Gliozzo. 2022. Re2G: Retrieve, rerank, generate. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2701-2715, Seattle, United States. Association for Computational Linguistics.</p>
<p>Ching Nam Hang, Pei-Duo Yu, and Chee Wei Tan. 2024. Trumorgpt: Query optimization and semantic reasoning over networks for automated fact-checking. In 2024 58th Annual Conference on Information Sciences and Systems (CISS), pages 1-6.</p>
<p>Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane A. Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022. Few-shot learning with retrieval augmented language models. J. Mach. Learn. Res., 24:251:1251:43.</p>
<p>Peter Jansen, Oyvind Tafjord, Marissa Radensky, Pao Siangliulue, Tom Hope, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Daniel S Weld, and Peter Clark. 2025. Codescientist: End-to-end semiautomated scientific discovery with code-based experimentation. arXiv preprint arXiv:2503.22708.</p>
<p>John M. Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, Alex Bridgland, Clemens Meyer, Simon A A Kohl, Andy Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, and 15 others. 2021. Highly accurate protein structure prediction with alphafold. Nature, 596:583 - 589.</p>
<p>Rodney Kinney, Chloe Anastasiades, Russell Authur, Iz Beltagy, Jonathan Bragg, Alexandra Buraczynski, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Arman Cohan, and 1 others. 2023. The semantic scholar open data platform. arXiv preprint arXiv:2301.10140.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, and 1 others. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:94599474 .</p>
<p>Ruochen Li, Teerth Patel, Qingyun Wang, and Xinya Du. 2024. Mlr-copilot: Autonomous machine learning research based on large language models agents. arXiv preprint arXiv:2408.14033.</p>
<p>Ryan Liu and Nihar B Shah. 2023. Reviewergpt? an exploratory study on using large language models for paper reviewing. arXiv preprint arXiv:2306.00622.</p>
<p>Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Yuguang Yao, Chris Yuhao Liu, Xiaojun Xu, Hang Li, and 1 others. 2025. Rethinking machine unlearning for large language models. Nature Machine Intelligence, pages $1-14$.</p>
<p>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. The ai scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292.</p>
<p>Xinyuan Lu, Liangming Pan, Qian Liu, Preslav Nakov, and Min-Yen Kan. 2023. SCITAB: A challenging benchmark for compositional reasoning and claim verification on scientific tables. In Proceedings of the 2023 Conference on Empirical Methods in Natural</p>
<p>Language Processing, pages 7787-7813, Singapore. Association for Computational Linguistics.</p>
<p>Fan Luo, Marco A. Valenzuela-Escárcega, Gus HahnPowell, and Mihai Surdeanu. 2018. Scientific discovery as link prediction in influence and citation graphs. In Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-12), pages 1-6, New Orleans, Louisiana, USA. Association for Computational Linguistics.</p>
<p>Aengus Lynch, Phillip Guo, Aidan Ewart, Stephen Casper, and Dylan Hadfield-Menell. 2024. Eight methods to evaluate robust unlearning in llms. arXiv preprint arXiv:2402.16835.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, and 1 others. 2023. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:46534-46594.</p>
<p>Bodhisattwa Prasad Majumder, Harshit Surana, Dhruv Agarwal, Bhavana Dalvi Mishra, Abhijeetsingh Meena, Aryan Prakhar, Tirth Vora, Tushar Khot, Ashish Sabharwal, and Peter Clark. 2025. Discoverybench: Towards data-driven discovery with large language models. In The Thirteenth International Conference on Learning Representations.</p>
<p>Mary L McHugh. 2012. Interrater reliability: the kappa statistic. Biochemia medica, 22(3):276-282.</p>
<p>Amil Merchant, Simon Batzner, Samuel S. Schoenholz, Muratahan Aykol, Gowoon Cheon, and Ekin Dogus Cubuk. 2023. Scaling deep learning for materials discovery. Nature, 624(7990):80-85.</p>
<p>Charles O'Neill, Tirthankar Ghosal, Roberta Rāileanu, Mike Walmsley, Thang Bui, Kevin Schawinski, and Ioana Ciucă. 2025. Sparks of science: Hypothesis generation using structured paper data. arXiv preprint arXiv:2504.12976.</p>
<p>Raúl Ortega and José Manuel Gómez-Pérez. 2025. Sciclaims: An end-to-end generative system for biomedical claim analysis. Preprint, arXiv:2503.18526.</p>
<p>Jiefu Ou, William Gantt Walden, Kate Sanders, Zhengping Jiang, Kaiser Sun, Jeffrey Cheng, William Jurayj, Miriam Wanner, Shaobo Liang, Candice Morgan, Seunghoon Han, Weiqi Wang, Chandler May, Hannah Recknor, Daniel Khashabi, and Benjamin Van Durme. 2025. Claimcheck: How grounded are llm critiques of scientific papers? ArXiv, abs/2503.21717.</p>
<p>Marissa Radensky, Simra Shahid, Raymond Fok, Pao Siangliulue, Tom Hope, and Daniel S Weld. 2024. Scideator: Human-llm scientific idea generation grounded in research-paper facet recombination. arXiv preprint arXiv:2409.14634.</p>
<p>Anku Rani, S.M Towhidul Islam Tonmoy, Dwip Dalal, Shreya Gautam, Megha Chakraborty, Aman Chadha, Amit Sheth, and Amitava Das. 2023. FACTIFY-5WQA: 5W aspect-based fact verification through question answering. In <em>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 10421–10440, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Arkadiy Saakyan, Tuhin Chakrabarty, and Smaranda Muresan. 2021. COVID-fact: Fact extraction and verification of real-world claims on COVID-19 pandemic. In <em>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, pages 2116–2129, Online. Association for Computational Linguistics.</p>
<p>Jonathan Schmidt, Noah Hoffmann, Hai-Chen Wang, Pedro Borlido, Pedro J. M. A. Carriço, Tiago F. T. Cerqueira, Silvana Botti, and Miguel A. L. Marques. 2023. Machine-learning-assisted determination of the global zero-temperature phase diagram of materials. <em>Advanced Materials</em>, 35.</p>
<p>Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. 2024. Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. <em>arXiv preprint arXiv:2409.04109</em>.</p>
<p>Ayush K Tarun, Vikram S Chundawat, Murari Mandal, and Mohan Kankanhalli. 2023. Fast yet effective machine unlearning. <em>IEEE Transactions on Neural Networks and Learning Systems</em>.</p>
<p>James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extraction and VERification. In <em>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</em>, pages 809–819, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. 2020. Fact or fiction: Verifying scientific claims. In <em>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 7534–7550, Online. Association for Computational Linguistics.</p>
<p>David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Iz Beltagy, Lucy Lu Wang, and Hannaneh Hajishirzi. 2022. SciFact-open: Towards open-domain scientific claim verification. In <em>Findings of the Association for Computational Linguistics: EMNLP 2022</em>, pages 4719–4734, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Haoran Wang and Kai Shu. 2023. Explainable claim verification via knowledge-grounded reasoning with large language models. In <em>Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pages 6288–6304, Singapore. Association for Computational Linguistics.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In <em>Proceedings of the 36th International Conference on Neural Information Processing Systems</em>, NIPS '22, Red Hook, NY, USA. Curran Associates Inc.</p>
<p>Zhiwei Zhang, Jiyi Li, Fumiyo Fukumoto, and Yanming Ye. 2021. Abstract, rationale, stance: A joint model for scientific claim verification. In <em>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pages 3580–3586, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Ruiyang Zhou, Lu Chen, and Kai Yu. 2024. Is llm a reliable reviewer? a comprehensive evaluation of llm on automatic paper reviewing tasks. In <em>Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</em>, pages 9340–9351.</p>
<h2>A Utility for Hypothesis Filtering</h2>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Relative efficiency (in terms of reduction in the number of experiments needed to be run) for a hypothetical automated scientific discovery (ASD) system that generates hypotheses with a certain true positive rate (<em>y-axis</em>), after those hypotheses have been pre-filtered by a feasibility assessment system such as the models described in this work. This plot assumes a true positive (i.e. <em>feasible</em>) detection rate of 0.60, corresponding to the RAG (SEMANTICSCHULAR, 04-MINI) model in Table 2, while the highlighted region corresponds to that model's infeasible claim detection rate (82%). For a hypothetical ASD system where 1% of the hypotheses it generated were <em>true/feasible</em>, the RAG model would reduce the number of experiments (i.e. cost) by 80%, while still discovering 60% of the true hypotheses.</p>
<p>Feasibility assessment has utility in impactful tasks such as (semi-automated) scientific discovery, particularly in the context of hypothesis generation. Hypothesis generation systems (e.g. Lu et al., 2024; Jansen et al., 2025; O'Neill et al., 2025) have the capacity to generate an impractically large number of possible hypotheses (framed as claims) that</p>
<p>one could test, and as a result running all their proposed experiments is costly (at best) and intractible (at worst). Coupling hypothesis generation with feasibility assessment would allow filtering out hypotheses that are unlikely to be feasible - i.e. yield experimental results that support the hypothesis - and ultimately increase the efficiency of scientific discovery systems in terms of the number of positive discoveries that can be made on a given budget. In automated hypothesis generation where overall likelihood of a hypothesis yielding positive results is low, increasing efficiency is dominated by correctly identifying (and filtering) infeasible hypotheses/claims. Figure 3 shows a plot of experiment efficiency (in terms of the reduction in the number of experiments that would need to be run) for hypothetical hypothesis generation systems that have different rates of generating true hypotheses, with the performance of the best-performing model (RAG (SEMANTIC SCHOLAR) using O4MINI) highlighted. For a hypothetical hypothesis generation system where $1 \%$ of its hypotheses are true, this model would reduce the number of experiments needed to be run by approximately $80 \%$, while still discovering $60 \%$ of the true hypotheses. This highlights that even systems with middle performance can have practical utility (in terms of cost savings) when coupled with scientific discovery systems.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Scientific articles tend to express positive claims rather than negative claims. We follow the approach of Saakyan et al. (2021) to first extract positive claims, then automatically generate negative claims from these positive references.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
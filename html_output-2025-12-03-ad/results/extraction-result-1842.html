<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1842 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1842</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1842</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-34.html">extraction-schema-34</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <p><strong>Paper ID:</strong> paper-378a545c3a1cf6c4aada8f9ee8820c0d8008220a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/378a545c3a1cf6c4aada8f9ee8820c0d8008220a" target="_blank">Benchmarking Foundation Models with Language-Model-as-an-Examiner</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> A novel benchmarking framework where the LM serves as a knowledgeable examiner that formulates questions based on its knowledge and evaluates responses in a reference-free manner is proposed, allowing for effortless extensibility as various LMs can be adopted as the examiner and the questions can be constantly updated given more diverse trigger topics.</p>
                <p><strong>Paper Abstract:</strong> Numerous benchmarks have been established to assess the performance of foundation models on open-ended question answering, which serves as a comprehensive test of a model's ability to understand and generate language in a manner similar to humans. Most of these works focus on proposing new datasets, however, we see two main issues within previous benchmarking pipelines, namely testing leakage and evaluation automation. In this paper, we propose a novel benchmarking framework, Language-Model-as-an-Examiner, where the LM serves as a knowledgeable examiner that formulates questions based on its knowledge and evaluates responses in a reference-free manner. Our framework allows for effortless extensibility as various LMs can be adopted as the examiner, and the questions can be constantly updated given more diverse trigger topics. For a more comprehensive and equitable evaluation, we devise three strategies: (1) We instruct the LM examiner to generate questions across a multitude of domains to probe for a broad acquisition, and raise follow-up questions to engage in a more in-depth assessment. (2) Upon evaluation, the examiner combines both scoring and ranking measurements, providing a reliable result as it aligns closely with human annotations. (3) We additionally propose a decentralized Peer-examination method to address the biases in a single examiner. Our data and benchmarking results are available at: https://lmexam.com.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1842.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1842.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-an-Examiner (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Model as an Examiner using GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A centralized evaluation setup where GPT-4 both generates (examiner) questions and evaluates other models' open-ended QA responses in a reference-free manner; used to compare automated judgments against human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking Foundation Models with Language-Model-as-an-Examiner</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>LLM-as-a-judge (centralized examiner)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>single rating on multiple dimensions (Likert-style on 4 dimensions + overall 1-5) and pairwise comparison / ranking (merge-sort over responses)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>open-ended QA responses (machine-generated answers)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>general knowledge / multi-domain (LMExamQA)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy, Coherence, Factuality, Comprehensiveness (1-3 each) + Overall score (1-5); also pairwise comparisons considering same four aspects</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human annotators scored responses on overall quality (1-5) and provided rankings; for metric evaluation 3 human scores and 3 human pairwise comparisons were collected per question</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>human annotators (instructed to perform web search for reference; expertise level not further specified)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman's rho, Kendall's tau for Likert-scale correlations; pairwise comparison accuracy for ranking</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>Spearman rho(GPT-4 vs humans)=0.633; Kendall tau(GPT-4 vs humans)=0.554; pairwise comparison accuracy=0.853</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>When the LM examiner is knowledgeable about the topic (examiner generated Q/A and provided ground-truth answers), and when using pairwise ranking rather than simple overlap metrics; absolute scoring and ranking both correlated closely with human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>When the LM evaluator lacks adequate domain knowledge about the question (leading to misjudgments), for stylistic biases (e.g., preference for ChatGPT-style paraphrases), and for complex/deeper multi-round questions where model understanding is required.</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Agreement and reliable judgments decrease for higher-cognitive-level / multi-round (deeper) questions; evaluator misjudgments were observed in challenging cases (pilot: 18/100 GPT-4 judgments incorrect on Natural Questions evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Defining clear evaluation dimensions (accuracy, coherence, factuality, comprehensiveness) and using structured rating + ranking improved alignment with humans; ranking (pairwise) was more discerning and achieved higher agreement than string-similarity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>300 model responses across 100 questions (300 annotated samples used for metric evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Automated LLM judge (GPT-4) showed substantially higher correlation with human judgments (Spearman rho=0.633, Kendall tau=0.554, pairwise accuracy=85.3%) compared to standard automatic metrics (ROUGE, BLEU, BERTScore had much lower correlations).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4 used as an examiner/judge correlates substantially better with human judgments than traditional automatic metrics (ROUGE/BLEU/BERTScore). Pairwise ranking by GPT-4 achieved >85% agreement with human pairwise labels. Major failure modes include cases where the LM judge lacks the specific knowledge required, and stylistic preference biases; generating questions and answers with the same LM (examiner) helps the examiner be confident and improves reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Centralized single-model evaluation can be biased (style or preference); evaluator misjudgments occur when the judge lacks domain knowledge; no reported inter-human agreement statistics; limited to models capable of consistent pairwise comparison (some models could not be used reliably as examiners).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking Foundation Models with Language-Model-as-an-Examiner', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1842.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1842.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Likert-style scoring (LM-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Likert-scale scoring on multiple dimensions performed by LMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Structured absolute scoring where the automated evaluator (an LM) rates responses on predefined dimensions (accuracy, coherence, factuality, comprehensiveness) with 1-3 scales plus an overall 1-5 score; compared against human Likert scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking Foundation Models with Language-Model-as-an-Examiner</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>Likert-style rating (LM produces numeric scores per dimension and overall)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4 (used to assign Likert scores in experiments); other peer examiners can use same rubric</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>single rating with rubric: four 1-3 dimension scores and one overall 1-5 score, evaluator instructed to assume its own answer as ground truth</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>open-ended QA responses (machine-generated answers)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>general knowledge / multi-domain (LMExamQA)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy, Coherence, Factuality, Comprehensiveness (each 1-3); Overall quality (1-5)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human annotators provided overall 1-5 scores and multidimensional 1-3 ratings for sampled responses; annotators instructed to consult web references if needed.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td>3 (per response/question in the metric-evaluation sample)</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>human annotators (not explicitly described as specialized domain experts)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman's rho and Kendall's tau between automated Likert overall score and human overall score</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>Reported for GPT-4 Likert overall vs human: Spearman rho=0.633; Kendall tau=0.554 (these numbers reported for the LM examiner overall metric as presented in Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Clear, structured rubric and use of knowledgeable LM examiner that produced the question-answer pairs; tasks where factual correctness and coherence are objective tended to align better.</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Subjective or holistic dimensions like comprehensiveness can vary across annotators and LMs; evaluator knowledge gaps and stylistic preferences reduce alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Higher-complexity (analysis-level) questions reduce the reliability of single-round Likert scores; few-shot prompting helped models perform better on higher-level questions but also complicates evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Explicitly specifying dimension definitions and scoring ranges improved correlation with human judgments; combining per-dimension scores into an overall score helps consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>300 annotated responses across 100 questions (3 human scores per response used to compute correlations)</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>LLM-assigned Likert scores (GPT-4) showed moderate-to-strong correlation with human overall scores (Spearman rho=0.633), substantially outperforming lexical/similarity automatic metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Likert-style scoring by a strong LM examiner (GPT-4) correlates well with human overall judgments; structured dimensions help produce reliable absolute scores, but ranking remains more discerning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Ceiling/floor and subjectivity risks for Likert scales; human expertise level and rubric interpretation may vary; no human inter-rater agreement numbers reported to contextualize LM-human agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking Foundation Models with Language-Model-as-an-Examiner', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1842.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1842.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pairwise Ranking (LM and automated metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pairwise comparison and ranking evaluation (LM-based and conventional metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Relative evaluation approach where evaluators compare two responses and choose the better one; implemented via LM pairwise comparison (used by GPT-4) and measured against human pairwise judgments; ranking derived via merge-sort requiring O(n log n) comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking Foundation Models with Language-Model-as-an-Examiner</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>Pairwise comparison / ranking (LM-as-judge pairwise comparisons aggregated into rank)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4 (primary), also used peer examiners (Claude, ChatGPT, Bard) for comparisons in peer-examination experiments</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>pairwise comparison (directly ask LM to pick better of two responses considering accuracy, coherence, factuality, comprehensiveness); results averaged and merge-sorted into ranking</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>open-ended QA responses (machine-generated answers)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>general knowledge / multi-domain (LMExamQA)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Holistic superiority considering accuracy, coherence, factuality, comprehensiveness in pairwise choice</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human annotators provided pairwise comparisons among model responses for sampled questions (used as ground truth for pairwise accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td>3 (pairwise comparisons collected per question)</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>human annotators (not explicitly specialized)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>pairwise comparison accuracy (fraction of LM decisions matching human-labeled comparisons); also Spearman/Kendall for ranked outputs vs human rankings</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>GPT-4 pairwise accuracy vs human comparisons = 0.853 (85.3%); conventional metrics yielded much lower pairwise agreement (ROUGE/BLEU/BERTScore accuracies reported between ~0.44â€“0.53).</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>When LM evaluators are knowledgeable and operate on objective correctness/coherence distinctions; pairwise comparisons are less sensitive to scale-interpretation variance and thus align better with humans.</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>When stylistic differences dominate or when responses are paraphrases of equal quality (causing evaluator bias); some LMs (e.g., Vicuna in experiments) produced inconsistent pairwise comparisons unless constrained.</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Pairwise alignment with humans is higher than absolute metrics even for complex responses, but overall reliability still drops when deeper multi-round understanding is required.</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Relative comparison reduces ambiguity about scoring scales and increases agreement with humans; ensuring symmetric evaluation (averaging original and reversed input order) was necessary to avoid ordering biases.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>300 pairwise comparisons derived from 100 questions (3 responses per question) used in metric-evaluation experiments; also larger ranking experiments across 8 models on LMExamQA (10,000 Qs) for benchmarking.</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Pairwise LM judgments (GPT-4) matched human pairwise labels at ~85.3%, substantially closer to human preference judgments than lexical similarity metrics; thus pairwise LM evaluation approaches human-like discrimination.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pairwise comparison by LMs (GPT-4) is highly aligned with human pairwise judgments and is more discerning than absolute scoring and reference-based metrics; averaging reversed-order comparisons improves consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Some LMs exhibit inconsistency or order sensitivity in pairwise judgments; pairwise evaluation is more expensive (O(n log n) comparisons) and may still reflect evaluator style preference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking Foundation Models with Language-Model-as-an-Examiner', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1842.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1842.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Peer-examination</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decentralized Peer-Examination using multiple LM examiners</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decentralized evaluation mechanism where multiple participating LMs each act as examiners (generate questions and evaluate others); combined results via voting/averaging to mitigate single-examiner biases and broaden question coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking Foundation Models with Language-Model-as-an-Examiner</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>Peer-examination (multiple LMs as examiners; voting/aggregation of per-examiner evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Claude-instant, ChatGPT, Bard, Vicuna-13B (used as examiners in experiments; GPT-4 also used in centralized experiments but peer set excluded GPT-4 in some runs)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>each LM generates questions and evaluates other models using Likert scoring and pairwise comparisons; results combined (mean and weighted-normalized averages) to form final evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>open-ended QA responses (machine-generated answers)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>general knowledge / multi-domain (LMExamQA and peer-generated subsets)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Same dimensions (accuracy, coherence, factuality, comprehensiveness) and pairwise ranking aggregated across examiners</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Peer-examination experiment compared LM examiners' rankings to existing leaderboards; human annotations used elsewhere for metric validation but peer results were aggregated LM judgments</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Full-mark percentages per examiner; win-rate heatmaps; comparisons against human-labeled rankings in other experiments</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>Not reported as a single correlation number vs humans for peer-examination. Individual examiners' full-mark percentages (as judged by other examiners) and consistency measures presented (e.g., Claude avg full-mark=98.0 after scaling in Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Combining multiple LM examiners with diverse stylistic preferences produces more balanced evaluations and broader question distributions (t-SNE shows diverse question embeddings from different LMs).</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>When some examiners apply stricter criteria (e.g., Claude strict on coherence) causing scale differences that require normalization; unavailable or inconsistent examiner APIs limited scale of peer-examination.</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Peer-examination increases domain/question diversity and can reduce single-examiner failure modes on complex items, but deeper multi-round evaluation still reveals model weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Differences in examiner strictness necessitated normalization/weighted averaging to harmonize scores; clear shared rubric helps but per-model preferences remain.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>Peer runs: each peer examiner posed 100 questions across 20 domains (limited by API access), evaluated other models' responses; centralized dataset comprised 10,000 questions generated by GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Peer-examination was presented as a mitigation for single-LM biases; no single aggregated correlation vs human raters reported, but peer rankings aligned with external leaderboards (Elo) and produced more diverse questions.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Peer-examination broadens question coverage and reduces individual examiner biases (e.g., stylistic preference), producing fairer and more diverse evaluations; combining evaluations requires normalization due to differing strictness across LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Requires multiple powerful LMs capable of consistent evaluation (few are available); API access and cost limit scale; examiners have differing strictness and style biases that must be normalized; still no absolute replacement for humans in all failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking Foundation Models with Language-Model-as-an-Examiner', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gptscore: Evaluate as you desire <em>(Rating: 2)</em></li>
                <li>Gpteval: Nlg evaluation using gpt-4 with better human alignment <em>(Rating: 2)</em></li>
                <li>Is chatgpt a good nlg evaluator? a preliminary study <em>(Rating: 2)</em></li>
                <li>Human-like summarization evaluation with chatgpt <em>(Rating: 1)</em></li>
                <li>Summeval: Re-evaluating summarization evaluation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1842",
    "paper_id": "paper-378a545c3a1cf6c4aada8f9ee8820c0d8008220a",
    "extraction_schema_id": "extraction-schema-34",
    "extracted_data": [
        {
            "name_short": "LLM-as-an-Examiner (GPT-4)",
            "name_full": "Language Model as an Examiner using GPT-4",
            "brief_description": "A centralized evaluation setup where GPT-4 both generates (examiner) questions and evaluates other models' open-ended QA responses in a reference-free manner; used to compare automated judgments against human annotations.",
            "citation_title": "Benchmarking Foundation Models with Language-Model-as-an-Examiner",
            "mention_or_use": "use",
            "proxy_evaluation_method": "LLM-as-a-judge (centralized examiner)",
            "llm_judge_model": "GPT-4",
            "llm_judge_prompt_approach": "single rating on multiple dimensions (Likert-style on 4 dimensions + overall 1-5) and pairwise comparison / ranking (merge-sort over responses)",
            "artifact_type": "open-ended QA responses (machine-generated answers)",
            "artifact_domain": "general knowledge / multi-domain (LMExamQA)",
            "evaluation_criteria": "Accuracy, Coherence, Factuality, Comprehensiveness (1-3 each) + Overall score (1-5); also pairwise comparisons considering same four aspects",
            "human_evaluation_setup": "Human annotators scored responses on overall quality (1-5) and provided rankings; for metric evaluation 3 human scores and 3 human pairwise comparisons were collected per question",
            "human_expert_count": "3",
            "human_expert_expertise": "human annotators (instructed to perform web search for reference; expertise level not further specified)",
            "agreement_metric": "Spearman's rho, Kendall's tau for Likert-scale correlations; pairwise comparison accuracy for ranking",
            "agreement_score": "Spearman rho(GPT-4 vs humans)=0.633; Kendall tau(GPT-4 vs humans)=0.554; pairwise comparison accuracy=0.853",
            "high_agreement_conditions": "When the LM examiner is knowledgeable about the topic (examiner generated Q/A and provided ground-truth answers), and when using pairwise ranking rather than simple overlap metrics; absolute scoring and ranking both correlated closely with human annotations.",
            "low_agreement_conditions": "When the LM evaluator lacks adequate domain knowledge about the question (leading to misjudgments), for stylistic biases (e.g., preference for ChatGPT-style paraphrases), and for complex/deeper multi-round questions where model understanding is required.",
            "artifact_complexity_effect": "Agreement and reliable judgments decrease for higher-cognitive-level / multi-round (deeper) questions; evaluator misjudgments were observed in challenging cases (pilot: 18/100 GPT-4 judgments incorrect on Natural Questions evaluation).",
            "criteria_clarity_effect": "Defining clear evaluation dimensions (accuracy, coherence, factuality, comprehensiveness) and using structured rating + ranking improved alignment with humans; ranking (pairwise) was more discerning and achieved higher agreement than string-similarity metrics.",
            "sample_size": "300 model responses across 100 questions (300 annotated samples used for metric evaluation)",
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "Automated LLM judge (GPT-4) showed substantially higher correlation with human judgments (Spearman rho=0.633, Kendall tau=0.554, pairwise accuracy=85.3%) compared to standard automatic metrics (ROUGE, BLEU, BERTScore had much lower correlations).",
            "calibration_or_training": null,
            "key_findings": "GPT-4 used as an examiner/judge correlates substantially better with human judgments than traditional automatic metrics (ROUGE/BLEU/BERTScore). Pairwise ranking by GPT-4 achieved &gt;85% agreement with human pairwise labels. Major failure modes include cases where the LM judge lacks the specific knowledge required, and stylistic preference biases; generating questions and answers with the same LM (examiner) helps the examiner be confident and improves reliability.",
            "limitations_noted": "Centralized single-model evaluation can be biased (style or preference); evaluator misjudgments occur when the judge lacks domain knowledge; no reported inter-human agreement statistics; limited to models capable of consistent pairwise comparison (some models could not be used reliably as examiners).",
            "uuid": "e1842.0",
            "source_info": {
                "paper_title": "Benchmarking Foundation Models with Language-Model-as-an-Examiner",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Likert-style scoring (LM-based)",
            "name_full": "Likert-scale scoring on multiple dimensions performed by LMs",
            "brief_description": "Structured absolute scoring where the automated evaluator (an LM) rates responses on predefined dimensions (accuracy, coherence, factuality, comprehensiveness) with 1-3 scales plus an overall 1-5 score; compared against human Likert scores.",
            "citation_title": "Benchmarking Foundation Models with Language-Model-as-an-Examiner",
            "mention_or_use": "use",
            "proxy_evaluation_method": "Likert-style rating (LM produces numeric scores per dimension and overall)",
            "llm_judge_model": "GPT-4 (used to assign Likert scores in experiments); other peer examiners can use same rubric",
            "llm_judge_prompt_approach": "single rating with rubric: four 1-3 dimension scores and one overall 1-5 score, evaluator instructed to assume its own answer as ground truth",
            "artifact_type": "open-ended QA responses (machine-generated answers)",
            "artifact_domain": "general knowledge / multi-domain (LMExamQA)",
            "evaluation_criteria": "Accuracy, Coherence, Factuality, Comprehensiveness (each 1-3); Overall quality (1-5)",
            "human_evaluation_setup": "Human annotators provided overall 1-5 scores and multidimensional 1-3 ratings for sampled responses; annotators instructed to consult web references if needed.",
            "human_expert_count": "3 (per response/question in the metric-evaluation sample)",
            "human_expert_expertise": "human annotators (not explicitly described as specialized domain experts)",
            "agreement_metric": "Spearman's rho and Kendall's tau between automated Likert overall score and human overall score",
            "agreement_score": "Reported for GPT-4 Likert overall vs human: Spearman rho=0.633; Kendall tau=0.554 (these numbers reported for the LM examiner overall metric as presented in Table 2)",
            "high_agreement_conditions": "Clear, structured rubric and use of knowledgeable LM examiner that produced the question-answer pairs; tasks where factual correctness and coherence are objective tended to align better.",
            "low_agreement_conditions": "Subjective or holistic dimensions like comprehensiveness can vary across annotators and LMs; evaluator knowledge gaps and stylistic preferences reduce alignment.",
            "artifact_complexity_effect": "Higher-complexity (analysis-level) questions reduce the reliability of single-round Likert scores; few-shot prompting helped models perform better on higher-level questions but also complicates evaluation.",
            "criteria_clarity_effect": "Explicitly specifying dimension definitions and scoring ranges improved correlation with human judgments; combining per-dimension scores into an overall score helps consistency.",
            "sample_size": "300 annotated responses across 100 questions (3 human scores per response used to compute correlations)",
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "LLM-assigned Likert scores (GPT-4) showed moderate-to-strong correlation with human overall scores (Spearman rho=0.633), substantially outperforming lexical/similarity automatic metrics.",
            "calibration_or_training": null,
            "key_findings": "Likert-style scoring by a strong LM examiner (GPT-4) correlates well with human overall judgments; structured dimensions help produce reliable absolute scores, but ranking remains more discerning.",
            "limitations_noted": "Ceiling/floor and subjectivity risks for Likert scales; human expertise level and rubric interpretation may vary; no human inter-rater agreement numbers reported to contextualize LM-human agreement.",
            "uuid": "e1842.1",
            "source_info": {
                "paper_title": "Benchmarking Foundation Models with Language-Model-as-an-Examiner",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Pairwise Ranking (LM and automated metrics)",
            "name_full": "Pairwise comparison and ranking evaluation (LM-based and conventional metrics)",
            "brief_description": "Relative evaluation approach where evaluators compare two responses and choose the better one; implemented via LM pairwise comparison (used by GPT-4) and measured against human pairwise judgments; ranking derived via merge-sort requiring O(n log n) comparisons.",
            "citation_title": "Benchmarking Foundation Models with Language-Model-as-an-Examiner",
            "mention_or_use": "use",
            "proxy_evaluation_method": "Pairwise comparison / ranking (LM-as-judge pairwise comparisons aggregated into rank)",
            "llm_judge_model": "GPT-4 (primary), also used peer examiners (Claude, ChatGPT, Bard) for comparisons in peer-examination experiments",
            "llm_judge_prompt_approach": "pairwise comparison (directly ask LM to pick better of two responses considering accuracy, coherence, factuality, comprehensiveness); results averaged and merge-sorted into ranking",
            "artifact_type": "open-ended QA responses (machine-generated answers)",
            "artifact_domain": "general knowledge / multi-domain (LMExamQA)",
            "evaluation_criteria": "Holistic superiority considering accuracy, coherence, factuality, comprehensiveness in pairwise choice",
            "human_evaluation_setup": "Human annotators provided pairwise comparisons among model responses for sampled questions (used as ground truth for pairwise accuracy)",
            "human_expert_count": "3 (pairwise comparisons collected per question)",
            "human_expert_expertise": "human annotators (not explicitly specialized)",
            "agreement_metric": "pairwise comparison accuracy (fraction of LM decisions matching human-labeled comparisons); also Spearman/Kendall for ranked outputs vs human rankings",
            "agreement_score": "GPT-4 pairwise accuracy vs human comparisons = 0.853 (85.3%); conventional metrics yielded much lower pairwise agreement (ROUGE/BLEU/BERTScore accuracies reported between ~0.44â€“0.53).",
            "high_agreement_conditions": "When LM evaluators are knowledgeable and operate on objective correctness/coherence distinctions; pairwise comparisons are less sensitive to scale-interpretation variance and thus align better with humans.",
            "low_agreement_conditions": "When stylistic differences dominate or when responses are paraphrases of equal quality (causing evaluator bias); some LMs (e.g., Vicuna in experiments) produced inconsistent pairwise comparisons unless constrained.",
            "artifact_complexity_effect": "Pairwise alignment with humans is higher than absolute metrics even for complex responses, but overall reliability still drops when deeper multi-round understanding is required.",
            "criteria_clarity_effect": "Relative comparison reduces ambiguity about scoring scales and increases agreement with humans; ensuring symmetric evaluation (averaging original and reversed input order) was necessary to avoid ordering biases.",
            "sample_size": "300 pairwise comparisons derived from 100 questions (3 responses per question) used in metric-evaluation experiments; also larger ranking experiments across 8 models on LMExamQA (10,000 Qs) for benchmarking.",
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "Pairwise LM judgments (GPT-4) matched human pairwise labels at ~85.3%, substantially closer to human preference judgments than lexical similarity metrics; thus pairwise LM evaluation approaches human-like discrimination.",
            "calibration_or_training": null,
            "key_findings": "Pairwise comparison by LMs (GPT-4) is highly aligned with human pairwise judgments and is more discerning than absolute scoring and reference-based metrics; averaging reversed-order comparisons improves consistency.",
            "limitations_noted": "Some LMs exhibit inconsistency or order sensitivity in pairwise judgments; pairwise evaluation is more expensive (O(n log n) comparisons) and may still reflect evaluator style preference.",
            "uuid": "e1842.2",
            "source_info": {
                "paper_title": "Benchmarking Foundation Models with Language-Model-as-an-Examiner",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Peer-examination",
            "name_full": "Decentralized Peer-Examination using multiple LM examiners",
            "brief_description": "A decentralized evaluation mechanism where multiple participating LMs each act as examiners (generate questions and evaluate others); combined results via voting/averaging to mitigate single-examiner biases and broaden question coverage.",
            "citation_title": "Benchmarking Foundation Models with Language-Model-as-an-Examiner",
            "mention_or_use": "use",
            "proxy_evaluation_method": "Peer-examination (multiple LMs as examiners; voting/aggregation of per-examiner evaluations)",
            "llm_judge_model": "Claude-instant, ChatGPT, Bard, Vicuna-13B (used as examiners in experiments; GPT-4 also used in centralized experiments but peer set excluded GPT-4 in some runs)",
            "llm_judge_prompt_approach": "each LM generates questions and evaluates other models using Likert scoring and pairwise comparisons; results combined (mean and weighted-normalized averages) to form final evaluation",
            "artifact_type": "open-ended QA responses (machine-generated answers)",
            "artifact_domain": "general knowledge / multi-domain (LMExamQA and peer-generated subsets)",
            "evaluation_criteria": "Same dimensions (accuracy, coherence, factuality, comprehensiveness) and pairwise ranking aggregated across examiners",
            "human_evaluation_setup": "Peer-examination experiment compared LM examiners' rankings to existing leaderboards; human annotations used elsewhere for metric validation but peer results were aggregated LM judgments",
            "human_expert_count": null,
            "human_expert_expertise": null,
            "agreement_metric": "Full-mark percentages per examiner; win-rate heatmaps; comparisons against human-labeled rankings in other experiments",
            "agreement_score": "Not reported as a single correlation number vs humans for peer-examination. Individual examiners' full-mark percentages (as judged by other examiners) and consistency measures presented (e.g., Claude avg full-mark=98.0 after scaling in Table 4).",
            "high_agreement_conditions": "Combining multiple LM examiners with diverse stylistic preferences produces more balanced evaluations and broader question distributions (t-SNE shows diverse question embeddings from different LMs).",
            "low_agreement_conditions": "When some examiners apply stricter criteria (e.g., Claude strict on coherence) causing scale differences that require normalization; unavailable or inconsistent examiner APIs limited scale of peer-examination.",
            "artifact_complexity_effect": "Peer-examination increases domain/question diversity and can reduce single-examiner failure modes on complex items, but deeper multi-round evaluation still reveals model weaknesses.",
            "criteria_clarity_effect": "Differences in examiner strictness necessitated normalization/weighted averaging to harmonize scores; clear shared rubric helps but per-model preferences remain.",
            "sample_size": "Peer runs: each peer examiner posed 100 questions across 20 domains (limited by API access), evaluated other models' responses; centralized dataset comprised 10,000 questions generated by GPT-4.",
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "Peer-examination was presented as a mitigation for single-LM biases; no single aggregated correlation vs human raters reported, but peer rankings aligned with external leaderboards (Elo) and produced more diverse questions.",
            "calibration_or_training": null,
            "key_findings": "Peer-examination broadens question coverage and reduces individual examiner biases (e.g., stylistic preference), producing fairer and more diverse evaluations; combining evaluations requires normalization due to differing strictness across LMs.",
            "limitations_noted": "Requires multiple powerful LMs capable of consistent evaluation (few are available); API access and cost limit scale; examiners have differing strictness and style biases that must be normalized; still no absolute replacement for humans in all failure modes.",
            "uuid": "e1842.3",
            "source_info": {
                "paper_title": "Benchmarking Foundation Models with Language-Model-as-an-Examiner",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gptscore: Evaluate as you desire",
            "rating": 2
        },
        {
            "paper_title": "Gpteval: Nlg evaluation using gpt-4 with better human alignment",
            "rating": 2
        },
        {
            "paper_title": "Is chatgpt a good nlg evaluator? a preliminary study",
            "rating": 2
        },
        {
            "paper_title": "Human-like summarization evaluation with chatgpt",
            "rating": 1
        },
        {
            "paper_title": "Summeval: Re-evaluating summarization evaluation",
            "rating": 1
        }
    ],
    "cost": 0.01716175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Benchmarking Foundation Models with Language-Model-as-an-Examiner</h1>
<p>Yushi Bai ${ }^{1 <em>}$, Jiahao Ying ${ }^{2 </em>}$, Yixin Cao ${ }^{2}$, Xin Lv ${ }^{1}$, Yuze $\mathrm{He}^{1}$, Xiaozhi Wang ${ }^{1}$, Jifan Yu ${ }^{1}$, Kaisheng Zeng ${ }^{1}$, Yijia Xiao ${ }^{3}$, Haozhe Lyu ${ }^{4}$, Jiayin Zhang ${ }^{1}$, Juanzi Li ${ }^{1}$, Lei Hou ${ }^{1 \text { Â® }}$<br>${ }^{1}$ Tsinghua University, Beijing, China ${ }^{2}$ Singapore Management University, Singapore<br>${ }^{3}$ University of California, Los Angeles, CA, USA<br>${ }^{4}$ Beijing University of Posts and Telecommunications, Beijing, China</p>
<h4>Abstract</h4>
<p>Numerous benchmarks have been established to assess the performance of foundation models on open-ended question answering, which serves as a comprehensive test of a model's ability to understand and generate language in a manner similar to humans. Most of these works focus on proposing new datasets, however, we see two main issues within previous benchmarking pipelines, namely testing leakage and evaluation automation. In this paper, we propose a novel benchmarking framework, Language-Model-as-an-Examiner, where the LM serves as a knowledgeable examiner that formulates questions based on its knowledge and evaluates responses in a reference-free manner. Our framework allows for effortless extensibility as various LMs can be adopted as the examiner, and the questions can be constantly updated given more diverse trigger topics. For a more comprehensive and equitable evaluation, we devise three strategies: (1) We instruct the LM examiner to generate questions across a multitude of domains to probe for a broad acquisition, and raise follow-up questions to engage in a more in-depth assessment. (2) Upon evaluation, the examiner combines both scoring and ranking measurements, providing a reliable result as it aligns closely with human annotations. (3) We additionally propose a decentralized Peer-examination method to address the biases in a single examiner. Our data and benchmarking results are available at: http://lmexam.xlore.cn.</p>
<h2>1 Introduction</h2>
<p>Recently, many large foundation models [1], such as ChatGPT [2], LLaMA [3], and PaLM [4], have emerged with impressive general intelligence and assisted billions of users worldwide. For various users' questions, they can generate a human-like response. However, the answers are not always trustworthy, e.g., hallucination [5]. To understand the strengths and weaknesses of foundation models, various benchmarks have been established $[6,7,8,9,10,11]$.
Nevertheless, we see two main hurdles in existing benchmarking methods, as summarized below. (1) Testing leakage. Along with increasing tasks and corpus involved in pre-training, the answer to the testing sample may have been seen and the performance is thus over-estimated. (2) Evaluation automation. Evaluating machine-generated texts is a long-standing challenge. Thus, researchers often convert the tasks into multi-choice problems to ease the quantitative analysis. This is clearly against real scenarios - as user-machine communications are mostly open-ended Question Answering (QA) or freeform QA [12]. On the other hand, due to the existence of a vast number of valid "good" answers, it is impossible to define one or several groundtruth, making similarity-based matching</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of our benchmarking method. The left part shows the use of language model as an examiner. The examiner generates questions from various domains, allowing it to probe for comprehensive understanding (knowledge breadth) as well as deep specialization (knowledge depth) through follow-up questions (FQs). It then scores and ranks other models' responses according to its understanding of the subject, providing a reliable evaluation. The right part presents peer-examination, a novel decentralized method that provides fairer evaluation results, which potentially demands higher workload of running multiple LM examiners, compared to running a single LM examiner.</p>
<p>Measurements (e.g., Exact Match, ROUGE-L [13], and BERTScore [14]) ineffective [12, 15, 16]. Therefore, recent works target a well-trained evaluator language model (LM) to assess the answer quality in a reference-free manner [17, 18, 19]. However, using LM as an evaluator also presents a problem: What if the evaluator hallucinates and makes wrong judgments during assessment?</p>
<p>As an attempt, our pilot study utilizes GPT-4 [20] to evaluate the correctness of LLaMA [3] on <em>Natural Questions</em> [21], where non-negligible 18 out of 100 judgments are incorrect (cases in Appendix A). We attribute the main reason to the inadequate knowledge of the evaluator itself regarding the questions. A straightforward solution is to use the LM not just as an evaluator to assess the responses, but as a knowledgeable examiner to also formulate questions, which is guaranteed a thorough understanding of the judgments. And, it naturally addresses the testing leakage issue by generating new questions periodically. Yet, relying on a centralized examiner can hardly be considered fair, especially when evaluating the examiner itself â€” <em>A man who is his own lawyer has a fool for his client</em>.</p>
<p>In this paper, we propose a novel benchmarking framework, <em>Language-Model-as-an-Examiner</em>, to assess current foundation models, mitigating the aforementioned issues. Herein, the language model acts as a knowledgeable examiner that poses questions based on its inherent knowledge and evaluates others on their responses. We devise three strategies to alleviate potential bias:</p>
<ul>
<li><strong>Increasing Knowledge Breadth and Depth</strong>. In terms of breadth, according to a predefined taxonomy, we select as many diverse domains as possible to generate questions. In terms of depth, to probe models deeply within a specific subfield, we propose a multi-round setting where the evaluator mimics an interviewer, posing more sophisticated follow-up questions based on the interviewee model's preceding responses. We release our dataset, namely LMExamQA, which is constructed using GPT-4 [20] as an examiner.</li>
<li><strong>Reliable Evaluation Measurement</strong>. We explore two evaluation metrics, namely Likert scale scoring and Ranking, offering a more comprehensive evaluation result. The results from both metrics correlate closely with human annotations, significantly outperforming all previous metrics.</li>
<li><strong>Peer-examination Mechanism</strong>. To avoid the potential bias arising from a single model as examiner, we propose a decentralized evaluation setting where all participating models are invited to be the examiner and assess each other.</li>
</ul>
<p>In experiments, our benchmarking pipeline yields fruitful results on 8 popular foundation models. We also demonstrate that peer-examination can generate a more diverse set of questions for knowledge probing and balance the biases from individual evaluator models, ultimately leading to a more equitable evaluation outcome.</p>
<h1>2 Related Work</h1>
<p>Benchmarks for Foundation Models. Various benchmarks have been proposed to assess foundation models on open-ended question answering, since it is the most natural setting for user-machine interaction in real scenarios. Some prominent such benchmarks include MS MARCO [22], SQuAD [23, 24], Natural Questions [21], WebQuestions [25] and OpenBookQA [26]. On the other hand, there exist a limited number of datasets that feature long-form QA. One of the widely-recognized examples is ELI5 [27], which comprises questions that necessitate lengthy descriptive and explanatory answers. One notable limitation of these benchmarks is their reliance on human curation and annotation, which inherently constrains their scalability. Our approach, by comparison, utilizes LMs to construct datasets, offering the advantage of effortless extensibility.
Automating NLG Evaluation. To evaluate machine-generated responses to the questions, several automatic metrics have been adopted, including the F1 score, Exact Match (EM), BLEU [28], ROUGE [13], and METEOR [29]. However, each metric has its own shortcomings, resulting in large discrepancies between the tested and actual performance [15, 30, 31].
To address these issues, well-trained LMs are utilized in NLG evaluation [32, 33, 34, 35]. One mainstream of previous methods is reference-based, where they derive the similarity between the candidate and the reference using an LM. Some prominent metrics in this class include MoverScore [36], BERTScore [14]. These metrics measure the distributional similarity rather than lexical overlap [37], making them appropriate for contexts that require more flexible generation. Recent studies [17, 18, 19, 38, 39, 40, 41, 42] have demonstrated that large language models (LLMs), such as ChatGPT [2], can conduct NLG evaluations in a reference-free manner. They can rate a candidate text (or perform a comparative assessment of two candidates) based on a specified evaluation aspect, displaying a high correlation with human assessments in tasks such as summarization and story generation [43, 44]. In these studies, the evaluations primarily focus on lexical quality aspects, such as coherence and fluency, of a generated text. However, their capability to evaluate crucial aspects in a QA response, including factual correctness and information comprehensiveness, remains uncertain. Moreover, a single evaluator inevitably brings bias to the assessment [18]. Our work aims to resolve these issues by leveraging LM not just as an evaluator but also as an examiner, assessing the performance of other models through self-generated questions, and deploying multiple LM examiners to ensure balanced evaluation.</p>
<h2>3 Methodology</h2>
<p>In this section, we discuss the methodology in language-model-as-an-examiner, including the LMExamQA dataset construction, the evaluation metric design, and the peer-examination pipeline.</p>
<h3>3.1 Dataset Construction</h3>
<p>Question Generation towards Knowledge Breadth. We employ a language model (LM) as an examiner that generates diversifying and high-quality questions across various domains. To ensure wide coverage of knowledge, we choose the Google Trends Categories ${ }^{2}$ as the domain taxonomy, and randomly select $n$ domains from it. For each domain, we prompt the LM to generate $m$ distinct questions. Our designed prompt (shown in Appendix B) is formulated to ensure that the generated questions possess three essential characteristics: diversified question forms, varied cognitive levels, and most importantly, assurance that the LM has a comprehensive understanding of the knowledge surrounding the question it poses. Figure 2 shows the distribution of question forms based on their interrogative words, and the distribution of question domains. According to Bloom's taxonomy [45], we divide the questions into 3 categories based on their required cognitive levels, from low to high-level, namely knowledge memorization, knowledge comprehension, and knowledge analysis:</p>
<ul>
<li>Knowledge memorization. Questions of such level demand recognition or recollection of certain entities and attributes, such as a person, location, or time.</li>
<li>Knowledge comprehension. These questions involve demonstrating an understanding of particular instances or concepts, such as "What is ...", "Why ...", and "How ...".</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Statistics of generated questions in LMExamQA.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MS [22]</th>
<th style="text-align: center;">SQuAD2.0 [24]</th>
<th style="text-align: center;">NQ [21]</th>
<th style="text-align: center;">ELI5 [27]</th>
<th style="text-align: center;">Ours</th>
<th style="text-align: center;">Example questions in our dataset</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Analysis</td>
<td style="text-align: center;">1\%</td>
<td style="text-align: center;">4\%</td>
<td style="text-align: center;">3\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">35\%</td>
<td style="text-align: center;">What are the potential short and long-term impacts of divorce on children?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Comprehension</td>
<td style="text-align: center;">4\%</td>
<td style="text-align: center;">13\%</td>
<td style="text-align: center;">19\%</td>
<td style="text-align: center;">100\%</td>
<td style="text-align: center;">43\%</td>
<td style="text-align: center;">How does towing capacity affect a truck's performance and what factors influence its maximum towing limit?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">memorization</td>
<td style="text-align: center;">95\%</td>
<td style="text-align: center;">83\%</td>
<td style="text-align: center;">78\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">22\%</td>
<td style="text-align: center;">Which international organization publishes the World Economic Outlook report?</td>
</tr>
</tbody>
</table>
<p>Table 1: Proportions of each level of questions. MS and NQ are short for MS MARCO and Natural Questions. We also list an example question in LMExamQA for each category.</p>
<ul>
<li>Knowledge analysis. Questions of this type require more advanced cognitive skills and they typically question the impact, comparison, or advantages and disadvantages of a given topic.</li>
</ul>
<p>By adopting GPT-4 to categorize the questions in LMExamQA and previous open-ended QA datasets into three levels ${ }^{3}$, we obtain the distribution with respect to the 3 cognitive levels as listed in Table 1, and show an example for each type of question. Compared with previous datasets, LMExamQA achieves a more balanced distribution across these 3 levels, thus providing a means of quantifying foundational models' proficiency at each cognitive level. Furthermore, LMExamQA includes a larger proportion of questions classified within higher cognitive levels, particularly at the analysis level, indicating a greater level of challenge.</p>
<p>To justify the reliability of the LM examiner as an evaluator on these questions, we employ it to produce a groundtruth answer with the prompt, "Answer the questions accurately and completely, without providing additional details." Upon evaluation by human experts on a random selection of 100 questions, the answers offered by the LM exhibit a $100 \%$ accuracy rate, thereby demonstrating mastery over the questions it generates.
Multi-round Follow-up Question Generation towards Knowledge Depth. To further probe the model's comprehension of a topic in depth, we develop an evaluation procedure involving multiple rounds of follow-up inquiries, drawing inspiration from the interview process. We utilize the LM examiner to construct a series of follow-up inquiries (prompt is shown in the Appendix B). These follow-up questions are specifically tailored to delve deeper into the concepts presented within the model-generated answers from the previous round. As the follow-up questions are dependent on the model's generated answers, we only ask follow-up questions for the correctly answered queries (determined by the LM examiner) and calculate the proportion of correct responses in the subsequent round. We limit the total number of rounds to $k$ in order to minimize topic deviation that might occur during longer sessions. Note that we only provide the interviewee model with the follow-up question as input, rather than engaging the "exam history" ${ }^{4}$, since most models are not capable of multi-round conversations. We show an example of a follow-up question to Flan-T5 [49]:</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Question: Which material is primarily used to manufacture semiconductor devices?
Flan-T5: Silicon $\checkmark$
Follow-up Question: What are the advantages of using silicon as the primary material for semiconductor devices?
Flan-T5: Silicon is a nonrenewable resource, and it is the most abundant element on Earth. $\boldsymbol{X}$</p>
<h1>3.2 Evaluation Metrics</h1>
<p>Several methodologies are commonly employed to facilitate human-like evaluation in LMs, prominent among these are the Likert scale scoring [17, 18, 42] and pairwise comparison [39, 42]. For the purposes of our benchmark, we incorporate both Likert scale scoring and a variant of pairwise comparison, namely ranking.
Likert scale scoring functions as an absolute evaluative measure, where the evaluator assigns scores to a given response along predefined dimensions. We establish four distinct dimensions on our dataset: (1) Accuracy. This assesses the extent to which the provided response accurately answers the question. (2) Coherence. This evaluates the logical structure and organization of the response and the degree to which it can be comprehended by non-specialists. (3) Factuality. This examines whether the response contains factual inaccuracies. (4) Comprehensiveness. This gauges whether the response encompasses multiple facets of the question, thus providing a thorough answer. Each of these dimensions is scored on a scale of 1 to 3 , ranging from worst to best. We also ask the evaluator to provide an overall score ranging from 1 to 5 , based on the scores assigned to the previous 4 dimensions. This score serves as an indicator of the overall quality of the answer.
On the other hand, pairwise comparison operates as a relative evaluation method and is often more discerning compared to scoring. In this process, evaluators are given two responses and are tasked with determining which is superior, taking into account their accuracy, coherence, factuality, and comprehensiveness. Given that there are $n$ contestant models, we implement a merge sort algorithm to rank the $n$ responses, involving $\mathcal{O}(n \log n)$ pairwise comparisons.</p>
<h3>3.3 Decentralized Evaluation: Peer-Examination</h3>
<p>We introduce a novel decentralized method that incorporates multiple models to serve as examiners, namely Peer-examination (illustrated in the right part of Figure 1), since relying only on one centralized model as the examiner introduces the following potential drawbacks to the benchmarking process. (1) Coverage of generated questions: The examiner may not have a holistic understanding of certain domain knowledge. As a result, the examiner may struggle to propose questions that examine in detail on these areas, which in turn renders the scope of generated questions insufficient. (2) Potential bias during evaluation: The model itself may have a bias during evaluation. The bias can manifest as a preference for certain types of responses or a predisposition towards perspectives irrelevant to the quality of the responses, such as response length or linguistic style. For example, [18] shows that GPT-4 [20] prefers ChatGPT [2] summaries compared to human-written summaries. Such biases may result in unfair ranking assessment outcomes.</p>
<p>To mitigate these issues, during peer-examination, each model is assigned the role of an examiner separately. As examiners, they are responsible for posing questions and evaluating the answers provided by the other models. We then combine the evaluation results from each of these models by voting, and obtain a final result. This approach leverages the collective expertise and diverse perspectives of all models to improve the coverage of questions as well as ensure fairer assessments.</p>
<h2>4 Experiments</h2>
<p>To demonstrate the effectiveness of our Language-model-as-an-examiner framework, we first employ GPT-4 [20] as the examiner for a centralized evaluation, since it exhibits a broad understanding of knowledge $[9,50,51]$ and a precise judgmental ability [17, 18]. In peer-examination, we also employ Claude (Claude-instant) [52], ChatGPT [2], Bard [53], and Vicuna-13B [39] as LM examiners.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">ROUGE-1</th>
<th style="text-align: center;">ROUGE-2</th>
<th style="text-align: center;">ROUGE-L</th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">BERTScore</th>
<th style="text-align: center;">GPT-4</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Overall Score</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Spearman $(\rho)$</td>
<td style="text-align: center;">0.197</td>
<td style="text-align: center;">0.211</td>
<td style="text-align: center;">0.158</td>
<td style="text-align: center;">0.318</td>
<td style="text-align: center;">0.219</td>
<td style="text-align: center;">0.633</td>
</tr>
<tr>
<td style="text-align: left;">Kendall $(\tau)$</td>
<td style="text-align: center;">0.147</td>
<td style="text-align: center;">0.159</td>
<td style="text-align: center;">0.118</td>
<td style="text-align: center;">0.241</td>
<td style="text-align: center;">0.164</td>
<td style="text-align: center;">0.554</td>
</tr>
<tr>
<td style="text-align: left;">Pairwise comparison</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Accuracy</td>
<td style="text-align: center;">0.473</td>
<td style="text-align: center;">0.533</td>
<td style="text-align: center;">0.530</td>
<td style="text-align: center;">0.437</td>
<td style="text-align: center;">0.487</td>
<td style="text-align: center;">0.853</td>
</tr>
</tbody>
</table>
<p>Table 2: LM examiner's correlation with human annotations, compared with previous metrics.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Single (0-shot / 5-shot)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Multi-</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">memorization</td>
<td style="text-align: center;">Comprehension</td>
<td style="text-align: center;">Analysis</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Models without SFT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">BLOOMZ [54]</td>
<td style="text-align: center;">$20.7 / 25.5$</td>
<td style="text-align: center;">$63.5 / 58.5$</td>
<td style="text-align: center;">$13.7 / 23.2$</td>
<td style="text-align: center;">$3.8 / 8.6$</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Flan-T5 [55]</td>
<td style="text-align: center;">$17.0 / 26.0$</td>
<td style="text-align: center;">$49.8 / 62.7$</td>
<td style="text-align: center;">$11.2 / 21.4$</td>
<td style="text-align: center;">$4.6 / 9.7$</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Flan-UL2 [55]</td>
<td style="text-align: center;">$15.5 / 24.8$</td>
<td style="text-align: center;">$51.3 / 64.4$</td>
<td style="text-align: center;">$9.2 / 20.1$</td>
<td style="text-align: center;">$2.2 / 7.0$</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GLM-130B [56]</td>
<td style="text-align: center;">$14.9 / 33.3$</td>
<td style="text-align: center;">$45.9 / 60.7$</td>
<td style="text-align: center;">$9.1 / 31.9$</td>
<td style="text-align: center;">$3.7 / 18.7$</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">LLaMA-13B [3]</td>
<td style="text-align: center;">$29.5 / 54.3$</td>
<td style="text-align: center;">$72.3 / 83.9$</td>
<td style="text-align: center;">$23.7 / 53.2$</td>
<td style="text-align: center;">$11.2 / 38.1$</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">LLaMA-65B [3]</td>
<td style="text-align: center;">$38.5 / 62.9$</td>
<td style="text-align: center;">$78.2 / 88.2$</td>
<td style="text-align: center;">$34.0 / 62.7$</td>
<td style="text-align: center;">$20.4 / 48.1$</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Fine-tuned Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Vicuna-13B [39]</td>
<td style="text-align: center;">96.5</td>
<td style="text-align: center;">89.9</td>
<td style="text-align: center;">98.27</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT [2]</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">97.7</td>
<td style="text-align: center;">99.71</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 3: Percentage (\%) of full-mark answers on LMExamQA. We show the 0 -shot and 5 -shot performance for models without SFT, with both results being separated by "/".</p>
<p>Figure 3: Scores on each aspect. We take the 5-shot performance for models without SFT.</p>
<h1>4.1 Metric Evaluation</h1>
<p>To verify the reliability of our method for scoring and comparison based assessment, we perform metric evaluation. We conduct human evaluations on machine-generated responses. These evaluations are quantified using a 1-5 Likert scale for the overall score, and we let annotators to rank different responses for each question based on their holistic quality. We collect 300 annotations across 100 questions from LMExamQA. For each question, we randomly select 3 of the model responses, and obtain 3 scoring annotations and 3 pairwise comparison results. For Likert scoring, we calculate Spearman's $\rho$ and Kendall's $\tau$ between the overall scores given by the automatic metrics and human experts; for ranking, we compute the accuracy of pairwise comparisons offered by the automatic metrics, according to the human-labeled comparison results. Then we compare the LM examiner, GPT-4 [20], with previous automatic metrics, including ROUGE-1, ROUGE-2, ROUGE-L [13], BLEU [28], BERTScore [14] (F1), and report their correlation with human judgments in Table 2. We observe that employing GPT-4 [20] as an examiner results in a much higher correlation with human annotations compared to prior metrics. More profoundly, GPT-4's pairwise comparison achieves an agreement of over $85 \%$ with human's.</p>
<h3>4.2 Centralized Benchmarking Results</h3>
<p>Experiment Setup. We conduct a centralized benchmarking with GPT-4 [20] as the examiner. Following the method in Section 3, we construct the LMExamQA dataset with GPT-4, where we set $n=1,000$ domains and $m=10$ questions for each domain, resulting in a total of 10,000 questions. We evaluate 8 popular and open-access foundation models on our LMExamQA dataset, including BLOOMZ (the 176B model) [54], Flan-T5 (the XXL model, 11B) [55], Flan-UL2 (20B) [55], GLM130B [56], LLaMA (the 13B model and the 65B model) [3], Vicuna-13B [39], and ChatGPT [2]. These models are categorized based on their training procedure: whether they have undergone Supervised Fine-Tuning (SFT) or not. The first 6 models are trained without SFT ${ }^{5}$, whereas the last 2 models are fine-tuned. For models without SFT, we assess their 0 -shot and 5 -shot performance. During generation, for the examiner and the subject models, we set the temperature to 0 for reproducibility. More details for reproducing our results are shown in Appendix C.2.
Single-round QA for Knowledge Breath. Table 3 presents the percentage of full-mark answers for each model on LMExamQA. Full-mark answers are defined as responses that receive a rating of 5 on the overall score, and the proportion of such responses is reported for each category of questions. Additionally, Figure 3 provides a radar plot depicting the average scores of models on 5 dimensions;</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>We also conduct ranking evaluation over the 8 models (we only show the few-shot performance for models without SFT). In Figure 4, we visualize the ranking results via a win-rate heatmap (the $(i, j)$-th entry denotes the fraction of model $i$ wins when compared against model $j$ ) along with each model's average win-rate against all other models. We summarize our key findings.</p>
<ol>
<li>The scaling law on LMExamQA. LLaMA-65B significantly outperforms LLaMA-13B across all question categories, adhering to the scaling law of LMs [4, 57].</li>
<li>Few-shot leads to more substantial improvement on higher cognitive-level questions. For models without SFT, we observe that 5-shot examples yield an average relative improvement of $17 \%, 123 \%$, and $206 \%$ on memorization, comprehension, and analysis type questions, respectively. This implies that the model may possess adequate knowledge to answer higher-level questions (e.g., distinguishing between two concepts). However, it may lack the ability to retrieve knowledge from its memory and structure appropriate language to form an answer. Few-shot examples serve to provide demonstrations on how to answer such questions.</li>
<li>What does SFT offer? We notice a huge performance gap between LLaMA-13B and Vicuna-13B (Vicuna-13B is fine-tuned on LLaMA-13B with 70k user-shared ChatGPT conversations), mainly on the latter two types of questions. This result suggests that SFT primarily plays a crucial role in aligning LM's responses for task adaptation, rather than enriching the model's knowledge especially in the context of higher-level questions that demand more sophisticated answers.</li>
<li>LLMs can provide factually correct and coherent responses, but struggle for more comprehensive accurate answers. The radar plot reveals that all models achieve relatively high scores concerning factuality and coherence (over $80 / 100$ ), but different models vary widely in terms of comprehensiveness, i.e., whether the response addresses all aspects of a question.</li>
<li>Ranking results interpretation. Fine-tuned models, including Vicuna and ChatGPT, demonstrate near-perfect performance in terms of their scores (Table 3). In our dataset, ranking proves to be a more discerning evaluation approach. For example, the win-rate heatmap 4 reveals that ChatGPT outperforms Vicuna-13B with a $68 \%$ win rate, indicating a notable difference in the quality of responses generated by the two models. A ranking or comparison based evaluation is rarely used in QA evaluation, we encourage the research community to adopt more deliberate evaluation techniques in benchmarking more advanced foundation models on open-ended QA.</li>
</ol>
<p>Multi-round QA for Knowledge Depth. To conduct the multi-round QA, we randomly select 1,000 question-and-answer from the full mark answers in the first round. We then engage the examiner GPT-4 to generate the second-round question and ask the examinee models to answer the second round questions. We limit the number of rounds to $k=2$ due to the high cost of API usage. The evaluation results are presented in the last column of Table 3. We observe that excluding ChatGPT and Vicuna-13B, all examinee models exhibit a notable decrease in performance in the second round. This suggests that while these models initially demonstrated a robust understanding and knowledge base, their performance deteriorated when faced with more complicated questions, highlighting the importance of conducting more in-depth evaluations during QA to thoroughly assess the models' capabilities. We provide more insights on the experimental results in Appendix C.2.</p>
<h1>4.3 Peer-Examination Results</h1>
<p>For the Peer-examination process, we choose four prominent models, including ChatGPT [2], Claude [52], Vicuna-13B [39], Bard [53], which are carefully selected based on their capabilities to generate questions and assess NLG quality. Each of these models is assigned the role of an</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 5: Win-rate heatmap under different LMs as examiners.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 6: t-SNE on query embeddings. Left figure visualizes the embeddings of questions (generated by a centralized GPT-4 examiner) in LMExamQA; Right figure shows the embeddings of questions generated by 4 peer examiners.
examiner, posing 100 questions ${ }^{6}$ according to the given 20 domains and evaluate the remaining three models' responses. We show the scoring results in Table 4 and the pairwise comparison results in Figure 5 (more experimental details are shown in Appendix C.3). We observe the overall rank, from highest to lowest, as follows: Claude, ChatGPT, Bard, and Vicuna-13B. Intriguingly, this aligns with the rank obtained from the popular leaderboard using the Elo rating system [58]. Our approach differs as we utilize LMs as evaluators instead of human judges.</p>
<h1>4.4 Bias Analysis: Centralized vs Decentralized</h1>
<p>We identify two potential biases in a centralized examination: one that originates from biases inherent in the questions generated by the model, and the other one rooted in the model's evaluation process.</p>
<p>Bias in Generated Questions. To analyze the bias in the generated questions, we employ t-SNE to visualize the distributions of questions across three datasets: LMExamQA, Natural Questions [21], and SQuAD2.0 [24]. These questions are encoded into 1,536-dimensional vectors using the OpenAI text-embedding model, text-embedding-ada-002 [59]. As shown in the left figure in Figure 6, we randomly select 1,000 questions on each dataset and visualize their respective t-SNE embeddings.
Through the embedding visualization, we observe that the questions in our LMExamQA dataset exhibit a more uniform distribution compared to those in previous datasets. Furthermore, we utilize 4 different LMs to generate questions across 20 domains and depict their respective question embeddings in the right panel of Figure 6. As we expected, questions within the same domain cluster together. More notably, questions produced by different models exhibit distinct distributions around the central region of the domain cluster, indicating potential biases in questions generated by a single LM. This observation motivates our adoption of peer-examination that harnesses multiple models to generate a diverse and comprehensive set of questions.</p>
<p>Bias in Evaluation. To explore potential bias in the evaluation process, we devise a bias detection experiment to test whether the centralized examiner GPT-4 exhibits a preference for a particular linguistic style. This was achieved by having GPT-4 compare a pair of responses that were identical</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Examiner</th>
<th>Claude</th>
<th>ChatGPT</th>
<th>Bard</th>
<th>Vicuna</th>
<th>AVG / AVG ${ }_{\text {weight }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td>Examiner</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Claude [52]</td>
<td>-</td>
<td>98</td>
<td>100</td>
<td>96</td>
<td>98.0 / 99.7</td>
</tr>
<tr>
<td>ChatGPT [2]</td>
<td>41</td>
<td>-</td>
<td>100</td>
<td>95</td>
<td>78.6 / 98.9</td>
</tr>
<tr>
<td>Bard [53]</td>
<td>41</td>
<td>99</td>
<td>-</td>
<td>92</td>
<td>77.3 / 97.8</td>
</tr>
<tr>
<td>Vicuna [39]</td>
<td>42</td>
<td>98</td>
<td>99</td>
<td>-</td>
<td>79.6 / 99.3</td>
</tr>
</tbody>
</table>
<p>Table 4: Percentage (%) of full-mark answer from PeerExamination. AVG is the mean score given by the three other examiners. $\mathrm{AVG}_{\text {weight }}$ is the mean of the scaled scores, wherein the highest score within each column is adjusted to 100 for standardization purposes.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 7: Win-rate of ChatGPT and Vicuna written responses under different LM examiners.
in content but varied in linguistic style. Specifically, we employ ChatGPT to rephrase the responses from Vicuna-13B. To ensure quality equivalence after paraphrasing, we ask human annotators to select 100 rewritten responses that mirror the quality of the original responses. We then present these pairs to GPT-4 for a comparative evaluation, and the results of their respective win-rate are as shown in the "GPT-4" bar in Figure 7. The results indicate that GPT-4 favor responses rephrased by ChatGPT, suggesting a potential bias towards ChatGPT style responses during the evaluation process.
To investigate whether the observed bias in the centralized examination can be mitigated using peer-examination, we incorporate two models, namely Bard [53] and Claude [52]. We instruct these models to compare the two responses, and show the results in Figure 7. The results reveal that different models possess distinct preferences. As a solution, combining them within a peerexamination framework can balance their individual biases (the rightmost bar), and lead to a more equitable evaluation process.</p>
<h1>4.5 Measuring Data Leakage in Model-generated Questions</h1>
<p>The use of model-generated questions in our framework potentially retains the risk of data leakage because the generated content by the models (e.g., the generated questions) may replicate what they have seen during pretraining. Several studies have directly and indirectly demonstrated LLMs are capable of generating creative content instead of mere replication. For instance, experiments in the GPT-2 paper [60] revealed that the median 8 -gram overlap rates between GPT-2's outputs and the exact completions from the WebText test set articles were a mere $2.6 \%$. Moreover, a line of research such as Self-Instruct [61] and Alpaca [62] demonstrates that fine-tuning models using LM-generated instructions can significantly enhance their performance. This demonstrates that under appropriate prompts, LMs are capable of generating creative content instead of replicating text encountered during training.
Nevertheless, we provide an analysis of the potential data leakage in questions generated by LM on our LMExamQA dataset. The ideal method to verify whether an LLM has encountered similar questions during training is to investigate its training data. However, the training data for most language models, often sized at several terabytes, is not openly accessible. Consequently, we investigate two primary sources of training data instead - web data and public datasets. Considering web data, we perform Google search on 100 randomly sampled questions from LMExamQA and retrieve the most similar web queries. For public datasets, we look into the Flan collection [63], which contains 1,836 tasks and 15 M instances, and is widely used during instruction fine-tuning for LLMs. For each question in LMExamQA, we implement a BM25 search over the inputs from the Flan collection. Subsequently, we compute the ROUGE-L score between the question and the retrieved query. We find an average ROUGE-L score of 0.293 and 0.046 between the LM-generated question and the top retrieved data from the web and Flan collection, respectively. The low similarity scores in both data sources imply that the majority of the questions generated by the models are not present in the two sources. We also show 3 random questions along with their corresponding queries retrieved from web data. We can see that the model adds more qualifiers and determiners to the questions it generates than the data they've seen during pretraining, where memorizing and retelling from pretraining data is clearly not enough to answer these more precise, complex questions.</p>
<table>
<thead>
<tr>
<th>Question</th>
<th>Retrieved query</th>
</tr>
</thead>
<tbody>
<tr>
<td>How have tarot cards been adopted in modern spirituality?</td>
<td>What is the significance of tarot card reading?</td>
</tr>
<tr>
<td>What are some benefits to local economies from hosting swap meets?</td>
<td>How do farmers markets help the local economy?</td>
</tr>
<tr>
<td>How has the rise of e-commerce impacted the grocery and food retail industry?</td>
<td>How has e-commerce changed the retail industry?</td>
</tr>
</tbody>
</table>
<p>Table 5: Example questions and their retrieved queries.</p>
<p>Besides, human-collected, fixed datasets fall short in adapting to future scenarios where more pretraining data, even including data that overlaps with existing datasets, is used. With our approach, overlap in the dataset can be seamlessly addressed by re-generating the dataset via more sophisticated prompt engineering.</p>
<h1>5 Conclusion</h1>
<p>In this paper, we propose Language-Model-as-an-Examiner to address the difficulties in open-ended QA benchmarks. We construct the LMExamQA dataset, which aims to probe a more comprehensive and stratified understanding of knowledge. The dataset is equipped with a reliable language model examiner and we use it to benchmark several widely-used foundational models. To ensure a fair evaluation process, we devise a peer-examination pipeline. This approach serves to mitigate potential biases that might arise from the reliance on a single examiner. Expanding our benchmarking framework to incorporate more domain-specific language models, or even vision language models, could potentially offer a more holistic evaluation.</p>
<h2>6 Limitation</h2>
<p>Our proposed framework, despite offering a more streamlined approach for benchmarking foundation models, is not without its shortcomings. We summarize into two limitations. The first lies in the potential bias during evaluation. As we have elaborated in the paper, different models have different preferences towards distinct linguistic styles. They may also possess biases along other dimensions such as radical bias or gender bias. It's crucial to recognize these biases in future works as they might influence and be reflected in the evaluation results. Another limitation involves a lack of robust evaluation capability among existing foundation models to facilitate large-scale peer-examination. In our work, we identify only four current accessible models that demonstrate the required capacity to assess machine-generated text. We foresee in the near future that the emergence of more powerful foundation models will empower the peer-examination method, enabling more sophisticated and expansive evaluations.
Ethical Consideration. Creating a QA dataset using a large language model such as GPT-4 involves several ethical considerations. Here are some considerations to take into account:</p>
<ul>
<li>Data Privacy: Since the generated contents by LLMs may include personal information [64], it is crucial to ensure the anonymity of the data and protect sensitive personal information. We do not observe such information leakage in our LMExamQA dataset.</li>
<li>Misinformation and Harmful Content: It is vital to ensure that the LLM generated dataset does not include or encourage misinformation, hate speech, or any form of harmful content. A rigorous review of the LMExamQA dataset assures us that such content does not appear.</li>
<li>Fairness and Bias: Large language models, can unintentionally inherit and amplify societal biases present in the training data. It is important to put significant effort into identifying and mitigating such biases, as we illustrated in previous limitations.</li>
</ul>
<h2>Acknowledgement</h2>
<p>This work is supported by a grant from the Institute for Guo Qiang, Tsinghua University (2019GQB0003), the Tsinghua University Initiative Scientific Research Program, and the NSFC Youth Project (62006136). We gracefully thank all our anonymous reviewers for their insightful suggestions.</p>
<h1>References</h1>
<p>[1] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al., "On the opportunities and risks of foundation models," arXiv preprint arXiv:2108.07258, 2021.
[2] OpenAI, "Introducing chatgpt," 2022.
[3] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. RoziÃ¨re, N. Goyal, E. Hambro, F. Azhar, et al., "Llama: Open and efficient foundation language models," arXiv preprint arXiv:2302.13971, 2023.
[4] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al., "Palm: Scaling language modeling with pathways," arXiv preprint arXiv:2204.02311, 2022.
[5] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, and P. Fung, "Survey of hallucination in natural language generation," ACM Computing Surveys, vol. 55, no. 12, pp. 1-38, 2023.
[6] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman, "Superglue: A stickier benchmark for general-purpose language understanding systems," Advances in neural information processing systems, vol. 32, 2019.
[7] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt, "Measuring massive multitask language understanding," in International Conference on Learning Representations, 2021.
[8] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y. Zhang, D. Narayanan, Y. Wu, A. Kumar, et al., "Holistic evaluation of language models," arXiv preprint arXiv:2211.09110, 2022.
[9] W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan, "Agieval: A human-centric benchmark for evaluating foundation models," arXiv preprint arXiv:2304.06364, 2023.
[10] J. Yu, X. Wang, S. Tu, S. Cao, D. Zhang-Li, X. Lv, H. Peng, Z. Yao, X. Zhang, H. Li, et al., "Kola: Carefully benchmarking world knowledge of large language models," arXiv preprint arXiv:2306.09296, 2023.
[11] Y. Bai, X. Lv, J. Zhang, H. Lyu, J. Tang, Z. Huang, Z. Du, X. Liu, A. Zeng, L. Hou, Y. Dong, J. Tang, and J. Li, "Longbench: A bilingual, multitask benchmark for long context understanding," arXiv preprint arXiv:2308.14508, 2023.
[12] A. Rogers, M. Gardner, and I. Augenstein, "Qa dataset explosion: A taxonomy of nlp resources for question answering and reading comprehension," ACM Computing Surveys, vol. 55, no. 10, pp. 1-45, 2023.
[13] C.-Y. Lin, "Rouge: A package for automatic evaluation of summaries," in Text summarization branches out, pp. 74-81, 2004.
[14] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi, "Bertscore: Evaluating text generation with bert," in International Conference on Learning Representations, 2020.
[15] A. Chen, G. Stanovsky, S. Singh, and M. Gardner, "Evaluating question answering evaluation," in Proceedings of the 2nd workshop on machine reading for question answering, pp. 119-124, 2019.
[16] K. Krishna, A. Roy, and M. Iyyer, "Hurdles to progress in long-form question answering," in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2021.
[17] J. Fu, S.-K. Ng, Z. Jiang, and P. Liu, "Gptscore: Evaluate as you desire," arXiv preprint arXiv:2302.04166, 2023.
[18] Y. Liu, D. Iter, Y. Xu, S. Wang, R. Xu, and C. Zhu, "Gpteval: Nlg evaluation using gpt-4 with better human alignment," arXiv preprint arXiv:2303.16634, 2023.
[19] J. Wang, Y. Liang, F. Meng, H. Shi, Z. Li, J. Xu, J. Qu, and J. Zhou, "Is chatgpt a good nlg evaluator? a preliminary study," arXiv preprint arXiv:2303.04048, 2023.
[20] OpenAI, "Openai: Gpt-4," 2023.</p>
<p>[21] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, et al., "Natural questions: a benchmark for question answering research," Transactions of the Association for Computational Linguistics, vol. 7, pp. 453-466, 2019.
[22] T. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary, R. Majumder, and L. Deng, "Ms marco: A human generated machine reading comprehension dataset," choice, vol. 2640, p. 660, 2016.
[23] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, "Squad: 100,000+ questions for machine comprehension of text," in Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383-2392, 2016.
[24] P. Rajpurkar, R. Jia, and P. Liang, "Know what you donâ€™t know: Unanswerable questions for squad," in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 784-789, 2018.
[25] J. Berant, A. Chou, R. Frostig, and P. Liang, "Semantic parsing on freebase from questionanswer pairs," in Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 1533-1544, 2013.
[26] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, "Can a suit of armor conduct electricity? a new dataset for open book question answering," in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2381-2391, 2018.
[27] A. Fan, Y. Jernite, E. Perez, D. Grangier, J. Weston, and M. Auli, "Eli5: Long form question answering," in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3558-3567, 2019.
[28] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, "Bleu: a method for automatic evaluation of machine translation," in Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311-318, 2002.
[29] S. Banerjee and A. Lavie, "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments," in Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pp. 65-72, 2005.
[30] A. Chen, G. Stanovsky, S. Singh, and M. Gardner, "Mocha: A dataset for training and evaluating generative reading comprehension metrics," in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 6521-6532, 2020.
[31] C. Si, C. Zhao, and J. Boyd-Graber, "What's in a name? answer equivalence for open-domain question answering," in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 9623-9629, 2021.
[32] T. Sellam, D. Das, and A. Parikh, "Bleurt: Learning robust metrics for text generation," in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7881-7892, 2020.
[33] R. Rei, C. Stewart, A. C. Farinha, and A. Lavie, "Comet: A neural framework for mt evaluation," in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 2685-2702, 2020.
[34] W. Yuan, G. Neubig, and P. Liu, "Bartscore: Evaluating generated text as text generation," Advances in Neural Information Processing Systems, vol. 34, pp. 27263-27277, 2021.
[35] A. B. Sai, A. K. Mohankumar, and M. M. Khapra, "A survey of evaluation metrics used for nlg systems," ACM Computing Surveys (CSUR), vol. 55, no. 2, pp. 1-39, 2022.
[36] W. Zhao, M. Peyrard, F. Liu, Y. Gao, C. M. Meyer, and S. Eger, "Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance," in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2019.
[37] S. Gehrmann, E. Clark, and T. Sellam, "Repairing the cracked foundation: A survey of obstacles in evaluation practices for generated text," arXiv preprint arXiv:2202.06935, 2022.
[38] M. Zhong, Y. Liu, D. Yin, Y. Mao, Y. Jiao, P. Liu, C. Zhu, H. Ji, and J. Han, "Towards a unified multi-dimensional evaluator for text generation," in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 2023-2038, 2022.
[39] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, and E. P. Xing, "Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality," March 2023.</p>
<p>[40] Y. Chen, R. Wang, H. Jiang, S. Shi, and R. Xu, "Exploring the use of large language models for reference-free text quality evaluation: A preliminary empirical study," arXiv preprint arXiv:2304.00723, 2023.
[41] W. Yidong, Y. Zhuohao, Z. Zhengran, Y. Linyi, H. Qiang, W. Cunxiang, C. Hao, J. Chaoya, X. Rui, W. Jindong, X. Xing, Y. Wei, Z. Shikun, and Z. Yue, "Pandalm: Reproducible and automated language model assessment." https://github.com/WeOpenML/PandaLM, 2023.
[42] M. Gao, J. Ruan, R. Sun, X. Yin, S. Yang, and X. Wan, "Human-like summarization evaluation with chatgpt," arXiv preprint arXiv:2304.02554, 2023.
[43] A. R. Fabbri, W. KryÅ›ciÅ„ski, B. McCann, C. Xiong, R. Socher, and D. Radev, "Summeval: Re-evaluating summarization evaluation," Transactions of the Association for Computational Linguistics, vol. 9, pp. 391-409, 2021.
[44] H. Chen, D. M. Vo, H. Takamura, Y. Miyao, and H. Nakayama, "Storyer: Automatic story evaluation via ranking, rating and reasoning," Journal of Natural Language Processing, vol. 30, no. 1, pp. 243-249, 2023.
[45] B. S. Bloom and D. R. Krathwohl, Taxonomy of educational objectives: The classification of educational goals. Book 1, Cognitive domain. longman, 2020.
[46] E. Choi, H. He, M. Iyyer, M. Yatskar, W.-t. Yih, Y. Choi, P. Liang, and L. Zettlemoyer, "Quac: Question answering in context," in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2174-2184, 2018.
[47] S. Reddy, D. Chen, and C. D. Manning, "Coqa: A conversational question answering challenge," Transactions of the Association for Computational Linguistics, vol. 7, pp. 249-266, 2019.
[48] J. A. Campos, A. Otegi, A. Soroa, J. M. Deriu, M. Cieliebak, and E. Agirre, "Doqa-accessing domain-specific faqs via conversational qa," in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7302-7314, 2020.
[49] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. Dehghani, S. Brahma, et al., "Scaling instruction-finetuned language models," arXiv preprint arXiv:2210.11416, 2022.
[50] OpenAI, "Gpt-4 technical report," arXiv preprint arXiv:2303.08774, 2023.
[51] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al., "Sparks of artificial general intelligence: Early experiments with gpt-4," arXiv preprint arXiv:2303.12712, 2023.
[52] Anthropic, "Anthropic: Claude," 2023.
[53] Google, "Google: Bard," 2023.
[54] N. Muennighoff, T. Wang, L. Sutawika, A. Roberts, S. Biderman, T. L. Scao, M. S. Bari, S. Shen, Z.-X. Yong, H. Schoelkopf, et al., "Crosslingual generalization through multitask finetuning," arXiv preprint arXiv:2211.01786, 2022.
[55] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. Dehghani, S. Brahma, et al., "Scaling instruction-finetuned language models," arXiv preprint arXiv:2210.11416, 2022.
[56] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu, W. Zheng, X. Xia, W. L. Tam, Z. Ma, Y. Xue, J. Zhai, W. Chen, Z. Liu, P. Zhang, Y. Dong, and J. Tang, "GLM-130b: An open bilingual pre-trained model," in The Eleventh International Conference on Learning Representations, 2023.
[57] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, "Scaling laws for neural language models," arXiv preprint arXiv:2001.08361, 2020.
[58] LMSYS, "Lmsys org: Chatbot arena," 2023.
[59] OpenAI, "Openai: New and improved embedding model," 2022.
[60] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al., "Language models are unsupervised multitask learners,"</p>
<p>[61] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi, "Self-instruct: Aligning language model with self generated instructions," arXiv preprint arXiv:2212.10560, 2022.
[62] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto, "Stanford alpaca: An instruction-following llama model." https://github.com/tatsu-lab/ stanford_alpaca, 2023.
[63] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y. Tay, D. Zhou, Q. V. Le, B. Zoph, J. Wei, et al., "The flan collection: Designing data and methods for effective instruction tuning," arXiv preprint arXiv:2301.13688, 2023.
[64] Y. Xiao, Y. Jin, Y. Bai, Y. Wu, X. Yang, X. Luo, W. Yu, X. Zhao, Y. Liu, H. Chen, et al., "Large language models can be good privacy protection learners," arXiv preprint arXiv:2310.02469, 2023.</p>
<h1>A Case Study in Pilot Study</h1>
<p>In our pilot study, we utilize GPT-4 [20] to evaluate the correctness of the responses from LLaMA13B [3] on Natural Questions [21]. For each case, we provide the question, the groundtruth (labeled in the original dataset), the response from LLaMA-13B, the exact match result, the judgment from GPT4 , and the judgment from human annotators. First, we draw attention to a case where LLaMA-13B's response is correct, yet GPT-4 incorrectly identifies it as false.</p>
<p>Question: When were 2 dollar bills stopped being made?
Groundtruth: current denomination of U.S. currency
Answer-LLaMA-13B: 1966
Exact match: $\boldsymbol{X}$
GPT-4 judge: $\boldsymbol{X}$
Human judge: $\checkmark$
Answer-GPT-4: 2 dollar bills have not been officially discontinued, but they are printed in limited quantities and are less commonly used. $\boldsymbol{X}$</p>
<p>Conversely, there is a case in which LLaMA-13B's answer is incorrect, but GPT-4 erroneously marks it as accurate.</p>
<p>Question: What is the name of the first earthquake early warning system?
Groundtruth: 1991 or The Mexican Seismic Alert System
Answer-LLaMA-13B: ShakeAlert
Exact match: $\boldsymbol{X}$
GPT-4 judge: $\checkmark$
Human judge: $\boldsymbol{X}$
Answer-GPT-4: P-wave alarm system $\boldsymbol{X}$
In both cases, the misjudgments by GPT-4 stem from its inadequate understanding of the knowledge related to the original questions.</p>
<h2>B Example Prompts</h2>
<h2>B. 1 Question Generation</h2>
<p>You have been assigned the task of developing a set of 10 different questions that demonstrate your comprehensive understanding of a specific domain. Please strictly follow these 6 rules for the task:</p>
<ol>
<li>Your questions should exhibit a thorough understanding of the domain, and should encompass both breadth and depth, incorporating different question words, such as "what", "which", "when", "where", "how", "why", etc.</li>
<li>Make sure the first 3 questions ask about concise knowledge and can be answered in 20 words.</li>
<li>The last 7 more complicated questions can be answered in 100 words. Among them, the last 3 questions should be compound questions.</li>
<li>You need to generate the questions as DIVERSIFY as possible.</li>
<li>Ensure that you can confidently answer the questions you are proposing.</li>
<li>DO NOT add other words other than the question itself. Each question in one line, add the serial number ("1.", "2.") before each question.
domain: [Domain]</li>
</ol>
<h2>B. 2 Multi-round Question Generation</h2>
<p>You have been provided with a specific domain and a question-and-answer pair related to that domain. Your task is to generate a follow-up question that delves deeper into the topic of the given question. The proposed question should be based on the answer provided in the question-and-answer pair and should aim to test the author's knowledge of the underlying concepts of the answer he proposed. To accomplish this task, please adhere to the following guidelines:</p>
<ol>
<li>The proposed question should be closely related to the topic of the given question and should explore the same subject matter in greater detail.</li>
<li>You should be able to confidently answer the question you propose.</li>
<li>Please only return the following question as: follow question: [your proposed question].
Question: {Previous round question} Answer: {Previous round response}</li>
</ol>
<h1>B. 3 Peer-Examination Question Generation</h1>
<p>I want you to act as a question writer expert. Your objective is to write 5 really complex and difficult questions of a specific domain to make those famous AI systems (e.g., ChatGPT and GPT-4) a bit harder to handle.</p>
<ol>
<li>The 5 questions should be very complex and difficult, you can ask compound question.</li>
<li>Ensure that you can confidently answer the questions you are proposing.</li>
<li>DO NOT add other words other than the question itself. Each question in one line, add the serial number ("1.", "2.") before each question.
domain: {Domain}</li>
</ol>
<h2>B. 4 Likert Scale Scoring</h2>
<p>You are a fair assessment expert, and you will be given a set of question-answer pairs. Your task is to score the answers according to the following requirements:
a. You should score the answer based on your knowledge of the corresponding question. You can assume your own answer to the corresponding question is the ground truth for the question.
b. You should rate the answer on 5 metrics, for the first 4 metrics, assign a score between 1 and 3, with 3 being the highest:</p>
<ol>
<li>For accuracy, you will score whether the answer correctly answers the question.</li>
<li>For coherence, you will assess the structure and logic of the answer, and whether the answer is understandable by non-professionals.</li>
<li>For factuality, you will only evaluate whether the answer contains factual errors.</li>
<li>For comprehensive, you will determine if the answer covers multiple aspects of the question and provides a comprehensive response. For simple questions (when, which, where, etc), the plain answer itself suffices and should be rated 3.</li>
<li>Finally, you will provide an overall score between 1 and 5, with 5 being the highest.
You should only give the score, Format like: coherence: 3
DO NOT complete the answer!
Question: {Question} Answer: {Response}</li>
</ol>
<h2>B. 5 Pairwise Evaluation</h2>
<p>You are a fair assessment expert, and you will be given one question along with 2 different responses. Your task is to decide which response is better. You should take into consideration the accuracy, coherence, factuality, and comprehensiveness of the responses to reach a judgment. Only return: "Response 1" or "Response 2". You do not need to explain the reason.
Question: {Question}
Response 1: {Response 1}
Response 2: {Response 2}</p>
<h2>B. 6 ChatGPT Rewrite</h2>
<p>You are a good writer. Paraphrase the given paragraph using more eloquent language. Include all the points and details without introducing any additional knowledge. Try to make what you write the same length as the given paragraph.
Paragraph: {Original paragraph}</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 8: Percentage of full-mark answers on different domains (in the first level of the domain taxonomy) in LMExamQA. We take the 5-shot performance for models without SFT.</p>
<h1>C Experimental Details</h1>
<h2>C. 1 Metric Evaluation</h2>
<p>For metric evaluation, we randomly select 100 questions from LMExamQA, and randomly select 3 model-generated responses for each question, resulting in a total of 300 samples. We ask the human annotators to score the responses between 1-5 based on their overall quality. Additionally, for each question, we ask the annotators to rank the 3 respective model-generated responses. Here we provide the annotation instructions.</p>
<ol>
<li>Each column represents "question, GPT4 answer, GPT4 answer correctness, answer 1, answer 1 rating, answer 2 , answer 2 rating, answer 3 , answer 3 rating, answer ranking".</li>
<li>Scoring is based on five dimensions, namely accuracy (whether the question is answered correctly); coherence (whether the answer is (does it flow smoothly and is easy to understand); factuality (does it not contain factual errors); comprehensiveness (does it cover multiple aspects of the question and provide a comprehensive answer), these four dimensions are scored between 1 and 3 , with 1 being the lowest and 3 the highest; the last score is a total score based on the previous evaluation dimensions, between 1 and 5 , with 1 being the lowest and 5 the highest. 3. According to the above scoring rules, the last column should give the ranking of the three answers (excluding GPT4 answers) in terms of overall answer quality, in a format such as $2&gt;3&gt;1$, representing answer $2&gt;$ answer $3&gt;$ answer 1 . If there are two answers that are almost the same and hard to distinguish the better and worse, you can add " $=$ " to the ranking, for example, $2&gt;3=1$. But please use " $=$ " sparingly.</li>
<li>It is recommended to conduct a web search for each question to get the reference answer, and then evaluate it. It is not recommended to use the generative language models to get the reference answer. If you are really unable to evaluate on some questions (such as even using the network cannot help locate the relevant knowledge), please fill in "NULL".</li>
<li>Please refer to the first four examples for the evaluation baselines and annotation format.</li>
</ol>
<h1>C. 2 Benchmarking</h1>
<p>Single-round QA. We show the evaluation results on each domain in LMExamQA in Figure 8. We provide the 0 -shot and 5 -shot prompts for foundation models (without SFT) to reproduce our results. We first show the 0 -shot prompt for each model:</p>
<ul>
<li>BLOOMZ [54]:</li>
</ul>
<p>Question: {Question} Answer:</p>
<ul>
<li>Flan-UL2 [55]:</li>
</ul>
<p>Answer the question: ${$ Question $}$</p>
<ul>
<li>Flan-T5 [55]:</li>
</ul>
<p>Question: {Question} Answer:</p>
<ul>
<li>GLM-130B [56] (we stop its generation once encountered a line break):</li>
</ul>
<p>Answer this question:
Question: {Question}
Answer:</p>
<ul>
<li>LLaMA (13B and 65B) [3] (we stop its generation once encountered a line break):</li>
</ul>
<p>Answer this question:
Question: {Question}
Answer:</p>
<p>To ensure fairness, all models share the same 5-shot prompts:
Answer the following questions:
Question: Which common household pests are controlled by professional pest control services? Answer: Common household pests controlled by professional pest control services include cockroaches, ants, termites, rodents, bed bugs, spiders, and wasps.
Question: What are the key differences between assisted living and long-term care facilities?
Answer: Assisted living facilities provide help with daily activities, social interactions, and minor medical assistance, while long-term care facilities offer extensive medical care, nursing staff support, and assistance with daily tasks for residents with serious illnesses or disabilities. Question: What is the primary objective of drug control policies?
Answer: The primary objective of drug control policies is to reduce the demand, supply, and harmful consequences of illegal drugs in society.
Question: Why is it essential to consider the type of fabric used in sleepwear when making a purchase?
Answer: It is essential to consider the type of fabric used in sleepwear when making a purchase because it affects comfort, breathability, temperature regulation, and potential allergies or skin sensitivities.
Question: Which historical figure is most associated with the origin of Buddhism?
Answer: Siddhartha Gautama
Question: {Question}
Answer:
For the fine-tuned models, we directly enter the questions to get their answers. For all models, we set the maximum output length to 200 tokens, and the temperature to 0 .
Multi-round QA. We also notice some notable phenomena during our multi-round QA experiment. We list them in the following.
The knowledge pattern of foundation models is inherently different from humans. In contrast to humans, who possess localized knowledge (e.g., knowing a person often implies knowing their actions and social connections), foundation models lack such localization. We observe that the models' accuracy remains close to that of the first round, even though they provide correct answers to the initial round questions.</p>
<p>Potential deceptive results in QA capabilities for foundation models. Our results imply that relying solely on single-round performance may lead to deceptive results, as models may simply recall previously seen texts without demonstrating genuine understanding and analysis of the knowledge.</p>
<h1>C. 3 Peer-Examination</h1>
<p>When conducting pairwise comparison evaluations, it is crucial to consider the issue of consistency. Specifically, the result should not change if we reverse "response 1 " and "response 2 ".</p>
<p>In our test, the selected models: GPT-4 [20], Bard [53], Claude [52], and ChatGPT [2] all possess the capability to perform reliable and consistent comparison evaluations. In contrast, Vicuna-13B [39] cannot produce consistent comparison results. Hence, we employ Bard, Claude, and ChatGPT as examiners in peer-examination.</p>
<p>To mitigate the influence of the input sequence on the comparison evaluation process, for each comparison, we take the average of the results from the original sequence and the reversed sequence.</p>
<p>During the peer-review process, we need to be mindful of ChatGPT's tendency to forget (such forgetfulness is not observed for the other examiners), especially when the generated answers are too long. To tackle this, we set a maximum length threshold for responses. This limit helps make sure the comparison result from ChatGPT stays consistent. However, cutting off the answer at a certain point could remove important details or context. This could affect how accurate and high-quality the answer is. Hence we need to find a balance between limiting ChatGPT's forgetfulness and keeping the evaluation fair for every examinee model. In our experiment, to make sure the results we get from ChatGPT are reliable, we choose to slightly sacrifice fairness by setting a maximum length of 400.</p>
<p>More explanations to the scoring results in Table 4. When Claude [52] acts as the examiner, the low percentage of full-mark answers can be attributed to its strict evaluation criteria, particularly regarding the coherence dimension. This is probably because when Claude tends to provide answers containing links to related documents while exhibiting clear structures. To address the imbalance caused by Claude's strict criterion we introduce a weighted average score in addition to the arithmetic mean value. By normalizing the scores, we aim to create a more balanced comparison among the examiner models.</p>
<h2>D Case Study</h2>
<h2>D. 1 Single-Round QA</h2>
<h2>D.1.1 Knowledge Memorization Question Case</h2>
<p>Field: Beauty \&amp; Fitness $\rightarrow$ Cosmetic Procedures $\rightarrow$ Cosmetic Surgery
Question: What are the most common types of cosmetic surgery procedures?
Ground Truth: The most common types of cosmetic surgery procedures are breast augmentation, liposuction, rhinoplasty, eyelid surgery, and facelift.
Answer-ChatGPT: The most common types of cosmetic surgery procedures include:1. Breast augmentation
2. Liposuction
3. Rhinoplasty (nose job)
4. Eyelid surgery
5. Facelift
6. Tummy tuck
7. Breast lift
8. Brazilian butt lift
9. Mommy makeover (combination of procedures to restore pre-pregnancy body)
10. Botox injections.</p>
<p>Score: accuracy: 3 coherence: 3 factuality: 3 comprehensive: 3 overall: 5
Reason: Accuracy: 3 (The answer correctly lists common types of cosmetic surgery procedures)
Coherence: 3 (The answer is well-structured, easy to understand, and logically organized)
Factuality: 3 (The answer contains no factual errors)</p>
<p>Comprehensiveness: 3 (The answer provides a comprehensive list covering various types of cosmetic surgery procedures) Overall Score: 5 (The answer meets all the required criteria and provides a complete and clear response to the question)
Answer-Flan-UL2: Breast augmentation, eyelid surgery, and rhinoplasty are the most common cosmetic surgery procedures.
Score: accuracy: 3 coherence: 3 factuality: 3 comprehensive: 3 overall: 5
Reason: Accuracy: 3 - The answer correctly lists some of the most common cosmetic surgery procedures.
Coherence: 3 - The answer is well-structured, logical, and can be understood by nonprofessionals.
Factuality: 3 - The answer contains no factual errors.
Comprehensiveness: 2 - The answer mentions some common procedures but could have included a few more to be more comprehensive, such as liposuction or tummy tuck.
Overall Score: 4 - The answer is accurate, coherent, and factual but could have been slightly more comprehensive.
Answer-BLOOMZ: The most common types of cosmetic surgery procedures include Botox injections, liposuction, and plastic surgery.
Score: accuracy: 2 coherence: 3 factuality: 2 comprehensive: 2 overall: 3
Reason: Accuracy: 2 - The answer lists some common cosmetic procedures, but Botox injections are not a surgery and plastic surgery is a general term, not a specific procedure.
Coherence: 3 - The answer is well-structured, logical, and can be understood by nonprofessionals.
Factuality: 2 - The answer contains a factual error by including Botox injections as a surgery and using the general term "plastic surgery."
Comprehensiveness: 2 - The answer mentions some common procedures but could have included more specific and accurate examples, such as breast augmentation, eyelid surgery, or rhinoplasty. Overall Score: 3 - The answer is coherent but has some inaccuracies and could be more comprehensive.
Rank: ChatGPT &gt; Flan-UL2 &gt; BLOOMZ</p>
<h1>D.1.2 Knowledge Comprehension Question Case</h1>
<p>Field: News $\rightarrow$ Health News $\rightarrow$ Health Policy
Question: How do public health emergencies such as pandemics influence changes in health policies?
Ground Truth: Public health emergencies such as pandemics influence changes in health policies by highlighting gaps in existing systems, prompting the development of new strategies, and accelerating the implementation of necessary measures to prevent, control, and mitigate the impact of the emergency.
Answer-ChatGPT: Public health emergencies such as pandemics can have a significant impact on health policies. Here are some ways in which pandemics can influence changes in health policies:</p>
<ol>
<li>Increased funding for public health: Pandemics can highlight the importance of investing in public health infrastructure and preparedness. Governments may increase funding for public health agencies and research to better prepare for future outbreaks.</li>
<li>Changes in healthcare delivery: Pandemics can lead to changes in healthcare delivery, such as increased use of telemedicine and remote consultations. This can help to reduce the spread of infectious diseases and improve access to healthcare for vulnerable populations.</li>
<li>Development of new policies: Pandemics can lead to the development of new policies to address specific issues related to the outbreak. For example, policies may be developed to ensure that healthcare workers have access to personal protective equipment (PPE) or to mandate social distancing measures.</li>
<li>Increased focus on prevention: Pandemics can highlight the importance of prevention in healthcare. Governments may invest in public health campaigns to promote healthy behaviors and encourage vaccination.</li>
<li>International cooperation: Pandemics can lead to increased international cooperation and collaboration on health policies.</li>
</ol>
<p>Countries may work together to share information and resources to better respond to outbreaks and prevent future pandemics.
Overall, pandemics can have a significant impact on health policies, leading to changes in healthcare delivery, increased funding for public health, and the development of new policies to address specific issues related to the outbreak
Score: accuracy: 3 coherence: 3 factuality: 3 comprehensive: 3 overall: 5
Reason: Accuracy: 3 The answer correctly addresses how pandemics influence changes in health policies.
Coherence: 3 The answer is well-structured, logical, and easily understandable by nonprofessionals.
Factuality: 3 The answer does not contain any factual errors.
Comprehensive: 3 The answer covers multiple aspects of the question and provides a comprehensive response.
Overall Score: 5 The answer is accurate, coherent, factual, and comprehensive, making it deserving of the highest overall score.
Answer-Vicuna-13B: Public health emergencies such as pandemics can have a significant impact on health policies. They can lead to changes in policies related to disease surveillance, detection, and response, as well as changes in policies related to vaccination, public health preparedness, and healthcare delivery. Governments may also take steps to increase funding for public health programs and to coordinate with international organizations to address the crisis. Additionally, during public health emergencies, there may be changes to laws and regulations related to public health, such as quarantine and isolation measures, to help prevent the spread of disease.
Score: accuracy: 3 coherence: 3 factuality: 3 comprehensive: 3 overall: 5
Reason: Accuracy: 3 The answer correctly addresses the question by mentioning various ways in which health policies are influenced by public health emergencies such as pandemics.
Coherence: 3 The answer is well-structured and logically organized, explaining different aspects of health policies that could be affected. The answer is understandable by non-professionals. Factuality: 3 There are no factual errors in the answer. The various policy changes mentioned are all reasonable responses to public health emergencies.
Comprehensive: 3 The answer covers multiple aspects of health policy changes - surveillance, detection, response, vaccination, preparedness, healthcare delivery, funding, coordination with international organizations, and changes to laws and regulations.
Overall Score: 5 The answer is accurate, coherent, fact-based, and comprehensive, providing an excellent overview of how public health emergencies can impact health policies.
Answer-LLaMA-65B: Public health emergencies such as pandemics influence changes in health policies by highlighting the need for improved healthcare infrastructure, better disease surveillance, and more effective vaccines.
Score: accuracy: 3 coherence: 3 factuality: 3 comprehensive: 2 overall: 4
Reason: Accuracy: 3 The answer correctly addresses how public health emergencies influence changes in health policies.
Coherence: 3 The answer is well-structured, logical, and easily understandable by nonprofessionals.
Factuality: 3 The answer contains no factual errors.
Comprehensive: 2 The answer covers some aspects of the question but could provide more details on how health policies change in response to public health emergencies.
Overall Score: 4 The answer is accurate, coherent, and factual, but could be more comprehensive in addressing the question.
Answer-LLaMA-13B: Public health emergencies such as pandemics influence changes in health policies by increasing the demand for healthcare services and resources.
Score: accuracy: 2 coherence: 3 factuality: 3 comprehensive: 2 overall: 3
Reason: Accuracy: 2 The answer is partially correct, as it mentions the increased demand for healthcare services and resources, but it doesn't cover other ways in which health policies may change during a pandemic, such as implementing new prevention measures or modifying existing ones.
Coherence: 3 The answer is well-structured and logical. It is easily understandable by nonprofessionals.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ We limit the total number of questions due to the unavailability of API access from Bard and Claude.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
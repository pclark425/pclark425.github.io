<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-456 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-456</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-456</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-271516474</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.18723v1.pdf" target="_blank">LLASP: Fine-tuning Large Language Models for Answer Set Programming</a></p>
                <p><strong>Paper Abstract:</strong> Recently, Large Language Models (LLMs) have showcased their potential in various natural language processing tasks, including code generation. However, while significant progress has been made in adapting LLMs to generate code for several imperative programming languages and tasks, there remains a notable gap in their application to declarative formalisms, such as Answer Set Programming (ASP). In this paper, we move a step towards exploring the capabilities of LLMs for ASP code generation. First, we perform a systematic evaluation of several state-of-the-art LLMs. Despite their power in terms of number of parameters, training data and computational resources, empirical results demonstrate inadequate performances in generating correct ASP programs. Therefore, we propose LLASP, a fine-tuned lightweight model specifically trained to encode fundamental ASP program patterns. To this aim, we create an ad-hoc dataset covering a wide variety of fundamental problem specifications that can be encoded in ASP. Our experiments demonstrate that the quality of ASP programs generated by LLASP is remarkable. This holds true not only when compared to the non-fine-tuned counterpart but also when compared to the majority of eager LLM candidates, particularly from a semantic perspective. All the code and data used to perform the experiments are publicly available at https://anonymous.4open.science/r/LLASP-D86C/.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e456.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e456.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NL->ASP faithfulness gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch between natural language problem specifications and generated Answer Set Programming code</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systematic discrepancy observed between high-level natural language problem descriptions and the ASP programs produced by general-purpose LLMs: outputs can be syntactically valid but fail to implement the intended semantics of the specification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-to-ASP generation and evaluation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipeline where a natural-language problem description is used as prompt to an LLM to generate an ASP program, which is then validated by running the program together with instance facts through the Clingo solver to test syntactic parsing and semantic equivalence with a gold program.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>problem specification in natural language (prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>generated ASP program (text output from LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete/incorrect semantic translation (ambiguous/under-specified mapping)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>LLMs often produce ASP code that superficially matches syntactic patterns but does not capture the semantics required by the natural-language prompt (e.g., enforces additional constraints not requested, encodes different relations, or mis-uses predicate arities). This is a mismatch between the intended logic expressed in natural language and the logical consequences of the generated ASP program.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>code generation step (model output) and semantic validation stage of evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>automated semantic validation using the Clingo ASP solver: (i) parse check for syntactic validity; (ii) compute answer sets AS(P) of generated program P unioned with facts and compare AS(P) to AS(P*) of gold program P*.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Binary 'syntactic hit' (Clingo parsing success) and 'semantic hit' (set equality AS(P) == AS(P*)). Aggregated as accuracies reported in Tables 3, 5 and 6 (fractions / percentages over repeated samples and test sets). Repeated sampling (5 runs) used for pre-trained models and larger D_test (9000 prompts) for LLASP.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Substantial: many pre-trained LLMs achieve high syntactic accuracy but low semantic accuracy, causing generated programs to be unusable despite being parseable; e.g., several models have semantic accuracy near 0.0 (Table 3). The fine-tuned model (LLASP) raised semantic accuracy to 0.89 (Table 3) and average semantic accuracy to 0.89 over the extended eval (Table 5), showing mitigation potential.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>High in this study: Table 3 shows multiple pre-trained LLMs with syntactic accuracy 1.0 but much lower semantic accuracy (e.g., ChatGPT 3.5 syntactic 1.0 semantic 0.64; Gemma 7B syntactic 0.44 semantic 0.0; several LLaMa variants semantic 0.0). LLASP achieved aligned syntactic/semantic performance more often (0.89/0.89 in Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>LLMs are not trained specifically on ASP semantics and rely on surface patterns; natural language prompts can be ambiguous w.r.t. ASP constructs; training corpora lack structured declarative patterns; and models tend to capture syntactic templates without guaranteeing logical equivalence.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Task-specific supervised fine-tuning (LLASP) using a large template-derived dataset of matched (natural language prompt, gold ASP program) pairs (≈3.7M tuples) and explicit semantic validation during evaluation with Clingo. Proposal of new hybrid metrics and controlled training methods incorporating ASP syntactic guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective in this work: fine-tuning Gemma 2B to produce LLASP increased semantic accuracy from baseline Gemma 2B (0.0) to 0.89 (Table 3) and produced higher average semantic accuracy across tasks (Table 5: total avg semantic 0.89).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning (LLMs) applied to declarative programming / automated code generation (Answer Set Programming)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLASP: Fine-tuning Large Language Models for Answer Set Programming', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e456.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e456.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Syntax–Semantics mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Syntactic correctness without semantic correctness in generated ASP</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent phenomenon where generated ASP programs parse successfully but do not produce the same answer sets as the gold program, i.e., syntactic hits do not guarantee semantic hits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Generation + Clingo-based semantic validator</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The experimental setup that treats Clingo parsing as a syntactic check and answer-set equality as a semantic check, applied to outputs from many pre-trained LLMs and the fine-tuned LLASP model.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>problem description prompts (natural language)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>generated ASP programs (textual outputs from LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>ambiguous description mapped to syntactically-correct but semantically-wrong code (semantic misalignment)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Models frequently produced code that was syntactically valid (no Clingo parse errors) yet had different answer sets than the gold, e.g., imposing unintended uniqueness constraints or encoding a different relation; syntactic validity thus can be a poor proxy for functional correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>output generation and evaluation (semantic validation) stages</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Compare AS(P_generated ∪ facts) to AS(P_gold ∪ facts) using Clingo; if equal, semantic hit; otherwise semantic failure even if parse succeeds.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Report of syntactic and semantic accuracy metrics per model and per task (Tables 3, 4, 5, 6). Also anecdotal per-task counts (e.g., 'average score 0 out of 5' on Guessing Assignment for many models).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Leads to false confidence when relying only on parsing or surface checks; e.g., some models (Gemma 7B) had 0.44 syntactic accuracy and 0 semantic accuracy; several large models (ChatGPT, Copilot, Mistral 141B) had 1.0 syntactic but substantially lower semantic accuracy, undermining suitability for deployment without semantic verification.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed across multiple models and tasks; the paper reports that syntactic hits often do not imply semantic correctness except notably for LLASP where syntactic and semantic accuracies align more closely (Table 3, Table 5). Specific task-level failures (guessing assignment) affected most top-performing models except ChatGPT 3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Surface-pattern reproduction by LLMs and lack of explicit enforcement of logical equivalence during training; natural language descriptions underspecify ASP semantics unless carefully constrained.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use semantic validation (Clingo) in evaluation; supervised fine-tuning on many (prompt, gold ASP) pairs that capture ASP idioms; design training to reflect semantic constraints rather than only syntactic patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Fine-tuning (LLASP) produced high alignment between syntactic and semantic accuracy (e.g., LLASP syntactic 0.89 semantic 0.89 in Table 3; Table 5 average syntactic 0.97 semantic 0.89), indicating substantial mitigation though not perfect.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning for code generation; logic programming validation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLASP: Fine-tuning Large Language Models for Answer Set Programming', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e456.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e456.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Training dataset composition gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Suboptimal training dataset design for compositional/combined prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Limitations in how the fine-tuning dataset was composed (random combinations and limited prompt rephrasings) caused failures when generating programs for combined or rephrased prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLASP fine-tuning dataset and training pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Dataset D (≈3.7M tuples) created by instantiating templates for fundamental ASP tasks; later expanded trials included random combinations of templates and limited rephrasings to probe compositional generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>templated natural language prompts and combined prompts</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>gold ASP encodings paired with prompts (training data) and generated ASP code (model outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>coverage / generalization gap (insufficient diversity/strategic combinations)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Although LLASP learned many atomic patterns, random combination strategies for composing multiple atomic templates into complex prompts resulted in mixed success: some complex combinations generated correctly while other simple combinations failed; limited rephrasing coverage (10 rephrasings per prompt) did not yield robust generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training data design and fine-tuning stage (dataset composition)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical evaluation on combined prompts and rephrasings showing mixed/unsuccessful generation; qualitative analysis reported in Limitations section.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Qualitative/mixed-result assessment rather than precise numeric quantification in the paper; observed success/failure rates described as 'mixed' and 'unsuccessful' for rephrasings.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Limits the model's ability to generalize to compositional and paraphrased prompts; some otherwise simple combined tasks failed, indicating reduced robustness in realistic usage scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Reported as non-trivial during additional experiments; no exact frequency statistics provided, only qualitative statements (mixed results for combined prompts, unsuccessful rephrasing tests).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Training set generation using random combinations rather than strategically curated compositional examples; insufficient rephrasing diversity and targeted coverage of combined templates.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Proposed more careful and strategic dataset design for combinations and more comprehensive inclusion of paraphrases/rephrasings; controlled training methodologies incorporating ASP syntactic guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not quantitatively evaluated in the paper; authors report current random-combination approach produced inconsistent results and that a more strategic dataset should be devised.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>ML fine-tuning for code generation; dataset engineering</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLASP: Fine-tuning Large Language Models for Answer Set Programming', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e456.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e456.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Example: incorrect uniqueness constraint</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unintended uniqueness constraint introduced by model output</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Concrete instance where an LLM-generated ASP program encoded an unintended uniqueness constraint (disallowing multiple assignments of the same label) that was not requested by the natural language prompt, thus changing problem semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-generated ASP for 'Guessing Assignment' prompt</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Single-task evaluation for 'guessing assignments' where models are expected to generate an ASP encoding that allows every element to be assigned a label (with possible repetition across elements), tested using Clingo semantics equivalence.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>task prompt describing assignment of labels to elements</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>generated ASP rules</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>wrong algorithmic constraint (semantic over-constraining)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Example (from Mistral 7B/141B) shows program that enforces each label to be assigned to exactly one element via an added constraint ':-assigned(X1, L), assigned(X2, L), X1 != X2.' — this enforces label uniqueness across different elements although the prompt did not request it.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>logic of generated rules (model output) affecting constraints</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Manual inspection of generated ASP code and semantic evaluation showing failure to match gold answer sets for the Guessing Assignment task (averaging 0/5 semantic score for most models aside from ChatGPT 3.5).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Task-level semantic score aggregated over 5 runs; reported 'average score of 0 out of 5' for many models on that task (Table 4 and anecdotal discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Caused incorrect program semantics: generated programs would be inconsistent for instances where number of labels < number of elements or would change solution space relative to specification; rendered generated code functionally incorrect for the intended problem.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed in multiple top-performing models for this task (Mistral variants, other models); the paper explicitly notes most top models except ChatGPT 3.5 failed the Guessing Assignment semantically.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Model learned an undesired constraint pattern from training data or over-generalized from similar tasks; prompt underspecification led to ambiguous mapping to ASP constructs.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Fine-tuning on targeted templates that encode correct guessing patterns and using semantic validation during development; example-driven template instantiations covering intended behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>LLASP produced correct guessing-assignments more frequently; anecdotal evidence shows LLASP outperformed pre-trained models on semantic correctness for the task, though precise per-model per-task counts are in the reported tables.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>automated code generation for declarative languages (ASP)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLASP: Fine-tuning Large Language Models for Answer Set Programming', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e456.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e456.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Clingo-based validation protocol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Syntactic and semantic validation using the Clingo ASP solver</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic evaluation protocol that uses Clingo to (i) check syntactic validity (parse) of generated ASP code and (ii) compute answer sets to test semantic equivalence between generated and gold programs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Clingo validation stage within LLM-to-ASP evaluation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Generated ASP program y is combined with instance facts F_y* and executed with Clingo; parsing success indicates syntactic hit, while equality of answer sets AS(y ∪ F_y*) == AS(y* ∪ F_y*) indicates semantic hit.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>problem specification prompts paired with gold ASP encodings for validation</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>ASP programs (gold and generated)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>evaluation coverage gap (syntactic validation insufficient as proxy for correctness)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Using only syntactic parsing as a correctness criterion is insufficient because programs can parse but not implement intended semantics; the paper formalizes a two-step check relying on Clingo to distinguish these cases.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation metrics and validation procedures</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Automated: attempt to parse generated program with Clingo; if parsed, compute answer sets and compare to answer sets of gold program.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Syntactic and semantic hit indicators aggregated into accuracies across models and datasets (Tables 3, 4, 5, 6). Repeated sampling and a large D_test (9000 prompts) used to estimate robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Enabled precise quantification of the faithfulness gap and demonstrated that many models require semantic checks beyond parsing; allowed the authors to quantify improvements from LLASP training.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Applied across all experiments in the paper; the protocol exposed many syntactic-but-not-semantic cases across models and tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Naive evaluation approaches that stop at syntactic checks or human inspection fail to catch semantic divergences; lack of formal semantic validation in many code-generation evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Adopt the Clingo-based two-step validation as standard for ASP generation tasks and incorporate semantic checks into training/evaluation loops (and propose a new combined metric that considers both syntactic and semantic correctness).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>The protocol made the gaps visible and motivated effective fine-tuning (LLASP) that improved semantic alignment; the paper suggests further developing a composite metric for training/evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>evaluation methodology for code generation in logic programming</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLASP: Fine-tuning Large Language Models for Answer Set Programming', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Leveraging large language models to generate answer set programs <em>(Rating: 2)</em></li>
                <li>CNL2ASP: converting controlled natural language sentences into <em>(Rating: 2)</em></li>
                <li>NL2ASP <em>(Rating: 2)</em></li>
                <li>A dual-system model based on GPT-3 for semantic parsing and reasoning <em>(Rating: 1)</em></li>
                <li>Solving puzzles described in English by automated translation to answer set programming <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-456",
    "paper_id": "paper-271516474",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "NL-&gt;ASP faithfulness gap",
            "name_full": "Mismatch between natural language problem specifications and generated Answer Set Programming code",
            "brief_description": "Systematic discrepancy observed between high-level natural language problem descriptions and the ASP programs produced by general-purpose LLMs: outputs can be syntactically valid but fail to implement the intended semantics of the specification.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LLM-to-ASP generation and evaluation pipeline",
            "system_description": "Pipeline where a natural-language problem description is used as prompt to an LLM to generate an ASP program, which is then validated by running the program together with instance facts through the Clingo solver to test syntactic parsing and semantic equivalence with a gold program.",
            "nl_description_type": "problem specification in natural language (prompt)",
            "code_implementation_type": "generated ASP program (text output from LLM)",
            "gap_type": "incomplete/incorrect semantic translation (ambiguous/under-specified mapping)",
            "gap_description": "LLMs often produce ASP code that superficially matches syntactic patterns but does not capture the semantics required by the natural-language prompt (e.g., enforces additional constraints not requested, encodes different relations, or mis-uses predicate arities). This is a mismatch between the intended logic expressed in natural language and the logical consequences of the generated ASP program.",
            "gap_location": "code generation step (model output) and semantic validation stage of evaluation",
            "detection_method": "automated semantic validation using the Clingo ASP solver: (i) parse check for syntactic validity; (ii) compute answer sets AS(P) of generated program P unioned with facts and compare AS(P) to AS(P*) of gold program P*.",
            "measurement_method": "Binary 'syntactic hit' (Clingo parsing success) and 'semantic hit' (set equality AS(P) == AS(P*)). Aggregated as accuracies reported in Tables 3, 5 and 6 (fractions / percentages over repeated samples and test sets). Repeated sampling (5 runs) used for pre-trained models and larger D_test (9000 prompts) for LLASP.",
            "impact_on_results": "Substantial: many pre-trained LLMs achieve high syntactic accuracy but low semantic accuracy, causing generated programs to be unusable despite being parseable; e.g., several models have semantic accuracy near 0.0 (Table 3). The fine-tuned model (LLASP) raised semantic accuracy to 0.89 (Table 3) and average semantic accuracy to 0.89 over the extended eval (Table 5), showing mitigation potential.",
            "frequency_or_prevalence": "High in this study: Table 3 shows multiple pre-trained LLMs with syntactic accuracy 1.0 but much lower semantic accuracy (e.g., ChatGPT 3.5 syntactic 1.0 semantic 0.64; Gemma 7B syntactic 0.44 semantic 0.0; several LLaMa variants semantic 0.0). LLASP achieved aligned syntactic/semantic performance more often (0.89/0.89 in Table 3).",
            "root_cause": "LLMs are not trained specifically on ASP semantics and rely on surface patterns; natural language prompts can be ambiguous w.r.t. ASP constructs; training corpora lack structured declarative patterns; and models tend to capture syntactic templates without guaranteeing logical equivalence.",
            "mitigation_approach": "Task-specific supervised fine-tuning (LLASP) using a large template-derived dataset of matched (natural language prompt, gold ASP program) pairs (≈3.7M tuples) and explicit semantic validation during evaluation with Clingo. Proposal of new hybrid metrics and controlled training methods incorporating ASP syntactic guidance.",
            "mitigation_effectiveness": "Effective in this work: fine-tuning Gemma 2B to produce LLASP increased semantic accuracy from baseline Gemma 2B (0.0) to 0.89 (Table 3) and produced higher average semantic accuracy across tasks (Table 5: total avg semantic 0.89).",
            "domain_or_field": "machine learning (LLMs) applied to declarative programming / automated code generation (Answer Set Programming)",
            "reproducibility_impact": true,
            "uuid": "e456.0",
            "source_info": {
                "paper_title": "LLASP: Fine-tuning Large Language Models for Answer Set Programming",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Syntax–Semantics mismatch",
            "name_full": "Syntactic correctness without semantic correctness in generated ASP",
            "brief_description": "A recurrent phenomenon where generated ASP programs parse successfully but do not produce the same answer sets as the gold program, i.e., syntactic hits do not guarantee semantic hits.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Generation + Clingo-based semantic validator",
            "system_description": "The experimental setup that treats Clingo parsing as a syntactic check and answer-set equality as a semantic check, applied to outputs from many pre-trained LLMs and the fine-tuned LLASP model.",
            "nl_description_type": "problem description prompts (natural language)",
            "code_implementation_type": "generated ASP programs (textual outputs from LLMs)",
            "gap_type": "ambiguous description mapped to syntactically-correct but semantically-wrong code (semantic misalignment)",
            "gap_description": "Models frequently produced code that was syntactically valid (no Clingo parse errors) yet had different answer sets than the gold, e.g., imposing unintended uniqueness constraints or encoding a different relation; syntactic validity thus can be a poor proxy for functional correctness.",
            "gap_location": "output generation and evaluation (semantic validation) stages",
            "detection_method": "Compare AS(P_generated ∪ facts) to AS(P_gold ∪ facts) using Clingo; if equal, semantic hit; otherwise semantic failure even if parse succeeds.",
            "measurement_method": "Report of syntactic and semantic accuracy metrics per model and per task (Tables 3, 4, 5, 6). Also anecdotal per-task counts (e.g., 'average score 0 out of 5' on Guessing Assignment for many models).",
            "impact_on_results": "Leads to false confidence when relying only on parsing or surface checks; e.g., some models (Gemma 7B) had 0.44 syntactic accuracy and 0 semantic accuracy; several large models (ChatGPT, Copilot, Mistral 141B) had 1.0 syntactic but substantially lower semantic accuracy, undermining suitability for deployment without semantic verification.",
            "frequency_or_prevalence": "Observed across multiple models and tasks; the paper reports that syntactic hits often do not imply semantic correctness except notably for LLASP where syntactic and semantic accuracies align more closely (Table 3, Table 5). Specific task-level failures (guessing assignment) affected most top-performing models except ChatGPT 3.5.",
            "root_cause": "Surface-pattern reproduction by LLMs and lack of explicit enforcement of logical equivalence during training; natural language descriptions underspecify ASP semantics unless carefully constrained.",
            "mitigation_approach": "Use semantic validation (Clingo) in evaluation; supervised fine-tuning on many (prompt, gold ASP) pairs that capture ASP idioms; design training to reflect semantic constraints rather than only syntactic patterns.",
            "mitigation_effectiveness": "Fine-tuning (LLASP) produced high alignment between syntactic and semantic accuracy (e.g., LLASP syntactic 0.89 semantic 0.89 in Table 3; Table 5 average syntactic 0.97 semantic 0.89), indicating substantial mitigation though not perfect.",
            "domain_or_field": "machine learning for code generation; logic programming validation",
            "reproducibility_impact": true,
            "uuid": "e456.1",
            "source_info": {
                "paper_title": "LLASP: Fine-tuning Large Language Models for Answer Set Programming",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Training dataset composition gap",
            "name_full": "Suboptimal training dataset design for compositional/combined prompts",
            "brief_description": "Limitations in how the fine-tuning dataset was composed (random combinations and limited prompt rephrasings) caused failures when generating programs for combined or rephrased prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LLASP fine-tuning dataset and training pipeline",
            "system_description": "Dataset D (≈3.7M tuples) created by instantiating templates for fundamental ASP tasks; later expanded trials included random combinations of templates and limited rephrasings to probe compositional generalization.",
            "nl_description_type": "templated natural language prompts and combined prompts",
            "code_implementation_type": "gold ASP encodings paired with prompts (training data) and generated ASP code (model outputs)",
            "gap_type": "coverage / generalization gap (insufficient diversity/strategic combinations)",
            "gap_description": "Although LLASP learned many atomic patterns, random combination strategies for composing multiple atomic templates into complex prompts resulted in mixed success: some complex combinations generated correctly while other simple combinations failed; limited rephrasing coverage (10 rephrasings per prompt) did not yield robust generalization.",
            "gap_location": "training data design and fine-tuning stage (dataset composition)",
            "detection_method": "Empirical evaluation on combined prompts and rephrasings showing mixed/unsuccessful generation; qualitative analysis reported in Limitations section.",
            "measurement_method": "Qualitative/mixed-result assessment rather than precise numeric quantification in the paper; observed success/failure rates described as 'mixed' and 'unsuccessful' for rephrasings.",
            "impact_on_results": "Limits the model's ability to generalize to compositional and paraphrased prompts; some otherwise simple combined tasks failed, indicating reduced robustness in realistic usage scenarios.",
            "frequency_or_prevalence": "Reported as non-trivial during additional experiments; no exact frequency statistics provided, only qualitative statements (mixed results for combined prompts, unsuccessful rephrasing tests).",
            "root_cause": "Training set generation using random combinations rather than strategically curated compositional examples; insufficient rephrasing diversity and targeted coverage of combined templates.",
            "mitigation_approach": "Proposed more careful and strategic dataset design for combinations and more comprehensive inclusion of paraphrases/rephrasings; controlled training methodologies incorporating ASP syntactic guidance.",
            "mitigation_effectiveness": "Not quantitatively evaluated in the paper; authors report current random-combination approach produced inconsistent results and that a more strategic dataset should be devised.",
            "domain_or_field": "ML fine-tuning for code generation; dataset engineering",
            "reproducibility_impact": true,
            "uuid": "e456.2",
            "source_info": {
                "paper_title": "LLASP: Fine-tuning Large Language Models for Answer Set Programming",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Example: incorrect uniqueness constraint",
            "name_full": "Unintended uniqueness constraint introduced by model output",
            "brief_description": "Concrete instance where an LLM-generated ASP program encoded an unintended uniqueness constraint (disallowing multiple assignments of the same label) that was not requested by the natural language prompt, thus changing problem semantics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LLM-generated ASP for 'Guessing Assignment' prompt",
            "system_description": "Single-task evaluation for 'guessing assignments' where models are expected to generate an ASP encoding that allows every element to be assigned a label (with possible repetition across elements), tested using Clingo semantics equivalence.",
            "nl_description_type": "task prompt describing assignment of labels to elements",
            "code_implementation_type": "generated ASP rules",
            "gap_type": "wrong algorithmic constraint (semantic over-constraining)",
            "gap_description": "Example (from Mistral 7B/141B) shows program that enforces each label to be assigned to exactly one element via an added constraint ':-assigned(X1, L), assigned(X2, L), X1 != X2.' — this enforces label uniqueness across different elements although the prompt did not request it.",
            "gap_location": "logic of generated rules (model output) affecting constraints",
            "detection_method": "Manual inspection of generated ASP code and semantic evaluation showing failure to match gold answer sets for the Guessing Assignment task (averaging 0/5 semantic score for most models aside from ChatGPT 3.5).",
            "measurement_method": "Task-level semantic score aggregated over 5 runs; reported 'average score of 0 out of 5' for many models on that task (Table 4 and anecdotal discussion).",
            "impact_on_results": "Caused incorrect program semantics: generated programs would be inconsistent for instances where number of labels &lt; number of elements or would change solution space relative to specification; rendered generated code functionally incorrect for the intended problem.",
            "frequency_or_prevalence": "Observed in multiple top-performing models for this task (Mistral variants, other models); the paper explicitly notes most top models except ChatGPT 3.5 failed the Guessing Assignment semantically.",
            "root_cause": "Model learned an undesired constraint pattern from training data or over-generalized from similar tasks; prompt underspecification led to ambiguous mapping to ASP constructs.",
            "mitigation_approach": "Fine-tuning on targeted templates that encode correct guessing patterns and using semantic validation during development; example-driven template instantiations covering intended behaviors.",
            "mitigation_effectiveness": "LLASP produced correct guessing-assignments more frequently; anecdotal evidence shows LLASP outperformed pre-trained models on semantic correctness for the task, though precise per-model per-task counts are in the reported tables.",
            "domain_or_field": "automated code generation for declarative languages (ASP)",
            "reproducibility_impact": true,
            "uuid": "e456.3",
            "source_info": {
                "paper_title": "LLASP: Fine-tuning Large Language Models for Answer Set Programming",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Clingo-based validation protocol",
            "name_full": "Syntactic and semantic validation using the Clingo ASP solver",
            "brief_description": "An automatic evaluation protocol that uses Clingo to (i) check syntactic validity (parse) of generated ASP code and (ii) compute answer sets to test semantic equivalence between generated and gold programs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Clingo validation stage within LLM-to-ASP evaluation pipeline",
            "system_description": "Generated ASP program y is combined with instance facts F_y* and executed with Clingo; parsing success indicates syntactic hit, while equality of answer sets AS(y ∪ F_y*) == AS(y* ∪ F_y*) indicates semantic hit.",
            "nl_description_type": "problem specification prompts paired with gold ASP encodings for validation",
            "code_implementation_type": "ASP programs (gold and generated)",
            "gap_type": "evaluation coverage gap (syntactic validation insufficient as proxy for correctness)",
            "gap_description": "Using only syntactic parsing as a correctness criterion is insufficient because programs can parse but not implement intended semantics; the paper formalizes a two-step check relying on Clingo to distinguish these cases.",
            "gap_location": "evaluation metrics and validation procedures",
            "detection_method": "Automated: attempt to parse generated program with Clingo; if parsed, compute answer sets and compare to answer sets of gold program.",
            "measurement_method": "Syntactic and semantic hit indicators aggregated into accuracies across models and datasets (Tables 3, 4, 5, 6). Repeated sampling and a large D_test (9000 prompts) used to estimate robustness.",
            "impact_on_results": "Enabled precise quantification of the faithfulness gap and demonstrated that many models require semantic checks beyond parsing; allowed the authors to quantify improvements from LLASP training.",
            "frequency_or_prevalence": "Applied across all experiments in the paper; the protocol exposed many syntactic-but-not-semantic cases across models and tasks.",
            "root_cause": "Naive evaluation approaches that stop at syntactic checks or human inspection fail to catch semantic divergences; lack of formal semantic validation in many code-generation evaluations.",
            "mitigation_approach": "Adopt the Clingo-based two-step validation as standard for ASP generation tasks and incorporate semantic checks into training/evaluation loops (and propose a new combined metric that considers both syntactic and semantic correctness).",
            "mitigation_effectiveness": "The protocol made the gaps visible and motivated effective fine-tuning (LLASP) that improved semantic alignment; the paper suggests further developing a composite metric for training/evaluation.",
            "domain_or_field": "evaluation methodology for code generation in logic programming",
            "reproducibility_impact": true,
            "uuid": "e456.4",
            "source_info": {
                "paper_title": "LLASP: Fine-tuning Large Language Models for Answer Set Programming",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Leveraging large language models to generate answer set programs",
            "rating": 2,
            "sanitized_title": "leveraging_large_language_models_to_generate_answer_set_programs"
        },
        {
            "paper_title": "CNL2ASP: converting controlled natural language sentences into",
            "rating": 2,
            "sanitized_title": "cnl2asp_converting_controlled_natural_language_sentences_into"
        },
        {
            "paper_title": "NL2ASP",
            "rating": 2
        },
        {
            "paper_title": "A dual-system model based on GPT-3 for semantic parsing and reasoning",
            "rating": 1,
            "sanitized_title": "a_dualsystem_model_based_on_gpt3_for_semantic_parsing_and_reasoning"
        },
        {
            "paper_title": "Solving puzzles described in English by automated translation to answer set programming",
            "rating": 1,
            "sanitized_title": "solving_puzzles_described_in_english_by_automated_translation_to_answer_set_programming"
        }
    ],
    "cost": 0.014288499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LLASP: Fine-tuning Large Language Models for Answer Set Programming
26 Jul 2024</p>
<p>Erica Coppolillo erica.coppolillo@unical.it 
University of Calabria
Via Bucci 28cRendeItaly</p>
<p>ICAR-CNR
Via Bucci 8/9CRende-Italy</p>
<p>Francesco Calimeri francesco.calimeri@unical.it 
University of Calabria
Via Bucci 28cRendeItaly</p>
<p>DLVSystem Srl
Viale della Resistenza 19CRendeItaly</p>
<p>Giuseppe Manco giuseppe.manco@icar.cnr.it 
ICAR-CNR
Via Bucci 8/9CRende-Italy</p>
<p>Simona Perri simona.perri@unical.it 
University of Calabria
Via Bucci 28cRendeItaly</p>
<p>Francesco Ricca francesco.ricca@unical.it 
University of Calabria
Via Bucci 28cRendeItaly</p>
<p>LLASP: Fine-tuning Large Language Models for Answer Set Programming
26 Jul 2024FCFE4B2FBCF9339E3F18F0939DF29E1EarXiv:2407.18723v1[cs.LG]
Recently, Large Language Models (LLMs) have showcased their potential in various natural language processing tasks, including code generation.However, while significant progress has been made in adapting LLMs to generate code for several imperative programming languages and tasks, there remains a notable gap in their application to declarative formalisms, such as Answer Set Programming (ASP).In this paper, we move a step towards exploring the capabilities of LLMs for ASP code generation.First, we perform a systematic evaluation of several state-of-the-art LLMs.Despite their power in terms of number of parameters, training data and computational resources, empirical results demonstrate inadequate performances in generating correct ASP programs.Therefore, we propose LLASP, a finetuned lightweight model specifically trained to encode fundamental ASP program patterns.To this aim, we create an ad-hoc dataset covering a wide variety of fundamental problem specifications that can be encoded in ASP.Our experiments demonstrate that the quality of ASP programs generated by LLASP is remarkable.This holds true not only when compared to the non-fine-tuned counterpart but also when compared to the majority of eager LLM candidates, particularly from a semantic perspective.All the code and data used to perform the experiments are publicly available: https://anonymous.4open.science/r/LLASP-D86C/.</p>
<p>Introduction</p>
<p>Answer Set Programming (ASP) (Brewka, Eiter, and Truszczynski 2011;Lifschitz 2019) is a pivotal tool for knowledge representation and reasoning, offering a range of benefits that contribute to its significance in various domains.One notable advantage is ASP capacity to handle knowledge in a concise and intuitive manner, allowing users to express complex problems with relative ease (Baral 2010).ASP provides a declarative approach for expressing knowledge and solving combinatorial optimization, planning, and reasoning tasks (Gebser et al. 2012).Its nonmonotonic nature allows for the representation of incomplete information and default reasoning, essential for capturing real-world scenarios.Moreover, ASP expressiveness enables the integration of various types of knowledge, including rules, constraints, and preferences, facilitating flexible problem-solving (Lifschitz 2019).</p>
<p>This formalism is typically regarded as a specialized tool utilized by domain experts and knowledge engineers familiar with specific problem domains.These individuals leverage ASP as a declarative language to articulate problem constraints, rules, and preferences succinctly and intuitively.However, despite the language simplicity and conciseness, coding in ASP can be an overwhelming obstacle for non-expert users, due to lack of familiarity with declarative paradigms and their intrinsic semantics, as well as potential complexity of application domains.Under this perspective, it is important to develop tools that enhance the efficiency and automation of creating ASP programs.These tools would minimize the disparity between natural language specifications and the corresponding ASP source code, facilitating smoother development processes.(Erdem and Yeniterzi 2009;Fang and Tompits 2017;Schwitter 2018;Caruso et al. 2024).</p>
<p>In this respect, the recent advancements in Artificial Intelligence and Machine/Deep Learning have led to the emergence of Large Language Models (LLMs) as indispensable assets across various natural language applications and tasks (Raiaan et al. 2024;Minaee et al. 2024).Notably, in the domain of automatic text-to-code translation, LLMs are employed to generate programs from natural language inputs.These models are trained on extensive datasets encompassing code repositories, technical forums, coding platforms, documentation, and relevant web data associated with programming.Such comprehensive training equips LLMs with the ability to grasp the nuances of code context, thereby enhancing the accuracy of contextually relevant code generation.</p>
<p>Indeed, a natural prosecution in this context is the investigation of LLMs-based approaches aimed at automating code generation within the context of declarative programming.This research challenge aims at narrowing the gap between natural language specifications and ASP source code, and aligns with the broader trend of leveraging language models for code automation.Further, leveraging LLMs for automating ASP program generation offers several advantages, which include: better semantic specification through high-level natural language descriptions of their problem domains; contextual awareness and interoperability, which simplifies the process of specifying ASP programs, making it accessible to a wider range of users without deep programming expertise; and enhanced efficiency and productivity.Surprisingly, the current literature exhibits a significant discrepancy in this respect, even if some initial attempts have been made (Ishay, Yang, and Lee 2023;Borroto, Kareem, and Ricca 2024).</p>
<p>In an effort to rectify this gap, the present work proposes a systematic study on the capabilities of currently available LLMs to automate the generation of ASP programs.In our study, we identify some structural properties (classes) of ASP programs specifically tailored to fundamental tasks, and correlate them with the generation capabilities of the currently available LLM architectures.Inspired by such a characterization, we further devise a methodology for obtaining more accurate ASP encodings.Our approach consists in the construction of a custom training dataset, specifically tailored for the identified relevant tasks, and in finetuning an LLM instance that can be successfully exploited for ASP coding.Through an extensive experimentation, we show that even a fine-tuned lightweight base-model is more effective than heavy-sized LLMs.As a result, the proposed methodology offers a pathway towards fine-tuned models ready for practical deployment.</p>
<p>Our contributions can be summarized as follows.</p>
<p>• To the best of our knowledge, we are the first to extensively compare state-of-the-art LLMs in terms of ASP code generation.Our findings suggest that, despite their potential, LLMs still underperform in terms of syntactic and semantic program correctness.</p>
<p>• We show that a tailored training strategy, even if applied on a lightweight LLM, can outperform state-of-the art large-size models.To address this, we curated a comprehensive dataset and used it to produce LLASP, the finetuned version of a Gemma 2B base-model specifically trained to capture ASP fundamental patterns.</p>
<p>• Via a comprehensive experimental evaluation, we prove that LLASP generate ASP programs that are highly valuable in terms of both syntactic and semantic accuracy, overcoming significantly larger and more powerful LLMs especially under a semantic perspective.</p>
<p>• Finally, we provide a discussion on the current limitations and potential directions for extensions of our proposed approach, inspired by the results of additional experiments.</p>
<p>The rest of the paper is structured as follows.Section 2 discusses the state of the art in automated code generation and the drawbacks of current approaches focused on ASP.In Section 3, we introduce the basic concepts, and in Section 4 we propose our methodology and design patterns for the evaluation and fine-tuning of the model.Section 5 reports the experiments and discusses our findings.Finally, in Section 6 we set some pointers for future research.</p>
<p>Related Work</p>
<p>The advantages of automating code synthesis are wellrecognized in the literature (Ernst and Bavota 2022;Kalliamvakou 2022;Peng et al. 2023;Dakhel et al. 2023), and nearly all mainstream programming languages are nowadays supported by automated program composition tools (Chen, Tworek, and et al. 2021).In this area, Large Language Models (LLMs) play a central role, and their performance has been extensively compared in the literature (Xu et al. 2022;Wang et al. 2023).The positive impact of fine-tuning models for code generation of imperative programming is also established (Ma et al. 2024).</p>
<p>In the context of declarative programming, a widely recognized research objective is to create tools that streamline and automate the development of Answer Set Programming (ASP) programs.The goal is to bridge the gap between natural language specifications and ASP source code (Erdem and Yeniterzi 2009;Fang and Tompits 2017;Schwitter 2018;Caruso et al. 2024).Initial proposals focused on automating the resolution of logic puzzles presented in simplified English by translating their descriptions into ASP (Baral and Dzifcak 2011), employing λcalculus and probabilistic combinatorial categorical grammars.Later, several efforts have been spent in the development of Controlled Natural Languages (CNLs) (Kuhn 2014) for ASP programs.CNLs represent subsets of full natural languages, featuring restricted grammar and vocabulary.Among them, the BIOQUERYCNL (Erdem and Yeniterzi 2009) defines the grammatical structure of a CNL and algorithm for transpling queries into ASP; Fang et al. introduced a CNL approach for ASP that leverages LANA annotations, that was implemented in the SeaLion IDE (Fang and Tompits 2017).In 2018, Schwitter developed a CNL called PENG ASP for specifying and verbalizing answer set programs (Schwitter 2018).More recently, Dodaro et al. introduced CNL2ASP, an extensive publicly-available tool for converting controlled natural language into ASP programs (Caruso et al. 2024).On the one hand, the CNLs provide the programmer with a language that is more similar to the natural one, thus greatly reducing the barrier of coding in ASP code; on the other hand, CNLs still have their own syntactic constraints that do not free completely the user from structuring the sentence according to a formal syntax.</p>
<p>Even powerful language tools such as LLMs have been exploited in conjunction with the ASP formalism in several ways.(Nye et al. 2021) introduced a dual-system model based on GPT-3, comprising neural System 1 and logical System 2, which generates semantic parsers from natural language sentences and integrates them with reasoning modules.In a similar vein, (Yang, Ishay, and Lee 2023) proposed that LLMs like GPT-3 can function as few-shot semantic parsers, transforming natural language into logical forms for ASP without necessitating distinct retraining for diverse question-answering tasks.(Ishay, Yang, and Lee 2023) utilized LLMs with prompt engineering to obtain ASP solutions for logic puzzles, leveraging the logic puzzle dataset from (Mitra and Baral 2016).Further, the effective utilization of Large Language Models in combination with ASP to perform a number of natural language understanding tasks has been explored and implemented in the STAR framework (Rajasekharan et al. 2023).</p>
<p>Despite the growing interest in this area, however, a notable gap still persists in the literature regarding the adoption of LLMs for code synthesis for declarative programming languages, such as ASP.In 2024, Borroto et al. moved the first steps in this regard.Their method, implemented in the NL2ASP tool, constructs ASP programs from natural language specifications through a two-step architecture (Borroto, Kareem, and Ricca 2024).Indeed, NL2ASP first applies neural machine translation (Stahlberg 2020) to transform natural language into the statements of the CNL by Dodaro et al.; then, in the second step, the produced sentences are used to generate ASP code by means of the CNL2ASP tool.NL2ASP has been implemented with two Transformerbased models for NMT tasks, i.e., T5-small, and Bart-base, and demonstrated promising performance.</p>
<p>Notably, there are some key differences between the approach proposed in this paper and the cited proposal.First, NL2ASP currently focuses on generating ASP programs for solving graph-related problem specifications, whereas the approach presented in this paper does not focus on a specific domain.Rather, it targets the generation of ASP programs suitable for solving general problems, starting from some fundamental specific ASP patterns, as we will show later.Another important difference is that our approach does not rely on an intermediate format: rather, it aims at producing ASP code directly from natural language specifications.Finally, it is worth observing that, even though low performance of LLMs in generating ASP code was already preliminarily observed (Borroto, Kareem, and Ricca 2024), a systematic and exhaustive empirical analysis assessing this behaviour was still missing, and is provided in this paper.</p>
<p>Preliminaries</p>
<p>Large Language Models.A Large Language Model L can be formally described as a function f , which stochastically maps input sequences of tokens x = [x 1 , x 2 , ..., x n ] to an output sequence y = [y 1 , y 2 , ..., y m ], where n represents the length of the input sequence and m represents the length of the output sequence.The model defines P L (y|x), i.e., the probability distribution of y given x, capturing complex patterns and relationships in natural language.Since x ∈ V * and y ∈ W * , where V , W represent vocabularies of tokens, both x and y should be compliant with respect to some grammar.The f function is finally defined based on a sampling of y from P L (•|x).</p>
<p>The above probability is computed by relying on the Transformer Encoder-Decoder architecture introduced in (Vaswani et al. 2017).In more details, the input sequence of tokens is first converted into dense vector representations, which encode context similarity into geometrical closeness and also embed positional information.These embeddings then feed the Transformer layers.The Encoder consists of multiple identical layers, each with selfattention to weigh word importance, and a fully connected feedforward network for independent position transformations.Similarly, Decoder layers feature self-attention with masking, Encoder-Decoder Attention for input focus during token generation, and position-wise feedforward networks.Finally, the output of the Transformer Decoder is passed through a linear layer followed by a softmax activation function, which generates a probability distribution over the vocabulary of possible output tokens.</p>
<p>The latter implements the function P L (y i |x, y :i−1 ) upon which the sampling process can be devised as a sequence of steps.Here, y :i−1 represents the prefix of y up to position (i − 1).Several architectural variants have been proposed (e.g., Causal-Decoder, Prefix Decoder, Autoregressive, Mixture Variants (Naveed et al. 2024;Raiaan et al. 2024)).The underlying architecture of the LLM is not deeply investigated in this paper, since it is orthogonal to our study.</p>
<p>The training of L is usually accomplished in steps.The first step consists in a pre-training task which is based on next-word prediction.In practice, given x and a partial response y :i−1 , the task is to learn to correctly predict y i .The next phase is called supervised fine-tuning (SFT), and it consists in refining L to predict the whole y given x, on a large corpora D = (x (1) , y (1) ), . . ., (x (N ) , y (N ) ) , where N is the number of training pairs.The refinement is based on the loss ℓ x (i) , P L y (i) |x (i) , usually defined in terms of cross-entropy:
ℓ y (i) , P L y (i) |x (i) = j y (i) j log P L y (i) j |x (i) , y (i) :j−1
In the following, we shall refer to this formulation of the loss, even though alternative formulations are possible.</p>
<p>Answer Set Programming.ASP is a powerful declarative formalism for Knowledge Representation and Reasoning that gained increasing interest for its high expressive power, and the availability of solid and effective implementations (Gebser et al. 2018).It is fully declarative (i.e., the ordering of literals and rules is immaterial), and the encoding of a large variety of problems is simple and concise.</p>
<p>In the following, we briefly recall the syntax of the language, focusing on the aspects relevant to this work, and provide an intuitive semantics, revisiting the concept of answer sets that will play a crucial role in the validation process outlined in Section 5.2.For further details and complete references to advanced ASP features, the reader may refer to the vast literature (Brewka, Eiter, and Truszczynski 2011;Eiter, Ianni, and Krennwallner 2009;Calimeri et al. 2016).</p>
<p>A term is either a constant or a variable.An atom is an expression p(t 1 , . . ., t n ), where p is a predicate of arity n and t 1 , . . ., t n are terms.A program P is a finite set of rules, constructs like Head :-Body where Head is a disjunction of atoms and Body is a conjunction of literals; more formally, a rule is of the form
α 1 | • • • |α k : − β 1 , . . . , β n , not β n+1 , . . . , not β m . where m ≥ 0, k ≥ 0; α 1 , • • • , α k and β 1 , . . . , β m are atoms.
A rule is interpreted according to common sense principles: roughly, its intuitive semantics corresponds to an implication.When a predicate p occurs in the head of a rule r, we say that r defines p. Rules featuring an atomic formula in the head and an empty body are used to represent information known to be certainly true and indeed called facts.A rule with empty head is called (strong) constraint, and intuitively expresses conditions that must be satisfied; basically, it is forbidden that all the literals in the body of a constraint are true.ASP also supports weak constraints, i.e, special rules with empty heads which allow to express preferences, and hence to deal with optimization problems.A weak constraint of the form
:∼ l 1 , • • • , l n . [w@l, t 1 , ..., t m ]
associates literals l 1 , • • • , l n with a weight w, a level l, and additional terms t 1 , • • • , t m for m ≥ 0. Intuitively, weak constraints allow to express conditions that should be satisfied, but not necessarily have to be; if a weak constraint is violated (i.e., its body is true), then the weight w ("cost") is paid at priority level l.Finally, a program (a rule, an atom) that does not contain variables is said to be ground.Models are defined over ground programs: a model for a program P is a subset of all possible ground atoms that satisfies all rules in P .The semantics of ASP programs is given in terms of special models called Answer Sets: according to this semantics, an ASP program may have several alternative answer sets (but possibly none), each corresponding to a possible view of the world (Gelfond and Lifschitz 1991;Brewka, Eiter, and Truszczynski 2011).In ASP, a computational problem is typically solved by modeling it via a program consisting of a collection of rules along with a set of facts representing the instance at hand; then, solutions are found by computing the intended answer sets, according to the so-called answer set semantics.Answer sets correspond one-to-one to the solutions of the given instance of the modeled problem: if a program has no answer sets, the corresponding instance has no solutions.If weak constraints are present, optimal answer sets are those minimizing the sum of weights of the violated weak constraints in the highest priority level.Among these, the optimal sets are those that also minimize the sum of weights of the violated weak constraints at the next lower level, and so forth.</p>
<p>Methodology and Knowledge Design</p>
<p>In the present work, we address a twofold objective.First, we focus on pre-trained general-purpose Large Language Models, not specifically tailored for ASP generation.Thus, given an LLM L and a natural language specification x of a problem, we aim at evaluating the robustness of L in sampling y ∼ P L (•|x), such that y is an ASP encoding compliant to x.Our second objective revolves around exploring how fine-tuning impacts the quality of generation, particularly when dealing with small-sized models.For a comprehensive evaluation, we need to consider various aspects: • Dataset diversity and complexity.We need to assemble a dataset encompassing a wide range of problem descriptions (x) covering different domains, complexities, and lengths.This diversity ensures that the evaluation captures L's performance across various contexts and complexity levels, thus helping in discerning L's efficacy in handling various ASP encoding challenges.• ASP compliance and robustness.Since each problem description x will be paired with a corresponding ASP encoding y ∼ P L (•|x), we need to verify whether y adhere to the ASP syntax, as well as its fidelity to the original problem description x.Metrics such as syntax accuracy, semantic coherence, and adherence to problem constraints should be considered.• Impact of model size.We need to investigate how the size and architecture of the L influence its ability to generate accurate ASP encodings.This analysis should involve comparing the performance of different L variants (in particular, smaller vs. larger models) and assessing any trade-offs between model size and encoding quality.</p>
<p>• Generalizability assessment.We need to examine whether L, trained on a diverse set of tasks, can generalize well to new problem domains not encountered during training.This helps devise the extent to which L's capabilities extend beyond its training data and whether it can effectively tackle novel ASP encoding tasks.</p>
<p>We concentrate on the first aspect for now, addressing the others in the subsequent section.Here, we need to define an ad-hoc approach for tuning L towards the production of ASP programs.In this regard, we take inspiration from the typical learning process experienced by humans while trained on the task of encoding problems into ASP programs.In particular, the process is based on modeling simple patterns encoding basic pieces of knowledge.For instance, how to represent with ASP a Cartesian Product, the Join between the extensions of two predicates, simple guesses, conditions on specific values, transitive closures, and so on.In principle, more complex patterns can be devised in a compositional way, starting from these basic building blocks to produce larger sets of rules that, in turn encode more complex knowledge.This approach to ASP generation is in general effective: the declarative nature of the formalism makes the order between rules in a program, and the order of literals in the bodies, immaterial; thus, in many cases, a program representing the desired solution can be composed by just adding subprograms representing additional knowledge.</p>
<p>According to this idea, we defined a template set T = (P, A), where P is a set of descriptions of basic tasks expressed in natural language and A is a set of text snippets consisting of corresponding (gold) ASP programs that encode them.In practice, T is partitioned into {T C 1 , T C 2 , ..., T C k }, where C i , represents the i-th task type and k is the total number of modeled task types.For instance, if C 1 refers to a transitive closure task, T C 1 consists of pairs (P C 1 , A C 1 ) featuring a description in natural language and one among the corresponding suitable gold ASP encodings, respectively.Each template (and its gold ASP counterpart) exhibits some placeholders to be instantiated.Such placeholders represent predicates, labels and values within the ASP encoding.</p>
<p>Below, we provide a brief overview of the fundamental tasks we examined.For each task, we offer an informal description, the template utilized, and an example of instantiated prompt along with its corresponding ASP encoding.Guessing Assignments.We started with considering a typical task where all elements of a given set must be assigned to a unique label picked from a fixed set.A common way for representing this in ASP is to use disjunction in order to express a so-called "guess".A possible pair associating a prompt with a suitable ASP encoding is reported next.Encoding: assign(X,"moscow") | assign(X,"rome") | assign(X,"dubai") :-city(X).</p>
<p>Expressing Constraints.Another typical task is to express conditions that must be fulfilled.Within ASP, this is commonly expressed by mean of (classical/strong) constraints, possibly associated with auxiliary rules if needed.A possible pair associating a prompt asking for preventing a specific assignment with a suitable ASP encoding is reported next.Encoding: combination(X,Y) :-city(X), airport(Y).</p>
<p>Joins.Joins over the elements of two different sets according to specific matching criteria over features can be typically defined in ASP via rules featuring proper body literals having a variable in common.A possible pair associating a prompt with a suitable ASP encoding is the following.Prompt: Write an ASP program for the following problem.Consider predicate "owner" having fields "ID","surname", "name","restaurantID", and the predicate "restaurant" having fields "ID","description".Define a predicate "owner_restaurant" that associates to each owner the description of restaurant.</p>
<p>Encoding: owner_restaurant(X,Z) :-owner(X,<em>,</em>,Y), restaurant(Y,Z).</p>
<p>Transitive Closure.Transitive closure is a fundamental tool for defining the structure of relationships in a variety of contexts as it catches not only direct relationships among elements, but also those that are indirectly resulting from a chain of relations.Expressing this with ASP in general requires to use more than a single rule.A possible pair associating a prompt describing a transitive closure with a suitable ASP encoding is reported next; in this case, the encoding make use of two rules: one for defining the direct relations, and another (relying on recursion) for catching the indirect ones.Filtering.When encoding ASP programs, it is frequently necessary to apply filters to the extensions of certain predicates based on specific requirements.We report next some common types of filtering criteria.</p>
<p>Filtering by values.The first type of filtering criteria consists in selecting the portion of the extension of a predicate that match with a specific value.A possible pair associating a prompt with a suitable ASP encoding is reported next.Encoding: select(X) :-color(X,"purple").</p>
<p>Filtering by negative.Another type of filtering criteria consists in excluding the portion of the extension of a predicate that matches with a given condition.This could consist, for instance, of a difference (subtraction) between two predicate extensions, but also the negation of compound conditions.A possible pair associating a prompt asking for filtering a predicate according to a negative condition defined over a filtered portion of a different predicate with a suitable ASP encoding is reported next.Encoding: select(X) :-vehicle(X), not moto(X,"kawasaki").</p>
<p>Filtering by numeric comparisons.Filtering portion of tables according to the results of comparisons between terms is another typical task.A possible pair associating a prompt asking for filtering a predicate according to a numeric comparison with a suitable ASP encoding is reported next.Encoding: select(X) :-size(X,C), C&gt;=5.</p>
<p>This is a small yet rich set of fundamental pieces of knowledge that an ASP user can draw upon and combine so to build more complex programs for solving different problems.As an example, think of many combinatorial problems, that basically consist in selecting or associating elements from a given collection in ways that comply with a given set of constraints; suitable programs for such problems are easily obtainable by combining guessing assignments and expressing constraints, possibly relying on the other types of tasks introduced above.If also optimization plays a role, suitable solutions can be obtained by additionally expressing preferences.</p>
<p>Experiments</p>
<p>In this section, we design a suite of experiments and evaluate their results.For the prompts described in the previous section, we aim at evaluating the following.• RQ1: To what extent state-of-the-art pre-trained LLMs are actually capable of encoding textual problems into accurate ASP programs?• RQ2: Can LLMs be fine-tuned for generating ASP rules that comply to a textual description?</p>
<p>• RQ3: To what extent can the output of the fine-tuned LLM be considered "ready-to-use" for practical applications, in terms of syntactic and semantic correctness?</p>
<p>• RQ4: What are the limitations and potentials of this approach, e.g., how the prompt structure and/or the complexity of the problem affect the generated programs?</p>
<p>In order to ensure reproducibility, all the code and data used to perform the experiments are publicly available: https:// anonymous.4open.science/r/LLASP-D86C/.</p>
<p>Pre-trained LLMs</p>
<p>Our initial goal is to assess the performance of several LLMs in generating ASP programs.Thus, in our experiments, we provide a structured evaluation of their performance in this domain.We focus on freely available models that also exemplify the latest advancements in LLM-based generation:</p>
<p>• ChatGPT 3.51 : released by OpenAI, it is a fine-tuned version from GPT-3.5, a language model trained to produce text, that has been optimized for conversation.</p>
<p>• Copilot2 : it is a conversational chat interface developed by Microsoft, built upon the language model GPT-4 and the text-to-image model DALL-E 3.</p>
<p>• Gemini (Team 2024a)3 : a new class of multimodal LLMs introduced by Google, showing remarkable capabilities across image, audio, video, and text understanding.</p>
<p>• Gemma (Team 2024b)4 : released by the same group that introduced Gemini, Gemma is a lightweight family of LLMs that outperforms similarly sized models across different tasks, such as understanding, reasoning and safety.</p>
<p>• LLaMa2 (Touvron and others 2023)5 : an LLM optimized for dialogue applications, developed by Meta.It shows promising results across various benchmarks, consistently outperforming open-source chat models.</p>
<p>• LLaMa36 : an enhanced state-of-the-art version of LLaMa models, showing improvements in tasks like reasoning, code generation and adherence to instructions.</p>
<p>• Mistral (Jiang et al. 2023) 7 : despite the reduced size of the first release (7B parameters), Mistral is a LLM that showed superior performance and efficiency across several benchmark tasks.Since then, bigger and more powerful versions have been introduced.</p>
<p>LLASP Fine-tuning</p>
<p>The next phase of the proposed methodology involves finetuning a lightweight model and assessing it on the problem categories outlined in Section 4. To achieve this, we build a training dataset by generating instances of the templates relative to such categories.Basically, for each problem class T C i , we produce variations of that template to generate a collection of prompts along with corresponding suitable ASP instances.Templates are instantiated by using a predefined set of predicates and labels, as shown in Section 4. sulting dataset D contains approximately 4 million tuples, with varying proportions based on the problem type, as detailed in Table 2.The quantity of tuples per task depends on its syntactic complexity: tasks supporting greater variability in their corresponding ASP programs (such as the number of predicates in the rule, arity of the atoms, and ground instances) require more data during training.We further split D into a training and a validation set with a 80-20 ratio, maintaining the proportions per problem.We opted for Gemma 2B as our model of choice for finetuning.As the smallest among the available models, we anticipate its baseline performance to be relatively modest; this allows us to obtain tangible feedback on the effectiveness of the methodology.We conduct Supervised Fine-Tuning (SFT) of L using the SFTTrainer component from the trl library8 , while leveraging QLora (Dettmers et al. 2023) for computational efficiency.The machine for training is an NVIDIA DGX server featuring a 20-Core Intel Xeon CPU, 256GB RAM and 4X NVIDIA Tesla V100 32 GB/GPU.Evaluation Protocol.To assess the correctness of the generated ASP encodings, we rely on the ASP solver Clingo (Gebser et al. 2016) via the dedicated Python API9 .We use Clingo for computing the answer sets of a program P , hence defining a function s(P ) = AS (P ), where AS (P ) is the set (possibly empty) of answer sets of P .Given both the ASP program y ∼ P L (.|x) generated by L from the prompt x and the gold program y * associated with x, we first build a set of facts F y * representing an instance of the textual problem x.Given P = y ∪ F y * and P * = y * ∪ F y * , we then perform the following checks.We invoke Clingo for computing s(P ); if no parsing error occurs, we have a syntactic hit on y.Further, we compute s(P ) and s(P * ), comparing AS (P ) with AS (P * ); if they match, we have a semantic hit on y.
ChatGPT 3.5 Copilot Gemini Gemma 2B Gemma 7B LLaMa2 13B LLaMa2 70B LLaMa2 7B LLaMa3 70B LLaMa3 8B Mistral 141B Mistral</p>
<p>Results</p>
<p>In an initial series of experiments, we conduct a comparative analysis between the pre-trained models and LLASP (i.e., the fine-tuned Gemma 2B model).It is noteworthy that many pre-trained models do not offer direct invocation through APIs.Hence, the evaluation is conducted by selecting, for each task, the example instantiation outlined in Section 4.Then, the corresponding prompt is submitted, and results are assessed according to the aforementioned protocol.The process is repeated 5 times to ensure statistical robustness over the sampling process intrinsic of the LLMs.</p>
<p>Table 3 reports accuracy of all the considered models, considering both syntactic and semantic perspectives; Figure 1 offers a concise visual representation of these findings.Notably, a syntactic hit does not necessarily imply with semantic correctness.For instance, Gemma 7B achieves 44% syntactic accuracy but shows no semantic quality at all.In general, no model achieves complete correctness across the board, and lightweight models report the weakest performance (with the only notable exception of Mistral 7B).Among the models, ChatGPT, Copilot, and Mistral 141B achieve 100% syntactic accuracy.Surprisingly, both Gemini and LLaMa3 70B display lower accuracy with respect to these methods, despite being comparable in model size.LLASP emerges as the model with the highest semantic accuracy among all the models considered.We note that whenever a generation produces a syntactically correct program, it also tends to be semantically correct.Table 3 confirms this observation, indicating that this phenomenon occurs only with LLASP.This is particularly noteworthy, given that the original Gemma 2B yields entirely inconsistent results.Table 4 further details the results for each task.Compared to large sized models, LLASP only fails on the join task, due to syntactic fails.By contrast, Mistral 141B exhibits semantic failures both on assignment and negative filtering.</p>
<p>We provide more insights regarding the generation flaws via anecdotal evidence of the ASP encoding provided by the identified top-performing models (see Figure 1): Mistral 141B, ChatGPT 3.5, Copilot, and Mistral 7B.In particular, we focus on semantic correctness of the generated programs, which in our context is the most representative quality measure.Surprisingly, almost all of the aforesaid model, except for ChatGPT 3.5, fail in representing the guessing assignment via a semantically correct ASP encoding, producing an average score of 0 out of 5 (see Table 4).ChatGPT 3.5, indeed generated a semantically correct program 4 times out of 5, achieving a score of 0.8.We report below the encoding produced by each of the models, comparing it with the one generated by LLASP, in the response to the Guessing Assignment task reported in Section 4. Mistral 7B/141B.Despite being syntactically correct, the generated program presents a semantic flow, since it does not allow multiple assignments for the same label.This property is not requested in the prompt, and would lead to inconsinstency whenever the number of labels is lower than the number of predicates.</p>
<p>Encoding:</p>
<p>1 { assigned(X, L) : label(L) } 1 :-city(X).:-assigned(X1, L), assigned(X2, L), X1 != X2.</p>
<p>ChatGPT 3.5.Similarly, the result of the generation is a syntactically correct ASP encoding, according to the Clingo solver.However, it fails in capturing the semantics of the textual prompt.</p>
<p>Encoding: 1 { assign(X, L) : label(L) } 1 :-city(X).:-assign(X, L1), assign(Y, L1), X != Y, label(L1).:-city(X), not assign(X, _).</p>
<p>Copilot.Here, the generated program encodes a completely different semantics with respect to the problem description: indeed, the above fragment specifies a join between the predicate "city" and the predicate "label" via the "City" label.</p>
<p>Encoding: assign_label(City, Label) :city(City), label(City, Label).</p>
<p>LLASP.Eventually, we report an example of wrong ASP program produced by LLASP over the join task discussed in Section 4. The generated encoding presents a syntactic error regarding both the arity of the predicate "owner" and the presence of the "Z" predicate, thus affecting the semantics of the program.</p>
<p>Encoding: owner_restaurant(X,Z):-owner(X,Y),Z(Y).</p>
<p>As an additional test, in order to further assess the robustness of the findings, we compare LLASP against the aforesaid models by performing a more detailed evaluation on an extended set of test instances, where prompts vary in terms of predicates and labels.Table 5 reports the results.Again, despite occasional syntactic flaws, LLASP offers more reliable encodings, outperforming the second-best model (Mistral 7B) of almost ten percentage points.Moreover, we observe that the problem semantics is fulfilled whenever syntactic correctness is ensured.</p>
<p>Encouraged by these results, we proceeded in testing LLASP on a larger scale.To this purpose, we generated a test set D test consisting of 9,000 prompts, following the same principles as the training set D described earlier, yet, again, on a different set of predicates and labels.In this test set, the number of examples for each task is evenly distributed.Table 6 presents the accuracy of LLASP over D test .We notice that: (i) unlike in the previous experiment, the consistency between syntactic and semantic accuracy is not upheld; (ii) the quality of the generated programs remains consistently high over all the tasks, even on the join problem which failed in our first experiment.Given the substantial scale of D test , we are confident that this experiment demonstrates the resilience of the proposed approach.</p>
<p>Limitations</p>
<p>To explore the ability of generating complex ASP programs from atomic tasks, we conducted further experiments for highlighting limitations and potentials, by fine-tuning LLASP on an expanded version of the dataset D, incorporating combined prompts.The results are mixed.While we observe a surprisingly successful generation rate for several complex combinations, we also witness failures on other simple ones.We believe that this is due to a suboptimal design of the training set regarding such combinations; indeed, we employed random combinations, whereas a more strategic approach should be devised.We further assessed the generation's generalization capabilities through a simple preliminary experiment: we generated and included 10 possible rephrasings for a given prompt in the dataset.However, the generation results were unsuccessful, suggesting that a more comprehensive evaluation requires a careful design of the training dataset.</p>
<p>Additionally, we explored various training strategies, including Reinforcement Learning with Human Feedback (RLHF) (Ziegler et al. 2020).In this setup, generation is linked to a reward that considers both syntactic and semantic aspects of the ASP program generated.However, the results did not surpass those of standard SFT training.We posit that a more meticulous design of the reward function is needed, were several aspects of the generation (e.g., rule redundancy, fluency or verbosity) are be considered.</p>
<p>Conclusions and Future Work</p>
<p>In this study, we conducted an extensive examination of the capability of Large Language Models (LLMs) to accurately encode Answer Set Programming (ASP) programs based on natural language specifications.To the best of our knowledge, this is the first extensive comparison of state-of-theart LLMs in terms of ASP code generation.Our findings show that, despite the large potential, LLMs are not ready for such a task, as they underperform in terms of programs correctness (both syntactic and semantic).With this respect, we demonstrate that tailored training can significantly mitigate this deficiency, offering a pathway towards fine-tuned models able to generate ASP programs ready for practical deployment.Indeed, we propose LLASP, a fine-tuned version of the lightweight Gemma 2B base-model.We train it over a comprehensive datased we designed and populated on purpose so to capture ASP fundamental patterns.LLASPgenerated programs proved to be significantly better than those generated by larger and more powerful LLMs, especially under a semantic perspective.</p>
<p>We also devised and performed a significant set of additional experiments, that provide interesting insights on the topic and suggest that this study can be expanded in multiple directions.In particular, besides the pointers already discussed in Section 5.4 (i.e., generalizability, complex prompts, RLHF training), we claim that a novel metric is necessary, to be used both in training and evaluation.In principle, this metric should take into account both syntactic correctness and capability of the generated ASP programs to effectively tackle problems described by the prompts in the first place.Furthermore, we aim at developing novel controlled training methodologies to incorporate ASP syntactic guidance within the training process of the LLMs.This would also require a careful inspection of the underlying architecture and the adoption of suitable modifications.</p>
<p>Figure 1 :
1
Figure1: Comparative analysis on both syntactic and semantic results.Some markers are hidden for overlapping in (0, 0).</p>
<p>Template: Write an ASP program for the following problem.Assign exactly a label among a given set of labels to a set of elements.The set of elements is expressed by predicate [PREDICATE].The labels are [LABEL]+.Prompt: Write an ASP program for the following problem.Assign exactly a label among a given set of labels to a set of elements.The set of elements is expressed by predicate city.The labels are moscow,rome,dubai.</p>
<p>Table 1
1
briefly compares the chosen LLMs in terms of model size, architectural details and training sources (Minaee et al. 2024).We can identify a group of large size models (≥ 70B parameters) and another group of reduced size models (&lt; 70B parameters); Gemma 2B is the smallest model in the batch.
ModelNo. Params.ArchitectureTraining DataChatGPT 3.5175BE-DOnline sourcesCopilot1.5TE-DGithub repositoriesGemini1.6TMoEDocs, Books, CodeGemma2B-7BD-OnlyDocs, Maths, CodeLLaMa27B-13BD-OnlyOnline sourcesLLaMa38B-70BD-Only + GQAOnline sourcesMistral7B-141BD-Only + GQAOnline sources</p>
<p>Table 1 :
1
Details on the evaluated pre-trained LLMs.Number of parameters is reported in billions(B)/trillions(T).E-D refers to the Encoder-Decoder architecture; D-Only is Decoder-Only; MoE is Mixture of Experts, and GQA is Grouped-Query Attention.</p>
<p>Table 2 :
2
The re-Number of tuples and relative proportions of each problem within the dataset D.
ProblemNo. Tuples Proportion (%)Assignment1,000,00027Constraint500,00013.5Combination100,0002.7Join900,00024.3Transitive closure100,0002.7Preference400,00010.8Value filtering100,0002.7Negative filtering100,0002.7Numeric filtering500,00013.5Total3,700,000100</p>
<p>Table 3 :
3
Comparative analysis in syntactic and semantic terms.
ModelSyntactic SemanticChatGPT 3.51.0.64Copilot1.0.67Gemini0.560.33Gemma 2B0.0.Gemma 7B0.440.LLaMa2 7B0.0.LLaMa2 13B0.0.LLaMa2 70B0.0.LLaMa3 8B0.0.LLaMa3 70B0.670.56Mistral 7B0.910.67Mistral 141B1.0.78LLASP0.890.89</p>
<p>Table 4 :
4
Generation accuracy in terms of syntactic (Syn.) and semantic (Sem.)perspective all the considered LLMs.
0.1.1.</p>
<p>Table 5 :
5
In-depth comparison using an extended dataset.
ProblemSyntactic SemanticAssignment0.760.73Constraint1.1.Combination1.0.81Join0.950.91Closure1.0.99Preference1.0.88Value Filtering1.0.89Negative Filtering1.0.89Numeric Filtering1.0.9Total (avg)0.970.89</p>
<p>Table 6 :
6
Results in terms of syntactic and semantic generation accuracy of LLASP over Dtest.</p>
<p>https://chatgpt.com/
https://learn.microsoft.com/en-us/copilot/overview
https://gemini.google.com/app
https://huggingface.co/blog/gemma
https://www.llama2.ai/
https://llama.meta.com/llama3/
https://chat.mistral.ai/chat
https://huggingface.co/docs/trl
https://potassco.org/clingo/python-api/5.4/
Model Assignment Constraint Combination Join Closure Preference Value Filter Neg.Filter Num.Filter Syn.Sem.Syn.Sem.Syn.Sem.Syn.Sem.Syn.Sem.Syn.Sem.Syn.Sem.Syn.Sem.Syn.Sem.ChatGPT 3.5 1. 0.8 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. Copilot 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0.4 0. 1. 1. 1. 1. 1. 1. Mistral 141B 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. LLASP 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.
Solving puzzles described in english by automated translation to answer set programming and learning how to do that translation. C Baral, J Dzifcak, AAAI Fall Symposium: Advances in Cognitive Systems, volume FS-11-01 of AAAI Technical Report. AAAI. Baral, C. 2010. Knowledge Representation, Reasoning and Declarative Problem Solving. I Kareem, F Ricca, Borroto, MCambridge University Press2011IJCAI. volume abs/2403.04541, to appear. ijcai.org</p>
<p>Design and results of the fifth answer set programming competition. G Brewka, T Eiter, M Truszczynski, F Calimeri, M Gebser, M Maratea, F Ricca, Artif. Intell. 54122011. 2016Commun. ACM</p>
<p>CNL2ASP: converting controlled natural language sentences into. S Caruso, C Dodaro, M Maratea, M Mochi, F Riccio, ASP. Theory Pract. Log. Program. 2422024</p>
<p>Github copilot AI pair programmer: Asset or liability?. M Chen, J Tworek, abs/2107.03374J. Syst. Softw. 2031117342021. 2023Evaluating large language models trained on code</p>
<p>Qlora: Efficient finetuning of quantized llms. T Dettmers, A Pagnoni, A Holtzman, L Zettlemoyer, T Eiter, G Ianni, T Krennwallner, S Tessaris, E Franconi, T Eiter, C Gutierrez, S Handschuh, M Rousset, Schmidt, Reasoning Web. Semantic Technologies for Information Systems, 5th International Summer School. Lecture Notes in Computer Science. R A , Brixen-Bressanone, ItalySpringer2023. 2009. 2009. August 30 -September 4, 20095689Answer set programming: A primer</p>
<p>Transforming controlled natural language biomedical queries into answer set programs. E Erdem, R Yeniterzi, BioNLP@HLT-NAACL. ACL2009</p>
<p>Ai-driven development is here: Should you worry?. N A Ernst, G Bavota, IEEE Softw. 3922022</p>
<p>An approach for representing answer sets in natural language. M Fang, H Tompits, DECLARE, volume 10997 of LNCS. Springer2017</p>
<p>Answer Set Solving in Practice. M Gebser, R Kaminski, B Kaufmann, T Schaub, Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan &amp; Claypool Publishers2012</p>
<p>Theory solving made easy with clingo 5. M Gebser, R Kaminski, B Kaufmann, M Ostrowski, T Schaub, P Wanko, International Conference on Logic Programming. 2016</p>
<p>Evaluation techniques and systems for answer set programming: a survey. M Gebser, N Leone, M Maratea, S Perri, F Ricca, T Schaub, M Lifschitz, V , Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018. J Lang, the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018Stockholm, Sweden2018. July 13-19. 2018. 19919New Gener. Comput.</p>
<p>Leveraging large language models to generate answer set programs. A Ishay, Z Yang, J Lee, KR. 2023</p>
<p>A Q Jiang, A Sablayrolles, A Mensch, C Bamford, D S Chaplot, D De Las Casas, F Bressand, G Lengyel, G Lample, L Saulnier, L R Lavaud, M.-A Lachaux, P Stock, T L Scao, T Lavril, T Wang, T Lacroix, W E Sayed, Mistral 7b. 2023</p>
<p>Research: quantifying github copilot's impact on developer productivity and happiness. E Kalliamvakou, 2022The GitHub Blog</p>
<p>A survey and classification of controlled natural languages. T Kuhn, Comput. Linguistics. 4012014</p>
<p>Llamoco: Instruction tuning of large language models for optimization code generation. V Lifschitz, Springer, Z Ma, H Guo, J Chen, G Peng, Z Cao, Y Ma, Y.-J Gong, S Minaee, T Mikolov, N Nikzad, M Chenaghlu, R Socher, X Amatriain, J Gao, 2019. 2024. 2024Answer Set Programming. Large language models: A survey</p>
<p>Addressing a question answering challenge by combining statistical methods with inductive rule learning and reasoning. A Mitra, C Baral, AAAI. AAAI Press2016</p>
<ol>
<li>Improving coherence and consistency in neural sequence models with dual-system, neuro-symbolic reasoning. H Naveed, A U Khan, S Qiu, M Saqib, S Anwar, M Usman, N Akhtar, N Barnes, A Mian, NeurIPS. M I Nye, M H Tessler, J B Tenenbaum, B M Lake, 2024A comprehensive overview of large language models</li>
</ol>
<p>A review on large language models: Architectures, applications, taxonomies, open issues and challenges. S Peng, E Kalliamvakou, P Cihon, M Demirer, M A K Raiaan, M S H Mukta, K Fatema, N M Fahad, S Sakib, M M J Mim, J Ahmad, M E Ali, S Azam, IEEE Access. 122023. 2024The impact of AI on developer productivity: Evidence from github copilot</p>
<p>Reliable natural language understanding with large language models and answer set programming. A Rajasekharan, Y Zeng, P Padalkar, G Gupta, Electronic Proceedings in Theoretical Computer Science. 2023385</p>
<p>Specifying and verbalising answer set programs in controlled natural language. R Schwitter, Theory Pract. Log. Program. 183-42018</p>
<p>Neural machine translation: A review. F Stahlberg, J. Artif. Intell. Res. 692020</p>
<p>Gemini: A family of highly capable multimodal models. G Team, 2024a</p>
<p>G 2024b Team, Gemma, Open models based on gemini research and technology. </p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, 2023</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L U Kaiser, I Polosukhin, Advances in Neural Information Processing Systems. I Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, R Garnett, Curran Associates, Inc201730</p>
<p>Codet5+: Open code large language models for code understanding and generation. Y Wang, H Le, A D Gotmare, N D Q Bui, J Li, S C H Hoi, 2023</p>
<p>A systematic evaluation of large language models of code. F F Xu, U Alon, G Neubig, V J Hellendoorn, Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming. the 6th ACM SIGPLAN International Symposium on Machine ProgrammingNew York, NY, USAAssociation for Computing Machinery20222022</p>
<p>Coupling large language models with logic programming for robust and general reasoning from text. Z Yang, A Ishay, J Lee, ACL (Findings). 2023</p>
<p>. Acl, D M Ziegler, N Stiennon, J Wu, T B Brown, A Radford, D Amodei, P Christiano, G Irving, 2020Finetuning language models from human preferences</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6881 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6881</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6881</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-132.html">extraction-schema-132</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <p><strong>Paper ID:</strong> paper-278129709</p>
                <p><strong>Paper Title:</strong> Large language models to accelerate organic chemistry synthesis</p>
                <p><strong>Paper Abstract:</strong> Chemical synthesis, as a foundational methodology in the creation of transformative molecules, exerts substantial influence across diverse sectors from life sciences to materials and energy. Current chemical synthesis practices emphasize laborious and costly trial-and-error workflows, underscoring the urgent needs for advanced AI assistants. Recently, large language models, typified by GPT-4, have been introduced as an efficient tool to facilitate scientific research. Here we present Chemma, a fully fine-tuned large language model with 1.28 million pairs of questions and answers about reactions, as an assistant to accelerate organic chemistry synthesis. Chemma surpasses the best-known results in multiple chemical tasks, for example, single-step retrosynthesis and yield prediction, which highlights the potential of general artificial intelligence for organic chemistry. By predicting yields across the experimental reaction space, Chemma significantly improves the reaction exploration capability of Bayesian optimization. More importantly, integrated in an active learning framework, Chemma exhibits advanced potentials of autonomously experimental exploration and optimization in open reaction spaces. For an unreported Suzuki–Miyaura cross-coupling reaction of cyclic aminoboronates and aryl halides for the synthesis of α-aryl N-heterocycles, the human–artificial intelligence collaboration successfully explored a suitable ligand (tri(1-adamantyl)phosphine) and solvent (1,4-dioxane) within only 15 runs, achieving an isolated yield of 67%. These results reveal that, without quantum-chemical calculations, Chemma can comprehend and extract chemical insights from reaction data, in a manner akin to human experts. This work opens avenues for accelerating organic chemistry synthesis with adapted large language models. Large language models (LLMs) can be useful tools for science, but they often lack expert understanding of complex domains that they were not trained on. Zhang and colleagues fine-tuned a LLaMA-2-7b-based LLM with questions on organic chemistry reactions.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6881.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6881.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chemma</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chemma (fine-tuned LLaMA-2-7B chemistry assistant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fully fine-tuned LLaMA-2-7B large language model adapted to organic synthesis via supervised fine-tuning (SFT) on ~1.28M Q&A reaction pairs and reinforcement learning from human feedback (Chemma-RM); used to generate SMILES-based reagents/ligands, predict yields/selectivities, propose reaction conditions, and drive active learning and BO-assisted experimental campaigns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-7B (fine-tuned as Chemma)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLM, fully fine-tuned; supervised fine-tuning (Chemma-SFT) plus reinforcement learning from human feedback (Chemma-RM, PPO)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pre-training foundation (LLaMA-2) then task-specific supervised fine-tuning on ~1.28 million Q&A reaction pairs assembled from literature, patents and HTE datasets (USPTO-50k, ORD, HTE Suzuki-Miyaura, Buchwald-Hartwig, imidazole C-H arylation, literature Pd-carbonylation, ELNs); 2,000 prompt templates per task were generated (with GPT-4) to construct supervised prompts; pairwise ranking data for RLHF derived from experimental yields/selectivities.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct sequence generation (SMILES and natural-language Q&A) via zero-shot prompting, in‑context learning (ICL), supervised fine-tuning outputs; RLHF (PPO) used to train ranking model Chemma-RM; two-stage approach for regression: extract embeddings from last hidden layer of Chemma-SFT then train MLP regressor (5-layer) to predict yields/selectivities; also used to generate predicted yields across full condition spaces for data augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES (sequence-based strings) for molecules and reaction SMILES; reaction information also encoded as textual prompts; DFT-derived descriptors used downstream in surrogate models but not as Chemma inputs for primary generation.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Organic synthesis assistance: single-step retrosynthesis, multi-step retrosynthesis route generation, ligand recommendation, reaction condition generation, yield prediction, regioselectivity and enantioselectivity prediction, reaction exploration and optimization (closed and open reaction spaces), and autonomous/human-in-the-loop discovery of unreported reactions (e.g., α-aryl N-heterocycles via Suzuki–Miyaura).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Closed-space experiments constrained to predefined pools of solvents/bases/ligands; when Chemma proposed out-of-pool conditions, outputs were regenerated until within the predefined space; deployed service includes hard-coded prompt templates and instruction guidelines plus automated chemical-property screening to block harmful requests; no explicit mention of scoring by synthetic-accessibility or other numeric filters during generation (beyond expert-in-the-loop validation).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Integrated into experimental pipelines: HTE platforms and ELNs for data collection and feedback; used with Gaussian process surrogate models (Chemma-BO) that fit the gap between observed and Chemma-predicted yields using DFT descriptors; used to augment RF models trained on DFT descriptors; active learning loop uses wet experiments and fine-tuning (RLHF) between rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Training/fine-tuning: USPTO-50k, Open Reaction Database (ORD), literature/patent corpora and curated HTE datasets (Pd-catalysed Suzuki–Miyaura HTE, Pd-catalysed Buchwald–Hartwig HTE/ELN, imidazole C–H arylation HTE), literature Pd-carbonylation data, regio- and enantioselectivity datasets (from refs. cited).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Retrosynthesis: top-k accuracy (top-1, top-5 etc.). Yield/selectivity prediction: R^2 (coefficient of determination), RMSE (percentage yield points or kcal mol−1 for ΔG), site accuracy for regioselectivity. BO/optimization: averaged cumulative maximum observed yield over batches and number of experiments to reach target yields. Additional assessments: comparison against baseline RF models and general-purpose LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Retrosynthesis: 72.2% top-1 accuracy on USPTO-50k (reaction class unknown, template-free), exceeding state-of-the-art transformer-based NAG2G (55.1%). Yield prediction: imidazole C–H arylation RMSE = 6.59% (R^2 = 0.74); Buchwald–Hartwig (ELN) RMSE = 6.56% (R^2 = 0.79); regioselectivity R^2 = 0.93, RMSE = 0.74 kcal mol−1, site accuracy = 78.74%; enantioselectivity R^2 = 0.89, RMSE = 0.25 kcal mol−1. Data augmentation: Chemma-enhanced RF achieved R^2 = 0.53 (Suzuki) and 0.72 (Buchwald–Hartwig) using only 5% real data (rest Chemma-generated), approaching RF with 90% real data (≈0.6 and 0.8). Chemma-BO optimization: reached 98.5% observed yield within first 15 experiments (Suzuki) and 98.7% within first 10 experiments (Buchwald–Hartwig) in averaged trials; GPT-4 reached ~93% max in comparison. Experimental open-space discovery: for an unreported Suzuki–Miyaura coupling to make α-aryl N-heterocycles, human–Chemma collaboration found tri(1-adamantyl)phosphine and 1,4-dioxane within 15 runs achieving 67% isolated yield; other generated ligands synthesized (L1, L3, L6) gave 6%, 16%, and no reactivity respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Limitations reported: poorer performance on reactions with very limited observations; initial round (round 0) for unreported reactions often failed and required iterative human feedback and fine-tuning (RLHF) to improve; generation can produce diverse responses requiring extra selection effort; hallucination risk (non-viable or incompatible conditions) — Chemma should act as copilot with chemist oversight; difficulty generating highly reactive ligands for completely unseen chemistry (low yields of synthesized novel ligands L1/L3/L6); potential for misuse (harmful compounds) and intellectual-property concerns; dependence on quality/diversity of open-source chemical data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models to accelerate organic chemistry synthesis', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6881.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6881.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chemma-BO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chemma-enhanced Bayesian Optimization (Chemma-BO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid optimization pipeline that uses Chemma to generate predicted yields across a reaction condition space and employs a Gaussian process surrogate (fitted to DFT descriptors) to model the residual between observed and Chemma-predicted yields, selecting experiments via an acquisition routine to accelerate yield optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chemma (LLM) + Gaussian process surrogate</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>hybrid LLM + probabilistic surrogate Bayesian optimization pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Chemma 7B; Gaussian process small surrogate</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Uses Chemma-generated yield predictions across condition grids plus a small set of observed HTE yields for GP fitting; DFT descriptors used for surrogate modelling as in prior BO frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Chemma predicts yields for all combinations in a search space; top candidates selected; GP trained on differences between observed yields and Chemma predictions (using DFT descriptors) to correct bias; iterative selection of top-k (top-5) candidates for wet testing, update loop repeats.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES for molecules; reaction condition vectors combined with DFT-derived descriptors for surrogate modelling.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Reaction optimization / yield maximization in HTE and open search spaces (closed-space experiments and unreported reaction exploration).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Selection of top-N (five) conditions per round; GP correction of Chemma bias relies on availability of DFT descriptors; closed-space experiments limit generation to predefined pools when desired.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Gaussian process surrogate models, DFT descriptor computation (used in surrogate), HTE experimental platforms for validation, ELN for storing observed data.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Suzuki–Miyaura (HTE) and Buchwald–Hartwig (HTE/ELN) datasets used for experiments and benchmarking Chemma-BO.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Averaged cumulative maximum observed yield across batches, number of experiments to reach target yields, comparison vs baseline BO and GPT-4 agent.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Chemma-BO reached 98.5% observed yield within 15 experiments (Suzuki) and 98.7% within 10 experiments (Buchwald–Hartwig) on averaged runs; outperformed classic BO which required ~50 experiments to reach similar yields, and outperformed GPT-4 agent (~93% averaged max yield after 100 experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Relies on quality of Chemma-generated priors; requires DFT descriptors for GP fitting in some contexts; bias of generated yields must be modelled and corrected; may be less effective if generated priors are poor for entirely novel chemistries without observed data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models to accelerate organic chemistry synthesis', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6881.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6881.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ActiveLearning-Chemma</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chemma-driven active learning framework (human-in-the-loop)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative human–AI active learning pipeline where Chemma proposes conditions (zero-shot / ICL), chemists run wet experiments (HTE or bench), results are stored in ELN and used to fine-tune Chemma (including RLHF) between rounds to improve proposals, enabling exploration of closed and open reaction spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chemma (fine-tuned LLaMA-2-7B) within active learning loop</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>human-in-the-loop active learning with LLM proposals and periodic fine-tuning (SFT + RLHF)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Chemma 7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Initial Chemma trained on literature/patent/HTE corpora; in-loop data are experimentally observed yields stored in ELN used for pairwise Q&A ranking datasets to fine-tune Chemma (RLHF) between rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Zero-shot prompts to generate initial candidates, in-context learning prompts to request 'higher-yield' candidates, iterative fine-tuning on experimental feedback, regeneration when conditions fall outside allowed pools.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES for reactants/products plus textual specification of conditions; outputs include ligand names/SMILES and solvents/bases/additives.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Exploration and optimization in both predefined (closed) and unconstrained (open) reaction condition spaces; applied to ligand discovery and solvent optimization for both reported HTE reactions and a previously unreported Suzuki–Miyaura coupling to synthesize α-aryl N-heterocycles.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Rounds can be constrained by expert-selected initial condition scopes; when outputs are outside allowed pools, researchers request regeneration; fine-tuning rounds require accumulation of experimental data in ELN.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>HTE platforms / bench experiments for data generation, ELN for data storage, RLHF training pipeline for model updates, DFT descriptors sometimes used downstream for surrogate modelling.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Round-specific experimental data (HTE datasets for imidazole C–H arylation and Buchwald–Hartwig, and bespoke experimental data for the unreported Suzuki–Miyaura reaction); initial training corpora as for Chemma.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Practical metrics: number of experimental runs to reach acceptable/high yields (e.g., 15 runs to reach 67% isolated yield for the unreported reaction), observed isolated yield percentages across substrate scope, qualitative success in discovering ligands/solvents.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Closed-space ligand exploration achieved high yields (e.g., XPhos >76% then CgMe-PPh 91% then 100% in later runs for imidazole C–H arylation); for unreported Suzuki–Miyaura, Chemma + human reached suitable ligand PAd3 and solvent 1,4-dioxane within 15 runs with 67% isolated yield and scope 45–67% across several electrophiles; some Chemma-proposed novel ligands synthesized (L1, L3, L6) gave low/no reactivity (6%, 16%, and no reactivity respectively).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Round-0 proposals for unreported chemistry often failed; requires iterative fine-tuning and chemist guidance; generated ligands sometimes non-reactive (low validation yields); risk of hallucinated or non-viable proposals; necessitates rigorous expert review and standardised prompt templates to reduce noise.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models to accelerate organic chemistry synthesis', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6881.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6881.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general-purpose, large pre-trained conversational LLM used as a baseline/comparator in retrosynthesis and reaction-optimization experiments; performed worse than the chemistry-adapted Chemma in complex optimization tasks and in BO simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>general-purpose conversational LLM (decoder-based), used via prompting (zero-shot/few-shot/agent-style prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompting (zero-shot/few-shot) and agent-style use for experiment suggestion (as in cited prior work); not fine-tuned on the task in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Textual prompts including SMILES used as inputs for queries; outputs as natural-language suggestions and sometimes SMILES or reagent names.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Evaluated for retrosynthesis and as an agent for reaction optimization (comparison in BO-style experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Not explicitly constrained in the study; used as generalist baseline without domain fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Compared to Chemma-BO; referenced prior works using GPT-4 as an agent. No DFT/GP integration reported for GPT-4 runs here.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Tested on same USPTO-50k retrosynthesis test set (for fair comparison) and on BO-style optimization benchmarks (Suzuki and Buchwald datasets) as a baseline agent.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Retrosynthesis top-k accuracies (reported in Supplementary); BO optimization: averaged cumulative maximum observed yield over experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Performed worse than Chemma: in BO experiments GPT-4 failed to identify acceptable conditions after 100 experiments with averaged maximum yield ~93%, versus Chemma-BO reaching ~99% in far fewer experiments; retrospective numerical comparisons present in supplementary materials.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>General-purpose LLM knowledge insufficient for complex chemical optimization without domain fine-tuning; poorer handling of reactions with many uncertain variables (e.g., Buchwald–Hartwig) and less effective at proposing high-performing conditions in open/complex spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models to accelerate organic chemistry synthesis', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6881.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6881.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CrystaLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CrystaLLM (GPT-2 based model for crystal structures)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work (cited) that adapted a GPT-2 architecture to model crystal structures through textual descriptions — cited as an example of domain-adapted LLMs in chemistry/materials.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CrystaLLM (GPT-2 architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLM adapted for crystal-structure textual modelling</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Trained on textual descriptions of crystal structures (details in the cited work, not in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Fine-tuning GPT-2 on domain-specific textual descriptions; generation via standard autoregressive decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Textual descriptions of crystal structures (not SMILES-based molecular generation).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Modeling and generation of crystal-structure related textual descriptions / representations (materials domain).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Mentioned only as related work; specific limitations are in the cited paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models to accelerate organic chemistry synthesis', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6881.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6881.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (Jablonka et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuned GPT-3 for inverse design and reaction classification (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited prior study that fine-tuned GPT-3 for inverse molecular design and reaction classification tasks in chemistry, used here as related work motivating domain-specific LLM fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (fine-tuned in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLM fine-tuned for chemical inverse design/classification</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Fine-tuning and prompt-based generation for inverse design (details in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Not specified in this paper (likely SMILES or text in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Inverse molecular design and reaction classification (drug/discovery-relevant tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Mentioned as prior successful fine-tuning example; detailed limitations are in the cited paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models to accelerate organic chemistry synthesis', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6881.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6881.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chemma-generated data augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Synthetic yield data generated by Chemma for data augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using Chemma to predict yields for a full reaction condition space to synthetically augment scarce experimental training data, then training classical models (e.g., RF with DFT descriptors) on a mix of real and Chemma-generated yields to improve predictive performance and BO efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chemma (generator) used to synthesize yield labels</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>LLM-based synthetic data generator used for data augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Chemma 7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Generated labels produced by Chemma for combinations in datasets (e.g., Suzuki and Buchwald condition spaces) using a small fraction of observed real data to guide augmentation scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Chemma predicts yield values across condition grids (zero-shot/ICL/fine-tuned outputs); these synthetic yields combined with limited real yields to train downstream RF models based on DFT descriptors.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Reaction SMILES and condition enumerations; downstream RF uses DFT descriptors per molecule/condition.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Improving yield-prediction models for reaction optimization and enabling few-shot learning when experimental data are scarce.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Scenarios tested: varying fractions of real vs. generated data (e.g., 5% real + 85% Chemma-generated out of 90% training set), held-out 10% test set.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Downstream RF models using DFT descriptors; used within Chemma-BO pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Suzuki–Miyaura (HTE) and Buchwald–Hartwig (HTE) datasets used for augmentation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>R^2 on held-out test sets, converging prediction error bands, comparison vs RF trained only on limited real data and vs RF trained on full real data.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Chemma-enhanced RF achieved R^2 = 0.53 (Suzuki) and 0.72 (Buchwald–Hartwig) with only 5% real data (rest synthetic), approaching RF trained on 90% real data (≈0.6 and 0.8 respectively).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Quality of downstream predictive improvement depends on reliability of Chemma-generated labels; synthetic-label bias must be modelled/fitted (Chemma-BO fits residuals); generated data may be less reliable for out-of-distribution chemistries.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large language models to accelerate organic chemistry synthesis', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Leveraging large language models for predictive chemistry <em>(Rating: 2)</em></li>
                <li>Autonomous chemical research with large language models <em>(Rating: 2)</em></li>
                <li>A GPT-4 reticular chemist for guiding MOF discovery <em>(Rating: 1)</em></li>
                <li>ChatGPT chemistry assistant for text mining and the prediction of MOF synthesis <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6881",
    "paper_id": "paper-278129709",
    "extraction_schema_id": "extraction-schema-132",
    "extracted_data": [
        {
            "name_short": "Chemma",
            "name_full": "Chemma (fine-tuned LLaMA-2-7B chemistry assistant)",
            "brief_description": "A fully fine-tuned LLaMA-2-7B large language model adapted to organic synthesis via supervised fine-tuning (SFT) on ~1.28M Q&A reaction pairs and reinforcement learning from human feedback (Chemma-RM); used to generate SMILES-based reagents/ligands, predict yields/selectivities, propose reaction conditions, and drive active learning and BO-assisted experimental campaigns.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-7B (fine-tuned as Chemma)",
            "model_type": "decoder-only LLM, fully fine-tuned; supervised fine-tuning (Chemma-SFT) plus reinforcement learning from human feedback (Chemma-RM, PPO)",
            "model_size": "7B parameters",
            "training_data_description": "Pre-training foundation (LLaMA-2) then task-specific supervised fine-tuning on ~1.28 million Q&A reaction pairs assembled from literature, patents and HTE datasets (USPTO-50k, ORD, HTE Suzuki-Miyaura, Buchwald-Hartwig, imidazole C-H arylation, literature Pd-carbonylation, ELNs); 2,000 prompt templates per task were generated (with GPT-4) to construct supervised prompts; pairwise ranking data for RLHF derived from experimental yields/selectivities.",
            "generation_method": "Direct sequence generation (SMILES and natural-language Q&A) via zero-shot prompting, in‑context learning (ICL), supervised fine-tuning outputs; RLHF (PPO) used to train ranking model Chemma-RM; two-stage approach for regression: extract embeddings from last hidden layer of Chemma-SFT then train MLP regressor (5-layer) to predict yields/selectivities; also used to generate predicted yields across full condition spaces for data augmentation.",
            "chemical_representation": "SMILES (sequence-based strings) for molecules and reaction SMILES; reaction information also encoded as textual prompts; DFT-derived descriptors used downstream in surrogate models but not as Chemma inputs for primary generation.",
            "target_application": "Organic synthesis assistance: single-step retrosynthesis, multi-step retrosynthesis route generation, ligand recommendation, reaction condition generation, yield prediction, regioselectivity and enantioselectivity prediction, reaction exploration and optimization (closed and open reaction spaces), and autonomous/human-in-the-loop discovery of unreported reactions (e.g., α-aryl N-heterocycles via Suzuki–Miyaura).",
            "constraints_used": "Closed-space experiments constrained to predefined pools of solvents/bases/ligands; when Chemma proposed out-of-pool conditions, outputs were regenerated until within the predefined space; deployed service includes hard-coded prompt templates and instruction guidelines plus automated chemical-property screening to block harmful requests; no explicit mention of scoring by synthetic-accessibility or other numeric filters during generation (beyond expert-in-the-loop validation).",
            "integration_with_external_tools": "Integrated into experimental pipelines: HTE platforms and ELNs for data collection and feedback; used with Gaussian process surrogate models (Chemma-BO) that fit the gap between observed and Chemma-predicted yields using DFT descriptors; used to augment RF models trained on DFT descriptors; active learning loop uses wet experiments and fine-tuning (RLHF) between rounds.",
            "dataset_used": "Training/fine-tuning: USPTO-50k, Open Reaction Database (ORD), literature/patent corpora and curated HTE datasets (Pd-catalysed Suzuki–Miyaura HTE, Pd-catalysed Buchwald–Hartwig HTE/ELN, imidazole C–H arylation HTE), literature Pd-carbonylation data, regio- and enantioselectivity datasets (from refs. cited).",
            "evaluation_metrics": "Retrosynthesis: top-k accuracy (top-1, top-5 etc.). Yield/selectivity prediction: R^2 (coefficient of determination), RMSE (percentage yield points or kcal mol−1 for ΔG), site accuracy for regioselectivity. BO/optimization: averaged cumulative maximum observed yield over batches and number of experiments to reach target yields. Additional assessments: comparison against baseline RF models and general-purpose LLMs.",
            "reported_results": "Retrosynthesis: 72.2% top-1 accuracy on USPTO-50k (reaction class unknown, template-free), exceeding state-of-the-art transformer-based NAG2G (55.1%). Yield prediction: imidazole C–H arylation RMSE = 6.59% (R^2 = 0.74); Buchwald–Hartwig (ELN) RMSE = 6.56% (R^2 = 0.79); regioselectivity R^2 = 0.93, RMSE = 0.74 kcal mol−1, site accuracy = 78.74%; enantioselectivity R^2 = 0.89, RMSE = 0.25 kcal mol−1. Data augmentation: Chemma-enhanced RF achieved R^2 = 0.53 (Suzuki) and 0.72 (Buchwald–Hartwig) using only 5% real data (rest Chemma-generated), approaching RF with 90% real data (≈0.6 and 0.8). Chemma-BO optimization: reached 98.5% observed yield within first 15 experiments (Suzuki) and 98.7% within first 10 experiments (Buchwald–Hartwig) in averaged trials; GPT-4 reached ~93% max in comparison. Experimental open-space discovery: for an unreported Suzuki–Miyaura coupling to make α-aryl N-heterocycles, human–Chemma collaboration found tri(1-adamantyl)phosphine and 1,4-dioxane within 15 runs achieving 67% isolated yield; other generated ligands synthesized (L1, L3, L6) gave 6%, 16%, and no reactivity respectively.",
            "experimental_validation": true,
            "challenges_or_limitations": "Limitations reported: poorer performance on reactions with very limited observations; initial round (round 0) for unreported reactions often failed and required iterative human feedback and fine-tuning (RLHF) to improve; generation can produce diverse responses requiring extra selection effort; hallucination risk (non-viable or incompatible conditions) — Chemma should act as copilot with chemist oversight; difficulty generating highly reactive ligands for completely unseen chemistry (low yields of synthesized novel ligands L1/L3/L6); potential for misuse (harmful compounds) and intellectual-property concerns; dependence on quality/diversity of open-source chemical data.",
            "uuid": "e6881.0",
            "source_info": {
                "paper_title": "Large language models to accelerate organic chemistry synthesis",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Chemma-BO",
            "name_full": "Chemma-enhanced Bayesian Optimization (Chemma-BO)",
            "brief_description": "A hybrid optimization pipeline that uses Chemma to generate predicted yields across a reaction condition space and employs a Gaussian process surrogate (fitted to DFT descriptors) to model the residual between observed and Chemma-predicted yields, selecting experiments via an acquisition routine to accelerate yield optimization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Chemma (LLM) + Gaussian process surrogate",
            "model_type": "hybrid LLM + probabilistic surrogate Bayesian optimization pipeline",
            "model_size": "Chemma 7B; Gaussian process small surrogate",
            "training_data_description": "Uses Chemma-generated yield predictions across condition grids plus a small set of observed HTE yields for GP fitting; DFT descriptors used for surrogate modelling as in prior BO frameworks.",
            "generation_method": "Chemma predicts yields for all combinations in a search space; top candidates selected; GP trained on differences between observed yields and Chemma predictions (using DFT descriptors) to correct bias; iterative selection of top-k (top-5) candidates for wet testing, update loop repeats.",
            "chemical_representation": "SMILES for molecules; reaction condition vectors combined with DFT-derived descriptors for surrogate modelling.",
            "target_application": "Reaction optimization / yield maximization in HTE and open search spaces (closed-space experiments and unreported reaction exploration).",
            "constraints_used": "Selection of top-N (five) conditions per round; GP correction of Chemma bias relies on availability of DFT descriptors; closed-space experiments limit generation to predefined pools when desired.",
            "integration_with_external_tools": "Gaussian process surrogate models, DFT descriptor computation (used in surrogate), HTE experimental platforms for validation, ELN for storing observed data.",
            "dataset_used": "Suzuki–Miyaura (HTE) and Buchwald–Hartwig (HTE/ELN) datasets used for experiments and benchmarking Chemma-BO.",
            "evaluation_metrics": "Averaged cumulative maximum observed yield across batches, number of experiments to reach target yields, comparison vs baseline BO and GPT-4 agent.",
            "reported_results": "Chemma-BO reached 98.5% observed yield within 15 experiments (Suzuki) and 98.7% within 10 experiments (Buchwald–Hartwig) on averaged runs; outperformed classic BO which required ~50 experiments to reach similar yields, and outperformed GPT-4 agent (~93% averaged max yield after 100 experiments).",
            "experimental_validation": true,
            "challenges_or_limitations": "Relies on quality of Chemma-generated priors; requires DFT descriptors for GP fitting in some contexts; bias of generated yields must be modelled and corrected; may be less effective if generated priors are poor for entirely novel chemistries without observed data.",
            "uuid": "e6881.1",
            "source_info": {
                "paper_title": "Large language models to accelerate organic chemistry synthesis",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "ActiveLearning-Chemma",
            "name_full": "Chemma-driven active learning framework (human-in-the-loop)",
            "brief_description": "An iterative human–AI active learning pipeline where Chemma proposes conditions (zero-shot / ICL), chemists run wet experiments (HTE or bench), results are stored in ELN and used to fine-tune Chemma (including RLHF) between rounds to improve proposals, enabling exploration of closed and open reaction spaces.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Chemma (fine-tuned LLaMA-2-7B) within active learning loop",
            "model_type": "human-in-the-loop active learning with LLM proposals and periodic fine-tuning (SFT + RLHF)",
            "model_size": "Chemma 7B",
            "training_data_description": "Initial Chemma trained on literature/patent/HTE corpora; in-loop data are experimentally observed yields stored in ELN used for pairwise Q&A ranking datasets to fine-tune Chemma (RLHF) between rounds.",
            "generation_method": "Zero-shot prompts to generate initial candidates, in-context learning prompts to request 'higher-yield' candidates, iterative fine-tuning on experimental feedback, regeneration when conditions fall outside allowed pools.",
            "chemical_representation": "SMILES for reactants/products plus textual specification of conditions; outputs include ligand names/SMILES and solvents/bases/additives.",
            "target_application": "Exploration and optimization in both predefined (closed) and unconstrained (open) reaction condition spaces; applied to ligand discovery and solvent optimization for both reported HTE reactions and a previously unreported Suzuki–Miyaura coupling to synthesize α-aryl N-heterocycles.",
            "constraints_used": "Rounds can be constrained by expert-selected initial condition scopes; when outputs are outside allowed pools, researchers request regeneration; fine-tuning rounds require accumulation of experimental data in ELN.",
            "integration_with_external_tools": "HTE platforms / bench experiments for data generation, ELN for data storage, RLHF training pipeline for model updates, DFT descriptors sometimes used downstream for surrogate modelling.",
            "dataset_used": "Round-specific experimental data (HTE datasets for imidazole C–H arylation and Buchwald–Hartwig, and bespoke experimental data for the unreported Suzuki–Miyaura reaction); initial training corpora as for Chemma.",
            "evaluation_metrics": "Practical metrics: number of experimental runs to reach acceptable/high yields (e.g., 15 runs to reach 67% isolated yield for the unreported reaction), observed isolated yield percentages across substrate scope, qualitative success in discovering ligands/solvents.",
            "reported_results": "Closed-space ligand exploration achieved high yields (e.g., XPhos &gt;76% then CgMe-PPh 91% then 100% in later runs for imidazole C–H arylation); for unreported Suzuki–Miyaura, Chemma + human reached suitable ligand PAd3 and solvent 1,4-dioxane within 15 runs with 67% isolated yield and scope 45–67% across several electrophiles; some Chemma-proposed novel ligands synthesized (L1, L3, L6) gave low/no reactivity (6%, 16%, and no reactivity respectively).",
            "experimental_validation": true,
            "challenges_or_limitations": "Round-0 proposals for unreported chemistry often failed; requires iterative fine-tuning and chemist guidance; generated ligands sometimes non-reactive (low validation yields); risk of hallucinated or non-viable proposals; necessitates rigorous expert review and standardised prompt templates to reduce noise.",
            "uuid": "e6881.2",
            "source_info": {
                "paper_title": "Large language models to accelerate organic chemistry synthesis",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A general-purpose, large pre-trained conversational LLM used as a baseline/comparator in retrosynthesis and reaction-optimization experiments; performed worse than the chemistry-adapted Chemma in complex optimization tasks and in BO simulations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_type": "general-purpose conversational LLM (decoder-based), used via prompting (zero-shot/few-shot/agent-style prompting)",
            "model_size": null,
            "training_data_description": null,
            "generation_method": "Prompting (zero-shot/few-shot) and agent-style use for experiment suggestion (as in cited prior work); not fine-tuned on the task in this study.",
            "chemical_representation": "Textual prompts including SMILES used as inputs for queries; outputs as natural-language suggestions and sometimes SMILES or reagent names.",
            "target_application": "Evaluated for retrosynthesis and as an agent for reaction optimization (comparison in BO-style experiments).",
            "constraints_used": "Not explicitly constrained in the study; used as generalist baseline without domain fine-tuning.",
            "integration_with_external_tools": "Compared to Chemma-BO; referenced prior works using GPT-4 as an agent. No DFT/GP integration reported for GPT-4 runs here.",
            "dataset_used": "Tested on same USPTO-50k retrosynthesis test set (for fair comparison) and on BO-style optimization benchmarks (Suzuki and Buchwald datasets) as a baseline agent.",
            "evaluation_metrics": "Retrosynthesis top-k accuracies (reported in Supplementary); BO optimization: averaged cumulative maximum observed yield over experiments.",
            "reported_results": "Performed worse than Chemma: in BO experiments GPT-4 failed to identify acceptable conditions after 100 experiments with averaged maximum yield ~93%, versus Chemma-BO reaching ~99% in far fewer experiments; retrospective numerical comparisons present in supplementary materials.",
            "experimental_validation": false,
            "challenges_or_limitations": "General-purpose LLM knowledge insufficient for complex chemical optimization without domain fine-tuning; poorer handling of reactions with many uncertain variables (e.g., Buchwald–Hartwig) and less effective at proposing high-performing conditions in open/complex spaces.",
            "uuid": "e6881.3",
            "source_info": {
                "paper_title": "Large language models to accelerate organic chemistry synthesis",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "CrystaLLM",
            "name_full": "CrystaLLM (GPT-2 based model for crystal structures)",
            "brief_description": "Prior work (cited) that adapted a GPT-2 architecture to model crystal structures through textual descriptions — cited as an example of domain-adapted LLMs in chemistry/materials.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "CrystaLLM (GPT-2 architecture)",
            "model_type": "decoder-only LLM adapted for crystal-structure textual modelling",
            "model_size": null,
            "training_data_description": "Trained on textual descriptions of crystal structures (details in the cited work, not in this paper).",
            "generation_method": "Fine-tuning GPT-2 on domain-specific textual descriptions; generation via standard autoregressive decoding.",
            "chemical_representation": "Textual descriptions of crystal structures (not SMILES-based molecular generation).",
            "target_application": "Modeling and generation of crystal-structure related textual descriptions / representations (materials domain).",
            "constraints_used": null,
            "integration_with_external_tools": null,
            "dataset_used": null,
            "evaluation_metrics": null,
            "reported_results": null,
            "experimental_validation": null,
            "challenges_or_limitations": "Mentioned only as related work; specific limitations are in the cited paper.",
            "uuid": "e6881.4",
            "source_info": {
                "paper_title": "Large language models to accelerate organic chemistry synthesis",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "GPT-3 (Jablonka et al.)",
            "name_full": "Fine-tuned GPT-3 for inverse design and reaction classification (prior work)",
            "brief_description": "Cited prior study that fine-tuned GPT-3 for inverse molecular design and reaction classification tasks in chemistry, used here as related work motivating domain-specific LLM fine-tuning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-3 (fine-tuned in cited work)",
            "model_type": "decoder-only LLM fine-tuned for chemical inverse design/classification",
            "model_size": null,
            "training_data_description": null,
            "generation_method": "Fine-tuning and prompt-based generation for inverse design (details in cited work).",
            "chemical_representation": "Not specified in this paper (likely SMILES or text in cited work).",
            "target_application": "Inverse molecular design and reaction classification (drug/discovery-relevant tasks).",
            "constraints_used": null,
            "integration_with_external_tools": null,
            "dataset_used": null,
            "evaluation_metrics": null,
            "reported_results": null,
            "experimental_validation": null,
            "challenges_or_limitations": "Mentioned as prior successful fine-tuning example; detailed limitations are in the cited paper.",
            "uuid": "e6881.5",
            "source_info": {
                "paper_title": "Large language models to accelerate organic chemistry synthesis",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Chemma-generated data augmentation",
            "name_full": "Synthetic yield data generated by Chemma for data augmentation",
            "brief_description": "Using Chemma to predict yields for a full reaction condition space to synthetically augment scarce experimental training data, then training classical models (e.g., RF with DFT descriptors) on a mix of real and Chemma-generated yields to improve predictive performance and BO efficiency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Chemma (generator) used to synthesize yield labels",
            "model_type": "LLM-based synthetic data generator used for data augmentation",
            "model_size": "Chemma 7B",
            "training_data_description": "Generated labels produced by Chemma for combinations in datasets (e.g., Suzuki and Buchwald condition spaces) using a small fraction of observed real data to guide augmentation scenarios.",
            "generation_method": "Chemma predicts yield values across condition grids (zero-shot/ICL/fine-tuned outputs); these synthetic yields combined with limited real yields to train downstream RF models based on DFT descriptors.",
            "chemical_representation": "Reaction SMILES and condition enumerations; downstream RF uses DFT descriptors per molecule/condition.",
            "target_application": "Improving yield-prediction models for reaction optimization and enabling few-shot learning when experimental data are scarce.",
            "constraints_used": "Scenarios tested: varying fractions of real vs. generated data (e.g., 5% real + 85% Chemma-generated out of 90% training set), held-out 10% test set.",
            "integration_with_external_tools": "Downstream RF models using DFT descriptors; used within Chemma-BO pipeline.",
            "dataset_used": "Suzuki–Miyaura (HTE) and Buchwald–Hartwig (HTE) datasets used for augmentation experiments.",
            "evaluation_metrics": "R^2 on held-out test sets, converging prediction error bands, comparison vs RF trained only on limited real data and vs RF trained on full real data.",
            "reported_results": "Chemma-enhanced RF achieved R^2 = 0.53 (Suzuki) and 0.72 (Buchwald–Hartwig) with only 5% real data (rest synthetic), approaching RF trained on 90% real data (≈0.6 and 0.8 respectively).",
            "experimental_validation": false,
            "challenges_or_limitations": "Quality of downstream predictive improvement depends on reliability of Chemma-generated labels; synthetic-label bias must be modelled/fitted (Chemma-BO fits residuals); generated data may be less reliable for out-of-distribution chemistries.",
            "uuid": "e6881.6",
            "source_info": {
                "paper_title": "Large language models to accelerate organic chemistry synthesis",
                "publication_date_yy_mm": "2025-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Leveraging large language models for predictive chemistry",
            "rating": 2,
            "sanitized_title": "leveraging_large_language_models_for_predictive_chemistry"
        },
        {
            "paper_title": "Autonomous chemical research with large language models",
            "rating": 2,
            "sanitized_title": "autonomous_chemical_research_with_large_language_models"
        },
        {
            "paper_title": "A GPT-4 reticular chemist for guiding MOF discovery",
            "rating": 1,
            "sanitized_title": "a_gpt4_reticular_chemist_for_guiding_mof_discovery"
        },
        {
            "paper_title": "ChatGPT chemistry assistant for text mining and the prediction of MOF synthesis",
            "rating": 1,
            "sanitized_title": "chatgpt_chemistry_assistant_for_text_mining_and_the_prediction_of_mof_synthesis"
        }
    ],
    "cost": 0.02154,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>nature machine intelligence Article
1 July 2025</p>
<p>Feng Zhu 
Xiaokang Yang xkyang@sjtu.edu.cn 
Yaohui Jin 
Yanyan Xu yanyanxu@sjtu.edu.cn 
nature machine intelligence Article
1 July 202519AC7AC4C8E3FA3FE471635BF7FF1F3910.1038/s42256-025-01066-yReceived: 6 June 2024 Accepted: 27 May 2025
Large language models to accelerate organic chemistry synthesisChemical synthesis, as a foundational methodology in the creation of transfor mative molecules, exerts substantial influence across diverse sectors from life sciences to materials and energy.Current chemical synthesis practices emphasize laborious and costly trial-and-error workflows, underscoring the urgent needs for advanced AI assistants.Recently, large language models, typified by GPT-4, have been introduced as an efficient tool to facilitate scientific research.Here we present Chemma, a fully fine-tuned large language model with 1.28 million pairs of questions and answers about reactions, as an assistant to accelerate organic chemistry synthesis.Chemma surpasses the best-known results in multiple chemical tasks, for example, single-step retrosynthesis and yield prediction, which highlights the potential of general artificial intelligence for organic chemistry.By predicting yields across the experimental reaction space, Chemma significantly improves the reaction exploration capability of Bayesian optimization.More importantly, integrated in an active learning framework, Chemma exhibits advanced potentials of autonomously experimental exploration and optimization in open reaction spaces.For an unreported Suzuki-Miyaura cross-coupling reaction of cyclic aminoboronates and aryl halides for the synthesis of α-aryl N-heterocycles, the human-artificial intelligence collaboration successfully explored a suitable ligand (tri(1-adamantyl)phosphine) and solvent (1,4-dioxane) within only 15 runs, achieving an isolated yield of 67%.These results reveal that, without quantum-chemical calculations, Chemma can comprehend and extract chemical insights from reaction data, in a manner akin to human experts.This work opens avenues for accelerating organic chemistry synthesis with adapted large language models.Chemical synthesis stands as the cornerstone of the discipline of chemistry, enabling the creation, exploration and understanding of new substances[1][2][3][4][5][6].Chemical synthesis also plays a crucial role in numerous cutting-edge scientific domains.For instance, in drug discovery, synthesis of complex molecules enables the creation of diverse candidates of small molecules with therapeutic potential 7 ; in the realm of renewable energy, the design and synthesis of catalysts are essential for enhancing the efficiency of processes such as hydrogen production, fuel cells and carbon capture8,9.Despite important advancements in chemical instrumentation over the past few decades, the process of chemical synthesis continues to be laborious and time-consuming due to the vast reaction space and complexity of molecular structures[10][11][12][13].This iterative process typically involves reviewing the literature, designing reaction steps and conducting wet lab experiments, which necessitates repeated trial and error10,14.Even with the aid of literature retrieval tools, for novel molecules, none of them has presented a valuable synthetic path towards integration of computer-aided synthesis planning and expert</p>
<p>Performance of Chemma on open benchmark data</p>
<p>We benchmark our model on various publicly available datasets to illustrate that Chemma can answer a wide range of organic synthesis questions via interactive conversations.The testing questions cover the aforementioned tasks, including forward prediction, single-step retrosynthesis, condition generation, yield prediction and selectivity (including regioselectivity and enantioselectivity) effort 15,16 .In summary, to realize efficient and highly selective chemical synthesis, there is an urgent need to reform the research paradigm of organic chemistry synthesis 17,18 .</p>
<p>Recently, the emergence of generative pre-trained transformer (GPT)-based large language models (LLMs), typified by GPT-4, has triggered keen interest in leveraging such techniques to tackle scientific challenges [19][20][21][22] .These include-but are not limited to-literature mining 23 , data annotation 24 , experimental planning [25][26][27][28] , scientific tools scheduling 29,30 and so on.Particularly in chemistry, Zheng et al. leveraged prompt engineering to instruct ChatGPT in the automation of text mining 31 , experimental design 25 and multi-agent collaboration 29 for the discovery of metal-organic frameworks.Antunes et al.  proposed CrystaLLM based on a GPT-2 architecture to focus on modelling crystal structures through their textual descriptions 32 .Jablonka et al. fine-tuned GPT-3 for inverse design and reaction classification tasks in organic chemistry 6 .Furthermore, other approaches have focused on designing chemical agents driven by LLMs to plan and conduct experiments 26,30,33,34 .Nevertheless, the chemical capabilities of GPT-4 remain limited to tasks such as molecular captioning and translation, making it challenging to autonomously explore and optimize unreported reactions within an open reaction space.</p>
<p>In the field of organic synthesis, advanced artificial intelligence (AI) techniques have made noteworthy progress.For seeking feasible routes, Coley et al. developed a preliminary platform that integrates retrosynthesis models and robotic apparatus 35,36 to seek feasible routes; Shield et al. reported a framework of Bayesian optimization (BO) to identify high-yield experimental conditions within largely distinct parameter spaces 37 for reaction condition exploration; Angello et al. reported a streamlined closed-loop workflow that leveraged machine learning algorithms to guide robotic experimentation for the discovery of general reaction conditions 11 ; and Tang et al. leveraged statistical modelling techniques to model the mechanism of the oxidative addition process 38 .Wang et al. applied a reinforcement learning bandit optimization to identify generally applicable conditions by examining experimental feedback 39 .Nevertheless, these methods rely heavily on density functional theory (DFT) calculations to parameterize molecules and feature manual selection by experts' knowledge, which leads to molecular screening, and prediction continues to be a trial-and-error process.Besides, these methods need a high-throughput experiment (HTE) platform to collect experimental data, which shows less generalization capabilities for new reactions 40,41 .More importantly, existing methods for reaction exploration aim at finding suitable conditions within experts' predefined reaction condition pools, also known as the closed reaction space.However, potential condition variables from the open reaction space with higher performance might be ignored.</p>
<p>Towards these points, considering that molecules can be expressed in sequence, and the reaction is described as a natural language in the literature, LLMs can be a potential solution owing to the following advances.(1) By learning molecular representations from SMILES, LLMs can capture the atomic arrangement of molecules and understand their chemical structures.(2) Pre-trained with extensive reaction data, LLMs have the capability to learn relationships among reactants, products and conditions in reactions, thereby acquiring chemical knowledge akin to the learning process of chemists.(3) The generative capability of LLMs enables the design of molecules, for example, in the ligand recommendation task, thereby facilitating the discovery of chemical reactions.To this end, we present Chemma, a fully fine-tuned LLM based on LLaMA-2-7b, as a generative assistant to accelerate organic synthesis chemistry.Chemma is trained to answer the questions of chemists, functionalized as human-AI interactions and an experiment assistant.Chemma tackles several primary tasks with simple chemistry prompts, including forward reaction prediction, retro synthesis, reaction performance prediction, condition generation, and reaction exploration and optimization.These abilities are Article https://doi.org/10.1038/s42256-025-01066-yprediction.Note that, to mitigate potential data leakage issues, we build a specialized Chemma model for each chemical task independently.All reactions for evaluation are posted in Extended Data Fig. 2, and the results are presented in Fig. 3.</p>
<p>Retrosynthesis</p>
<p>Retrosynthesis planning in organic chemistry aims at designing a pathway to synthesize the target product starting from a set of purchasable molecules.Single-step retrosynthesis is the first key step, which provides the most possible reactants, often without consideration of reaction conditions.By expressing molecules as SMILES, Chemma models single-step retrosynthesis as a sequence-to-sequence problem.For a fair comparison, we build Chemma from a plain LLaMA-2 model and all methods are trained and tested on the same USPTO-50k dataset 44 (40,000 reactions for training).In the dataset, 40,000 samples are used for model training, with 5,000 samples for model validation and another 5,000 for testing.We select multiple reputable methods published in recent years as baselines for numerical comparison.These solutions can be categorized into two paradigms, graph-based methods [45][46][47][48] and transformer-based methods [49][50][51][52] .We measure the top-k accuracy of the predictions, defined as the proportion of test cases in which the correct answer appears among the top k.We also test the performance of general-purpose LLMs for the retrosynthesis task with the same testing set, including GPT-4, GPT-3.5, GPT-4o and LLaMA-2-13B (Supplementary Note 3 and Supplementary Table 1).</p>
<p>It is clear that, with the configuration of reaction class unknown and template free, Chemma achieves 72.2% top-1 accuracy, confidently outperforming the baseline methods (Fig. 3a).Specifically, Chemma outperforms the state-of-the-art transformer-based methods, NAG2G 52 (55.1%), by 17.1% in terms of top-1 accuracy.All detailed results can be seen in Supplementary Table 2. Furthermore, based on the remarkable capability of Chemma on single-step retrosynthesis, we generate the multi-step retrosynthesis routes for five drug molecules (Supplementary Note 3 and Extended Data Fig. 3).In these test cases, Chemma can mostly generate proper synthetic steps in the entire route, including reductive amination, aryl substitution or nitro reduction reaction.</p>
<p>Ligand recommendation</p>
<p>We investigate the performance of ligand recommendation under identical reaction conditions.The objective is to recommend or generate the 'optimal' ligands with the highest yield under identical reaction conditions in a predefined reaction space.We select the Pd-catalysed imidazole C-H arylation reaction extracted from the ORD 53 for numerical evaluation, which consists of boronic acid derivative, aryl halide, ligand, base and solvent substrates (Fig. 3b).Given a specific reaction, Chemma recommends a ligand under a predefined solvent-base combination of conditions.Compared with other ligands, the yield distribution under XPhos is clearly on the right.We also analyse the recommendation performance between diverse ligands for each base-solvent combination.We can observe that, for 15 of the 16 base-solvent combinations, the recommended ligand performs best in terms of the median value of reaction yields, suggesting that Chemma can recommend a highyield ligand.</p>
<p>Yield prediction</p>
<p>Yield prediction in organic synthesis frequently presents a significant challenge; whereas product selectivity tends to be influenced by a limited number of elementary steps, numerous on-cycle and offcycle events significantly affect reactivity 53,54 .Primary yield prediction methods calculate DFT descriptors for each molecule and fit yields with machine learning models 54,55 .In contrast, without DFT descriptors, Chemma predicts the yields with the latent representation of reactions extracted from Chemma-SFT (Fig. 2b).</p>
<p>Here we report that Chemma can be used to predict the yields of reactions collected through the HTE, electronic laboratory notebooks (ELNs) or literature-derived data.For the HTE datasets, we select Pd-catalysed Suzuki-Miyaura and imidazole C-H arylation reactions (Fig. 3b and Extended Data Fig. 2b).Pd-catalysed C-H direct functionalization has earned increasing interest in pharmaceutical development for its ability to generate molecule complexity without the need for pre-functionalized starting material 39,56 .Each reaction in the datasets is associated with conditions including aryl bromides, ligands, bases and solvents, and the corresponding yield.For the ELN datasets, we select the Buchwald-Hartwig reaction reported in ref. 57.The distributions of the reaction yields in these three datasets are given in Extended Data Fig. 4a-c.For the Suzuki-Miyaura and C-H arylation reactions, we allocate 30% of the reactions of the full capacity whose substrates are not included in the training set as an out-of-sample test set; for the Buchwald-Hartwig (ELN) reaction, we randomly split 30% of the entire dataset for testing.</p>
<p>The first four panels in Fig. 3d present the results for the Suzuki-Miyaura reaction.In the dataset, there are 28 combinations of boronics</p>
<p>Article</p>
<p>https://doi.org/10.1038/s42256-025-01066-yand aryl halides and four of them are selected as test cases.The other two panels present the results for the imidazole C-H arylation and Buchwald-Hartwig (ELN) reactions, respectively.The colour bands in Fig. 3d depict confidence intervals of the linear fitting between observations and predictions.Overall, Chemma provides acceptable performance for the three reactions.The main reason is that Chemma has been pre-trained on a large number of reaction data in stage 1 (Fig. 2b) and endowed with a certain degree of chemical knowledge.Furthermore, the test-set root mean square error (RMSE) for the imidazole C-H arylation reaction is 6.59% with a coefficient of determination R 2 value of 0.74, and the RMSE for Buchwald-Hartwig (ELN) is 6.56% with an R 2 value of 0.79.They are slightly weaker than b Suzuki-Miyaura (HTE) reaction  Considering various solvents and bases in Suzuki-Miyaura datasets, we depict the yield distribution for each base-solvent-ligand (B-S-L) combination.The blue plot represents the distribution of observed yields of all ligands excluding the recommended one and the red plot represents the yield distribution of the recommended ligand.d, Performance of yield prediction on Pd-catalysed Suzuki-Miyaura (HTE), Buchwald-Hartwig (ELN) and imidazole arylation (HTE) reactions.Error bars range from −0.01 to 0.01.We set the value of uncertainty as 0.05.It is expected that the actual values will fall within the shaded area in a specified 95% proportion of cases.We conduct the validation in an out-ofsample fashion, randomly splitting a dataset into 70% training and 30% test sets.e, Performance of selectivity prediction on two HTE datasets.We assess regioselectivity prediction on the regioselective radical C-H functionalization reactions and enantioselectivity prediction on the chiral phosphoric acidcatalysed thiol of addition N-imides reactions.Scatter plots compare the predicted and observed ΔG.</p>
<p>Article</p>
<p>https://doi.org/10.1038/s42256-025-01066-y the Suzuki-Miyaura reaction, possibly due to the imbalance of yield distribution, the diversity of products and the sparsity of reaction conditions.</p>
<p>Also, we examine the performances of models via different training/testing split strategies (Supplementary Note 4).Chemma's consistent superiority over random forest (RF) 54 across varying data proportions emphasizes its more effective use of limited datasets.Furthermore, we also evaluate the performance of Chemma for yield prediction of Pd-catalysed carbonylation reactions using literature-derived data 58 .The R 2 value of Chemma reaches 0.74, clearly surpassing traditional machine learning methods (Supplementary Note 4 and Extended Data Fig. 5).</p>
<p>Regioselectivity and enantioselectivity prediction</p>
<p>We apply Chemma on both regioselectivity and enantioselectivity prediction tasks.For regioselectivity, we adopt the dataset created by Li et al. 59 , which consists of reactions for radical C-H functionalization over heterocycles.The dataset provides the reaction SMILES including different arene scaffolds, substitution patterns and radicals, along with the free energy barriers (ΔG) calculated by DFT annotations.We randomly select 30% of the original dataset as the out-of-sample test set.For enantioselectivity, we focus on chiral phosphoric acidcatalysed thiol addition to N-acylimines from Zahrt et al. 60 .The dataset includes 25 reactants and 43 catalysts, making up 1,075 reactions.We randomly select 600 reactions for training and 475 reactions for tests.</p>
<p>On the regioselectivity dataset, the R 2 , RMSE and site accuracy of the predicted results are 0.93, 0.74 kcal mol −1 and 78.74%, respectively, which is close to the performance of the RF model 59 .A case study for regioselectivity can be found in Supplementary Note 5 and Extended Data Fig. 6.On the enantioselectivity dataset, Chemma achieves an R 2 of 0.89 and an RMSE of 0.25 kcal mol −1 , compared with the R 2 of 0.915 and the RMSE of 0.197 kcal mol −1 reported by Li et al. 55 .Notably, the only input to our model is the SMILES expression of the molecules, without the selection of physical organic features or DFT descriptors, suggesting that Chemma has learned representation of molecules equipped with chemical knowledge.</p>
<p>In addition, we further evaluate the performance of Chemma for enantiomeric-excess prediction.We focus on the enantioselective addition of diethyl zinc to benzaldehyde in the presence of a racemic catalyst and an enantiopure chiral additive.Chemma outperforms all baseline models, indicating superior accuracy in predictions (Supplementary Note 5.2).</p>
<p>Chemma-generated data benefits yield prediction and reaction optimization</p>
<p>We first examine 'Can Chemma-generated data improve the yield prediction of an RF model driven by DFT descriptors?' 54 .To this end, we randomly allocate a 10% subset of datasets as the test set.For the other 90% training data, we presume that only a fraction of the data can be observed (for example, 5%) and utilize Chemma to complete their yields (for example, 85%).The seven scenarios are illustrated in Fig. 4a.We use the RF and DFT descriptors to build a yield prediction model 54 and select two HTE datasets for testing, the Pd-catalysed cross-coupling Suzuki-Miyaura reaction 61 and the Pd-catalysed Buchwald-Hartwig reaction 54 .</p>
<p>Prepare input data based on reaction condition space:</p>
<p>Step 1: Using Chemma to predict yields for the entire reaction condition space We randomly allocate a 10% subset of datasets as the test set.We design seven scenarios for evaluation using varying fractions of real and synthetic data.For the other 90% training data, we presume that only a fraction of data can be observed (for example, scenario one, 5%) and utilize Chemma to complete their yields (for example, scenario one, 85%).b,c, Performance of yield prediction with varying fractions of real and generated data on the Suzuki-Miyaura (b) and Buchwald-Hartwig (c) reactions.Prediction error converges to −0.05 to 0.05.We set the value of uncertainty as 0.5.It is expected that the actual values will fall within the shaded area in a specified 95% proportion of cases.The Chemma-enhanced RF approach takes both real and generated data for training; the RF without enhancement approach utilizes only the real data.d, Workflow of Chemma-BO for reaction optimization.In contrast to classic BO, we use the Gaussian process surrogate model to fit the gap between the collected observation and the predicted yield by Chemma.e,f, Averaged cumulative maximum observed yield with three methods-Chemma-BO, GPT-4 and BO-on the Suzuki-Miyaura (e) and Buchwald-Hartwig (f) reactions.Initial experimental conditions are chosen randomly and five experiments are conducted per batch.</p>
<p>Article</p>
<p>https://doi.org/10.1038/s42256-025-01066-y</p>
<p>Figure 4b,c compares the performance of yield prediction with varying fractions of real and generated data on the Suzuki-Miyaura and Buchwald-Hartwig reactions, respectively.The Chemma-enhanced RF approach takes 90% of the data for training, including both real and generated data; the RF without enhancement approach utilizes only the partially observed real data for training; the horizontal green line indicates the performance reported in ref. 54 using RF with 90% real data.It is clear that Chemma-enhanced RF outperforms RF without enhancement particularly when the fraction of real data is as low as a mere 5%.Notably, Chemma-enhanced RF achieves an R 2 value of 0.53 (0.72) on the Suzuki-Miyaura (Buchwald-Hartwig) reactions even when utilizing only 5% real data, which is closely approaching 0.6 (0.8) using RF with 90% real data.</p>
<p>In other words, the results substantiate that the generated data present an advantageous alternative for experimental observations, which is particularly important for few-shot learning scenarios.Furthermore, with reliable generated data, Chemma can enhance chemists' prior knowledge of the yield distribution within the reaction space.Such prior insights are of significant value in the optimization of reactions.In view of this, we also examine 'Can Chemma-generated data improve the efficiency of reaction exploration with BO?' .On this basis, we propose Chemma-BO, a modified BO framework, integrating the Chemma-generated data.Chemma-BO is designed based on the BO framework reported in Shields et al. 37 .The detailed workflow of Chemma-BO is represented in Fig. 4d and Methods.Figure 4e,f presents the cumulative maximum observed yields for optimizations using three distinct approaches, Chemma-BO, GPT-4 and BO, on both the Suzuki-Miyaura and Buchwald-Hartwig reactions.The curves show the average performance of experiments repeated ten times.Five experimental runs are conducted in each batch.In addition, we also compare the capability of GPT-4 as an agent for reaction optimization, as reported by Boiko et al. 26 .</p>
<p>For the Suzuki-Miyaura reaction (Fig. 4e), compared with BO, Chemma-BO obtains more significant observed yields despite them having similar initial conditions.Furthermore, Chemma-BO achieves 98.5% yield within the first 15 experiments (3 batches), while both BO and GPT-4 require at least 50 experiments to achieve similar results.For the Buchwald-Hartwig reaction (Fig. 4f), Chemma-BO surprisingly achieves 98.7% within the first 10 experiments.Within the first 25 experiments, Chemma-BO achieves 99.8% yield, while BO requires at least 50 experiments to achieve similar performance.It is worth noting that GPT-4 is unable to identify an acceptable reaction condition after 100 experiments.Ultimately, GPT-4's averaged maximum yield is limited to approximately 93%, which is significantly lower than the maximum yields of Chemma-BO or BO.</p>
<p>The suboptimal performance of GPT-4 can be attributed to that GPT-4 is best at general tasks that involve natural languages.However, reaction optimization is a complex task that depends not only on general knowledge but also on understanding the underlying chemical rules and principles 30,62 .From the experimental results, we note that the GPT-4's performance on the Buchwald-Hartwig reaction is much weaker than for the Suzuki-Miyaura reaction.The primary reason is that the Buchwald-Hartwig reaction includes more uncertain variables such as reactants, conditions and products than the Suzuki-Miyaura reaction.Given this complexity, prior knowledge provided to GPT-4 may not be sufficient to understand this type of reaction.In contrast, Chemma-BO and BO can still work on the Buchwald-Hartwig reaction due to the introduction of DFT descriptors.Moreover, we can see that Chemma has the potential to hasten the optimization process of the baseline BO approach by introducing generated yields.</p>
<p>Exploring and optimizing open reaction spaces with Chemma</p>
<p>Exploring the reaction space is a pivotal task in organic chemistry synthesis.It holds the key to discovering reactions and optimizing the known condition space, leading to a deeper understanding of reaction mechanisms.Recent work has focused on reaction exploration within predefined conditions, also known as closed reaction spaces 14,37,39 .However, these methods heavily rely on the prior knowledge of experts, limiting the discovery of reactions.In recent years, LLMs have showcased their capabilities of hypothesis generating, and therefore have the potential to facilitate scientific discovery 18 .Thus, we investigate 'Can Chemma explore a reaction without predefined condition pools, also known as the open reaction space?'.</p>
<p>In response to this, we position Chemma as a chemistry assistant and integrate it into an active learning framework for reaction exploration and optimization in both closed and open spaces (Fig. 5a and Extended Data Fig. 7).To start the active learning loop, Chemma first generates the initial reaction conditions.Next, in the 'suggestion and feedback loop', chemists iteratively follow Chemma's suggestion and conduct wet experiments.After a round of experiments, if the target yield is not reached, chemists can fine-tune Chemma to start a new round of exploration.This allows Chemma to learn the chemical knowledge of the new reaction.The detailed workflow is presented in Methods.</p>
<p>Figure 5b,c illustrates the results of exploring ligands for the imidazole C-H arylation and Buchwald-Hartwig reactions via the active learning framework (Extended Data Fig. 2).Our objective is to explore a suitable ligand that yields a high outcome under various combinations of other conditions.The results reveal that an acceptable yield can be achieved in the first round of active learning (round 0).The exploration process for the Pd-catalysed imidazole C-H arylation and Buchwald-Hartwig (HTE) reactions is presented in Extended Data Fig. 8.The detailed Q&amp;As between the chemist and Chemma are presented in Supplementary Fig. 3.</p>
<p>For the Pd-catalysed imidazole C-H arylation reaction (Fig. 5b), Chemma generates a ligand, XPhos, under the combination of caesium acetate (CsOAc)-pentanenitrile (BuCN) using zero-shot prompts for the first experimental run with a yield of 76.63%.In the second run, keeping CsOAc-BuCN unchanged, we instruct Chemma with in-context learning (ICL) prompts to generate a 'higher-yield' ligand.As a result, Chemma proposes another ligand, 1,3,5,7-tetramethyl-2,4,8-trioxa-6-phospha-adamantane (CgMe-PPh), which impressively produces a higher yield of 91.19%.As Chemma generates consistent CgMe-PPh across multiple attempts by ICL prompts, we proceed to randomly select a new base-solvent combination for the next round of ligand exploration.In the fourth run, Chemma suggests CgMe-PPh under the combinations of potassium pivalate (KOPiv)-p-xylene, which achieves the yield of 100%.</p>
<p>For the Pd-catalysed Buchwald-Hartwig reaction (Fig. 5c), Chemma generates a ligand, bis(1-adamantyl)-[3,6-dimethoxy-2-[2,4,6-tri(propan-2-yl)phenyl]phenyl]phosphane (AdBrettPhos), under the combination of phosphazene base (P 2 -Et)-2,1-benzisoxazole using zero-shot prompts for the first experimental run.The achieved yield for this generated ligand XPhos is 4.95%.In the second run, keeping P 2 -Et-2,1-benzisoxazole unchanged, we instruct Chemma with ICL prompts to generate a 'higher-yield' ligand.As a result, Chemma proposes another ligand, di-tert-butyl(2′,4′,6′-tris(propan-2-yl)-(1,1′-biphenyl)-2-yl)phosphane (tBuXPhos), which produces a higher yield of 21.66%.In the 12th run, Chemma suggests tBuXPhos under the combinations of 7-methyl-1,5,7-triazabicyclo[4.4.0]dec-5-ene (MTBD)-isoxazole, which achieves the yield of 84.42%.Note that Chemma may propose conditions outside the closed reaction space.In such cases, we ask Chemma to iteratively regenerate a new ligand until it falls into the predefined space.We list several unreported ligands generated by Chemma in Extended Data Fig. 9 and synthesize three ligands (L1, L3 and L6) to verify their effectiveness.We observe that the yields of L1 and L3 are 6% and 16%, respectively, while the reactivity of the L6 remains notably weak.That is, Chemma faces challenges in generating ligand molecules with high reactivity for unseen reactions in its training stage.The active learning Article https://doi.org/10.1038/s42256-025-01066-ypipeline could be a solution, in which Chemma can be fine-tuned in the loop to enhance its capability to understand the reactions.</p>
<p>Furthermore, we utilize the active learning framework to explore the open condition space for an unreported reaction, the synthesis of α-aryl N-heterocycles, with the goal of maximizing the yield.α-Aryl N-heterocycles are prevalent structural motifs encountered in various natural products, pharmaceuticals, agrochemicals and chiral catalysts 63 .However, surprisingly, efficient methods to access α-aryl N-heterocycles through Suzuki-Miyaura cross-coupling of cyclic aminoboronates and aryl halides have not yet been reported.This underscores the considerable demand for the synthesis of α-aryl N-heterocycles, which are becoming areas of active research in organic chemistry 64,65 .</p>
<p>L1</p>
<p>Closed space exploration</p>
<p>Predefined conditions from diverse search spaces (also known as closed reaction space, for example, panels b and c) Within the predefined reaction space, we first construct zeroshot prompts and ask Chemma to recommend a suitable ligand.We conduct 16 experimental runs for both two reactions.d, Exploring the open reaction space with Chemma for an unreported reaction, synthesis of Pd-catalysed N-heterocycles.Nine ligands selected by experts' experience and Chemma's suggestion constitute the initial condition scope, defined as round 0. After that, we update Chemma by RLHF.We then ask Chemma to generate three possible ligands by interacting with chemists and conduct wet experiments in round 1. Next, in round 2, we test three more solvents suggested by Chemma via ICL prompts.e, Test for substrates' compatibility.PAd 3 /Pd-catalysed C-C coupling of (1-benzoylpiperidin-2-yl)boronic acid ester and diverse aryl bromides.The selected reaction conditions are 1,4-dioxane (2 ml), Cu 2 O and NaOH (0.6 mmol), performed at 120 °C.Credit: chemistry apparatus in a, Freepik.com.</p>
<p>Open space exploration</p>
<p>Article</p>
<p>https://doi.org/10.1038/s42256-025-01066-y</p>
<p>To synthesize this product, we explore the efficient ligands under the combinations of identical base-solvents within an open reaction space.The prompts that we interact with Chemma are shown in Extended Data Fig. 10.With the assistance of Chemma, we prioritize ligands because they often generate reactive intermediates with reactants (piperidin-1-yl benzoate), which affect the reactivity of aryl and nitrogen heterocycles coupling.Next, we expect to optimize the solvent with Chemma.</p>
<p>Specifically in Fig. 5d, (1-benzoylpiperidin-2-yl)boronic acid ester and 4-bromobiphenyl are selected as substrates, and the initial combinations of starting condition variables are selected by chemists' experience 66 , which include base (NaOH), solvents (p-xylene), ligands and additives (cuprous oxide, water).We also ask Chemma to suggest proper ligands for the given reactants and product.As presented in round 0 in Fig. 5d, L1 and L5 are proposed by Chemma, and the other seven ligands are selected by chemists.Unfortunately, all ligands L1-L9 show poor performance in the reaction, with yields consistently below 15%.</p>
<p>After iterative experiments in round 0, we fine-tune Chemma by constructing pair-wise Q&amp;A ranking datasets.Subsequently, in round 1, we ask Chemma to suggest three more potential ligands through a zero-shot interaction.Note that, following the suggestion of chemists, we change the solvent from p-xylene to 1,4-dioxane, because of the low yields in round 0. Chemma recommends the following three potential ligands: Brettphos phosphine (BrettPhos, L10), phosphorus (PhS-Phos, L11) and tri(1-adamantyl)phosphine (PAd 3 , L12).We find that PAd 3 (L12) performs better in the cross-coupling reaction, with a desired yield of 67%.Subsequently in round 2, further interacting with Chemma, we conduct more wet experiments with various solvents including 1,2-dimethoxyethane (DME), methyl cyanide (MeCN) and tetrahydrofuran (THF).However, we find that 1,4-dioxane adopted in the previous round performs best among all evaluated solvents.</p>
<p>Having established the acceptable conditions for substrates of aryl and nitrogen heterocycles, we test its efficiency on four more reactants, as presented in Fig. 5e.Regarding the scope of aromatic electrophiles, a wide range of aryl electrophiles, bearing both electron-donating and electron-withdrawing substituents, underwent C(sp 2 )-C(sp 2 ) coupling with (1-benzoylpiperidin-2-yl)boronic acid ester to afford final products in 45-67% isolated yields.In summary, the human-Chemma collaboration successfully explores suitable ligands and solvents within only 15 runs.The results confirm that Chemma can be utilized to explore suitable conditions in open reaction spaces, assisting chemists in synthesizing unreported compounds.</p>
<p>Discussion</p>
<p>In the traditional chemistry research paradigm, chemists always acquire synthesis experience through reading reactions in the literature.Inspired by this, we propose Chemma to learn the chemical reactions in the form of natural languages.We find that Chemma achieves state-of-the-art performance in multiple chemical tasks and can effectively instruct chemical synthesis experiments.Nevertheless, it is crucial to ensure the responsible development and use of LLM-based models.Thus, we discuss the limitations and unintended risks of Chemma and propose possible mitigation strategies.</p>
<p>Limitations</p>
<p>Although Chemma has been trained with a substantial amount of chemical reaction data, it is still a formidable task to tackle reactions with very limited observations.In this context, Chemma might generate suboptimal outcomes.To cope with this, developers can focus on improving the quality and diversity of the open-source chemical data and introducing abundant knowledge from experts into the LLM.</p>
<p>For an unreported reaction, Chemma is unable to generate effective answers without chemists' feedback (round 0 in Fig. 5d).In this case, we iteratively ask Chemma to generate the next reaction condition based on the feedback of the last wet experiment and fine-tune Chemma.After rounds of human-AI collaboration, Chemma can better understand the specific reaction.</p>
<p>Moreover, when tasked with specific chemical problems, Chemma may generate diverse responses, resulting in additional effort to select reasonable answers.To alleviate this issue, we propose standardized templates of prompts for each task, and well-defined protocols for data collection and analysis (Supplementary Fig. 3).We will also integrate other expert-designed tools to ensure the robustness and accuracy of its chemical analyses.</p>
<p>As an LLM, Chemma inevitably faces hallucination concerns.Hallucinations could be a consequence of LLMs extrapolating beyond their training data.Especially in the exploration of open reaction spaces, Chemma might propose non-viable synthesis routes, or recom mend incompatible conditions.To tackle this, Chemma should work as a copilot in the chemical laboratory governed by chemists.Furthermore, hallucinations could also be addressed by introducing more reliable knowledge, including chemists' experience or chemical mechanisms.However, some Chemma-generated hypotheses might be valuable, for example, for designing new catalysts or new routes for complex compounds (Extended Data Fig. 9).This could be an important subject for further study.</p>
<p>Unintended risks</p>
<p>While Chemma has demonstrated significant potential as a generative chemistry assistant, several unintended risks must be considered and managed.</p>
<p>One of the primary risks associated with Chemma is its potential misuse.Although Chemma is specifically trained to assist chemists, there remains a possibility that it could produce harmful compounds.For example, individuals with malicious intent might leverage Chemma's capabilities to synthesize toxins, illegal drugs or other hazardous materials.Therefore, in our released service (https://ai4chem.sjtu.edu.cn),we design hard-coded templates of prompts and instruction guidelines to ensure safety.Furthermore, we will incorporate automated chemical property screening algorithms to identify and block attempts to generate harmful compounds.In addition, we strongly recommend that the Chemma-generated protocols should be rigorously reviewed by experienced chemists before conducting any wet experiments.</p>
<p>Intellectual property is another concern for the responsible development of generative AI models such as Chemma.To address this issue, it is imperative to establish clear guidelines and policies concerning the ownership of synthesized chemical structures or materials generated by the models.We can introduce a comprehensive database comprising literature and patent information for post-processing.In addition, collaborating with legal professionals and industry stakeholders will aid in tackling these complex issues and instituting effective measures to safeguard intellectual property.</p>
<p>We strongly encourage users to critically evaluate the information provided by Chemma and corroborate it with established scientific literature and expert opinions to mitigate the risk of relying on a potentially biased generation.By integrating these approaches, developers can endeavour to minimize the impact of any deficiencies in Chemma's chemistry knowledge, thereby enhancing the overall effectiveness and reliability of LLM-powered chemistry assistants.</p>
<p>Outlook</p>
<p>In this paper, we present a fine-tuned LLM, Chemma, for organic synthesis chemistry.Compared with general-purpose LLMs, such as GPT-4, Chemma is designed as a generative chemistry assistant.Chemma is capable of human-AI interactions for primary tasks in chemistry, including retrosynthesis, reaction performance prediction, condition generation, and reaction exploration and optimization.These abilities have been assessed with open benchmark data and wet experiments.</p>
<p>Article</p>
<p>https://doi.org/10.1038/s42256-025-01066-y</p>
<p>In particular, Chemma is able to explore open reaction spaces through an active learning pipeline.For a previously unreported N-heterocycle Suzuki reaction, Chemma explored optimized conditions within 15 runs, achieving a yield at 67%.These capabilities underscore Chemma's design intent as a tool to facilitate and enhance the practice of synthetic organic chemistry.</p>
<p>Moreover, the chemical explanation of Chemma is another aspect worthy of investigation.Through pre-training on numerous literature corpus, and fine-tuning on specific chemical domains, Chemma can extract task-specific information from reactions, forming numerical features known as representations or embeddings.Next, we will investigate the downstream applications of embeddings for reactions or molecules, by explaining the chemical semantics of embeddings.</p>
<p>Overall, our work highlights the value of adapted LLMs for scientific research, opening up opportunities to accelerate organic chemistry synthesis.Moreover, by integrating this scientific tool with embodied AI, there would be a great opportunity to construct an autonomous chemical laboratory, accelerating the discovery of compounds and reactions.</p>
<p>Methods</p>
<p>In this section, we start by introducing the workflow of Chemma, highlighting its significance in the field of organic synthesis.We then provide a detailed introduction to the model structure and training strategy.Finally, we elaborate on our reaction optimization methods, including BO and the active learning framework, which collectively enhance the exploration and improvement of reaction conditions.</p>
<p>Workflow of Chemma</p>
<p>Here we complement some details of the workflow of Chemma, as delineated in Fig. 1.All the reaction data were collected from the literature, patents and HTEs.The details of the datasets for training can be found in Supplementary Note 1 and Extended Data Fig. 1.For the construction of prompts, we generated 2,000 question prompt templates for each task using GPT-4 to construct the supervised fine-tuning dataset.These templates were carefully designed to ensure diversity, consistency and completeness, providing a robust foundation for training the model.Details of the dataset introduction and construction process are provided in Supplementary Note 2, with representative examples of prompts provided in Supplementary Fig. 1.For the application of human-AI interaction, Chemma receives task-specific queries constructed via instruction prompts such as 'Please give me an optimized ligand of this reaction [Reactant1.Reactant2 &gt; &gt; Product]', and generates potential solutions.For the application of experiment assistant, Chemma is equipped with the capability to optimize reaction performance through the active learning framework.After optimizing Chemma to be adapted to the new reaction, we can interact with it by asking questions such as 'We already obtain the yield of this reaction is 11.94%, could you please give me some new ligands that potentially get higher yields?'.The framework works not only on predefined reaction spaces by experts, also known as the closed space, but also on the open reaction space where the conditions are not limited to experts' prior knowledge.</p>
<p>Model architecture</p>
<p>Here we introduce the details of the model architecture, as shown in Fig. 2. Chemma-SFT was achieved by fully fine-tuning the foundational LLaMA-2 model using datasets introduced in Supplementary Note 2. Chemma-RM is expected to predict the most effective conditions by learning the performance differences between various reactions.Thus, pair-wise ranking Q&amp;A datasets contain questions and answers with varying levels of preference.These levels of preference were determined based on reaction performance, including yield and selectivity.For example, given the Buchwald-Hartwig reaction (Fig. 2a), we employed two ligands XPhos and tBuXPhos, and the annotation was marked as 'XPhos &gt; tBuXPhos', indicating that XPhos is better than tBuXPhos for reaction optimization.Furthermore, we leveraged a reinforcement learning training strategy from human feedback and the proximal policy optimization algorithm to train Chemma-RM.The objective of Chemma-SFT is to minimize the cross-entropy loss between the actual next tokens and the predicted next tokens.The loss function for Chemma-RM is also based on cross-entropy, where comparisons serve as labels-specifically, the difference in rewards reflects the log odds that one response will be preferred over another by chemical experiment results.</p>
<p>Chemma-SFT and Chemma-RM can handle the generation tasks.For the regression tasks (performance prediction), we developed a two-stage procedure.In stage 1, we extract the embedding of a given reaction from the last hidden-state layer of Chemma-SFT.For stage 2, we adopt a five multilayer perceptron feed-forward neural network to take in the embeddings of reactions and predict their yields or selectivities.We utilize the mean squared error function as a regression loss to optimize the networks.</p>
<p>For Chemma's training strategy, we performed full fine-tuning of the LLaMA-2-7B model for 4 epochs over approximately 72 hours using 8 × NVIDIA A800 graphics processing units (GPUs).For multi-GPU training, we employed the distributed data parallel training strategy.The detailed architecture of Chemma and training strategies is shown in Supplementary Note 2.</p>
<p>Implementation of Chemma-BO</p>
<p>For a given reaction search space, BO begins by collecting initial reaction yield data through an HTE platform.These data are then used to train a probabilistic surrogate model.Once the surrogate model is trained, new experiments in the reaction space are sequentially selected by optimizing an acquisition function designed to maximize the expected utility of candidate experiments for subsequent evaluation.The proposed experiments are then conducted and the surrogate model posterior is updated.This process continues iteratively until the reaction yield is maximized, the resources are depleted or the space is explored to the degree that finding improved conditions is improbable.</p>
<p>Using the predefined settings of BO, our objective was to enhance reaction performance by adapting algorithm components critical to maximizing yield in reaction optimization.First, we utilized Chemma to generate the yields for all reactions with varying conditions.Then, we selected the top-five conditions based on Chemma-generated yields for chemical experimental validation.Notably, we directly acquired the observed yields of certain conditions from HTEs for validating the performance of BO.However, during new experimental explorations, wet lab experiments are necessary to collect the corresponding yield data.In the follow-up steps, we built a probabilistic surrogate model, for example, Gaussian process, to fit the gap between the previously observed and Chemma-generated yields with the DFT descriptors.With the trained Gaussian process, we then updated the predicted yields in the entire reaction space by adding the output of the Gaussian process to Chemma-generated yields.Next, we selected the top-5five conditions to rerun experiments and repeat the above steps until acceptable yields were reached.In summary, the classic BO utilizes a surrogate model to fit the observed yields, while the bias of Chemma-generated yields is fitted in Chemma-BO.</p>
<p>Active learning framework of Chemma for reaction optimization</p>
<p>We position Chemma as a chemistry assistant and integrate it into an active learning framework for reaction exploration and optimization (Extended Data Fig. 7).Given a new reaction, chemists utilize instruction prompts to interact with Chemma within two reaction condition scenarios: (1) reaction space with predefined or condition compounds, also known as the closed reaction space; (2) selecting some initial conditions as the first try without limiting the reaction space, also Article https://doi.org/10.1038/s42256-025-01066-yknown as the open reaction space.In Extended Data Fig. 7, we ask Chemma to generate initial conditions by zero-shot prompts demonstration as round 0. Subsequently, we utilize generated conditions from Chemma to conduct the wet experiment and obtain observed yields.If experiment queries are reached, chemists can fine-tune Chemma with collected experimental data, which is stored in the ELN, and start a new round of exploration and optimization, this is, rounds 1 − N. In round 1 − N, we ask Chemma to recommend new conditions and obtain the observed yield through the wet experiment.If chemists are unable to achieve target performance or gather sufficient data in this round, they can continually generate new conditions.Alternatively, if they obtain ample experimental data without reaching promising performance, they can leverage experimental records to fine-tune Chemma and start a new round.Once chemists obtain acceptable performance, the optimization process is finished.In addition, the detailed prompts are shown in Extended Data Fig. 10 and Supplementary Fig. 3.</p>
<p>Inclusion and ethics</p>
<p>All contributors who fulfil the authorship criteria are listed as co-authors in this paper.Other contributors who do not meet all criteria for authorship are listed in 'Acknowledgements'.</p>
<p>Reporting summary</p>
<p>Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.Step-3 Rank-1</p>
<p>Step-5</p>
<p>Rank-1
HN N S O O N F O S O N N O F O HN F O S O Cl N HN F O O unknown
Step-2, Rank-1</p>
<p>Step-1 unknown</p>
<p>Step-3 Step-1, Rank-1</p>
<p>Reductive amination</p>
<p>Step-2, Rank-1</p>
<p>Step-3, Rank-1</p>
<p>Sulfonylation</p>
<p>Pirtobrutinib (vs Barreca et al., European Journal of Medicinal Chemistry, 2024)</p>
<p>Step-1, Rank-1</p>
<p>Step-2, Rank-1</p>
<p>Step-3, Rank-1</p>
<p>Step-4, Rank-1</p>
<p>Step-5, Rank-1</p>
<p>Step-6, Rank-1</p>
<p>Ritlecitinib (vs Wang et al., European Journal of Medicinal</p>
<p>Acylation</p>
<p>Step-1</p>
<p>Step-2</p>
<p>Step-3, Rank-1</p>
<p>Step-4, Rank-1</p>
<p>Step-5, Rank-1</p>
<p>Step-6, Rank-1</p>
<p>Extended Data Fig.</p>
<p>base (46), oxidant (10)  additive (32), solvent     Taking imidazole C-H arylation reaction as a example, for each optimized process, the initial solvent-base variables are randomly selected from the entire reaction space.We interact with Chemma by zero-shot prompts to acquire a suitable ligand.Initial generation provides a preliminary exploration of the reaction space.Subsequently, we leverage both the observed yield from the initial exploration and the generated ligand to construct the ICL prompts and ask Chemma to suggest a 'higher-yield' ligand.If the suggested ligand has been tested in previous runs, we randomly change the reaction condition variables except the ligand for the next group of zero-shot interaction and experimental runs.</p>
<p>https://doi.org/10.1038/s42256-025-01066-y</p>
<p>Fig. 1 |
1
Fig.1| Functions and applications of Chemma.a, Illustration of using Chemma to accelerate organic chemistry synthesis.Scientists can interact with Chemma about four main tasks, including forward prediction, retrosynthesis, condition generation and performance prediction (for example, yields and selectivities).We also present an active learning framework, using Chemma as an experiment assistant, to accelerate the exploration of new reaction spaces.b, Prompt templates for the preparation of Q&amp;A pairs.The construction of supervised</p>
<p>ith token given k preceding tokens and model paremeters</p>
<p>Fig. 2 |
2
Fig. 2 | Model architecture of Chemma and training strategies for chemical tasks.a, Model architecture of Chemma.We first develop Chemma-SFT by fine-tuning the base LLaMA-2-7b model with multi-task Q&amp; A datasets, including forward prediction, retrosynthesis and condition generation.Specifically for the condition generation, we further derive Chemma-RM for reaction optimization.RLHF, reinforcement learning from human feedback.b, Two-stage training strategy for reaction performance prediction.In stage 1, we extract the embeddings of reactions in a well-trained Chemma-SFT.In stage 2, we employ multilayer perceptron networks for regression tasks, including yield and selectivity prediction.xN, number of hidden layers; ℒ, objective function for LLM; , matrix of the entire sequence of word tokens; u, tokens preceding the ith position; N, length of a sequence of word tokens; Θ, the parameters of LLM.</p>
<p>Fig. 3 |
3
Fig. 3 | Performance evaluation of Chemma's capabilities for different organic synthesis tasks using both open benchmark and HTE datasets.a, Performance comparison of the one-step retrosynthesis task on the USPTO-50k test set.For a fair comparison, all of the competitive methods including Chemma are trained with the same USPTO-50k training set.The bars depict the top-one accuracy of retrosynthesis and are colour-coded by categories of the methods.b, General schemes of the three Pd-catalysed reactions tested for condition recommendation and yield prediction.They are Suzuki-Miyaura 61 , Buchwald-Hartwig 68 and imidazole C-H arylation 39 reactions.c, Performance of ligand recommendation by Chemma on Pd-catalysed C-H arylation reaction.Given a reaction and its solvent and reagent, Chemma recommends a suitable ligand.Considering various solvents and bases in Suzuki-Miyaura datasets, we depict the yield distribution for each base-solvent-ligand (B-S-L) combination.The</p>
<p>Step 2 :Fig. 4 |
24
Fig.4| Value of Chemma-generated data for enhancing yield prediction and reaction optimization.a, Data organization for yield prediction.We randomly allocate a 10% subset of datasets as the test set.We design seven scenarios for evaluation using varying fractions of real and synthetic data.For the other 90% training data, we presume that only a fraction of data can be observed (for example, scenario one, 5%) and utilize Chemma to complete their yields (for example, scenario one, 85%).b,c, Performance of yield prediction with varying fractions of real and generated data on the Suzuki-Miyaura (b) and Buchwald-Hartwig (c) reactions.Prediction error converges to −0.05 to 0.05.We set the value of uncertainty as 0.5.It is expected that the actual values will fall within</p>
<p>nFig. 5 |
5
Fig. 5 | Illustrations of using Chemma to explore reaction spaces.a, Overview of active learning framework for reaction exploration and optimization driven by Chemma.Given a new reaction, Chemma can work in two scenarios: (1) predefining the reaction space and (2) the open reaction space.Next, in the 'suggestion and feedback loop', chemists follow Chemma's suggestion to conduct wet experiments.The results are stored in ELNs.After a round of experiments, if the target yield is not reached, chemists can fine-tune Chemma and start a new round of exploration.b,c, Optimization of ligand for the Pdcatalysed imidazole C-H arylation (b) and Buchwald-Hartwig (c) reactions on HTE data.Within the predefined reaction space, we first construct zeroshot prompts and ask Chemma to recommend a suitable ligand.We conduct</p>
<p>Extended Data Fig. 1 |. 2 |
12
Distribution of types of reactions in the USPTO-50k and ORD datasets.(a) Data organization of USPTO-50k and ORD datasets.All of reactions in USPTO-50k are from patents in the United States; Most of reactions in ORD are from literature.(b-c) Power law fitting of the reactant distribution in the USPTO-50k and the catalyst distribution in the ORD, where the shallow points show the probability density and the deep dashed-line shows the ideal power-law fitting, respectively.(d-e) The bar charts of fifteen most common reactants and catalysts in the USPTO-50k and ORD, respectively.The shallow color presents the decimal-scale proportion and the deep color presents the log-scale count.https://doi.org/10.1038/s42256-025-01066-yDescription of the three HTE datasets.(a) Pd-catalysed Buchwald-Hartwig C-N coupling reaction: aryl halides, isoxazole additives, Pd precatalyst, ligands and bases.(b) Suzuki-Miyaura reaction, consisting of the reaction yield measured as a function of boronic acid derivative, aryl halide, ligand, base and solvent.(c) C-H arylation dataset components: ligands, imidazoles, aryl bromides.</p>
<p>. 4 |Extended Data Fig. 4 |
44
See next page for caption.Assessment of yield prediction performance by RF model and Chemma on two HTE reactions: Suzuki-Miyaura and Buchwald-Hartwig.(a-c) The distribution of the yields for Pd-</p>
<p>. 5 |
5
Performance evaluation of Chemma for yield prediction with literature-derived Pd-catalyzed homogeneous carbonylation reactions.(a) Proportions of the top five most frequently used catalyst precursors, ligands, bases, oxidants, additives, and solvents in the data set.(b) Schematic representation and yield distribution of the Pd-catalyzed carbonylation reaction.(c)Predictive results of the Chemma.We assess the generalization capability of Chemma in the out-of-sample testing strategy.Within the training set, we select literature IDs &lt; 100, while the testing set exclusively included data from the original testing set with literature IDs &gt; 100, challenging the Chemma to predict reaction performance in chemical spaces it has not previously encountered.</p>
<p>. 6 |
6
Case study and visualization of selectivity prediction performance.(a-c) Successful case: comparison of sites predictions and experimentally determined for products.For each reaction, the predicted site is exactly same with the labeled one.Reactive sites of products are highlighted with a circle.(d) Illustration of failure prediction results.The measured probability of the target product with N m-meta site is 0.7652, but the predicted result is 0.6016 with N o-ortho site.https://doi.org/10.1038/s42256-025-01066-yZero-shotprompt demonstration ( Instruction + Contents)In-context learning (ICL) prompt demonstrationare an experienced chemcial assistant.Considering a chemical reaction, SMlLES is sequenced-based string used to encode the molecule structure.A chemical reaction includes reactants, conditions and products.Thus, reactants for this reaction are Cn1cnc(C#N)c1.Fc1ccccc1Br, SMILES forproducts of reactions are N#CC1=C(C2=CC=CC=C2F)N(C)C=N1, the reaction can be described as Cn1cnc(C#N)c1.Fc1ccccc1Br&gt;&gt;N#CC1=C(C2=CC=C-C=C2F)N(C)C=N, base for this reaction is [Cs+].CC(=O)[O-], solvents for this reaction is Cciccc(C)cc1, Could you o er any recommendations for catalysts that could have been used in this speci c chemical reaction?Input: Cn1cnc(C#N)c1.Fc1ccccc1Br&gt;&gt;N#CC1=C(C2=CC=C-C=C2F)N(C)C=N1 Output:A speci c example for zero-shot promptInstruction prompts for C-H Arylation and Buchwald-Hartwig reactionsChemist: You are an experienced chemcial assistant.Considering a chemicalreaction, SMlLES is sequenced-based string used to encode the moleculaistructure.A chemical reaction includes reactants, conditions and products.Thus, reactants for this reaction are Cn1cnc(C#N)c1.Fc1ccccc1Br, SMILES forproducts of reactions are N#CC1=C(C2=CC=CC=C2F)N(C)C=N1, the reaction can be described as Cn1cnc(C#N)c1.Fc1ccccc1Br&gt;&gt;N#CC1=C(C2=C-C= C-C=C2F)N(C)C=N1, reagents for this reaction is [Cs+].CC(=O)[O-], catalyst for this reaction is CC(C)c1cc(C(C)C)c(-c2ccccc2P(C2CCCC C2)C2CCCCC2)c(C(C)C)c1, product yield for this reaction is 76.63%, could you o er any recommendations for another catalysts that could have been used to get high yield in this speci c chemical reaction?Input: Cn1cnc(C#N)c1.Fc1cccc1Br &gt;&gt; N#CC1=C(C2=CC=C-C=C2F)N(C)C=N1 Output: CC(C)c1cc(C(C)C)c(-c2ccccc2P(C2CCCCC2)C2CCCCC2)c(C(C)C)c1 Input: Cn1cnc(C#N)c1.Fc1cccc1Br &gt;&gt; N#CC1=C(C2=CC=C-C=C2F)N(</p>
<p>. 8 |
8
Illustration of detailed prompts for optimizing reactions including Pd-catalysed imidazole C-H arylation reaction, Pd-catalysed Buchwald-Hartwig reaction, and our unreported synthesis of aryl-substituted reaction of nitrogen heterocycles, respectively.(a) The detailed prompts used for optimizing conditions of imidazole C-H arylation and Buchwald-Hartwig reactions.(b-c) Details of reaction optimization process.A total of 16 experiments are conducted in s closed-loop fashion continuously.</p>
<p>catalysed Suzuki-Miyaura [HTE], Buchwald-Hartwig [ELN], and imidazole C-H arylation [HTE] reactions.(d,e)Test set performance of the RF model and Chemma with randomly split strategy.A gradual erosion in predictive accuracy occurred from 90% of the entire dataset down to 5% of the full data set.(f, g) Test set performance of the RF and Chemma when training and test sets are split by diverse substrate scopes.For Suzuki-Miyaura reaction, all reactions can be split by twenty different kinds of substrates.For the Buchwald-Hartwig reaction, all reactions are split by fifteen aryl chloride substrates.We define four scenarios for evaluation characterized by variable training and testing substrates.For instance, a scenario encompasses reactions involving eight substrates in the training phase and reactions with four substrates for testing.(h, i) Test set performance of the RF model and Chemma by isolating conditions sets.For the Suzuki-Miyaura reaction, all reactions are split by eleven different kinds of ligands.For the Buchwald-Hartwig reaction, we select four case scenarios that had been tested in for evaluation.Training and testing sets are divided with additives scopes.Accordingly, in Case-1, tested additives are enumerated by indices a10, a18, a15, a23, and a4; in Case-2, indices a11, a9, a1, a17 and a5 are selected for testing; in Case-3, indices a14, a8, a21, a12, a6 are selected for testing; in Case-4, indices a16, a2, a22, a20, a3 are selected for testing.(j-k) The distribution of free energy barriers on two reactions: chiral phosphoric acidcatalyzed thiol addition and radical C-H functionalization reaction.(l) Chemma's performance on two selective datasets.Each training sets with different proportions are selected randomly from the original full data set, and 30% data of the entire datasets are randomly selected as test sets.
https://doi.org/10.1038/s42256-025-01066-yAc 2 O 20.2%15.1%OthersBuPAd17.3%3.4%PdCl 2 (PPh 3 ) 2TBAI8.9%49.8%OthersPd(OAc) 261.6%7.2% 4.4%PdCl 2 Pd(TFA) 2PPh14.5%41.7%OthersDCCPhI(OAc) 2 PivOH 5.1% 7.2% 8.8%8.3%[Pd(cinnamyl)Cl] 2Xantphos 9.7% 9.2% DPPPPCy 7.6%Others11.5%3.9% AgTFAtoluene22.5%32.9%OthersNEt28.4%Cu(OAc) 243.7%6.3% 6.1%CuBr CuI36.6% OthersDMF17.0%6.2%K CO9.7%28.5%1,4-BQ11.7% dioxane9.8% THFMeCN9.3% 8.8% DBU Na CO7.2% TMEDA<a href="19">Pd</a>, ligand400R X[CO]Nu-HR 2 = 0.75Count100 200 300Predicted yield (%)0 20 40 60 80 100RMSE = 16.1600204060801000 20 40 60 80 100Yield (%)Observed yield (%)</p>
<p>Zero-shot 8 ICL Optimization results for C-H Arylation reaction Optimization results for Buchwald-Hartwig reaction Yield (%)
QueryBaseAdditivesLigandsStatusZero-shot1B1A1L14.95B1S3L291.191 ICL2B1A1L221.66B4S1L148.34Zero-shot3B1A2L125.99B4S1L21002 ICL4B1A2L265.48B4S2L179.76Zero-shot5B2A3L141.28B4S2L275.353 ICL6B2A3L266.39B3S1L188.41Zero-shot7B2A4L131.02B3S1L298.374 ICL8B2A4L251.21B3S2L178.59Zero-shot9B2A2L119.53B3S2L280.865 ICL10B2A2L250.5B3S4L176.24Zero-shot11B3A3L182.64B3S4L252.476 ICL12B3A3L284..42B3S3L167.04Zero-shot13B3A4L139.71B3S3L260.297 ICL14B3A4L278.7B2S1L182.2815B3A1L136.35B2S1L299.8116B3A1L260.85ICL: In-context learning promptICL: In-context learning prompt</p>
<p>Generated by Chemma Notes: ligands designed by Chemma has not been tested by wet experiments.
Buchwald-Hartwig [HTE] reactionRX+MeNH 2DMSO (0.1 M), 16h Pd catalyst (10 mol%) Base (1.5 equiv) Additive (1 equiv)MeH NROMeOMeReported fromP(t-Bu) 2MeOP(t-Bu) 2PCy2MeOPAd 2Ahneman et.al.,i-Pri-Pri-Pri-Pri-Pri-Pri-Pri-PrScience, 2018i-Pri-Pri-Pri-Prt-But-But-BuPt-BuPhPOPPh(t-Bu) 2 POPCyOPCyP(t-Bu) 2t-BuPt-But-But-But-But-BuL1L2L3L4L5L6L7</p>
<p>Extended Data Fig. 9 | The new ligands designed by Chemma for the Pd-catalysed Buchwald-Hartwig reaction.</p>
<p>For a reported HTE reaction, we ask Chemma to design new ligands (L1 to L7) that have not been explored.It is worth noting that we synthesize L1, L3, and L6, and conduct wet experiments to evaluate the effectiveness of the generated ligands.The yield of L1 and L3 is 6% and 16%, respectively.L6 exhibits no reactivity.</p>
<p>AcknowledgementsWe thank K. Ding for valuable discussion on the design of this work and the SJTU AI for Science platform for computing support.This work was jointly supported by the Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), the National Natural Science Foundation of China (62102258) and the Fundamental Research Funds for the Central Universities.Data availabilityAll the source data used for model training were from ORD42and USPTO44.The HTE data for the Suzuki-Miyaura reaction and the Pd-catalysed Buchwald-Hartwig reaction were collected from refs.54,61, respectively.Literature data for the Pd-catalysed carbonylation reactions were released by ref.58.Regioselectivity and enantioselectivity data were collected from refs.59,60.Code availabilityThe source code and inference for Chemma is available via Zenodo at https://doi.org/10.5281/zenodo.15295848(ref.67).Chemma is available for free usage at https://ai4chem.sjtu.edu.cn/.Article https://doi.org/10.1038/s42256-025-01066-yAuthor contributionsY.X. and Y.Z.conceived of the research and designed the analyses.Y.Z.designed and implemented the Chemma model.Y.H., S.C., F.Z. and Y.Z.performed the wet experiments.Y.Z., Y.X., R.Y., X.Z., X.L. and K.Z.processed the data and performed the results analyses.M.Y. and J.T. discussed the model design.Y.J. and X.Y. built the computing platform for model training.Y.Z., Y.X. and F.Z. wrote the paper.Y.X., F.Z., Y.J. and X.Y. supervised the research.Competing interestsThe authors declare no competing interests.Peer review information Nature Machine Intelligence thanks Victor Batista and the other, anonymous, reviewer(s) for their contribution to the peer review of this work.Reprints and permissions informationStep-1Rank-1Step-2Rank-1HNStep-4Rank-1 In round 0, Chemma iteratively suggests the next reaction condition considering the feedback of last wet experiment.After a round of 'suggestion-feedback loop'.In rounds 1-N, we fine-tune Chemma to adapt the reactions.The framework works not only on pre-defined reaction spaces by experts, but also on open reaction space where the conditions are not limited to experts' prior knowledge.Credit: chemistry apparatus icons, Freepik.com.
Scalable enantioselective total synthesis of taxanes. A Mendoza, Y Ishihara, P S Baran, Nat. Chem. 42012</p>
<p>The past, present and potential for microfluidic reactor technology in chemical synthesis. K S Elvira, X C Solvas, R C Wootton, A J Demello, Nat. Chem. 52013</p>
<p>Chemistry: why synthesize?. P Ball, Nature. 5282015</p>
<p>Univariate classification of phosphine ligation state and reactivity in cross-coupling catalysis. S H Newman-Stonebraker, Science. 3742021</p>
<p>Computational planning of the synthesis of complex natural products. B Mikulak-Klucznik, Nature. 5882020</p>
<p>Leveraging large language models for predictive chemistry. K M Jablonka, P Schwaller, A Ortega-Guerrero, B Smit, Nat. Mach. Intell. 62024</p>
<p>Automation and computer-assisted planning for chemical synthesis. Y Shen, Nat. Rev. Methods Primers. 1232021</p>
<p>Nanoparticle synthesis assisted by machine learning. H Tao, Nat. Rev. Mater. 62021</p>
<p>Scaling deep learning for materials discovery. A Merchant, Nature. 6242023</p>
<p>A mobile robotic chemist. B Burger, Nature. 5832020</p>
<p>Closed-loop optimization of general reaction conditions for heteroaryl Suzuki-Miyaura coupling. N H Angello, Science. 3782022</p>
<p>A data-driven workflow for assigning and predicting generality in asymmetric catalysis. I O Betinol, J Lai, S Thakur, J P Reid, J. Am. Chem. Soc. 1452023</p>
<p>A machine-learning tool to predict substrate-adaptive conditions for Pd-catalyzed C-N couplings. N I Rinehart, Science. 3812023</p>
<p>Controlling an organic synthesis robot with machine learning to search for new reactivity. J M Granda, L Donina, V Dragone, D.-L Long, L Cronin, Nature. 5592018</p>
<p>A universal system for digitization and automatic execution of the chemical synthesis literature. S H M Mehr, M Craven, A I Leonov, G Keenan, L Cronin, Science. 3702020</p>
<p>Digitization and validation of a chemical synthesis literature database in the ChemPU. S Rohrbach, Science. 3772022</p>
<p>Inverse molecular design using machine learning: generative models for matter engineering. B Sanchez-Lengeling, A Aspuru-Guzik, Science. 3612018</p>
<p>Scientific discovery in the age of artificial intelligence. H Wang, Nature. 6202023</p>
<p>Unassisted noise reduction of chemical reaction datasets. A Toniato, P Schwaller, A Cardinale, J Geluykens, T Laino, Nat. Mach. Intell. 32021</p>
<p>GPT-4 technical report. J Achiam, 10.48550/arXiv.2303.087742023Preprint at</p>
<p>ChatGPT as research scientist: probing GPT's capabilities as a research librarian. S A Lehr, A Caliskan, S Liyanage, M R Banaji, Proc. Natl Acad. Sci. USA. 121e24043281212024</p>
<p>ChatMOF: an artificial intelligence system for predicting and generating metal-organic frameworks using large language models. Y Kang, J Kim, Nat. Commun. 1547052024</p>
<p>Structured information extraction from scientific text with large language models. J Dagdelen, Nat. Commun. 1514182024</p>
<p>Assessing GPT-4 for cell type annotation in single-cell RNA-seq analysis. W Hou, Z Ji, Nat. Methods. 212024</p>
<p>A GPT-4 reticular chemist for guiding MOF discovery. Z Zheng, Angew. Chem. Int. Ed. 62e2023119832023</p>
<p>Autonomous chemical research with large language models. D A Boiko, R Macknight, B Kline, G Gomes, Nature. 6242023</p>
<p>Reproducibility in automated chemistry laboratories using computer science abstractions. R B Canty, M Abolhasani, Nat. Synth. 32024</p>
<p>An automatic end-to-end chemical synthesis development platform powered by large language models. Y Ruan, Nat. Commun. 15101602024</p>
<p>ChatGPT research group for optimizing the crystallinity of MOFs and COFs. Z Zheng, ACS Cent. Sci. 92023</p>
<p>Augmenting large language models with chemistry tools. A M Bran, Nat. Mach. Intell. 62024</p>
<p>ChatGPT chemistry assistant for text mining and the prediction of MOF synthesis. Z Zheng, O Zhang, C Borgs, J T Chayes, O M Yaghi, J. Am. Chem. Soc. 1452023</p>
<p>Crystal structure generation with autoregressive large language modeling. L M Antunes, K T Butler, R Grau-Crespo, Nat. Commun. 15105702024</p>
<p>Integrating machine learning and large language models to advance exploration of electrochemical reactions. Z Zheng, Angew. Chem. Int. Ed. 137e2024180742024</p>
<p>A review of large language models and autonomous agents in chemistry. M C Ramos, C J Collison, A D White, Chem. Sci. 162025</p>
<p>10.1038/s42256-025-01066-yArticle. </p>
<p>Planning chemical syntheses with deep neural networks and symbolic AI. M H Segler, M Preuss, M P Waller, Nature. 5552018</p>
<p>A robotic platform for flow synthesis of organic compounds informed by AI planning. C W Coley, Science. 36515662019</p>
<p>Bayesian reaction optimization as a tool for chemical synthesis. B J Shields, Nature. 5902021</p>
<p>Interrogating the mechanistic features of Ni (I)-mediated aryl iodide oxidative addition using electroanalytical and statistical modeling techniques. T Tang, J. Am. Chem. Soc. 1452023</p>
<p>Identifying general reaction conditions by bandit optimization. J Y Wang, Nature. 6262024</p>
<p>Dataset design for building models of chemical reactivity. P Raghavan, ACS Cent. Sci. 92023</p>
<p>Neural scaling of deep chemical models. N C Frey, Nat. Mach. Intell. 52023</p>
<p>The Open Reaction Database. S M Kearnes, J. Am. Chem. Soc. 1432021</p>
<p>Computerassisted retrosynthesis based on molecular similarity. C W Coley, L Rogers, W H Green, K F Jensen, ACS Cent. Sci. 32017</p>
<p>Extraction of Chemical Structures and Reactions from the Literature. D M Lowe, 2012University of CambridgePhD thesis</p>
<p>Permutation invariant graph-to-sequence model for template-free retrosynthesis and reaction prediction. Z Tu, C W Coley, J. Chem. Inf. Model. 622022</p>
<p>Molecule edit graph attention network: modeling chemical reactions as sequences of graph edits. M Sacha, J. Chem. Inf. Model. 612021</p>
<p>GTA: graph truncated attention for retrosynthesis. S.-W Seo, Proc. AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial IntelligenceAAAI Press202135</p>
<p>Learning graph models for retrosynthesis prediction. V R Somnath, C Bunne, C Coley, A Krause, R Barzilay, Adv. Neural Inf. Process. Syst. 342021</p>
<p>RetroPrime: a diverse, plausible and transformerbased method for single-step retrosynthesis predictions. X Wang, Chem. Eng. J. 4201298452021</p>
<p>Retroformer: pushing the limits of end-to-end retrosynthesis transformer. Y Wan, C.-Y Hsieh, B Liao, S Zhang, International Conference on Machine Learning 22475-22490. PMLR2022</p>
<p>Deep retrosynthetic reaction prediction using local reactivity and global attention. S Chen, Y Jung, JACS Au. 12021</p>
<p>Node-aligned graph-to-graph: elevating templatefree deep learning approaches in single-step retrosynthesis. L Yao, 2024JACS Au4</p>
<p>Prediction of organic reaction outcomes using machine learning. C W Coley, R Barzilay, T S Jaakkola, W H Green, K F Jensen, ACS Cent. Sci. 32017</p>
<p>Predicting reaction performance in C-N cross-coupling using machine learning. D T Ahneman, J G Estrada, S Lin, S D Dreher, A G Doyle, Science. 3602018</p>
<p>Reaction performance prediction with an extrapolative and interpretable graph model based on chemical knowledge. S.-W Li, L.-C Xu, C Zhang, S.-Q Zhang, X Hong, Nat. Commun. 1435692023</p>
<p>An autonomous laboratory for the accelerated synthesis of novel materials. N J Szymanski, Nature. 6242023</p>
<p>On the use of real-world datasets for reaction yield prediction. M Saebi, Chem. Sci. 142023</p>
<p>Challenges with literature-derived data in machine learning for yield prediction: a case study on Pd-catalyzed carbonylation reactions. D.-Z Li, X.-Q Gong, J. Phys. Chem. A. 1282024</p>
<p>Predicting regioselectivity in radical C-H functionalization of heterocycles through machine learning. X Li, S.-Q Zhang, L.-C Xu, X Hong, Angew. Chem. Int. Ed. 592020</p>
<p>Prediction of higher-selectivity catalysts by computer-driven workflow and machine learning. A F Zahrt, Science. 36356312019</p>
<p>A platform for automated nanomole-scale reaction screening and micromole-scale synthesis in flow. D Perera, Science. 3592018</p>
<p>What can large language models do in chemistry? A comprehensive benchmark on eight tasks. T Guo, Adv. Neural Inf. Process. Syst. 362023</p>
<p>Rings in drugs: miniperspective. R D Taylor, M Maccoss, A D Lawson, J. Med. Chem. 572014</p>
<p>A general approach to stereospecific cross-coupling reactions of nitrogen-containing stereocenters. X Ma, 20206</p>
<p>Modular access to chiral α-(hetero) aryl amines via Ni/photoredox-catalyzed enantioselective cross-coupling. X Shu, D Zhong, Y Lin, X Qin, H Huo, J. Am. Chem. Soc. 1442022</p>
<p>General and selective metal-free radical α-C-H borylation of aliphatic amines. S Sarkar, S Wagulde, X Jia, V Gevorgyan, 20228</p>
<p>Large language models to accelerate organic chemistry synthesis. Y Zhang, 10.5281/zenodo.152958482025</p>
<p>Applications of palladiumcatalyzed C-N cross-coupling reactions. P Ruiz-Castillo, S L Buchwald, 10.1038/s42256-025-01066-yChem. Rev. 1162016</p>
<p>Extended Data Fig. 10 | Illustration of detailed prompts for the expiration of the new reaction. We show the detailed prompts designed as input to the Chemma for optimizing the reported synthesis of aryl-substituted reaction of nitrogen heterocycles. </p>            </div>
        </div>

    </div>
</body>
</html>
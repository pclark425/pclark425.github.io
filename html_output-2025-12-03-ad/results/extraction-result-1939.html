<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1939 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1939</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1939</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-279074780</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.00411v1.pdf" target="_blank">LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks</a></p>
                <p><strong>Paper Abstract:</strong> Real-world embodied agents face long-horizon tasks, characterized by high-level goals demanding multi-step solutions beyond single actions. Successfully navigating these requires both high-level task planning (i.e., decomposing goals into sub-tasks) and low-level motion control (i.e., generating precise robot actions). While existing vision language action (VLA) models and hierarchical architectures offer potential in embodied tasks, the former often falter in planning, and the latter can suffer from coordination issues, both hampering performance. We introduce a new unified VLA framework for long-horizon tasks, dubbed LoHoVLA, to overcome these limitations. LoHoVLA leverages a large pretrained vision language model (VLM) as the backbone to jointly generate language and action tokens for sub-task generation and robot action prediction, respectively. This shared representation promotes better generalization across tasks. Additionally, LoHoVLA embraces a hierarchical closed-loop control mechanism to mitigate errors originating from both high-level planning and low-level control. To train LoHoVLA, we introduce LoHoSet, a dataset built on the Ravens simulator, containing 20 long-horizon tasks, each with 1,000 expert demonstrations composed of visual observations, linguistic goals, sub-tasks, and robot actions. Experimental results show that LoHoVLA significantly surpasses both hierarchical and standard VLA approaches on long-horizon embodied tasks in the Ravens simulator. These findings underscore the promise of unified architectures for advancing generalizable embodied intelligence.</p>
                <p><strong>Cost:</strong> 0.027</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1939.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1939.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LoHoVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unified vision-language-action model that uses a pretrained VLM backbone to jointly generate linguistic sub-task tokens and discrete action tokens, combined with a hierarchical closed-loop controller to solve long-horizon manipulation tasks in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LoHoVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Auto-regressive Transformer based VLA that uses a pretrained PaliGemma vision-language model as backbone to autoregressively produce both language tokens (sub-task descriptions) and discrete action tokens; actions are de-tokenized from 1024-bin discretization. Employs a hierarchical closed-loop control strategy that re-predicts actions more frequently than it re-plans sub-tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language on image-text pairs (pretrained PaliGemma backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not exhaustively detailed in paper; described as PaliGemma being a multi-modal foundation model pretrained on large-scale image-text corpora providing semantic priors (paper refers generally to internet-scale vision-language pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Long-horizon robotic manipulation / object rearrangement (LoHoSet in Ravens simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Simulation (Ravens) with a UR5e robotic arm + suction gripper performing 20 long-horizon tasks and 3 pick-and-place primitives; RGB and depth top-down orthographic observations; action space is continuous Cartesian end-effector pick/place commands discretized into 1024 uniform bins (recovered by de-tokenizer); scenes contain blocks, bowls, and zones with variations in color and size; simulated sensor noise and drop probability.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper claims pretrained VLM backbones provide rich semantic priors and grounding capabilities; semantic overlap between pretraining data and task objects/actions is not explicitly measured in the paper (alignment is asserted qualitatively rather than quantified).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>LoHoVLA (fine-tuned from PaliGemma) achieves the highest average score and success rate across nearly all evaluated tasks; example: on the reasoning-heavy task 'put-even-blocks-in-same-color-zone' LoHoVLA attains score 85.1 and success rate 81.0; achieves near-perfect accuracy on 'put-block-into-matching-bowl' (exact number not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>No explicit experiment with a non-language-pretrained backbone is reported. The paper does report a 'Vanilla VLA' baseline (trained without sub-task supervision) that performs poorly, with zero success rates on several tasks, but this baseline was trained on the same pretrained backbone (i.e., it is not a no-pretraining comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>The paper does not report explicit quantitative sample-efficiency comparisons (e.g., demonstrations-to-performance ratios) between language-pretrained and non-pretrained models. It reports dataset sizes used for training (1,000 demos per long-horizon task; additional 10,000 demonstrations per primitive task in stage two) and ablations indicating dataset expansion and two-stage training improve generalization and planning, but not sample-efficiency numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No explicit attention visualization or attention-pattern analysis is reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No explicit analysis of embedding space geometry, clustering, or semantic organization is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Mechanistic evidence: LoHoVLA maps visual features to the language model token space (via a linear projection) and represents actions as discrete tokens that are produced by the same autoregressive model—this joint token space and joint training of sub-task text and actions is presented as the grounding mechanism. Empirical evidence: superior task performance and improved sub-task planning success rate relative to baselines; however, no direct neural-level probes (e.g., causal interventions or representational probes) are provided to demonstrate how verbs/affordances are encoded.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>The paper provides behavioral evidence that hierarchical closed-loop control (different frequencies of action re-prediction vs. sub-task re-planning) mitigates errors from planning vs. low-level control; there is no layer-wise feature analysis separating low-level vs high-level representational benefits from language pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Paper reports that transfer/generalization improves with dataset expansion (adding 10 additional long-horizon tasks reduces overfitting). Fails when training set is small and scenes are similar (example: model overfits to frequent pattern and ignores language goal). Zone-match tasks tolerate spatial error and favor LoHoVLA, while pose-match tasks require precise motor control and are more sensitive to discretized action limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Paper evaluates seen vs unseen tasks (LoHoRavens unseen tasks) and reports LoHoVLA generalizes to unseen tasks and outperforms baselines despite no prior exposure; numeric per-object breakdowns for novel vs familiar object instances are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Demonstrates zero-shot generalization to unseen tasks in the LoHoRavens benchmark (paper states LoHoVLA consistently outperforms baselines on unseen tasks 'despite having no prior exposure'). No few-shot adaptation curves are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Training detail: image encoder and linear projection are kept frozen during fine-tuning while the language-model backbone is optimized (LoRA applied to linear layers). The paper does not provide a deeper ablation identifying which specific layers/components are most important for transfer beyond this freezing/fine-tuning choice.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Evidence of negative effects when not using sufficient task diversity: models fine-tuned on only the small set of seen tasks overfit (e.g., vanilla VLA and LoHoVLA trained without dataset expansion overfit to frequent patterns and ignore language goals). The discrete action tokenization is cited as a limitation reducing action precision (limiting performance on pose-match tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>No direct controlled comparison between vision-language pretraining and vision-only pretraining (e.g., ImageNet or self-supervised vision) is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Two-stage training (first optimize text loss for planning, then add primitives and action labels) yields higher sub-task planning success rate and higher task success than one-stage training; introducing action labels too early is reported to hinder effective optimization of high-level planning.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No explicit dimensionality or intrinsic-dimension analysis of learned representations is reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1939.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1939.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vanilla VLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vanilla Vision-Language-Action model (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline VLA that directly maps high-level goals and observations to low-level robot actions without producing intermediate sub-task language outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vanilla VLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Direct action-generation VLA that takes visual observations and language goals and predicts actions autoregressively; in the paper this baseline is trained on the same datasets as LoHoVLA but without sub-task (language) supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining (same pretrained backbone family used for LoHoVLA in the paper's comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not explicitly detailed for the baseline in the paper; baseline was trained on LoHoSet without sub-task labels but leveraging the same pretrained backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Long-horizon robotic manipulation / object rearrangement (LoHoSet in Ravens simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Same Raven simulator tasks as LoHoVLA: long-horizon pick-and-place and rearrangement tasks; receives RGB+depth observations and high-level language goal; outputs discretized action tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not directly analyzed. Paper attributes poor performance to the absence of explicit sub-task supervision leading to overfitting to frequent patterns (i.e., weaker alignment between language goals and action choices).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Trained on same pretrained backbone but without sub-task labels, the vanilla VLA performs worst among methods and has zero success rates on several tasks (qualitative statement in paper; exact aggregated numbers not fully enumerated in text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not applicable — the baseline itself is a variant that omits sub-task language supervision rather than language pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No explicit sample-efficiency numbers reported comparing this baseline to LoHoVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>No direct evidence; qualitative failure modes (e.g., placing blocks in incorrect bowls, ignoring goal conditions) are reported that suggest lacking grounded sub-task planning.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not applicable — this model does not represent hierarchical sub-tasks explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Paper shows vanilla VLA overfits to frequent training patterns and generalizes poorly to unseen tasks; training set diversity improves generalization for models that explicitly model sub-tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Baseline performs poorly on unseen tasks; specific numeric comparisons to familiar objects not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Does not demonstrate strong zero-shot generalization; reported as performing poorly on several unseen tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Reported systematic failure to follow language goals and overfitting to frequent patterns indicates negative outcomes when sub-task supervision is removed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>No direct comparison to vision-only models reported.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1939.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1939.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LoHoRavens</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LoHoRavens (hierarchical architecture baseline / benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical baseline architecture used for comparison, consisting of a Planner (LLM), Actor (controller), and Reporter (feedback module) supporting both explicit and implicit feedback loops for closed-loop control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Lohoravens: A long-horizon language-conditioned benchmark for robotic tabletop manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LoHoRavens (hierarchical baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Modular hierarchical architecture: Planner (LLaMA 2 13B) generates sub-tasks, Actor (CLIPort) executes low-level actions, Reporter either provides textual feedback via OpenFlamingo (explicit) or encodes observations via frozen CLIP and projects them to the Planner via an MLP and LLaVA (implicit).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Mixture: textual Planner uses text-only LLM pretraining; visual modules use vision-language pretraining (CLIP/OpenFlamingo) — as described in the paper's baseline setup.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in this paper beyond referring to the pretrained components (LLM and VLM) used by the baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Long-horizon robotic manipulation on LoHoRavens benchmark / LoHoSet tasks in Ravens simulator</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Same family of long-horizon rearrangement tasks in simulation; Planner outputs sub-tasks, Actor executes pick-and-place primitives, Reporter provides outcome captions or embeddings for closed-loop re-planning.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper discusses that modular baselines rely on pretrained LLM planning and VLM perception but suffer coordination overheads and mismatches between vision-language pretraining objectives and low-level control needs; no quantitative alignment measures provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported in paper as being outperformed by LoHoVLA on average; explicit and implicit feedback variants have differing performance (numbers are shown in the paper's Table 2 but aggregated textual summaries state LoHoVLA achieves higher average score and success rate).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported as explicit sample-efficiency numbers; hierarchical method examined qualitatively and compared by success/score metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported in this work for the baseline components.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Baseline relies on a separate actor module (CLIPort) for grounding actions; the paper critiques modular separation as causing coordination and robustness issues but does not present internal grounding analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Behavioral observations: explicit-feedback variants (textual Reporter) vs implicit-feedback embeddings behave differently; implicit feedback uses frozen CLIP embeddings to inform planning via LLaVA. No representational analysis is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Implicit vs explicit feedback strategies have trade-offs; hierarchical modularity can reduce robustness due to coordination overhead. Paper shows LoHoVLA (unified) generally transfers better under evaluated conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Benchmarks include seen and unseen tasks; LoHoRavens baseline achieves lower performance on unseen tasks compared to LoHoVLA in this paper's evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>No explicit zero-shot claims for LoHoRavens baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not performed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Paper notes that modular architectures can suffer coordination issues and reduced robustness which harm transfer/generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>No direct vision-only vs vision-language comparison provided for the baseline components within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1939.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1939.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaliGemma</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaliGemma (Pali-3 vision-language model used as backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-modal foundation model combining a SigLIP image encoder, a Gemma-2B decoder-only language model, and a linear projection to map visual features into the LM token space; selected as LoHoVLA's backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pali-3 vision language models: Smaller, faster, stronger.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaliGemma</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multi-modal VLM: image encoder (SigLIP), Gemma-2B decoder-only language model, and linear projection to map visual features to language token space; used as a pretrained backbone for downstream VLA fine-tuning in LoHoVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language pretraining on image-text pairs (as described for PaliGemma in paper's references).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in detail in this paper; described generally as large-scale multimodal image-text data that imbues semantic priors.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Used as a backbone for long-horizon embodied manipulation tasks (LoHoSet in Ravens simulator) when fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>See LoHoVLA target task description; PaliGemma was fine-tuned (language head) while image encoder and linear projection were kept frozen during LoHoVLA training.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper uses PaliGemma's token-space mapping as the mechanism for aligning visual features and language/action tokens, but does not quantify semantic overlap between pretraining and downstream objects/actions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Performance reported is for LoHoVLA fine-tuned from PaliGemma (see LoHoVLA entry). PaliGemma itself is not separately benchmarked in these tasks in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported for PaliGemma specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported in this paper for PaliGemma.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>PaliGemma's architecture (projection of visual features into LM token space) is used as the substrate for joint generation of language and action tokens in LoHoVLA, enabling a proposed grounding mechanism though no representational probes are presented.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not analyzed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>PaliGemma is used as a general pretrained backbone; the paper finds that fine-tuning the LM head (with frozen image encoder) plus dataset augmentation is necessary for transfer to long-horizon tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not directly analyzed for PaliGemma alone.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>PaliGemma enablement of LoHoVLA's zero-shot generalization to unseen tasks is reported indirectly via LoHoVLA results.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Training choices: image encoder and linear projection frozen; language model backbone fine-tuned with LoRA. No per-layer importance analysis provided.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported specifically for PaliGemma beyond general comments about fine-tuning and overfitting risks when task diversity is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1939.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1939.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIPort</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIPort (Actor in LoHoRavens baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A robotic manipulation actor module used in the hierarchical LoHoRavens baseline to execute low-level motion/control primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cliport: What and where pathways for robotic manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIPort</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used as the low-level actor in the LoHoRavens baseline to execute pick-and-place actions; treated as a separate control module in a hierarchical pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Not specified in detail within this paper (the actor is used as a baseline module; the paper references CLIPort's original work).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Primitive execution within long-horizon manipulation tasks (LoHoSet / LoHoRavens benchmark) in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Executes pick-and-place primitives produced by a planner; operates in the Ravens simulator scenes (blocks/bowls/zones) with RGB+depth observations.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not analyzed in this paper; CLIPort is used as a pre-existing actor module in the hierarchical baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported as part of the hierarchical baseline performance; overall the hierarchical baseline (with CLIPort actor) is outperformed by LoHoVLA on aggregate metrics in the paper (specific CLIPort-only metrics are not given).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not analyzed here; CLIPort serves as the controller that grounds planned sub-tasks into motor actions in the hierarchical baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Used as the low-level component in a modular architecture; paper's discussion focuses on coordination issues rather than CLIPort internals.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not analyzed in detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Paper attributes some hierarchical baseline limitations to coordination overhead between planner and actor (which could involve CLIPort), but no actor-specific negative transfer numbers are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1939.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1939.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenFlamingo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenFlamingo (Reporter in LoHoRavens explicit feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source autoregressive vision-language model used in LoHoRavens as the textual Reporter to provide outcome captions for feedback-driven re-planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Openflamingo: An opensource framework for training large autoregressive vision-language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenFlamingo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive vision-language model used to produce textual outcome captions in the hierarchical baseline's explicit feedback loop.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language pretraining (as a VLM) — specific pretraining corpus is not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Provides textual feedback (captions) about execution outcomes to support planner re-planning in LoHoRavens baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Operates on visual observations to produce textual captions describing outcome success/failure for closed-loop planning in the Raven simulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not analyzed in this work; used as a reporter for explicit feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not separately reported; used as a component of a baseline whose overall performance is reported and outperformed by LoHoVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not applicable in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not applicable (provides textual descriptions rather than action grounding).</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Serves as the explicit textual feedback module; no internal analysis provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1939.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1939.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP (Contrastive Language–Image Pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language model providing frozen visual embeddings used in the LoHoRavens implicit-feedback Reporter to produce embeddings fed to the planner.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning transferable visual models from natural language supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-language model used in a frozen form by the LoHoRavens implicit Reporter to encode visual observations into embeddings that are projected and sent to the Planner via LLaVA.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language contrastive pretraining on image-text pairs (as per CLIP's standard approach referenced in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in detail in this paper (paper references CLIP's original work).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Used to encode visual observations for closed-loop implicit feedback in LoHoRavens baseline on Raven manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Encodes RGB (and possibly depth) observations into embeddings; embeddings are projected via an MLP and provided to the planner to adjust sub-task plans.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Used as an embedding provider; the paper does not present an explicit measurement of semantic alignment between CLIP pretraining and the Raven tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not separately quantified in this paper; forms part of the implicit-feedback variant of LoHoRavens which is outperformed by LoHoVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported beyond noting usage of CLIP embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not applicable directly; used for perception/feedback rather than action generation.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>No direct comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1939.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1939.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA 2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 2 (used as Planner in LoHoRavens baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large text-only language model used as the Planner in the hierarchical LoHoRavens baseline to generate sub-task plans from high-level instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llama 2: Open foundation and fine-tuned chat models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA 2 (13B in baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Text-only LLM used as the Planner component in the hierarchical baseline to generate sub-task sequences from language goals (explicit feedback variant).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Text-only language model pretraining (as per LLaMA 2).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in this paper (paper references LLaMA 2 original work).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Sub-task planning for long-horizon robotic manipulation (LoHoRavens baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Generates sub-tasks (text) from high-level goals and reporter feedback; used in simulation experiments as part of the hierarchical baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not quantified in the paper; planner uses textual reasoning capabilities of LLaMA 2 but coordination with low-level actor is noted as a challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Planner-based hierarchical baseline using LLaMA 2 is outperformed by LoHoVLA in aggregated metrics reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Indirect: LLaMA 2 provides sub-task text which must be grounded by the actor; paper discusses coordination challenges but not direct grounding analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Planner/controller modular separation is discussed qualitatively; no representational analyses are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported beyond general modular coordination critique.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1939.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1939.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaVA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaVA (used to connect CLIP embeddings to planner in LoHoRavens implicit feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal adapter that ingests visual embeddings (from CLIP) and bridges them to the Planner to enable continuous plan adjustment (implicit-feedback pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA (as adapter)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described in the paper as the mechanism for sending projected CLIP embeddings (via MLP) to the Planner for continuous plan adjustment in the implicit feedback variant of LoHoRavens.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Not specified in detail in the paper's description of the baseline; functions as a multimodal adapter.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Implicit closed-loop feedback for long-horizon manipulation in LoHoRavens baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Receives CLIP visual embeddings and forwards processed embeddings to the planner for continuous re-planning.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not quantified; used as a mechanism to feed visual information to the planner.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not isolated in the paper; part of an implicit-feedback baseline whose overall performance is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1939.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1939.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-2 (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced vision-language-action model that transfers web-scale knowledge to robotic control (appears in related work / references).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rt-2: Vision-languageaction models transfer web knowledge to robotic control.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mentioned in related work as an example of VLA models that repurpose web-scale vision-language knowledge for robotic control; no experimental use in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language pretraining (as described in the cited work; in this paper it is only mentioned).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not detailed within this paper (only cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation in referenced work (not evaluated here).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Not described in this paper beyond the citation context.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not applicable here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not applicable here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1939.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e1939.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-E / PaLM-e</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-E (referenced embodied multimodal language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced embodied multimodal language model for embodied tasks (cited in related work), not used in experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Palm-e: An embodied multimodal language model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-E</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Cited in related work as an example of embodied multimodal language models that integrate perception and language for robotics; not used experimentally in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Multimodal (vision + language) pretraining as described in the referenced literature; not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Referenced for embodied/robotic control tasks (not evaluated here).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Not specified within this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not discussed in this paper beyond citation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not applicable here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not applicable here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Rt-2: Vision-languageaction models transfer web knowledge to robotic control. <em>(Rating: 2)</em></li>
                <li>Palm-e: An embodied multimodal language model. <em>(Rating: 2)</em></li>
                <li>A vision-language-action flow model for general robot control. <em>(Rating: 2)</em></li>
                <li>Gr-2: A generative video-language-action model with web-scale knowledge for robot manipulation. <em>(Rating: 2)</em></li>
                <li>Diffusion-vla: Scaling robot foundation models via unified diffusion and autoregression. <em>(Rating: 2)</em></li>
                <li>Openvla: An open-source vision-language-action model. <em>(Rating: 2)</em></li>
                <li>Cliport: What and where pathways for robotic manipulation. <em>(Rating: 2)</em></li>
                <li>LoHoRavens: A long-horizon language-conditioned benchmark for robotic tabletop manipulation. <em>(Rating: 2)</em></li>
                <li>Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. <em>(Rating: 1)</em></li>
                <li>Plan-and-act: Improving planning of agents for long-horizon tasks. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1939",
    "paper_id": "paper-279074780",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "LoHoVLA",
            "name_full": "LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks",
            "brief_description": "A unified vision-language-action model that uses a pretrained VLM backbone to jointly generate linguistic sub-task tokens and discrete action tokens, combined with a hierarchical closed-loop controller to solve long-horizon manipulation tasks in simulation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LoHoVLA",
            "model_description": "Auto-regressive Transformer based VLA that uses a pretrained PaliGemma vision-language model as backbone to autoregressively produce both language tokens (sub-task descriptions) and discrete action tokens; actions are de-tokenized from 1024-bin discretization. Employs a hierarchical closed-loop control strategy that re-predicts actions more frequently than it re-plans sub-tasks.",
            "pretraining_type": "vision-language on image-text pairs (pretrained PaliGemma backbone)",
            "pretraining_data_description": "Not exhaustively detailed in paper; described as PaliGemma being a multi-modal foundation model pretrained on large-scale image-text corpora providing semantic priors (paper refers generally to internet-scale vision-language pretraining).",
            "target_task_name": "Long-horizon robotic manipulation / object rearrangement (LoHoSet in Ravens simulator)",
            "target_task_description": "Simulation (Ravens) with a UR5e robotic arm + suction gripper performing 20 long-horizon tasks and 3 pick-and-place primitives; RGB and depth top-down orthographic observations; action space is continuous Cartesian end-effector pick/place commands discretized into 1024 uniform bins (recovered by de-tokenizer); scenes contain blocks, bowls, and zones with variations in color and size; simulated sensor noise and drop probability.",
            "semantic_alignment": "Paper claims pretrained VLM backbones provide rich semantic priors and grounding capabilities; semantic overlap between pretraining data and task objects/actions is not explicitly measured in the paper (alignment is asserted qualitatively rather than quantified).",
            "performance_with_language_pretraining": "LoHoVLA (fine-tuned from PaliGemma) achieves the highest average score and success rate across nearly all evaluated tasks; example: on the reasoning-heavy task 'put-even-blocks-in-same-color-zone' LoHoVLA attains score 85.1 and success rate 81.0; achieves near-perfect accuracy on 'put-block-into-matching-bowl' (exact number not provided).",
            "performance_without_language_pretraining": "No explicit experiment with a non-language-pretrained backbone is reported. The paper does report a 'Vanilla VLA' baseline (trained without sub-task supervision) that performs poorly, with zero success rates on several tasks, but this baseline was trained on the same pretrained backbone (i.e., it is not a no-pretraining comparison).",
            "sample_efficiency_comparison": "The paper does not report explicit quantitative sample-efficiency comparisons (e.g., demonstrations-to-performance ratios) between language-pretrained and non-pretrained models. It reports dataset sizes used for training (1,000 demos per long-horizon task; additional 10,000 demonstrations per primitive task in stage two) and ablations indicating dataset expansion and two-stage training improve generalization and planning, but not sample-efficiency numbers.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No explicit attention visualization or attention-pattern analysis is reported in the paper.",
            "embedding_space_analysis": "No explicit analysis of embedding space geometry, clustering, or semantic organization is provided.",
            "action_grounding_evidence": "Mechanistic evidence: LoHoVLA maps visual features to the language model token space (via a linear projection) and represents actions as discrete tokens that are produced by the same autoregressive model—this joint token space and joint training of sub-task text and actions is presented as the grounding mechanism. Empirical evidence: superior task performance and improved sub-task planning success rate relative to baselines; however, no direct neural-level probes (e.g., causal interventions or representational probes) are provided to demonstrate how verbs/affordances are encoded.",
            "hierarchical_features_evidence": "The paper provides behavioral evidence that hierarchical closed-loop control (different frequencies of action re-prediction vs. sub-task re-planning) mitigates errors from planning vs. low-level control; there is no layer-wise feature analysis separating low-level vs high-level representational benefits from language pretraining.",
            "transfer_conditions": "Paper reports that transfer/generalization improves with dataset expansion (adding 10 additional long-horizon tasks reduces overfitting). Fails when training set is small and scenes are similar (example: model overfits to frequent pattern and ignores language goal). Zone-match tasks tolerate spatial error and favor LoHoVLA, while pose-match tasks require precise motor control and are more sensitive to discretized action limitations.",
            "novel_vs_familiar_objects": "Paper evaluates seen vs unseen tasks (LoHoRavens unseen tasks) and reports LoHoVLA generalizes to unseen tasks and outperforms baselines despite no prior exposure; numeric per-object breakdowns for novel vs familiar object instances are not provided.",
            "zero_shot_or_few_shot": "Demonstrates zero-shot generalization to unseen tasks in the LoHoRavens benchmark (paper states LoHoVLA consistently outperforms baselines on unseen tasks 'despite having no prior exposure'). No few-shot adaptation curves are reported.",
            "layer_analysis": "Training detail: image encoder and linear projection are kept frozen during fine-tuning while the language-model backbone is optimized (LoRA applied to linear layers). The paper does not provide a deeper ablation identifying which specific layers/components are most important for transfer beyond this freezing/fine-tuning choice.",
            "negative_transfer_evidence": "Evidence of negative effects when not using sufficient task diversity: models fine-tuned on only the small set of seen tasks overfit (e.g., vanilla VLA and LoHoVLA trained without dataset expansion overfit to frequent patterns and ignore language goals). The discrete action tokenization is cited as a limitation reducing action precision (limiting performance on pose-match tasks).",
            "comparison_to_vision_only": "No direct controlled comparison between vision-language pretraining and vision-only pretraining (e.g., ImageNet or self-supervised vision) is reported.",
            "temporal_dynamics": "Two-stage training (first optimize text loss for planning, then add primitives and action labels) yields higher sub-task planning success rate and higher task success than one-stage training; introducing action labels too early is reported to hinder effective optimization of high-level planning.",
            "dimensionality_analysis": "No explicit dimensionality or intrinsic-dimension analysis of learned representations is reported.",
            "uuid": "e1939.0"
        },
        {
            "name_short": "Vanilla VLA",
            "name_full": "Vanilla Vision-Language-Action model (baseline)",
            "brief_description": "A baseline VLA that directly maps high-level goals and observations to low-level robot actions without producing intermediate sub-task language outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Vanilla VLA",
            "model_description": "Direct action-generation VLA that takes visual observations and language goals and predicts actions autoregressively; in the paper this baseline is trained on the same datasets as LoHoVLA but without sub-task (language) supervision.",
            "pretraining_type": "vision-language pretraining (same pretrained backbone family used for LoHoVLA in the paper's comparisons)",
            "pretraining_data_description": "Not explicitly detailed for the baseline in the paper; baseline was trained on LoHoSet without sub-task labels but leveraging the same pretrained backbone.",
            "target_task_name": "Long-horizon robotic manipulation / object rearrangement (LoHoSet in Ravens simulator)",
            "target_task_description": "Same Raven simulator tasks as LoHoVLA: long-horizon pick-and-place and rearrangement tasks; receives RGB+depth observations and high-level language goal; outputs discretized action tokens.",
            "semantic_alignment": "Not directly analyzed. Paper attributes poor performance to the absence of explicit sub-task supervision leading to overfitting to frequent patterns (i.e., weaker alignment between language goals and action choices).",
            "performance_with_language_pretraining": "Trained on same pretrained backbone but without sub-task labels, the vanilla VLA performs worst among methods and has zero success rates on several tasks (qualitative statement in paper; exact aggregated numbers not fully enumerated in text).",
            "performance_without_language_pretraining": "Not applicable — the baseline itself is a variant that omits sub-task language supervision rather than language pretraining.",
            "sample_efficiency_comparison": "No explicit sample-efficiency numbers reported comparing this baseline to LoHoVLA.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "No direct evidence; qualitative failure modes (e.g., placing blocks in incorrect bowls, ignoring goal conditions) are reported that suggest lacking grounded sub-task planning.",
            "hierarchical_features_evidence": "Not applicable — this model does not represent hierarchical sub-tasks explicitly.",
            "transfer_conditions": "Paper shows vanilla VLA overfits to frequent training patterns and generalizes poorly to unseen tasks; training set diversity improves generalization for models that explicitly model sub-tasks.",
            "novel_vs_familiar_objects": "Baseline performs poorly on unseen tasks; specific numeric comparisons to familiar objects not provided.",
            "zero_shot_or_few_shot": "Does not demonstrate strong zero-shot generalization; reported as performing poorly on several unseen tasks.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Reported systematic failure to follow language goals and overfitting to frequent patterns indicates negative outcomes when sub-task supervision is removed.",
            "comparison_to_vision_only": "No direct comparison to vision-only models reported.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1939.1"
        },
        {
            "name_short": "LoHoRavens",
            "name_full": "LoHoRavens (hierarchical architecture baseline / benchmark)",
            "brief_description": "A hierarchical baseline architecture used for comparison, consisting of a Planner (LLM), Actor (controller), and Reporter (feedback module) supporting both explicit and implicit feedback loops for closed-loop control.",
            "citation_title": "Lohoravens: A long-horizon language-conditioned benchmark for robotic tabletop manipulation.",
            "mention_or_use": "use",
            "model_name": "LoHoRavens (hierarchical baseline)",
            "model_description": "Modular hierarchical architecture: Planner (LLaMA 2 13B) generates sub-tasks, Actor (CLIPort) executes low-level actions, Reporter either provides textual feedback via OpenFlamingo (explicit) or encodes observations via frozen CLIP and projects them to the Planner via an MLP and LLaVA (implicit).",
            "pretraining_type": "Mixture: textual Planner uses text-only LLM pretraining; visual modules use vision-language pretraining (CLIP/OpenFlamingo) — as described in the paper's baseline setup.",
            "pretraining_data_description": "Not specified in this paper beyond referring to the pretrained components (LLM and VLM) used by the baseline.",
            "target_task_name": "Long-horizon robotic manipulation on LoHoRavens benchmark / LoHoSet tasks in Ravens simulator",
            "target_task_description": "Same family of long-horizon rearrangement tasks in simulation; Planner outputs sub-tasks, Actor executes pick-and-place primitives, Reporter provides outcome captions or embeddings for closed-loop re-planning.",
            "semantic_alignment": "Paper discusses that modular baselines rely on pretrained LLM planning and VLM perception but suffer coordination overheads and mismatches between vision-language pretraining objectives and low-level control needs; no quantitative alignment measures provided.",
            "performance_with_language_pretraining": "Reported in paper as being outperformed by LoHoVLA on average; explicit and implicit feedback variants have differing performance (numbers are shown in the paper's Table 2 but aggregated textual summaries state LoHoVLA achieves higher average score and success rate).",
            "performance_without_language_pretraining": "Not reported.",
            "sample_efficiency_comparison": "Not reported as explicit sample-efficiency numbers; hierarchical method examined qualitatively and compared by success/score metrics.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported in this work for the baseline components.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Baseline relies on a separate actor module (CLIPort) for grounding actions; the paper critiques modular separation as causing coordination and robustness issues but does not present internal grounding analyses.",
            "hierarchical_features_evidence": "Behavioral observations: explicit-feedback variants (textual Reporter) vs implicit-feedback embeddings behave differently; implicit feedback uses frozen CLIP embeddings to inform planning via LLaVA. No representational analysis is provided.",
            "transfer_conditions": "Implicit vs explicit feedback strategies have trade-offs; hierarchical modularity can reduce robustness due to coordination overhead. Paper shows LoHoVLA (unified) generally transfers better under evaluated conditions.",
            "novel_vs_familiar_objects": "Benchmarks include seen and unseen tasks; LoHoRavens baseline achieves lower performance on unseen tasks compared to LoHoVLA in this paper's evaluations.",
            "zero_shot_or_few_shot": "No explicit zero-shot claims for LoHoRavens baseline in this paper.",
            "layer_analysis": "Not performed in the paper.",
            "negative_transfer_evidence": "Paper notes that modular architectures can suffer coordination issues and reduced robustness which harm transfer/generalization.",
            "comparison_to_vision_only": "No direct vision-only vs vision-language comparison provided for the baseline components within this paper.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1939.2"
        },
        {
            "name_short": "PaliGemma",
            "name_full": "PaliGemma (Pali-3 vision-language model used as backbone)",
            "brief_description": "A multi-modal foundation model combining a SigLIP image encoder, a Gemma-2B decoder-only language model, and a linear projection to map visual features into the LM token space; selected as LoHoVLA's backbone.",
            "citation_title": "Pali-3 vision language models: Smaller, faster, stronger.",
            "mention_or_use": "use",
            "model_name": "PaliGemma",
            "model_description": "Multi-modal VLM: image encoder (SigLIP), Gemma-2B decoder-only language model, and linear projection to map visual features to language token space; used as a pretrained backbone for downstream VLA fine-tuning in LoHoVLA.",
            "pretraining_type": "Vision-language pretraining on image-text pairs (as described for PaliGemma in paper's references).",
            "pretraining_data_description": "Not specified in detail in this paper; described generally as large-scale multimodal image-text data that imbues semantic priors.",
            "target_task_name": "Used as a backbone for long-horizon embodied manipulation tasks (LoHoSet in Ravens simulator) when fine-tuned.",
            "target_task_description": "See LoHoVLA target task description; PaliGemma was fine-tuned (language head) while image encoder and linear projection were kept frozen during LoHoVLA training.",
            "semantic_alignment": "Paper uses PaliGemma's token-space mapping as the mechanism for aligning visual features and language/action tokens, but does not quantify semantic overlap between pretraining and downstream objects/actions.",
            "performance_with_language_pretraining": "Performance reported is for LoHoVLA fine-tuned from PaliGemma (see LoHoVLA entry). PaliGemma itself is not separately benchmarked in these tasks in the paper.",
            "performance_without_language_pretraining": "Not reported.",
            "sample_efficiency_comparison": "Not reported for PaliGemma specifically.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported in this paper for PaliGemma.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "PaliGemma's architecture (projection of visual features into LM token space) is used as the substrate for joint generation of language and action tokens in LoHoVLA, enabling a proposed grounding mechanism though no representational probes are presented.",
            "hierarchical_features_evidence": "Not analyzed in the paper.",
            "transfer_conditions": "PaliGemma is used as a general pretrained backbone; the paper finds that fine-tuning the LM head (with frozen image encoder) plus dataset augmentation is necessary for transfer to long-horizon tasks.",
            "novel_vs_familiar_objects": "Not directly analyzed for PaliGemma alone.",
            "zero_shot_or_few_shot": "PaliGemma enablement of LoHoVLA's zero-shot generalization to unseen tasks is reported indirectly via LoHoVLA results.",
            "layer_analysis": "Training choices: image encoder and linear projection frozen; language model backbone fine-tuned with LoRA. No per-layer importance analysis provided.",
            "negative_transfer_evidence": "Not reported specifically for PaliGemma beyond general comments about fine-tuning and overfitting risks when task diversity is limited.",
            "comparison_to_vision_only": "Not reported.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1939.3"
        },
        {
            "name_short": "CLIPort",
            "name_full": "CLIPort (Actor in LoHoRavens baseline)",
            "brief_description": "A robotic manipulation actor module used in the hierarchical LoHoRavens baseline to execute low-level motion/control primitives.",
            "citation_title": "Cliport: What and where pathways for robotic manipulation.",
            "mention_or_use": "use",
            "model_name": "CLIPort",
            "model_description": "Used as the low-level actor in the LoHoRavens baseline to execute pick-and-place actions; treated as a separate control module in a hierarchical pipeline.",
            "pretraining_type": "Not specified in detail within this paper (the actor is used as a baseline module; the paper references CLIPort's original work).",
            "pretraining_data_description": "Not specified in this paper.",
            "target_task_name": "Primitive execution within long-horizon manipulation tasks (LoHoSet / LoHoRavens benchmark) in simulation.",
            "target_task_description": "Executes pick-and-place primitives produced by a planner; operates in the Ravens simulator scenes (blocks/bowls/zones) with RGB+depth observations.",
            "semantic_alignment": "Not analyzed in this paper; CLIPort is used as a pre-existing actor module in the hierarchical baseline.",
            "performance_with_language_pretraining": "Reported as part of the hierarchical baseline performance; overall the hierarchical baseline (with CLIPort actor) is outperformed by LoHoVLA on aggregate metrics in the paper (specific CLIPort-only metrics are not given).",
            "performance_without_language_pretraining": "Not reported.",
            "sample_efficiency_comparison": "Not reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported in this paper.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Not analyzed here; CLIPort serves as the controller that grounds planned sub-tasks into motor actions in the hierarchical baseline.",
            "hierarchical_features_evidence": "Used as the low-level component in a modular architecture; paper's discussion focuses on coordination issues rather than CLIPort internals.",
            "transfer_conditions": "Not analyzed in detail in this paper.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not reported.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Paper attributes some hierarchical baseline limitations to coordination overhead between planner and actor (which could involve CLIPort), but no actor-specific negative transfer numbers are provided.",
            "comparison_to_vision_only": "Not provided.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1939.4"
        },
        {
            "name_short": "OpenFlamingo",
            "name_full": "OpenFlamingo (Reporter in LoHoRavens explicit feedback)",
            "brief_description": "An open-source autoregressive vision-language model used in LoHoRavens as the textual Reporter to provide outcome captions for feedback-driven re-planning.",
            "citation_title": "Openflamingo: An opensource framework for training large autoregressive vision-language models.",
            "mention_or_use": "use",
            "model_name": "OpenFlamingo",
            "model_description": "Autoregressive vision-language model used to produce textual outcome captions in the hierarchical baseline's explicit feedback loop.",
            "pretraining_type": "Vision-language pretraining (as a VLM) — specific pretraining corpus is not detailed in this paper.",
            "pretraining_data_description": "Not specified within this paper.",
            "target_task_name": "Provides textual feedback (captions) about execution outcomes to support planner re-planning in LoHoRavens baseline.",
            "target_task_description": "Operates on visual observations to produce textual captions describing outcome success/failure for closed-loop planning in the Raven simulation tasks.",
            "semantic_alignment": "Not analyzed in this work; used as a reporter for explicit feedback.",
            "performance_with_language_pretraining": "Not separately reported; used as a component of a baseline whose overall performance is reported and outperformed by LoHoVLA.",
            "performance_without_language_pretraining": "Not applicable in this paper.",
            "sample_efficiency_comparison": "Not reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Not applicable (provides textual descriptions rather than action grounding).",
            "hierarchical_features_evidence": "Serves as the explicit textual feedback module; no internal analysis provided.",
            "transfer_conditions": "Not reported.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not reported.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Not reported.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1939.5"
        },
        {
            "name_short": "CLIP",
            "name_full": "CLIP (Contrastive Language–Image Pretraining)",
            "brief_description": "A vision-language model providing frozen visual embeddings used in the LoHoRavens implicit-feedback Reporter to produce embeddings fed to the planner.",
            "citation_title": "Learning transferable visual models from natural language supervision.",
            "mention_or_use": "use",
            "model_name": "CLIP",
            "model_description": "Vision-language model used in a frozen form by the LoHoRavens implicit Reporter to encode visual observations into embeddings that are projected and sent to the Planner via LLaVA.",
            "pretraining_type": "Vision-language contrastive pretraining on image-text pairs (as per CLIP's standard approach referenced in the paper).",
            "pretraining_data_description": "Not specified in detail in this paper (paper references CLIP's original work).",
            "target_task_name": "Used to encode visual observations for closed-loop implicit feedback in LoHoRavens baseline on Raven manipulation tasks.",
            "target_task_description": "Encodes RGB (and possibly depth) observations into embeddings; embeddings are projected via an MLP and provided to the planner to adjust sub-task plans.",
            "semantic_alignment": "Used as an embedding provider; the paper does not present an explicit measurement of semantic alignment between CLIP pretraining and the Raven tasks.",
            "performance_with_language_pretraining": "Not separately quantified in this paper; forms part of the implicit-feedback variant of LoHoRavens which is outperformed by LoHoVLA.",
            "performance_without_language_pretraining": "Not applicable.",
            "sample_efficiency_comparison": "Not reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported here.",
            "embedding_space_analysis": "Not reported beyond noting usage of CLIP embeddings.",
            "action_grounding_evidence": "Not applicable directly; used for perception/feedback rather than action generation.",
            "hierarchical_features_evidence": "Not analyzed.",
            "transfer_conditions": "Not analyzed.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not reported.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "No direct comparison in this paper.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1939.6"
        },
        {
            "name_short": "LLaMA 2",
            "name_full": "LLaMA 2 (used as Planner in LoHoRavens baseline)",
            "brief_description": "A large text-only language model used as the Planner in the hierarchical LoHoRavens baseline to generate sub-task plans from high-level instructions.",
            "citation_title": "Llama 2: Open foundation and fine-tuned chat models.",
            "mention_or_use": "use",
            "model_name": "LLaMA 2 (13B in baseline)",
            "model_description": "Text-only LLM used as the Planner component in the hierarchical baseline to generate sub-task sequences from language goals (explicit feedback variant).",
            "pretraining_type": "Text-only language model pretraining (as per LLaMA 2).",
            "pretraining_data_description": "Not specified in this paper (paper references LLaMA 2 original work).",
            "target_task_name": "Sub-task planning for long-horizon robotic manipulation (LoHoRavens baseline).",
            "target_task_description": "Generates sub-tasks (text) from high-level goals and reporter feedback; used in simulation experiments as part of the hierarchical baseline.",
            "semantic_alignment": "Not quantified in the paper; planner uses textual reasoning capabilities of LLaMA 2 but coordination with low-level actor is noted as a challenge.",
            "performance_with_language_pretraining": "Planner-based hierarchical baseline using LLaMA 2 is outperformed by LoHoVLA in aggregated metrics reported in the paper.",
            "performance_without_language_pretraining": "Not reported.",
            "sample_efficiency_comparison": "Not reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Indirect: LLaMA 2 provides sub-task text which must be grounded by the actor; paper discusses coordination challenges but not direct grounding analyses.",
            "hierarchical_features_evidence": "Planner/controller modular separation is discussed qualitatively; no representational analyses are provided.",
            "transfer_conditions": "Not analyzed.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not reported.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported beyond general modular coordination critique.",
            "comparison_to_vision_only": "Not applicable.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1939.7"
        },
        {
            "name_short": "LLaVA",
            "name_full": "LLaVA (used to connect CLIP embeddings to planner in LoHoRavens implicit feedback)",
            "brief_description": "A multimodal adapter that ingests visual embeddings (from CLIP) and bridges them to the Planner to enable continuous plan adjustment (implicit-feedback pipeline).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaVA (as adapter)",
            "model_description": "Described in the paper as the mechanism for sending projected CLIP embeddings (via MLP) to the Planner for continuous plan adjustment in the implicit feedback variant of LoHoRavens.",
            "pretraining_type": "Not specified in detail in the paper's description of the baseline; functions as a multimodal adapter.",
            "pretraining_data_description": "Not specified.",
            "target_task_name": "Implicit closed-loop feedback for long-horizon manipulation in LoHoRavens baseline.",
            "target_task_description": "Receives CLIP visual embeddings and forwards processed embeddings to the planner for continuous re-planning.",
            "semantic_alignment": "Not quantified; used as a mechanism to feed visual information to the planner.",
            "performance_with_language_pretraining": "Not isolated in the paper; part of an implicit-feedback baseline whose overall performance is reported.",
            "performance_without_language_pretraining": "Not reported.",
            "sample_efficiency_comparison": "Not reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Not provided.",
            "hierarchical_features_evidence": "Not provided.",
            "transfer_conditions": "Not analyzed.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not reported.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Not reported.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1939.8"
        },
        {
            "name_short": "RT-2",
            "name_full": "RT-2 (referenced)",
            "brief_description": "Referenced vision-language-action model that transfers web-scale knowledge to robotic control (appears in related work / references).",
            "citation_title": "Rt-2: Vision-languageaction models transfer web knowledge to robotic control.",
            "mention_or_use": "mention",
            "model_name": "RT-2",
            "model_description": "Mentioned in related work as an example of VLA models that repurpose web-scale vision-language knowledge for robotic control; no experimental use in this paper.",
            "pretraining_type": "Vision-language pretraining (as described in the cited work; in this paper it is only mentioned).",
            "pretraining_data_description": "Not detailed within this paper (only cited in related work).",
            "target_task_name": "Robotic manipulation in referenced work (not evaluated here).",
            "target_task_description": "Not described in this paper beyond the citation context.",
            "semantic_alignment": "Not analyzed in this paper.",
            "performance_with_language_pretraining": "Not reported in this paper.",
            "performance_without_language_pretraining": "Not reported in this paper.",
            "sample_efficiency_comparison": "Not reported in this paper.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not applicable here.",
            "embedding_space_analysis": "Not applicable here.",
            "action_grounding_evidence": "Not provided in this paper.",
            "hierarchical_features_evidence": "Not provided here.",
            "transfer_conditions": "Not analyzed here.",
            "novel_vs_familiar_objects": "Not analyzed here.",
            "zero_shot_or_few_shot": "Not analyzed here.",
            "layer_analysis": "Not analyzed here.",
            "negative_transfer_evidence": "Not analyzed here.",
            "comparison_to_vision_only": "Not analyzed here.",
            "temporal_dynamics": "Not analyzed here.",
            "dimensionality_analysis": "Not analyzed here.",
            "uuid": "e1939.9"
        },
        {
            "name_short": "PaLM-E / PaLM-e",
            "name_full": "PaLM-E (referenced embodied multimodal language model)",
            "brief_description": "Referenced embodied multimodal language model for embodied tasks (cited in related work), not used in experiments in this paper.",
            "citation_title": "Palm-e: An embodied multimodal language model.",
            "mention_or_use": "mention",
            "model_name": "PaLM-E",
            "model_description": "Cited in related work as an example of embodied multimodal language models that integrate perception and language for robotics; not used experimentally in this paper.",
            "pretraining_type": "Multimodal (vision + language) pretraining as described in the referenced literature; not detailed here.",
            "pretraining_data_description": "Not specified in this paper.",
            "target_task_name": "Referenced for embodied/robotic control tasks (not evaluated here).",
            "target_task_description": "Not specified within this paper's experiments.",
            "semantic_alignment": "Not discussed in this paper beyond citation.",
            "performance_with_language_pretraining": "Not reported in this paper.",
            "performance_without_language_pretraining": "Not reported in this paper.",
            "sample_efficiency_comparison": "Not reported in this paper.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not applicable here.",
            "embedding_space_analysis": "Not applicable here.",
            "action_grounding_evidence": "Not analyzed in this paper.",
            "hierarchical_features_evidence": "Not analyzed in this paper.",
            "transfer_conditions": "Not analyzed in this paper.",
            "novel_vs_familiar_objects": "Not analyzed in this paper.",
            "zero_shot_or_few_shot": "Not analyzed in this paper.",
            "layer_analysis": "Not analyzed in this paper.",
            "negative_transfer_evidence": "Not analyzed in this paper.",
            "comparison_to_vision_only": "Not analyzed in this paper.",
            "temporal_dynamics": "Not analyzed in this paper.",
            "dimensionality_analysis": "Not analyzed in this paper.",
            "uuid": "e1939.10"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Rt-2: Vision-languageaction models transfer web knowledge to robotic control.",
            "rating": 2
        },
        {
            "paper_title": "Palm-e: An embodied multimodal language model.",
            "rating": 2
        },
        {
            "paper_title": "A vision-language-action flow model for general robot control.",
            "rating": 2
        },
        {
            "paper_title": "Gr-2: A generative video-language-action model with web-scale knowledge for robot manipulation.",
            "rating": 2
        },
        {
            "paper_title": "Diffusion-vla: Scaling robot foundation models via unified diffusion and autoregression.",
            "rating": 2
        },
        {
            "paper_title": "Openvla: An open-source vision-language-action model.",
            "rating": 2
        },
        {
            "paper_title": "Cliport: What and where pathways for robotic manipulation.",
            "rating": 2
        },
        {
            "paper_title": "LoHoRavens: A long-horizon language-conditioned benchmark for robotic tabletop manipulation.",
            "rating": 2
        },
        {
            "paper_title": "Cot-vla: Visual chain-of-thought reasoning for vision-language-action models.",
            "rating": 1
        },
        {
            "paper_title": "Plan-and-act: Improving planning of agents for long-horizon tasks.",
            "rating": 1
        }
    ],
    "cost": 0.027143749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks
31 May 2025</p>
<p>Yi Yang 
Jiaxuan Sun 
Siqi Kou 
Yihan Wang 
Zhijie Deng zhijied@sjtu.edu.cn </p>
<p>Fudan University</p>
<p>ShanghaiTech University</p>
<p>Shanghai Jiao Tong University</p>
<p>Fudan University</p>
<p>Shanghai Jiao Tong University</p>
<p>LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks
31 May 20251CBD7B771E1D8B3F7F4D857E00BDA632arXiv:2506.00411v1[cs.RO]
Real-world embodied agents face long-horizon tasks, characterized by high-level goals demanding multi-step solutions beyond single actions.Successfully navigating these requires both high-level task planning (i.e., decomposing goals into sub-tasks) and low-level motion control (i.e., generating precise robot actions).While existing vision language action (VLA) models and hierarchical architectures offer potential in embodied tasks, the former often falter in planning, and the latter can suffer from coordination issues, both hampering performance.We introduce a new unified VLA framework for long-horizon tasks, dubbed LoHoVLA, to overcome these limitations.LoHoVLA leverages a large pretrained vision language model (VLM) as the backbone to jointly generate language and action tokens for sub-task generation and robot action prediction, respectively.This shared representation promotes better generalization across tasks.Additionally, LoHoVLA embraces a hierarchical closed-loop control mechanism to mitigate errors originating from both high-level planning and low-level control.To train LoHoVLA, we introduce LoHoSet, a dataset built on the Ravens simulator, containing 20 long-horizon tasks, each with 1,000 expert demonstrations composed of visual observations, linguistic goals, sub-tasks, and robot actions.Experimental results show that LoHoVLA significantly surpasses both hierarchical and standard VLA approaches on long-horizon embodied tasks in the Ravens simulator.These findings underscore the promise of unified architectures for advancing generalizable embodied intelligence.</p>
<p>Introduction</p>
<p>Embodied agents operating in real world are required to handle tasks that are long-horizon, compositional, and dynamically changing [30,14,41,45].Unlike short-horizon tasks [47,21,49,38,55,22], the long-horizon ones involve high-level goals that cannot be achieved in a single action.Agents must do reasoning, execute movements, and adapt to failures or changes in the environment.This 2 Related Work</p>
<p>Vision-Language-Action Models</p>
<p>Vision language action (VLA) models have become a central paradigm in robot learning by bridging perception, language understanding, and control.These models typically repurpose large-scale vision language models (VLMs) [2,16,10,23] pretrained on internet-scale datasets for downstream robotic tasks.Such pretrained VLMs offer rich semantic priors and strong generalization across modalities, making them attractive backbones for visuomotor policies.Recent works [13,24,34] fine-tune these backbones on robot demonstration datasets to map image and language inputs to motor actions, showing promising results in generalizing to novel objects, instructions, and environments.Despite these advances, most existing VLA frameworks are limited in their ability to perform multi-step reasoning or structured task decomposition, which are critical for executing complex or long-horizon tasks.To address this, some approaches introduce external planners [46,15,54] or use action experts [44] to handle sub-task planing, though this modularity often incurs coordination overhead and reduces system robustness.Furthermore, while pretrained VLMs offer strong grounding capabilities, their integration into control pipelines remains challenging due to the mismatch between vision-language pretraining objectives and the fine-grained demands of robotic control.Our work contributes to the growing effort to enhance VLA models by exploring architectural designs that support structured reasoning and low-level control within a cohesive framework.</p>
<p>Long-Horizon Embodied Task Planning</p>
<p>Effective execution of long-horizon tasks in embodied agents necessitates advanced planning capabilities that can decompose high-level goals into coherent sequences of sub-tasks.Recent approaches [1,4,9,11,12,18,25,27,33,37] have leveraged large language models (LLMs) to enhance such planning processes.These methods typically employ LLMs to generate structured plans from natural language instructions, which are then executed by specialized low-level controllers.To improve adaptability, some frameworks [9,12,33,37] incorporate re-planning mechanisms that allow the agent to adjust its strategy in response to environmental changes, thereby enhancing robustness and generalization across diverse tasks and settings.Beyond textual planning, there is a growing interest in integrating chain-of-thought (CoT) reasoning [42] into embodied systems.This paradigm [53,48,43] involves generating intermediate reasoning steps that bridge the gap between high-level instructions and low-level actions.By conditioning action prediction on these subgoals, agents can achieve more structured and interpretable behaviors.However, existing approaches often suffer from limitations such as reliance on modular architectures that separate high-level planning and low-level control, leading to suboptimal coordination and limited generalization.Our proposed LoHoVLA model integrates both high-level task planning and low-level motion control within a single unified framework, leveraging a shared vision-language backbone to improve reasoning, planning, and generalization capabilities in long-horizon tasks.</p>
<p>LoHoVLA</p>
<p>This section elaborates on LoHoVLA, starting with various modeling configurations, followed by a description of the LoHoSet dataset for training LoHoVLA.We then present the model architecture and outline the training strategies.</p>
<p>Modeling Configurations</p>
<p>We address the problem of learning a policy π θ that produces robot actions a t based on a visual observation o t and a high-level language goal g.The visual observation o t consists of images of the scene captured by the robot's cameras.The goal g defines a high-level instruction for a long-horizon task (e.g., "Clean the desk"), which implicitly incurs the sequential execution of multiple sub-tasks [ĝ 1 , ĝ2 , • • • , ĝN ] (e.g., "Put the pen in the pen holder" → "Close the laptop" → "Put the book on the bookshelf" → etc.).For simplicity, we assume that each sub-task can be completed within a single time step.The action a t represents low-level robot actions, such as specifying the Cartesian position of the robot's end-effector.These commands are executed by the robot to interact with the environment.New observations o t+1 are then captured to determine subsequent actions a t+1 .One typical trajectory for this task can be represented as {g, γ 1 , γ 2 , • • • , γ N }, where g denotes the overall goal and γ t = (o t , ĝt , a t ) denotes a triplet of observation, sub-task, and robot action at time step t.During inference, the policy is only provided with the overall goal and does not have access to the sub-tasks.Consequently, it must rely on implicit or explicit planning to infer the sub-tasks.</p>
<p>Vanilla VLA The vanilla VLA model (Figure 1, left top) is limited to action generation and cannot produce language outputs.Thus, in fact, it implicitly performs sub-task planning, with only a resultant action a t yielded based on the high-level goal g and the current observation o t :</p>
<p>As the equation implies, LoHoVLA first infers the next atomic sub-task and then uses it as contextual guidance to predict the robot's action.High-level task planning corresponds to modeling π θ (ĝ t | o t , g), while low-level motion control corresponds to modeling π θ (a t | o t , g, ĝt ), both distributions are represented within a single unified model.</p>
<p>A Synthetic Dataset for Long-horizon Embodied Tasks: LoHoSet</p>
<p>The training of LoHoVLA relies on a collection of demonstrations (g, γ t ) with γ t = (o t , ĝt , a t ).The primary challenge is that sub-task annotation for real-world long-horizon tasks can rarely be obtained in a scalable manner without human intervention.To address this, we opt for a simulator-based approach following prior works [38,50,32].Concretely, we construct the LoHoSet dataset based on the Ravens robot simulator [49].The simulation environment includes a UR5e robotic arm with a suction gripper and several objects placed on a table.The environment provides a reward signal only when the predicted action is both semantically correct and successfully executed.To simulate real-world uncertainties, the simulator adds observation noise and introduces a dropping probability p for the end-effector to drop the picked block every second.The visual observation o = (I color , I depth ) comprises RGB and depth top-down orthographic reconstructions of the scene.The goal instruction g mainly focuses on rearranging the objects into the desired configuration (e.g., "Stack blocks in alternate colors on the green zone").</p>
<p>We collect decomposed sub-tasks based on manually designed rules, thanks to the availability of complete information about the scenes from the simulator.In particular, we randomly initialize the scene, and we directly estimate the action a = (T pick , T place ) based on the positions of target objects.For sub-tasks without dependency constraints, their execution order is randomized; otherwise, we generate them with pre-defined logics.Each object is assigned a textual label, which is inserted into
Algorithm 1: LoHoVLA Test-time Closed-Loop Control Data: LoHoVLA policy π θ , initial observation o0, high-level goal g, reward r, failure count k, failure threshold K, done flag done t ← 0, k ← 0, r ← 0, done ← f alse while not done do if t = 0 or r &gt; 0 or k &gt; K then ĝt ∼ π θ (ĝt | ot, g); k ← 0; end at ∼ π θ (at | ot, g, ĝt); execute at; get ot+1, r, done from environment; if r = 0 then k ← k + 1; end t ← t + 1; end
sub-task templates to generate sub-task descriptions (e.g., "Pick up the green block and place it in the green zone" → "Pick up the blue block and place it on the green block" → etc.).This process enables the generation of a large and diverse set of demonstrations.</p>
<p>The resultant LoHoSet finally includes three types of objects: blocks, bowls, and zones, available in 11 distinct colors.The blocks are of two sizes, large and small.Following LoHoRavens [50], we adopt the 10 long-horizon tasks (detailed in Table 1) and 3 pick-and-place primitive tasks to facilitate comparison with the baselines.We include 10 additional long-horizon tasks in LoHoSet to improve the generalization capacities of the trained model (see Figure 3 left).An example of these tasks is shown in Figure 2.For more details on LoHoSet, please refer to Appendix A.</p>
<p>Model Architecture</p>
<p>LoHoVLA employs a large pretrained vision language model (VLM) as its backbone to generate the next tokens, with specialized de-tokenizers to translate them as linguistic sub-tasks and actions, respectively.To address errors in both high-level planning and low-level control, it incorporates a hierarchical closed-loop control mechanism.An overview of LoHoVLA is presented in Figure 1.</p>
<p>The Base Vision Language Model.We select PaliGemma [5] as the backbone for our model due to its proven effectiveness in prior studies [6,20].PaliGemma is a multi-modal foundation model processing both images and textual prompts for text generation.It integrates a SigLIP-based image encoder, a Gemma-2B decoder-only language model, and a linear projection layer that maps visual features into the language model's token space.</p>
<p>The Action De-Tokenizer.Following prior work [1,7,24], we represent robot actions as discrete tokens to enable joint training with vision-language datasets.Specifically, we discretize the normalized action values into 1,024 uniform bins.During inference, robot actions are recovered by de-tokenizing and de-normalization.</p>
<p>Hierarchical closed-loop control mechanism.Compared to closed-loop control for atomic tasks, managing long-horizon tasks is more complex.Execution failures may arise from sub-task planning errors, inaccurate action predictions, or external disturbances.More formally, the three error types are: (1) sub-task planning error, (2) correct sub-task planning but incorrect action prediction, and (3) correct planning and prediction, with failures caused by external disturbances.</p>
<p>LoHoVLA embraces a hierarchical closed-loop control strategy that re-predicts actions more frequently than it re-plans sub-tasks.Specifically, if the number of failures during the current sub-task exceeds a predefined threshold K, the system triggers sub-task re-planning; otherwise, it only repredicts the action.The control procedure used during evaluation is outlined in Algorithm 1.We assume that the robot receives a positive reward r upon completing a sub-task.The done flag is used to determine whether the overall goal has been achieved.Table 1: The seen and unseen tasks from LoHoRavens benchmark.Note that we also add the pick-and-place primitive task to the seen tasks.This design avoids unnecessary sub-task re-planning in cases ( 2) and (3).Our experiments demonstrate that the hierarchical closed-loop control scheme effectively mitigates errors in long-horizon tasks while avoiding redundant inference steps.</p>
<p>Training Configurations</p>
<p>During LoHoVLA's training, we optimize the language model backbone while keeping the image encoder and the linear projection layer fixed.The training objective consists of two components: sub-task generation and action prediction.Both outputs are produced by the language model head and optimized using cross-entropy loss.The total loss is defined as:
L = L text + L action .(4)
We adopt a two-stage training strategy.In the first stage, we fine-tune PaliGemma on long-horizon tasks, optimizing only the text loss to improve high-level task planning.In the second stage, we augment the dataset with pick-and-place primitive tasks and optimize both text and action losses to enhance action prediction.</p>
<p>Experiments</p>
<p>Our experimental evaluations aim to assess LoHoVLA's capabilities in high-level task planning, low-level motion control, and generalization to novel tasks unseen during training.Specifically, we address the following questions:</p>
<p>• How does LoHoVLA perform compared to hierarchical architecture baselines and standard VLA models in terms of performance and generalization on long-horizon tasks?• How effective are hierarchical closed-loop control schemes at mitigating errors in long-horizon tasks, where failures can arise from both high-level task planning and low-level motion control?• Can dataset expansion and a two-stage training strategy improve overall model performance?</p>
<p>Experimental Setup</p>
<p>Baselines.We use LoHoRavens [50] as the baseline with a hierarchical architecture.It consists of a Planner for high-level sub-task planning, an Actor for low-level motion control, and a Reporter for feedback-driven closed-loop control.It supports two types of Reporter-based feedback: (1) Explicit feedback, where the Planner (LLaMA 2 13B [40]) generates sub-tasks, the Actor (CLIPort [38]) executes them, and the Reporter (OpenFlamingo [3]) provides outcome captions to refine future plans; and (2) Implicit feedback, where the Reporter encodes visual observations using a frozen CLIP model [35], projects them via an MLP, and sends the embeddings to the Planner through LLaVA [29] for continuous plan adjustment.Training Details.LoHoVLA uses the PaliGemma-3b-mix-224 model as its backbone and is trained in two stages.In the first stage, we fine-tune PaliGemma on 14 long-horizon tasks-comprising 4 seen tasks from LoHoRavens and 10 additional tasks we designed-each with 1,000 demonstrations.We optimize only the text loss to improve high-level task planning.Unlike LoHoRavens, which uses prompt engineering to preserve generalization, our model requires fine-tuning, which leads to overfitting on the limited set of 4 tasks.To address this, we include 10 additional tasks to enhance generalization.In the second stage, we augment the dataset with 10,000 demonstrations for each pick-and-place primitive and optimize both text and action losses to improve low-level motion control.</p>
<p>Training is performed using the AdamW optimizer [31] with weight decay and gradient clipping.We utilize DeepSpeed [36] for efficient distributed training.In training stage one, we train with 8 NVIDIA 4090 GPUs (24GB VRAM), a per-device batch size of 2, a learning rate of 5e-5, and for 3 epochs.We apply LoRA [17] with a rank of 16 targeting all linear layers.In training stage two, we use a learning rate of 1e-5 for 1 epoch, keeping other settings unchanged.For comparison, we train the standard VLA model on the same dataset without sub-task labels using a learning rate of 1e-5, for 5 epochs, under the same hardware and LoRA configuration.</p>
<p>Evaluation Metrics.There are two evaluation methods for determining whether object states match the ground-truth, depending on the task category.The first is pose match, which requires the object's position and orientation to exactly align with the ground-truth.The second is zone match, where the overlap area between the predicted and ground-truth object must exceed a predefined threshold.We evaluate each task instance using a score from 0 (failure) to 100 (success), based on the proportion of correctly completed pick-and-place steps.For example, if a task requires ten steps and the model completes eight, the score would be 80%.In our evaluations, we report both the average score and the success rate on the test set.</p>
<p>Main Results</p>
<p>We evaluate LoHoVLA against the vanilla VLA and LoHoRavens on both seen and unseen tasks from LoHoRavens (Table 1).LoHoVLA and Vanilla VLA are trained on five seen tasks and ten additional tasks introduced in this work.Evaluation results, including those for LoHoRavens with both explicit and implicit feedback mechanisms, are shown in Table 2. LoHoVLA achieves the highest average score and success rate across nearly all tasks.On the putblock-into-matching-bowl task, it attains near-perfect accuracy.On the most challenging reasoning task put-even-blocks-in-same-color-zone, which requires integrating color recognition, counting, spatial reasoning, and logic, LoHoVLA achieves a score of 85.1 and a success rate of 81.0, while all baselines perform poorly.Notably, LoHoVLA demonstrates strong generalization to unseen tasks, consistently outperforming all baselines despite having no prior exposure.</p>
<p>Interestingly, LoHoVLA occasionally outperforms on long-horizon tasks compared to pick-andplace-primitive tasks.This is largely due to differences in evaluation criteria: zone-match tasks (e.g., involving bowls or colored zones) tolerate minor spatial inaccuracies, which LoHoVLA handles effectively.In contrast, pose-match tasks (e.g., block stacking) require precise alignment, where occasional sub-optimal motor trajectories can slightly reduce performance.Nevertheless, LoHoVLA remains robust across both task types.</p>
<p>Vanilla VLA performs the worst among all models, with zero success rates on several tasks.Our qualitative analysis reveals that this is primarily due to the absence of sub-task supervision, which leads the model to overfit to frequent patterns in the training data.For instance, in the put-block-intomatching-bowl task, it often places blocks in incorrect bowls, disregarding the goal condition.</p>
<p>Closed-Loop Strategies Comparison</p>
<p>To assess the effectiveness of our specialized closed-loop control mechanism in addressing task execution failures, we compare the following three control strategies.</p>
<p>• Strategy (a).On failure, the system re-predicts the action only without re-planning the sub-task.</p>
<p>• Strategy (b).The system re-plans the sub-task and then re-predicts the action after every failure.</p>
<p>• Strategy (c).The hierarchical closed-loop control strategy: if the number of failures within the current sub-task exceeds a predefined threshold K, the system initiates sub-task re-planning; otherwise, it only re-predicts the action.In our experiments, we set K = 2.</p>
<p>In addition to measuring the average reward and success rate, we track the number of high-level sub-task planning.All three strategies are evaluated using the fine-tuned LoHoVLA model.Results are reported in Table 3.</p>
<p>As expected, strategy (a) yields the poorest performance.When failures stem from incorrect sub-task planning, this approach continues executing flawed plans, potentially leading to deadlocks.Strategies (b) and (c) perform comparably in overall metrics; however, strategy (c) requires fewer high-level sub-task planning steps.This is because many failures result from low-level prediction errors or external disturbances, where re-planning the sub-task is unnecessary.</p>
<p>Furthermore, task characteristics influence strategy performance.For reasoning-heavy tasks (e.g., put-even-blocks-in-same-color-zone), strategy (b) excels by promptly correcting sub-task assignments.</p>
<p>In contrast, for tasks demanding precise motor control (e.g., stack-block-in-absolute-area), strategy (c) performs better, as repeated low-level attempts improve the likelihood of success.</p>
<p>Ablation Studies</p>
<p>We examine the effects of training set expansion and the two-stage training method on model performance.To better evaluate planning ability, we introduce a new metric: sub-task planning success rate.Unlike sub-task execution success, this metric isolates planning performance by removing the influence of low-level action prediction and external disturbances.Specifically, for each task, we randomly sample 10 timesteps from all episodes and manually enumerate all valid subsequent sub-tasks.An LLM is then used to assess whether the model's predicted sub-task at each timestep is semantically equivalent to any of the enumerated ground-truth options.Further details are provided in Appendix B.</p>
<p>Evaluate Training Set Expansion.We trained LoHoVLA with and without training set expansion (i.e., the extra 10 tasks used for training mentioned in Section 3.2), and evaluated the generalization ability on the unseen tasks of the LoHoRavens Benchmark.The results are shown in Figure 3 left.It can be observed that the model trained without extra data has poor generalization ability.This is due to severe overfitting to the seen tasks.For example, the success rate of the task put-blockinto-mismatching-bowl is 0, because its scene is similar to that of put-block-into-matching-bowl, which led the model to overfit to the latter and ignore the language goal, placing the block into the matching-colored bowl instead.The expanded dataset alleviates such overfitting issues.</p>
<p>Evaluate Two-stage Training.We conduct both one-stage and two-stage training experiments on LoHoVLA using identical configurations.In both settings, the model is trained for 5 epochs.The key difference is that in the two-stage approach, the primitive tasks and action labels are introduced only after the first 3 epochs.The results-average sub-task planning success rate (%) and average task success rate (%)-are presented in Figure 3 right.The one-stage training setup yields a lower sub-task planning success rate, which consequently results in reduced task success.This indicates that introducing action labels and primitive tasks too early hinders the effective optimization of high-level task planning.</p>
<p>Conclusion</p>
<p>For long-horizon embodied tasks requiring both high-level planning and low-level control, existing VLA models and hierarchical approaches struggle with planning and coordination.To address this, we propose LoHoVLA, a unified VLA framework that uses a large pretrained vision-language model to jointly generate sub-tasks and robot actions.It incorporates a hierarchical closed-loop control mechanism to correct errors at both levels.Empirical results show that LoHoVLA outperforms prior VLA and hierarchical methods by decent margins and demonstrates strong generalization.</p>
<p>Limitations.The limitations stem primarily from the limited precision of robot actions due to their discrete structure.Additionally, our assumption that a sub-task can be completed within a single timestep may not be practical in real-time applications.</p>
<p>A LoHoSet  LoHoSet consists of 3 pick-and-place primitive tasks and 20 long-horizon tasks.Among them, 10 long-horizon tasks and all 3 primitive tasks are adapted from the LoHoRavens benchmark to facilitate comparison with existing baselines.The other 10 long-horizon tasks are introduced to improve the generalization ability of the trained model.A complete list of tasks is provided in Table 4, and examples of all tasks are illustrated in Figure 4. Table 4: Summary of all tasks in LoHoSet, including 3 primitive pick-and-place tasks and 20 longhorizon tasks, with distinctions between those adopted from LoHoRavens and those introduced for improved generalization.</p>
<p>Figure 1 :
1
Figure 1: Left top: Vanilla VLA directly maps high-level goals and observations to actions.Left bottom: The hierarchical architecture separates planning and execution-the planner infers sub-tasks, and the controller executes them.Right: LoHoVLA integrates high-level task planning and low-level motion control into a unified model.It uses an auto-regressive (AR) Transformer as its backbone and employs a hierarchical closed-loop control mechanism.</p>
<p>"</p>
<p>Put the blocks in the zones with mismatched colors.""Stack all the smaller blocks over the bigger blocks of the same color.""Put the blocks of an even number in the zone with the matching color" "Put hidden blocks in the two-layer towers into the bowls with matching colors.""Stack all the blocks in the red zone."</p>
<p>Figure 2 :
2
Figure 2: An example of the long-horizon LoHoSet.Object attributes like size, color, quantity, and position vary across cases.</p>
<p>Figure 3 :
3
Figure 3: (a) Comparison of performance on unseen tasks between training with and without dataset expansion, evaluated using the sub-task planning success rate (%).(b) Comparison of performance between one-stage and two-stage training strategies, evaluated using both the sub-task planning success rate (%) and task completion success rate (%).</p>
<p>pick-and-place-primitive pick-and-place-primitive-with-size pick-and-place-primitive-withabsolute-position put-block-into-matching-bowl stack-smaller-over-biggerwith-same-color stack-block-in-absolute-area put-even-blocks-in-same-color-zone put-block-into-mismatching-bowl stack-blocks-of-same-size stack-blocks-with-alternate-color stack-smaller-over-biggerwith-same-color-insame-color-zone move-blocks-betweenabsolute-positions stack-blocks-of-same-color put-block-into-mismatching-zone put-hidden-blocks-in-two-layertowers-into-matching-bowls put-hidden-blocksin-two-layer-towersinto-mismatching-bowls put-hidden-blocksin-three-layer-towersinto-matching-bowls put-hidden-blocks-in-pyramidinto-matching-bowls stack-bigger-over-smallerwith-same-colorin-same-color-zone stack-all-blocks-on-a-zone stack-blocks-by-relative-position move-blocks-betweenabsolute-positions-by-size move-blocks-betweenabsolute-positions-by-color</p>
<p>Figure 4 :
4
Figure 4: Visual examples of all tasks in the LoHoSet dataset, including 3 pick-and-place primitives and 20 long-horizon tasks, showcasing the diversity and complexity of the task scenarios.</p>
<p>Table 2 :
2
Comparison of the average award (%) and success rate (%) on LoHoRavens benchmark.Bold entries indicate the highest success rates, underlined entries indicate the second-highest.
TasksVanilla VLALoHoRavensLoHoVLAExplicit feedback Implicit feedbackA79.0 / 79.067.3 / -67.3 / -77.5 / 77.5Seen TasksB C D14.9 / 0.0 26.8 / 0.5 32.3 / 3.031.4 / -18.0 / -30.4 / -37.0 / -22.1 / -33.2 / -97.8 / 91.5 34.9 / 22.5 35.8 / 11.5E22.1 / 3.59.6 / -8.2 / -85.1 / 81.0F52.1 / 9.028.5 / -21.1 / -86.1 / 41.0G6.8 / 0.021.9 / -14.7 / -40.1 / 25.0UnseenH7.3 / 0.013.2 / -5.2 / -16.7 / 7.5TasksI43.1 / 1.512.8 / -11.7 / -77.2 / 52.0J38.6 / 10.527.4 / -27.2 / -43.6 / 22.0K58.2 / 33.04.0 / -6.8 / -73.8 / 54.5
As a secondary baseline, we train a vanilla VLA model to isolate the effect of explicit sub-task prediction.This model directly predicts low-level robot actions without producing intermediate language outputs, relying solely on implicit sub-task inference.Our goal is to test whether it can learn sub-task reasoning without explicit planning.</p>
<p>Table 3 :
3
Comparison of the average reward (%), success rate (%), and number of sub-task planning across three strategies.Strategy (a) performs the worst.Strategies (b) and (c) are comparable overall, but (c) requires fewer high-level planning steps.
TasksStrategy (a)Strategy (b)Strategy (c)A 77.5 / 77.51.377.5 / 77.51.377.5 / 77.51.3Seen TasksB 89.5 / 74.0 C 30.3 / 8.5 D 20.1 / 2.55.7 3.7 2.296.4 / 88.5 35.1 / 24.0 31.3 / 9.06.4 7.9 7.697.8 / 91.5 34.9 / 22.5 35.8 / 11.56.2 7.8 7.3E 63.1 / 59.05.986.4 / 84.06.685.1 / 81.06.2F 62.3 / 29.03.686.2 / 42.06.886.1 / 41.06.7G 32.1 / 19.55.239.9 / 24.58.240.1 / 25.07.9UnseenH14.1 / 4.04.515.9 / 6.08.116.7 / 7.56.4TasksI66.4 / 41.03.778.2 / 56.06.977.2 / 52.06.8J31.3 / 14.54.244.5 / 23.57.443.6 / 22.07.1K 52.9 / 34.04.771.1 / 52.07.273.8 / 54.56.9
Tasks InstructionLoHoRavensB Ablation Studies DetailsWe investigate the effects of training set expansion and a two-stage training strategy on model performance.To more accurately evaluate planning capabilities, we introduce a new metric: sub-task planning success rate.Unlike sub-task execution success, this metric isolates high-level planning by excluding the influence of low-level action prediction and external noise.For each task, we randomly sample 10 timesteps across all episodes and manually enumerate valid next sub-tasks.A large language model (LLM) then assesses whether the model's predicted sub-task at each timestep is semantically equivalent to any of the ground-truth options.The evaluation prompt is as follows:Evaluation Prompt You are given the current sub-task in a sequential task and a predicted next sub-task generated by a model.Additionally, you are provided with a list of valid ground-truth next sub-tasks.Determine whether the predicted sub-task is semantically equivalent to any of the groundtruth options.Focus solely on semantic similarity in intent and meaning, ignoring differences in wording or phrasing.Respond with 'Yes' if the prediction is semantically equivalent to at least one ground-truth sub-task; otherwise, respond with 'No'.
Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Chuyuan Finn, Keerthana Fu, Karol Gopalakrishnan, Hausman, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Flamingo: a visual language model for few-shot learning. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Advances in neural information processing systems. 202235</p>
<p>Openflamingo: An opensource framework for training large autoregressive vision-language models. Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Yonatan Kalyani Marathe, Samir Bitton, Shiori Gadre, Sagawa, arXiv:2308.013902023arXiv preprint</p>
<p>Tianli Suneel Belkhale, Ted Ding, Pierre Xiao, Quon Sermanet, Jonathan Vuong, Yevgen Tompson, Chebotar, arXiv:2403.01823Debidatta Dwibedi, and Dorsa Sadigh. Rt-h: Action hierarchies using language. 2024arXiv preprint</p>
<p>Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, arXiv:2407.07726A versatile 3b vlm for transfer. 2024arXiv preprint</p>
<p>0 : A vision-language-action flow model for general robot control. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, arXiv:2410.241642024arXiv preprint</p>
<p>Rt-2: Vision-languageaction models transfer web knowledge to robotic control. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Gr-2: A generative video-language-action model with web-scale knowledge for robot manipulation. Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, arXiv:2410.061582024arXiv preprint</p>
<p>Automating robot failure recovery using vision-language models with optimized prompts. Hongyi Chen, Yunchao Yao, Ruixuan Liu, Changliu Liu, Jeffrey Ichnowski, arXiv:2409.039662024arXiv preprint</p>
<p>Pali-3 vision language models: Smaller, faster, stronger. Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, Paul Voigtlaender, Basil Mustafa, Sebastian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski, arXiv:2310.091992023arXiv preprint</p>
<p>An-Chieh Cheng, Yandong Ji, Zhaojing Yang, Zaitian Gongye, Xueyan Zou, Jan Kautz, Erdem Bıyık, Hongxu Yin, Sifei Liu, Xiaolong Wang, Navila, arXiv:2412.04453Legged robot vision-language-action model for navigation. 2024arXiv preprint</p>
<p>Racer: Rich language-guided failure recovery policies for imitation learning. Yinpei Dai, Jayjun Lee, Nima Fazeli, Joyce Chai, arXiv:2409.146742024arXiv preprint</p>
<p>Palm-e: An embodied multimodal language model. Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Ayzaan Chowdhery, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Wenlong Yu, Huang, 2023</p>
<p>A survey of embodied ai: From simulators to research tasks. Jiafei Duan, Samson Yu, Hui Li Tan, Hongyuan Zhu, Cheston Tan, IEEE Transactions on Emerging Topics in Computational Intelligence. 622022</p>
<p>Plan-and-act: Improving planning of agents for long-horizon tasks. Nicholas Lutfi Eren Erdogan, Sehoon Lee, Suhong Kim, Hiroki Moon, Gopala Furuta, Kurt Anumanchipalli, Amir Keutzer, Gholami, arXiv:2503.095722025arXiv preprint</p>
<p>Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Chengzhuo Tong, Peng Gao, Chunyuan Li, Pheng-Ann Heng, arXiv:2408.16768Sam2point: Segment any 3d as videos in zero-shot and promptable manners. 2024arXiv preprint</p>
<p>Low-rank adaptation of large language models. J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, ICLR. 1232022</p>
<p>Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning. Yingdong Hu, Fanqi Lin, Tong Zhang, Li Yi, Yang Gao, arXiv:2311.178422023arXiv preprint</p>
<p>Inner monologue: Embodied reasoning through planning with language models. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, arXiv:2207.056082022arXiv preprint</p>
<p>5 : a visionlanguage-action model with open-world generalization. Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, arXiv:2504.160542025arXiv preprintet al. π 0.</p>
<p>Rlbench: The robot learning benchmark &amp; learning environment. Stephen James, Zicong Ma, David Rovick Arrojo, Andrew J Davison, IEEE Robotics and Automation Letters. 522020</p>
<p>Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, Linxi Fan, Vima, arXiv:2210.03094General robot manipulation with multimodal prompts. 202226arXiv preprint</p>
<p>Prismatic vlms: Investigating the design space of visually-conditioned language models. Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, Dorsa Sadigh, Forty-first International Conference on Machine Learning. 2024</p>
<p>Openvla: An open-source vision-language-action model. Jin Moo, Karl Kim, Siddharth Pertsch, Ted Karamcheti, Ashwin Xiao, Suraj Balakrishna, Rafael Nair, Ethan Rafailov, Grace Foster, Pannag Lam, Sanketi, arXiv:2406.092462024arXiv preprint</p>
<p>Boyi Li, Philipp Wu, Pieter Abbeel, Jitendra Malik, arXiv:2310.10645Interactive task planning with language models. 2023arXiv preprint</p>
<p>Towards generalist robot policies: What matters in building vision-language-action models. Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, Hanbo Zhang, Huaping Liu, arXiv:2412.140582024arXiv preprint</p>
<p>Yi Li, Yuquan Deng, Jesse Zhang, Joel Jang, Marius Memmel, Raymond Yu, Caelan Reed Garrett, Fabio Ramos, Dieter Fox, Anqi Li, arXiv:2502.05485Hierarchical action models for open-world robot manipulation. 2025arXiv preprint</p>
<p>Code as policies: Language model programs for embodied control. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Advances in neural information processing systems. 202336</p>
<p>Aligning cyber space with physical world: A comprehensive survey on embodied ai. Yang Liu, Weixing Chen, Yongjie Bai, Xiaodan Liang, Guanbin Li, Wen Gao, Liang Lin, arXiv:2407.068862024arXiv preprint</p>
<p>Ilya Loshchilov, Frank Hutter, arXiv:1711.05101Decoupled weight decay regularization. 2017arXiv preprint</p>
<p>Data-agnostic robotic long-horizon manipulation with vision-language-guided closed-loop feedback. Yuan Meng, Xiangtong Yao, Haihui Ye, Yirui Zhou, Shengqiang Zhang, Zhenshan Bing, Alois Knoll, arXiv:2503.219692025arXiv preprint</p>
<p>Pivot: Iterative visual prompting elicits actionable knowledge for vlms. Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess, Ayzaan Wahid, Zhuo Xu, arXiv:2402.078722024arXiv preprint</p>
<p>Open xembodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. Abby O' Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PmLR2021</p>
<p>Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, Yuxiong He, Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining. the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining2020</p>
<p>Lucy Xiaoyang Shi, Zheyuan Hu, Tony Z Zhao, Archit Sharma, Karl Pertsch, Jianlan Luo, Sergey Levine, Chelsea Finn, arXiv:2403.12910Yell at your robot: Improving on-the-fly from language corrections. 2024arXiv preprint</p>
<p>Cliport: What and where pathways for robotic manipulation. Mohit Shridhar, Lucas Manuelli, Dieter Fox, Conference on robot learning. PMLR2022</p>
<p>Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, arXiv:2405.12213An open-source generalist robot policy. 2024arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Zixuan Wang, Bo Yu, Junzhe Zhao, Wenhao Sun, Sai Hou, Shuai Liang, Xing Hu, Yinhe Han, Yiming Gan, Karma, arXiv:2409.14908Augmenting embodied ai agents with long-and-short term memory systems. 2024arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Diffusion-vla: Scaling robot foundation models via unified diffusion and autoregression. Junjie Wen, Minjie Zhu, Yichen Zhu, Zhibin Tang, Jinming Li, Zhongyi Zhou, Chengmeng Li, Xiaoyu Liu, Yaxin Peng, Chaomin Shen, arXiv:2412.032932024arXiv preprint</p>
<p>Dexvla: Vision-language model with plug-in diffusion expert for general robot control. Junjie Wen, Yichen Zhu, Jinming Li, Zhibin Tang, Chaomin Shen, Feifei Feng, arXiv:2502.058552025arXiv preprint</p>
<p>Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, Haibin Yan, arXiv:2307.01848Embodied task planning with large language models. 2023arXiv preprint</p>
<p>Zhutian Yang, Caelan Garrett, Dieter Fox, Tomás Lozano-Pérez, Leslie Pack, Kaelbling , arXiv:2410.02193Guiding long-horizon task and motion planning with vision language models. 2024arXiv preprint</p>
<p>Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, Sergey Levine, Conference on robot learning. PMLR2020</p>
<p>Robotic control via embodied chain-of-thought reasoning. Michał Zawalski, William Chen, Karl Pertsch, Oier Mees, Chelsea Finn, Sergey Levine, arXiv:2407.086932024arXiv preprint</p>
<p>Transporter networks: Rearranging the visual world for robotic manipulation. Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian, Travis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani, Conference on Robot Learning. PMLR2021</p>
<p>Lohoravens: A long-horizon languageconditioned benchmark for robotic tabletop manipulation. Shengqiang Zhang, Philipp Wicke, Kerem Lütfi, Luis Şenel, Abdeldjallil Figueredo, Sami Naceri, Barbara Haddadin, Hinrich Plank, Schütze, arXiv:2310.120202023arXiv preprint</p>
<p>Vlabench: A large-scale benchmark for language-conditioned robotics manipulation with long-horizon reasoning tasks. Shiduo Zhang, Zhe Xu, Peiju Liu, Xiaopeng Yu, Yuan Li, Qinghui Gao, Zhaoye Fei, Zhangyue Yin, Zuxuan Wu, Yu-Gang Jiang, arXiv:2412.181942024arXiv preprint</p>
<p>Erra: An embodied representation and reasoning architecture for long-horizon languageconditioned manipulation tasks. Chao Zhao, Shuai Yuan, Chunli Jiang, Junhao Cai, Hongyu Yu, Michael Yu Wang, Qifeng Chen, IEEE Robotics and Automation Letters. 862023</p>
<p>Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. Qingqing Zhao, Yao Lu, Jin Moo, Zipeng Kim, Zhuoyang Fu, Yecheng Zhang, Zhaoshuo Wu, Qianli Li, Song Ma, Chelsea Han, Finn, arXiv:2503.220202025arXiv preprint</p>
<p>Isr-llm: Iterative self-refined large language model for long-horizon sequential task planning. Zhehua Zhou, Jiayang Song, Kunpeng Yao, Zhan Shu, Lei Ma, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Yuke Zhu, Josiah Wong, Ajay Mandlekar, Roberto Martín-Martín, Abhishek Joshi, Soroush Nasiriany, Yifeng Zhu, arXiv:2009.12293robosuite: A modular simulation framework and benchmark for robot learning. 2020arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7857 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7857</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7857</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-274777190</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.10424v3.pdf" target="_blank">LLM-as-an-Interviewer: Beyond Static Testing Through Dynamic LLM Evaluation</a></p>
                <p><strong>Paper Abstract:</strong> We introduce LLM-as-an-Interviewer, a novel paradigm for evaluating large language models (LLMs). This approach leverages multi-turn interactions where the LLM interviewer actively provides feedback on responses and poses follow-up questions to the evaluated LLM. At the start of the interview, the LLM interviewer dynamically modifies datasets to generate initial questions, mitigating data contamination. We apply the LLM-as-an-Interviewer framework to evaluate six models on the MATH and DepthQA tasks. Our results show that the framework effectively provides insights into LLM performance, including the quality of initial responses, adaptability to feedback, and ability to address follow-up queries like clarification or additional knowledge requests. The framework also addresses key limitations of conventional methods like LLM-as-a-Judge, including verbosity bias and inconsistency across runs. Finally, we propose the Interview Report, which aggregates insights from the interview process, providing examples and a comprehensive analysis of the LLM's strengths and weaknesses. This report offers a detailed snapshot of the model's real-world applicability. The code for our framework is publicly available at https://github.com/interview-eval/.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7857.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7857.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-Judge (prior claim)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-Judge evaluation framework (prior literature claim)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work claims that using large LMs as automatic judges yields evaluations that correlate better with human judgments than lexical or embedding-based metrics, but those works also report reliability issues (e.g., sensitivity to length/order and self-enhancement).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>LLM-AS-AN-INTERVIEWER: Beyond Static Testing Through Dynamic LLM Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>general free-form text evaluation (summaries/completions/instruction-following)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>various LLMs reported in prior work (e.g., GPT-family in cited papers)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Not specified in detail in this paper; referenced prior-work claims that LLM judges (closed models) often show higher correlation with humans than lexical/embedding metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>human judgments referenced from prior work (unspecified annotator pools in cited papers)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>static evaluation vulnerability to data contamination; verbosity bias; sensitivity to response length and order; self-enhancement bias; low self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Paper cites prior literature reporting higher correlation of LLM-as-a-Judge with human judgments versus BLEU/ROUGE/BERTScore but also notes known reliability issues (ordering/length sensitivity, self-enhancement, low self-consistency).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>More human-aligned than lexical/embedding metrics; scalable and reproducible relative to human evaluation (as reported in prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Mentioned as background / related work; no single experimental protocol in this paper for these prior claims.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-as-an-Interviewer: Beyond Static Testing Through Dynamic LLM Evaluation', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7857.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7857.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Judge vs Interviewer performance (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of static LLM-as-a-Judge evaluation vs dynamic LLM-as-an-Interviewer evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper compares the static LLM-as-a-Judge setting to its proposed LLM-as-an-Interviewer dynamic multi-turn evaluation on MATH, DepthQA and instruction-following tasks, reporting systematic differences (performance drops for many models under dynamic/modifed queries), contamination mitigation, and changes in robustness/variance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>LLM-AS-AN-INTERVIEWER: Beyond Static Testing Through Dynamic LLM Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>reasoning (MATH/MINT), factuality (DepthQA), instruction-following (MT-Bench)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>MATH, DepthQA, MINT (reasoning subset), MT-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>LLM-as-a-Judge (Judge setting) vs GPT-4o used as Interviewer in Interviewer setting</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>The paper uses GPT-4o as the default Interviewer; 'Judge' refers to the single-turn evaluation baseline (LLM-as-a-Judge) against which the Interviewer multi-turn results are compared.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Not applicable for this automated comparison; experiments compare automated Judge-mode evaluations vs the Interviewer multi-turn evaluations and also include human annotation for interviewer validation (see another result).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Under Judge (static) contaminated models can show inflated accuracy due to memorization; Judge is vulnerable to data contamination and can produce misleadingly high scores; single-turn Judges may be biased toward longer responses (verbosity) and show higher variance across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Key observations: (1) Moving from Judge to Interviewer (with query modification) causes a clear performance drop for many models on modified queries—e.g., contaminated models that scored very high in Judge drop substantially under Interviewer. (2) For DepthQA, GPT-4o showed only minimal decrease (≈0.2%) while other models experienced large declines. (3) Query modification in Interviewer mitigates contamination: contaminated models averaged much higher Judge accuracy (paper reports averages ~0.7 for OLMoE and ~0.4 for Zephyr under Judge) but drop substantially when evaluated with modified queries, becoming comparable to uncontaminated models.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>LLM-as-a-Judge (single-turn) is fast and low-cost for single-turn scoring but can be unreliable when contamination/memorization exist; LLM-as-a-Judge also generally aligns better than lexical metrics per prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>500 sampled queries (MATH and DepthQA), seed query modification applied for Interviewer setting; evaluations report Acc_seed@1/@2/@3 (performance at interaction turns), Interviewer uses GPT-4o; Interviewee models included GPT-4o, GPT-3.5, Llama-3.1-{8,70}b, DeepSeek-Math-7b, Qwen2.5-Math-7b. Contamination experiments trained models under several configurations (in-distribution train/test, OOD, instruction-tuning) and compared Judge vs Interviewer accuracies (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-as-an-Interviewer: Beyond Static Testing Through Dynamic LLM Evaluation', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7857.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7857.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Verbosity bias / length correlation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Verbosity bias: correlation of response length with Judge scores and mitigation in Interviewer setting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper measures the correlation between answer length and assigned score, showing a positive correlation in LLM-as-a-Judge that weakens across Interviewer interactions, indicating reduced verbosity bias.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>LLM-AS-AN-INTERVIEWER: Beyond Static Testing Through Dynamic LLM Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>DepthQA (analysed for length vs score)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>DepthQA (analysis plots in Appendix H.1)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>LLM-as-a-Judge (unspecified judge model in general analyses) and Interviewer using GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Analysis shows LLM-as-a-Judge setting exhibits a significant positive correlation between length and score (r = 0.371, p < .05); as interactions increase in the Interviewer process, the correlation coefficient decreases and p-value rises.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson r (length vs score)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.371</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Judge scores are positively correlated with reply length (verbosity bias); Judge can favor longer responses even when longer does not imply better factual quality.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Interviewer interactions (feedback + revisions) reduce length dependency and weaken verbosity bias; the paper also references length-control mechanisms (Dubois et al.) as complementary debiasing approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Single-turn Judges are susceptible to verbosity bias; Interviewer reduces this bias through iterative interaction and feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>DepthQA analysis using GPT-4o as Interviewer; they plot token-length distributions vs scores and compute Pearson r at different interaction turns (@1, @2, @3).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-as-an-Interviewer: Beyond Static Testing Through Dynamic LLM Evaluation', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7857.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7857.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-enhancement bias & human annotation reliability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Observed self-enhancement bias in LLM judges and human evaluation metrics used to validate Interviewer roles</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper inspects self-enhancement (models rating themselves higher) and validates Interviewer roles with human annotators, reporting annotator agreement and interviewer accuracy estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>LLM-AS-AN-INTERVIEWER: Beyond Static Testing Through Dynamic LLM Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>MATH and DepthQA analyses; human validation of interviewer components</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>MATH, DepthQA; human validation used ≥100 samples per interviewer-dimension (300+ samples per model)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Examined models acting as judges/interviewers: GPT-4o, Llama-3.1-70B, Llama-3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Paper reports GPT-4o and Llama-3.1-8B tend to give themselves slightly lower scores; Llama-3.1-70B assigns itself marginally higher scores (self-enhancement) though impact on rankings is negligible and bias diminishes with interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Four authors annotated interviewer outputs (query modification, feedback generation, follow-up generation); additional two annotators labeled 100 cases for reliability check.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Fleiss' kappa (inter-annotator agreement)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.653</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Some models show mild self-enhancement bias when rating their own outputs; automated judges can be inconsistent and introduce biases similar to those in LLM-as-a-Judge.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Self-enhancement bias was observable mainly for Llama-3.1-70B; bias reduced over interactions. Human validation showed interviewer components (GPT-4o etc.) achieve high task-appropriate accuracy (paper reports GPT-4o ≈ 90% accuracy as an interviewer in analyses) but also expose failure modes in feedback and follow-ups.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Human validation supports the Interviewer design; using human annotators allowed measurement of interviewer output correctness and revealed failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Human evaluation: four authors evaluated ≥100 samples per dimension for three interviewer models (query modification, feedback, follow-ups), yielding >300 samples per model; additional 100 cases labeled by two extra annotators produced Fleiss' kappa = 0.653. The paper reports ~90% interviewer accuracy for GPT-4o from these analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-as-an-Interviewer: Beyond Static Testing Through Dynamic LLM Evaluation', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7857.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7857.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Error propagation & multi-turn degradation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Observed error propagation across multi-turn interactions and small degradation compared to independent error compounding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper measures how interviewer errors compound (or fail to) across turns: observed degradation across turns is much smaller than naïvely expected if errors were independent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>LLM-AS-AN-INTERVIEWER: Beyond Static Testing Through Dynamic LLM Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>multi-round interviewer evaluation (general across tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Interviewer model (GPT-4o) measuring multi-round accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Multi-round accuracy measured; if independent errors compounded, expected ≈9% drop by second turn and ≈18% by third, but observed degradation was much smaller.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Errors correlate across turns leading to less-than-expected compounding degradation; nevertheless, some accuracy degradation exists (reported Turn2: -1.6%, Turn3: -4%).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Because errors are correlated across turns, multi-turn degradation is smaller than independent-error model would predict; nonetheless error-propagation remains a failure mode to monitor.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>N/A (this concerns Interviewer behavior vs naive expectations).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Measured multi-round accuracy and reported observed accuracy degradations: Turn2 -1.6%, Turn3 -4% (Table 8).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-as-an-Interviewer: Beyond Static Testing Through Dynamic LLM Evaluation', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can large language models be an alternative to human evaluations? <em>(Rating: 2)</em></li>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Length-controlled AlpacaEval: A simple way to debias automatic evaluators <em>(Rating: 2)</em></li>
                <li>MT-Bench <em>(Rating: 2)</em></li>
                <li>FActScore: Fine-grained atomic evaluation of factual precision in long form text generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7857",
    "paper_id": "paper-274777190",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "LLM-as-a-Judge (prior claim)",
            "name_full": "LLM-as-a-Judge evaluation framework (prior literature claim)",
            "brief_description": "Prior work claims that using large LMs as automatic judges yields evaluations that correlate better with human judgments than lexical or embedding-based metrics, but those works also report reliability issues (e.g., sensitivity to length/order and self-enhancement).",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "LLM-AS-AN-INTERVIEWER: Beyond Static Testing Through Dynamic LLM Evaluation",
            "evaluation_task": "general free-form text evaluation (summaries/completions/instruction-following)",
            "dataset_name": "",
            "judge_model_name": "various LLMs reported in prior work (e.g., GPT-family in cited papers)",
            "judge_model_details": "Not specified in detail in this paper; referenced prior-work claims that LLM judges (closed models) often show higher correlation with humans than lexical/embedding metrics.",
            "human_evaluator_type": "human judgments referenced from prior work (unspecified annotator pools in cited papers)",
            "agreement_metric": null,
            "agreement_score": null,
            "reported_loss_aspects": "static evaluation vulnerability to data contamination; verbosity bias; sensitivity to response length and order; self-enhancement bias; low self-consistency",
            "qualitative_findings": "Paper cites prior literature reporting higher correlation of LLM-as-a-Judge with human judgments versus BLEU/ROUGE/BERTScore but also notes known reliability issues (ordering/length sensitivity, self-enhancement, low self-consistency).",
            "advantages_of_llm_judge": "More human-aligned than lexical/embedding metrics; scalable and reproducible relative to human evaluation (as reported in prior work).",
            "experimental_setting": "Mentioned as background / related work; no single experimental protocol in this paper for these prior claims.",
            "uuid": "e7857.0",
            "source_info": {
                "paper_title": "LLM-as-an-Interviewer: Beyond Static Testing Through Dynamic LLM Evaluation",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Judge vs Interviewer performance (this paper)",
            "name_full": "Comparison of static LLM-as-a-Judge evaluation vs dynamic LLM-as-an-Interviewer evaluation",
            "brief_description": "This paper compares the static LLM-as-a-Judge setting to its proposed LLM-as-an-Interviewer dynamic multi-turn evaluation on MATH, DepthQA and instruction-following tasks, reporting systematic differences (performance drops for many models under dynamic/modifed queries), contamination mitigation, and changes in robustness/variance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "LLM-AS-AN-INTERVIEWER: Beyond Static Testing Through Dynamic LLM Evaluation",
            "evaluation_task": "reasoning (MATH/MINT), factuality (DepthQA), instruction-following (MT-Bench)",
            "dataset_name": "MATH, DepthQA, MINT (reasoning subset), MT-Bench",
            "judge_model_name": "LLM-as-a-Judge (Judge setting) vs GPT-4o used as Interviewer in Interviewer setting",
            "judge_model_details": "The paper uses GPT-4o as the default Interviewer; 'Judge' refers to the single-turn evaluation baseline (LLM-as-a-Judge) against which the Interviewer multi-turn results are compared.",
            "human_evaluator_type": "Not applicable for this automated comparison; experiments compare automated Judge-mode evaluations vs the Interviewer multi-turn evaluations and also include human annotation for interviewer validation (see another result).",
            "agreement_metric": null,
            "agreement_score": null,
            "reported_loss_aspects": "Under Judge (static) contaminated models can show inflated accuracy due to memorization; Judge is vulnerable to data contamination and can produce misleadingly high scores; single-turn Judges may be biased toward longer responses (verbosity) and show higher variance across runs.",
            "qualitative_findings": "Key observations: (1) Moving from Judge to Interviewer (with query modification) causes a clear performance drop for many models on modified queries—e.g., contaminated models that scored very high in Judge drop substantially under Interviewer. (2) For DepthQA, GPT-4o showed only minimal decrease (≈0.2%) while other models experienced large declines. (3) Query modification in Interviewer mitigates contamination: contaminated models averaged much higher Judge accuracy (paper reports averages ~0.7 for OLMoE and ~0.4 for Zephyr under Judge) but drop substantially when evaluated with modified queries, becoming comparable to uncontaminated models.",
            "advantages_of_llm_judge": "LLM-as-a-Judge (single-turn) is fast and low-cost for single-turn scoring but can be unreliable when contamination/memorization exist; LLM-as-a-Judge also generally aligns better than lexical metrics per prior work.",
            "experimental_setting": "500 sampled queries (MATH and DepthQA), seed query modification applied for Interviewer setting; evaluations report Acc_seed@1/@2/@3 (performance at interaction turns), Interviewer uses GPT-4o; Interviewee models included GPT-4o, GPT-3.5, Llama-3.1-{8,70}b, DeepSeek-Math-7b, Qwen2.5-Math-7b. Contamination experiments trained models under several configurations (in-distribution train/test, OOD, instruction-tuning) and compared Judge vs Interviewer accuracies (Table 5).",
            "uuid": "e7857.1",
            "source_info": {
                "paper_title": "LLM-as-an-Interviewer: Beyond Static Testing Through Dynamic LLM Evaluation",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Verbosity bias / length correlation",
            "name_full": "Verbosity bias: correlation of response length with Judge scores and mitigation in Interviewer setting",
            "brief_description": "The paper measures the correlation between answer length and assigned score, showing a positive correlation in LLM-as-a-Judge that weakens across Interviewer interactions, indicating reduced verbosity bias.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "LLM-AS-AN-INTERVIEWER: Beyond Static Testing Through Dynamic LLM Evaluation",
            "evaluation_task": "DepthQA (analysed for length vs score)",
            "dataset_name": "DepthQA (analysis plots in Appendix H.1)",
            "judge_model_name": "LLM-as-a-Judge (unspecified judge model in general analyses) and Interviewer using GPT-4o",
            "judge_model_details": "Analysis shows LLM-as-a-Judge setting exhibits a significant positive correlation between length and score (r = 0.371, p &lt; .05); as interactions increase in the Interviewer process, the correlation coefficient decreases and p-value rises.",
            "human_evaluator_type": null,
            "agreement_metric": "Pearson r (length vs score)",
            "agreement_score": 0.371,
            "reported_loss_aspects": "Judge scores are positively correlated with reply length (verbosity bias); Judge can favor longer responses even when longer does not imply better factual quality.",
            "qualitative_findings": "Interviewer interactions (feedback + revisions) reduce length dependency and weaken verbosity bias; the paper also references length-control mechanisms (Dubois et al.) as complementary debiasing approaches.",
            "advantages_of_llm_judge": "Single-turn Judges are susceptible to verbosity bias; Interviewer reduces this bias through iterative interaction and feedback.",
            "experimental_setting": "DepthQA analysis using GPT-4o as Interviewer; they plot token-length distributions vs scores and compute Pearson r at different interaction turns (@1, @2, @3).",
            "uuid": "e7857.2",
            "source_info": {
                "paper_title": "LLM-as-an-Interviewer: Beyond Static Testing Through Dynamic LLM Evaluation",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Self-enhancement bias & human annotation reliability",
            "name_full": "Observed self-enhancement bias in LLM judges and human evaluation metrics used to validate Interviewer roles",
            "brief_description": "The paper inspects self-enhancement (models rating themselves higher) and validates Interviewer roles with human annotators, reporting annotator agreement and interviewer accuracy estimates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "LLM-AS-AN-INTERVIEWER: Beyond Static Testing Through Dynamic LLM Evaluation",
            "evaluation_task": "MATH and DepthQA analyses; human validation of interviewer components",
            "dataset_name": "MATH, DepthQA; human validation used ≥100 samples per interviewer-dimension (300+ samples per model)",
            "judge_model_name": "Examined models acting as judges/interviewers: GPT-4o, Llama-3.1-70B, Llama-3.1-8B",
            "judge_model_details": "Paper reports GPT-4o and Llama-3.1-8B tend to give themselves slightly lower scores; Llama-3.1-70B assigns itself marginally higher scores (self-enhancement) though impact on rankings is negligible and bias diminishes with interactions.",
            "human_evaluator_type": "Four authors annotated interviewer outputs (query modification, feedback generation, follow-up generation); additional two annotators labeled 100 cases for reliability check.",
            "agreement_metric": "Fleiss' kappa (inter-annotator agreement)",
            "agreement_score": 0.653,
            "reported_loss_aspects": "Some models show mild self-enhancement bias when rating their own outputs; automated judges can be inconsistent and introduce biases similar to those in LLM-as-a-Judge.",
            "qualitative_findings": "Self-enhancement bias was observable mainly for Llama-3.1-70B; bias reduced over interactions. Human validation showed interviewer components (GPT-4o etc.) achieve high task-appropriate accuracy (paper reports GPT-4o ≈ 90% accuracy as an interviewer in analyses) but also expose failure modes in feedback and follow-ups.",
            "advantages_of_llm_judge": "Human validation supports the Interviewer design; using human annotators allowed measurement of interviewer output correctness and revealed failure modes.",
            "experimental_setting": "Human evaluation: four authors evaluated ≥100 samples per dimension for three interviewer models (query modification, feedback, follow-ups), yielding &gt;300 samples per model; additional 100 cases labeled by two extra annotators produced Fleiss' kappa = 0.653. The paper reports ~90% interviewer accuracy for GPT-4o from these analyses.",
            "uuid": "e7857.3",
            "source_info": {
                "paper_title": "LLM-as-an-Interviewer: Beyond Static Testing Through Dynamic LLM Evaluation",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Error propagation & multi-turn degradation",
            "name_full": "Observed error propagation across multi-turn interactions and small degradation compared to independent error compounding",
            "brief_description": "The paper measures how interviewer errors compound (or fail to) across turns: observed degradation across turns is much smaller than naïvely expected if errors were independent.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "LLM-AS-AN-INTERVIEWER: Beyond Static Testing Through Dynamic LLM Evaluation",
            "evaluation_task": "multi-round interviewer evaluation (general across tasks)",
            "dataset_name": "",
            "judge_model_name": "Interviewer model (GPT-4o) measuring multi-round accuracy",
            "judge_model_details": "Multi-round accuracy measured; if independent errors compounded, expected ≈9% drop by second turn and ≈18% by third, but observed degradation was much smaller.",
            "human_evaluator_type": null,
            "agreement_metric": null,
            "agreement_score": null,
            "reported_loss_aspects": "Errors correlate across turns leading to less-than-expected compounding degradation; nevertheless, some accuracy degradation exists (reported Turn2: -1.6%, Turn3: -4%).",
            "qualitative_findings": "Because errors are correlated across turns, multi-turn degradation is smaller than independent-error model would predict; nonetheless error-propagation remains a failure mode to monitor.",
            "advantages_of_llm_judge": "N/A (this concerns Interviewer behavior vs naive expectations).",
            "experimental_setting": "Measured multi-round accuracy and reported observed accuracy degradations: Turn2 -1.6%, Turn3 -4% (Table 8).",
            "uuid": "e7857.4",
            "source_info": {
                "paper_title": "LLM-as-an-Interviewer: Beyond Static Testing Through Dynamic LLM Evaluation",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can large language models be an alternative to human evaluations?",
            "rating": 2,
            "sanitized_title": "can_large_language_models_be_an_alternative_to_human_evaluations"
        },
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "Length-controlled AlpacaEval: A simple way to debias automatic evaluators",
            "rating": 2,
            "sanitized_title": "lengthcontrolled_alpacaeval_a_simple_way_to_debias_automatic_evaluators"
        },
        {
            "paper_title": "MT-Bench",
            "rating": 2
        },
        {
            "paper_title": "FActScore: Fine-grained atomic evaluation of factual precision in long form text generation",
            "rating": 1,
            "sanitized_title": "factscore_finegrained_atomic_evaluation_of_factual_precision_in_long_form_text_generation"
        }
    ],
    "cost": 0.0172695,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LLM-AS-AN-INTERVIEWER: Beyond Static Testing Through Dynamic LLM Evaluation</p>
<p>Eunsu Kim 
Juyoung Suk 
Seungone Kim 
Carnegie Mellon University</p>
<p>Niklas Muennighoff 
Stanford University</p>
<p>ContextualAI</p>
<p>Dongkwan Kim 
Alice Oh alice.oh@kaist.edu 
Kaist 
Aaron Grattafiori 
Abhimanyu Dubey 
Abhinav Jauhri 
Abhinav Pandey 
Abhishek Kadian 
Ahmad Al- Dahle 
Aiesha Letman 
Akhil Mathur 
Alan Schel- Ten 
Alex Vaughan 
Amy Yang 
Angela Fan 
Anirudh Goyal 
Anthony Hartshorn 
Aobo Yang 
Archi Mi- Tra 
Archie Sravankumar 
Artem Korenev 
Arthur Hinsvark 
Arun Rao 
Aston Zhang 
Aurelien Ro- Driguez 
Austen Gregerson 
Ava Spataru 
Bap- Tiste Roziere 
Bethany Biron 
Binh Tang 
Bob- Bie Chern 
Charlotte Caucheteux 
Chaya Nayak 
Chloe Bi 
Chris Marra 
Chris Mcconnell 
Christian Keller 
Christophe Touret 
Chunyang Wu 
Corinne Wong 
Cristian Canton Ferrer 
Cyrus Nikolaidis 
Damien Allonsius 
Daniel Song 
Danielle Pintz 
Danny Livshits 
Danny Wyatt 
David Esiobu 
Dhruv Choudhary 
Dhruv Mahajan 
Diego Garcia-Olano 
Diego Perino 
Dieuwke Hupkes 
Egor Lakomkin 
Ehab Albadawy 
Elina Lobanova 
Emily Dinan 
Eric Michael Smith 
Filip Radenovic 
Francisco Guzmán 
Frank Zhang 
Gabriel Synnaeve 
Gabrielle Lee 
Georgia Lewis Anderson 
Govind Thattai 
Graeme Nail 
Gregoire Mialon 
Guan Pang 
Guillem Cucurell 
Hailey Nguyen 
Hannah Korevaar 
Hu Xu 
Hugo Touvron 
ImanolIliyan Zarov 
Arrieta Ibarra 
Isabel Kloumann 
Ishan Misra 
Ivan Evtimov 
Jack Zhang 
Jade Copet 
Jaewon Lee 
Jan Geffert 
Jana Vranes 
Jason Park 
Jay Mahadeokar 
Jeet Shah 
Jelmer Van Der Linde 
Jennifer Billock 
Jenny Hong 
Jenya Lee 
Jeremy Fu 
Jianfeng Chi 
Jianyu Huang 
Jiawen Liu 
Jie Wang 
Jiecao Yu 
Joanna Bitton 
Joe Spisak 
Jongsoo Park 
Joseph Rocca 
Joshua Johnstun 
Joshua Saxe 
Junteng Jia 
Kalyan Vasuden Alwala 
Karthik Prasad 
Kartikeya Upasani 
Kate Plawiak 
Keqian Li 
Kenneth Heafield 
Kevin Stone 
Khalid El-Arini 
Krithika Iyer 
Kshitiz Malik 
Kuen- Ley Chiu 
Kunal Bhalla 
Kushal Lakhotia 
Lauren Rantala-Yeary 
Laurens Van Der Maaten 
Lawrence Chen 
Liang Tan 
Liz Jenkins 
Louis Martin 
Lo- Vish Madaan 
Lubo Malo 
Lukas Blecher 
Lukas Landzaat 
Luke De Oliveira 
Madeline Muzzi 
Ma- Hesh Pasupuleti 
Mannat Singh 
Manohar Paluri 
Marcin Kardas 
Maria Tsimpoukelli 
Mathieu Rita 
Maya Pavlova 
Melanie Kam- Badur 
Mike Lewis 
MiteshMin Si 
Kumar Singh 
Mona Hassan 
Naman Goyal 
Narjes Torabi 
Niko- Lay Bashlykov 
Nikolay Bogoychev 
Niladri Chat- Terji 
Ning Zhang 
Olivier Duchenne 
Onur Çelebi 
Patrick Alrassy 
Pengchuan Zhang 
Pengwei Li 
Petar Vasic 
Peter Weng 
Prajjwal Bhargava 
Pratik Dubal 
PunitPraveen Krishnan 
Singh Koura 
Puxin Xu 
Qing He 
Qingxiao Dong 
Ragavan Srini- Vasan 
Raj Ganapathy 
Ramon Calderer 
Ricardo Sil- Veira Cabral 
Robert Stojnic 
Roberta Raileanu 
Ro- Han Maheswari 
Rohit Girdhar 
Rohit Patel 
Ro- Main Sauvestre 
Ronnie Polidoro 
Roshan Sum- Baly 
Ross Taylor 
Ruan Silva 
Rui Hou 
Rui Wang 
Saghar Hosseini 
Sahana Chennabasappa 
Sanjay Singh 
SeohyunSean Bell 
Sonia Kim 
Sergey Edunov 
Shaoliang Nie 
Sharan Narang 
Sharath Ra- Parthy 
Sheng Shen 
Shengye Wan 
Shruti Bhos- Ale 
Shun Zhang 
Simon Vandenhende 
Soumya Ba- Tra 
Spencer Whitman 
Sten Sootla 
Stephane Col- Lot 
Suchin Gururangan 
Sydney Borodinsky 
Tamar Herman 
Tara Fowler 
Tarek Sheasha 
Thomas Georgiou 
Thomas Scialom 
Tobias Speckbacher 
Todor Mihaylov 
Tong Xiao 
Ujjwal Karn 
Vedanuj Goswami 
Vibhor Gupta 
Vignesh Ramanathan 
Viktor Kerkez 
Vincent Gonguet 
Virginie Do 
Vish Vogeti 
Vítor Albiero 
Vladan Petrovic 
Wei- Wei Chu 
Wenhan Xiong 
Wenyin Fu 
Whitney Meers 
Xavier Martinet 
Xiaodong Wang 
XiaoqingXiao- Fang Wang 
Ellen Tan 
Xide Xia 
Xin- Feng Xie 
Xuchao Jia 
Xuewei Wang 
Yaelle Gold- Schlag 
Yashesh Gaur 
Yasmine Babaei 
Yi Wen 
Yi- Wen Song 
Yuchen Zhang 
Yue Li 
Yuning Mao 
Zacharie Delpierre Coudert 
Zheng Yan 
Zhengx- Ing Chen 
Zoe Papakipos 
Aaditya Singh 
Aayushi Srivastava 
Abha Jain 
Adam Kelsey 
Adam Sha- Jnfeld 
Adithya Gangidi 
Adolfo Victoria 
Ahuva Goldstand 
Ajay Menon 
Ajay Sharma 
Alex Boe- Senberg 
Alexei Baevski 
Allie Feinstein 
Amanda Kallet 
Amit Sangani 
Amos Teo 
Anam Yunus 
An- Drei Lupu 
Andres Alvarado 
Andrew Caples 
An- Drew Gu 
Andrew Ho 
Andrew Poulton 
Andrew Ryan 
Ankit Ramchandani 
Annie Dong 
Annie Franco 
Anuj Goyal 
Aparajita Saraf 
Arkabandhu Chowdhury 
Ashley Gabriel 
Ashwin Bharambe 
Assaf Eisenman 
Azadeh Yazdan 
Beau James 
Ben Maurer 
Benjamin Leonhardi 
Bernie Huang 
Beth Loyd 
Beto De Paola 
Bhargavi Paranjape 
Bing Liu 
Bo Wu 
Boyu Ni 
Braden Hancock 
Bram Wasti 
Brandon Spence 
Brani Stojkovic 
Brian Gamido 
Britt Montalvo 
Carl Parker 
Carly Burton 
Catalina Mejia 
Ce Liu 
Changhan Wang 
Changkyu Kim 
Chao Zhou 
Chester Hu 
Ching-Hsiang Chu 
Elaine Montgomery 
Eleonora Presani 
Emily Hahn 
Emily Wood 
Eric-Tuan Le 
Erik Brinkman 
Esteban Arcaute 
Evan Dunbar 
Evan Smothers 
Fei Sun 
Fe- Lix Kreuk 
Feng Tian 
Filippos Kokkinos 
Oz- Genel Firat 
Francesco Caggioni 
Frank Kanayet 
Frank Seide 
Gabriela Medina Florez 
Gabriella Schwarz 
Gada Badeer 
Georgia Swee 
Gil Halpern 
Grant Herman 
Grigory Sizov 
Guangyi 
Guna Zhang 
Hakan Lak- Shminarayanan 
Hamid Inan 
Han Shojanazeri 
Hannah Zou 
Hanwen Wang 
Haroun Zha 
Harrison Habeeb 
Helen Rudolph 
Henry Suk 
Hunter As- Pegren 
Hongyuan Goldman 
Ibrahim Zhan 
Igor Damlaj 
Igor Molybog 
Ilias Tufanov 
Leon- Tiadis 
Jennifer Chan 
Jenny Zhen 
Jeremy Reizenstein 
Jeremy Teboul 
Jessica Zhong 
Jian Jin 
Jingyi Yang 
Joe Cummings 
Jon Carvill 
Jon Shepard 
Jonathan Mcphie 
Jonathan Torres 
Josh Ginsburg 
Junjie Wang 
Kai Wu 
Kam Hou 
Karan Saxena 
Kar- Tikay Khandelwal 
Katayoun Zand 
Kathy Matosich 
Kaushik Veeraraghavan 
Kelly Michelena 
Kiran Jagadeesh 
Kun Huang 
Kunal Chawla 
Kyle Huang 
Lailin Chen 
Lakshya Garg 
Leandro Silva 
Lee Bell 
Lei Zhang 
Liang- Peng Guo 
Licheng Yu 
Liron Moshkovich 
Luca Wehrstedt 
Madian Khabsa 
Manav Avalani 
Manish Bhatt 
Martynas Mankus 
Matan Hasson 
Matthew Lennie 
Matthias Reso 
Maxim Groshev 
Maxim Naumov 
Maya Lathi 
Meghan Keneally 
Miao Liu 
Michael L Seltzer 
Michal Valko 
Michelle Restrepo 
Mihir Patel 
Mik Vyatskov 
Mikayel Samvelyan 
Mike Clark 
Mike Macey 
Mike Wang 
Miquel Jubert Hermoso 
Mo Metanat 
Mohammad Rastegari 
Munish Bansal 
Nandhini Santhanam 
Natascha Parks 
Natasha White 
Navyata Bawa 
Nayan Singhal 
Nick Egebo 
Nicolas Usunier 
NikolayNikhil Mehta 
Pavlovich Laptev 
Ning Dong 
Nor- Man Cheng 
Oleg Chernoguz 
Olivia Hart 
Omkar Salpekar 
Ozlem Kalinli 
Parkin Kent 
Parth Parekh 
Paul Saab 
Pavan Balaji 
Pedro Rittner 
Philip Bon- Trager 
Pierre Roux 
PolinaPiotr Dollar 
Prashant Ratanchandani 
Pritish Yuvraj 
Qian Liang 
Rachad Alao 
Rachel Rodriguez 
Rafi Ayub 
Raghotham Murthy 
Raghu Nayani 
Rahul Mi- Tra 
Rangaprabhu Parthasarathy 
Raymond Li 
Sargun Hunt 
Sasha Dhillon 
Satadru Sidorov 
Saurabh Pan 
Saurabh Mahajan 
Seiji Verma 
Sharadh Yamamoto 
Shaun Ramaswamy 
Shaun Lindsay 
Sheng Lind- Say 
Shenghao Feng 
ShengxinCindy Lin 
Shishir Zha 
Shiva Patil 
Shuqiang Shankar 
Shuqiang Zhang 
Sinong Zhang 
Sneha Wang 
Soji Agarwal 
Soumith Sajuyigbe 
Stephanie Chintala 
Stephen Max 
Steve Chen 
Steve Kehoe 
Sudarshan Satterfield 
Sumit Govindaprasad 
Summer Gupta 
Sungmin Deng 
Sunny Cho 
Suraj Virk 
Sy Subrama- Nian 
Sydney Choudhury 
Tal Goldman 
Tamar Remez 
Tamara Glaser 
Thilo Best 
Thomas Koehler 
Tianhe Robinson 
Tianjun Li 
Tim Zhang 
Timothy Matthews 
Tzook Chou 
Varun Shaked 
Victoria Vontimitta 
Victoria Ajayi 
Vijai Montanez 
Vinay Satish Mohan 
Vishal Kumar 
Vlad Mangla 
Vlad Ionescu 
Vlad Tiberiu Poenaru 
Vladimir Mihailescu 
Wei Ivanov 
Wenchen Li 
Wenwen Wang 
Wes Jiang 
Will Bouaziz 
Xiaocheng Constable 
Xi- Aojian Tang 
Xiaolan Wu 
Xilun Wang 
Xinbo Wu 
Yaniv Gao 
Yanjun Kleinman 
Ye Chen 
Ye Hu 
Ye Jia 
Yenda Qi 
Yilin Li 
Ying Zhang 
Yossi Zhang 
Youngjin Adi 
Yu Nam 
Yu Wang 
Yuchen Zhao 
Yundi Hao 
Yunlu Qian 
Yuzi Li 
Zach He 
Zachary Rait 
Zef Devito 
Zhaoduo Rosnbrick 
Zhenyu Wen 
Zhiwei Yang 
Zhiyu Zhao 
Ma 
Jiawei Gu 
Xuhui Jiang 
Zhichao Shi 
Hexiang Tan 
Xuehao Zhai 
Chengjin Xu 
Wei Li 
Yinghan Shen 
Luca Soldaini 
Dirk Groeneveld 
Kyle Lo 
Jacob Morrison 
Sewon Min 
Weijia Shi 
Pete Walsh 
Oyvind Tafjord 
Nathan Lam- Bert 
Yuling Gu 
Shane Arora 
Akshita Bhagia 
Dustin Schwenk 
David Wadden 
Alexander Wet- Tig 
Binyuan Hui 
Tim Dettmers 
Douwe Kiela 
Ali Farhadi 
PangNoah A Smith 
Wei Koh 
Aman- Preet Singh 
Hannaneh 2024 Hajishirzi 
Ol 
Josh Openai 
Steven Achiam 
Sandhini Adler 
Lama Agar- Wal 
Ilge Ahmad 
Florencia Leoni Akkaya 
Diogo Aleman 
Janko Almeida 
Sam Altenschmidt 
Shyamal Altman 
Red Anadkat 
Igor Avila 
Suchir Babuschkin 
Valerie Balaji 
Paul Balcom 
Haiming Baltescu 
Mohammad Bao 
Jeff Bavarian 
Irwan Belgum 
Jake Bello 
Gabriel Berdine 
Christopher Bernadett-Shapiro 
Lenny Berner 
Oleg Bog- Donoff 
Madelaine Boiko 
Anna-Luisa Boyd 
Greg Brakman 
Tim Brockman 
Miles Brooks 
Kevin Brundage 
Trevor Button 
Rosie Cai 
Andrew Camp- Bell 
Brittany Cann 
Chelsea Carey 
Rory Carl- Son 
Brooke Carmichael 
Che Chan 
Fotis Chang 
Derek Chantzis 
Sully Chen 
Ruby Chen 
Jason Chen 
Mark Chen 
Ben Chen 
Chester Chess 
Casey Cho 
HyungWon Chu 
Dave Chung 
Jeremiah Cum- Mings 
Yunxing Currier 
Cory Dai 
Thomas De- Careaux 
Noah Degry 
Damien Deutsch 
Arka Deville 
David Dhar 
Steve Dohan 
Sheila Dowling 
Adrien Dunning 
Atty Ecoffet 
Tyna Eleti 
David Eloundou 
Liam Farhi 
Niko Fedus 
SimónPosada Felix 
Juston Fishman 
Isabella Forte 
Leo Ful- Ford 
Elie Gao 
Christian Georges 
Vik Gibson 
Tarun Goel 
Gabriel Gogineni 
Rapha Goh 
Jonathan Gontijo- Lopes 
Morgan Gordon 
Scott Grafstein 
Ryan Gray 
Joshua Greene 
ShixiangShane Gross 
Yufei Gu 
Chris Guo 
Jesse Hallacy 
Jeff Han 
Yuchen Harris 
Mike He 
Johannes Heaton 
Chris Heidecke 
Alan Hesse 
Wade Hickey 
Peter Hickey 
Brandon Hoeschele 
Kenny Houghton 
Shengli Hsu 
Xin Hu 
Joost Hu 
Shantanu Huizinga 
Shawn Jain 
Joanne Jain 
Angela Jang 
Roger Jiang 
Haozhun Jiang 
Denny Jin 
Shino Jin 
Billie Jomoto 
Hee- Woo Jonn 
Tomer Jun 
Łukasz Kaftan 
Ali Kaiser 
Ingmar Ka- Mali 
Kanitscheider 
Shirish Nitish 
Tabarak Keskar 
Logan Khan 
Jong Wook Kilpatrick 
Christina Kim 
Yongjik Kim 
Jan Hendrik Kim 
Jamie Kirchner 
Matt Kiros 
Daniel Knight 
Łukasz Kokotajlo 
Andrew Kondraciuk 
Aris Kondrich 
Kyle Konstantinidis 
Gretchen Kosic 
Vishal Krueger 
Michael Kuo 
Ikai Lampe 
Teddy Lan 
Jan Lee 
Jade Leike 
Daniel Le- Ung 
ChakMing Levy 
Rachel Li 
Molly Lim 
Stephanie Lin 
Mateusz Lin 
Theresa Litwin 
Ryan Lopez 
Patricia Lowe 
Anna Lue 
Kim Makanju 
Sam Malfacini 
Todor Manning 
Yaniv Markov 
Bianca Markovski 
Katie Martin 
Andrew Mayer 
Bob Mayne 
Scott Mayer Mcgrew 
Christine Mckinney 
Paul Mcleavey 
Jake Mcmillan 
David Mcneil 
Aalok Medina 
Jacob Mehta 
Luke Menick 
Andrey Metz 
Pamela Mishchenko 
Vin- Nie Mishkin 
Evan Monaco 
Daniel Morikawa 
Tong Mossing 
Mira Mu 
Oleg Murati 
David Murk 
Ashvin Mély 
Reiichiro Nair 
Rajeev Nakano 
Arvind Nayak 
Richard Neelakantan 
Hyeonwoo Ngo 
Long Noh 
Cullen Ouyang 
Jakub O'keefe 
Alex Pachocki 
Joe Paino 
Ashley Palermo 
Pantuliano 
Nick Ryder 
Mario Saltarelli 
Ted Sanders 
Shibani Santurkar 
Girish Sastry 
Heather Schmidt 
David Schnurr 
John Schulman 
Daniel Selsam 
Kyla Shep- Pard 
Toki Sherbakov 
Jessica Shieh 
Sarah Shoker 
Pranav Shyam 
Szymon Sidor 
Eric Sigler 
Maddie Simens 
Jordan Sitkin 
Katarina Slama 
Ian Sohl 
Benjamin Sokolowsky 
Yang Song 
Natalie Stau- Dacher 
Felipe Petroski Such 
Natalie Summers 
Ilya Sutskever 
Jie Tang 
Nikolas Tezak 
Madeleine B Thompson 
Phil Tillet 
Amin Tootoonchian 
Eliz- Abeth Tseng 
Preston Tuggle 
Nick Turley 
Jerry Tworek 
Juan Felipe 
Cerón Uribe 
Andrea Vallone </p>
<p>Chris Cai
Chris Tindal</p>
<p>Christoph Feichtenhofer
Cynthia Gao, Damon Civin, Daniel Kreymer, Daniel Li, David XuDana Beaty, David Adkins</p>
<p>Da-vide Testuggine
Delia David, Devi Parikh, Didem Foss, Dingkang Wang, Dustin Holland, Edward DowlingDi-ana Liskovich, Duc Le</p>
<p>Eissa Jamil</p>
<p>Itai Gat
Irina-Elena Veliche
Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff MarcusJeff Tang</p>
<p>Russ Howes
Robin Battey, Rocky Wang, Ruty Rinott, Sachin Mehta, Sachin SibyRe-bekkah Hogan</p>
<p>Sai Jayesh Bondu
Samyak Datta</p>
<p>Sara Chugh
Sara</p>
<p>Giambat-tista Parascandolo
Joel Parish
Emy Parparita, Mikhail Pavlov, Andrew PengAlex Passos</p>
<p>Filipe de Avila Belbute Peres
Adam Perel-man
Michael Petrov</p>
<p>Henrique Ponde de Oliveira Pinto
Poko-rnyMichael</p>
<p>Michelle Pokrass
Vitchyr H. Pong, Tolly Pow-ell, Boris PowerAlethea Power</p>
<p>Elizabeth Proehl
Raul Puri, Alec Radford, Jack Rae, Aditya RameshCameron Raymond</p>
<p>Francis Real
Kendra Rim-bach
Carl Ross, Bob Rotsted</p>
<p>Henri Roussez</p>
<p>Lauren Work-man
Arun Vijayvergiya, Chelsea Voss, Carroll Wain-wright, Justin Jay Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Win-ter, Samuel Wolrich, Hannah Wong, Sherwin Wu, Jeff Wu, Michael Wu, Sarah Yoo, Kevin Yu, Qiming Yuan, Rowan Zellers, Chong Zhang, Shengjia Zhao, Juntang Zhuang, William Zhuk, and Barret Zoph2024Alvin Wang, Ben Wang, Ak-ila Welihinda, Kai Xiao, Tao Xu, Woj-ciech Zaremba, Mar-vin Zhang, Tianhao Zheng</p>
<p>LLM-AS-AN-INTERVIEWER: Beyond Static Testing Through Dynamic LLM Evaluation
FFCE43F8AE13426FF4438C4C3942E82C
We introduce LLM-AS-AN-INTERVIEWER, a novel paradigm for evaluating large language models (LLMs).This approach leverages multi-turn interactions where the LLM interviewer actively provides feedback on responses and poses follow-up questions to the evaluated LLM.At the start of the interview, the LLM interviewer dynamically modifies datasets to generate initial questions, mitigating data contamination.We apply the LLM-as-an-Interviewer framework to evaluate six models on the reasoning, factuality and instruction-following tasks.Our results show that the framework effectively provides insights into LLM performance, including the quality of initial responses, adaptability to feedback, and ability to address follow-up queries like clarification or additional knowledge requests.The framework also addresses key limitations of conventional methods like LLM-as-a-Judge, including verbosity bias and inconsistency across runs.Finally, we propose the Interview Report, which aggregates insights from the interview process, providing examples and a comprehensive analysis of the LLM's strengths and weaknesses.This report offers a detailed snapshot of the model's realworld applicability 12 .Follow-Up Question TYPE and Example Knowledge/Factuality ADDITIONAL FACTS Can you provide an example of determining the 6th roots of unity and specifying their arguments in radians?Reasoning CLARIFICATION What does it mean for a and b to be consecutive integers?RATIONALE How did you determine Jasmine's rate of water consumption per mile?Instruction Following MODIFICATION OF CONDITIONS If we change the previous question and assume that each sister of David has two brothers, how many brothers would David have?ADDITIONAL EXPLANATION Can you provide a specific example of how you would address a potential objection your friend might have about public speaking in your persuasive email?CORRECTION Can you now reformulate your earlier reply, outputting it in JSON format and only including books published after 1980?3 We use the Azure API for GPT models, leveraging the GPT-4o-0514 and GPT-3.5-turbo-0125versions. 4We use the Together API to access Llama models.</p>
<p>Introduction</p>
<p>With large language models (LLMs) becoming increasingly proficient in generating fluent free-form responses, it has become crucial to properly assess their capabilities and limitations (Liang et al., 2022;Chang et al., 2024).Recently, LLM-as-ajudge has emerged as a promising framework for automatic free-form response evaluation.Compared to traditional lexical matching-based metrics (e.g., ROUGE (Lin, 2004), BLEU (Papineni et al., 2002)) or embedding-based metrics (e.g., BERTScore (Zhang et al., 2019)), previous works on LLM-as-a-Judge have reported higher correlations with human judgments (Chiang and Lee, 2023;Zheng et al., 2023a;Dubois et al., 2024b).</p>
<p>Despite its potential, the LLM-as-a-Judge framework faces several practical limitations that hinder its widespread adoption, primarily due to its static nature (Li et al., 2024a;Gu et al., 2024).First, using a fixed set of test inputs raises concerns about data contamination (Sainz et al., 2023;Zhou et al., 2023a;Oren et al., 2023b), where the evaluated models may achieve high scores on instances encountered during training.Second, single-turn interactions fail to thoroughly probe a model's true comprehension (Li et al., 2019;Wang et al., 2023;Kwan et al., 2024).For instance, the judge model may assess the confined performance of LLMs distant from the use case, be influenced by superficial factors (e.g., favoring longer responses), and exhibit high variance across runs.</p>
<p>In this study, we propose LLM-AS-AN-INTERVIEWER, a new paradigm for evaluating LLMs.Inspired by human interviews, this approach starts with general questions but dynamically adapts by posing different types of questions based on the model's responses.As shown in Figure 1, the LLM interviewer plays three key roles:</p>
<p>(1) Question Modification, adapting benchmark datasets to generate diverse and challenging initial interview questions; (2) Providing Feedback, guiding the model to refine its responses; and</p>
<p>(3) Generating Follow-up Questions, exploring related concepts through clarification requests or additional explanations.This dynamic evaluation reveals behaviors that static benchmarks cannot capture, such as the model's ability to improve through feedback and provide more detailed explanations.</p>
<p>We demonstrate the efficacy of our framework through experiments on three tasks: Reasoning, To solve this problem, you need the of flour for a two-pound cake.exact value of y, which is 1.5 cups It appears there was a misinterpretation of the question.The problem asks for ... We first find that … cups of flour will make a ten pounds cake.We can rearrange our equation to solve for and find that ... $5y$ $y$ $y=\frac{5}{5}\times\frac{2} {2}=2$ ... two-pound cake requires cups of flour, then a two five-pound cakes will require cups of flour.Thus, and cups of flour.</p>
<p>$y$ $5y$ $5y=2*2$ $y=1.5$Factuality, and Instruction Following.In §5, we evaluate six different models using GPT-4o as the interviewer.We demonstrate the impact of providing feedback on the model's performance and suggest that follow-up questions can offer deeper insights into its behavior.These follow-ups can help uncover failure reasons or assess performance on additional requests.Furthermore, in § 6 and § 7, we show that our interactive and dynamic framework addresses key limitations of static evaluations, such as data contamination, verbosity bias in LLM evaluators, and high variance across multiple runs.Moreover, we find that the extended interaction between the LLM interviewer and interviewee contains rich information that reveals the LLM interviewee's strengths and limitations.Based on this, we introduce a new evaluation protocol, Interview Report, which summarizes the interaction into a structured format.</p>
<p>Our contributions are as follows:</p>
<p>• Introduce LLM-as-an-Interviewer, a novel evaluation paradigm that mimics the dynamic nature of how humans evaluate humans.</p>
<p>• Demonstrate that LLM-as-an-Interviewer mitigates several limitations of traditional evaluation approaches, such as verbosity bias, data contamination, and high variance.</p>
<p>• Propose the Interview Report, offering a detailed snapshot of an LLM's capabilities, including examples and its common errors.</p>
<p>2 Related Works</p>
<p>LLM-based Evaluation</p>
<p>LLM-as-a-Judge is widely used for text evaluation, providing more human-aligned assessments than traditional methods like BLEU and ROUGE, while addressing scalability and consistency issues in human-based approaches (Gu et al., 2024).However, LLM-as-a-Judge faces reliability issues, including self-enhancement bias, sensitivity to response length and order, and low self-consistency.</p>
<p>To enhance reliability, Li et al. (2023b) and Dubois et al. (2024b) mitigate biases related to ordering and length by adjusting positions or win rates.</p>
<p>Additionally, works such as Wang et al. (2024b); Kim et al. (2024) train open-sourced critique models to tackle the inherent inconsistencies of closed LLMs, ensuring reproducibility.LLM-based evaluation is also used to simulate multi-turn interactions between evaluators and models in certain tasks.For instance, Wang et al. (2023) evaluate LLMs by simulating user-model interactions in contexts where users provide feedback during tool usage.Similarly, Yu et al. (2024) simulate knowledge-focused dialogues in multiple interactions to mitigate data contamination issues.Li et al. (2023a) share a motivation similar to ours, as they are also inspired by human interviews.However, their approach is tailored to Conversational Question Answering tasks and emphasizes generating new questions during evaluation.</p>
<p>LLM-as-an-Interviewer is a generalized bench-mark that mimics a multiple interview process to better assess a model's capabilities.Our framework enables the simulation of diverse interactions, such as giving feedback and asking followup questions, which align much more closely with how humans use models in real scenarios.</p>
<p>Data contamination in LLMs</p>
<p>LLMs are trained on large corpora that can be contaminated with benchmark data (Dodge et al., 2021;Soldaini et al., 2024), undermining benchmark reliability (Zhou et al., 2023b).This has sparked interest in contamination detection (Magar and Schwartz, 2022).Shi et al. (2024) and Oren et al. (2023a) propose methods to estimate the likelihood of text in LLM pretraining data using only API access, as is common for frontier models (OpenAI et al., 2024).This has led to further research on contamination (Yax et al., 2024;Deng et al., 2024;Jiang et al., 2024;Dekoninck et al., 2024b;Xu et al., 2024), including methods to evade detection by training on rephrased benchmark data (Dekoninck et al., 2024a).</p>
<p>Overall, efforts to prevent contamination through detection have had limited success, prompting researchers to focus on contamination mitigation in benchmarks.This typically involves dynamic evaluation in two ways: (1) The evaluation data is dynamic, such as crawling new instances live (White et al., 2024;Jain et al., 2024) or generating new instances on the fly (Wang et al., 2024a;Li et al., 2024b); (2) The evaluator is dynamic, as in AlpacaEval (Li et al., 2023b) and MT-Bench (Zheng et al., 2023b), where the same samples are reused but evaluation depends on a dynamic LLM rating completions instead of ground-truth solutions.Some benchmarks, like Chatbot Arena (Chiang et al., 2024), combine dynamic data and evaluators, with users creating and rating responses on the fly.</p>
<p>LLM-as-an-Interviewer is a benchmark that mitigates contamination through both dynamic evaluation data and a dynamic evaluator.The LLM interviewer rephrases questions and proposes novel follow-ups while also evaluating the model's responses and problem-solving process.This setup makes cheating via contamination difficult.Additionally, it enables fast, low-cost evaluation in minutes, unlike Chatbot Arena, which requires human access and takes days to yield statistically significant results at a higher cost.</p>
<p>LLM-AS-AN-INTERVIEWER</p>
<p>We introduce the LLM-AS-AN-INTERVIEWER, which simulates an interview where one LLM dynamically evaluates another.The LLM being evaluated is referred to as the Interviewee, while the LLM conducting the evaluation is referred to as the Interviewer.In this section, we describe (1) the overall interview framework ( § 3.1) and (2) the Interview Report, which summarizes and presents the results of the interview ( § 3.2).We design an interview framework for LLMs as illustrated in Figure 2. The Interviewer plays three main roles throughout the interview process:</p>
<p>Interview Framework</p>
<p>(1) Question Modification, (2) Providing Feedback, and (3) Generating Follow-up Questions.This structure ensures a comprehensive evaluation of the Interviewee's capabilities.These roles are modular and pluggable, allowing users to enable or disable specific roles based on their evaluation needs.Algorithm 1 provides the implementation of the interview process.</p>
<p>Question Modification</p>
<p>Before conducting the interview, the Interviewer prepares seed questions by modifying queries from existing benchmark datasets.This ensures diverse scenarios that align with established evaluation standards while preventing data contamination.This approach mirrors human interviewers, who often tweak existing questions or introduce new challenges.In our experiment, we employ two query modification strategies, prompting the model to modify queries.Each strategy is designed to enhance question diversity while maintaining ease of real-time adjustments and compa-rable complexity.We provide a detailed description of these methods in Appendix C.1.</p>
<p>Our primary goal in query modification is to mitigate data contamination and enable evaluation of multiple models using the same modified questions.We recommend that users without contamination concerns or those preferring direct comparison with original questions may avoid query modification.</p>
<p>Providing Feedback</p>
<p>During the interview process, the Interviewer evaluates the response from the Interviewee and provides constructive feedback.Providing feedback aims to guide the Interviewee in identifying errors, refining the response, and improving solutions.This phase closely resembles realworld LLM interaction scenarios, where LLMs iteratively refine their responses based on usergenerated feedback (Wang et al., 2023).</p>
<p>Generating Follow-up Questions</p>
<p>After assessing the response to the initial question, the Interviewer poses follow-up questions to further evaluate the Interviewee's ability.The purpose of follow-up questions is to gain deeper insights into the model's behavior, such as uncovering reasons for failures or assessing knowledge omitted in the initial response of the Interviewee model, rather than directly comparing the accuracy across models.Follow-up questioning is crucial in many interview processes (e.g., Google coding interviews) and real-world LLM applications (Bai et al., 2024).Table 1 provides follow-up type examples for the Reasoning, Knowledge, and Instruction Following tasks.</p>
<p>Interview Report</p>
<p>The Interview Report, generated as a result of the interview process, includes (1) Performance Scores, ( 2 (1) Performance Scores We obtain two main performance metrics through the interview process:</p>
<p>• Problem Solving Ability -Measures how effectively the model solves a given problem at its n-th interaction with the Interviewer.We define Score seed @n as the model's performance at the n-th interaction, where an interaction involves receiving feedback and making revisions.Thus, Score seed @1 corresponds to the LLM-as-a-Judge setting without feedback, while Score seed @n represents performance after n−1 iterations of feedback and revision.• Follow-up Question Handling -Evaluates how well the model responds to followup questions.To ensure fairness across models with varying performance levels, we categorize follow-up question accuracy into three types: accuracy within correctly answered questions, accuracy within incorrectly answered questions, and overall accuracy.</p>
<p>These metrics reflect abilities crucial in realworld interactions with LLMs, where responding effectively to feedback and follow-up questions significantly influences user satisfaction beyond just answering the initial query.</p>
<p>(2) Error Analysis and Examples A detailed breakdown of error types across multiple interactions, along with illustrative examples.These highlight common failure reasons and examples in the models, offering insights into their limitations and areas for improvement.</p>
<p>(3) Comprehensive Summary A summary of the model's behavior throughout the interview process across all samples.It highlights the model's strengths and weaknesses in performing the task.(e.g.GPT-4o in DepthQA: "The model struggles with conciseness, often providing overly detailed responses...")</p>
<p>Experimental Setup</p>
<p>In this section, we describe the overall experimental setup used in § 5-6, as well as the design of the LLM-as-an-Interviewer framework.In our experiment, we use three tasks: Reasoning, Knowledge, and Instruction Following.For Reasoning, we use MATH (Hendrycks et al., 2021) and the reasoning subset of MINT (Wang et al., 2023).For Knowledge, we use DepthQA (Ko et al., 2024), a dataset of real-world STEM-related questions assessing factuality, rebuilt from the TutorEval dataset.For Instruction Following, we leverage MT-Bench (Chiang et al., 2024), which evaluates an ability to follow multi-turn instructions.</p>
<p>Table 1: Examples of Follow-Up Questions posed by the interviewer (GPT-4o) along with the tasks and types.</p>
<p>Models</p>
<p>We list all the models 34 used throughout our experiments in Section 5-6.</p>
<p>Interviewer Models We adopt GPT-4o as the default Interviewer, as it demonstrates the best performance (about 90% accuracy as an Interviewer) in our analyses.In Appendix C.4-C.5, we present detailed analyses on the accuracy of various interviewers, error analysis, and average cost.</p>
<p>Interviewee Models We use a total of 8 models for the Interviewee, including 2 proprietary models (GPT-4o (OpenAI, 2024), ) and 6 open-source models (Llama-3.1-{8,70}b(Grattafiori et al., 2024), deepseek-Math-7b (Shao et al., 2024), Qwen2.5-Math-7b (Yang et al., 2024), OLMoE (Muennighoff et al., 2024) and Zephyr-7b (Tunstall et al., 2023)).</p>
<p>Interview Process Design</p>
<p>LLM-as-an-Interviewer framework adapts to each task type's characteristics while maintaining rigorous evaluation standards.This section describes the interview process design used in the experiments.Additional details regarding the prompts used are provided in the Appendix C-D.An example of the full interview log is attached in the Appendix F.</p>
<p>Feedback Criteria</p>
<p>The Interviewer provides feedback when there is an error in the model's response or final answer (if the task involves absolute answers).We prompt the Interviewer to guide the model to recognize its mistakes and revise accordingly, but not to provide the correct answer directly.We provide feedback examples in Table 10 of the Appendix, where the Interviewer gives feedback on incorrect responses or missing aspects based on the evaluation criteria.</p>
<p>Follow-Up question Type For each task, we categorize follow-up question types as shown in Table 1.For Knowledge/Factuality tasks, followups probe for additional facts, partially covering the role of a recall metric by identifying missing information from the model's previous responsea common limitation in factuality evaluation (Min et al., 2023).For reasoning tasks, follow-ups aim to uncover the underlying rationale or failure reasons.For instruction following, we define three follow-up types based on the type of secondturn questions by users summarized in MT-Bench-101 (Bai et al., 2024).</p>
<p>Evaluating LLMs with LLM-AS-AN-INTERVIEWER</p>
<p>We simulate the interview process with various Interviewee models.We present insights from interviews, focusing on the seed question-solving phase in § 5.1, and discuss the follow-up questions phase in § 5.2.</p>
<p>Test set We sample 500 queries5 from both MATH and DepthQA, ensuring an equal distribution across the difficulty levels specified in the datasets.We use all 316 samples from the MINT reasoning set and 80 samples from MT-Bench.Interviewee Models We use 6 models for the Interviewee, GPT-4o, GPT-3.5, Llama-3.1-{8,70}b,DeepSeek-Math-7b, Qwen2.5-Math-7b.
Factuality (DepthQA) Model Judge @1 @2 @3 ∆ Judge @1 @2 @3 ∆ GPT-</p>
<p>Seed Question Solving Phase</p>
<p>Table 2 presents the scores of seed questions in MATH and DepthQA for both the LLM-as-a-Judge and LLM-as-an-Interviewer settings.In this experiment, query modification is applied at the beginning of the interview, and the same questions are used for the interviewee models.We present the results from MINT in Table 13 of Appendix E.1.</p>
<p>Comparison with LLM-as-a-Judge Score</p>
<p>There is a clear performance drop in the LLMas-an-Interviewer setting, with most models struggling more when handling modified queries.In MATH, most models achieve performance comparable to the Judge during the second interaction.</p>
<p>For DepthQA, all models except GPT-4o experience a significant performance decline, while GPT-4o shows only a minimal decrease of 0.2%.In addition, the evaluation criteria that assess both the reasoning process and the final answers can contribute to this performance drop.For example, GPT-3.5 correctly answers the questions but introduces errors in the solving process in 3% of the MATH test set.</p>
<p>Model's Capability to Revise Through Feedback Across all models, performance improves when feedback is provided for incorrect answers (@2, @3), compared to the initial accuracy (@1).Interestingly, as the models receive feedback and iteratively revise their answers, the standard deviation (std) of scores across models generally decreases compared to that in the LLM-as-a-Judge setting and @1 score.This is mainly because models that initially perform poorly show substantial improvement through interaction.</p>
<p>Follow-UP Question Solving Phase</p>
<p>Table 3 presents follow-up question accuracy in Reasoning and Factuality tasks.Overall, models exhibit higher accuracy on follow-up questions for correctly answered seed questions than for failed ones.In § 5.2.1, we provide a discussion based on the MT-Bench evaluation.</p>
<p>Comparing with Static Multi-turn Benchmark</p>
<p>We compare follow-up questions in LLM-as-an-Interviewer with MT-Bench (Chiang et al., 2024), a 2-turn static benchmark with 80 samples across various tasks (e.g., writing, math, coding, roleplay).MT-Bench primarily uses general second-turn questions that are independent of previous responses (e.g., "Explain like I'm five," "Change format"), limiting diversity, and realism.In contrast, LLM-as-an-Interviewer generates contextaware follow-ups (e.g., "You mentioned Thomas might be in a clinical trial.Can you explain what a clinical trial is and why it requires daily hospital visits?").Some MT-Bench tasks reference previous responses (e.g., "Explain the N-th example from your previous response."),which can cause contradictions if the model fails to follow instructions in turn 1, as seen in the following example.Alter your previous response.Make the following adjustments to the 2nd option: 1. Make the tone sound casual (...)</p>
<p>MT-bench Failure Example of Deepseek</p>
<p>Table 4 shows the number of dependent and independent follow-up questions for each setting.For the Interviewer setting, we compare GPT-4o, the best-performing model at turn-1, and Qwen, the worst-performing model with low instructionfollowing ability 6 .Aligned with MT-Bench failure examples, Qwen receives more independent follow-ups (e.g., topic shifts), while GPT-4o receives more dependent ones.As such, considering previous responses dynamically during multi-turn evaluations is crucial for assessing the model in a more diverse and natural context.</p>
<p>Can LLM-AS-AN-INTERVIEWER Mitigate Data Contamination Issue?</p>
<p>This section demonstrates the efficiency of the modification strategy in generating questions based on established benchmarks.</p>
<p>Experimental Setting</p>
<p>We 6 See Table 15 in Appendix E for the results in MT-bench</p>
<p>Uncontaminated Model Training</p>
<p>As shown in the setting column of Table 5, we train uncontaminated models using three distinct configurations:</p>
<p>1 Models trained exclusively on the train set of the target dataset (In-Distribution, T rain ID ). 2 Models trained on both the T rain ID and additional datasets from the same domain (Out-Of-Distribution, T rain OOD ). 3 Models trained solely on T rain OOD .</p>
<p>Contaminated Model Training</p>
<p>For contaminated models, we employ four different configurations:</p>
<p>4 Models trained exclusively on the test set of the target dataset (In-Distribution, T est ID ). 5 Models trained on both the T est ID and T rain ID sets.6 Models trained on the T est ID combined with an instruction-tuning (Instruct) dataset.7 Models trained on both T est ID and T rain OOD .</p>
<p>Dataset We use the in-distribution test set(T est ID ) when testing the model, and MATH and DepthQA are used as in-distribution datasets.Since DepthQA is not pre-divided into train and test sets, we manually split it into 851 instances for the DepthQA train set and 848 instances for the DepthQA test set.For other datasets, we randomly select 2000 samples each.For the out-ofdistribution dataset of MATH, we use GSM8K.</p>
<p>For MATH, we experiment with all seven settings, while for DepthQA, we include only the four settings that do not involve out-of-distribution (OOD) data.Details about model training, including training configurations, parameters, and datasets used, are provided in Appendix G.1.Table 5: Performance of uncontaminated and contaminated models on MATH dataset."Judge" refers to the LLM-as-a-Judge setting, and "Interv" refers to the LLM-as-an-Interver setting.</p>
<p>Result</p>
<p>applied while the Interviewer prepares the seed queries.The tables compare accuracy for both the original queries just as LLM-as-an-Judge setting (Judge) and modified queries (Interview).Settings [1, 2, 3] represent uncontaminated models, serving as a proxy for the model's true ability on the task.The goal of the strategy is to bring the performance of contaminated models closer to these uncontaminated models.</p>
<p>In MATH, contaminated models often outperform uncontaminated ones under the Judge setting.These contaminated models achieve significantly higher accuracy, averaging 0.7 for OL-MoE and 0.4 for Zephyr.However, applying the modification strategy leads to a substantial performance drop, making their performance comparable to the uncontaminated model.A similar trend is observed in the DepthQA results (Table 16 in Appendix G).The results show that our framework mitigates the contamination issue by having the model solve similar yet modified questions.</p>
<p>Discussion</p>
<p>We assess whether LLM-as-an-Interviewer exhibits verbosity bias and maintains robustness across multiple runs, a common consideration in model-based evaluation (Zheng et al., 2023b). 7  Verbosity Bias To assess whether the framework favors longer answers, we examine the correlation between answer length and score using the Interact.</p>
<p>@1 @2 @3 @4 @5</p>
<p>DepthQA Depth-QA task.GPT-4o acts as the Interviewer, with six models as Interviewee.8Figures 3 in the Appendix H.1 reveal that the linear correlation (r) between length and score weakens as interactions increase.In the LLM-asa-Judge setting , a significant positive correlation is found (r = .371,p &lt; .05);however, as interactions progress, the p-value rises and r-value declines.This indicates that our interview process could reduce verbosity bias and length dependency, alongside the length control mechanism for adjusting win rates or scores (Dubois et al., 2024a).</p>
<p>Does LLM-AS-AN-INTERVIEWER Function</p>
<p>Robust?We assess the robustness of our framework across multiple interactions by repeating the experiment five times for each setting.Specifically, We calculate the standard deviation of these five runs for each combination of different Interviewer temperatures (0 or 1) and different Interviewee models.The Interviewee's temperature remains fixed at 1.We use GPT-4o as the Interviewer and GPT-3.5 and Llama-3.1-{8/70}b as the Interviewee.</p>
<p>As shown in Table 6, the standard deviation for DepthQA decreases with more interactions.In contrast, for MATH, it increases over time.Breaking MATH down by difficulty, we find that for easier problems (levels ≤2, MATH-easy), the standard deviation remains stable or decreases, whereas for harder problems (levels ≥3, MATHhard), it rises due to initially low accuracy and occasional correct answers from feedback.Excluding MATH-hard, the standard deviation remains stable, demonstrating the framework's robustness in most cases.</p>
<p>Conclusion</p>
<p>In this paper, we introduce LLM-as-an-Interviewer, a novel approach for evaluating LLMs through interviews.Our multi-turn evaluation framework with Interview Report reveals the abilities of LLMs, such as revising answers based on feedback or handling follow-up questions, while providing comprehensive insights into the task.Our analysis demonstrates that our framework mitigates common issues in static evaluation methods, including verbosity bias and data contamination.We hope our framework is widely adopted and brings a paradigm shift in LLM evaluation.</p>
<p>Limitations</p>
<p>While our framework addresses several biases and limitations of the LLM-as-a-judge approach, it still inherits some of the inherent limitations of large language models, such as inconsistency and the potential to introduce additional biases, similar to the issues raised in the LLM-as-a-judge approach.</p>
<p>A AI Writing Assistance</p>
<p>We used ChatGPT for grammar and language refinement of the manuscript.</p>
<p>B LLM-as-an-Interviewer B.1 Implementation Details</p>
<p>Building upon the interview framework described in § 3.1, this section explains our interview system implementation.The system operates as a twoagent dialogue with additional complexity in controlling the Interviewer to conduct evaluations accurately and efficiently.Controlling Interviewer is done in two stages: (1) keeping the interview state in track, and (2) using pre-written prompts for each state.While we implemented the interview process specifically for MATH and STEM domains, the framework can be easily adapted to other domains with minor modifications.</p>
<p>B.2 Adapting to New Tasks</p>
<p>To adapt a new task, one can modify the Main Interview process, specifically the response evaluation criteria and follow-up question generation strategy.This can be done by rewriting the prewritten prompts.Here are examples of adaptations for different domains:</p>
<p>Programming Interviews: q ← q seed ▷ Start with the seed question 10:</p>
<p>while not termination do 11:</p>
<p>response ← i(q) ▷ Interviewee answers the question 12:</p>
<p>feedback, is_correct ← G(q, response) ▷ Grader evaluates response For Arithmetic Reasoning tasks (MATH), LLM Interviewer replaces numerical values in a query with unknown variables (e.g., x) or omits specific details.For example, "If there were 5 chocolates and you ate 1, how many are left?"becomes "If there were 5 chocolates and you ate x, how many are left?".This maintains the same reasoning level while preventing the direct application of memorized solutions, as seen in methods like those of Mirzadeh et al. (2024).For factuality-based longform tasks (depth-QA), new problems are created by referencing ground truth solutions and questions from the original dataset to ensure relevance and diversity.Examples of these strategies are in Table 11.</p>
<p>Interview Process: Consists of two primary components:</p>
<ol>
<li>
<p>Response Evaluation: Interviewer assesses Interviewee responses using domain-specific criteria (mathematical correctness for MATH; factual accuracy and completeness for STEM) and identifies error sources.</p>
</li>
<li>
<p>Follow-up Question Generation: Based on evaluation results, Interviewer either probes deeper concepts (for correct responses) or provides targeted feedback to reveal misconceptions (for incorrect responses).</p>
</li>
</ol>
<p>Evaluation Metric: For MATH, our evaluation framework implements binary correctness assessment for both final answers and solution processes, along with error categorization that distinguishes between conceptual understanding, misinterpretation, and calculation errors.The framework also incorporates step-by-step verification of mathematical reasoning and generates targeted follow-up questions when errors are detected.For DepthQA (Ko et al., 2024), our evaluation framework uses FActScore (Min et al., 2023) as a metric to calculate factual precision in the model's long-form generation.These follow-up questions partially address a limitation of previous fact-based evaluations by enabling a somewhat broader assessment that includes aspects of recall in addition to precision.employs a more nuanced approach, decomposing responses into atomic facts for systematic assessment.The framework also evaluates quality across multiple dimensions, including completeness, redundancy, readability, and depth, while generating follow-up questions to probe understanding of missing concepts.</p>
<p>Interview Report: The interview report, based on the detailed interaction logs from the Main Interview stage, is shown to the user at the end of the interview.This content for the report is explained in § 3.2.Followings are the example of Interview Report.</p>
<p>C.2 Statistical analysis of the Sampling</p>
<p>We compare the sampling variability of our framework using bootstrapping (sampling with replacement).From the 500 samples, we randomly sample n samples, repeated the process 10,000 times, and calculated the p-value.The results showed no significant statistical difference between n ≥ 100 samples and 500 samples, with results comparable to those of LLM-as-a-Judge.1. Performance Scores(%)</p>
<p>• Accuracy(@ # of interaction): 72(@1), 82(@2), 84(@3)</p>
<p>• Follow-up Accuracy:</p>
<ul>
<li>
<p>MATH, Interviewee: Llama-3.1-8b(Interviewer: GPT-4o)</p>
</li>
<li>
<p>Performance Scores(%)</p>
</li>
</ul>
<p>• Accuracy(@ # of interaction): 58(@1), 68(@2), 75(@3)</p>
<p>• Follow-up Accuracy:</p>
<p>-</p>
<p>Summary</p>
<p>The model demonstrates strong mathematical problem-solving skills, particularly in algebra, quadratic equations, and logical reasoning.It excels in providing clear, stepby-step explanations, making it useful for educational purposes.The model is responsive to follow-up questions and user feedback, showing adaptability and a willingness to improve its responses.However, it struggles with maintaining accuracy in complex calculations, avoiding unnecessary complexity, and handling more abstract or less structured problems.It may also require user intervention to correct its approach and simplify its methods.</p>
<p>Overall, the model is effective in structured mathematical tasks but needs improvement in dealing with more complex, ambiguous, or creative problems.</p>
<p>DepthQA, Interviewee: GPT-4o (Interviewer: GPT-4o)</p>
<ol>
<li>Performance Scores(%)</li>
</ol>
<p>• Precision(@ # of interaction): 96.8(@1), 98.8(@2), 99.2(@3)</p>
<p>• Follow-up Accuracy (Additional Facts):</p>
<p>-Total 92 • redundancy: 64.9</p>
<p>• readability: 96.9</p>
<p>• depth: 64.9</p>
<p>Summary</p>
<p>The model excels in providing detailed, structured, and accurate explanations, particularly in technical, scientific, and mathematical domains.It effectively breaks down complex concepts into understandable parts and maintains coherence and continuity in responses, making it suitable for educational purposes.The model handles follow-up questions well and positively acknowledges user feedback, although it may not dynamically adapt based on feedback within a single session.However, the model struggles with conciseness, often providing overly detailed responses that can overwhelm users seeking brief answers.It may also have difficulty with highly specialized or nuanced queries, maintaining context over multiple interactions, and handling ambiguous or poorly defined questions.Additionally, the model may not always incorporate the latest information or trends beyond its training data.Overall, the model demonstrates strong capabilities in delivering comprehensive and accurate information but could improve in providing concise answers, handling more abstract or context-specific queries, and better incorporating user feedback.</p>
<p>DepthQA, Interviewee: Llama-3.1-8b(Interviewer: GPT-4o)</p>
<ol>
<li>Performance Scores(%)</li>
</ol>
<p>• Precision(@ # of interaction): 89.8(@1), 98.1(@2), 99.6(@3)</p>
<p>• Follow-up Accuracy (Additional Facts):</p>
<p>-Total 80</p>
<p>C.3 Feedback Example</p>
<p>See Table 10.</p>
<p>C.4 Human Evaluation of LLM-as-an-Interviewer</p>
<p>This section details the evaluation process and criteria for validating the three roles of Interviewer models.Four authors conducted the human evaluation to systematically assess the effectiveness of each model in three key dimensions: (1) Query Modification, (2) Feedback Generation, and (3) Follow-up Question Generation.</p>
<p>In this study, we evaluate GPT-4o, Llama-3.1-70B, and Llama-3.1-8B as interviewers, while using GPT-4o, GPT-3.5, Llama-3.1-70B and Llama-3.1-8B as interviewees to simulate the interview with models of varying capabilities.We validate at least 100 samples per dimension, resulting in over 300 samples per model.This sample size is comparable to or larger than the human-annotated datasets used in prior studies (Wang et al., 2023;Yu et al., 2024).Table 12 presents the evaluation results, showing the accuracy of each model's responses along with the number of evaluated samples in parentheses. 9</p>
<p>C.4.1 Evaluation Criteria</p>
<p>Each model's responses were evaluated to determine whether they correctly followed the intended prompts and adhered to the reference solution.Annotators referenced both the problem statement and the reference solution throughout the evaluation process.The following criteria outline the assessment methodology:</p>
<p>Query Modification For MATH tasks, we assessed whether the query retained only the key information necessary for solving the problem, avoiding unnecessary modifications that might alter the problem's complexity.For DepthQA, we evaluated whether the query was reformulated into a well-structured and solvable question aligned with the reference solution.</p>
<p>Feedback Generation We evaluated whether the feedback was contextually appropriate to the interviewee's prior response.The feedback had to avoid contradictions with the reference solution while also refraining from directly revealing the correct answer.Annotators additionally reviewed the model's intermediate reasoning (chainof-thought) to ensure logical coherence.</p>
<p>Follow-up Question Generation Follow-up questions were assessed based on their alignment with the problem type and intent.They needed to be relevant to the previous discussion, logically structured, and free from redundancy.For 9 Annotator reliability was validated by having two additional annotators label 100 cases.The resulting Fleiss' kappa of 0.653 indicates substantial inter-annotator agreement.</p>
<p>STEM-related questions, we ensured that followup queries were grounded in the reference solution and contributed meaningfully to the progression of the interview.</p>
<p>By adopting this structured evaluation framework, we ensure a comprehensive and transparent assessment of Interviewer models, allowing for a clear comparison of their capabilities across different interview phases.</p>
<p>C.4.2 Qualitative Analysis of LLM-as-an-Interviewer</p>
<p>In this section, we present a qualitative analysis highlighting common issues encountered with the Interviewer (GPT-4o) based on the annotation of results summarized in Table 10.Due to space constraints, we do not provide example excerpts for each case here; however, all actual responses will be included in the appendix.</p>
<p>Feedback Generation</p>
<p>We identify three primary failure modes:</p>
<p>• The Interviewer incorrectly criticizes aspects that the Interviewee has addressed correctly.</p>
<p>• The Interviewer repeats identical feedback across multiple turns without modification.</p>
<p>• The Interviewer rigidly enforces the reference solution as the only correct answer.</p>
<p>Given that the feedback phase is repeated N times, these issues may contribute to an increased number of turns required for problem resolution.</p>
<p>Follow-up Question Generation Two main issues arise during follow-up question generation:</p>
<p>• The Interviewer asks follow-up questions about points the Interviewee has not yet addressed.</p>
<p>• The Interviewer focuses on minor or unnecessary details in follow-up questions.</p>
<p>Although these occurrences are relatively infrequent, such questions may hinder the extraction of meaningful insights into the model's behavior during the respective turns.</p>
<p>C.4.3 Error Propagation in Multi-turn Interactions</p>
<p>We measure multi-round accuracy, where, for example, the accuracy at turn 2 represents the percentage of cases in which the Interviewer provides correct feedback across both turns 1 and 2.</p>
<p>Table 8 shows the result of error propagation in multi-turn interaction.If each interaction is truly independent, we expect a compounding error rate, resulting in approximately a 9% drop in accuracy by the second turn and an 18% drop by the third.However, the observed degradation is much smaller because errors correlate across turns, with initial errors influencing subsequent interactions.</p>
<p>Turn 2 Turn 3</p>
<p>Accuracy Degradation -1.6% -4%</p>
<p>C.5 Cost Analysis</p>
<p>We calculate the average cost per evaluation round.Table 9 shows that the cost of our framework is comparable to other multi-turn evaluation methods.For fair comparison, we normalized all costs to the scenario of using GPT-4-turbo as the evaluator based on the reported token usage and pricing (Wang et al., 2023;Yu et al., 2024).</p>
<p>Method Cost (USD) Human (Wang et al., 2023) 2.437 LLM-Judge (Single-turn) 0.0103 MINT (Multi-turn) (Wang et al., 2023) 0.0563 KI-eval (Multi-turn) (Yu et al., 2024) 0.135 Ours 0.0720 While multi-turn evaluations are generally more expensive than single-turn methods, they provide more realistic assessments, which single-turn approaches often fail to capture (Wang et al., 2023).It is noted that our method is more scalable than using human evaluators.</p>
<p>Feedback Example</p>
<p>Knowledge/Factuality Your response effectively outlines the purpose and communication methods of the AQI and is clearly articulated.To improve further, ensure that the AQI scale range examples are accurate and align with commonly accepted values.Additionally, providing a more detailed explanation of the AQI calculation and data collection process would enhance the depth and completeness of your answer.</p>
<p>Reasoning</p>
<p>Your initial steps show some understanding of the order of operations, but you incorrectly simplified the expression inside the brackets.Remember to first evaluate the power, then apply the subtraction, and follow through with the multiplication and division before finally adding the constant outside the brackets.The feedback generation follows two steps approach.First, the response is graded (using the Grader Prompt below), and then feedback for the Interviewee is generated based on that grading (using the Feedback Generator Prompt below).</p>
<p>MATH Grader</p>
<p>Task: You are provided with a model's output, a reference solution, and a correct answer.Your goal is to determine the correctness of the model's output by comparing it with the reference solution and the correct answer.Note that the reference solution is just a guide-there could be other valid methods to solve the problem.</p>
<p>STEM Grader</p>
<p>Task:</p>
<p>You are given a question, a reference solution, and the model's output, which is broken down into atomic fact units, along with their correctness and justification.The model has revised the incorrect parts of its original output, and these revisions are provided in the model's correction statement.Your task is to update the model's original output by replacing the corresponding revised facts with those found in the model's correction statement.Ensure that updated facts are from the model's correction statement, not the reference solution.For facts that were not revised, maintain the same content and correctness as before, so that your output contains the same number of atomic facts as the model's original output.</p>
<p>STEM Feedback Generator</p>
<p>You are an expert tasked with evaluating and providing feedback on an assistant's performance.Your response should contain the following elements: 1. "status": Indicate whether the answer is "complete" (fully correct), "par-tially_correct" (some elements are correct but not all), or "incorrect" (entirely wrong).2. "feedback": Provide specific, constructive feedback based on the differences between the correct solution and the model's output.Your feedback should:</p>
<p>-Highlight what aspects of the answer are correct (if any).</p>
<p>-Identify specific areas where the model's output differs from the correct solution, without revealing the entire solution.</p>
<p>-Offer guidance or hints that can help the model improve its answer, focusing on the concepts or steps that need correction.</p>
<p>-If the process is incorrect but the final answer is right, explain that the reasoning needs improvement.</p>
<p>-Encourage the model to think step-by- If the seed question is answered incorrectly, generate a clarification question based on the model's error type.The question should focus on the concept or interpretation of the seed question to help identify the source of the misunderstanding.</p>
<p>MATH Success:</p>
<p>If the seed question is answered correctly, generate a follow-up rationale question to assess the model's understanding of the reasoning behind its solution.</p>
<p>DepthQA: Regardless of whether the seed question is answered correctly or not, generate a follow-up question based on the reference material.The question should inquire about additional facts or details that the model did not address in its response.</p>
<p>MATH Fail</p>
<p>Task: You are a question generator for assessing the model.Below are the list of the answer history of an Evaluatee model that keeps getting the question wrong, its error type, the corresponding problem, and the solution.Your role is to generate a proper follow-up question based on the model's error type.</p>
<p>MATH Success</p>
<p>Task:</p>
<p>You are an evaluator assessing the Evaluatee model.The Evaluatee has successfully answered the problem, and the following is the Evaluatee's solution.Your role is to determine whether the Evaluatee truly understands their solution or if they arrived at the answer through memorization without proper understanding.To do this, you should ask questions that can assess the model's understanding of its own solution.Guidelines: 1.If there are errors or missing steps in the Evaluatee's solution, ask questions to clarify or correct these issues.2. If there are no errors in the solution, ask questions to confirm the Evaluatee's understanding of the solution steps.3. Generate one question and provide the answer in JSON format.Be sure not to ask questions that are already present in the previous history below.The original question that was asked.2. Reference Answer: A list of atomic facts with labels indicating whether each fact is supported or unsupported by a model's output.3. Model's Output: The model's output and the corresponding correctness for the given question.</p>
<p>Instruction:</p>
<p>1.Your task is to generate follow-up questions based on the atomic facts from the Reference Answer that were labeled as unsupported by the model's output.2. The follow-up questions should be designed to test whether the model understands the unsupported facts.3. Never include any facts from the reference solution to the question.Instead, ask questions to verify knowledge.4. The questions should indirectly reference the concepts in a way that allows us to assess the model's understanding. 5.Your follow-up questions should not simply ask the fact directly but should guide the model to demonstrate whether it knows the fact.Output Format (JSON): {Format Exam-ple} Input Data: Question: question Reference Answer: solution Output:</p>
<p>D.3 Summarizing Interview</p>
<p>Session Summarize Prompt</p>
<p>Task:</p>
<p>The following is a conversation between user and system.Summarize the conversation.</p>
<p>Instructions:</p>
<ol>
<li>Instead of focusing on the specific details of the questions and answers, provide a summary that highlights: -The overall flow of the conversation.-system's problem-solving abilities.Input: Session History: {session_history} Output: Summary: Summarize Prompt for Interview Report Summarize the following problem summaries of the system's problem-solving abilities.In your summary, try to provide general insights into the capabilities of the model, such as: Strength and weakness of the model Does the model respond well to the user's follow-up questions?Does the model handle the user's feedback effectively?What types of information or tasks does the model handle well?What types of problems or details does the model struggle with?Answer the following questions one by one.Do not focus on specific examples, but rather offer a general overview of the model's strengths and weaknesses.Here are the system's problem-solving abilities to summarize.Don't generate too long: {chunk_dict} E Evaluating LLMs with LLM-as-an-Interviewer</li>
</ol>
<p>E.1 Evaluation Result of MINT</p>
<p>As highlighted in the Related Work section, prior studies of multi-turn dynamic evaluation are designed with distinct objectives and methodologies, making direct comparisons difficult.Among the available benchmarks, we select MINT (Wang et al., 2023), which evaluates Multi-turn Interaction with Tool usage, as it aligns closely with our focus.Specifically, we choose the Reasoning subset of MINT, which excludes external tools or code execution and consists of 316 samples sourced from GSM8K, MATH, HotpotQA, Theo-remQA, and MMLU.</p>
<p>For this evaluation, we leverage our LLM-asan-Interviewer framework, implemented as a PyPI module, showcasing its applicability beyond datasets like MATH and DepthQA.</p>
<p>E.1.1 Evaluation</p>
<p>We assess six models and compare three models-Llama-{8B/70B} and GPT-3.5-Turbo-thathave prior results reported in MINT.To maintain consistency in measuring accuracy without feedback or interaction-comparing Acc_seed(1) from our framework with k=1 from MINT-we do not apply any query modifications.</p>
<p>Table 13 presents the performance assessed by the LLM-as-an-Interviewer framework, while Table 14 shows the results from the MINT framework.Discrepancies between Acc_seed(1) and k=1 may arise due to differences in tool usage and prompt structures.MINT employs longer initial prompts that explicitly guide tool usage and define output formats, potentially influencing accuracy at k=1.</p>
<p>E.2 Evaluation Result of MT-bench</p>
<p>We compare performance on MT-Bench (Chiang et al., 2024), a 2-turn static benchmark with 80 samples across various tasks (e.g., writing, reasoning, math, coding, roleplay).For the baseline, model responses are evaluated sequentially using turn = 1 and turn = 2 queries from the dataset.In the Interviewer setting, turn = 1 queries serve as seed questions without modification, followed by follow-up questions posed by the Interviewer (turn = 2).The scores range from 1 to 10 (refer to the method in (Chiang et al., 2024)).</p>
<p>E.3 Follow-Up Question Example of DepthQA</p>
<p>In the following example, the follow-up question targets the n-th roots mentioned in the reference solution but missing from the model's previous response.This type of follow-up question is central to DepthQA.It partially address a key limitation of traditional factuality scoring, which only evaluates the facts the model provides (i.e., precision).By probing further with additional questions, we can assess the model's broader factuality, capturing missing or incomplete information.</p>
<p>DepthQA Follow-up Question Example</p>
<p>User: What is the significance of the roots of unity in mathematics and physics?System: The roots of unity are important in mathematics and physics because they have connections to many different areas of study.In mathematics, they are used in algebra, number theory, and complex analysis.In physics, they appear in wave functions, quantum mechanics, and signal processing.The roots of unity also have applications in computer science, cryptography, and engineering.Overall, they play a key role in understanding the behavior of periodic functions and symmetries in various mathematical and physical systems.</p>
<h1>Follow-up Question User: Can you provide an example of determining the 6th roots of unity and specifying their arguments in radians?</h1>
<p>Reference Solution</p>
<p>The roots of unity refer to the complex solutions of the equation z n = 1, where n is a positive integer.These roots are regularly spaced around the circumference of the unit circle in the complex plane and hence, are of significant importance in many areas of mathematics and physics.The term "unity" signifies the number 1, and "roots" refer to the solutions of an equation.So, the roots of unity are essentially the solutions to the equation that equal 1 when raised to the power n.</p>
<p>Each n-th root of unity is given by the formula: z = cos 2πk Thank you for pointing out the inconsistencies in my initial explanation.I hope this revised explanation helps clarify the steps.</p>
<p>DepthQA Full Log</p>
<p>Interviewee : Llama-3.1-70b,Interviewer : GPT-4o User: Question: What are some scenarios where the forward or backward difference approximations are preferred over the central difference approximation?System: There are several scenarios where the forward or backward difference approximations are preferred over the central difference approximation: 1. <strong>Boundary conditions</strong>: When approximating the derivative at a boundary point, the central difference approximation is not feasible because it requires points on both sides of the boundary.In such cases, the forward or backward difference approximation is used, depending on whether the boundary point is at the beginning or end of the interval.2. <strong>Non-uniform grids</strong>: When the grid points are not uniformly spaced, the central difference approximation may not be the most accurate choice.In such cases, the forward or backward difference approximation can be used, taking into account the non-uniform spacing of the grid points.3. <strong>Noise or irregular data</strong>: When the data is noisy or irregular, the central difference approximation may amplify the noise or irregularities.In such cases, the forward or backward difference approximation can be used to reduce the effect of noise or irregularities.4. <strong>Stability and convergence</strong>: In some numerical methods, such as finite difference methods for solving partial differential equations, the forward or backward difference approximation may be preferred over the central difference approximation for stability and convergence reasons. 5. <strong>Computational efficiency</strong>: In some cases, the forward or backward difference approximation may be computationally more efficient than the central difference approximation, especially when the number of grid points is large.</p>
<ol>
<li><strong>Advection-dominated problems</strong>: In problems where advection dominates diffusion, the backward difference approximation may be preferred over the central difference approximation to reduce numerical diffusion.7. <strong>Time-dependent problems</strong>: In timedependent problems, the backward difference approximation may be preferred over the central difference approximation to ensure stability and convergence.In summary, the choice of difference approximation depends on the specific problem, the nature of the data, and the requirements of the numerical method.User: Your explanation covers several important scenarios where forward or backward difference approximations might be preferred, but some areas need more detailed context and justification.For instance, could you explain when grid points are not uniformly spaced and why central difference may not be accurate there?Also, clarify how the central difference method might amplify noise or irregularities compared to other methods.Additionally, elaborating on why the backward difference might be used in advection-dominated problems and the suitability of backward difference for timedependent problems would strengthen your response.Overall, consider diving deeper into these points to improve the completeness and depth of your answer.</li>
</ol>
<p>System: I'd be happy to provide more detailed explanations for the scenarios where the forward or backward difference approximations are preferred over the central difference approximation.<strong>Non-uniform grids:</strong> When the grid points are not uniformly spaced, the central difference approximation may not be the most accurate choice.This is because the central difference formula assumes that the grid points are evenly spaced, and it uses the average of the function values at the neighboring points to approximate the derivative.However, when the grid points are not uni-formly spaced, this assumption is no longer valid.For example, consider a scenario where the grid points are spaced more closely near the boundary of the domain than in the interior.In this case, the central difference formula would use the same weighting for the function values at the neighboring points, even though the points near the boundary are more closely spaced.This can lead to inaccurate results, especially if the function is rapidly changing near the boundary.In contrast, the forward or backward difference approximations can be adapted to handle non-uniform grids by taking into account the spacing between the grid points.For example, the forward difference formula can be modified to use the distance between the current point and the next point, rather than assuming a uniform spacing.<strong>Noise or irregular data:</strong> When the data is noisy or irregular, the central difference approximation may amplify the noise or irregularities.This is because the central difference formula uses the difference between the function values at the neighboring points, which can accentuate any noise or irregularities in the data.For example, consider a scenario where the data has a high-frequency noise component.The central difference formula would use the difference between the function values at the neighboring points, which would amplify the noise component.In contrast, the forward or backward difference approximations would use the difference between the current point and the next or previous point, respectively, which can help to reduce the effect of the noise.Additionally, the central difference formula can also be sensitive to outliers or irregularities in the data.For example, if there is a single outlier in the data, the central difference formula would use this outlier in the calculation of the derivative, which can lead to inaccurate results.In contrast, the forward or backward difference approximations can be more robust to outliers, as they only use the current point and the next or previous point, respectively.<strong>Advection-dominated problems:</strong> In problems where advection dominates diffusion, the backward difference approximation may be preferred over the central difference approximation to reduce numerical diffusion.Advection-dominated problems are characterized by a strong convective flow, which can lead to numerical diffusion when using the central difference approximation.Numerical diffusion is a phenomenon where the numerical solution of the problem becomes smeared or diffused, even though the underlying physics of the problem does not involve diffusion.This can happen when the central difference approximation is used, as it can introduce artificial diffusion into the solution.The backward difference approximation, on the other hand, can help to reduce numerical diffusion by using the upwind direction to calculate the derivative.The upwind direction is the direction in which the flow is moving, and using this direction can help to reduce the artificial diffusion introduced by the numerical method.For example, consider a problem where a pollutant is being transported by a fluid flow.The central difference approximation would use the average of the pollutant concentrations at the neighboring points to calculate the derivative, which can lead to numerical diffusion.In contrast, the backward difference approximation would use the pollutant concentration at the upwind point to calculate the derivative, which can help to reduce numerical diffusion.<strong>Time-dependent problems:</strong> In time-dependent problems, the backward difference approximation may be preferred over the central difference approximation to ensure stability and convergence.Timedependent problems involve solving a partial differential equation that changes over time, and the numerical method must be able to capture the time-dependent behavior of the solution.The backward difference approximation is often preferred in time-dependent prob-lems because it is unconditionally stable, meaning that it will converge to the correct solution regardless of the time step size.The central difference approximation, on the other hand, can be conditionally stable, meaning that it will only converge to the correct solution if the time step size is small enough.For example, consider a problem where a wave is propagating through a medium over time.The central difference approximation would use the average of the wave amplitude at the neighboring points to calculate the derivative, which can lead to instability if the time step size is too large.In contrast, the backward difference approximation would use the wave amplitude at the previous time step to calculate the derivative, which can help to ensure stability and convergence.In summary, the choice of difference approximation depends on the specific problem, the nature of the data, and the requirements of the numerical method.The forward or backward difference approximations can be preferred over the central difference approximation in scenarios such as non-uniform grids, noisy or irregular data, advection-dominated problems, and time-dependent problems."</p>
<p>User: Your output provides a comprehensive and detailed explanation of various scenarios where forward or backward difference approximations may be preferred over central difference.However, consider if the preference for specific difference approximations might depend on factors beyond stability and convergence, and reflect on whether computational efficiency is a significant deciding factor.Additionally, some redundancy in the discussion of non-uniform grids and noisy data could be consolidated to improve conciseness.Overall, the depth and readability of your explanation are strong, but ensure all points are necessary to the question.</p>
<p>System: I'd be happy to refine my explanation to address your points and provide a more concise response.<strong>Non-uniform grids and noisy data:</strong> In scenarios where the grid points are not uniformly spaced or the data is noisy or irregular, the forward or backward difference approximations may be preferred over the central difference approximation.The central difference formula assumes uniform spacing and can be sensitive to noise or irregularities, which can lead to inaccurate results.In contrast, the forward or backward difference approximations can be adapted to handle non-uniform grids and reduce the effect of noise or irregularities.For example, in problems with highfrequency noise or outliers, the forward or backward difference approximations can provide a more robust estimate of the derivative by using the difference between the current point and the next or previous point, respectively.Similarly, in nonuniform grids, the forward or backward difference approximations can take into account the varying spacing between grid points to provide a more accurate estimate of the derivative.<strong>Advection-dominated problems:</strong> In problems where advection dominates diffusion, the backward difference approximation may be preferred over the central difference approximation to reduce numerical diffusion.The central difference approximation can introduce artificial diffusion into the solution, which can be mitigated by using the backward difference approximation and the upwind direction to calculate the derivative.<strong>Time-dependent problems:</strong> In time-dependent problems, the backward difference approximation may be preferred over the central difference approximation to ensure stability and convergence.The backward difference approximation is unconditionally stable, whereas the central difference approximation can be conditionally stable, requiring a small enough time step size to converge.<strong>Computational efficiency:</strong> While computational efficiency can be a consideration in choosing a difference ap-proximation, it is often secondary to stability and convergence.In some cases, the forward or backward difference approximations may be computationally more efficient than the central difference approximation, especially when the number of grid points is large.However, this advantage may be outweighed by the need for stability and convergence in many applications.<strong>Additional considerations:</strong> Beyond stability and convergence, the choice of difference approximation may depend on factors such as: * <strong>Boundary conditions:</strong> The forward or backward difference approximations may be preferred at boundary points, where the central difference approximation is not feasible.<em> </em><em>Problem physics:</em><em> The choice of difference approximation may depend on the specific physics of the problem, such as the dominance of advection or diffusion.</em> <strong>Numerical method:</strong> The choice of difference approximation may be influenced by the overall numerical method being used, such as finite difference or finite element methods.In summary, the choice of difference approximation depends on a range of factors, including stability, convergence, computational efficiency, and problemspecific considerations.While the central difference approximation is often a good choice, the forward or backward difference approximations may be preferred in scenarios with non-uniform grids, noisy data, advection-dominated problems, or time-dependent problems.</p>
<p>User: You got a correct explanation!Good Job.</p>
<p>User: In the context of boundary conditions, how would you choose between forward and backward difference approximations at the start and end of the domain?</p>
<p>System: When choosing between forward and backward difference approximations at the start and end of the domain, it ultimately depends on the specific boundary conditions and the direction of information flow.Here's a general guideline: <strong>Start of the domain:</strong> * If the boundary condition at the start of the domain is a <strong>Dirichlet boundary condition</strong> (i.e., the value of the function is specified at the boundary), a <strong>forward difference approximation</strong> is often used.This is because the forward difference approximation looks ahead to the next point, which is inside the domain, to estimate the derivative.This approach is consistent with the direction of information flow, where the boundary condition is imposed from the outside and influences the behavior of the solution inside the domain.<em> If the boundary condition at the start of the domain is a </em><em>Neumann boundary condition</em><em> (i.e., the derivative of the function is specified at the boundary), a </em><em>backward difference approximation</em><em> is not suitable, as it would look backwards outside the domain.In this case, a forward difference approximation or a ghost point method (where a fictional point is introduced outside the domain to apply the Neumann boundary condition) can be used.</em><em>End of the domain:</em><em> * If the boundary condition at the end of the domain is a </em><em>Dirichlet boundary condition</em><em>, a </em><em>backward difference approximation</em><em> is often used.This is because the backward difference approximation looks backwards to the previous point, which is inside the domain, to estimate the derivative.This approach is consistent with the direction of information flow, where the boundary condition is imposed from the outside and influences the behavior of the solution inside the domain.</em> If the boundary condition at the end of the domain is a <strong>Neumann boundary condition</strong>, a <strong>forward difference approximation</strong> is not suitable, as it would look forward outside the domain.In this case, a backward difference approximation or a ghost point method can be used.<strong>Additional considerations:</strong> * When using forward or backward difference approximations at the boundaries, it's essential to ensure that the chosen method is consistent with the overall numerical method being used (e.g., finite difference or finite element methods).<em> In some cases, a </em><em>ghost point method</em><em> or a </em><em>one-sided finite difference</em>* can be used to apply boundary conditions.These methods introduce fictional points outside the domain to apply the boundary conditions, which can be useful when dealing with complex boundary conditions or non-uniform grids.In summary, the choice between forward and backward difference approximations at the start and end of the domain depends on the specific boundary conditions and the direction of information flow.A forward difference approximation is often used at the start of the domain with Dirichlet boundary conditions, while a backward difference approximation is often used at the end of the domain with Dirichlet boundary conditions."</p>
<p>H Reliability of LLM-as-an-Interviewer H.1 Verbosity Bias Table 3 presents the correlation between scores and interaction lengths across different stages of the interaction.</p>
<p>H.2 Self-Enhancement Bias</p>
<p>Existing papers primarily analyze the selfenhancement bias of LLM-as-a-Judge through relative rating-comparing the responses of two models and selecting the preferred one (Chiang et al., 2024).However, since LLM-as-an-Interviewer uses absolute rating-evaluating only a single model's response-it is difficult to directly analyze self-enhancement bias.Therefore, we only check whether an apparently observable self-enhancement bias exists.</p>
<p>We analyze self-enhancement bias using GPT-4o, Llama-3.1-70B, and Llama-3.1-8B as both Interviewers and Interviewee.Figures 4-5 indicate that self-enhancement bias, defined as assigning higher scores to oneself, is observed only in Llama-3.1-70Bamong the models.In the MATH dataset, GPT-4o and Llama-3.1-8Btend to assign themselves slightly lower scores compared to other Interviewers, whereas Llama-3.1-70Bassigns itself marginally higher scores.Despite this, the behavior has negligible impact on the overall rankings.As interactions progress, this bias diminishes further, and the score gap between GPT-4o and Llama-3.1-70Bincreases significantly.A similar pattern of diminishing bias and widening score differences is observed in the DepthQA dataset.</p>
<p>Figure 4 and 5 show the trend of selfenhancement bias in MATH and DepthQA.Similar to MATH (Figure 4), no significant selfenhancement bias is observed except for Llama-3.1-70b.Figure 6 illustrates the trends in MATH and DepthQA for follow-up questions.In MATH, as in previous results, Llama-3.1-70bconsistently gives itself higher scores when grading its own responses compared to other models.However, Llama-3.1-70b also tends to give higher scores when grading other models, making it difficult to determine whether this is due to self-enhancement bias or because it generates easier follow-up questions compared to other models.In DepthQA, GPT-4 shows a similar pattern, giving itself higher scores when grading its own responses, but also assigning higher scores to other models when acting as the interviewer.</p>
<p>Figure 1 :
1
Figure 1: Overview of LLM-as-an-Interviewer.LLM-as-an-Interviewer aims to assess LLMs by 1) constructing seed questions of interviews based on existing benchmark datasets, and 2) providing feedback or asking additional follow-up questions.Unlike evaluation being limited to a single score with LLM-as-a-Judge, the Interview Report provided by our framework offers a snapshot of what the model excels at and where it falls short, along with scores for various abilities and examples.</p>
<p>) Error Analysis and Examples, and (3) Comprehensive Summary.The real examples of Interview Report are in Appendix C.1.</p>
<p>809 p = 0.832 p = 0.766 Judge p = 0.464 p = 0.740 p = 0.714</p>
<p>Instructions: 1. Model's Error Type is {error_type}.Do not disclose the error type to the model.2. Based on the error type, generate appropriate feedback as described below.Possible Error Types: 1. Concept: This error type indicates the model lacks the concept used to solve the problem.The feedback should contain a question to assess the understanding of the concept.(e.g., What is the Pythagorean theorem?) 2. Misinterpret: The model misunderstood or misinterpreted the question.The feedback should include a follow-up question or clarification to help the model reassess and correctly interpret the original question.This ensures the model understands the context and requirements before proceeding.3. Calculation: The model made a mistake in calculation.Feedback should focus on identifying the part that requires recalculation.4. N/A: None of the above.Provide a custom description.Response Format (JSON):</p>
<ol>
<li>Create the question solely based on the model's solution.Response Format (JSON): {Format Ex-ample} Input Data: Initial Question: {initial_question} Correct Answer (DO NOT disclose): {answer} Reference Solution (DO NOT disclose): {solution} Model's Solution: {model_solution} Output: DepthQA Task: You are given three inputs: 1. Question:</li>
</ol>
<p>n+</p>
<p>i sin 2πk n for k = 0, 1, . . ., n − 1.Here, cos and sin are the standard trigonometric functions, i refers to the imaginary unit, and π is the mathematical constant Pi (approximately equal to 3.14).Let's say if you wanted to determine the 4th roots of unity, (omitted) b 2 = 32 5. Take the square root of both sides: b = √ 32 b = 4 (since b is a positive integer) 6.Now that we have the value of b, we can find the value of a using the equation ab = 52: , find the value of c using the equation ac = 234: c = 234 a c = 234 c = 18 These values satisfy the original equations ab = 52, ac = 234, and bc = 72.User: Your explanation is generally correct, but there are some inconsistencies in your steps.Specifically, your calculation of b and the adjustment for c requires clarification.You initially found b = 4, which is correct, but then multiplied b by 2 without clear reasoning.Finally, you identified the correct values, but the intermediate explanations need to be streamlined for consistency.Could you revisit and clarify these steps to arrive at a consistent conclusion?You are correct, my initial explanation had some inconsistencies.Here's a revised explanation for determining the values of 'a', 'b', and 'c' from the equations ab = 52, ac = 234, and bc = 72: 1. Divide the equation ac = 234 by the equation ab = 52: both sides by b to get: c = 9 4 b 3. Now, substitute this expression for c into the equation bc = 72: the square root of both sides: b = √ 32 b = 4 (since b is a positive integer) 6.Now that we have the value of b, we can find the value of a using the equation ab = 52: 7. Next, find the value of c using the equation ac = 234: c = 234 a c = 234 13 c = 18 This way, we arrive at the correct values for a, b, and c in a consistent and straightforward manner: a = 13, b = 4, c = 18.</p>
<p>Figure 3 :Figure 4 :
34
Figure 3: Distribution of Response Lengths (measured in tokens) Across Scores in DepthQA.The regression correlation coefficient (r) decreases with the number of Interaction, suggesting a reduction in Verbosity Bias as Interactions progress.</p>
<p>Figure 5 :
5
Figure 5: Performance Score(Precision) by Number of Interactions in DepthQA.The red-highlighted boxes indicate self-evaluated cases.</p>
<p>Figure 6 :
6
Figure 6: Follow-Up Accuracy in MATH and DepthQA.The red-highlighted boxes indicate self-evaluated cases.</p>
<p>Table 3 :
3
Follow-up accuracy of six models on MATH, MINT, and Depth-QA datasets.
Question-Turn-1:Help me construct a catchy, yet scientifically accurate,headline for an article on the (...). Propose 4 options.Model Response (Deepseek): (Irrelevant Response)from sympy import symbols, simplify, solvedef solve_equation():x = symbols('x')equation = x**2 -16 (...)Question-Turn-2:</p>
<p>Table 4 :
4
Dependent and Independent question Counts in 2-turn questions.GPT-4o and Qwen denote the Interviewee.</p>
<p>Table 5 presents the effectiveness of the modifying strategy in mitigating data contamination, which is
OLMoEZephyrModel.Judge. Interv. Judge. Interv.Uncontam.Train setID[1]0.100.050.100.01+Train setOOD [2]0.070.050.080.04Train setOOD[3]0.070.060.060.02Avg.0.080.050.080.02Contam.Test setID[4]0.840.080.610.10+Train setID[5]0.770.150.640.17+Instruct[6]0.560.120.190.09+Train setOOD [7]0.750.100.420.08Avg.0.690.120.420.11</p>
<p>Table 6 :
6
0.0244 0.0151 0.0116 0.0090 0.0082 Standard Deviation of Multiple Runs Across Interactions.
MATH0.0358 0.0480 0.0583 0.0642 0.0704easy0.1115 0.1127 0.0839 0.0797 0.0890hard0.0531 0.1032 0.1281 0.1372 0.1332</p>
<p>Table 7 :
7
Statistical Comparison of Sampling Variability
MATH, Interviewee: GPT-4o(Interviewer: GPT-4o)</p>
<p>Table 8 :
8
Observed accuracy degradation across multiround interactions.</p>
<p>Table 9 :
9
Comparison of average cost per evaluation round normalized to GPT-4-turbo pricing.</p>
<p>Table 10 :
10
Examples of Feedback posed by the Interviewer (GPT-4o).
OriginalModifiedIf a recipe for a two-pound cake requiresIf a recipe for a two-pound cake requiresMATH1.5 cups of flour, how many cups arey cups of flour, how many cups areneeded for 2 five-pound cakes?needed for 2 five-pound cakes?QuestionQuestionDepthQAWhat are the properties of straight lines in geometry?What are parallel and perpendicular lines in geometry?Reference SolutionReference Solution1. A straight line is the shortest distance (...)1. A straight line is the shortest distance (...)8. Parallel lines are always the same8. Parallel lines are always the samedistance apart and never meet.(...)distance apart and never meet.(...)10. Two lines on a plane that never10. Two lines on a plane that nevermeet are called parallel lines.meet are called parallel lines.11. Two lines that intersect at a right angle11. Two lines that intersect at a right angle(90 degrees) are called perpendicular lines.(90 degrees) are called perpendicular lines.</p>
<p>Table 11 :
11
Example of Query Modification.Changed parts are colored blue in the original and purple in the modified question.
Interviewer Model</p>
<p>Table 12 :
12
Each Interviewer's Performance Across Different Phases (%)
D Prompt 10Output Format: {Format Example}D.1 Preparing Seed QuestionsInput:Question: {Question}MATHSolution: {Solution}You are given a mathematical expressionwhere key information must be removed,but the remaining structure and operationsD.2 Interview Processshould not change. Only remove criticaldata that makes the problem unsolvableD.2.1 Feedback Generationwithout that information.Guidelines:1. Remove only key information requiredfor the solution (e.g., constants, values)without changing the operations and con-vert to unknown variables or ambiguouswords.2. Ensure the revised question retains thesame structure and other information.3. Avoid removing trivial or subtle details.4. Do not modify the mathematical opera-tions or alter the structure of the equation.5. In the explanation, detail exactly howand where deleted information fits intothe equation or text. For example, ex-plain which unknown variable or ambigu-ous words denote what value.Response format must be in JSON asshown below:Example 1: {Example}Example 2: {Example}Input Question: {Question}Output:DepthQAYou are provided with a question and its so-lution. Your task is to create a unique ques-tion based on these.Instructions:1. Create one new question that is distinctfrom the original question:-The new question should be directly an-swerable using the provided solution.-It should avoid focusing on overly minoror trivial details.2. Ensure the new question aligns well withthe given solution and maintains relevanceto the original context.
10 Not all prompts are included here.Refer to our GitHub repository for additional prompts.</p>
<p>Table 15 :
15
Performance comparison of models on MT-Bench and the Interviewer setting.</p>
<p>The code for our framework is publicly available at https://github.com/interview-eval
We provide a general Python library applicable to various tasks: https://pypi.org/project/interview-eval/
We provide a statistical analysis of the sampling in Appendix C.2.
We also provide an analysis of self-enhancement bias in the appendix H.2.
Interviewee: GPT-4o, GPT-3.5-turbo, Llama-3.1-70b, Llama-3.1-8b, DeepSeek-Math, Qwen2-Math.
AcknowledgementsThis research project has benefitted from the Microsoft Accelerate Foundation Models Research (AFMR) grant program through which leading foundation models hosted by Microsoft Azure along with access to Azure credits were provided to conduct the research.This work was supported by Institute for Information &amp; communications Technology Promotion(IITP) grant funded by the Korea government (MSIP) (No.RS-2024-00443251, Accurate and Safe Multimodal, Multilingual Personalized AI Tutors)ModelJudge Acc_seed(1) Acc_seed(2Table13: Performance of different models using LLM-as-an-Interviewer Framework in MINT reasoning dataset.∆ represents the difference between Acc_seed(3) and Acc_seed(1).The given expression can be written as, where a, b, and c are positive integers.We are given the expression:x √ 6 + y √ 10 + z √ 15 + w Squaring the expression to get rid of the outer square root:, and √ 15: x = 2ab, y = 2ac, z = 2bc Also, the constant term w is:We know that a, b, and c are positive integers.From the coefficients of √ 6, √ 10, and √ 15:We can see that x, y, and z can be expressed as multiples of 2. Also, the constant term w is: w = 2a 2 + 3b 2 + 5c 2 Since x, y, and z are multiples of 2, w must be a multiple of 2 as well.The least possible values for a, b, and c that satisfy the conditions are:However, w = 10 is not a multiple of 2 that satisfies the conditions.User: You got a correct answer!Good Job.G.1 Training Details and ConfigurationsFor training, we utilized four 80GB NVIDIA A100 GPUs.We trained Zephyr models using the implementation from the Alignment Handbook repository. 11We trained OLMoE models using the instructions provided on its GitHub repository.12 Training hyperparameters are detailed in Table17 and 18
MT-bench-101: A fine-grained benchmark for evaluating large language models in multi-turn dialogues. Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, Wanli Ouyang, 10.18653/v1/2024.acl-long.401Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, ACM Transactions on Intelligent Systems and Technology. 1532024</p>
<p>Can large language models be an alternative to human evaluations?. Cheng- , Han Chiang, Hung-Yi Lee, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics2023</p>
<p>Chatbot arena: An open platform for evaluating llms by human preference. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E Gonzalez, Ion Stoica, 2024</p>
<p>Evading data contamination detection for language models is (too) easy. Jasper Dekoninck, Mark Niklas Müller, Maximilian Baader, Marc Fischer, Martin Vechev, 2024a</p>
<p>Constat: Performance-based contamination detection in large language models. Jasper Dekoninck, Mark Niklas Müller, Martin Vechev, 2024b</p>
<p>Investigating data contamination in modern benchmarks for large language models. Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Gerstein, Arman Cohan, 2024</p>
<p>. Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, Matt Gardner, 2021Documenting large webtext corpora: A case study on the colossal clean crawled corpus</p>
<p>Length-controlled alpacaeval: A simple way to debias automatic evaluators. Yann Dubois, Balázs Galambosi, Percy Liang, Tatsunori B Hashimoto, 2024a</p>
<p>Length-controlled alpacaeval: A simple debiasing of automatic evaluators. Yann Dubois, Percy Liang, Tatsunori Hashimoto, First Conference on Language Modeling. 2024b</p>
<p>Armando Solar-Lezama, Koushik Sen, and Ion Stoica. 2024. Livecodebench: Holistic and contamination free evaluation of large language models for code. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, </p>
<p>Investigating data contamination for pretraining language models. Minhao Jiang, Ken Ziyu Liu, Ming Zhong, Rylan Schaeffer, Siru Ouyang, Jiawei Han, Sanmi Koyejo, 2024</p>
<p>Prometheus: Inducing finegrained evaluation capability in language models. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, Minjoon Seo, 2024</p>
<p>Hierarchical deconstruction of LLM reasoning: A graph-based framework for analyzing knowledge utilization. Miyoung Ko, Sue , Hyun Park, Joonsuk Park, Minjoon Seo, 10.18653/v1/2024.emnlp-main.288Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USA2024Association for Computational Linguistics</p>
<p>Wai-Chung Kwan, Xingshan Zeng, Yuxin Jiang, Yufei Wang, Liangyou Li, Lifeng Shang, Xin Jiang, Qun Liu, Kam-Fai Wong, arXiv:2401.16745Mt-eval: A multiturn capabilities evaluation benchmark for large language models. 2024arXiv preprint</p>
<p>Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, arXiv:2411.16594From generation to judgment: Opportunities and challenges of llm-as-a-judge. 2024aarXiv preprint</p>
<p>Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons. Margaret Li, Jason Weston, Stephen Roller, arXiv:1909.030872019arXiv preprint</p>
<p>Autobencher: Creating salient, novel, difficult datasets for language models. Lisa Xiang, Evan Zheran Li, Percy Liu, Tatsunori Liang, Hashimoto, 2024b</p>
<p>Interview evaluation: A novel approach for automatic evaluation of conversational question answering models. Xibo Li, Bowei Zou, Yifan Fan, Yanling Li, Ai Ti Aw, Yu Hong, 10.18653/v1/2023.emnlp-main.209Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023a</p>
<p>Alpacaeval: An automatic evaluator of instruction-following models. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023b</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, arXiv:2211.09110Holistic evaluation of language models. Ananya Kumar, et al. 2022arXiv preprint</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>Data contamination: From memorization to exploitation. Inbal Magar, Roy Schwartz, 2022</p>
<p>FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-Tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, Hannaneh Hajishirzi, 10.18653/v1/2023.emnlp-main.741Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, Mehrdad Farajtabar, 2024</p>
<p>Proving test set contamination in black box language models. Yonatan Oren, Nicole Meister, Niladri Chatterji, Faisal Ladhak, Tatsunori B Hashimoto, 2023a</p>
<p>Proving test set contamination in black-box language models. Yonatan Oren, Nicole Meister, S Niladri, Faisal Chatterji, Tatsunori Ladhak, Hashimoto, The Twelfth International Conference on Learning Representations. 2023b</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Nlp evaluation in trouble: On the need to measure llm data contamination for each benchmark. Oscar Sainz, Jon Campos, Iker García-Ferrero, Julen Etxaniz, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023Oier Lopez de Lacalle, and Eneko Agirre</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y K Li, Y Wu, Daya Guo, Deepseekmath: Pushing the limits of mathematical reasoning in open language models. 2024</p>
<p>Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, Luke Zettlemoyer, Detecting pretraining data from large language models. 2024</p>
<p>. Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, Kyle Lo, 2024Dolma: an open corpus of three trillion tokens for language model pretraining research</p>
<p>. Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro Von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M Rush, Thomas Wolf, 2023Direct distillation of lm alignmentZephyr</p>
<p>Benchmark selfevolving: A multi-agent framework for dynamic llm evaluation. Siyuan Wang, Zhuohan Long, Zhihao Fan, Zhongyu Wei, Xuanjing Huang, 2024a</p>
<p>Mint: Evaluating llms in multi-turn interaction with tools and language feedback. Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, Heng Ji, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, Yue Zhang, 2024b</p>
<p>. Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, Chinmay Hegde, Yann Lecun, Tom Goldstein, Willie Neiswanger, Micah Goldblum, 2024Livebench: A challenging, contamination-free llm benchmark</p>
<p>Benchmarking benchmark leakage in large language models. Ruijie Xu, Zengzhi Wang, Run-Ze Fan, Pengfei Liu, 2024</p>
<p>An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. 2024. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. </p>
<p>Assessing contamination in large language models: Introducing the logprober method. Nicolas Yax, Pierre-Yves Oudeyer, Stefano Palminteri, 2024</p>
<p>KIEval: A knowledgegrounded interactive evaluation framework for large language models. Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Wei Ye, Jindong Wang, Xing Xie, Yue Zhang, Shikun Zhang, 10.18653/v1/2024.acl-long.325Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, Thailand20241Association for Computational Linguistics</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, arXiv:1904.09675Bertscore: Evaluating text generation with bert. 2019arXiv preprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 2023a36</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P Xing, Hao Zhang, Joseph E Gonzalez, and Ion Stoica. 2023b. Judging llm-as-a-judge with mt-bench and chatbot arena. </p>
<p>Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, Jiawei Han, arXiv:2311.01964Don't make your llm an evaluation benchmark cheater. 2023aarXiv preprint</p>
<p>Don't make your llm an evaluation benchmark cheater. Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, Jiawei Han, 2023b</p>            </div>
        </div>

    </div>
</body>
</html>
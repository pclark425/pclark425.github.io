<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4851 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4851</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4851</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-106.html">extraction-schema-106</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-168170169</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1905.12149v1.pdf" target="_blank">SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver</a></p>
                <p><strong>Paper Abstract:</strong> Integrating logical reasoning within deep learning architectures has been a major goal of modern AI systems. In this paper, we propose a new direction toward this goal by introducing a differentiable (smoothed) maximum satisfiability (MAXSAT) solver that can be integrated into the loop of larger deep learning systems. Our (approximate) solver is based upon a fast coordinate descent approach to solving the semidefinite program (SDP) associated with the MAXSAT problem. We show how to analytically differentiate through the solution to this SDP and efficiently solve the associated backward pass. We demonstrate that by integrating this solver into end-to-end learning systems, we can learn the logical structure of challenging problems in a minimally supervised fashion. In particular, we show that we can learn the parity function using single-bit supervision (a traditionally hard task for deep networks) and learn how to play 9x9 Sudoku solely from examples. We also solve a"visual Sudok"problem that maps images of Sudoku puzzles to their associated logical solutions by combining our MAXSAT solver with a traditional convolutional architecture. Our approach thus shows promise in integrating logical structures within deep learning.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4851.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4851.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SATNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SATNet (Differentiable MAXSAT / SAT-Net layer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A differentiable, low-rank semidefinite-program (SDP) relaxation MAXSAT solver implemented as a neural network layer; uses block coordinate descent for forward and backward passes and can be embedded end-to-end in larger networks to learn logical constraints from data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SATNet (differentiable MAXSAT SDP layer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A custom neural network layer implementing a smoothed MAXSAT SDP relaxation solved via fast block coordinate descent; low-rank parameterization S (m clauses), continuous vector relaxations v_i in R^k with k = sqrt(2n)+1, randomized rounding / probability outputs, analytical backpropagation via a derived matrix solve. For 9x9 Sudoku experiments the authors used a single SATNet layer with 300 auxiliary variables and m = 600 (low-rank clause matrix); trained with Adam (learning rate 2e-3). GPU-parallel CUDA implementation available.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>9x9 Sudoku (original and permuted); Visual Sudoku (MNIST images of boards); 4x4 Sudoku (in Appendix E)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Logic-based spatial puzzle: fill a 9x9 grid so each row, column, and each 3x3 subgrid contains digits 1-9 exactly once. Permuted Sudoku removes spatial locality by applying a fixed permutation to the bit-vector encoding. Visual Sudoku provides MNIST images of digits in cells, requiring perception + reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Encodes unknown cells and known inputs as a MAXSAT instance via continuous SDP relaxation; solves via coordinate descent to obtain relaxed output vectors, converts to probabilistic / discrete outputs via randomized rounding or thresholding. The SATNet layer's weights S (clause coefficients) are learned end-to-end from supervision (digitwise NLL / cross-entropy). For visual Sudoku, SATNet is appended to a per-cell digit classifier (LeNet) so the perception and constraint-solver are trained jointly.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Strong empirical evidence that SATNet learns logical constraints rather than exploiting local spatial patterns: performance is essentially identical (98.3% board-wise test accuracy) on original Sudoku and on permuted Sudoku where spatial locality is removed. This indicates the model is capturing global combinatorial constraints (rows/cols/blocks) rather than image-local heuristics. Visual Sudoku results show the layer integrates perception and reasoning but rely on classifier accuracy (theoretical best limited by digit recognition).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Classic 9x9 logical Sudoku: 98.3% board-wise test accuracy (trained on 9k examples, tested on 1k). Reached 95.0% test accuracy in 22 epochs (37 minutes) on a GTX 1080 Ti; 98.3% after 100 epochs (172 minutes). Permuted Sudoku: 98.3% test accuracy. Visual Sudoku (end-to-end with LeNet digit classifier): 63.2% board-wise test accuracy (theoretical 'best' for their chosen MNIST classifier is 74.7% given filled-cell statistics); 93.6% train accuracy reported for visual setting. 4x4 Sudoku in appendix: converged to 100% board-wise test accuracy in 2 epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires user-chosen capacity hyperparameters (max clauses m, number of auxiliary variables). Visual Sudoku performance limited by the accuracy of the front-end digit classifier (authors note theoretical upper bound of 74.7% for their MNIST classifier on their dataset); performance degrades if perception is poor. The solver is an approximate smoothed MAXSAT SDP relaxation (low-rank), so representational capacity depends on m and auxiliary vars; some domains may require larger m/k. Training/time tradeoffs: while GPU implementation is much faster than CPU, larger problems may still be costly.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Outperforms convolutional neural network baselines and OptNet on Sudoku tasks: ConvNet baseline achieved 0.04% test accuracy on logical 9x9 Sudoku (trained 72.6% train acc), ConvNetMask 15.1% test; OptNet required more epochs and more GPU time on 4x4 (OptNet: ~3-4 epochs to converge vs SATNet 1 epoch; OptNet 12 minutes for 20 epochs on GTX1080Ti vs SATNet 2 minutes). On permuted Sudoku the ConvNet baselines fail (0% test), while SATNet retains high accuracy, demonstrating that SATNet learns non-local logical structure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4851.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4851.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ConvNet baseline (Park 2016)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Convolutional neural network baseline modeled on Park (2016)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A strong convolutional baseline that treats the Sudoku bit-encoding as multi-channel images (one channel per digit) and applies deep convolutional layers exploiting spatial locality; used for comparison with SATNet.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can neural networks crack sudoku?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>10-layer ConvNet (512 filters per layer, 3x3 kernels) as in Park (2016)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A convolutional architecture with 10 convolutional layers, each with 512 3x3 filters, that interprets the bit-vector Sudoku input as 9 image channels; trained with mean-squared-error loss and Adam optimizer (learning rate 1e-4) in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>9x9 Sudoku (logical inputs); Visual Sudoku variant that feeds per-cell probabilistic outputs</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same Sudoku tasks as above; model processes a spatial image-like representation and attempts to predict missing digits using learned local patterns and spatial context.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Standard feedforward convolutional processing of the bit-encoded Sudoku board (or of a perception-produced probabilistic bit representation for visual Sudoku), relying on spatial locality and translation-invariant filters to learn mapping from partial boards to complete solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>The model is designed to exploit spatial locality; however, empirical evidence shows it largely relies on such locality and fails when locality is removed (permuted Sudoku) — poor generalization indicates it did not learn global logical constraints robustly.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Logical 9x9 Sudoku: Training accuracy 72.6% but held-out test accuracy 0.04% (overfit/trained but failed to generalize). Permuted Sudoku: Train and test accuracy near 0%. Visual Sudoku: essentially no improvement out-of-sample; tiny training improvements only.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails to generalize beyond training distribution; overfits to training examples and spatial patterns, collapses on permuted input encoding (0% test accuracy). Performs poorly in visual end-to-end setting compared to SATNet.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly to SATNet; SATNet vastly superior on generalization and test accuracy, indicating that ConvNet baseline does not discover Sudoku's global logical constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4851.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4851.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ConvNetMask</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ConvNet with binary mask indicating unknown cells</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variant of the ConvNet baseline augmented with binary mask channels that explicitly indicate which board entries are unknown; intended to help the convolutional model focus on missing cells.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ConvNetMask (ConvNet with additional mask input channels)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same ConvNet architecture as the ConvNet baseline but augmented by additional image channels encoding a binary mask that flags which entries are to be predicted; trained with MSE and Adam (learning rate 1e-4).</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>9x9 Sudoku (logical inputs) and permuted Sudoku</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>As above — Sudoku completion tasks. Mask provides explicit information about prediction targets but does not change global combinatorial structure.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Uses the mask channels to condition the convnet on unknown positions; otherwise standard convolutional prediction approach relying on local receptive fields.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Mask aids learning when spatial locality exists (training accuracy improved), but fails to enable learning global logical Sudoku rules: performance collapses in permuted setting, showing reliance on spatial cues rather than learned logical constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Logical 9x9 Sudoku: 91.4% training accuracy, 15.1% test accuracy. Permuted Sudoku: training accuracy ~0% and test 0%. Visual Sudoku: ConvNetMask achieves 0.1% test (with 89% train reported in table) — does not generalize well.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Although the mask helps the network overfit less in training, test generalization remains poor for discovering non-local logical structure; fails on permuted encodings and visual end-to-end tasks compared to SATNet.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Performs better than pure ConvNet on training but much worse than SATNet on held-out generalization; shows mask is insufficient to enable learning of Sudoku constraints without an explicit reasoning module.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4851.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4851.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LeNet (per-cell digit classifier)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LeNet (LeCun et al., 1998) per-cell convolutional classifier</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classical convolutional neural network architecture used per Sudoku cell to convert MNIST-like cell images into per-digit probability distributions, whose outputs feed into the SATNet layer for end-to-end Visual Sudoku.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gradient-based learning applied to document recognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LeNet (per-cell CNN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard LeNet digit classifier (as in LeCun et al., 1998) applied independently to each Sudoku cell image; outputs per-cell probabilistic bit representations that are input to the SATNet MAXSAT layer. In training the visual pipeline the authors used Adam with learning rate 1e-5 for the convolutional layer and 2e-3 for the SATNet layer.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Visual Sudoku (MNIST-digit images in Sudoku cells)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Each Sudoku cell is an image of a digit (or blank); the perception module produces digit probabilities that must be combined with learned logical constraints to produce a consistent filled board.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Per-cell digit recognition via LeNet; predicted per-cell digit distributions are fed as (probabilistic) inputs to the SATNet layer which enforces learned global constraints to produce consistent board solutions. Entire pipeline trained end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Integration with SATNet provides evidence that the system learns to combine per-cell perceptual outputs with global logical constraints; visual Sudoku performance reaching 63.2% (85% of theoretical best given classifier accuracy) indicates the reasoning component successfully enforces spatial/structural puzzle constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When combined with SATNet: Visual Sudoku board-wise test accuracy 63.2%. The authors report a theoretical best of 74.7% for their chosen MNIST classifier and dataset statistics; SATNet + LeNet achieves ~85% of that bound.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Visual Sudoku performance is bounded by the front-end digit classifier accuracy; errors in perception propagate to the reasoning layer and limit end-to-end accuracy. Authors note the theoretical upper bound computation and attribute gap to classifier generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against ConvNet-based baselines (which pipe the ConvNet visual outputs into a ConvNet solver), the LeNet+SATNet pipeline substantially outperforms them in end-to-end board-wise accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4851.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4851.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OptNet (comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OptNet: Differentiable optimization as a layer in neural networks (Amos & Kolter, 2017)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A differentiable optimization layer (quadratic program solver) that can be embedded in neural networks; included as a comparison point for optimization-as-layer approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Optnet: Differentiable optimization as a layer in neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OptNet (differentiable QP layer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A differentiable quadratic-program (QP) layer that can be used inside neural networks to impose optimization-based constraints. The authors attempted to use OptNet for Sudoku but found it made little progress on 9x9 and therefore compared to OptNet primarily on a 4x4 Sudoku variant where OptNet converged more slowly.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>4x4 Sudoku (Appendix E) — used for controlled comparison; attempted on 9x9 but training was impractical</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Reduced-size Sudoku (4x4) used to benchmark optimization-layer approaches under more tractable computational budget.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Uses differentiable QP inference as an embedded layer; compared in terms of epochs-to-convergence and runtime per epoch against SATNet's coordinate-descent SDP relaxation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>OptNet, as an optimization-layer approach, can encode global constraints; on 4x4 Sudoku OptNet attains high test accuracy but converges in more epochs and with higher runtime compared to SATNet. No explicit probing of spatial representations reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>On 4x4 Sudoku OptNet reached similar final test accuracy as SATNet but required 3-4 epochs to converge (SATNet 1 epoch) and in preliminary benchmarks required ~12 minutes for 20 epochs on a GTX 1080 Ti versus SATNet's ~2 minutes for the same workload.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>OptNet scaled poorly in the authors' experiments for the full 9x9 Sudoku (training made little progress even after days), indicating computational scalability limitations for larger combinatorial instances compared to SATNet's low-rank SDP solver.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used by authors as comparison for optimization-as-layer approaches: SATNet converged faster (fewer epochs) and was faster per epoch on GPU; both achieved high accuracy on small 4x4 task but OptNet was impractical for 9x9 in their setup.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Recurrent relational networks <em>(Rating: 2)</em></li>
                <li>Can neural networks crack sudoku? <em>(Rating: 2)</em></li>
                <li>Learning a sat solver from single-bit supervision <em>(Rating: 2)</em></li>
                <li>Optnet: Differentiable optimization as a layer in neural networks <em>(Rating: 2)</em></li>
                <li>Learning explanatory rules from noisy data <em>(Rating: 1)</em></li>
                <li>Deepproblog: Neural probabilistic logic programming <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4851",
    "paper_id": "paper-168170169",
    "extraction_schema_id": "extraction-schema-106",
    "extracted_data": [
        {
            "name_short": "SATNet",
            "name_full": "SATNet (Differentiable MAXSAT / SAT-Net layer)",
            "brief_description": "A differentiable, low-rank semidefinite-program (SDP) relaxation MAXSAT solver implemented as a neural network layer; uses block coordinate descent for forward and backward passes and can be embedded end-to-end in larger networks to learn logical constraints from data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SATNet (differentiable MAXSAT SDP layer)",
            "model_description": "A custom neural network layer implementing a smoothed MAXSAT SDP relaxation solved via fast block coordinate descent; low-rank parameterization S (m clauses), continuous vector relaxations v_i in R^k with k = sqrt(2n)+1, randomized rounding / probability outputs, analytical backpropagation via a derived matrix solve. For 9x9 Sudoku experiments the authors used a single SATNet layer with 300 auxiliary variables and m = 600 (low-rank clause matrix); trained with Adam (learning rate 2e-3). GPU-parallel CUDA implementation available.",
            "puzzle_name": "9x9 Sudoku (original and permuted); Visual Sudoku (MNIST images of boards); 4x4 Sudoku (in Appendix E)",
            "puzzle_description": "Logic-based spatial puzzle: fill a 9x9 grid so each row, column, and each 3x3 subgrid contains digits 1-9 exactly once. Permuted Sudoku removes spatial locality by applying a fixed permutation to the bit-vector encoding. Visual Sudoku provides MNIST images of digits in cells, requiring perception + reasoning.",
            "mechanism_or_strategy": "Encodes unknown cells and known inputs as a MAXSAT instance via continuous SDP relaxation; solves via coordinate descent to obtain relaxed output vectors, converts to probabilistic / discrete outputs via randomized rounding or thresholding. The SATNet layer's weights S (clause coefficients) are learned end-to-end from supervision (digitwise NLL / cross-entropy). For visual Sudoku, SATNet is appended to a per-cell digit classifier (LeNet) so the perception and constraint-solver are trained jointly.",
            "evidence_of_spatial_reasoning": "Strong empirical evidence that SATNet learns logical constraints rather than exploiting local spatial patterns: performance is essentially identical (98.3% board-wise test accuracy) on original Sudoku and on permuted Sudoku where spatial locality is removed. This indicates the model is capturing global combinatorial constraints (rows/cols/blocks) rather than image-local heuristics. Visual Sudoku results show the layer integrates perception and reasoning but rely on classifier accuracy (theoretical best limited by digit recognition).",
            "performance_metrics": "Classic 9x9 logical Sudoku: 98.3% board-wise test accuracy (trained on 9k examples, tested on 1k). Reached 95.0% test accuracy in 22 epochs (37 minutes) on a GTX 1080 Ti; 98.3% after 100 epochs (172 minutes). Permuted Sudoku: 98.3% test accuracy. Visual Sudoku (end-to-end with LeNet digit classifier): 63.2% board-wise test accuracy (theoretical 'best' for their chosen MNIST classifier is 74.7% given filled-cell statistics); 93.6% train accuracy reported for visual setting. 4x4 Sudoku in appendix: converged to 100% board-wise test accuracy in 2 epochs.",
            "limitations_or_failure_cases": "Requires user-chosen capacity hyperparameters (max clauses m, number of auxiliary variables). Visual Sudoku performance limited by the accuracy of the front-end digit classifier (authors note theoretical upper bound of 74.7% for their MNIST classifier on their dataset); performance degrades if perception is poor. The solver is an approximate smoothed MAXSAT SDP relaxation (low-rank), so representational capacity depends on m and auxiliary vars; some domains may require larger m/k. Training/time tradeoffs: while GPU implementation is much faster than CPU, larger problems may still be costly.",
            "comparison_baseline": "Outperforms convolutional neural network baselines and OptNet on Sudoku tasks: ConvNet baseline achieved 0.04% test accuracy on logical 9x9 Sudoku (trained 72.6% train acc), ConvNetMask 15.1% test; OptNet required more epochs and more GPU time on 4x4 (OptNet: ~3-4 epochs to converge vs SATNet 1 epoch; OptNet 12 minutes for 20 epochs on GTX1080Ti vs SATNet 2 minutes). On permuted Sudoku the ConvNet baselines fail (0% test), while SATNet retains high accuracy, demonstrating that SATNet learns non-local logical structure.",
            "uuid": "e4851.0",
            "source_info": {
                "paper_title": "SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "ConvNet baseline (Park 2016)",
            "name_full": "Convolutional neural network baseline modeled on Park (2016)",
            "brief_description": "A strong convolutional baseline that treats the Sudoku bit-encoding as multi-channel images (one channel per digit) and applies deep convolutional layers exploiting spatial locality; used for comparison with SATNet.",
            "citation_title": "Can neural networks crack sudoku?",
            "mention_or_use": "use",
            "model_name": "10-layer ConvNet (512 filters per layer, 3x3 kernels) as in Park (2016)",
            "model_description": "A convolutional architecture with 10 convolutional layers, each with 512 3x3 filters, that interprets the bit-vector Sudoku input as 9 image channels; trained with mean-squared-error loss and Adam optimizer (learning rate 1e-4) in the paper's experiments.",
            "puzzle_name": "9x9 Sudoku (logical inputs); Visual Sudoku variant that feeds per-cell probabilistic outputs",
            "puzzle_description": "Same Sudoku tasks as above; model processes a spatial image-like representation and attempts to predict missing digits using learned local patterns and spatial context.",
            "mechanism_or_strategy": "Standard feedforward convolutional processing of the bit-encoded Sudoku board (or of a perception-produced probabilistic bit representation for visual Sudoku), relying on spatial locality and translation-invariant filters to learn mapping from partial boards to complete solutions.",
            "evidence_of_spatial_reasoning": "The model is designed to exploit spatial locality; however, empirical evidence shows it largely relies on such locality and fails when locality is removed (permuted Sudoku) — poor generalization indicates it did not learn global logical constraints robustly.",
            "performance_metrics": "Logical 9x9 Sudoku: Training accuracy 72.6% but held-out test accuracy 0.04% (overfit/trained but failed to generalize). Permuted Sudoku: Train and test accuracy near 0%. Visual Sudoku: essentially no improvement out-of-sample; tiny training improvements only.",
            "limitations_or_failure_cases": "Fails to generalize beyond training distribution; overfits to training examples and spatial patterns, collapses on permuted input encoding (0% test accuracy). Performs poorly in visual end-to-end setting compared to SATNet.",
            "comparison_baseline": "Compared directly to SATNet; SATNet vastly superior on generalization and test accuracy, indicating that ConvNet baseline does not discover Sudoku's global logical constraints.",
            "uuid": "e4851.1",
            "source_info": {
                "paper_title": "SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "ConvNetMask",
            "name_full": "ConvNet with binary mask indicating unknown cells",
            "brief_description": "Variant of the ConvNet baseline augmented with binary mask channels that explicitly indicate which board entries are unknown; intended to help the convolutional model focus on missing cells.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ConvNetMask (ConvNet with additional mask input channels)",
            "model_description": "Same ConvNet architecture as the ConvNet baseline but augmented by additional image channels encoding a binary mask that flags which entries are to be predicted; trained with MSE and Adam (learning rate 1e-4).",
            "puzzle_name": "9x9 Sudoku (logical inputs) and permuted Sudoku",
            "puzzle_description": "As above — Sudoku completion tasks. Mask provides explicit information about prediction targets but does not change global combinatorial structure.",
            "mechanism_or_strategy": "Uses the mask channels to condition the convnet on unknown positions; otherwise standard convolutional prediction approach relying on local receptive fields.",
            "evidence_of_spatial_reasoning": "Mask aids learning when spatial locality exists (training accuracy improved), but fails to enable learning global logical Sudoku rules: performance collapses in permuted setting, showing reliance on spatial cues rather than learned logical constraints.",
            "performance_metrics": "Logical 9x9 Sudoku: 91.4% training accuracy, 15.1% test accuracy. Permuted Sudoku: training accuracy ~0% and test 0%. Visual Sudoku: ConvNetMask achieves 0.1% test (with 89% train reported in table) — does not generalize well.",
            "limitations_or_failure_cases": "Although the mask helps the network overfit less in training, test generalization remains poor for discovering non-local logical structure; fails on permuted encodings and visual end-to-end tasks compared to SATNet.",
            "comparison_baseline": "Performs better than pure ConvNet on training but much worse than SATNet on held-out generalization; shows mask is insufficient to enable learning of Sudoku constraints without an explicit reasoning module.",
            "uuid": "e4851.2",
            "source_info": {
                "paper_title": "SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "LeNet (per-cell digit classifier)",
            "name_full": "LeNet (LeCun et al., 1998) per-cell convolutional classifier",
            "brief_description": "A classical convolutional neural network architecture used per Sudoku cell to convert MNIST-like cell images into per-digit probability distributions, whose outputs feed into the SATNet layer for end-to-end Visual Sudoku.",
            "citation_title": "Gradient-based learning applied to document recognition",
            "mention_or_use": "use",
            "model_name": "LeNet (per-cell CNN)",
            "model_description": "Standard LeNet digit classifier (as in LeCun et al., 1998) applied independently to each Sudoku cell image; outputs per-cell probabilistic bit representations that are input to the SATNet MAXSAT layer. In training the visual pipeline the authors used Adam with learning rate 1e-5 for the convolutional layer and 2e-3 for the SATNet layer.",
            "puzzle_name": "Visual Sudoku (MNIST-digit images in Sudoku cells)",
            "puzzle_description": "Each Sudoku cell is an image of a digit (or blank); the perception module produces digit probabilities that must be combined with learned logical constraints to produce a consistent filled board.",
            "mechanism_or_strategy": "Per-cell digit recognition via LeNet; predicted per-cell digit distributions are fed as (probabilistic) inputs to the SATNet layer which enforces learned global constraints to produce consistent board solutions. Entire pipeline trained end-to-end.",
            "evidence_of_spatial_reasoning": "Integration with SATNet provides evidence that the system learns to combine per-cell perceptual outputs with global logical constraints; visual Sudoku performance reaching 63.2% (85% of theoretical best given classifier accuracy) indicates the reasoning component successfully enforces spatial/structural puzzle constraints.",
            "performance_metrics": "When combined with SATNet: Visual Sudoku board-wise test accuracy 63.2%. The authors report a theoretical best of 74.7% for their chosen MNIST classifier and dataset statistics; SATNet + LeNet achieves ~85% of that bound.",
            "limitations_or_failure_cases": "Visual Sudoku performance is bounded by the front-end digit classifier accuracy; errors in perception propagate to the reasoning layer and limit end-to-end accuracy. Authors note the theoretical upper bound computation and attribute gap to classifier generalization.",
            "comparison_baseline": "Compared against ConvNet-based baselines (which pipe the ConvNet visual outputs into a ConvNet solver), the LeNet+SATNet pipeline substantially outperforms them in end-to-end board-wise accuracy.",
            "uuid": "e4851.3",
            "source_info": {
                "paper_title": "SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "OptNet (comparison)",
            "name_full": "OptNet: Differentiable optimization as a layer in neural networks (Amos & Kolter, 2017)",
            "brief_description": "A differentiable optimization layer (quadratic program solver) that can be embedded in neural networks; included as a comparison point for optimization-as-layer approaches.",
            "citation_title": "Optnet: Differentiable optimization as a layer in neural networks",
            "mention_or_use": "use",
            "model_name": "OptNet (differentiable QP layer)",
            "model_description": "A differentiable quadratic-program (QP) layer that can be used inside neural networks to impose optimization-based constraints. The authors attempted to use OptNet for Sudoku but found it made little progress on 9x9 and therefore compared to OptNet primarily on a 4x4 Sudoku variant where OptNet converged more slowly.",
            "puzzle_name": "4x4 Sudoku (Appendix E) — used for controlled comparison; attempted on 9x9 but training was impractical",
            "puzzle_description": "Reduced-size Sudoku (4x4) used to benchmark optimization-layer approaches under more tractable computational budget.",
            "mechanism_or_strategy": "Uses differentiable QP inference as an embedded layer; compared in terms of epochs-to-convergence and runtime per epoch against SATNet's coordinate-descent SDP relaxation.",
            "evidence_of_spatial_reasoning": "OptNet, as an optimization-layer approach, can encode global constraints; on 4x4 Sudoku OptNet attains high test accuracy but converges in more epochs and with higher runtime compared to SATNet. No explicit probing of spatial representations reported.",
            "performance_metrics": "On 4x4 Sudoku OptNet reached similar final test accuracy as SATNet but required 3-4 epochs to converge (SATNet 1 epoch) and in preliminary benchmarks required ~12 minutes for 20 epochs on a GTX 1080 Ti versus SATNet's ~2 minutes for the same workload.",
            "limitations_or_failure_cases": "OptNet scaled poorly in the authors' experiments for the full 9x9 Sudoku (training made little progress even after days), indicating computational scalability limitations for larger combinatorial instances compared to SATNet's low-rank SDP solver.",
            "comparison_baseline": "Used by authors as comparison for optimization-as-layer approaches: SATNet converged faster (fewer epochs) and was faster per epoch on GPU; both achieved high accuracy on small 4x4 task but OptNet was impractical for 9x9 in their setup.",
            "uuid": "e4851.4",
            "source_info": {
                "paper_title": "SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver",
                "publication_date_yy_mm": "2019-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Recurrent relational networks",
            "rating": 2,
            "sanitized_title": "recurrent_relational_networks"
        },
        {
            "paper_title": "Can neural networks crack sudoku?",
            "rating": 2,
            "sanitized_title": "can_neural_networks_crack_sudoku"
        },
        {
            "paper_title": "Learning a sat solver from single-bit supervision",
            "rating": 2,
            "sanitized_title": "learning_a_sat_solver_from_singlebit_supervision"
        },
        {
            "paper_title": "Optnet: Differentiable optimization as a layer in neural networks",
            "rating": 2,
            "sanitized_title": "optnet_differentiable_optimization_as_a_layer_in_neural_networks"
        },
        {
            "paper_title": "Learning explanatory rules from noisy data",
            "rating": 1,
            "sanitized_title": "learning_explanatory_rules_from_noisy_data"
        },
        {
            "paper_title": "Deepproblog: Neural probabilistic logic programming",
            "rating": 1,
            "sanitized_title": "deepproblog_neural_probabilistic_logic_programming"
        }
    ],
    "cost": 0.014992249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver</p>
<p>Po-Wei Wang 
School of Computer Science
Carnegie Mellon University
PittsburghPennsylvaniaUSA</p>
<p>Priya L Donti 
School of Computer Science
Carnegie Mellon University
PittsburghPennsylvaniaUSA</p>
<p>Department of Engineering &amp; Public Policy
Carnegie Mellon University
Pittsburgh, Pennsyl-vaniaUSA</p>
<p>Bryan Wilder 
Department of Computer Science
University of Southern California
Los AngelesCaliforniaUSA</p>
<p>Zico Kolter 
School of Computer Science
Carnegie Mellon University
PittsburghPennsylvaniaUSA</p>
<p>Bosch Center for Artificial Intelligence
PittsburghPennsylvaniaUSA</p>
<p>SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver
Cor-respondence to: Po-Wei Wang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#112;&#111;&#119;&#101;&#105;&#119;&#64;&#99;&#115;&#46;&#99;&#109;&#117;&#46;&#101;&#100;&#117;">&#112;&#111;&#119;&#101;&#105;&#119;&#64;&#99;&#115;&#46;&#99;&#109;&#117;&#46;&#101;&#100;&#117;</a>, Priya Donti <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#112;&#100;&#111;&#110;&#116;&#105;&#64;&#99;&#109;&#117;&#46;&#101;&#100;&#117;">&#112;&#100;&#111;&#110;&#116;&#105;&#64;&#99;&#109;&#117;&#46;&#101;&#100;&#117;</a>, Bryan Wilder <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#98;&#119;&#105;&#108;&#100;&#101;&#114;&#64;&#117;&#115;&#99;&#46;&#101;&#100;&#117;">&#98;&#119;&#105;&#108;&#100;&#101;&#114;&#64;&#117;&#115;&#99;&#46;&#101;&#100;&#117;</a>, Zico Kolter <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#122;&#107;&#111;&#108;&#116;&#101;&#114;&#64;&#99;&#115;&#46;&#99;&#109;&#117;&#46;&#101;&#100;&#117;">&#122;&#107;&#111;&#108;&#116;&#101;&#114;&#64;&#99;&#115;&#46;&#99;&#109;&#117;&#46;&#101;&#100;&#117;</a>. Proceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).
Integrating logical reasoning within deep learning architectures has been a major goal of modern AI systems. In this paper, we propose a new direction toward this goal by introducing a differentiable (smoothed) maximum satisfiability (MAXSAT) solver that can be integrated into the loop of larger deep learning systems. Our (approximate) solver is based upon a fast coordinate descent approach to solving the semidefinite program (SDP) associated with the MAXSAT problem. We show how to analytically differentiate through the solution to this SDP and efficiently solve the associated backward pass. We demonstrate that by integrating this solver into end-to-end learning systems, we can learn the logical structure of challenging problems in a minimally supervised fashion. In particular, we show that we can learn the parity function using single-bit supervision (a traditionally hard task for deep networks) and learn how to play 9 × 9 Sudoku solely from examples. We also solve a "visual Sudoku" problem that maps images of Sudoku puzzles to their associated logical solutions by combining our MAXSAT solver with a traditional convolutional architecture. Our approach thus shows promise in integrating logical structures within deep learning.</p>
<p>Introduction</p>
<p>Although modern deep learning has produced groundbreaking improvements in a variety of domains, state-of-the-art methods still struggle to capture "hard" and "global" constraints arising from discrete logical relationships. Motivated by this deficiency, there has been a great deal of recent interest in integrating logical or symbolic reasoning into neural network architectures (Palm et al., 2017;Yang et al., 2017;Cingillioglu &amp; Russo, 2018;Evans &amp; Grefenstette, 2018). However, with few exceptions, previous work primarily focuses on integrating preexisting relationships into a larger differentiable system via tunable continuous parameters, not on discovering the discrete relationships that produce a set of observations in a truly end-to-end fashion. As an illustrative example, consider the popular logic-based puzzle game Sudoku, in which a player must fill in a 9 × 9 partially-filled grid of numbers to satisfy specific constraints. If the rules of Sudoku (i.e. the relationships between problem variables) are not given, then it may be desirable to jointly learn the rules of the game and learn how to solve Sudoku puzzles in an end-to-end manner.</p>
<p>We consider the problem of learning logical structure specifically as expressed by satisfiability problems -concretely, problems that are well-modeled as instances of SAT or MAXSAT (the optimization analogue of SAT). This is a rich class of domains encompassing much of symbolic AI, which has traditionally been difficult to incorporate into neural network architectures since neural networks rely on continuous and differentiable parameterizations. Our key contribution is to develop and derive a differentiable smoothed MAXSAT solver that can be embedded within more complex deep architectures, and show that this solver enables effective end-to-end learning of logical relationships from examples (without hard-coding of these relationships). More specifically, we build upon recent work in fast block coordinate descent methods for solving SDPs (Wang et al., 2017) to build a differentiable solver for the smoothed SDP relaxation of MAXSAT. We provide an efficient mechanism to differentiate through the optimal solution of this SDP by using a similar block coordinate descent solver as used in the forward pass. Our module is amenable to GPU acceleration, greatly improving training scalability. arXiv:1905.12149v1 [cs.LG] 29 May 2019</p>
<p>Using this framework, we are able to solve several problems that, despite their simplicity, prove essentially impossible for traditional deep learning methods and existing logical learning methods to reliably learn without any prior knowledge. In particular, we show that we can learn the parity function, known to be challenging for deep classifiers (Shalev-Shwartz et al., 2017), with only single bit supervision. We also show that we can learn to play 9 × 9 Sudoku, a problem that is challenging for modern neural network architectures (Palm et al., 2017). We demonstrate that our module quickly recovers the constraints that describe a feasible Sudoku solution, learning to correctly solve 98.3% of puzzles at test time without any hand-coded knowledge of the problem structure. Finally, we show that we can embed this differentiable solver into larger architectures, solving a "visual Sudoku" problem where the input is an image of a Sudoku puzzle rather than a binary representation. We show that, in a fully end-to-end setting, our method is able to integrate classical convolutional networks (for digit recognition) with the differentiable MAXSAT solver (to learn the logical portion). Taken together, this presents a substantial advance toward a major goal of modern AI: integrating logical reasoning into deep learning architectures.</p>
<p>Related work</p>
<p>Recently, the deep learning community has given increasing attention to the concept of embedding complex, "nontraditional" layers within deep networks in order to train systems end-to-end. Major examples have included logical reasoning modules and optimization layers. Our work combines research in these two areas by exploiting optimizationbased relaxations of logical reasoning structures, namely an SDP relaxation of MAXSAT. We explore each of these relevant areas of research in more detail below.</p>
<p>Logical reasoning in deep networks. Our work is closely related to recent interest in integrating logical reasoning into deep learning architectures (Garcez et al., 2015). Most previous systems have focused on creating differentiable modules from an existing set of known relationships, so that a deep network can learn the parameters of these relationships (Dai et al., 2018;Manhaeve et al., 2018;Sourek et al., 2018;Xu et al., 2018;Hu et al., 2016;Yang et al., 2017;Selsam et al., 2018). For example, Palm et al. (2017) introduce a network that carries out relational reasoning using hand-coded information about which variables are allowed to interact, and test this network on 9 × 9 Sudoku. Similarly, Evans &amp; Grefenstette (2018) integrate inductive logic programming into neural networks by constructing differentiable SAT-based representations for specific "rule templates." While these networks are seeded with prior information about the relationships between variables, our approach learns these relationships and their associated pa-rameters end-to-end. While other recent work has also tried to jointly learn rules and parameters, the problem classes captured by these architectures have been limited. For instance, Cingillioglu &amp; Russo (2018) train a neural network to apply a specific class of logic programs, namely the binary classification problem of whether a given set of propositions entails a specific conclusion. While this approach does not rely on prior hand-coded structure, our method applies to a broader class of domains, encompassing any problem reducible to MAXSAT.</p>
<p>Differentiable optimization layers. Our work also fits within a line of research leveraging optimization as a layer in neural networks. For instance, previous work has introduced differentiable modules for quadratic programs Donti et al., 2017), submodular optimization problems (Djolonga &amp; Krause, 2017;Tschiatschek et al., 2018;Wilder et al., 2018), and equilibrium computation in zero-sum games (Ling et al., 2018). To our knowledge, ours is the first work to use differentiable SDP relaxations to capture relationships between discrete variables.</p>
<p>MAXSAT SDP relaxations. We build on a long line of research exploring SDP relaxations as a tool for solving MAXSAT and related problems. Classical work shows that such relaxations produce strong approximation guarantees for MAXCUT and MAX-2SAT (Goemans &amp; Williamson, 1995), and are empirically tighter than standard linear programming relaxations (Gomes et al., 2006). More recent work, e.g. Wang et al. (2017); Wang &amp; Kolter (2019), has developed low-rank SDP solvers for general MAXSAT problems. We extend the work of Wang et al. (2017) to create a differentiable optimization-based MAXSAT solver that can be employed in the loop of deep learning.</p>
<p>A differentiable satisfiability solver</p>
<p>The MAXSAT problem is the optimization analogue of the well-known satisfiability (SAT) problem, in which the goal is to maximize the number of clauses satisfied. We present a differentiable, smoothed approximate MAXSAT solver that can be integrated into modern deep network architectures. This solver uses a fast coordinate descent approach to solving an SDP relaxation of MAXSAT. We describe our MAXSAT SDP relaxation as well as the forward pass of our MAXSAT deep network layer (which employs this relaxation). We then show how to analytically differentiate through the MAXSAT SDP and efficiently solve the associated backward pass.</p>
<p>Solving an SDP formulation of satisfiability</p>
<p>Consider a MAXSAT instance with n variables and m clauses. Letṽ ∈ {−1, 1} n denote binary assignments of the problem variables, whereṽ i is the truth value of</p>
<p>SDP relaxation (weights )</p>
<h1>∈ ℝ &amp; for ∈ ( ∈ ℝ &amp; for ∈ # ∈ 0, 1 for ∈ ( ∈ 0, 1 for ∈ Inputs (discrete or probabilistic) relax round</h1>
<p>Outputs (discrete or probabilistic)</p>
<p>MAXSAT Layer</p>
<p>Relaxed inputs</p>
<p>Relaxed outputs Figure 1. The forward pass of our MAXSAT layer. The layer takes as input the discrete or probabilistic assignments of known MAXSAT variables, and outputs guesses for the assignments of unknown variables via a MAXSAT SDP relaxation with weights S.</p>
<p>variable i ∈ {1, . . . , n}, and defines i ∈ {−1, 0, 1} m for i ∈ {1, . . . , n}, wheres ij denotes the sign ofṽ i in clause j ∈ {1, . . . , m}. We then write the MAXSAT problem as
maximizẽ v∈{−1,1} n m j=1 n i=1 1{s ijṽi &gt; 0}.(1)
As derived in Goemans &amp; Williamson (1995); Wang &amp; Kolter (2019), to form a semidefinite relaxation of (1), we first relax the discrete variablesṽ i into associated continuous variables v i ∈ R k , v i = 1 with respect to some "truth direction" v ∈ R k , v = 1. Specifically, we relate the continuous v i to the discreteṽ i probabilistically via P (ṽ i = 1) = cos −1 (−v T i v ) /π based on randomized rounding (Goemans &amp; Williamson (1995); see Section 3.2.4). We additionally define a coefficient vectors = {−1} m associated with v . Our SDP relaxation of MAXSAT is then
minimize V ∈R k×(n+1) S T S, V T V , subject to v i = 1, i = , 1, . . . , n (2) where V ≡ v v 1 . . . v n ∈ R k×(n+1) , and S ≡ s s 1 . . .s n diag( 1 / √ 4|sj |) ∈ R m×(n+1)
. We note that this problem is a low-rank (but non-convex) formulation of MIN-UNSAT, which is equivalent to MAXSAT. This formulation can be rewritten as an SDP, and has been shown to recover the optimal SDP solution given k &gt; √ 2n (Barvinok, 1995;Pataki, 1998).</p>
<p>Despite its non-convexity, problem (2) can then be solved optimally via coordinate descent for all i = , 1, . . . , n. In particular, the objective terms that depend on v i are given by v T i n j=0 s T i s j v j , where s i is the ith column vector of S. Minimizing this quantity over v i subject to the constraint that v i = 1 yields the coordinate descent update
v i = −g i / g i , where g i = V S T s i − s i 2 v i .(3)
These updates provably converge to the globally optimal fixed point of the SDP (2) (Wang et al., 2017). A more detailed derivation of this update can be found in Appendix A.</p>
<p>Algorithm 1 SATNet Layer The outputs Z O are generated from inputs Z I via the SDP (2), and the weights of our layer correspond to the SDP's low-rank coefficient matrix S. This forward pass procedure is pictured in Figure 1. We describe the steps of layer initialization and the forward pass in Algorithm 1, and in more detail below.</p>
<p>LAYER INITIALIZATION</p>
<p>When initializing SATNet, the user must specify a maximum number of clauses m that this layer can represent. It is often desirable to set m to be low; in particular, low-rank structure can prevent overfitting and thus improve generalization.</p>
<p>Given this low-rank structure, a user may wish to somewhat increase the layer's representational ability via auxiliary variables. The high-level intuition here follows from the conjunctive normal form (CNF) representation of boolean satisfaction problems; adding additional variables to a problem can dramatically reduce the number of CNF clauses needed to describe that problem, as these variables play a role akin to register memory that is useful for inference.</p>
<p>Finally, we set k = √ 2n + 1, where here n captures the number of actual problem variables in addition to auxiliary variables. This is the minimum value of k required for our MAXSAT relaxation (2) to recover the optimal solution of its associated SDP (Barvinok, 1995;Pataki, 1998).</p>
<p>STEP 1: RELAXING LAYER INPUTS</p>
<p>Our layer first relaxes its inputs Z I into continuous vectors for use in the SDP formulation (2). That is, we relax each layer input z ι , ι ∈ I to an associated random unit vector
v ι ∈ R k so that v T ι v = − cos(πz ι ).(4)
(This equation is derived from the probabilistic relationship described in Section 3.1 between discrete variables and their continuous relaxations.) Constraint (4) can be satisfied by
v ι = − cos(πz ι )v + sin(πz ι )(I k − v v T )v rand ι , (5) where v rand ι
is a random unit vector. For simplicity, we use the notation V I ∈ R k×|I| (i.e. the I-indexed column subset of V ) to collectively refer to all relaxed layer inputs derived via Equation (5) are not computed for variables whose assignments are given as input to the layer.</p>
<p>Our coordinate descent algorithm for the forward pass is detailed in Algorithm 2. This algorithm maintains the term Ω = V S T needed to compute g o , and then modifies it via a rank-one update during each inner iteration. Accordingly, the per-iteration runtime is O(nmk) (and in practice, only a small number of iterations is required for convergence).</p>
<p>Algorithm 2 Forward pass coordinate descent (3) 7: (3) 8:
1: input V I // inputs for known variables 2: init v o with random vector v rand o , ∀o ∈ O. 3: compute Ω = V S T 4: while not converged do 5: for o ∈ O do // for all output variables 6: compute g o = Ωs o − s o 2 v o as incompute v o = −g o / g o as in
update
Ω = Ω + (v o − v prev o )s T o 9: output V O // final guess for output cols of V 3.2.4. STEP 3: GENERATING DISCRETE OR PROBABILISTIC OUTPUTS
Given the relaxed outputs V O from coordinate descent, our layer converts these outputs to discrete or probabilistic variable assignments Z O via either thresholding or randomized rounding (which we describe here).</p>
<p>The main idea of randomized rounding is that for every v o , o ∈ O, we can take a random hyperplane r from the unit sphere and assigñ
v o = 1 if sign(v T o r) = sign(v T r) −1 otherwise , o ∈ O,(6)
whereṽ o is the boolean output for v o . Intuitively, this scheme setsṽ o to "true" if and only if v o and the truth vector v are on the same side of the random hyperplane r. Given the correct weights S, this randomized rounding procedure assures an optimal expected approximation ratio for certain NP-hard problems (Goemans &amp; Williamson, 1995).</p>
<p>During training, we do not explicitly perform randomized rounding. We instead note that the probability that v o and v are on the same side of any given r is
P (ṽ o ) = cos −1 (−v T o v )/π,(7)
and thus set z o = P (ṽ o ) to equal this probability.</p>
<p>During testing, we can either output probabilistic outputs in the same fashion, or output discrete assignments via thresholding or randomized rounding. If using randomized rounding, we round multiple times, and then set z o to be the boolean solution maximizing the MAXSAT objective in Equation (1). Prior work has observed that such repeated rounding improves approximation ratios in practice, especially for MAXSAT problems (Wang &amp; Kolter, 2019).</p>
<p>Computing the backward pass</p>
<p>We now derive backpropagation updates through our SAT-Net layer to enable its integration into a neural network. That is, given the gradients ∂ /∂Z O of the network loss with respect to the layer outputs, we must compute the gradients ∂ /∂Z I with respect to layer inputs and ∂ /∂S with respect to layer weights. As it would be inefficient in terms of time and memory to explicitly unroll the forward-pass computations and store intermediate Jacobians, we instead derive analytical expressions to compute the desired gradients directly, employing an efficient coordinate descent algorithm. The procedure for computing these gradients is summarized in Algorithm 1 and derived below.</p>
<p>FROM PROBABILISTIC OUTPUTS TO THEIR</p>
<p>CONTINUOUS RELAXATIONS</p>
<p>Given ∂ /∂Z O (with respect to the layer outputs), we first derive an expression for ∂ /∂V O (with respect to the output relaxations) by pushing gradients through the probability assignment mechanism described in Section 3.2.4. That is,
for each o ∈ O, ∂ ∂v o = ∂ ∂z o ∂z o ∂v o = ∂ ∂z o 1 π sin(πz o ) v ,(8)
where we obtain ∂zo /∂vo by differentiating through Equation (7) (or, more readily, by implicitly differentiating through its rearrangement cos(
πz o ) = −v T v o ).</p>
<p>BACKPROPAGATION THROUGH THE SDP</p>
<p>Given the analytical form for ∂ /∂V O (with respect to the output relaxations), we next seek to derive ∂ /∂V I (with respect to the input relaxations) and ∂ /∂S (with respect to the layer weights) by pushing gradients through our SDP solution procedure (Section 3.2.3). We describe the analytical form for the resultant gradients in Theorem 1.
Theorem 1. Define P o ≡ I k − v o v T o for each o ∈ O.
Then, define U ∈ R k×n , where the columns U I = 0 and the columns U O are given by
vec(U O ) = (P ((C + D) ⊗ I k )P ) † vec ∂ ∂V O , (9) where P ≡ diag(P o ), where C ≡ S T O S O − diag( s o 2 ), and where D ≡ diag( g o ).
Then, the gradient of the network loss with respect to the relaxed layer inputs is
∂ ∂V I = − o∈O u o s T o S I ,(10)
where S I is the I-indexed column subset of S, and the gradient with respect to the layer weights is
∂ ∂S = − o∈O u o s T o T V − (SV T )U.(11)
We defer the derivation of Theorem 1 to Appendix B. Although this derivation is somewhat involved, the concept 
compute dg o = Ψs o − s o 2 u o − ∂ /∂v o . 7: compute u o = −P o dg o / g o . 8: update Ψ = Ψ + (u o − u prev o )s T o 9: output U O
at a high level is quite simple: we differentiate the solution of the SDP problem (Section 3.1) with respect to the problem's parameters and input, which requires computing the (relatively large) matrix-vector solve given in Equation (9).</p>
<p>To solve Equation (9), we use a coordinate descent approach that closely mirrors the coordinate descent procedure employed in the forward pass, and which has similar fast convergence properties. This procedure, described in Algorithm 3, enables us to compute the desired gradients without needing to maintain intermediate Jacobians explicitly. Mirroring the forward pass, we use rank-one updates to maintain and modify the term Ψ = U S T needed to compute dg o , which again enables our algorithm to run in O(nmk) time.</p>
<p>We defer the derivation of Algorithm 3 to Appendix D.</p>
<p>FROM RELAXED TO ORIGINAL INPUTS</p>
<p>As a final step, we must use the gradient ∂ /∂V I (with respect to the input relaxations) to derive the gradient ∂ /∂Z I (with respect to the actual inputs) by pushing gradients through the input relaxation procedure described in Section 3.2.2. For each ι ∈ I, we see that
∂ ∂z ι = ∂ ∂z ι + ∂ ∂v ι T ∂v ι ∂z ι = ∂ ∂z ι − ∂v ι ∂z ι T o∈O u o s T o s ι(12)
where
∂v ι ∂z ι = π sin(πz ι )v + cos(πz ι )(I k − v v T )v rand ι ,(13)
and where ∂ /∂z ι captures any direct dependence of on z ι (as opposed to dependence through v ι ). Here, the expression for ∂ /∂vι comes from Equation (10), and we obtain ∂vι /∂zι by differentiating Equation (5).</p>
<p>An efficient GPU implementation</p>
<p>The coordinate descent updates in Algorithms 2 and 3 dominate the computational costs of the forward and backward passes, respectively. We thus present an efficient, parallel GPU implementation of these algorithms to speed up training and inference. During the inner loop of coordinate descent, our implementation parallelizes the computation of all g o (dg o ) terms by parallelizing the computation of Ω (Ψ), as well as of all rank-one updates of Ω (Ψ). This underscores the benefit of using a low-rank SDP formulation in our MAXSAT layer, as traditional full-rank coordinate descent cannot be efficiently parallelized. We find in our preliminary benchmarks that our GPU CUDA-C implementation is up to 18 − 30x faster than the corresponding OpenMP implementation run on Xeon CPUs. Source code for our implementation is available at https://github.com/locuslab/SATNet.</p>
<p>Experiments</p>
<p>We test our MAXSAT layer approach in three domains that are traditionally difficult for neural networks: learning the parity function with single-bit supervision, learning 9 × 9 Sudoku solely from examples, and solving a "visual Sudoku" problem that generates the logical Sudoku solution given an input image of a Sudoku puzzle. We find that in all cases, we are able to perform substantially better on these tasks than previous deep learning-based approaches.</p>
<p>Learning parity (chained XOR)</p>
<p>This experiment tests SATNet's ability to differentiate through many successive SAT problems by learning to compute the parity function. The parity of a bit string is defined as one if there is an odd number of ones in the sequence and zero otherwise. The task is to map input sequences to their parity, given a dataset of example sequence/parity pairs. Learning parity functions from such single-bit supervision is known to pose difficulties for conventional deep learning approaches (Shalev-Shwartz et al., 2017). However, parity is simply a logic function -namely, a sequence of XOR operations applied successively to the input sequence.</p>
<p>Hence, for a sequence of length L, we construct our model to contain a sequence of L − 1 SATNet layers with tied weights (similar to a recurrent network). The first layer receives the first two binary values as input, and layer d receives value d along with the rounded output of layer d − 1. If each layer learns to compute the XOR function, the combined system will correctly compute parity. However, this requires the model to coordinate a long series of SAT problems without any intermediate supervision. &amp; Ba, 2015) with a learning rate of 10 −1 . We compare to an LSTM sequence classifier, which uses 100 hidden units and a learning rate of 10 −3 (we tried varying the architecture and learning rate but did not observe any improvement).</p>
<p>In each case, our model quickly learns the target function, with error on the held-out set converging to zero within 20 epochs. In contrast, the LSTM is unable to learn an appropriate representation, with only minor improvement over the course of 100 training epochs; across both input lengths, it achieves a testing error rate of at best 0.476 (where a random guess achieves value 0.5).</p>
<p>Sudoku (original and permuted)</p>
<p>In this experiment, we test SATNet's ability to infer and recover constraints simply from bit supervision (i.e. without any hard-coded specification of how bits are related). We demonstrate this property via Sudoku. In Sudoku, given a (typically) 9 × 9 partially-filled grid of numbers, a player must fill in the remaining empty grid cells such that each row, each column, and each of nine 3 × 3 subgrids contains exactly one of each number from 1 through 9. While this constraint satisfaction problem is computationally easy to solve once the rules of the game are specified, actually learning the rules of the game, i.e. the hard constraints of the puzzle, has proved challenging for traditional neural network architectures. In particular, Sudoku problems are often solved computationally via tree search, and while tree search cannot be easily performed by neural networks, it is easily expressible using SAT and MAXSAT problems.</p>
<p>We construct a SATNet model for this task that takes as input a logical (bit) representation of the initial Sudoku board along with a mask representing which bits must be learned (i.e. all bits in empty Sudoku cells). This input is vectorized, which means that our SATNet model cannot exploit the locality structure of the input Sudoku grid when learning to solve puzzles. Given this input, the SATNet layer then outputs a bit representation of the Sudoku board with guesses for the unknown bits. Our model architecture consists of a single SATNet layer with 300 auxiliary variables and low rank structure m = 600, and we train it to minimize a digitwise negative log likelihood objective (optimized via Adam with a 2 × 10 −3 learning rate). We compare our model to a convolutional neural network baseline modeled on that of Park (2016), which interprets the bit inputs as 9 input image channels (one for each square in the board) and uses a sequence of 10 convolutional layers (each with 512 3×3 filters) to output the solution. The ConvNet makes explicit use of locality in the input representation since it treats the nine cells within each square as a single image. We also compare to a version of the ConvNet which receives a binary mask indicating which bits need to be learned (ConvNetMask). The mask is input as a set of additional image channels in the same format as the board. We trained both architectures using mean squared error (MSE) loss (which gave better results than negative log likelihood for this architecture). The loss was optimized using Adam (learning rate 10 −4 ). We additionally tried to train an Opt-Net   . On the other hand, the ConvNet baseline does poorly. It learns to correctly solve 72.6% of puzzles in the training set but fails altogether to generalize: accuracy on the held-out set reaches at most 0.04%. The ConvNetMask baseline, which receives a binary mask denoting which entries must be completed, performs only somewhat better, correctly solving 15.1% of puzzles in the held-out set. We note that our test accuracy is qualitatively similar to the results obtained in Palm et al. (2017), but that our network is able to learn the structure of Sudoku without explicitly encoding the relationships between variables.</p>
<p>To underscore that our architecture truly learns the rules of the game, as opposed to overfitting to locality or other structure in the inputs, we test our SATNet architecture on permuted Sudoku boards, i.e. boards for which we apply a fixed permutation of the underlying bit representation (and adjust the corresponding input masks and labels accordingly). This removes any locality structure, and the resulting Sudoku boards do not have clear visual analogues that can be solved by humans. However, the relationships between bits are unchanged (modulo the permutation) and should therefore be discoverable by architectures that can truly learn the underlying logical structure. Table 1 shows results for this problem in comparison to the convolutional neural network baselines. Our architecture is again able to learn the rules of the (permuted) game, demonstrating the same 98.3% board-wise test accuracy as in the original game. In contrast, the convolutional neural network baselines perform even more poorly than in the original game (achieving 0% test accuracy even with the binary mask as input), as there is little locality structure to exploit. Overall, these results demonstrate that SATNet can truly learn the logical relationships between discrete variables.</p>
<p>Visual Sudoku</p>
<p>In this experiment, we demonstrate that SATNet can be integrated into larger deep network architectures for end-to-end training. Specifically, we solve the visual Sudoku problem: that is, given an image representation of a Sudoku board (as opposed to a one-hot encoding or other logical representation) constructed with MNIST digits, our network must output a logical solution to the associated Sudoku problem. An example input is shown in Figure 3. This problem cannot traditionally be represented well by neural network architectures, as it requires the ability to combine multiple neural network layers without hard-coding the logical structure of the problem into intermediate logical layers.</p>
<p>Our architecture for this problem uses a convolutional neural network connected to a SATNet layer. Specifically, we apply a convolutional layer for digit classification (which uses the LeNet architecture (LeCun et al., 1998)) to each cell of the Sudoku input. Each cell-wise probabilistic output of this convolutional layer is then fed as logical input to the SATNet layer, along with an input mask (as in Section 4.2). This SATNet layer employs the same architecture and training parameters as described in the previous section. The whole model is trained end-to-end to minimize cross-entropy loss, and is optimized via Adam with learning rates 2 × 10 −3 for the SATNet layer and 10 −5 for the convolutional layer.</p>
<p>We compare our approach against a convolutional neural network which combines two sets of convolutional layers. First, the visual inputs are passed through the same convolutional layer as in our SATNet model, which outputs a probabilistic bit representation. Next, this representation is passed through the convolutional architecture that we compared to for the original Sudoku problem, which outputs a solution. We use the same training approach as above. ; additional plots are shown in Appendix F. We contextualize these results against the theoretical "best" testing accuracy of 74.7%, which accounts for the Sudoku digit classification accuracy of our specific convolutional architecture; that is, assuming boards with 36.2 out of 81 filled cells on average (as in our test set) and an MNIST model with 99.2% test accuracy (LeCun et al., 1998), we would expect a perfect Sudoku solver to output the correct solution 74.7% (= 0.992 36.2 ) of the time.</p>
<p>In 100 epochs, our model learns to correctly solve 63.2% of boards at test time, reaching 85% of this theoretical "best." Hence, our approach demonstrates strong performance in solving visual Sudoku boards end-to-end. On the other hand, the baseline convolutional networks make only minuscule improvements to the training loss over the course of 100 epochs, and fail altogether to improve out-of-sample performance. Accordingly, our SATNet architecture enables end-to-end learning of the "rules of the game" directly from pictorial inputs in a way that was not possible with previous architectures.</p>
<p>Conclusion</p>
<p>In this paper, we have presented a low-rank differentiable MAXSAT layer that can be integrated into neural network architectures. This layer employs block coordinate descent methods to efficiently compute the forward and backward passes, and is amenable to GPU acceleration. We show that our SATNet architecture can be successfully used to learn logical structures, namely the parity function and the rules of 9 × 9 Sudoku. We also show, via a visual Sudoku task, that our layer can be integrated into larger deep network architectures for end-to-end training. Our layer thus shows promise in allowing deep networks to learn logical structure without hard-coding of the relationships between variables.</p>
<p>More broadly, we believe that this work fills a notable gap in the regime spanning deep learning and logical reasoning. While many "differentiable logical reasoning" systems have been proposed, most of them still require fairly handspecified logical rules and groundings, and thus are somewhat limited in their ability to operate in a truly end-to-end fashion. Our hope is that by wrapping a powerful yet generic primitive such as MAXSAT solving within a differentiable framework, our solver can enable "implicit" logical reasoning to occur where needed within larger frameworks, even if the precise structure of the domain is unknown and must be learned from data. In other words, we believe that SATNet provides a step towards integrating symbolic reasoning and deep learning, a long-standing goal in artificial intelligence. </p>
<p>A. Derivation of the forward pass coordinate descent update</p>
<p>Our MAXSAT SDP relaxation (described in Section 3.1) is given by
minimize V ∈R k×(n+1) S T S, V T V , subject to v i = 1, i = 0, . . . , n, (A.1)
where S ∈ R m×(n+1) and v i is the ith column vector of V .</p>
<p>We rewrite the objective of (A.1) as S T S, V T V ≡ tr((S T S) T (V T V )) = tr(V T V S T S) by noting that S T S is symmetric and by cycling matrices within the trace. We then observe that the objective terms that depend on any given v i are given by
v T i n j=0 s T j s i v j = v T i n j=0 (j =i) s T j s i v j + v T i s T i s i v i , (A.2)
where s i is the ith column vector of S. Observe v T i v i in the last term cancels to 1, and the remaining coefficient
g i ≡ n j=0 (j =i) s T j s i v j = V S T s i − s i 2 v i (A.3)
is constant with respect to v i . Thus, (A.2) can be simply rewritten as v </p>
<p>B. Details on backpropagation through the MAXSAT SDP</p>
<p>Given the result ∂ /∂V O , we next seek to compute ∂ /∂V I and ∂ /∂S by pushing gradients through the SDP solution procedure described in Section 3.1. We do this by taking the total differential through our coordinate descent updates (3) for each output o ∈ O at the optimal fixed-point solution to which these updates converge.</p>
<p>Computing the total differential. Computing the total differential of the updates (3) and rearranging, we see that for every o ∈ O, 
go I k − so 2 Po dvo + Po j∈O s T o sjdvj = −Poξo, (B.1) where ξo ≡ j∈I s T o sjdvj + V dS T so + V S T dso − 2ds T o sovo ,(B.diag( g o ) ⊗ I k + P C ⊗ I k vec(dV O ) = −P vec(ξ o ) ⇒ vec(dV O ) = − P ( diag( g o ) + C ⊗ I k )P † vec(ξ o ), (B.3) where C = S T O S O − diag( s o 2 ), P = diag(P o )
, and the second step follows from the lemma presented in Appendix C.</p>
<p>We then see that by the chain rule, the gradients ∂ /∂V I and ∂ /∂S are given by the left matrix-vector product
∂ ∂ vec(VO) T vec(dVO) = − ∂ ∂ vec(VO) T P ( diag( go ) + C ⊗ I k )P † vec(ξo) (B.4)
where the second equality comes from plugging in the result of (B.3). Now, define U ∈ R k×n , where the columns U I = 0 and the columns U O are given by
vec(U O ) = P ( diag( g o )+C ⊗I k )P † vec ∂ ∂ vec(V O )
.</p>
<p>(B.5) Then, we see that (B.4) can be written as
∂ ∂ vec(V O ) T vec(dV O ) = − vec(U O ) T vec(ξ o ),
(B.6) which is the implicit linear form for our gradients.</p>
<p>Computing desired gradients from implicit linear form. Once we have obtained U O (via coordinate descent), we can explicitly compute the desired gradients ∂ /∂V I and ∂ /∂S from the implicit form (B.6). For instance, to compute the gradient ∂ /∂vι for some ι ∈ I, we would set dv ι = 1 and all other gradients to zero in Equation (B.6) (where these gradients are captured within the terms ξ o ).</p>
<p>Explicitly, we compute each ∂ /∂vιj by setting dv ιj = 1 and all other gradients to zero, i.e. Similarly, we compute each ∂ /∂Si,j by setting dS i,j = 1 and all other gradients to zero, i.e.
∂ ∂S i,j = − o∈O u T o ξ o = − o∈O u T o v i s oj − u T i (V S T ) j + u T i (s ij P i v i ) = −v T i ( o∈O u o s oj ) − u T i (V S T ) j .
(B.8) In matrix form, these gradients are
∂ ∂V I = − o∈O u o s T o S I , (B.9) ∂ ∂S = − o∈O u o s T o T V − (SV T )U, (B.10)
where u i is the ith column of U , and where S I denotes the I-indexed column subset of S.</p>
<p>C. Proof of pseudoinverse computations</p>
<p>We prove the following lemma, used to derive the implicit total differential for vec(dV O ). Lemma C.1. The quantity
vec(dV O ) = (P ((D + C) ⊗ I k ) P ) † vec(ξ o ) (C.1)
is the solution of the linear system Proof. Examining the equation with respect to dv i gives
(D ⊗ I k + P C ⊗ I k ) vec(dV O ) = P vec(ξ o ), (C.2) where P = diag(I k − v o v T o ), C = S T O S O − diag( s o 2 ), D = diag( g i ),g i dv i + P i   j c ij dv j − ξ j   = 0, (C.3)
which implies that for all i, dv i = P i y i for some y i . Substituting y i into the equality gives
(D ⊗ I k + P C ⊗ I k )P vec(y i ) (C.4) =P ((D + C) ⊗ I k )P vec(y i ) = P vec(ξ o ). (C.5)
Note that the last equation comes form D ⊗ I k P = D ⊗ I k P P = P (D ⊗ I k )P due to the block diagonal structure of the projection P . Thus, by the properties of projectors and the pseudoinverse, Note that the first equation comes from the idempotence property of P (that is, P P = P ). Substituting vec(dV O ) = P vec(Y ) back gives the solution of dV O .</p>
<p>D. Derivation of the backward pass coordinate descent algorithm</p>
<p>Consider solving for U O as mentioned in Equation (B.5):
P ( diag( g o )+C ⊗I k )P vec(U O ) = vec ∂ ∂ vec(V O ) , where C = S T O S O − diag( s o 2
). The linear system can be computed using block coordinate descent. Specifically, observe this linear system with respect to only the u o variable. Since we start from U O = 0, we can assume that P vec(U o ) = vec(U o ). This yields
g o P o u o + P o U O S T O s o − s o 2 u o = P o ∂ ∂v o . (D.1) Let Ψ = (U O )S T O . Then we have g o P o u o = −P o (Ψs o − s o 2 u o − ∂ /∂v o ). (D.2)
Define −dg i to be the terms contained in parentheses in the right-hand side of the above equation. Note that dg i does not depend on the variable u o . Thus, we have the closed-form feasible solution
u o = −P o dg o / g o . (D.3)
After updating u o , we can maintain the term Ψ by replacing the old u prev o with the new u o . This yields the rank 1 update
Ψ := Ψ + (u o − u prev o )s T o . (D.4)
The above procedure is summarized in Algorithm 3. Further, we can verify that the assumption P vec(U O ) = vec(U O ) still holds after each update by the projection P o .</p>
<p>E. Results for the 4 × 4 Sudoku problem</p>
<p>We compare the performance of our SATNet architecture on a 4 × 4 reduced version of the Sudoku puzzle against OptNet ) and a convolutional neural network architecture. These results (over 9K training and 1K testing examples) are shown in Figure E.1. We note that our architecture converges quickly -in just two epochs -to 100% board-wise test accuracy.</p>
<p>OptNet takes slightly longer to converge to similar performance, in terms of both time and epochs. In particular, we see that OptNet takes 3-4 epochs to converge (as opposed to 1 epoch for SATNet). Further, in our preliminary benchmarks, OptNet required 12 minutes to run 20 epochs on a GTX 1080 Ti GPU, whereas SATNet took only 2 minutes to run the same number of epochs. In other words, we see that SATNet requires fewer epochs to converge and takes less time per epoch than OptNet. Both our SATNet architecture and OptNet outperform the traditional convolutional neural network in this setting, as the ConvNet somewhat overfits to the training set and therefore does not generalize as well to the test set (achieving 93% accuracy). The ConvNetMask, which additionally receives a binary input mask, performs much better (99% test accuracy) but does not achieve perfect performance as in the case of OptNet and SATNet.</p>
<p>F. Convergence plots for 9 × 9 Sudoku experiments Convergence plots for our 9 × 9 Sudoku experiments (original and permuted) are shown in Figure F.1. SATNet performs nearly identically in both the original and permuted settings, generalizing well to the test set at every epoch without overfitting to the training set. The ConvNet and ConvNetMask, on the other hand, do not generalize well.</p>
<p>In the original setting, both architectures overfit to the training set, showing little-to-no improvement in generalization performance over the course of training. In the permuted setting, both ConvNet and ConvNetMask make little progress even on the training set, as they are not able to rely on spatial locality of inputs.</p>
<p>Convergence plots for the visual Sudoku experiments are shown in Figure F.2. Here, we see that SATNet generalizes well in terms of loss throughout the training process, and generalizes somewhat well in terms of whole-board accuracy. The difference in generalization performance between the logical and visual Sudoku settings can be attributed to the generalization performance of the MNIST classifier trained end-to-end with our SATNet layer. The ConvNetMask architecture overfits to the training set, and the ConvNet architecture makes little-to-no progress even on the training set.    Lower loss (mean NLL loss and mean MSE loss) and higher whole-board accuracy (% puzzles correct) are better. The theoretical "best" test accuracy plotted is for our specific choice of MNIST classifier architecture.</p>
<p>Figure 2 Figure 2 .
22shows that our model accomplishes this task for input sequences of length L = 20 and L = 40. For each sequence length, we generate a dataset of 10K random examples (9K training and 1K testing). We train our model using cross-entropy loss and the Adam optimizer (Kingma Error rate for the parity task with L = 20 (top) and L = 40 (bottom). Solid lines denote test values, while dashed lines represent training values.</p>
<p>Figure 3 .
3An example visual Sudoku image input, i.e. an image of a Sudoku board constructed with MNIST digits. Cells filled with the numbers 1-9 are fixed, and zeros represent unknowns.</p>
<p>T i g i + s T i s i . (A.4)Minimizing this expression over v i with respect to the constraint v i = 1 yields the block coordinate descent update v i = −g i / g i . (A.5)</p>
<p>∂ ∂v ιj = − vec(U O ) T vec(ξ o )</p>
<p>and ξ o is as defined in Equation (B.2).</p>
<p>vec(Y ) = (P ((D + C) ⊗ I k )P ) † P vec(ξ o ) (C.6) = (P ((D + C) ⊗ I k )P ) † vec(ξ o ). (C.7)</p>
<p>Figure F. 1 .
1Results for our 9 × 9 Sudoku experiments. Lower loss (mean NLL loss and mean MSE loss) and higher whole-board accuracy (% puzzles correct) are better.</p>
<p>Figure F. 2 .
2Results for our visual Sudoku experiments.</p>
<p>Algorithm 3 Backward pass coordinate descent 1: input { ∂ /∂vo | o ∈ O} // grads w.r.t. relaxed outputs 2: // Compute U O from Equation (9) 3: init U O = 0 and Ψ = (U O )S T O = 0 4: while not converged do 5:for o ∈ O do // for all output variables6: </p>
<p>Table 1. Results for 9 × 9 Sudoku experiments with 9K train/1K test examples. We compare our SATNet model against a vanilla convolutional neural network (ConvNet) as well as one that receives a binary mask indicating which bits need to be learned (ConvNetMask).Model 
Train 
Test </p>
<p>ConvNet 
72.6% 0.04% 
ConvNetMask 91.4% 15.1% 
SATNet (ours) 99.8% 98.3% </p>
<p>(a) Original Sudoku. </p>
<p>Model 
Train 
Test </p>
<p>ConvNet 
0% 
0% 
ConvNetMask 0.01% 
0% 
SATNet (ours) 99.7% 98.3% </p>
<p>(b) Permuted Sudoku. </p>
<p>Model 
Train 
Test </p>
<p>ConvNet 
0.31% 
0% 
ConvNetMask 
89% 
0.1% 
SATNet (ours) 93.6% 63.2% </p>
<p>(c) Visual Sudoku. (Note: the theoretical 
"best" test accuracy for our architecture is 
74.7%.) </p>
<p>model for comparison, but this model made little progress even after a few days of training. (We compare our method to OptNet on a simpler 4 × 4 version of the Sudoku problem in Appendix E.) Our results for the traditional 9 × 9 Sudoku problem (over 9K training examples and 1K test examples) are shown in Table 1. (Convergence plots for this experiment are shown in Appendix F.) Our model is able to learn the constraints of the Sudoku problem, achieving high accuracy early in the training process (95.0% test accuracy in 22 epochs/37 minutes on a GTX 1080 Ti GPU), and demonstrating 98.3% board-wise test accuracy after 100 training epochs (172 minutes)</p>
<p>Table 1
1summarizes our experimental results (over 9K training examples and 1K test examples)</p>
<p>Figure E.1. Results for 4 × 4 Sudoku. Lower loss (mean NLL loss and mean MSE loss) and higher whole-board accuracy (% puzzles correct) are better.0 10 20 30 40 50 60 70 
Epoch </p>
<p>10 1 
10 3 
10 5 
10 7 
10 9 
Mean NLL Loss </p>
<p>0 10 20 30 40 50 60 70 
Epoch </p>
<p>10 1 </p>
<p>10 2 </p>
<p>10 3 
Mean MSE Loss </p>
<p>0 10 20 30 40 50 60 70 
Epoch </p>
<p>0 
20 
40 
60 
80 
100 </p>
<p>% Puzzles Correct </p>
<p>SATNet (train) 
SATNet (test) 
ConvNet (train) 
ConvNet (test) 
ConvNetMask (train) 
ConvNetMask (test) 
OptNet (train) 
OptNet (test) </p>
<p>Acknowledgments
B Amos, J Z Kolter, Optnet, arXiv:1703.00443Differentiable optimization as a layer in neural networks. arXiv preprintAmos, B. and Kolter, J. Z. Optnet: Differentiable opti- mization as a layer in neural networks. arXiv preprint arXiv:1703.00443, 2017.</p>
<p>Problems of distance geometry and convex properties of quadratic maps. A I Barvinok, Discrete &amp; Computational Geometry. 132Barvinok, A. I. Problems of distance geometry and convex properties of quadratic maps. Discrete &amp; Computational Geometry, 13(2):189-202, 1995.</p>
<p>Deeplogic: End-to-end logical reasoning. N Cingillioglu, A Russo, arXiv:1805.07433arXiv preprintCingillioglu, N. and Russo, A. Deeplogic: End-to-end logi- cal reasoning. arXiv preprint arXiv:1805.07433, 2018.</p>
<p>W.-Z Dai, Q.-L Xu, Y Yu, Zhou , arXiv:1802.01173Tunneling neural perception and logic reasoning through abductive learning. arXiv preprintDai, W.-Z., Xu, Q.-L., Yu, Y., and Zhou, Z.-H. Tunneling neural perception and logic reasoning through abductive learning. arXiv preprint arXiv:1802.01173, 2018.</p>
<p>Differentiable learning of submodular models. J Djolonga, A Krause, Advances in Neural Information Processing Systems. Djolonga, J. and Krause, A. Differentiable learning of submodular models. In Advances in Neural Information Processing Systems, pp. 1013-1023, 2017.</p>
<p>Task-based endto-end model learning in stochastic optimization. P L Donti, B Amos, J Z Kolter, arXiv:1703.04529arXiv preprintDonti, P. L., Amos, B., and Kolter, J. Z. Task-based end- to-end model learning in stochastic optimization. arXiv preprint arXiv:1703.04529, 2017.</p>
<p>Learning explanatory rules from noisy data. R Evans, E Grefenstette, Journal of Artificial Intelligence Research. 61Evans, R. and Grefenstette, E. Learning explanatory rules from noisy data. Journal of Artificial Intelligence Re- search, 61:1-64, 2018.</p>
<p>Neural-symbolic learning and reasoning: contributions and challenges. A Garcez, T R Besold, L De Raedt, P Földiak, P Hitzler, T Icard, K.-U Kühnberger, L C Lamb, R Miikkulainen, D L Silver, Proceedings of the AAAI Spring Symposium on Knowledge Representation and Reasoning: Integrating Symbolic and Neural Approaches. the AAAI Spring Symposium on Knowledge Representation and Reasoning: Integrating Symbolic and Neural ApproachesStanfordGarcez, A., Besold, T. R., De Raedt, L., Földiak, P., Hitzler, P., Icard, T., Kühnberger, K.-U., Lamb, L. C., Miikku- lainen, R., and Silver, D. L. Neural-symbolic learning and reasoning: contributions and challenges. In Proceedings of the AAAI Spring Symposium on Knowledge Represen- tation and Reasoning: Integrating Symbolic and Neural Approaches, Stanford, 2015.</p>
<p>Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. M X Goemans, D P Williamson, Journal of the ACM (JACM). 426Goemans, M. X. and Williamson, D. P. Improved approx- imation algorithms for maximum cut and satisfiability problems using semidefinite programming. Journal of the ACM (JACM), 42(6):1115-1145, 1995.</p>
<p>The power of semidefinite programming relaxations for max-sat. C P Gomes, W.-J Van Hoeve, L Leahu, International Conference on Integration of Artificial Intelligence (AI) and Operations Research (OR) Techniques in Constraint Programming. SpringerGomes, C. P., van Hoeve, W.-J., and Leahu, L. The power of semidefinite programming relaxations for max-sat. In International Conference on Integration of Artificial Intel- ligence (AI) and Operations Research (OR) Techniques in Constraint Programming, pp. 104-118. Springer, 2006.</p>
<p>Harnessing deep neural networks with logic rules. Z Hu, X Ma, Z Liu, E Hovy, E Xing, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational Linguistics1Hu, Z., Ma, X., Liu, Z., Hovy, E., and Xing, E. Harnessing deep neural networks with logic rules. In Proceedings of the 54th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), volume 1, pp. 2410-2420, 2016.</p>
<p>Amethod for stochastic optimization. D P Kingma, J L Ba, Adam, International Conference on Learning Representations. Kingma, D. P. and Ba, J. L. Adam: Amethod for stochastic optimization. In International Conference on Learning Representations, 2015.</p>
<p>Gradientbased learning applied to document recognition. Proceedings of the IEEE. Y Lecun, L Bottou, Y Bengio, P Haffner, 86LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient- based learning applied to document recognition. Proceed- ings of the IEEE, 86(11):2278-2324, 1998.</p>
<p>What game are we playing? end-to-end learning in normal and extensive form games. C K Ling, F Fang, J Z Kolter, 10.24963/ijcai.2018/55Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence. the Twenty-Seventh International Joint Conference on Artificial IntelligenceLing, C. K., Fang, F., and Kolter, J. Z. What game are we playing? end-to-end learning in normal and extensive form games. In Proceedings of the Twenty-Seventh Inter- national Joint Conference on Artificial Intelligence, pp. 396-402, 2018. doi: 10.24963/ijcai.2018/55.</p>
<p>Deepproblog: Neural probabilistic logic programming. R Manhaeve, S Dumancic, A Kimmig, T Demeester, De Raedt, L , Advances in Neural Information Processing Systems. Manhaeve, R., Dumancic, S., Kimmig, A., Demeester, T., and De Raedt, L. Deepproblog: Neural probabilistic logic programming. In Advances in Neural Information Processing Systems, pp. 3749-3759, 2018.</p>
<p>R B Palm, U Paquet, O Winther, arXiv:1711.08028Recurrent relational networks. arXiv preprintPalm, R. B., Paquet, U., and Winther, O. Recurrent rela- tional networks. arXiv preprint arXiv:1711.08028, 2017.</p>
<p>Can neural networks crack sudoku?. K Park, Park, K. Can neural networks crack sudoku?, 2016. URL https://github.com/Kyubyong/sudoku.</p>
<p>On the rank of extreme matrices in semidefinite programs and the multiplicity of optimal eigenvalues. G Pataki, Mathematics of operations research. 232Pataki, G. On the rank of extreme matrices in semidefi- nite programs and the multiplicity of optimal eigenval- ues. Mathematics of operations research, 23(2):339-358, 1998.</p>
<p>Learning a sat solver from single-bit supervision. D Selsam, M Lamm, B Bunz, P Liang, L De Moura, D L Dill, arXiv:1802.03685arXiv preprintSelsam, D., Lamm, M., Bunz, B., Liang, P., de Moura, L., and Dill, D. L. Learning a sat solver from single-bit supervision. arXiv preprint arXiv:1802.03685, 2018.</p>            </div>
        </div>

    </div>
</body>
</html>
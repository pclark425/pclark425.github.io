<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7315 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7315</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7315</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-264146024</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.10461v3.pdf" target="_blank">Model Selection of Anomaly Detectors in the Absence of Labeled Validation Data</a></p>
                <p><strong>Paper Abstract:</strong> Anomaly detection is the task of identifying abnormal samples in large unlabeled datasets. While the advent of foundation models has produced powerful zero-shot anomaly detection methods, their deployment in practice is often hindered by the absence of labeled validation data -- without it, their detection performance cannot be evaluated reliably. In this work, we propose SWSA (Selection With Synthetic Anomalies): a general-purpose framework to select image-based anomaly detectors without labeled validation data. Instead of collecting labeled validation data, we generate synthetic anomalies without any training or fine-tuning, using only a small support set of normal images. Our synthetic anomalies are used to create detection tasks that compose a validation framework for model selection. In an empirical study, we evaluate SWSA with three types of synthetic anomalies and on two selection tasks: model selection of image-based anomaly detectors and prompt selection for CLIP-based anomaly detection. SWSA often selects models and prompts that match selections made with a ground-truth validation set, outperforming baseline selection strategies.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7315",
    "paper_id": "paper-264146024",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0036845,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Model Selection of Anomaly Detectors in the Absence of Labeled Validation Data
16 Sep 2024</p>
<p>Clement Fung <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#99;&#108;&#101;&#109;&#101;&#110;&#116;&#102;&#64;&#99;&#115;&#46;&#99;&#109;&#117;&#46;&#101;&#100;&#117;">&#99;&#108;&#101;&#109;&#101;&#110;&#116;&#102;&#64;&#99;&#115;&#46;&#99;&#109;&#117;&#46;&#101;&#100;&#117;</a>. 
Software and Societal Systems Department
Carnegie Mellon University
PittsburghPAUSA</p>
<p>Chen Qiu 
Bosch Center for Artificial Intelligence
PittsburghPAUSA</p>
<p>Aodong Li 
Department of Computer Science
University of California
IrvineCAUSA</p>
<p>Maja Rudolph 
Bosch Center for Artificial Intelligence
PittsburghPAUSA</p>
<p>Model Selection of Anomaly Detectors in the Absence of Labeled Validation Data
16 Sep 202456A2EAE0C654CE7B31434F1A6CF715DAarXiv:2310.10461v3[cs.LG]
Anomaly detection is the task of identifying abnormal samples in large unlabeled datasets.While the advent of foundation models has produced powerful zero-shot anomaly detection methods, their deployment in practice is often hindered by the absence of labeled validation datawithout it, their detection performance cannot be evaluated reliably.In this work, we propose SWSA (Selection With Synthetic Anomalies): a general-purpose framework to select image-based anomaly detectors without labeled validation data.Instead of collecting labeled validation data, we generate synthetic anomalies without any training or fine-tuning, using only a small support set of normal images.Our synthetic anomalies are used to create detection tasks that compose a validation framework for model selection.In an empirical study, we evaluate SWSA with three types of synthetic anomalies and on two selection tasks: model selection of image-based anomaly detectors and prompt selection for CLIP-based anomaly detection.SWSA often selects models and prompts that match selections made with a ground-truth validation set, outperforming baseline selection strategies.</p>
<p>Introduction</p>
<p>Anomaly detection, automatically identifying samples that deviate from normal behavior, is an important technique for supporting medical diagnosis (Fernando et al., 2021), safeguarding financial transactions (Ahmed et al., 2016), bolstering cybersecurity (Mirsky et al., 2018;Siadati &amp; Memon, 2017), and ensuring smooth industrial operations (Bergmann et al., 2019).While there has been significant progress in approaches for unsupervised anomaly de-In this work, we study how effective generated synthetic anomalies are for model selection of image-based anomaly detectors through our proposed framework: SWSA (Selection With Synthetic Anomalies).We compare two promising strategies for generating synthetic anomalies: (i) augmentation methods from prior work (Li et al., 2021) and (ii) a novel approach that leverages pre-trained diffusion models (Ho et al., 2020;Song et al., 2021;Jeong et al., 2023a).For a given anomaly detection task, our anomaly generation methods assume access to only a small support set of normal images and do not require any training, fine-tuning, or domain-specific techniques.We create synthetic validation datasets for SWSA by combining synthetic anomalies with normal examples.When evaluated across a variety of anomaly detection tasks, ranging from natural images to industrial defects, we find that SWSA often matches the selections made with real ground-truth validation sets and outperforms baseline selection strategies.Our work makes the following contributions:</p>
<p>• In Sec.3.1, we propose SWSA: a framework for selecting anomaly detection models with synthetic anomalies.Fig. 1 shows the outline of our approach.</p>
<p>• In Sec.3.2, we propose a practical technique for generating synthetic anomalies with a general-purpose pretrained diffusion model-without any fine-tuning or auxiliary datasets.When used in SWSA, we show that these synthetic anomalies are most effective for model selection in natural settings (i.e., birds and flowers).</p>
<p>• In Sec. 4, we empirically evaluate SWSA with a variety of anomaly-generation methods, datasets, and anomaly-detection tasks.We find that SWSA is effective in two use cases: model selection amongst candidate anomaly detectors (Sec.4.2) and prompt selection for zero-shot CLIP-based anomaly detection (Sec.4.3).</p>
<p>Related Work</p>
<p>Unsupervised anomaly detection.Recent advances in anomaly detection models include autoencoder-based methods (Chen &amp; Konukoglu, 2018;Principi et al., 2017), deep one-class classification (Ruff et al., 2018;2019), and selfsupervised learning-based methods (Bergman &amp; Hoshen, 2020;Hendrycks et al., 2019b;Qiu et al., 2021;Sohn et al., 2020;Qiu et al., 2022a;Schneider et al., 2022).While these methods do not require labeled data, their architectures and training frameworks depend on various hyper-parameters, and the choice of hyperparameter can have a strong impact on detection accuracy (Campos et al., 2016;Goldstein &amp; Uchida, 2016;Han et al., 2022).Prior work in semisupervised anomaly detection (Görnitz et al., 2013;Das et al., 2016;Trittenbach et al., 2021;Li et al., 2023a) assumes access to a training set with labeled anomalies but obtaining such data is unrealistic for many applications.</p>
<p>In outlier exposure (Hendrycks et al., 2019a), which has been extended to contaminated data settings (Qiu et al., 2022b) and meta-learning (Li et al., 2023b), further improvements in detection accuracy come from fine-tuning on an auxiliary dataset, usually Tiny-ImageNet (Chrabaszcz et al., 2017).Although these auxiliary samples provide valuable training signal, we find that they are too dissimilar from normal samples and are easily detected, making them ineffective for model selection.</p>
<p>Anomaly detection with foundation models.Foundation models are pre-trained on massive datasets to learn rich semantic image features and can be effective for anomaly detection in new vision domains with no additional training.</p>
<p>Examples include models such as vision transformers (Dosovitskiy et al., 2021) (ViT) or residual networks (He et al., 2016) (ResNet) pre-trained on the ImageNet dataset (Deng et al., 2009).</p>
<p>Vision-language models, such as CLIP (Radford et al., 2021), are another powerful class of foundation models for anomaly detection.With hand-crafted text prompts, CLIP can be employed on a new anomaly detection (Liznerski et al., 2022;Zhou et al., 2024;Li et al., 2024b) or anomaly segmentation task (Jeong et al., 2023b;Zhou et al., 2021) without training data.However, detection performance depends on the choice of prompts; a variety of prior work uses prompt learning to find the best-performing prompts for anomaly detection (Li et al., 2024a;b;Jeong et al., 2023b;Zhou et al., 2024;Esmaeilpour et al., 2022) but prompt learning also requires additional training or validation data.</p>
<p>Meta-evaluation of anomaly detection.Various prior works (Nguyen et al., 2016;Marques et al., 2015;2020) propose unsupervised model selection strategies with internal metrics computed from predicted anomaly scores on unlabeled data (Ma et al., 2023).Meta-training offers another set of approaches for unsupervised anomaly detector selection (Schubert et al., 2023;Zhao et al., 2021;2022).However, since meta-learning requires a large number of related labeled datasets, their application has been limited to tabular data, and prior work on internal metrics has been applied to tabular anomaly detection only (Nguyen et al., 2016;Marques et al., 2015;2020;Ma et al., 2023).Shoshan et al. (Shoshan et al., 2023) also explore synthetic data for model selection, but their work focuses on optimizing model training; synthetic data is used to select hyper-parameters and stopping criteria.Furthermore, their method trains a GAN to generate synthetic data, which is not applicable to our setting where data is scarce.</p>
<p>Guided image synthesis.Diffusion models (Ho et al., 2020;Song et al., 2021) have recently shown state-of-theart performance for image synthesis (Dhariwal &amp; Nichol, 2021).Although these models traditionally generate indistribution data, prior work guides generative models to generate data from new distributions.Specifically, textguided generation (Kim et al., 2022;Kwon et al., 2023;Kawar et al., 2023;Mokady et al., 2023;Tumanyan et al., 2023) with CLIP (Radford et al., 2021) guides the image generation process with text prompts to generate samples from a distribution of interest, e.g."cat with glasses".CLIP embeddings can be used to guide image synthesis in tasks such as style transfer (Karras et al., 2019;2020), classifier evaluation (Luo et al., 2023), and model diagnosis (Jain et al., 2023).However, these approaches require text labels (e.g, gender, glasses, etc.) as inputs, but we assume that the nature of anomalies is unknown.Instead, we rely on DiffStyle (Jeong et al., 2023a), which performs trainingfree, guided image generation by interpolating two images during a reverse DDIM process (Song et al., 2021).We find that interpolating between two normal samples can preserve dominant visual features (i.e., realistic textures and background) while introducing manipulations that make the generated images promising candidates for SWSA.</p>
<p>Method</p>
<p>In this section, we propose to leverage data augmentation and diffusion-based image generation techniques to generate synthetic anomalies.By using these synthetic anomalies as a synthetic validation dataset, we enable model selection in the absence of a real validation dataset.We call our framework SWSA.In Sec.3.1, we describe how synthetic anomalies are used in SWSA.We then propose our synthetic anomaly generation approaches in Sec.3.2.Fig. 1 demonstrates the overall process used in SWSA.</p>
<p>Model Selection with Synthetic Anomalies</p>
<p>The absence of labeled validation data is a major roadblock in the deployment of anomaly detection methods; however, normal data can usually be obtained.For this reason, we follow prior work (Zhao et al., 2021) and assume access to a set of normal samples we call the support set X support .In our empirical study, we show that the support set can have as few as 10 samples.We use this support set to construct a synthetic validation set and perform model selection in the following steps:</p>
<p>Step 1: Partitioning the support set.The support set X support , is randomly partitioned into seed images X seed and normal validation images X in .X seed is used for anomaly generation, and X in is held out for evaluation.</p>
<p>Step 2: Generating synthetic anomalies.The seed images are processed with either DiffStyle (Jeong et al., 2023a) or CutPaste (Li et al., 2021) to produce synthetic anomalies Xout .Details are given in Sec.3.2.</p>
<p>Step 3: Mixing the synthetic validation set.X in and Xout are combined to produce a labeled synthetic validation set,
D = {(x, 1)|x ∈ Xout } ∪ {(x, 0)|x ∈ X in },(1)
where label 1 indicates an anomaly and label 0 indicates a normal image.</p>
<p>Step 4: Evaluating detection with candidate models.We evaluate candidate models by their detection performance on the synthetic validation set D. We use AUROC, the area under the receiving operator characteristic curve, which is typically used to evaluate anomaly detection models (Emmott et al., 2015).</p>
<p>Since we assume access to only a small support set, training or fine-tuning a generative (diffusion) model is infeasible.Instead, in Sec.3.2, we propose to use DiffStyle, a trainingfree method for diffusion-based image-to-image style transfer, and adapt it to generate synthetic anomalies.We also propose to use CutPaste, a data augmentation technique that uses local modifications.Our applications of the diffusionbased method and CutPaste do not require any training and do not require additional data beyond X seed .</p>
<p>Generating Synthetic Anomalies</p>
<p>We propose two methods for generating synthetic anomalies: CutPaste and a diffusion-based method.Our two methods do not require any training or fine-tuning and only require a small seed set of images X seed .</p>
<p>CutPaste.CutPaste (Li et al., 2021) is a data augmentation method that is used for training unsupervised anomaly detection models.To modify an image with CutPaste, we randomly select a cropped region of an image and paste it onto a different region of the image.In our work, we propose a different application for CutPaste's data augmentation: after modifying benign, in-class images from the seed set X seed , the resulting images are treated as synthetic anomalies for model selection.</p>
<p>Diffusion-based generation.We use DiffStyle (Jeong et al., 2023a), diffusion-based style transfer, to generate synthetic anomalies with a pretrained DDIM.We first equally divide the seed set X seed into style images X style and content images X content .DiffStyle takes any style-content image pair {I (1) , I (2) } as input-and generates a new image with I (2) 's content and I (1) 's style.To achieve this, I (1) and I (2) are mapped into the diffusion model's latent space through the forward diffusion process to produce latent vectors x</p>
<p>(1) T and x</p>
<p>(2)</p>
<p>T .We refer to the h-space (i.e., the inner-most layer of the UNet) of x</p>
<p>(1) T and x</p>
<p>(2) T as h (1) and h (2) respectively.h-space has been shown to be a meaningful semantic space for images, enabling linearity and composition properties, and can be manipulated during a diffusion model's image generation process.We refer readers for more details related to h-space to their original works (Kwon et al., 2023;Jeong et al., 2023a).</p>
<p>Given two latent vectors h (1) and h (2) , we perform a linear interpolation: h (gen) = (1 − γ)h (1) + γh (2) where γ represents the relative strength of the content image during style transfer.We use γ = 0.7 in our experiments. 1We then perform the asymmetric reverse diffusion process using x</p>
<p>(1) T , replacing the h-space with h (gen) :
x t−1 = √ α t−1 P t (ϵ θ t (x (1) T |h (gen) )) + D t (ϵ θ t (x (1) T )). (2)
Once the reverse process is completed, the final output x 0 is saved as a synthetic anomaly.To generate the full set of synthetic anomalies Xout , we apply all possibilities of (I (1) , I (2) ) in the cross product of X style and X content .</p>
<p>We use a pre-trained ImageNet diffusion model (Deng et al., 2009;Dhariwal &amp; Nichol, 2021) to interpolate between normal images.Our method can be applied even for image domains that are far from ImageNet, such as images of industrial defects (Bergmann et al., 2019;Zou et al., 2022).</p>
<p>Fig. 2 shows examples of images generated with this approach; we find that these images have realistic backgrounds and textures, but can contain various semantic corruptions expected of anomalous images.Our anomaly generation method assumes no knowledge of the distribution of potential anomalies: the general-purpose diffusion model is not fine-tuned and only images from the seed set X seed are used as inputs.</p>
<p>Empirical Study</p>
<p>To study the efficacy of synthetic anomalies for model selection, we investigate whether SWSA selects similar models and settings as ground-truth validation sets.Our evaluation spans various vision domains, including natural and industrial images.We first describe the datasets, anomaly detection tasks, and anomaly generation details in Sec.4.1.</p>
<p>Next, we showcase two use cases of SWSA: model selection and CLIP prompt selection-we find that SWSA selects the true best-performing model in seven of eight cases (Sec.4.2) and outperforms all other strategies for CLIP prompt selection (Sec.4.3); these results are achieved without any access to the real validation data.</p>
<p>Experimental Setup</p>
<p>We present an experimental setup that can be used as a benchmark to evaluate synthetic validation data.Our benchmark uses a set of anomaly detectors and anomaly detection tasks spanning four datasets.The tasks vary by difficulty from the one-vs-rest to the more difficult one-vs-closest setting.The goal of this benchmark is to evaluate how well results on synthetic validation data correspond to results one would obtain with ground-truth validation data; we estimate the absolute detection performance, the relative ranking of anomaly detectors, and the optimal hyper-parameters (such as prompts for CLIP-based anomaly detection).</p>
<p>Datasets.Our benchmark spans four frequently-used image datasets: Caltech-UCSD Birds (CUB) (Wah et al., 2011), Oxford Flowers (Nilsback &amp; Zisserman, 2008), MVTec Anomaly Detection (MVTec-AD) (Bergmann et al., 2019), and Visual Anomaly Detection (VisA) (Zou et al., 2022).These datasets span both the natural image domain (CUB and Flowers) and the industrial anomaly domain (MVTec-AD and VisA).CUB and Flowers are multi-class datasets containing 200 bird species and 102 flower species respectively.MVTec-AD and VisA contain several (i.e., 15 in MVTec-AD, 12 in VisA) real industrial product categories; for each category, the training subset contains images of defect-free products, and the testing subset contains labeled images of both good and defective products.</p>
<p>Anomaly detection tasks.Our benchmark contains 329 anomaly detection tasks: 15 from MVTec-AD, 12 from VisA, 200 from CUB, and 102 from Flowers.For all datasets, each class or product is treated as normal for an individual anomaly detection task.In addition to the one-vsrest anomaly detection setup for multi-class datasets CUB and Flowers, we also adopt the near-anomaly-detection setup used by Mirzaei et al. (Mirzaei et al., 2023) to simulate more difficult anomaly detection tasks.Specifically, after individually selecting each class as the inlier class, we consider each out-class individually and report the class with the worst performance (i.e., one-vs-closest).For MVTec-AD and VisA, we predict if an image contains a defective product.Each product contains multiple types of defects; we consider all defect types as a single anomalous class when evaluating the one-vs-rest performance, and we consider the worst-performing defect type when reporting the one-vs-closest performance.For all tasks, images from the in-class training subset are used as the support set, and images from the relevant in-class and out-class validation subsets are used to construct the ground-truth validation set.</p>
<p>Generating synthetic anomalies.For all four datasets and all 329 anomaly detection tasks, we generate synthetic anomalies with training examples from the in-class distribution only.Both the diffusion-based method and CutPaste as used to generate the same number of images with X seed .</p>
<p>For the CUB, VisA, and MVTec-AD datasets, we sample 20 images for X seed from the training set, generating 100 synthetic anomalies with each method.For the Flowers dataset, only 10 images are included in the training set for each class, so we generate 25 synthetic anomalies with each method.Fig. 2 shows 15 examples of generated synthetic anomalies for a single CUB class (left) and MVTec-AD product (right).</p>
<p>Although prior results (Kwon et al., 2023;Kim et al., 2022) suggest that a diffusion model trained in the same domain is required to generate high-quality images, we find that using one common diffusion model can generate sufficiently effective anomalies for SWSA.We use a pre-trained diffusion model (256x256 model trained on ImageNet without class conditioning) from prior work (Dhariwal &amp; Nichol, 2021) for all datasets and anomaly detection tasks.</p>
<p>Model Selection with Synthetic Data</p>
<p>We first demonstrate the effectiveness of SWSA for model selection.Given a set of candidate models, we show that SWSA can select the true optimal model.</p>
<p>Candidate anomaly detection models.We experiment across five pre-trained ResNet models (ResNet-152, ResNet-101, ResNet-50, ResNet-34, ResNet-18) and five pre-trained Vision Transformers (ViT-H-14, ViT-L-32, ViT-L-16, ViT-B-32, ViT-B-16).For all models, we use the ImageNet pre-trained model weights from prior work (Dosovitskiy et al., 2021;He et al., 2016).</p>
<p>Deep-nearest-neighbor anomaly detection.To perform anomaly detection, we use the nearest-neighbor-based method of Bergman et al. (Bergman et al., 2020).We use the values of a candidate model's penultimate layer as the output of a feature extractor F and process the support set X support with F to establish a feature bank Z:
z s = F (x s ), ∀x s ∈ X support (3)
To perform anomaly detection on an input example d, the Euclidean distance between F (d) and its k-nearest neighbors in Z is used as an anomaly score s:
s = zs∈Z k (d) ||F (d) − z s || 2 2 (4)
where Z k (d) are the k-nearest neighbors to d in the feature bank.Bergman et al. (Bergman et al., 2020) find that small values of k are effective, and we use k = 3.</p>
<p>Evaluation setup.For each task, we compute the AUROC for each candidate model on both the synthetic validation and real validation datasets.We average the AUROC across tasks to compute the "synthetic AUROC".We repeat this process with real validation datasets, again averaging over tasks to compute the "real validation AUROC".To evaluate our model selection, we select the model with the best synthetic AUROC for a given task and report the selected model's corresponding real validation AUROC.We also compare real and synthetic AUROCs to investigate if the rankings of candidate models are similar.</p>
<p>To establish a baseline for this evaluation, we compare SWSA using our synthetic anomalies to SWSA using the Tiny-ImageNet dataset.Prior work uses Tiny-Imagenet for outlier exposure (Hendrycks et al., 2019a) In addition to evaluating SWSA for model selection, we also consider the performance of SWSA in model ranking (i.e.beyond selecting only the best-performing true model).Fig. 3 shows the real and synthetic AUROC for all 10 models in the one-vs-closest (top) and one-vs-rest (bottom) settings.</p>
<p>Ideally, the model ranking with synthetic data (along the xaxis) should match the model ranking with real data (along the y-axis).</p>
<p>For Flowers and CUB, we observe that SWSA with diffusion-based anomalies on one-vs-rest anomaly detection tasks provides the most consistent ranking.Although the number of datapoints per setting is low (n = 10), we also provide a quantitative evaluation in Tab. 2 by calculating the Kendall's Tau rank correlation coefficients between the real and synthetic AUROC. 2 To determine how many synthetic anomalies are sufficient, we vary the number of synthetic anomalies from the full set of anomalies to as few as five, keeping the anomalies with the lowest anomaly score (i.e., the most difficult anomalies).We find that one-vs-closest anomaly detection tasks (i.e., worst-case performance) are harder to estimate with SWSA, but (i) model rankings with synthetic data are more highly correlated with real rankings on one-vs-rest tasks and (ii) diffusion-based anomalies are effective for Flowers in all settings.</p>
<p>For MVTec-AD and VisA, unlike the datasets of natural images, SWSA is less effective for these datasets.The model selection results are less consistent and the correlation from model ranking is not statistically significant.These datasets contain anomalies with fine-grained industrial defects and are in general more difficult for model selection; we provide additional analysis and discussion of this phenomenon in Sec.4.4.</p>
<p>CLIP Prompt Selection with Synthetic Data</p>
<p>The performance of CLIP-based anomaly detection models depends on the choice of prompts (Jeong et al., 2023b;Liznerski et al., 2022).Prior works evaluate candidate CLIP prompts for zero-shot image anomaly detection on real labeled validation data, which we assume is not available in our setting.Thus, we next evaluate the efficacy of SWSA in selecting CLIP prompts for our 329 anomaly detection tasks.</p>
<p>Zero-shot anomaly detection with CLIP.We perform zero-shot image anomaly detection with CLIP using the method suggested in prior work: given an input image, we submit two text prompts and predict the class with the higher CLIP similarity to the image (Radford et al., 2021).We use the same backbone ("ViT-B-16-plus-240") and data transformations as in prior work (Jeong et al., 2023b).When creating prompts, we assume that the name of the inlier class is known.For CUB and Flowers, we use "some" to describe the anomaly class; for example, for the CUB dataset, if "red cardinal" is the name of the inlier class, we compare the CLIP similarities of "a photo of a red cardinal" to "a photo of some bird".For MVTec-AD and VisA, we use "with defect" for anomalous images; for example, if "transistor" is the name of a product, we compare "a photo of a transistor" to "a photo of a transistor with defect".We select amongst a pool of ten prompt templates used in prior work (Jeong et al., 2023b) for our anomaly detection tasks.A full list of candidate prompts used for each dataset can be found in Supp. A.</p>
<p>Evaluation setup.We evaluate each candidate prompt in the one-vs-closest and one-vs-rest settings for all 329 anomaly detection tasks.We select the prompt with the best synthetic AUROC and report how often each strategy's selected prompt matches the best prompt on real validation data-we report the resulting AUROC for each strategy.For comparison, we also include two baseline strategies: (i) the default prompt template (e.g., "a photo of a [class name] bird" vs "a photo of some bird") and (ii) a full prompt ensemble as proposed by Radford et al. (Radford et al., 2021) and evaluated in prior work (Jeong et al., 2023b;Zhou et al., 2024).</p>
<p>Table 1: When using synthetic anomalies, we report both the accuracy in picking the best model/prompt ("pick rate") and the resulting AUROC.As an upper bound, we show the AUROC when always picking the best model/prompt (in grey).For all settings, SWSA picks the best model/prompt more often and produces higher AUROCs than baseline strategies (largest model, default prompt, or prompt ensemble).In particular, SWSA is effective on CUB and Flowers with diffusion-based synthetic anomalies.</p>
<p>Table 2: To evaluate SWSA for model ranking for Diffusion-based and CutPaste-based anomalies, we calculate the Kendall's Tau rank correlation between the rankings with the synthetic and real validation datasets with varying sizes of synthetic validation dataset.Since we perform repeated tests on the same data, we apply Bonferroni correction to our tests and lower the significance threshold; cases with a statistically significant rank correlation (p &lt; 5.56e-3) are bolded.We find that diffusion-based anomalies are effective for model ranking on (i) the one-vs-rest tasks for both CUB and Flowers, and (ii) one-vs-closest for the Flowers dataset.</p>
<h1>of synthetic anomalies All anomalies 10 5</h1>
<p>One-vs-Closest Diffusion (CUB) 0.644 (p=9.14e-3)0.600 (p=1.67e-2)0.600 (p=1.67e-2)CutPaste (CUB) 0.377 (p=1.55e-1)0.511 (p=4.66e-2)0.555 (p=2.86e-2)Diffusion (Flowers) 0.777 (p=9.46e-4)0.777 (p=9.46e-4)0.822 (p=3.57e-4)CutPaste (Flowers) 0.644 (p=9.14e-3)0.466 (p=7.25e-2)0.244 (p=3.81e-1)One-vs-Rest Diffusion (CUB) 0.866 (p=1.15e-4)0.822 (p=3.57e-4)0.822 (p=3.57e-4)CutPaste (CUB) 0.600 (p=1.67e-2)0.733 (p=2.21e-3)0.688 (p=4.68e-3)Diffusion (Flowers) 0.866 (p=1.15e-4)0.866 (p=1.15e-4)0.911 (p=2.97e-5)CutPaste (Flowers) 0.733 (p=2.21e-3)0.555 (p=2.86e-2)0.333 (p=2.16e-1)CUB ( 54) Flowers ( 99) VisA (PCB1) (Cable) Evaluation results.For all four datasets, in Tab. 1, we report the rate at which the best prompt is picked (i.e., pick rate) and the resulting AUROC of the selected prompt.We find that SWSA performs best for Flowers and CUB with diffusion-based anomalies; we are able to select better prompts with our synthetic validation sets and improve the zero-shot CLIP AUROC over the popular prompt ensemble for all settings.We find that SWSA performs best for MVTec-AD and VisA with CutPaste-based anomalies; although SWSA is unable to outperform the prompt ensemble in all settings, it does outperform the prompt ensemble in the one-vs-closest setting for both MVTec-AD and VisA.This suggests that, although the prompt ensemble is more effective in general, it is not always the best strategy and SWSA can be effective for prompt selection in particularly difficult settings (i.e., the worst-case anomaly detection task).Overall, we find that our synthetic validation sets can be used to select the best prompts for CLIP-based anomaly detectors on tasks of varying domain and difficulty.</p>
<p>Qualitative Results</p>
<p>One assumption of our work is that validation results based on synthetic anomalies can replace validation results based on labeled ground truth validation sets.We have shown quantitatively that synthetic anomalies provide competitive model selection strategies.While it would be desirable for the synthetic anomalies to reflect the underlying true distribution of anomalies, this is not an assumption we can make-anomalies, by definition, are too rare and the distribution of anomalies cannot be estimated in practice.Hence, in this section we qualitatively compare true anomalies with artificial anomalies.</p>
<p>Figure 4 shows a t-SNE visualization of the ViT-B-16 model's embeddings for four different one-vs-closest anomaly-detection tasks-one from each dataset.Each plot shows the embeddings of (i) real in-class images, (ii) diffusion-generated synthetic anomalies, (iii) CutPastegenerated synthetic anomalies, and (iv) real anomalies.Real anomalies are not available for our method in practice, but we provide their embeddings for illustration.</p>
<p>We find that, when anomalies come from natural variations (i.e., different species of flowers), they are further from normal data and are similar to diffusion-based anomalies.Conversely, when anomalies come from subtle, local changes (i.e., a defective pin on a chip), they are more tightly clustered around normal data and are similar to Cutpaste-based anomalies.Figure 4 shows that ground-truth anomalies for MVTec-AD and VisA are harder to distinguish from normal data and overlap with the normal data distribution, which makes detecting these anomalies with foundation models more difficult and explains why performance is worse.</p>
<p>Following Shoshan et al. (Shoshan et al., 2023), we also perform a theoretical analysis of SWSA by computing the total variation between our real and synthetic validation sets to determine if a tight bound exists for model rank preservation.Details are provided in Supp.B. We ultimately find that a tight bound does not exist, although our empirical results in Tab. 1 show that SWSA often selects models and prompts that match selections made with the ground-truth validation set.</p>
<p>Conclusion</p>
<p>In this work, we propose and evaluate SWSA: an approach for selecting image-based anomaly detection models without labeled validation data.We use a general-purpose diffusion model to generate synthetic anomalies using only a small support set of in-class examples, without relying on any model training or fine-tuning.Our empirical study shows that SWSA can be used to select amongst candidate imagebased anomaly detection models and to select prompts for zero-shot CLIP-based anomaly detection.SWSA can outperform baseline selection strategies, such as using the largest model available or a prompt ensemble.</p>
<p>Impact Statement</p>
<p>This paper presents work whose goal is to advance the field of Machine Learning.There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.</p>
<p>A. Prompt Templates used for CLIP-based Anomaly Detection</p>
<p>For our experiments in Sec.4.3, we evaluated across a set of ten candidate prompt templates.Our evaluated prompts are general-purpose, and only the term "bird" or "flower" is added to the template for the CUB and Flowers dataset respectively.For MVTec-AD and VisA, we only perform mild class-name cleaning: we remove trailing numbers from class names and fully write all acronyms (e.g., "PCB1" becomes "printed circuit board").Unlike the techniques used in prior work (Jeong et al., 2023b), we do not perform any other class-specific modifications.For each dataset, the candidate prompt templates are provided below.For the results shown in Tab. 1, the first prompt listed is the default prompt, and all ten prompts are averaged for the prompt ensemble.computing the total variation, and report the average of ten executions of k-means clustering.When computing the total variation between validation datasets, we let D 1 = X support ∪ X out and D 2 = X support ∪ Xout , where Xout are the synthetic anomalies produced by our diffusion-based or CutPaste-based method.</p>
<p>Table 3: For the classes shown in Fig. 4, we compute the total variation between the real validation set and the synthetic validation set.Despite positive empirical results, no setting provides a tight bound, and we cannot provide strong guarantees of rank preservation on anomaly detection tasks.Table 3 shows the total variation for each dataset and type of synthetic anomaly.Although our empirical results in Sec.4.2 and Sec.4.3 show that our synthetic anomalies can be used for model selection and prompt selection, we find that the total variation does not provide a tight bound for synthetic validation sets with Cutpaste-based nor diffusion-based anomalies (the empirical risk difference is over 0.5 in all settings).</p>
<p>Figure2: Examples of synthetic anomalies generated with our diffusion-based method for CUB class 1 (left) and MVTec-AD "cable" (right).For each example, the top row of images (in green) are used as source "style" images, and the left column of images (in cyan) are used as source "content" images.The inner grid (in red) shows each pairwise interpolation between the source style and content image, performed with our modified DiffStyle process.All source images are drawn from the distribution of class 1 support images; no validation data or images from other classes are used.</p>
<p>Figure 4 :
4
Figure 4: For each anomaly detection task, the ViT-B-16 embeddings of in-class images (orange triangle), diffusiongenerated anomalies (black circle), Cutpaste-generated anomalies (green square), and real one-vs-closest anomalies (blue triangle) are shown.When anomalies come from natural variations between classes (CUB and Flowers), they are better represented by diffusion-based anomalies.When anomalous images come from local changes, they are better represented by Cutpaste-based anomalies.</p>
<p>Section 3.2 Section 3.1 Model Selection (SWSA) Outlier Generation Benign In-class Dataset Synthetic Validation Dataset Diffusion Model Anomaly Detection Models Local Augmentation Figure</p>
<p>1: We propose two methods for generating synthetic anomalies, as described in Sec.3.2: image-guided generation with a diffusion model and local augmentation.We produce a synthetic validation set by combining real normal images with synthetic anomalies.The synthetic validation set is then used for model selection, as described in Sec.3.1.Components in blue are frozen, components in green are real data, and components in orange are methods implemented in this work.</p>
<p>, so we investigate if auxiliary examples are effective for model selection.When sampling anomalies from Tiny-Imagenet, we sample uniformly at random to generate a dataset Xout of the same size: 100 images for tasks with CUB, VisA, and MVTec-AD; 25 images for tasks with the Flowers dataset.Evaluation results.Tab. 1 shows, for different model selection strategies, how often the best model is picked (i.e., pick rate) and the resulting AUROC from the selected model.We find that diffusion-based synthetic anomalies and CutPaste-based synthetic anomalies pick the best model at the highest rate and produce the highest AUROC for six out of eight evaluation settings, even outperforming the strategy that always selects the largest model (ViT-H-14).In particular, SWSA with diffusion-based anomalies selects the best model with the highest frequency for CUB and Flowers in all settings.</p>
<p>The original DiffStyle work implements a spherical interpolation strategy to produce higher-quality images, but we found this was not necessary for our use case.
For Tiny-Imagenet, the synthetic AUROC ≈ 1.0 for most cases, and the rank correlation is near zero.
To evaluate SWSA for model ranking, we compare the real and synthetic validation AUROC for all models when using our three candidate synthetic validation sets (Tiny-Imagenet, our diffusion-based anomalies, and our Cutpaste-based anomalies).SWSA performs best when ranking models with diffusion-based anomalies in the one-vs-rest anomaly detection setting on datasets with natural variation (i.e., CUB and Flowers).For a quantitative evaluation, we provide Kendall's Tau rank correlation values in Tab. 2.
A survey of anomaly detection techniques in financial domain. M Ahmed, A N Mahmood, M R Islam, Future Generation Computer Systems. 55C2016</p>
<p>Classification-based anomaly detection for general data. L Bergman, Y Hoshen, International Conference on Learning Representations. 2020</p>
<p>L Bergman, N Cohen, Y Hoshen, arXiv:2002.10445Deep nearest neighbor anomaly detection. 2020arXiv preprint</p>
<p>MVTec AD -a comprehensive real-world dataset for unsupervised anomaly detection. P Bergmann, M Fauser, D Sattlegger, C Steger, IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019</p>
<p>On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study. G O Campos, A Zimek, J Sander, R J Campello, B Micenková, E Schubert, I Assent, M E Houle, Data Mining and Knowledge Discovery. 302016</p>
<p>Unsupervised detection of lesions in brain MRI using constrained adversarial autoencoders. X Chen, E Konukoglu, MIDL Conference Book. 2018</p>
<p>A downsampled variant of imagenet as an alternative to the cifar datasets. P Chrabaszcz, I Loshchilov, F Hutter, arXiv:1707.088192017arXiv preprint</p>
<p>Incorporating expert feedback into active anomaly discovery. S Das, W.-K Wong, T Dietterich, A Fern, A Emmott, International Conference on Data Mining. 2016</p>
<p>Transfer-based semantic anomaly detection. L Deecke, L Ruff, R A Vandermeulen, H Bilen, International Conference on Machine Learning. 2021</p>
<p>ImageNet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, IEEE Conference on Computer Vision and Pattern Recognition. 2009</p>
<p>Diffusion models beat GANs on image synthesis. P Dhariwal, A Nichol, Advances in Neural Information Processing Systems. 2021</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. A Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, T Unterthiner, M Dehghani, M Minderer, G Heigold, S Gelly, J Uszkoreit, N Houlsby, International Conference on Learning Representations. 2021</p>
<p>A meta-analysis of the anomaly detection problem. A Emmott, S Das, T Dietterich, A Fern, W.-K Wong, arXiv:1503.011582015arXiv preprint</p>
<p>Zeroshot out-of-distribution detection based on the pretrained model CLIP. S Esmaeilpour, B Liu, E Robertson, L Shu, T Fernando, H Gammulle, S Denman, S Sridharan, C Fookes, AAAI conference on artificial intelligence. 2022542021Deep learning for medical anomaly detectiona survey</p>
<p>A comparative evaluation of unsupervised anomaly detection algorithms for multivariate data. M Goldstein, S Uchida, PloS one. 1142016</p>
<p>Toward supervised anomaly detection. N Görnitz, M Kloft, K Rieck, U Brefeld, Journal of Artificial Intelligence Research. 462013</p>
<p>AD-Bench: Anomaly detection benchmark. S Han, X Hu, H Huang, M Jiang, Y Zhao, Advances in Neural Information Processing Systems. 2022</p>
<p>Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, D Hendrycks, M Mazeika, T Dietterich, IEEE Conference on Computer Vision and Pattern Recognition. 2016. 2019aInternational Conference on Learning Representations</p>
<p>Using self-supervised learning can improve model robustness and uncertainty. D Hendrycks, M Mazeika, S Kadavath, D Song, Advances in Neural Information Processing Systems. 2019b</p>
<p>Denoising diffusion probabilistic models. J Ho, A Jain, P Abbeel, Advances in Neural Information Processing Systems. 202033</p>
<p>Distilling model failures as directions in latent space. S Jain, H Lawrence, A Moitra, A Madry, International Conference on Learning Representations. 2023</p>
<p>Training-free style transfer emerges from h-space in diffusion models. J Jeong, M Kwon, Y Uh, arXiv:2303.154032023aarXiv preprint</p>
<p>Zero-/few-shot anomaly classification and segmentation. J Jeong, Y Zou, T Kim, D Zhang, A Ravichandran, O Dabeer, Winclip, IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023b</p>
<p>A style-based generator architecture for generative adversarial networks. T Karras, S Laine, Aila , T , IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019</p>
<p>Analyzing and improving the image quality of StyleGAN. T Karras, S Laine, M Aittala, J Hellsten, J Lehtinen, Aila , T , IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020</p>
<p>Imagic: Text-based real image editing with diffusion models. B Kawar, S Zada, O Lang, O Tov, H Chang, T Dekel, I Mosseri, M Irani, IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023</p>
<p>DiffusionCLIP: Textguided diffusion models for robust image manipulation. G Kim, T Kwon, J C Ye, IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022</p>
<p>Diffusion models already have a semantic latent space. M Kwon, J Jeong, Y Uh, International Conference on Learning Representations. 2023</p>
<p>Deep anomaly detection under labeling budget constraints. A Li, C Qiu, M Kloft, P Smyth, S Mandt, M Rudolph, International Conference on Machine Learning. 2023a</p>
<p>Zero-shot anomaly detection without foundation models. A Li, C Qiu, M Kloft, P Smyth, M Rudolph, S Mandt, arXiv:2302.078492023barXiv preprint</p>
<p>Cutpaste: Self-supervised learning for anomaly detection and localization. C.-L Li, K Sohn, J Yoon, T Pfister, IEEE/CVF conference on Computer Vision and Pattern Recognition. 2021</p>
<p>PromptAD: Learning prompts with only normal samples for few-shot anomaly detection. X Li, Z Zhang, X Tan, C Chen, Y Qu, Y Xie, L Ma, IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024a</p>
<p>Zero-shot anomaly detection using text prompts. Y Li, A G David, F Liu, C.-S Foo, Promptad, IEEE/CVF Winter Conference on Applications of Computer Vision. 2024b</p>
<p>Exposing outlier exposure: What can be learned from few, one, and zero outlier images. P Liznerski, L Ruff, R A Vandermeulen, B J Franks, K R Muller, M Kloft, Transactions on Machine Learning Research. 2022</p>
<p>Zero-shot model diagnosis. J Luo, Z Wang, C H Wu, D Huang, Torre De La, IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023</p>
<p>The need for unsupervised outlier model selection: A review and evaluation of internal evaluation strategies. M Q Ma, Y Zhao, X Zhang, L Akoglu, ACM SIGKDD Explorations Newsletter. 2512023</p>
<p>On the internal evaluation of unsupervised outlier detection. H O Marques, R J Campello, A Zimek, J Sander, International conference on scientific and statistical database management. 2015</p>
<p>Internal evaluation of unsupervised outlier detection. H O Marques, R J Campello, J Sander, A Zimek, ACM Transactions on Knowledge Discovery from Data (TKDD). 1442020</p>
<p>Kitsune: an ensemble of autoencoders for online network intrusion detection. Y Mirsky, T Doitshman, Y Elovici, A Shabtai, Network and Distributed System Security Symposium. 2018</p>
<p>Fake it until you make it: Towards accurate near-distribution novelty detection. H Mirzaei, M Salehi, S Shahabi, E Gavves, C G M Snoek, M Sabokrou, M H Rohban, International Conference on Learning Representations. 2023</p>
<p>Null-text editing real images using guided diffusion models. R Mokady, A Hertz, K Aberman, Y Pritch, D Cohen-Or, IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023</p>
<p>An evaluation method for unsupervised anomaly detection algorithms. T T Nguyen, U Q Nguyen, Journal of Computer Science and Cybernetics. 3232016</p>
<p>Automated flower classification over a large number of classes. M.-E Nilsback, A Zisserman, Indian Conference on Computer Vision, Graphics and Image Processing. 2008</p>
<p>Acoustic novelty detection with adversarial autoencoders. E Principi, F Vesperini, S Squartini, F Piazza, 2017 International Joint Conference on Neural Networks. 2017</p>
<p>Neural transformation learning for deep anomaly detection beyond images. C Qiu, T Pfrommer, M Kloft, S Mandt, M Rudolph, International Conference on Machine Learning. 2021</p>
<p>Raising the bar in graph-level anomaly detection. C Qiu, M Kloft, S Mandt, M Rudolph, International Joint Conference on Artificial Intelligence. 2022a</p>
<p>Latent outlier exposure for anomaly detection with contaminated data. C Qiu, A Li, M Kloft, M Rudolph, S Mandt, International Conference on Machine Learning. 2022b</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021. 2021International Conference on Machine Learning</p>
<p>Deep one-class classification. L Ruff, R Vandermeulen, N Goernitz, L Deecke, S A Siddiqui, A Binder, E Müller, M Kloft, International Conference on Machine Learning. 2018</p>
<p>Deep semisupervised anomaly detection. L Ruff, R A Vandermeulen, N Görnitz, A Binder, E Müller, K.-R Müller, M Kloft, International Conference on Learning Representations. 2019</p>
<p>Assessing generative models via precision and recall. M S Sajjadi, O Bachem, M Lucic, O Bousquet, S Gelly, Advances in Neural Information Processing Systems. 2018</p>
<p>T Schneider, C Qiu, M Kloft, D A Latif, S Staab, S Mandt, M Rudolph, arXiv:2202.03944Detecting anomalies within time series using local neural transformations. 2022arXiv preprint</p>
<p>Meta-learning for automated selection of anomaly detectors for semisupervised datasets. D Schubert, P Gupta, M Wever, International Symposium on Intelligent Data Analysis. 2023</p>
<p>Synthetic data for model selection. A Shoshan, N Bhonker, I Kviatkovsky, M Fintz, G Medioni, International Conference on Machine Learning. 2023</p>
<p>Detecting structurally anomalous logins within enterprise networks. H Siadati, N Memon, ACM SIGSAC Conference on Computer and Communications Security. 2017</p>
<p>Learning and evaluating representations for deep one-class classification. K Sohn, C.-L Li, J Yoon, M Jin, T Pfister, International Conference on Learning Representations. 2020</p>
<p>Denoising diffusion implicit models. J Song, C Meng, S Ermon, International Conference on Learning Representations. 2021</p>
<p>An overview and a benchmark of active learning for outlier detection with one-class classifiers. Expert Systems with Applications. H Trittenbach, A Englhardt, K Böhm, 2021168</p>
<p>Plugand-play diffusion features for text-driven image-toimage translation. N Tumanyan, M Geyer, S Bagon, T Dekel, IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023</p>
<p>C Wah, S Branson, P Welinder, P Perona, S Belongie, CNS-TR-2011-001The Caltech-UCSD Birds-200-2011 Dataset. 2011California Institute of TechnologyTechnical Report</p>
<p>Automatic unsupervised outlier model selection. Y Zhao, R Rossi, L Akoglu, Advances in Neural Information Processing Systems. 2021</p>
<p>Toward unsupervised outlier model selection. Y Zhao, S Zhang, L Akoglu, IEEE International Conference on Data Mining. 2022</p>
<p>C Zhou, C C Loy, B Dai, Denseclip, arXiv:2112.01071Extract free dense labels from CLIP. 2021arXiv preprint</p>
<p>Anoma-lyCLIP: Object-agnostic prompt learning for zero-shot anomaly detection. Q Zhou, G Pang, Y Tian, S He, J Chen, International Conference on Learning Representations. 2024</p>
<p>Spot-the-difference self-supervised pre-training for anomaly detection and segmentation. Y Zou, J Jeong, L Pemula, D Zhang, O Dabeer, European Conference on Computer Vision, 2022. % CLIP Templates for Flowers. % CLIP Templates for CUB. CLIP Templates for MVTec-AD and VisA. a blurry photo of {} with defect'. with defect'], ['a photo of a large {}', 'a photo of a large {} with defect'</p>
<p>2023) study model selection with synthetic data in the binary classification setting; their theoretical analysis also assumes that synthetic validation data does not come from the ground-truth data distribution. They show that the total variation distance between a synthetic validation set and true data provides an upper bound on the empirical risk difference between any two classifiers. We follow the method proposed by. B Shoshan, We then use the histogram of the corresponding cluster assignments to compute the total variation: T V = i |K i,1 − K i. Sajjadi, We assign each point to a cluster y ∈ {0 . . . k} and separate the cluster assignments into Y 1 and Y 2 based on their original datasets. 2 |, where K i,j is the number of points assigned to cluster i in dataset j. As a heuristic, we use k = |D 1 | + |D 2 | clusters when</p>            </div>
        </div>

    </div>
</body>
</html>
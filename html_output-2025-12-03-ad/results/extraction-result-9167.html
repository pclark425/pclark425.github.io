<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9167 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9167</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9167</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-278782257</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.15094v1.pdf" target="_blank">SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have shown impressive capabilities in contextual understanding and reasoning. However, evaluating their performance across diverse scientific domains remains underexplored, as existing benchmarks primarily focus on general domains and fail to capture the intricate complexity of scientific data. To bridge this gap, we construct SciCUEval, a comprehensive benchmark dataset tailored to assess the scientific context understanding capability of LLMs. It comprises ten domain-specific sub-datasets spanning biology, chemistry, physics, biomedicine, and materials science, integrating diverse data modalities including structured tables, knowledge graphs, and unstructured texts. SciCUEval systematically evaluates four core competencies: Relevant information identification, Information-absence detection, Multi-source information integration, and Context-aware inference, through a variety of question formats. We conduct extensive evaluations of state-of-the-art LLMs on SciCUEval, providing a fine-grained analysis of their strengths and limitations in scientific context understanding, and offering valuable insights for the future development of scientific-domain LLMs.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9167.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9167.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary large language model (OpenAI) evaluated in this study as a text-based simulator for scientific context understanding across multiple scientific subdomains and modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary multimodal large language model from OpenAI; used in evaluations reported in this paper (no parameter count disclosed in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Multiple: materials science, biology, chemistry, physics, biomedicine (evaluated across sub-datasets such as MatText, BioText, MolTab, ProtTab, IaeaTab, PhaKG, PriKG, HipKG, GoKG).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based simulation of scientific context understanding tasks: locating relevant information, detecting information-absence, integrating multi-source evidence, and performing context-aware inference from unstructured text, structured tables, and knowledge graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy for deterministic question types (QA/MCQ/CC) and rejection rate for information-absence detection; scores reported per competency and modality (Tables 3-5).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Overall accuracy 61.52% (Table 3). Competency-level: Relevant Information Identification 89.72%, Information-absence Detection 19.51% (rejection rate metric), Multi-source Information Integration 54.90%, Context-aware Inference 65.97% (Table 4). Modality-level: Text 71.91%, Table 55.91%, KG 63.13% (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Model size and scale (correlated with performance), modality of input (text > tables/KGs), reasoning augmentations (explicit thinking/CoT improves structured-data tasks), prompt design and standardized templates, presence of injected noisy distractors (noise injection k values), and quality/length of context (long context handling).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against multiple other LLMs evaluated in the paper (e.g., DeepSeek-R1, Qwen3-8B, Claude-3.5-Sonnet). Also compared 'direct answering' versus 'answering with context' where context integration improved performance (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Relatively poor information-absence detection (low rejection rate ~19.5%), vulnerability to hallucination when context is insufficient, degraded performance on structured tables and KGs compared to text, and sensitivity to noisy distractors injected in contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors suggest improving uncertainty quantification (confidence-based rejection), calibrated outputs, reinforcement learning with human feedback, verification-based prompting, and joint multimodal pretraining to better handle structured modalities and reduce hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9167.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9167.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3.5-Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude-3.5-Sonnet (Anthropic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary LLM evaluated as a text-based scientific context processor, notable for higher conservative answering (higher rejection on absence detection) in this benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3.5-Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary model from Anthropic evaluated in this paper (parameter count not specified here).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Multiple scientific subdomains in the benchmark (biology, materials, chemistry, physics, biomedicine).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Processing long scientific contexts (text/tables/KGs) to identify relevant facts, detect missing information, integrate multiple sources, and perform context-aware inference.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy for QA/MCQ/CC and rejection rate for absence detection.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Overall accuracy 59.20% (Table 3). Competency-level: Relevant Information Identification 82.95%, Information-absence Detection 49.10%, Multi-source Information Integration 50.85%, Context-aware Inference 47.29% (Table 4). Modality-level: Text 59.75%, Table 53.41%, KG 64.99% (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Conservative answering strategy (higher absence detection), modality (better on KG and text than some models), prompt design, and reasoning capability constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Benchmarked against other LLMs (e.g., GPT-4o, DeepSeek-R1, Qwen3-8B). Compared direct answering vs answering with context where context improved results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Moderate context-aware inference performance and variable integration capability; still susceptible to selecting incorrect context rows in KGs leading to false answers in absence-detection tasks (example failure cases noted).</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors note that Claude-3.5-Sonnet's conservative behavior is beneficial for absence detection and recommend incorporating uncertainty-aware strategies and retrieval verification to further reduce hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9167.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9167.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning-augmented open-source model evaluated as a high-performing text-based simulator for scientific context understanding, especially strong at multi-source integration and inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source model with explicit reasoning mechanisms (reported by the authors to have enhanced factual grounding and reasoning; precise parameter count not provided in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Multiple: tested across materials, biology, chemistry, physics and biomedical subdatasets in SciCUEval.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Simulating multi-step reasoning over noisy scientific contexts to identify relevant entries, integrate multi-source information, and perform context-aware inference across text, tables, and KGs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy for deterministic answers and rejection rate for absence detection.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Overall accuracy 69.72% (Table 3). Competency-level: Relevant Information Identification 94.13%, Information-absence Detection 11.75%, Multi-source Information Integration 72.78%, Context-aware Inference 79.44% (Table 4). Modality-level: Text 69.00%, Table 73.19%, KG 66.64% (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Presence of explicit reasoning mechanisms (e.g., Chain-of-Thought or reasoning augmentation) which improved integration and inference; modality (strong on structured modalities relative to many others); model size and training; susceptibility to noisy distractors when CoT not used.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Outperformed many general-purpose and specialized models in integration and inference; compared directly against Qwen3-8B, GPT-4o, DeepSeek-V3, SciGLM etc.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Very low information-absence detection (11.75%), indicating a tendency to answer even when evidence is insufficient; still errors under heavy noise injection and in some KG multi-hop integration tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors highlight that reasoning-augmented architectures (like DeepSeek-R1) substantially benefit structured-data tasks and recommend combining explicit reasoning with uncertainty mechanisms to reduce overconfidence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9167.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9167.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-3 (Qwen3-8B with thinking)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen3-8B (with explicit thinking)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source LLM evaluated both with and without explicit 'thinking' (CoT) activation; 'thinking' mode markedly improved structured retrieval and table/space-group extraction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen3-8B (with explicit thinking)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 8B-parameter family model (Qwen series) evaluated in both standard and reasoning-activated modes; paper emphasizes the effect of 'thinking' activation on performance.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Materials science, biology, chemistry, and other subdomains in SciCUEval (notably strong on some table tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based simulation of multi-source extraction and structured-data querying (e.g., listing material IDs with specific properties, retrieving table columns), and context-aware inference.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy for deterministic answers; also precision/recall examples discussed for specific tasks (case studies).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Overall accuracy 64.69% (Table 3). Competency-level: Relevant Information Identification 88.29%, Information-absence Detection 43.57%, Multi-source Information Integration 53.87%, Context-aware Inference 64.38% (Table 4). Modality-level: Text 62.53%, Table 66.02%, KG 64.27% (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Activation of explicit thinking/CoT strongly improves precision and recall on structured tasks (case study showed 100% precision & recall with CoT vs many false positives without), model size, prompt constraints that focus attention on relevant columns, and noisy distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared with its own non-thinking variant (large performance drop without CoT), and other top models (DeepSeek-R1, GPT-4o).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Without CoT, generated many false positives in multi-entry extraction tasks; moderate information-absence detection; performance depends strongly on prompt engineering targeting specific fields.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors recommend Chain-of-Thought or explicit reasoning activation when querying structured data and constraining search scope (e.g., column-focused prompts) to reduce spurious outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9167.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9167.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-V3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-V3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source model evaluated for context understanding; shows strong performance on relevant-information identification but moderate integration and inference capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-V3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source model from DeepSeek-AI family; described as having good in-context retrieval and language understanding but weaker than reasoning-augmented variants for deeper inference.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Evaluated across SciCUEval subdomains including biology, materials, chemistry, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Locating relevant information from noisy contexts and performing basic multi-source integration and inference over text/tables/KGs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy for QA/MCQ/CC tasks; rejection rate for absence detection.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Overall accuracy 57.50% (Table 3). Competency-level: Relevant Information Identification 90.80%, Information-absence Detection 6.05%, Multi-source Information Integration 49.80%, Context-aware Inference 60.53% (Table 4). Modality-level: Text 56.18%, Table 55.68%, KG 59.77% (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Strong retrieval/in-context capabilities aid identification tasks; lacks deeper reasoning augmentations leading to weaker integration and inference; heavily sensitive to noisy distractors and absence-detection challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Underperforms compared to DeepSeek-R1 (reasoning-augmented sibling) in integration and inference, but better than many domain-specific models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Very poor information-absence detection (~6%), which causes hallucination when context lacks evidence; moderate integration performance.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors note that enhancing explicit reasoning and uncertainty mechanisms would help models like DeepSeek-V3 perform better on integration/inference and reduce hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9167.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9167.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemDFM-v1.5-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemDFM-v1.5-8B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chemical-domain open-source dialogue foundation model evaluated in the benchmark; despite domain specialization, it performed substantially worse overall and across modalities compared to general-purpose reasoning models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChemDFM-v1.5-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 8B-parameter chemistry-focused dialogue foundation model (as referenced in the paper); specialized training for chemical tasks but limited general reasoning and multimodal robustness in SciCUEval.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Chemistry-centric subdatasets (MolTab) and also evaluated across other scientific subdomains in SciCUEval.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Simulating question-answering, table and KG retrieval, and context-aware inference in chemistry and related scientific contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy for QA/MCQ/CC and rejection rate for absence detection; per-dataset and per-competency metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Overall accuracy 36.80% (Table 3). Competency-level: Relevant Information Identification 45.49%, Information-absence Detection 19.31%, Multi-source Information Integration 22.23%, Context-aware Inference 46.40% (Table 4). Modality-level: Text 22.92%, Table 34.44%, KG 44.05% (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Narrow domain specialization (training data coverage) leading to limited general reasoning; inability to handle long contexts for some models (context-length errors noted in examples), and weaker cross-modal capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Underperforms compared to general-purpose and reasoning-augmented models (e.g., GPT-4o, DeepSeek-R1, Qwen3-8B) across most metrics and modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Significant underperformance across competencies and modalities; example errors include failing context-aware inference and triggering context-length errors in long-document queries.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors suggest that domain-specific models should incorporate targeted fine-tuning with curated scientific evidence, improve reasoning components, and expand multimodal training to improve structured-data comprehension.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9167.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9167.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciGLM-6B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciGLM-6B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scientific-domain 6B-parameter open-source model evaluated in this benchmark that underperformed substantially compared to general-purpose and reasoning-augmented models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SciGLM-6B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source scientific language model (6B parameters referenced in the paper) trained with domain-targeted techniques (self-reflective instruction tuning referenced), but shown to lack general reasoning depth in SciCUEval.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Scientific domains including biology, chemistry, materials and biomedical subdatasets used in SciCUEval.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based simulation of scientific context understanding (identification, absence detection, integration, inference) across multiple modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy across tasks and rejection rate for absence detection.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Overall accuracy 21.58% (Table 3). Competency-level: Relevant Information Identification 33.24%, Information-absence Detection 9.01%, Multi-source Information Integration 18.00%, Context-aware Inference 29.48% (Table 4). Modality-level: Text 38.48%, Table 14.25%, KG 22.51% (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Narrow training domain and insufficient reasoning capacity, difficulty handling structured modalities (tables/KGs), limited generalization across diverse tasks and long noisy contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Performs substantially worse than large general-purpose and reasoning-augmented models; compared across all models in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Low overall and competency-level performance; struggles with integration and inference, and shows high rates of incorrect/incomplete outputs and format failures.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors recommend enhancing reasoning depth and robustness, adopting domain-aware prompt engineering, and mixing specialized fine-tuning with techniques that improve cross-modal and structured-data reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LongBench <em>(Rating: 2)</em></li>
                <li>Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks. <em>(Rating: 2)</em></li>
                <li>Qwen technical report. <em>(Rating: 1)</em></li>
                <li>SciGLM: Training scientific language models with self-reflective instruction annotation and tuning. <em>(Rating: 1)</em></li>
                <li>ChemDFM: Dialogue foundation model for chemistry. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9167",
    "paper_id": "paper-278782257",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o",
            "brief_description": "A proprietary large language model (OpenAI) evaluated in this study as a text-based simulator for scientific context understanding across multiple scientific subdomains and modalities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Proprietary multimodal large language model from OpenAI; used in evaluations reported in this paper (no parameter count disclosed in the paper).",
            "scientific_subdomain": "Multiple: materials science, biology, chemistry, physics, biomedicine (evaluated across sub-datasets such as MatText, BioText, MolTab, ProtTab, IaeaTab, PhaKG, PriKG, HipKG, GoKG).",
            "simulation_task": "Text-based simulation of scientific context understanding tasks: locating relevant information, detecting information-absence, integrating multi-source evidence, and performing context-aware inference from unstructured text, structured tables, and knowledge graphs.",
            "evaluation_metric": "Accuracy for deterministic question types (QA/MCQ/CC) and rejection rate for information-absence detection; scores reported per competency and modality (Tables 3-5).",
            "simulation_accuracy": "Overall accuracy 61.52% (Table 3). Competency-level: Relevant Information Identification 89.72%, Information-absence Detection 19.51% (rejection rate metric), Multi-source Information Integration 54.90%, Context-aware Inference 65.97% (Table 4). Modality-level: Text 71.91%, Table 55.91%, KG 63.13% (Table 5).",
            "factors_affecting_accuracy": "Model size and scale (correlated with performance), modality of input (text &gt; tables/KGs), reasoning augmentations (explicit thinking/CoT improves structured-data tasks), prompt design and standardized templates, presence of injected noisy distractors (noise injection k values), and quality/length of context (long context handling).",
            "comparison_baseline": "Compared against multiple other LLMs evaluated in the paper (e.g., DeepSeek-R1, Qwen3-8B, Claude-3.5-Sonnet). Also compared 'direct answering' versus 'answering with context' where context integration improved performance (Table 7).",
            "limitations_or_failure_cases": "Relatively poor information-absence detection (low rejection rate ~19.5%), vulnerability to hallucination when context is insufficient, degraded performance on structured tables and KGs compared to text, and sensitivity to noisy distractors injected in contexts.",
            "author_recommendations_or_insights": "Authors suggest improving uncertainty quantification (confidence-based rejection), calibrated outputs, reinforcement learning with human feedback, verification-based prompting, and joint multimodal pretraining to better handle structured modalities and reduce hallucinations.",
            "uuid": "e9167.0",
            "source_info": {
                "paper_title": "SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Claude-3.5-Sonnet",
            "name_full": "Claude-3.5-Sonnet (Anthropic)",
            "brief_description": "A proprietary LLM evaluated as a text-based scientific context processor, notable for higher conservative answering (higher rejection on absence detection) in this benchmark.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Claude-3.5-Sonnet",
            "model_description": "Proprietary model from Anthropic evaluated in this paper (parameter count not specified here).",
            "scientific_subdomain": "Multiple scientific subdomains in the benchmark (biology, materials, chemistry, physics, biomedicine).",
            "simulation_task": "Processing long scientific contexts (text/tables/KGs) to identify relevant facts, detect missing information, integrate multiple sources, and perform context-aware inference.",
            "evaluation_metric": "Accuracy for QA/MCQ/CC and rejection rate for absence detection.",
            "simulation_accuracy": "Overall accuracy 59.20% (Table 3). Competency-level: Relevant Information Identification 82.95%, Information-absence Detection 49.10%, Multi-source Information Integration 50.85%, Context-aware Inference 47.29% (Table 4). Modality-level: Text 59.75%, Table 53.41%, KG 64.99% (Table 5).",
            "factors_affecting_accuracy": "Conservative answering strategy (higher absence detection), modality (better on KG and text than some models), prompt design, and reasoning capability constraints.",
            "comparison_baseline": "Benchmarked against other LLMs (e.g., GPT-4o, DeepSeek-R1, Qwen3-8B). Compared direct answering vs answering with context where context improved results.",
            "limitations_or_failure_cases": "Moderate context-aware inference performance and variable integration capability; still susceptible to selecting incorrect context rows in KGs leading to false answers in absence-detection tasks (example failure cases noted).",
            "author_recommendations_or_insights": "Authors note that Claude-3.5-Sonnet's conservative behavior is beneficial for absence detection and recommend incorporating uncertainty-aware strategies and retrieval verification to further reduce hallucinations.",
            "uuid": "e9167.1",
            "source_info": {
                "paper_title": "SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "DeepSeek-R1",
            "name_full": "DeepSeek-R1",
            "brief_description": "A reasoning-augmented open-source model evaluated as a high-performing text-based simulator for scientific context understanding, especially strong at multi-source integration and inference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DeepSeek-R1",
            "model_description": "Open-source model with explicit reasoning mechanisms (reported by the authors to have enhanced factual grounding and reasoning; precise parameter count not provided in main text).",
            "scientific_subdomain": "Multiple: tested across materials, biology, chemistry, physics and biomedical subdatasets in SciCUEval.",
            "simulation_task": "Simulating multi-step reasoning over noisy scientific contexts to identify relevant entries, integrate multi-source information, and perform context-aware inference across text, tables, and KGs.",
            "evaluation_metric": "Accuracy for deterministic answers and rejection rate for absence detection.",
            "simulation_accuracy": "Overall accuracy 69.72% (Table 3). Competency-level: Relevant Information Identification 94.13%, Information-absence Detection 11.75%, Multi-source Information Integration 72.78%, Context-aware Inference 79.44% (Table 4). Modality-level: Text 69.00%, Table 73.19%, KG 66.64% (Table 5).",
            "factors_affecting_accuracy": "Presence of explicit reasoning mechanisms (e.g., Chain-of-Thought or reasoning augmentation) which improved integration and inference; modality (strong on structured modalities relative to many others); model size and training; susceptibility to noisy distractors when CoT not used.",
            "comparison_baseline": "Outperformed many general-purpose and specialized models in integration and inference; compared directly against Qwen3-8B, GPT-4o, DeepSeek-V3, SciGLM etc.",
            "limitations_or_failure_cases": "Very low information-absence detection (11.75%), indicating a tendency to answer even when evidence is insufficient; still errors under heavy noise injection and in some KG multi-hop integration tasks.",
            "author_recommendations_or_insights": "Authors highlight that reasoning-augmented architectures (like DeepSeek-R1) substantially benefit structured-data tasks and recommend combining explicit reasoning with uncertainty mechanisms to reduce overconfidence.",
            "uuid": "e9167.2",
            "source_info": {
                "paper_title": "SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Qwen-3 (Qwen3-8B with thinking)",
            "name_full": "Qwen3-8B (with explicit thinking)",
            "brief_description": "An open-source LLM evaluated both with and without explicit 'thinking' (CoT) activation; 'thinking' mode markedly improved structured retrieval and table/space-group extraction tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen3-8B (with explicit thinking)",
            "model_description": "Open-source 8B-parameter family model (Qwen series) evaluated in both standard and reasoning-activated modes; paper emphasizes the effect of 'thinking' activation on performance.",
            "scientific_subdomain": "Materials science, biology, chemistry, and other subdomains in SciCUEval (notably strong on some table tasks).",
            "simulation_task": "Text-based simulation of multi-source extraction and structured-data querying (e.g., listing material IDs with specific properties, retrieving table columns), and context-aware inference.",
            "evaluation_metric": "Accuracy for deterministic answers; also precision/recall examples discussed for specific tasks (case studies).",
            "simulation_accuracy": "Overall accuracy 64.69% (Table 3). Competency-level: Relevant Information Identification 88.29%, Information-absence Detection 43.57%, Multi-source Information Integration 53.87%, Context-aware Inference 64.38% (Table 4). Modality-level: Text 62.53%, Table 66.02%, KG 64.27% (Table 5).",
            "factors_affecting_accuracy": "Activation of explicit thinking/CoT strongly improves precision and recall on structured tasks (case study showed 100% precision & recall with CoT vs many false positives without), model size, prompt constraints that focus attention on relevant columns, and noisy distractors.",
            "comparison_baseline": "Compared with its own non-thinking variant (large performance drop without CoT), and other top models (DeepSeek-R1, GPT-4o).",
            "limitations_or_failure_cases": "Without CoT, generated many false positives in multi-entry extraction tasks; moderate information-absence detection; performance depends strongly on prompt engineering targeting specific fields.",
            "author_recommendations_or_insights": "Authors recommend Chain-of-Thought or explicit reasoning activation when querying structured data and constraining search scope (e.g., column-focused prompts) to reduce spurious outputs.",
            "uuid": "e9167.3",
            "source_info": {
                "paper_title": "SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "DeepSeek-V3",
            "name_full": "DeepSeek-V3",
            "brief_description": "An open-source model evaluated for context understanding; shows strong performance on relevant-information identification but moderate integration and inference capabilities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DeepSeek-V3",
            "model_description": "Open-source model from DeepSeek-AI family; described as having good in-context retrieval and language understanding but weaker than reasoning-augmented variants for deeper inference.",
            "scientific_subdomain": "Evaluated across SciCUEval subdomains including biology, materials, chemistry, etc.",
            "simulation_task": "Locating relevant information from noisy contexts and performing basic multi-source integration and inference over text/tables/KGs.",
            "evaluation_metric": "Accuracy for QA/MCQ/CC tasks; rejection rate for absence detection.",
            "simulation_accuracy": "Overall accuracy 57.50% (Table 3). Competency-level: Relevant Information Identification 90.80%, Information-absence Detection 6.05%, Multi-source Information Integration 49.80%, Context-aware Inference 60.53% (Table 4). Modality-level: Text 56.18%, Table 55.68%, KG 59.77% (Table 5).",
            "factors_affecting_accuracy": "Strong retrieval/in-context capabilities aid identification tasks; lacks deeper reasoning augmentations leading to weaker integration and inference; heavily sensitive to noisy distractors and absence-detection challenges.",
            "comparison_baseline": "Underperforms compared to DeepSeek-R1 (reasoning-augmented sibling) in integration and inference, but better than many domain-specific models.",
            "limitations_or_failure_cases": "Very poor information-absence detection (~6%), which causes hallucination when context lacks evidence; moderate integration performance.",
            "author_recommendations_or_insights": "Authors note that enhancing explicit reasoning and uncertainty mechanisms would help models like DeepSeek-V3 perform better on integration/inference and reduce hallucination.",
            "uuid": "e9167.4",
            "source_info": {
                "paper_title": "SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "ChemDFM-v1.5-8B",
            "name_full": "ChemDFM-v1.5-8B",
            "brief_description": "A chemical-domain open-source dialogue foundation model evaluated in the benchmark; despite domain specialization, it performed substantially worse overall and across modalities compared to general-purpose reasoning models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChemDFM-v1.5-8B",
            "model_description": "Open-source 8B-parameter chemistry-focused dialogue foundation model (as referenced in the paper); specialized training for chemical tasks but limited general reasoning and multimodal robustness in SciCUEval.",
            "scientific_subdomain": "Chemistry-centric subdatasets (MolTab) and also evaluated across other scientific subdomains in SciCUEval.",
            "simulation_task": "Simulating question-answering, table and KG retrieval, and context-aware inference in chemistry and related scientific contexts.",
            "evaluation_metric": "Accuracy for QA/MCQ/CC and rejection rate for absence detection; per-dataset and per-competency metrics reported.",
            "simulation_accuracy": "Overall accuracy 36.80% (Table 3). Competency-level: Relevant Information Identification 45.49%, Information-absence Detection 19.31%, Multi-source Information Integration 22.23%, Context-aware Inference 46.40% (Table 4). Modality-level: Text 22.92%, Table 34.44%, KG 44.05% (Table 5).",
            "factors_affecting_accuracy": "Narrow domain specialization (training data coverage) leading to limited general reasoning; inability to handle long contexts for some models (context-length errors noted in examples), and weaker cross-modal capabilities.",
            "comparison_baseline": "Underperforms compared to general-purpose and reasoning-augmented models (e.g., GPT-4o, DeepSeek-R1, Qwen3-8B) across most metrics and modalities.",
            "limitations_or_failure_cases": "Significant underperformance across competencies and modalities; example errors include failing context-aware inference and triggering context-length errors in long-document queries.",
            "author_recommendations_or_insights": "Authors suggest that domain-specific models should incorporate targeted fine-tuning with curated scientific evidence, improve reasoning components, and expand multimodal training to improve structured-data comprehension.",
            "uuid": "e9167.5",
            "source_info": {
                "paper_title": "SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "SciGLM-6B",
            "name_full": "SciGLM-6B",
            "brief_description": "A scientific-domain 6B-parameter open-source model evaluated in this benchmark that underperformed substantially compared to general-purpose and reasoning-augmented models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SciGLM-6B",
            "model_description": "Open-source scientific language model (6B parameters referenced in the paper) trained with domain-targeted techniques (self-reflective instruction tuning referenced), but shown to lack general reasoning depth in SciCUEval.",
            "scientific_subdomain": "Scientific domains including biology, chemistry, materials and biomedical subdatasets used in SciCUEval.",
            "simulation_task": "Text-based simulation of scientific context understanding (identification, absence detection, integration, inference) across multiple modalities.",
            "evaluation_metric": "Accuracy across tasks and rejection rate for absence detection.",
            "simulation_accuracy": "Overall accuracy 21.58% (Table 3). Competency-level: Relevant Information Identification 33.24%, Information-absence Detection 9.01%, Multi-source Information Integration 18.00%, Context-aware Inference 29.48% (Table 4). Modality-level: Text 38.48%, Table 14.25%, KG 22.51% (Table 5).",
            "factors_affecting_accuracy": "Narrow training domain and insufficient reasoning capacity, difficulty handling structured modalities (tables/KGs), limited generalization across diverse tasks and long noisy contexts.",
            "comparison_baseline": "Performs substantially worse than large general-purpose and reasoning-augmented models; compared across all models in the paper.",
            "limitations_or_failure_cases": "Low overall and competency-level performance; struggles with integration and inference, and shows high rates of incorrect/incomplete outputs and format failures.",
            "author_recommendations_or_insights": "Authors recommend enhancing reasoning depth and robustness, adopting domain-aware prompt engineering, and mixing specialized fine-tuning with techniques that improve cross-modal and structured-data reasoning.",
            "uuid": "e9167.6",
            "source_info": {
                "paper_title": "SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LongBench",
            "rating": 2
        },
        {
            "paper_title": "Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks.",
            "rating": 2,
            "sanitized_title": "longbench_v2_towards_deeper_understanding_and_reasoning_on_realistic_longcontext_multitasks"
        },
        {
            "paper_title": "Qwen technical report.",
            "rating": 1,
            "sanitized_title": "qwen_technical_report"
        },
        {
            "paper_title": "SciGLM: Training scientific language models with self-reflective instruction annotation and tuning.",
            "rating": 1,
            "sanitized_title": "sciglm_training_scientific_language_models_with_selfreflective_instruction_annotation_and_tuning"
        },
        {
            "paper_title": "ChemDFM: Dialogue foundation model for chemistry.",
            "rating": 1,
            "sanitized_title": "chemdfm_dialogue_foundation_model_for_chemistry"
        }
    ],
    "cost": 0.015439,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models</p>
<p>Jing Yu yujing17@zju.edu.cn 
ZJU-Hangzhou Global Scientific and Technological Innovation Center
Zhejiang University</p>
<p>The Polytechnic Institute
Zhejiang University</p>
<p>Yuqi Tang 
ZJU-UIUC
Zhejiang University</p>
<p>Kehua Feng 
ZJU-Hangzhou Global Scientific and Technological Innovation Center
Zhejiang University</p>
<p>College of Computer Science and Technology
Zhejiang University</p>
<p>Mingyang Rao 
ZJU-Hangzhou Global Scientific and Technological Innovation Center
Zhejiang University</p>
<p>College of Computer Science and Technology
Zhejiang University</p>
<p>Lei Liang 
Zhiqiang Zhang 
Mengshu Sun 
Wen Zhang 
College of Computer Science and Technology
Zhejiang University</p>
<p>Qiang Zhang 
ZJU-UIUC
Zhejiang University</p>
<p>Keyan Ding dingkeyan@zju.edu.cn 
ZJU-Hangzhou Global Scientific and Technological Innovation Center
Zhejiang University</p>
<p>Huajun Chen huajunsir@zju.edu.cn 
ZJU-Hangzhou Global Scientific and Technological Innovation Center
Zhejiang University</p>
<p>College of Computer Science and Technology
Zhejiang University</p>
<p>Qingyu Chen 
Yan Hu 
Xueqing Peng 
Qianqian Xie 
Qiao Jin 
Aidan Gilson 
XuguangMaxwell B Singer 
Wenhu Chen 
Hanwen Zha 
Zhiyu Chen 
Wenhan Xiong 
Hong Wang 
William Wang 
Hy 
Chengqi Zhao 
Chenyu Deng 
Chong Zhang 
Damai Ruan 
Daya Dai 
Dejian Guo 
Deli Yang 
Dongjie Chen 
Erhang Ji 
Fangyun Li 
Fucong Lin 
Fuli Dai 
Guangbo Luo 
Guanting Hao 
Guowei Chen 
H Li 
Han Zhang 
Bao 
Daya Guo 
Dejian Yang 
Haowei Zhang 
Junxiao Song 
Ruoyu Zhang 
Runxin Xu 
Qihao Zhu 
Shirong Ma 
Yunpeng Huang 
Jingwei Xu 
Junyu Lai 
Zixu Jiang 
Taolue Chen 
Zenan Li 
Yuan Yao 
Xiaoxing Ma 
Li- Juan Yang 
Hao Chen 
Albert Q Jiang 
Alexandre Sablayrolles 
Arthur Men- Sch 
Chris Bamford 
Singh Devendra 
Diego Chaplot 
Patrick Lewis 
Ethan Perez 
Aleksandra Piktus 
Fabio Petroni 
Vladimir Karpukhin 
Naman Goyal 
Aaron Openai 
Adam Hurst 
Adam P Lerer 
Adam Goucher 
Aditya Perelman 
Aidan Ramesh 
A J Clark 
Akila Ostrow 
Alan Welihinda 
Alec Hayes 
Aleksander M Radford 
Alex dry 
Alex Baker-Whitcomb 
Alex Beutel 
Alex Borzunov 
Alex Carney 
Alex Chow 
Alex Kirillov 
Alex Nichol 
Alex Paino </p>
<p>Ai
Po-Ting Lai, Zhizheng Wang2025</p>
<p>Charese Smiley
Sameena Shah</p>
<p>Iana Borova
Matt Beane, Ting-Hao HuangDylan Langdon, Reema Moussa, Bryan Routledge</p>
<p>DeepSeek-AI
Bingx-uan Wang
Bing XueAixin Liu, Bei Feng, Bochao Wu</p>
<p>Chengda Lu
Chenggang</p>
<p>Kehua Feng
Keyan Ding, Weijie Wang, Xiang Zhuang, Ming Qin, Yu ZhaoZeyuan Wang, Jianhua Yao</p>
<p>de las Casas
Florian BressandGianna Lengyel</p>
<p>SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models
E7F5316F605BCB375DABF05C51E84C43arXiv:2503.04013.
Large Language Models (LLMs) have shown impressive capabilities in contextual understanding and reasoning.However, evaluating their performance across diverse scientific domains remains underexplored, as existing benchmarks primarily focus on general domains and fail to capture the intricate complexity of scientific data.To bridge this gap, we construct SciCUEval, a comprehensive benchmark dataset tailored to assess the scientific context understanding capability of LLMs.It comprises ten domain-specific subdatasets spanning biology, chemistry, physics, biomedicine, and materials science, integrating diverse data modalities including structured tables, knowledge graphs, and unstructured texts.SciCUEval systematically evaluates four core competencies: Relevant information identification, Information-absence detection, Multisource information integration, and Contextaware inference, through a variety of question formats.We conduct extensive evaluations of state-of-the-art LLMs on SciCUEval, providing a fine-grained analysis of their strengths and limitations in scientific context understanding, and offering valuable insights for the future development of scientific-domain LLMs.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have demonstrated strong capabilities in natural language understanding, reasoning, and generation across a wide range of general-domain tasks (Bai et al., 2023a;OpenAI et al., 2024;Dubey et al., 2024;Qwen et al., 2025).However, their application to scientific domains remains challenging due to the unique characteristics of scientific language and knowledge.Scientific texts are often dense with technical terminology, implicit assumptions, multimodal data representations, and tightly interlinked concepts that require deeper contextual comprehension (Beltagy et al., 2019;Mann et al., 2020).</p>
<p>Existing LLM benchmarks in scientific domains (Chen et al., 2025;Sun et al., 2024;Feng et al., 2024;Saikh et al., 2022;Pedersen et al., 2020;Rubungo et al., 2024;Jiang et al., 2025) primarily focus on direct question-answering tasks, offering limited insight into how well LLMs perform in scientific context understanding, particularly for noisy and lengthy contexts.Additionally, they often neglect the heterogeneous and structured nature of scientific data, which can span textual descriptions, relational graphs (Talmor and Berant, 2018;He et al., 2024), and tabular datasets (Fang et al., 2024).In contrast, robust scientific context understanding demands precise information extraction, the ability to identify gaps or missing elements in context, and the integration of multiple evidence sources to support accurate conclusions.</p>
<p>To address this gap, we introduce SciCUEval, a comprehensive benchmark dataset designed to rigorously evaluate the scientific context understanding capabilities of LLMs.As shown in Figure 1, SciCUEval spans ten domain-specific subdatasets covering diverse disciplines such as biology, chemistry, physics, biomedicine, and materials science.Each subset incorporates rich external knowledge in multiple forms (structured tables, semi-structured knowledge graphs, and unstructured scientific texts) to represent the data modalities commonly encountered in scientific research.</p>
<p>SciCUEval targets four core competencies essential for scientific understanding: (1) Relevant Information Identification that locate and extract relevant information from complex and lengthy inputs;</p>
<p>(2) Information-Absence Detection that recognize missing, ambiguous, or incomplete contextual elements; (3) Multi-source Information Integration that aggregate and compare information from diverse sources; and (4) Context-Aware Inference that deduce accurate conclusions grounded in sci- For the material with CID 13182, what is its inchikey?entific contexts.These competencies are evaluated using diverse question types, including open-ended Q&amp;A, multiple-choice, content completion, and true/false validation.The contributions of this paper are summarized as follows:
CID-13182,   CID XXXXX,   CID XXXXX,  
 Establishing a scientific context understanding benchmark: We establish a benchmark to evaluate context understanding capabilities of LLMs in scientific domains, serving as a standardized evaluation suite for assessing LLMs' capabilities in identifying, detecting, integrating, and reasoning over scientific contexts.</p>
<p> Constructing a diverse set of domain-specific context understanding datasets: We construct ten sub-datasets across multiple disciplines, encompassing various data modalities and a wide range of question types to ensure comprehensive evaluation.</p>
<p> Extensive evaluation and analysis of LLMs:</p>
<p>We systematically evaluate and analyze the performance of various state-of-the-art LLMs on SciCUEval, highlighting their strengths and limitations, and offering insights for improvement.</p>
<p>Backgrounds and Related Works</p>
<p>Context Understanding Tasks Large language models (LLMs) have demonstrated remarkable capabilities in context understanding (Huang et al., 2023;Dong et al., 2022).This paradigm allows LLMs to adapt flexibly to new external knowledge without requiring additional fine-tuning or retraining (Lewis et al., 2020;Liu et al., 2024).Importantly, LLMs have shown strong performance across a wide range of domains when equipped with additional context.Furthermore, in-context learning strengthens LLMs' transparency by firmly establishing their arguments in the documents that were obtained (Mialon et al., 2023;Xiong et al., 2024).However, despite its potential, context understanding remains sensitive to prompt design and the quality of the provided context.It also demands that the model possess a robust ability to process and comprehend long texts.In this study, we systematically investigate the robustness and effectiveness of LLMs for context understanding in diverse scientific domains.</p>
<p>Context Understanding Benchmarks The development of robust and comprehensive benchmarks for evaluating long context understanding has gained increasing attention in recent research (Li et al., 2024;Liu et al., 2024;Bai et al., 2023b;Zhang et al., 2024c;Wu et al., 2025;Chen et al., 2020Chen et al., , 2021))</p>
<p>Datasets</p>
<p>This section presents the dataset construction process in SciCUEval, which involves formulating evaluation competencies, collecting scientific data, generating questions and answers, and conducting rigorous verification.</p>
<p>Evaluation Competencies</p>
<p>Inspired by the ability definition in (Chen et al., 2024), we formulate four capabilities essential for evaluating LLMs in scientific contexts:</p>
<p> Relevant Information Identification: LLMs must effectively distinguish between relevant information and extraneous noise within complex scientific contexts.In real-world scenarios, scientific data often contains contextually related but non-essential information.Robust models should be able to filter out such noise to ensure accurate understanding and responses.</p>
<p> Information-absence Detection: The ability to abstain from responding when all contextual data is irrelevant or unreliable.Scientific queries often require precise evidence, and when no valid information is present in the context, LLMs should refrain from generating speculative or hallucinated responses.</p>
<p> Multi-source Information Integration: Scientific queries often require synthesizing data from multiple sources.LLMs must aggregate and compare information across different contextual segments to generate precise and contextually grounded answers.These four competencies form the foundation of SciCUEval and provide a systematic evaluation framework for context understanding in scientific domains.</p>
<p>Source Data Collection</p>
<p>Scientific Domains To evaluate LLMs in scientific contexts comprehensively, we curate data from diverse scientific domains, including Biology, Chemistry, Physics, Biomedicine, and Materials Science.These disciplines are fundamental to modern science, encompassing a wide range of knowledge from theoretical principles to experimental data, ensuring a broad and representative assessment of long-context understanding capabilities in scientific applications.</p>
<p>Data Modalities</p>
<p>To support a broad evaluation of scientific context understanding capabilities, we consider three distinct data modalities: (1) Unstructured Text, (2) Structured Tables , and (3) Semistructured Knowledge Graphs (KGs).Each modality presents unique challenges, enabling a holistic assessment of LLMs' retrieval, synthesis, reasoning, and integration capabilities in scientific domains.Specifically, unstructured text corpora consist of scientific literature, allowing LLMs to retrieve, synthesize, and infer domain knowledge from textual sources.We collect thousands of recent research papers and experimental protocols from open-access repositories such as arXiv.Structured tables contain numerical and categorical data, testing LLMs' capacity to interpret structured knowledge, recognize contextual dependencies, and perform quantitative reasoning.We collect nuclear data from IAEA 1 , material properties from Material Project 2 , and molecular and protein properties from PubChem 3 .Knowledge graphs encode scientific knowledge as interconnected entities and relational networks, enabling the assessment of LLMs' abilities in relational inference, hierarchical knowledge traversal, and crossdomain knowledge synthesis.We collect wellestablished scientific KGs, including Gene Ontology 4 for gene-function relationships, HIPPIE (Alanis-Lobato et al., 2016) for protein-protein interactions, PharmKG (Zheng et al., 2021) for drugtarget interactions, and PrimeKG (Chandak et al., 2023) for clinical entity relationships.</p>
<p>Data Generation</p>
<p>Building on the collected source data, we construct corresponding datasets tailored to assess the proposed four competencies outlined in Sec.3.1.The data generation pipeline is illustrated in Figure 2, which involves (1) question generation, (2) noise injection, and (3) quality control.</p>
<p>Question Generation</p>
<p>We first sample a subset of texts, table rows, or KG triples from the full databases,
D = {d i | d i = (S), i = 1, 2, . . . , N }, (1)
where S denote the large-scale scientific data source, and  is the sampling operation.d i is a 1 https://www-nds.iaea.org 2 https://next-gen.materialsproject.org 3 https://pubchem.ncbi.nlm.nih.gov 4 https://geneontology.org  single data record or a small set of related entries.</p>
<p>Given each selected data unit d i , the question generation process is defined as:
(q i , a i ) = f LLM (p  d i ),(2)
where p is a manually designed prompt, and  denotes string concatenation.f LLM represents the LLM that generates semantically diverse and contextually relevant questions q i and corresponding answers a i based on d i .For each evaluation competency, we craft the prompt to ensure the questions are reasonable and aligned with the required competencies.The detailed prompts are provided in Appendix B. These generated questions span various formats, including Q&amp;A, multiple-choice, content completion, and true/false validation, offering a robust assessment of context understanding abilities.</p>
<p>Noise Injection Following question generation, we extract noisy information from the source data and inject them into the context.Specifically, we inject semantically similar yet unrelated entries into the context using an embedding-based similarity search.Formally, each sample before noise injection is denoted as x i = (q i , a i , d i ).To select distractor entries, we first compute embeddings for all candidate entries in the dataset D using Sentence-BERT (Reimers and Gurevych, 2019).
h d j = f S-BERT (d j ), d j  D,(3)
where f S-BERT denotes the embedding function.</p>
<p>We then employ the cosine similarity to efficiently retrieve the Top-k entries most similar to the se- lected entry d i :
N i = Top -k d j D{d i } sim h d i , h d j ,(4)
where sim(, ) denotes the cosine similarity between embedding vectors.The final sample after noise injection is represented as xi = (q i , a i , d i  N i ), where N i contains the k selected distractor entries used to augment the context.We sample k  [200,300] for structured tables and KGs, and set k = 5 for unstructured text.Through this approach, the injected noise closely mimics the type of confusing or misleading information that LLMs may encounter in practice, ensuring the benchmark dataset remains both challenging and realistic.</p>
<p>Quality Control To maintain the rigor of the constructed dataset, we implement a two-stage verification process to ensure data quality:</p>
<p> LLM as a Judge.we used advanced LLMs (e.g.,  as automated evaluators to check if each answer is directly extractable or logically deducible from the provided context, using a clear prompt.Only instances marked "Yes" were kept.</p>
<p> Human Expert Validation.Domain experts then manually reviewed the filtered data based on three aspects: (1) whether the question tests the intended competency, (2) whether the question is expressed clearly and logically, and (3) weather the answer is fully supported by contexts and factually correct.Only instances that received a "Yes" for all three criteria were accepted.</p>
<p>As a result, 90.83% of instances in our dataset met the high-quality criteria.Detailed information about data quality control can be found in Appendix C.</p>
<p>The Final Dataset</p>
<p>Based on the data collection, generation, and quality control processes described above, we construct the final SciCUEval</p>
<p>Experiments</p>
<p>In this section, we evaluate the performance of various LLMs on SciCUEval, and provide a thorough analysis of their capabilities in understanding scientific contexts.2024), ChemLLM-7B-Chat (Zhang et al., 2024b), ChemDFM-v1.5-8B (Zhao et al., 2024)).For detailed information about these models, please refer to Appendix F.</p>
<p>Settings To ensure a fair evaluation across all models, we adopt a unified prompting template that standardizes input formatting.Specifically, each input consists of a system prompt that specifies the question type and defines answer format requirements, contexts, and a question designed to assess one of the four core competencies in SciCUEval.</p>
<p>Given that each question in SciCUEval has a deterministic answer, we adopt accuracy as the evaluation metric for all question types across the tasks of relevant information identification, multi-source information integration, and context-aware inference.</p>
<p>For the task of information-absence detection, we use the rejection rate as the evaluation metric.</p>
<p>Overall Results</p>
<p>Table 3  Second, proprietary models such as GPT-4o and Claude-3.5-Sonnetmaintain competitive performance, especially in unstructured text-based domains (e.g., BioText, MatText), benefiting from their superior language understanding and generalization capabilities.Third, scientific-domain LLMs such as ChemDFM-v1.5-8B and SciGLM-6B exhibit substantially lower performance across all datasets.Although designed for scientific domains, these models tend to lack general reasoning capacity and robustness across modalities.Fourth, there is a strong positive correlation between model size and effectiveness.Large-scale models (e.g., GPT-4o, Llama4-Maverick, and Llama3.1-70B)consistently outperform their smaller counterparts (e.g., GPT-4o-mini, Llama4-Scout, and Llama3.1-8B)across most domains.</p>
<p>Evaluation Results of Four Competencies</p>
<p>Relevant Information Identification This competency measures a model's ability to locate and select the correct pieces of information from the provided context.As shown in Figure 4, DeepSeek-R1 leads all of the evaluated models, suggesting that explicit reasoning mechanisms effectively enhance factual grounding.DeepSeek-V3, GPT-4o, and Qwen-8B also exhibit strong performance, showing the advantages of in-context retrieval capabilities.In contrast, scientific-domain LLMs exhibit notably weaker performance in identifying relevant contexts across diverse scenarios.Information-absence Detection This metric evaluates whether a model appropriately withholds an answer when the required information is absent.Claude-3.5-Sonnetand Qwen-8B demonstrate relatively high accuracy, suggesting their conservative answering strategy and stronger understanding of uncertainty.Most models struggle in this competency, with scores below 20%, indicating a tendency to hallucinate answers when uncertain.This highlights the risk of "overconfidence" in current models, which may pose potential safety risks in the scientific domain.
-7 0 B -i t Q w e n 2 . 5 -7 B -i t G L M 4 -9 B L l a m a 3 . 1 -8 B -i t G e m m a 2 -9 B -i t M i n i s t r a l -8 B -i t C h e m D F M -v 1 . 5 -8 B S c i G L M -6 B L l a S M o l -M i s t r a l -7 B C h e m L L M -7 B -C h</p>
<p>Multi-source Information Integration</p>
<p>This task assesses a model's ability to synthesize information from multiple entries to construct a complete answer.DeepSeek-R1 achieves the highest performance, followed by GPT-4o and Llama4-Maverick, suggesting that these models are better equipped to combine multiple data points into coherent and accurate answers.Among smaller opensource models, GLM4-9B shows a competitive score, even surpassing DeepSeek-V3 in this competency.However, scientific LLMs significantly lag behind, indicating that while these domain-specific models are adept at handling scientific text, they face challenges in effectively synthesizing information from diverse sources.</p>
<p>Context-aware Inference This capability reflects a model's ability to reason over contextually relevant information.DeepSeek-R1 achieves the highest performance, and GPT-4o and Qwen3-8B also perform well, indicating that large-scale models and those enhanced with explicit thinking benefit significantly in contextual reasoning tasks.In contrast, models like Claude-3.5-Sonnetand DeepSeek-V3 show moderate capabilities but fall behind on deeper inference.Scientific-domain models such as ChemLLM-7B-Chat and SciGLM perform poorly, indicating limited general reasoning capabilities despite domain specialization.</p>
<p>Evaluation Results of Three Modalities</p>
<p>Figure 4 shows the performance of LLMs across three modalities: Text, Table , and KG, highlighting their strengths and weaknesses in handling diverse scientific data formats.Overall, LLMs tend to perform best on the text modality, reflecting their strong natural language understanding and generation capabilities.Notably, some smaller models even exceed their average overall performance on text, indicating a relative maturity in handling unstructured text data.In the table modality, reasoning-augmented models demonstrate a clear advantage, suggesting that explicit reasoning mechanisms and the ability to process structured data significantly benefit table understanding.</p>
<p>In contrast, general LLMs show weaker performance on tables, implying challenges in leveraging tabular structure with traditional language modeling approaches.Similarly, for KG data, models with reasoning enhancements again lead, reflecting their ability to leverage relational and graphstructured information effectively.Additionally, domain-specific scientific models consistently underperform across all three modalities compared to general-purpose or reasoning-augmented models.</p>
<p>Further Discussions</p>
<p>Our experimental results highlight three key discrepancies in the performance of LLMs on scientific context understanding tasks, underscoring fundamental challenges that require further advancements.</p>
<p>Competency Discrepancy</p>
<p>The evaluation results reveal notable disparities across the four core competencies.While top-performing models exhibit relatively strong capabilities in iden-  tifying relevant information, they struggle with information-absence detection-the ability to abstain from answering when faced with unreliable or insufficient evidence.This suggests that models prioritize generating responses over ensuring accuracy, increasing the risk of hallucinations in scientific applications where factual correctness is critical.To address this, models should incorporate uncertainty quantification techniques, such as confidence-based rejection mechanisms and calibrated probability outputs, to enhance their ability to detect and reject misleading retrievals.Furthermore, reinforcement learning with human feedback and verification-based prompting strategies could help improve the model's reliability in rejecting incorrect information.
B -i t Q w e n 2 . 5 -7 B -i t G L M 4 -9 B -C h a t L l a m a 3 . 1 -8 B -i t G e m m a 2 -9 B -i t M i n i s t r a l -8 B -i t C h e m D F M -v 1 . 5 -8 B S c i G L M -6 B L l a S M o l -M i s t r a l -7 B C h e m L L M -7 B -c h a t Text Table KG
Modality Discrepancy LLMs exhibit relatively better performance on unstructured text compared to structured tables and KGs.This suggests that existing models rely heavily on linguistic patterns and semantic context rather than structured inference and multi-modal data integration.The weaker performance on tables and KGs indicates a bottleneck in structured data comprehension, where models struggle to extract, synthesize, and infer information effectively from unstructured data.To bridge this gap, models need improved cross-modal alignment, integrating structured data reasoning into their training paradigm.Techniques such as joint pretraining on text, tables, and graphs could enhance structured data understanding.</p>
<p>Specialized vs. General Model Discrepancy</p>
<p>Although scientific LLMs are explicitly designed for knowledge-intensive tasks, our evaluation shows that they often fail to outperform generalpurpose models on our dataset.This suggests that current specialized models lack sufficient reasoning depth, robustness, and flexibility to fully lever-age domain knowledge in complex scientific contexts.Their narrower training scope may limit generalization across diverse data modalities and reasoning challenges.To improve their contextual understanding, scientific models should incorporate targeted fine-tuning using curated scientific evidence and adopt domain-aware prompt engineering strategies.These approaches can help balance deep specialization with the adaptability required to tackle a broad range of scientific tasks, enhancing their effectiveness across diverse scenarios.</p>
<p>Conclusion</p>
<p>In this work, we introduced SciCUEval, a comprehensive dataset for evaluating context understanding capabilities in large language models within scientific domains.SciCUEval encompasses multiple data modalities (structured tables, knowledge graphs, and unstructured text), spanning diverse scientific disciplines.By systematically assessing four key competencies (Relevant Information Identification, Information-absence Detection, Multisource Information Integration, and Context-aware Inference), we provide a unified framework to quantify how effectively LLMs perform on scienceintensive tasks.Our experimental findings reveal that, despite notable progress, existing models encounter substantial challenges in accurately interpreting scientific data.The primary challenge lies in the inherent complexity of scientific data, particularly structured formats such as tables and knowledge graphs, which demand high specialization, precise contextual understanding, and the ability to synthesize fragmented and implicitly related information.Even state-of-the-art LLMs show limitations in fully mastering these skills, underscoring the need for significant advancements to enhance their scientific context understanding.</p>
<p>Appendix A More Results on SciCUEval</p>
<p>Table 4 and 5 present the quantitative evaluation results of LLMs across four competencies and three modalities on SciCUEval, respectively.Table 6 presents the more detailed results of SciCUEval.</p>
<p>Table 7 shows the performance comparison between direct answering and answering with context.The results demonstrate that the integration of context consistently enhances performance.</p>
<p>B Prompts for Data Generation</p>
<p>We present distinct prompt templates for each of the four capabilities below.</p>
<p> A prompt for generating questions about relevant information identification System Message: You're a brilliant in scientific domain.</p>
<p>User Message:</p>
<p>You will be provided with several triples from PriKG that form a path connecting a starting point to an endpoint.Based on this path, you need to generate a scientific question designed to test the respondent's ability to find the correct answer in the noise, with information from the knowledge C Data Quality Verification LLM as a Judge: We use advanced LLMs (e.g., GPT-4o) as automated evaluators to verify that each generated answer is both extractable and logically deducible from the relevant context, ensuring factual consistency and relevance.The prompt is presented below.</p>
<p>System Message:</p>
<p>You're a highly capable evaluator in the scientific domain.</p>
<p>User Message:</p>
<p>Below is a question, its relevant context, and an answer.Your task is to verify whether the answer meets the following standard:</p>
<p>1.The answer must be explicitly extractable or logically deducible from the provided context.</p>
<ol>
<li>
<p>The answer must adhere strictly to the relevant information in the context and be factually correct.</p>
</li>
<li>
<p>If the answer meets the standard, output "Yes".If it does not meet the standard, output "No".
[Relevant Context start] {Context} [Relevant Context end] [Question start] {Question} [Question end] [Answer start] {Answer} [Answer end]
Please evaluate and output either "Yes" or "No" based on the above criteria.</p>
</li>
</ol>
<p>Human Expert Evaluation: To further ensure the quality and accuracy of the generated data, we subjected the data that passed the initial LLM validation to manual review by five PhD-level researchers with strong STEM backgrounds.These experts were tasked with thoroughly evaluating each instance based on the following three criteria:</p>
<ol>
<li>
<p>Whether the question effectively tests the intended competency, ensuring that it is aligned with the targeted skill or knowledge domain and accurately reflects the underlying construct it aims to assess.</p>
</li>
<li>
<p>Whether the question is expressed clearly and logically, such that its wording is unambiguous, coherent, and easily understood by both human evaluators and automated systems, thereby minimizing potential misinterpretations 3. Whether the given contexts fully support the given answer and is factually correct, which requires that the answer not only directly derives from or can be logically inferred based on the supporting materials, but also adheres to facts and scientific evidence.Together, these criteria are designed to ensure the evaluation process's validity, clarity, and reliability.</p>
</li>
</ol>
<p>Only instances that received "Yes" for all three criteria were accepted.After the experts reviewed all instances, the results revealed that 90.83% of the instances met the required high-quality standards.</p>
<p>We invited five PhD-level researchers with STEM backgrounds, including two domain experts in bioinformatics.We compensated them based on the number of questions reviewed.We paid $30 for every 100 questions reviewed, totaling $3,300 for 11k questions.The entire review process took one week.</p>
<p>D Dataset Examples</p>
<p>In this part, we demonstrate several examples of questions aligned with four core competencies.</p>
<p>For each competency, we present three examples corresponding to three distinct data modalities.User Message: How are the genes "nbc 1" and "nbc 3" related?</p>
<p>User Message:</p>
<p>Based on the methods and results described in the first part of the study on epitaxial growth of GaAs on Si(001), which of the following is the most plausible reasoning for the effectiveness of the GaSb buffer layer in reducing defect densities such as threading dislocations and antiphase boundaries in the GaAs layer?(A) The antimonides, such as GaSb, have a significant lattice mismatch with silicon, leading to the generation of interfacial misfit dislocation arrays that efficiently alleviate strain without forming threading dislocations.(B) The presence of the GaSb buffer layer increases the thickness of the overall film, which inherently reduces the formation of threading dislocations and antiphase boundaries in the GaAs layer.(C) The GaSb buffer layer chemically reacts with silicon to form a new compound at the interface, which serves as an ideal seed layer for epitaxial GaAs growth, minimizing defect densities.(D) The GaSb buffer layer promotes planar defects, such as stacking faults, that counterbalance and neutralize threading dislocations and antiphase boundaries in the GaAs layer.</p>
<p>User Message:</p>
<p>Given that there exists a shared intermediate term, fill in the blank: GO:0003399 (cytoneme morphogenesis) _____ GO:0048858 (cell projection morphogenesis).</p>
<p>[GO:0003399, is_a</p>
<p>G Case Studies</p>
<p>In this section, we provide several typical bad cases by LLMs.</p>
<p>Ability: Relevant Information Identification</p>
<p>Question:</p>
<p>Could you determine the chemical formula for the compound identified as mp-775760?</p>
<p>Correct Answer: "LiFeF3"</p>
<p>Prediction of GPT-4o-mini: "C17H20ClN3O2S"  Prediction of GPT-4o: "LiFeF3"  Remarks: GPT-4o-mini accurately identified the target column and provided a chemical formula as the response; however, it incorrectly identified the context data row, leading to a mismatch between the generated formula and the corresponding Material ID.</p>
<p>Ability: Relevant Information Identification</p>
<p>Question:</p>
<p>Ability: Information-absence Detection</p>
<p>Question:</p>
<p>Can you enumerate all the PMIDs related to the interaction between id: 25840 and id: 1528?</p>
<p>Correct Answer:</p>
<p>"I cannot answer the question due to insufficient information in the retrieved data."</p>
<p>Prediction of Claude-3.5-Sonnet:"16239215, 15604093"  Remarks: Claude-3.5-Sonnetfailed to detect the absence of question-relevant context in context.Instead, it identified an incorrect Context Row in KG as the relevant context, and thus did not refuse to answer the question, but rather provided an incorrect answer.</p>
<p>Ability: Multi-source Information Integration</p>
<p>Question:</p>
<p>What are all the pairs of entity names that have a Gene-Gene relationship type?</p>
<p>Correct Answer: "cyp4f2,ggcx", "hras,kdr", "cyb5r3,cyb5a" Prediction of SciGLM-6B: "Gene", "Gene"  Remarks: SciGLM-6B failed to provide the correct answer and merely repeated the vocabulary from the question.It also failed to output the response in the required format.mp-1205400,mp-1219471,mp-1219958,mp-1221742,mp-1222109,mp-1233960,mp-1245579,mp-1272454,mp-1372845,mp-1406912,mp-14107,mp-1411625,mp-1517069,mp-1518293,mp-1518633,mp-15644,mp-1638589,mp-17955,mp-18026,mp-18027,mp-18028,mp-18029,mp-18030,mp-18031,mp-18032,mp-18033,mp-18034,mp-18035,mp-18036,...  Prediction of Qwen2.5-7B-it:mp-1219958  Remarks: The Qwen3-8B model with thinking activation achieved 100% precision and recall by accurately identifying all three correct material IDs.In contrast, the non-thinking variant generated numerous false positives, indicating a substantial decline in performance.The Chain-of-Thought (CoT) mechanism effectively directed the model's attention to relevant fields-specifically, by constraining the search explicitly to the "Space Group Symbol" column-thereby preventing the inclusion of erroneous data from unrelated columns or positional artifacts.Moreover, CoT activation suppressed redundant and irrelevant output patterns.</p>
<p>Figure 1 :
1
Figure 1: Overview of the SciCUEval dataset.It spans five scientific domains, supports three data modalities (structured tables, knowledge graphs, and unstructured text), and includes four question types.Data are collected from high-quality scientific sources.The dataset enables evaluation across four key competencies: (1) relevant information identification, (2) information-absence detection, (3) multi-source information integration, and (4) context-aware inference.</p>
<p>[</p>
<p>interleukin 1 receptor antagonist protein, L , shock]</p>
<ol>
<li>1
1
Experimental Setup Models We select 18 advanced LLMs, including 3 proprietary models (GPT-4o (OpenAI et al., 2024), Claude-3.5-Sonnet(Anthropic, 2024), GPT-4o-mini), 11 open-source general-purpose models (DeepSeek-V3 (DeepSeek-AI et al., 2024), DeepSeek-R1 (Guo et al., 2025), Qwen2.5-7B-Instruct(Qwen et al., 2025), Qwen3-8B (with explicit thinking) (Yang et al., 2025), Llama3.1-8B-Instruct,Llama3.1-70B-Instruct(Dubey et al., 2024), Llama-4-Maverick-17B-128E-Instruct, Llama-4-Scout-17B-16E-Instruct (Meta, 2025) , Ministral-8B-Instruct (Jiang et al., 2023), GLM4-9B-Chat(GLM et al., 2024), Gemma2-9B-it(Team et al., 2024), 4 open-source scientific-domain models (SciGLM-6B(Zhang et al., 2024a), LlaSMol-Mistral-7B(Yu et al.,</li>
</ol>
<p>Figure 3 :
3
Figure 3: Performance of LLMs across four competencies on SciCUEval.</p>
<p>Figure 4 :
4
Figure 4: Performance of LLMs across three modalities on SciCUEval.</p>
<p>Please answer the scientific questions based on the content.Your answer only needs to include the one or more correct option labels, not the full options.You should give your answer directly without any other characters.User Message: What is the primary objective of the statistical framework proposed in the paper 'Augmented Doubly Robust Post-Imputation Inference for Proteomic Data'?(A) To develop a method for directly measuring protein abundances without missing values.(B) To create a statistical framework that offers valid and efficient inference for proteomic data by addressing the challenge of missing values.(C) To replace the Plugin method with a simpler imputation strategy that discards missing values.(D) To develop a tool that solely relies on low-dimensional covariates for analyzing proteomic data.Corpus 1 ......... (Irrelevant Content) Corpus 2 ......... (Irrelevant Content) Corpus 3 ......... (Correct Content) Corpus 4 ......... (Irrelevant Content) Corpus 5 ......... (Irrelevant Content) Please answer the scientific questions based on the content.You should give your answer directly without any other characters.User Message: For the material with CID 13182, what is its inchikey?cid, mw, mf, xlog... inchikey ... exactmass CID XXXXX .......................................................  CID 13182 .......................................................  CID XXXXX .......................................................  CID XXXXX .......................................................  Please answer the scientific questions based on the content.You should give your answer directly without any other characters.User Message:How is the gene or protein known as 'GDPD3' connected to the anatomical structure called the 'lymph node'?Please answer the scientific questions based on the content.Your answer only needs to include the one or more correct option labels, not the full options.You should give your answer directly without any other characters.User Message:What key feature of elliptically geared isostatic metamaterials enables their nonlinear topological transitions?(A) The unique soliton-induced mechanical deformation in linear gear mechanisms.(B) The nonlinear Berry phase transition facilitated by geometric nonlinearity.(C) The presence of circular gear geometry that allows reversible deformation.(D) The linear topological index change due to minor gear rotations.Corpus 1 ......... (Irrelevant Content) Corpus 2 ......... (Irrelevant Content) Corpus 3 ......... (Irrelevant Content) Corpus 4 ......... (Irrelevant Content) Corpus 5 ......... (Irrelevant Content) Expected Answer: I cannot answer the question due to insufficient information in the retrieved data.</p>
<p>Material ID, Formula ... Sites ... Volume, Density mp-xxxxx .......................................................  mp-xxxxx .......................................................  mp-xxxxx .......................................................  mp-xxxxx .......................................................  Expected Answer: I cannot answer the question due to insufficient information in the retrieved data.Knowledge Graph System Message: Please answer the scientific questions based on the content.You should give your answer directly without any other characters.</p>
<p>[</p>
<p>x_name, relation, y_name] [Stiripentol, drug_drug, Sumatriptan ]  [GDPD3, anatomy_protein_present, lymph node]  [TROAP, protein_protein, NBPF19]  [DB00351, drug_drug, Reserpine ]  Expected Answer: I cannot answer the question due to insufficient information in the retrieved data.</p>
<p>Please answer the scientific questions based on the content.Your answer only needs to include the one or more correct option labels, not the full options.You should give your answer directly without any other characters.User Message:Based on the findings of the study, what is the primary long-term effect of local SBRT/IL-12 therapy on the bone marrow of treated mice?(A) A permanent increase in hematopoietic stem cells (HSCs).(B) A transient increase in IL-12 levels followed by long-term activation of myeloid cells.(C) A significant reduction in hematopoietic stem cells (HSCs) accompanied by skewing toward a myeloid lineage bias.(D) A substantial increase in IL-12 and IFN concentrations in the bone marrow.Corpus 1 ......... (Irrelevant Content) Corpus 2 ......... (Correct Content) Corpus 3 ......... (Correct Context) Corpus 4 ......... (Irrelevant Content) Corpus 5 ......... (Irrelevant Content) Please answer the scientific questions based on the content.You should give your answer directly without any other characters.User Message: Given the following isotopes ID: NDS-54874, NDS-30453, NDS-69167, NDS-58315, tell me which isotopes has the largest energy?id, Z, N, symbol... energy[kev]... relative intensity NDS-XXXXX .......................................................  NDS-30453 .......................................................  NDS-58315 .......................................................  NDS-XXXXX .......................................................  NDS-69167.......................................................  NDS-XXXXX .......................................................  NDS-54874.......................................................  Please answer the scientific questions based on the content.You should give your answer directly without any other characters.User Message: Could you list the substances that have the potential to interact with DB131_HUMAN?Please answer the scientific questions based on the content.Your answer only needs to include the one or more correct option labels, not the full options.You should give your answer directly without any other characters.</p>
<p>Corpus 1 ......... (Irrelevant Content) Corpus 2 ......... (Correct Content) Corpus 3 ......... (Irrelevant Content) Corpus 4 ......... (Irrelevant Content) Corpus 5 ......... (Irrelevant Content) Expected Answer: ATable System Message: Please answer the scientific questions based on the content.Your answer only needs to include the one or more correct option labels, not the full options.You should give your answer directly without any other characters.User Message: Comparing materials mp-760154 and mp-1208151, which statement is correct?(A) Both materials have identical band gaps and belong to the same crystal system.(B) The material mp-1208151 has a much larger volume and higher density than mp-760154.(C) The material mp-760154 is metallic, while mp-1208151 is semiconducting.(D) Both materials are predicted to be stable with similar formation energies.Material Formula ... Sites ... Volume, Density mp-xxxxx .......................................................  mp-760154 .....................................................  mp-xxxxx .......................................................  mp-1208151 .....................................................  mp-xxxxx .......................................................  Please answer the scientific questions based on the content.You should give your answer directly without any other characters.</p>
<p>Llama4-Maverick failed to correctly identify the corresponding relation in the knowledge graph and provided a completely irrelevant answer ("C"), whereas DeepSeek-V3 responded correctly.</p>
<p>The result from ChemLLM-7B-Chat is entirely unrelated to the question.For LLMs with weaker context understanding capabilities and instruction-following abilities, the occurrence of such responses is a key reason for their poor performance.Ability: Context-aware InferenceQuestion:Based on the findings of the study, how do ovarian hormones in females influence the metabolic reprogramming effects of Schistosoma mansoni infection on bone marrow-derived macrophages (BMDM)?Options: A.Ovarian hormones enhance the metabolic reprogramming... B.Ovarian hormones do not affect the metabolic... C.Ovarian hormones inhibit the metabolic reprogramming... D.Ovarian hormones cause an increase in glycolysis... Correct Answer: C Prediction of ChemDFM-v1.5-8B:None Error: This model's maximum context length is 8192 tokens.However, you requested 13432 tokens in the messages, Please reduce the length of the messages.Remarks: Some individual papers exceed the maximum length limit of certain models.In such cases, we can only classify them as errors.Ability: Context-aware InferenceQuestion:Based on the first part of the article, what conclusions can be inferred about the role of surface imperfections in the anisotropic Rashba effect observed in the 2D Janus XA2Y monolayers, and what implications might this have for spintronic applications?-4o demonstrates some reasoning ability and selected a correct answer; however, it failed to identify all the correct answers in a multiple-choice question.Ability: Context-aware InferenceQuestion: List all the material IDs that have Space Group Symbol as "Amm2".Correct Answer: mp-1017367, mp-1021395, mp-1219958 Prediction of Qwen3-8B with thinking: <think>Okay, so I need to...</think> mp-1219958,mp-1021395,mp-1017367  Prediction of Qwen3-8B without thinking:</p>
<p>Based on the methods and experimental results of the article, which factors could likely contribute to the more effective anti-PD1 response in the orthotopic murine model of mismatch repair-deficient lung adenocarcinoma compared to the subcutaneous model?A: The subclonality of tumor cells resulting in increased clonal neoantigen frequency in the orthotopic model.B: Higher overall immune cell infiltration, particularly T cells and dendritic cells, in the orthotopic lung environment.C: A similar proportion of tumor-associated macrophages and CD8+ T cells but different responses to anti-PD1 treatment in the subcutaneous model.D: Lower mutational burden resulting from fewer unique variants in the orthotopic model, affecting the immune response.Correct Answer: A Prediction of DeepSeek-V3: B  Prediction of DeepSeek-R1: <think>Alright, let's tackle...</think> A Remarks: In the task of context-aware inference within unstructured text, DeepSeek-R1 demonstrates stronger reasoning capabilities than DeepSeek-V3.This indicates that reasoning models also possess significant advantages in understanding unstructured text.Ability: Multi-source Information Integration Question: What entity engages in interactions with the entities: epilepsy and diabetes mellitus?Correct Answer: gabapentin Prediction of DeepSeek-R1: <think>Okay, let's...</think> No entity in the provided data interacts with both epilepsy and diabetes mellitus. Remarks: Even the well-performing reasoning model DeepSeek-R1 struggles to answer complex questions involving knowledge graph multi-source information integration.</p>
<p>Table 1 :
1
Comparison of SciCUEval with existing benchmark datasets.Question Types: QA (Question Answering), MCQ (Multiple Choice Question), T/F (True/False Question), and CC(Cloze Completion).
DatasetsContextsDomainsData ModalitiesQuestion TypesEvaluation Competencies# NumsLongICLBench (Li et al., 2024)GeneralTextQAIdentification2,618LongBench (Bai et al., 2023b)General, CodeTextQAIdentification, Integration4,750LongBench V2 (Bai et al., 2024)General, Law, FinanceTextMCQIdentification, Integration, Inference503RGB(Chen et al., 2024)GeneralTextQAIdentification, Detec., Integration, Inference1,000ChemLit-QA (Wellawatte et al., 2025)ChemistryTextQAIdentification, Detec., Inference1,054CHEMRAG-BENCH (Zhong et al., 2025)ChemistryTextQA, MCQIdentification, Inference1,932SciCUEvalComprehensive ScienceText, Table, KGQA, MCQ, T/F, CC Identification, Detec., Integration, Inference11,343</p>
<p>Table</p>
<p>Recent studies have shown that honeybees flying through short, narrow tunnels with visually textured walls perform waggle dancesa ...
Selected FragmentsGenerationSelected Rowscidmvmfpolarareacomplexityxlogp561513 419.19C13H8F11NO238.3525.04.9Selected Triples</p>
<p>Text KG Final Dataset Question Context Answer Question
What can be inferred about theinteraction between Beauvericin(BEA) and Cathepsin B (CTSB)?Options:A. BEA acts as a com...B. BEA binds to a s..C. BEA's binding to C...D. BEA's inhibitory me...AnswerBData SourceSelected EntryLLM as a Judge +Similarity Retrieval Top-k Noisy Entries(Target + Noise)Human Validation   ...... ...... ...... XNoise InjectionQuality ControlFigure 2: Illustration of data generation pipeline inSciCUEval, mainly consisting of question generation,noise injection, and quality control.</p>
<p>Table 2 :
2
Statistic of the SciCUEval dataset, which comprises ten sub-datasets derived from diverse scientific data.The detailed data sources are listed in Appendix E.
Sub-dataset DomainSourceModality #Info. Indent. # Abs. Detec. # Info. Integ. # Con. Infer. # TotalMatTextMaterialsarXivText216146222356940BioTextBiologyBiorxivText23697318317968MatTabMaterialsMaterial ProjectTable299150287200936IaeaTabPhysicsIAEATable4422222861801130ProtTabBiologyPubchemTable4962493271801252MolTabChemistryPubchemTable5162593501801305GoKGBiologyGene OntologyKG5072542391801180HipKGBiologyHIPPIEKG4702363191401165PhaKGBiomedicine PharmKGKG5122562811681217PriKGBiomedicine PrimeKGKG4102053822531250</p>
<p>Table 3 :
3
Performance of LLMs across ten sub-datasets on SciCUEval.Underline results indicate the best results among all models.Bold results indicate the best results in each category.
ModelsMatTab IaeaTab MolTab ProtTab PhaKG PriKG HipKG GoKG BioText MatText OverallGPT-4o68.7956.5555.7952.6455.7154.8068.5074.3279.0364.5761.52GPT-4o-mini40.7138.8546.6744.5740.5952.6465.2073.1479.2465.0054.57Claude-3.5-Sonnet48.4842.0367.9152.2250.9445.9675.7884.0758.0661.4959.20DeepSeek-R173.7171.8974.6972.4458.6658.2069.6679.1874.7963.0969.72Qwen3-8B63.1459.2070.8069.3355.1654.4874.6873.9869.7355.1164.69DeepSeek-V356.6254.0759.8552.0852.1851.9263.4272.2966.7445.3157.50Llama4-Maverick46.4747.7948.2043.6148.3249.2864.8172.7163.0254.1553.65Llama4-Scout48.9347.7046.9046.1739.7748.0859.5766.2761.8848.5151.16Llama3.1-70B-it38.2539.7344.4441.2944.7044.0059.3170.1766.5351.9149.80Qwen2.5-7B-it28.1032.6543.3039.4636.1545.6053.9962.4668.1859.6846.62GLM4-9B-Chat31.4125.8447.8243.4536.0344.5657.9460.5167.7750.9646.46Llama3.1-8B-it28.8534.3442.7639.7838.2946.5652.6259.3264.2649.3645.50Gemma2-9B-it32.9132.2142.9137.2237.3950.4856.5757.2937.7729.6742.21Ministral-8B-it23.0819.1235.5637.3822.7637.9248.5152.8848.1445.3237.58ChemDFM-v1.5-8B33.6531.1535.5636.8240.4330.7249.7056.4426.1118.9136.80SciGLM-6B11.8611.5017.7014.9419.5620.8821.4628.3144.1731.3521.58LlaSMol-Mistral-7B13.3512.8316.5514.7021.5419.8422.8329.9233.1320.9820.42ChemLLM-7B-chat3.426.028.818.1513.455.925.1515.5139.9422.6712.16</p>
<p>Table 4 :
4
Evaluation results of LLMs across four competencies on SciCUEval.Underline results indicate the best results among all models.Bold results indicate the best results in each category.
ModelsInfo. Ident. Detec. Integ. Infer. Abs. Info. Con. OverallGPT-4o89.72 19.51 54.90 65.9761.52GPT-4o-mini77.81 14.71 47.54 57.6854.57Claude-3.5-Sonnet82.95 49.10 50.85 47.2959.20DeepSeek-R194.13 11.75 72.78 79.4469.72Qwen3-8B88.29 43.57 53.87 64.3864.69DeepSeek-V390.806.0549.80 60.5357.50Llama4-Maverick77.617.1654.16 58.5253.65Llama4-Scout71.38 25.42 43.65 54.9551.16Llama3.1-70B-it81.056.8745.44 47.7649.80Qwen2.5-7B-it69.929.0242.95 50.3446.62GLM4-9B71.482.5350.78 43.8246.46Llama3.1-8B-it75.345.8841.47 39.9945.50Gemma2-9B-it66.972.3828.74 48.2242.20Ministral-8B-it56.804.7631.32 39.6237.58ChemDFM-v1.5-8B 45.49 19.31 22.23 46.4036.80SciGLM-6B33.249.0118.00 29.4821.58LlaSMol-Mistral-7B 31.966.8314.63 26.5920.42ChemLLM-7B-Chat 20.294.0916.857.5712.16</p>
<p _Answer_="&quot;[Answer]&quot;" _Question="&quot;[Question" _answer_:="&quot;answer&quot;:" _question_:="&quot;question&quot;:" _question_type_:="&quot;question_type&quot;:" or="or" rejection_="rejection]&quot;," type_="type]&quot;,">Table 5 :
5
Evaluation results of LLMs across three modalities on SciCUEval.Underline results indicate the best results among all models.Bold results indicate the best results in each category.
ModelsTextTableKGOverallGPT-4o71.91 55.91 63.1361.52GPT-4o-mini72.22 42.98 55.8454.57Claude-3.5-Sonnet59.75 53.41 64.9959.20DeepSeek-R169.00 73.19 66.6469.72Qwen3-8B62.53 66.02 64.2764.69DeepSeek-V356.18 55.68 59.7757.50Llama4-Maverick58.65 46.51 58.5453.65Llama4-Scout55.29 47.31 53.2251.16Llama3.1-70B-it59.33 41.19 54.3049.80Qwen2.5-7B-it69.93 36.58 49.3846.24GLM4-9B-Chat59.49 37.94 49.4646.46Llama3.1-8B-it56.92 37.08 49.0645.50Gemma2-9B-it34.27 36.73 50.2342.21Ministral-8B-it46.75 29.50 41.2437.58ChemDFM-v1.5-8B 22.92 34.44 44.0536.80SciGLM-6B38.48 14.25 22.5121.58LlaSMol-Mistral-7B 27.74 14.49 23.4520.42ChemLLM-7B-chat 32.286.8610.0112.16graph. The question types can be Q&amp;Aor fill-in-the-blank. The answers to QAquestions should be simple, concise, andeasily verifiable phrases, not long sentences.Start Node: {start_node}End Node: {end_node}Path: {data['path']}Triples:{data['triplets']}Please generate a scientific question basedon this information. Ensure that the questionrequires the respondent to find the correctanswer in the noise in the knowledge graphand the difficulty level should be moderate.Please output the question in JSON formatonly. Do not output anything other than theJSON format. The JSON format should looklike this:</p>
<p>Table 6 :
6
Detailed evaluation results of LLMs across four competencies on the ten sub-datasets of SciCUEval
ModelsMatTab Info. Ident. Abs. Detec. Info. Integ. Con. Infer.AllIaeaTab Info. Ident. Abs. Detec. Info. Integ. Con. Infer.AllGPT-4o88.1658.0029.9764.0068.7981.0026.1344.7652.7856.55GPT-4o-mini71.9139.3310.1039.0040.7157.015.8637.0637.7838.85Claude-3.5-Sonnet77.2156.4619.5741.0048.4856.3123.2937.6837.7842.03DeepSeek-R197.3016.2269.0188.0073.7191.8216.3676.0684.4471.89Qwen3-8B82.9490.0019.1676.5063.1468.7856.7641.9666.1159.20DeepSeek-V393.3128.0024.7469.0056.6279.865.4144.7665.5654.07Llama4-Maverick69.909.3323.0073.0046.4766.2912.6139.5158.8947.79Llama4-Scout53.8573.3315.6871.0048.9359.0534.2336.3654.4447.70Llama3.1-70B69.5726.006.9745.5038.2562.446.3130.7739.4439.73Qwen2.5-7B-instruct48.839.333.1447.0028.1045.708.1132.8730.5632.65GLM4-9B-Chat52.842.6718.8239.0031.4129.640.4540.5624.4425.84Llama3.1-8B58.534.674.8837.0028.8553.856.3125.5234.4434.34Gemma2-9B-it65.552.006.9744.5032.9150.232.7016.7848.8932.21Ministral-8B-it44.152.003.1436.0023.0818.789.019.7947.2219.12ChemDFM-v1.5-8B42.8110.679.4172.0033.6436.6514.4121.3353.8931.15SciGLM-6B5.691.332.7942.0011.8611.990.903.8535.5611.50LlaSMol-Mistral-7B7.695.332.7943.0013.3516.296.311.7530.0012.83ChemLLM-7B-Chat1.672.670.0011.503.429.280.454.906.676.02ModelsMolTab Info. Ident. Abs. Detec. Info. Integ. Con. Infer.AllProtTab Info. Ident. Abs. Detec. Info. Integ. Con. Infer.AllGPT-4o91.099.2730.2971.1155.7990.5214.4618.9662.2252.64GPT-4o-mini68.9913.1332.2958.8946.6772.589.2419.2762.2244.57Claude-3.5-Sonnet92.8458.5942.5360.5667.9177.0831.4320.1270.0052.22DeepSeek-R196.9014.2975.8693.3374.6996.7717.7465.4393.3372.44Qwen3-8B85.8562.1646.0088.3370.8089.7262.2532.7289.4469.33DeepSeek-V394.385.7939.7177.7859.8596.576.0222.9446.1152.08Llama4-Maverick73.062.7039.4359.4448.2071.771.2022.6362.7843.61Llama4-Scout60.479.2736.5782.2246.9064.9216.4726.6171.1146.17Llama3.1-70B71.901.9330.2954.4444.4479.442.8114.0738.8941.29Qwen2.5-7B-instruct57.755.4137.7168.3343.3061.699.6419.8855.0039.46GLM4-9B66.860.3950.2956.6747.8260.892.0144.3451.1143.45Llama3.1-8B-instruct70.740.0031.1446.6742.7667.742.4121.1048.3339.78Gemma2-9B-it63.570.3936.8656.6742.9162.700.8018.6551.1137.22Ministral-8B-it51.551.1631.7146.6735.5659.071.2023.2453.3337.38ChemDFM-v1.5-8B38.9533.2022.2955.0035.5638.9134.9419.2765.5636.82SciGLM-6B23.262.328.8641.1117.7019.763.216.4233.3314.94LlaSMol-Mistral-7B22.485.7910.0027.7816.5522.185.623.9826.1114.70ChemLLM-7B-Chat11.630.0014.003.338.8113.310.009.482.788.15ModelsPriKG Info. Ident. Abs. Detec. Info. Integ. Con. Infer.AllHipKG Info. Ident. Abs. Detec. Info. Integ. Con. Infer.AllGPT-4o70.9824.8841.6272.7354.8097.0233.9043.2688.5768.50GPT-4o-mini74.8827.8031.1569.1752.6483.5120.3443.2667.8665.20Claude-3.5-Sonnet63.6628.2929.6656.1345.9697.8575.3243.2677.1475.78DeepSeek-R177.4515.6943.1684.1358.2097.443.3968.3594.2969.66Qwen3-8B79.2730.2435.8662.0654.4894.4742.8057.9998.5774.68DeepSeek-V373.901.9540.5873.9151.9294.479.3252.6675.0063.42Llama4-Maverick69.764.3939.7966.8049.2887.0213.5669.5965.7164.81Llama4-Scout77.3220.9828.8051.7848.0880.4342.8047.6545.0059.57Llama3.1-70B-it74.889.2720.6857.3144.0091.0614.4142.9565.7159.31Qwen2.5-7B-it68.0522.9327.7554.5545.6060.643.3948.9062.1453.99GLM4-9B-Chat70.986.8327.2358.5044.5685.325.5152.9865.7157.94Llama3.1-8B-it75.3718.5427.7550.9946.5682.5510.5945.1440.0052.62Gemma2-9B-it78.786.8330.3770.3650.4891.493.8136.9972.8656.57Ministral-8B-it55.1213.6623.0452.1737.9266.601.2731.0347.5048.51ChemDFM-v1.5-8B42.2034.1510.4739.9230.7250.8572.4614.7387.1449.70SciGLM-6B42.934.394.7122.9220.8833.191.275.3352.8621.46LlaSMol-Mistral-7B33.9010.735.2426.4819.8426.388.479.7265.0022.83ChemLLM-7B-Chat8.053.417.591.985.923.621.278.1510.005.15</p>
<p>Table 7 :
7
Performance comparison on SciCUEval: Direct Answering vs. Answering with Contexts.Material ID, Formula ... Sites ... Volume, Density mp-xxxxx ......................................................... mp-xxxxx ......................................................... mp-xxxxx ......................................................... mp-xxxxx .........................................................
ModelMatTab Direct Context Direct Context Direct Context Direct Context Direct Context IaeaTab MolTab ProtTab PhaKGGPT-4o14.6468.7915.3156.5526.8255.7923.6452.6416.8155.71GPT-4o-mini15.3840.7118.6738.8525.5246.6724.8414.0140.59Claude-3.5-Sonnet15.2248.4823.4542.0332.9567.9131.0752.2226.6250.94DeepSeek-R16.3473.7116.0171.8912.6174.6910.2672.4411.5158.66Qwen3-8B14.1063.149.1259.2013.9570.8013.2669.3311.8555.16DeepSeek-V314.8256.6222.6554.0731.8859.8526.2052.0815.8052.18Llama4-Maverick25.5346.4720.3547.7935.3348.2039.7043.6118.3248.32Llama4-Scout25.4348.9328.9447.7046.2146.9039.7846.1718.4139.77Llama3.1-70B-it10.0438.2516.1939.7321.3044.4422.6041.2913.1544.70Qwen2.5-7B-it14.6428.1018.9432.6521.0743.3020.6939.4615.9336.15GLM4-9B-Chat12.6131.4116.7325.8422.5347.8222.2843.4513.5336.03Llama3.1-8B-it14.2128.8514.7834.3418.5442.7617.9739.7816.3538.29Ministral-8B-it0.8223.088.2919.128.0035.564.6637.3815.0522.76Gemma2-9B-it13.2532.9110.2732.2112.8742.9113.7437.2211.4537.39ChemDFM-v1.5-8B14.3833.6513.5431.1526.3635.5528.6336.8244.5340.43SciGLM-6B12.1811.8610.4411.5015.5617.7013.5814.9413.5619.56LlaSMol-Mistral-7B11.9713.3510.8812.8313.7116.5511.9814.7023.6221.54ChemLLM-7B-chat17.523.4213.456.0219.168.8115.738.1518.0113.45ModelPriKG Direct Context Direct Context Direct Context Direct Context Direct Context HipKG GoKG BioText MatTextGPT-4o17.4454.8014.4268.5043.4774.3253.4179.0341.2864.57GPT-4o-mini16.4852.6410.9965.2042.8073.1455.6879.2448.0965.00claude-3.5-sonnet26.8045.9621.5575.7845.5984.0755.6858.0641.6061.49DeepSeek-R110.2958.2010.6969.6631.4079.1856.4374.7945.3063.09Qwen3-8B12.8054.4810.8274.6831.4473.9848.7669.7339.0455.11Deepsee-V317.3351.9214.7663.4239.7572.2960.0766.7451.1845.31Llama4-Maverick20.1849.2817.8564.8143.8172.7161.7763.0254.7954.15Llama4-Scout23.8448.0821.9759.5737.5466.2755.5861.8842.9848.51Llama3.1-70B-it14.4044.0015.8859.3132.1270.1749.8066.5340.2151.91Qwen2.5-7B-it16.5645.609.8753.9933.6462.4647.1168.1836.8159.68GLM4-9B-Chat16.7244.5611.9357.9430.1760.5147.5267.7736.3850.96Llama3.1-8B-it16.2446.5614.5152.6235.5159.3247.3164.2637.3449.36Gemma2-9B-it15.3650.489.9656.5731.2757.2951.8137.7735.8829.67Ministral-8B-it15.0537.9213.2448.5128.2752.8841.8448.1432.2345.32ChemDFM-v1.5-8B33.6630.7230.2149.7039.8456.4450.8826.1130.8318.91SciGLM-6B15.2020.8818.8021.4625.9328.3133.4444.1721.6331.35LlaSMol-Mistral-7B15.5219.8420.1722.8323.3929.9233.8533.1323.5820.98ChemLLM-7B-chat16.805.9223.865.1527.8015.5145.9239.9430.4422.67</p>
<p>Table System Message:
System
Please answer the scientific questions based on the content.You should give your answer directly without any other characters.
User Message:For the material with ID mp-768851, what isits number of site?</p>
<p>Table 8 :
8
Detailed URL, description, and license of the source data involved in this paper.</p>
<p>bridqa: A dataset of multi-hop question answering over tabular and textual data.arXiv preprint arXiv:2004.07347.
Hippie v2. 0: enhancing meaningfulness and reliability of protein-protein interaction networks. Gregorio Alanis-Lobato, Miguel A Andrade-Navarro, Martin H Schaefer, Nucleic acids research. 9852016</p>
<p>Ai Anthropic, The Claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card. 2024</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, arXiv:2309.16609Qwen technical report. 2023aarXiv preprint</p>
<p>Longbench: A bilingual, multitask benchmark for long context understanding. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, arXiv:2308.145082023barXiv preprint</p>
<p>Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, arXiv:2412.15204Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks. 2024arXiv preprint</p>
<p>Iz Beltagy, Kyle Lo, Arman Cohan, arXiv:1903.10676Scibert: A pretrained language model for scientific text. 2019arXiv preprint</p>
<p>Building a knowledge graph to enable precision medicine. Payal Chandak, Kexin Huang, Marinka Zitnik, Scientific Data. 101672023</p>
<p>Benchmarking retrievalaugmented generation for medicine. Guangzhi Xiong, Qiao Jin, Zhiyong Lu, Aidong Zhang, arXiv:2402.131782024arXiv preprint</p>
<p>An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang ; Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, Zihan Qiu, arXiv:2505.09388Qwen3 technical report. Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu2025Preprint</p>
<p>LlaSMol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset. Botao Yu, Frazier N Baker, Ziqi Chen, Xia Ning, Huan Sun, arXiv:2402.093912024</p>
<p>SciGLM: Training scientific language models with self-reflective instruction annotation and tuning. Dan Zhang, Ziniu Hu, Sining Zhoubian, Zhengxiao Du, Kaiyu Yang, Zihan Wang, Yisong Yue, Yuxiao Dong, Jie Tang, arXiv:2401.079502024a</p>
<p>Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue, Dongzhan Zhou, arXiv:2402.06852ChemLLM: A chemical large language model. 2024b</p>
<p>Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Khai Moo, Xu Hao, Zhen Han, Shuo Leng Thai, Zhiyuan Wang, Liu, arXiv:2402.13718bench: Extending long context evaluation beyond 100k tokens. 2024carXiv preprint</p>
<p>Zihan Zhao, Da Ma, Lu Chen, Liangtai Sun, Zihao Li, Hongshen Xu, Zichen Zhu, Su Zhu, Shuai Fan, Guodong Shen, arXiv:2401.14818ChemDFM: Dialogue foundation model for chemistry. 2024</p>
<p>Pharmkg: a dedicated knowledge graph benchmark for bomedical data mining. Shuangjia Zheng, Jiahua Rao, Ying Song, Jixian Zhang, Xianglu Xiao, Evandro Fei Fang, Yuedong Yang, Zhangming Niu, Briefings in bioinformatics. 2243442021</p>
<p>Benchmarking retrieval-augmented generation for chemistry. Xianrui Zhong, Bowen Jin, Siru Ouyang, Yanzhen Shen, Qiao Jin, Yin Fang, Zhiyong Lu, Jiawei Han, arXiv:2505.076712025arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
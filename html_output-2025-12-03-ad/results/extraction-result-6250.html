<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6250 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6250</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6250</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-122.html">extraction-schema-122</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <p><strong>Paper ID:</strong> paper-cdcf3f36866ef1e16eba26d57c2324362247ba84</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cdcf3f36866ef1e16eba26d57c2324362247ba84" target="_blank">Zephyr: Direct Distillation of LM Alignment</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work applies distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment, Zephyr-7B, which sets the state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation.</p>
                <p><strong>Paper Abstract:</strong> We aim to produce a smaller language model that is aligned to user intent. Previous research has shown that applying distilled supervised fine-tuning (dSFT) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. To distill this property, we experiment with the use of preference data from AI Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment. The approach requires only a few hours of training without any additional sampling during fine-tuning. The final result, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation. In particular, results on MT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access RLHF-based model. Code, models, data, and tutorials for the system are available at https://github.com/huggingface/alignment-handbook.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6250.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6250.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-judge (MT-Bench / AlpacaEval)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Use of large LLMs (e.g., GPT-4, Claude) as automatic evaluators in MT-Bench and AlpacaEval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper uses GPT-4 (and cites Claude) as an automated judge for chat/instruction-following benchmarks (MT-Bench and AlpacaEval), relying on LLM-generated scores or pairwise comparisons in place of human raters; the authors note these LLM-based scores correlate with human ratings but carry important biases and limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Judging LLM-as-a-Judge with MT-Bench and chatbot arena</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multi-turn dialogue / instruction-following (chatbot evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4 (primary); Claude referenced as example evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>This paper did not run fresh human annotation for the main chat benchmarks — instead it relies on MT-Bench and AlpacaEval evaluations that are scored by GPT-4 (MT-Bench: GPT-4 rates responses 1–10; AlpacaEval: GPT-4 performs pairwise comparisons versus a baseline). The authors state that these benchmarks yield scores that correlate closely with human ratings, but they did not perform a direct human vs. LLM judge experiment themselves.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>MT-Bench mean score (GPT-4 1–10), AlpacaEval pairwise win-rate (GPT-4 comparisons); the paper references correlation between these LLM-derived metrics and human ratings (qualitative claim, not reproduced with new human data here).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>The authors state that MT-Bench and AlpacaEval scores correlate closely with human ratings in prior work, but caution that using GPT-4 as an evaluator introduces biases: evaluations can favor models distilled from the same teacher or those that produce verbose but potentially incorrect answers. They explicitly warn results should be interpreted with care and that some benchmark prompts (e.g., AlpacaEval) may not be representative of real-world usage.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Reported limitations include evaluator bias (GPT-4 tends to favor outputs similar to its own generative style or those distilled from it), preference for verbosity (verbose answers may be scored highly despite being incorrect), and the lack of representativeness of benchmark prompts; the paper emphasizes these limitations rather than attempting broad correction.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>The paper does not present detailed empirical examples of specific human vs. GPT-4 disagreement instances; instead it cites known biases (e.g., favouring models distilled from GPT-4 and verbose-but-wrong outputs) as a conceptual failure mode that can skew measured rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>The authors recommend interpreting LLM-judge-based benchmark results with care and highlight the need for future work (including human annotation) to validate and complement automated scoring. No concrete procedural mitigation is implemented in this work beyond cautioning readers and suggesting future investigation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Zephyr: Direct Distillation of LM Alignment', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6250.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6250.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UltraFeedback (GPT-4-scored AIF)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UltraFeedback: AI Feedback dataset where GPT-4 rates multiple model outputs to produce preferences</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper uses the UltraFeedback dataset, where GPT-4 scores four candidate model responses per prompt and binary preferences are constructed (top-scoring vs a random lower-scoring reply); these AI-derived preferences are used to train distilled direct preference optimization (dDPO) without human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>UltraFeedback: Boosting language models with high-quality feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Preference learning for alignment in chat/instruction-following</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>No human raters were used to create UltraFeedback for this paper; UltraFeedback as used here contains 64k prompts each with four model responses scored by GPT-4 according to instruction-following, honesty, and helpfulness; binary preference pairs were formed by taking the highest-scoring response and pairing it with a random lower-scoring response.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>The dataset provides GPT-4 numerical scores (used to pick the top response) and the paper uses those derived binary preferences as the training signal for dDPO. There is no direct comparison in this paper between GPT-4-derived preferences and human preference labels for UltraFeedback.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>The paper does not present a head-to-head quantitative comparison of UltraFeedback (GPT-4) preferences versus human preferences; it relies on prior claims that GPT-4-based judgments correlate with humans but also acknowledges the risk that GPT-4's scoring biases will propagate into the distilled model.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Limitations noted include propagation of GPT-4 scoring bias into the student model (favoring GPT-4-like outputs), potential promotion of verbosity or stylistic artifacts that GPT-4 prefers, and absence of human oversight which means safety-related harms may be insufficiently penalized by AI-only preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>The authors show qualitative safety-related examples where the dDPO model sometimes complies with harmful prompts (Figure 6) or provides polite but potentially unsafe responses; they tie this to the absence of human safety labels and suggest that AI-only feedback may fail to penalize certain unsafe behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>The paper suggests (qualitatively) that including red-teaming examples in the dDPO training data would likely improve safety capabilities; more generally the authors call for future work and human annotation for safety-critical curation, but do not implement large-scale human-based mitigations in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Zephyr: Direct Distillation of LM Alignment', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging LLM-as-a-Judge with MT-Bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>MT-Bench <em>(Rating: 2)</em></li>
                <li>AlpacaEval: An automatic evaluator of instruction-following models <em>(Rating: 2)</em></li>
                <li>UltraFeedback: Boosting language models with high-quality feedback <em>(Rating: 2)</em></li>
                <li>GPT-4 technical report <em>(Rating: 1)</em></li>
                <li>The false promise of imitating proprietary LLMs <em>(Rating: 1)</em></li>
                <li>Alpacafarm: A simulation framework for methods that learn from human feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6250",
    "paper_id": "paper-cdcf3f36866ef1e16eba26d57c2324362247ba84",
    "extraction_schema_id": "extraction-schema-122",
    "extracted_data": [
        {
            "name_short": "LLM-as-a-judge (MT-Bench / AlpacaEval)",
            "name_full": "Use of large LLMs (e.g., GPT-4, Claude) as automatic evaluators in MT-Bench and AlpacaEval",
            "brief_description": "The paper uses GPT-4 (and cites Claude) as an automated judge for chat/instruction-following benchmarks (MT-Bench and AlpacaEval), relying on LLM-generated scores or pairwise comparisons in place of human raters; the authors note these LLM-based scores correlate with human ratings but carry important biases and limitations.",
            "citation_title": "Judging LLM-as-a-Judge with MT-Bench and chatbot arena",
            "mention_or_use": "use",
            "task_domain": "Multi-turn dialogue / instruction-following (chatbot evaluation)",
            "llm_judge_model": "GPT-4 (primary); Claude referenced as example evaluator",
            "human_evaluation_setup": "This paper did not run fresh human annotation for the main chat benchmarks — instead it relies on MT-Bench and AlpacaEval evaluations that are scored by GPT-4 (MT-Bench: GPT-4 rates responses 1–10; AlpacaEval: GPT-4 performs pairwise comparisons versus a baseline). The authors state that these benchmarks yield scores that correlate closely with human ratings, but they did not perform a direct human vs. LLM judge experiment themselves.",
            "metrics_compared": "MT-Bench mean score (GPT-4 1–10), AlpacaEval pairwise win-rate (GPT-4 comparisons); the paper references correlation between these LLM-derived metrics and human ratings (qualitative claim, not reproduced with new human data here).",
            "reported_differences": "The authors state that MT-Bench and AlpacaEval scores correlate closely with human ratings in prior work, but caution that using GPT-4 as an evaluator introduces biases: evaluations can favor models distilled from the same teacher or those that produce verbose but potentially incorrect answers. They explicitly warn results should be interpreted with care and that some benchmark prompts (e.g., AlpacaEval) may not be representative of real-world usage.",
            "llm_specific_limitations": "Reported limitations include evaluator bias (GPT-4 tends to favor outputs similar to its own generative style or those distilled from it), preference for verbosity (verbose answers may be scored highly despite being incorrect), and the lack of representativeness of benchmark prompts; the paper emphasizes these limitations rather than attempting broad correction.",
            "notable_failure_cases": "The paper does not present detailed empirical examples of specific human vs. GPT-4 disagreement instances; instead it cites known biases (e.g., favouring models distilled from GPT-4 and verbose-but-wrong outputs) as a conceptual failure mode that can skew measured rankings.",
            "mitigation_strategies": "The authors recommend interpreting LLM-judge-based benchmark results with care and highlight the need for future work (including human annotation) to validate and complement automated scoring. No concrete procedural mitigation is implemented in this work beyond cautioning readers and suggesting future investigation.",
            "uuid": "e6250.0",
            "source_info": {
                "paper_title": "Zephyr: Direct Distillation of LM Alignment",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "UltraFeedback (GPT-4-scored AIF)",
            "name_full": "UltraFeedback: AI Feedback dataset where GPT-4 rates multiple model outputs to produce preferences",
            "brief_description": "The paper uses the UltraFeedback dataset, where GPT-4 scores four candidate model responses per prompt and binary preferences are constructed (top-scoring vs a random lower-scoring reply); these AI-derived preferences are used to train distilled direct preference optimization (dDPO) without human labels.",
            "citation_title": "UltraFeedback: Boosting language models with high-quality feedback",
            "mention_or_use": "use",
            "task_domain": "Preference learning for alignment in chat/instruction-following",
            "llm_judge_model": "GPT-4",
            "human_evaluation_setup": "No human raters were used to create UltraFeedback for this paper; UltraFeedback as used here contains 64k prompts each with four model responses scored by GPT-4 according to instruction-following, honesty, and helpfulness; binary preference pairs were formed by taking the highest-scoring response and pairing it with a random lower-scoring response.",
            "metrics_compared": "The dataset provides GPT-4 numerical scores (used to pick the top response) and the paper uses those derived binary preferences as the training signal for dDPO. There is no direct comparison in this paper between GPT-4-derived preferences and human preference labels for UltraFeedback.",
            "reported_differences": "The paper does not present a head-to-head quantitative comparison of UltraFeedback (GPT-4) preferences versus human preferences; it relies on prior claims that GPT-4-based judgments correlate with humans but also acknowledges the risk that GPT-4's scoring biases will propagate into the distilled model.",
            "llm_specific_limitations": "Limitations noted include propagation of GPT-4 scoring bias into the student model (favoring GPT-4-like outputs), potential promotion of verbosity or stylistic artifacts that GPT-4 prefers, and absence of human oversight which means safety-related harms may be insufficiently penalized by AI-only preferences.",
            "notable_failure_cases": "The authors show qualitative safety-related examples where the dDPO model sometimes complies with harmful prompts (Figure 6) or provides polite but potentially unsafe responses; they tie this to the absence of human safety labels and suggest that AI-only feedback may fail to penalize certain unsafe behaviors.",
            "mitigation_strategies": "The paper suggests (qualitatively) that including red-teaming examples in the dDPO training data would likely improve safety capabilities; more generally the authors call for future work and human annotation for safety-critical curation, but do not implement large-scale human-based mitigations in this study.",
            "uuid": "e6250.1",
            "source_info": {
                "paper_title": "Zephyr: Direct Distillation of LM Alignment",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging LLM-as-a-Judge with MT-Bench and chatbot arena",
            "rating": 2
        },
        {
            "paper_title": "MT-Bench",
            "rating": 2
        },
        {
            "paper_title": "AlpacaEval: An automatic evaluator of instruction-following models",
            "rating": 2
        },
        {
            "paper_title": "UltraFeedback: Boosting language models with high-quality feedback",
            "rating": 2
        },
        {
            "paper_title": "GPT-4 technical report",
            "rating": 1
        },
        {
            "paper_title": "The false promise of imitating proprietary LLMs",
            "rating": 1
        },
        {
            "paper_title": "Alpacafarm: A simulation framework for methods that learn from human feedback",
            "rating": 1
        }
    ],
    "cost": 0.012621750000000001,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ZEPhYR: DIRECT DISTILLATION OF LM ALIGNMENT</h1>
<p>Lewis Tunstall,<em> Edward Beeching,</em> Nathan Lambert, Nazneen Rajani,<br>Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra,<br>Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero,<br>Alexander M. Rush, and Thomas Wolf<br>The H4 (Helpful, Honest, Harmless, Huggy) Team<br>https://huggingface.co/HuggingFaceH4<br>lewis@huggingface.co</p>
<h4>Abstract</h4>
<p>We aim to produce a smaller language model that is aligned to user intent. Previous research has shown that applying distilled supervised fine-tuning (dSFT) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. To distill this property, we experiment with the use of preference data from AI Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment. The approach requires only a few hours of training without any additional sampling during fine-tuning. The final result, ZEPHYR7B, sets a new state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation. In particular, results on MT-Bench show that ZEPHYR-7B surpasses LLAMA2-CHAT-70B, the best open-access RLHFbased model. Code, models, data, and tutorials for the system are available at https://github.com/huggingface/alignment-handbook.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Model performance on MT-Bench. We compare ZEPHYR-7B, trained with distilled direct preference optimization (dDPO), to proprietary models as well as larger, open-access models like LLAMA2-CHAT-70B that were additionally trained using reinforcement learning on a large amount of human feedback.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>1 INTRODUCTION</h1>
<p>Smaller, open large language models (LLMs) have greatly increased in ability in recent years, from early GPT-2-like models (Wang \&amp; Komatsuzaki, 2021) to accurate and compact models (Touvron et al., 2023; Penedo et al., 2023; Jiang et al., 2023) that are trained on significantly more tokens than the "compute-optimal" amount suggested by the Chincilla scaling laws (De Vries, 2023). In addition, researchers have shown that these models can be further trained through distilled supervised fine-tuning (dSFT) based on proprietary models to increase their accuracy (Taori et al., 2023). In this approach, the output of a more capable teacher model is used as supervised data for the student model.</p>
<p>Distillation has proven to be an effective tool for improving open models on a range of different tasks (Chiang et al., 2023); however, it does not reach the performance of the teacher models (Gudibande et al., 2023). Users have noted that these models are not "intent aligned", i.e. they do not behave in a manner that aligns with human users' preferences. This property often leads to outputs that do not provide correct responses to queries.</p>
<p>Intention alignment has been difficult to quantify, but recent work has led to the development of benchmarks like MT-Bench (Zheng et al., 2023) and AlpacaEval (Li et al., 2023) that specifically target this behavior. These benchmarks yield scores that correlate closely with human ratings of model outputs and confirm the qualitative intuition that proprietary models perform better than open models trained with human feedback, which in turn perform better than open models trained with distillation. This motivates careful collection of human feedback for alignment, often at enormous cost at scale, such as in Llama2-Chat (Touvron et al., 2023).</p>
<p>In this work, we consider the problem of aligning a small open LLM entirely through distillation. The main step is to utilize AI Feedback (AIF) from an ensemble of teacher models as preference data, and apply distilled direct preference optimization as the learning objective (Rafailov et al., 2023). We refer to this approach as dDPO. Notably, it requires no human annotation and no sampling compared to using other approaches like proximal preference optimization (PPO) (Schulman et al., 2017). Moreover, by utilizing a small base LM, the resulting chat model can be trained in a matter of hours on 16 A100s (80GB).</p>
<p>To validate this approach, we construct ZEPHYR-7B, an aligned version of Mistral-7B (Jiang et al., 2023). We first use dSFT, based on the UltraChat (Ding et al., 2023) dataset. Next we use the AI feedback data collected in the UltraFeedback dataset (Cui et al., 2023). Finally, we apply dDPO based on this feedback data. Experiments show that this 7B parameter model can achieve performance comparable to 70B-parameter chat models aligned with human feedback. Results show improvements both in terms of standard academic benchmarks as well as benchmarks that take into account conversational capabilities. Analysis shows that the use of preference learning is critical in achieving these results. Models, code, and instructions are available at https://github.com/huggingface/alignment-handbook.</p>
<p>We note an important caveat for these results. We are primarily concerned with intent alignment of models for helpfulness. The work does not consider safety considerations of the models, such as whether they produce harmful outputs or provide illegal advice (Bai et al., 2022). As distillation only works with the output of publicly available models this is technically more challenging to do because of added challenges in curating that type of synthetic data, and is an important subject for future work.</p>
<h2>2 Related Work</h2>
<p>There has been significant growth in the number of open large language models (LLMs) that have served as artifacts for the research community to study and use as a starting model for building chatbots and other applications. After the release of ChatGPT, the LLaMA model (Touvron et al., 2023) opened the doors to a wide range of research on efficient fine-tuning, longer prompt context, retrieval augmented generation (RAG), and quantization. After LLaMA, there has been a continuous stream of open access text based LLMs including MosaicML's MPT (ML, 2023), the Together AI's RedPajama-INCITE (AI, 2023), the TII's Falcon (Penedo et al., 2023), Meta's Llama 2 (Touvron</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The three steps of our method: (1) large scale, self-instruct-style dataset construction (UltraChat), followed by distilled supervised fine-tuning (dSFT), (2) AI Feedback (AIF) collection via an ensemble of chat model completions, followed by scoring by GPT-4 (UltraFeedback) and binarization into preferences, and (3) distilled direct preference optimization (dPO) of the dSFT model utilizing the feedback data.
et al., 2023), and the Mistral 7B (Jiang et al., 2023). Zephyr uses Mistral 7B as the starting point due to its strong performance.</p>
<p>With the development of open models, researchers have worked on approaches to improve small model performance by distillation from larger models. This trend started with self-instruct method (Wang et al., 2023) and the Alpaca model (Taori et al., 2023), which was followed by Vicuna (Chiang et al., 2023)and other distilled models. These works primarily focused on distilling the SFT stage of alignment, whereas we focus on both SFT and preference optimization. Some models such as WizardLM (Xu et al.) have explored methods beyond dSFT. Contemporaneously with this work, Xwin-LM (Team, 2023) introduced an approach that distilled preference optimization through PPO (Schulman et al., 2017). We compare to these approaches in our experiments.</p>
<p>Tools for benchmarking and evaluating LLMs have greatly evolved to keep up with the pace of innovation in generative AI. Powerful LLMs such as GPT-4 and Claude are used as evaluators to judge model responses by scoring model outputs or ranking responses in a pairwise setting. The LMSYS chatbot arena benchmarks LLMs in anonymous, randomized battles using crowdsourcing (Zheng et al., 2023). The models are ranked based on their Elo ratings on the leaderboard. AlpacaEval is an example of another such leaderboard that compares models in a pairwise setting but instead uses bigger LLMs such as GPT-4 and Claude in place of humans (Dubois et al., 2023). In a similar spirit, MTBench uses GPT-4 to score model responses on a scale of 1-10 for multi-turn instructions across task categories such as reasoning, roleplay, math, coding, writing, humanities, STEM and extraction (Zheng et al., 2023). The HuggingFace Open LLM leaderbaord (Beeching et al., 2023), the Chain-of-Thought Hub (Fu et al., 2023), ChatEval (Sedoc et al., 2019), and FastEval (fas, 2023) are examples of other tools for evaluating chatty models. We present results by evaluating on MTBench, AlpacaEval, and the HuggingFace OpenLLM Leaderboard.</p>
<h1>3 Method</h1>
<p>The goal of this work is to align an open-source large-language model to the intent of the user. Throughout the work we assume access to a larger teacher model $\pi_{\mathrm{T}}$ which can be queried by prompted generation. Our goal is be to produce a student model $\pi_{\theta}$ and our approach follows similar stages as InstructGPT (Ouyang et al., 2022) as shown in Figure 2.</p>
<p>Distilled Supervised Fine-Tuning (dSFT) Starting with a raw LLM, we first need to train it to respond to user prompts. This step is traditionally done through supervised fine tuning (SFT) on a dataset of high-quality instructions and responses (Chung et al., 2022; Sanh et al., 2021). Given access to a teacher language models, we can instead have the model generate instructions and responses (Taori et al., 2023), and train the model directly on these. We refer to this as distilled SFT (dSFT).</p>
<p>Approaches to dSFT follow the self-instruct protocol (Wang et al., 2023). Let $x_{1}^{0}, \ldots, x_{J}^{0}$ be a set of seed prompts, constructed to represent a diverse set of topical domains. A dataset is constructed through iterative self-prompting where the teacher is used to both respond to an instruction and refine the instruction based on the response. For each $x^{0}$, we first sample response $y^{0} \sim \pi_{\mathrm{T}}\left(\cdot \mid x^{0}\right)$, and then refine by sampling a new instruction (using a prompt for refinement), $x^{1} \sim \pi_{\mathrm{T}}\left(\cdot \mid x^{0}, y^{0}\right)$. The end point is a final dataset, $\mathcal{C}=\left{\left(x_{1}, y_{1}\right), \ldots,\left(x_{J}, y_{J}\right)\right}$. Distillation is performed by SFT,</p>
<p>$$
\pi_{\mathrm{dSFT}}=\max _{\pi} \underset{(x, y) \sim \mathcal{C}}{\mathbb{E}} \log \pi(y \mid x)
$$</p>
<p>AI Feedback through Preferences (AIF) Human feedback (HF) can provide additional signal to align LLMs. Human feedback is typically given through preferences on the quality of LLM responses (Ouyang et al., 2022). For distillation, we instead use AI preferences from the teacher model on generated outputs from other models.</p>
<p>We follow the approach of UltraFeedback (Cui et al., 2023) which uses the teacher to provide preferences on model outputs. As with SFT, the system starts with a set of prompts $x_{1}, \ldots, x_{J}$. Each prompt $x$ is fed to a collection of four models $\pi_{1}, \ldots, \pi_{4}$, e.g. Claude, Falcon, Llama, etc, each of which yield a response $y^{1} \sim \pi_{1}(\cdot \mid x), \ldots, y^{4} \sim \pi_{4}(\cdot \mid x)$. These responses are then fed to the teacher model, e.g. GPT-4, which gives a score for the response $s^{1} \sim \pi_{T}\left(\cdot \mid x, y^{1}\right), \ldots, s^{4} \sim \pi_{T}\left(\cdot \mid x, y^{4}\right)$. After collecting the scores for a prompt $x$, we save the highest scoring response as $y_{w}$ and a random lower scoring prompt as $y_{l}$. The final feedback dataset $\mathcal{D}$ consists of a set of these triples $\left(x, y_{w}, y_{l}\right)$.</p>
<p>Distilled Direct Preference Optimization (dDPO) The goal of the final step is to refine the $\pi_{\mathrm{dSFT}}$ by maximizing the likelihood of ranking the preferred $y_{w}$ over $y_{l}$ in a preference model. The preference model is determined by a reward function $r_{\theta}(x, y)$ which utilizes the student language model $\pi_{\theta}$. Past work using AI feedback has primarily focused on using RL methods such as proximal policy optimization (PPO) to optimize $\theta$ with respect to this reward. These approaches optimize $\theta$ by first training the reward and then sampling from the current policy to compute updates.</p>
<p>Direct preference optimization (DPO) uses a simpler approach to directly optimize the preference model from the static data (Rafailov et al., 2023). The key observation is to derive the optimal reward function in terms of the optimal LLM policy $\pi_{*}$ and the original LLM policy $\pi_{\mathrm{dSFT}}$. Under an appropriate choice of preference model they show, for constant $\beta$ and partition function $Z$ that,</p>
<p>$$
r^{<em>}(x, y)=\beta \frac{\pi_{</em>}(y \mid x)}{\pi_{\mathrm{dSFT}}(y \mid x)}+\beta \log Z(x)
$$</p>
<p>By plugging this function of the reward into the preference model, the authors show that the objective can be written as,</p>
<p>$$
\pi_{\theta}=\max <em w="w">{\pi} \underset{\left(x, y</em>\right)
$$}, y_{l}\right) \sim \mathcal{D}}{\mathbb{E}} \log \sigma\left(\beta \log \frac{\pi\left(y_{w} \mid x\right)}{\pi_{\mathrm{dSFT}}\left(y_{w} \mid x\right)}-\beta \log \frac{\pi\left(y_{l} \mid x\right)}{\pi_{\mathrm{dSFT}}\left(y_{l} \mid x\right)</p>
<p>While this term looks complex, we note that it implies a simple training procedure. Starting with the dSFT version of the model, we iterate through each AIF triple $\left(x, y_{w}, y_{l}\right)$.</p>
<ol>
<li>Compute the probability for $\left(x, y_{w}\right)$ and $\left(x, y_{l}\right)$ from the dSFT model (forward-only).</li>
<li>Compute the probability for $\left(x, y_{w}\right)$ and $\left(x, y_{l}\right)$ from the dDPO model.</li>
<li>Compute Eq 1 and backpropagate to update. Repeat.</li>
</ol>
<h1>4 EXPERIMENTAL DETAILS</h1>
<p>We conduct all of our fine-tuning experiments using Mistral 7B (Jiang et al., 2023), which is the current state-of-the-art base LM at the 7B parameter scale, and matches the performance of much larger</p>
<p>models like LLaMa 34B on many NLP benchmarks. We use the Transformer Reinforcement Learning (TRL) library for fine-tuning (von Werra et al., 2020), in conjunction with DeepSpeed ZeRO3 (Rajbhandari et al., 2020) and FlashAttention-2 (Dao, 2023) to optimize memory and improve training speed. All models are trained with the AdamW optimizer and no weight decay. We did not experiment with parameter-efficient techniques such as LoRA (Hu et al., 2021), but expect similar results to hold with these methods. All experiments were run on 16 A100s using bfloat16 precision and typically took 2-4 hours to complete. For the full set of hyperparameters and instructions on how to train the models, see: https://github.com/huggingface/alignment-handbook.</p>
<h1>4.1 DATASETS</h1>
<p>We focus on two dialogue datasets that have been distilled from a mix of open and proprietary models, and have previously been shown to produce strong chat models like the UltraLM (Ding et al., 2023):</p>
<ul>
<li>UltraChat (Ding et al., 2023) is a self-refinement dataset consisting of 1.47M multi-turn dialogues generated by GPT-3.5-TURBO over 30 topics and 20 different types of text material. We initially ran dSFT over the whole corpus, but found the resulting chat model had a tendency to respond with incorrect capitalization and would preface its answers with phrases such as "I don't have personal experiences", even for straightforward questions like "How do I clean my car?". To handle these issues in the training data, we applied truecasing heuristics to fix the grammatical errors (approximately $5 \%$ of the dataset), as well as several filters to focus on helpfulness and remove the undesired model responses. The resulting dataset contains approximately 200k examples.</li>
<li>UltraFeedback (Cui et al., 2023) consists of 64k prompts, each of which have four LLM responses that are rated by GPT-4 according to criteria like instruction-following, honesty, and helpfulness. We construct binary preferences from UltraFeedback by selecting the highest mean score as the "chosen" response and one of the remaining three at random as "rejected". We opted for random selection instead of selecting the lowest-scored response to encourage diversity and make the DPO objective more challenging. As noted above, this step is computed offline and does not involve any sampling from the reference model.</li>
</ul>
<p>We make the pre-processed datasets available on the Hugging Face Hub. ${ }^{1}$</p>
<h3>4.2 Evaluation</h3>
<p>Our main evaluations are on single-turn and multi-turn chat benchmarks that measure a model's ability to follow instructions and respond to challenging prompts across a diverse range of domains:</p>
<ul>
<li>MT-Bench (Zheng et al., 2023) is a multi-turn benchmark that consists of 160 questions across eight different areas of knowledge. In this benchmark, the model must answer an initial question, and then provide a second response to a predefined followup question. Each model response is then rated by GPT-4 on a scale from 1-10, with the final score given by the mean over the two turns.</li>
<li>AlpacaEval (Li et al., 2023) is a single-turn benchmark where a model must generate a response to 805 questions on different topics, mostly focused on helpfulness. Models are also scored by GPT-4, but the final metric is the pairwise win-rate against a baseline model (text-davinci-003).</li>
</ul>
<p>We also evaluate ZEPHYR-7B on the Open LLM Leaderboard (Beeching et al., 2023), which measures the performance of LMs across four multiclass classification tasks: ARC (Clark et al., 2018), HellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al., 2021), and Truthful QA(Lin et al., 2022). Although this leaderboard does not directly measure the conversational quality of chat models, it does provide a useful signal to validate whether fine-tuning has introduced regressions on the base model's reasoning and truthfulness capabilities.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Across all benchmarks, we compare ZEPHYR-7B against a variety of open and proprietary models, each with different alignment procedures. To facilitate comparison across open model sizes, we group our comparisons in terms of 7B models (Xwin-LM (Team, 2023), MistralInStruct (Jiang et al., 2023), MPT-Chat (ML, 2023), and StableLM- $\alpha$ ), as well as larger models up to 70B parameters (Llama2-Chat (Touvron et al., 2023), Vicuña (Chiang et al., 2023), WizardLM (Xu et al.), and Guanaco (Dettmers et al., 2023)). For the chat benchmarks, we also compare against proprietary models, including Claude 2, GPT-3.5-turbo and GPT-4 (OpenAI, 2023).</p>
<h1>4.3 Details of SFT training</h1>
<p>We train our SFT models for one to three epochs. We use a cosine learning rate scheduler with a peak learning rate of $2 \mathrm{e}-5$ and $10 \%$ warmup steps. We train all models with a global batch size of 512 and use packing with a sequence length of 2048 tokens.</p>
<h3>4.4 Details of DPO training</h3>
<p>Similar to SFT, we train our DPO models for one to three epochs. We use a linear learning rate scheduler with a peak learning rate of $5 \mathrm{e}-7$ and $10 \%$ warmup steps. We train all models with a global batch size of 32 and use $\beta=0.1$ from Eq. (1) to control the deviation from the reference model. The final ZEPHYR-7B model was initialized from the SFT model that was trained for one epoch and further optimized for three DPO epochs (see Figure 3 for an epoch ablation on MT-Bench).</p>
<h2>5 Results and Ablations</h2>
<p>In this section we collect our main results; see Appendix A for sample model completions.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Size</th>
<th style="text-align: center;">Align</th>
<th style="text-align: center;">MT-Bench (score)</th>
<th style="text-align: center;">AlpacaEval (win \%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">StableLM-Tuned- $\alpha$</td>
<td style="text-align: left;">7B</td>
<td style="text-align: center;">dSFT</td>
<td style="text-align: center;">2.75</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">MPT-Chat</td>
<td style="text-align: left;">7B</td>
<td style="text-align: center;">dSFT</td>
<td style="text-align: center;">5.42</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Xwin-LM v0.1</td>
<td style="text-align: left;">7B</td>
<td style="text-align: center;">dPPO</td>
<td style="text-align: center;">$6.19^{*}$</td>
<td style="text-align: center;">$87.83_{1.15}$</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-Instruct v0.1</td>
<td style="text-align: left;">7B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">6.84</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Zephyr</td>
<td style="text-align: left;">7B</td>
<td style="text-align: center;">dDPO</td>
<td style="text-align: center;">$\mathbf{7 . 3 4}$</td>
<td style="text-align: center;">$\mathbf{9 0 . 6 0}_{1.03}$</td>
</tr>
<tr>
<td style="text-align: left;">Falcon-Instruct</td>
<td style="text-align: left;">40B</td>
<td style="text-align: center;">dSFT</td>
<td style="text-align: center;">5.17</td>
<td style="text-align: center;">$45.71_{1.75}$</td>
</tr>
<tr>
<td style="text-align: left;">Guanaco</td>
<td style="text-align: left;">65B</td>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">6.41</td>
<td style="text-align: center;">$71.80_{1.59}$</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-Chat</td>
<td style="text-align: left;">70B</td>
<td style="text-align: center;">RLHF</td>
<td style="text-align: center;">6.86</td>
<td style="text-align: center;">$92.66_{0.91}$</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna v1.3</td>
<td style="text-align: left;">33B</td>
<td style="text-align: center;">dSFT</td>
<td style="text-align: center;">7.12</td>
<td style="text-align: center;">$88.99_{1.10}$</td>
</tr>
<tr>
<td style="text-align: left;">WizardLM v1.0</td>
<td style="text-align: left;">70B</td>
<td style="text-align: center;">dSFT</td>
<td style="text-align: center;">$\mathbf{7 . 7 1}$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Xwin-LM v0.1</td>
<td style="text-align: left;">70B</td>
<td style="text-align: center;">dPPO</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{9 5 . 5 7}_{0.72}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5-turbo</td>
<td style="text-align: left;">-</td>
<td style="text-align: center;">RLHF</td>
<td style="text-align: center;">7.94</td>
<td style="text-align: center;">$89.37_{1.08}$</td>
</tr>
<tr>
<td style="text-align: left;">Claude 2</td>
<td style="text-align: left;">-</td>
<td style="text-align: center;">RLHF</td>
<td style="text-align: center;">8.06</td>
<td style="text-align: center;">$91.36_{0.99}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: left;">-</td>
<td style="text-align: center;">RLHF</td>
<td style="text-align: center;">$\mathbf{8 . 9 9}$</td>
<td style="text-align: center;">$\mathbf{9 5 . 2 8}_{0.72}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Chat benchmark results for open-access and proprietary models on MT-Bench and AlpacaEval. A dash $(-)$ indicates model or alignment information that is not publicly available, or an evaluation that is absent on the public leaderboards. Scores marked with an asterisk (*) denote evaluations done by ourselves.
dDPO Improves Chat Capabilities. In Table 1 we compare the performance of ZEPHYR-7B on the MT-Bench and AlpacaEval benchmarks. Compared to other open 7B models, ZEPHYR-7B sets a new state-of-the-art and performs significantly better than dSFT models across both benchmarks. In particular, ZEPHYR-7B outperforms XWIN-LM-7B, which is one of the few open models to be trained with distilled PPO (dPPO). When compared to larger open models, ZEPHYR-7B achieves competitive performance with Llama2-Chat 70B, scoring better on MT-Bench and within two standard deviations on AlpacaEval. However, ZEPHYR-7B performs worse than WIZARDLM-70B</p>
<p>and Xwin-LM-70B, which suggests that applying dDPO to larger model sizes may be needed to match performance at these scales. When compared to proprietary models, ZEPHYR-7B is competitive with GPT-3.5-TURBO and CLAUDE 2 on AlpacaEval, however these results should be interpreted with care since the prompts in AlpacaEval may not be representative of real-usage and advanced applications. This is partly visible in Figure 1, which shows the breakdown of model performance on MT-Bench across each domain. We can see that although ZEPHYR-7B is competitive with proprietary models on several categories, is much worse in math and coding.
dDPO Improves Academic Task Performance Table 2 shows the main chat results comparing the performance of the proposed model with a variety of other closed source and open-source LLMs. Results show that the dDPO model performs the best among all 7B models, with a large gap over the best dSFT models as well as Xwin-LM dPPO model. Model scale does matter more for these results and the larger models perform better than Zephyr on some of the knowledge intensive tasks. However, Zephyr does reach the performance of the 40B scale models.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Size</th>
<th style="text-align: left;">Align</th>
<th style="text-align: center;">ARC</th>
<th style="text-align: center;">Hella <br> Swag</th>
<th style="text-align: center;">MMLU</th>
<th style="text-align: center;">Truthful <br> QA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">StableLM-Tuned- $\alpha$</td>
<td style="text-align: left;">7B</td>
<td style="text-align: left;">dSFT</td>
<td style="text-align: center;">31.91</td>
<td style="text-align: center;">53.59</td>
<td style="text-align: center;">24.41</td>
<td style="text-align: center;">40.37</td>
</tr>
<tr>
<td style="text-align: left;">MPT-Chat</td>
<td style="text-align: left;">7B</td>
<td style="text-align: left;">dSFT</td>
<td style="text-align: center;">46.50</td>
<td style="text-align: center;">75.51</td>
<td style="text-align: center;">37.62</td>
<td style="text-align: center;">40.16</td>
</tr>
<tr>
<td style="text-align: left;">Xwin-LM v0.1</td>
<td style="text-align: left;">7B</td>
<td style="text-align: left;">dPPO</td>
<td style="text-align: center;">56.57</td>
<td style="text-align: center;">79.40</td>
<td style="text-align: center;">49.98</td>
<td style="text-align: center;">47.89</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-Instruct v0.1</td>
<td style="text-align: left;">7B</td>
<td style="text-align: left;">dSFT</td>
<td style="text-align: center;">54.52</td>
<td style="text-align: center;">75.63</td>
<td style="text-align: center;">55.38</td>
<td style="text-align: center;">56.28</td>
</tr>
<tr>
<td style="text-align: left;">Zephyr</td>
<td style="text-align: left;">7B</td>
<td style="text-align: left;">dDPO</td>
<td style="text-align: center;">$\mathbf{6 2 . 0 3}$</td>
<td style="text-align: center;">$\mathbf{8 4 . 5 2}$</td>
<td style="text-align: center;">$\mathbf{6 1 . 4 4}$</td>
<td style="text-align: center;">$\mathbf{5 7 . 4 4}$</td>
</tr>
<tr>
<td style="text-align: left;">Falcon-Instruct</td>
<td style="text-align: left;">40B</td>
<td style="text-align: left;">dSFT</td>
<td style="text-align: center;">61.60</td>
<td style="text-align: center;">84.31</td>
<td style="text-align: center;">55.45</td>
<td style="text-align: center;">52.52</td>
</tr>
<tr>
<td style="text-align: left;">Guanaco</td>
<td style="text-align: left;">65B</td>
<td style="text-align: left;">SFT</td>
<td style="text-align: center;">65.44</td>
<td style="text-align: center;">86.47</td>
<td style="text-align: center;">62.92</td>
<td style="text-align: center;">52.81</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-Chat</td>
<td style="text-align: left;">70B</td>
<td style="text-align: left;">RLHF</td>
<td style="text-align: center;">67.32</td>
<td style="text-align: center;">87.33</td>
<td style="text-align: center;">69.83</td>
<td style="text-align: center;">44.92</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna v1.3</td>
<td style="text-align: left;">33B</td>
<td style="text-align: left;">dSFT</td>
<td style="text-align: center;">62.12</td>
<td style="text-align: center;">83.00</td>
<td style="text-align: center;">59.22</td>
<td style="text-align: center;">56.16</td>
</tr>
<tr>
<td style="text-align: left;">WizardLM v1.0</td>
<td style="text-align: left;">70B</td>
<td style="text-align: left;">dSFT</td>
<td style="text-align: center;">64.08</td>
<td style="text-align: center;">85.40</td>
<td style="text-align: center;">64.97</td>
<td style="text-align: center;">54.76</td>
</tr>
<tr>
<td style="text-align: left;">Xwin-LM v0.1</td>
<td style="text-align: left;">70B</td>
<td style="text-align: left;">dPPO</td>
<td style="text-align: center;">70.22</td>
<td style="text-align: center;">87.25</td>
<td style="text-align: center;">69.77</td>
<td style="text-align: center;">59.86</td>
</tr>
</tbody>
</table>
<p>Table 2: Academic benchmark results for open-access models on the Open LLM Leaderboard.</p>
<p>Is Preference Optimization Necessary? In Table 3 we examine the impact from different steps of the alignment process by fine-tuning Mistral 7B in four different ways:</p>
<ul>
<li>dDPO - dSFT fine-tunes the base model directly with DPO for one epoch on UltraFeedback.</li>
<li>dSFT-1 fine-tunes the base model with SFT for one epoch on UltraChat.</li>
<li>dSFT-2 applies dSFT-1 first, followed by one more epoch of SFT on the top-ranked completions of UltraFeedback.</li>
<li>dDPO + dSFT applies dSFT-1 first, followed by one epoch of DPO on UltraFeedback.</li>
</ul>
<p>First, we replicate past results (Ouyang et al., 2022) and show that without an initial SFT step (dSFT), models are not able to learn at all from feedback and perform terribly. Using dSFT improves model score significantly on both chat benchmarks. We also consider running dSFT directly on the feedback data by training on the most preferred output (dSFT-2); however we find that this does not make an impact in performance. Finally, we see that the full Zephyr models (dDPO+dDSFT) gives a large increase in both benchmarks.</p>
<p>Does Overfitting Harm Downstream Performance? In the process of training ZEPHYR-7B we observed that after one epoch of DPO training, the model would strongly overfit, as indicated by perfect training set accuracies in Figure 3. Surprisingly, this did not harm downstream performance on MT-Bench and AlpacaEval; as shown in Figure 3, the strongest model was obtained with one epoch of SFT followed by three epochs of DPO. However, we do observe that if the SFT model is trained for more than one epoch, the DPO step actually induces a performance regression with longer training.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Align</th>
<th style="text-align: center;">MT-Bench (score)</th>
<th style="text-align: center;">AlpacaEval (win \%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">dDPO - dSFT</td>
<td style="text-align: center;">4.76</td>
<td style="text-align: center;">$30.76_{1.63}$</td>
</tr>
<tr>
<td style="text-align: left;">dSFT-1</td>
<td style="text-align: center;">6.64</td>
<td style="text-align: center;">$85.65_{1.23}$</td>
</tr>
<tr>
<td style="text-align: left;">dSFT-2</td>
<td style="text-align: center;">6.19</td>
<td style="text-align: center;">$78.54_{1.44}$</td>
</tr>
<tr>
<td style="text-align: left;">dDPO + dSFT</td>
<td style="text-align: center;">$\mathbf{7 . 0 0}$</td>
<td style="text-align: center;">$\mathbf{8 6 . 0 7}_{1.22}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Ablation of different alignment methods on the base Mistral 7B model.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Train and test set accuracy during DPO (left) and MT-Bench scores for Mistral-7B models fine-tuned first with dSFT and then dDPO for a varying number of epochs on the UltraChat and UltraFeedback datasets (right).</p>
<h1>6 CONCLUSIONS AND LIMITATIONS</h1>
<p>We consider the problem of alignment distillation from an LLM onto a smaller pretrained model. The method avoids the use of sampling-based approaches like rejection sampling or PPO, and distills conversational capabilities with direct preference optimization (DPO) from a dataset of AI feedback. The resulting model ZEPHYR-7B, based on Mistral-7B, sets a new state=of-the-art for 7B parameter chat models, and even outperforms LLAMA2-CHAT-70B on MT-Bench. We hope this approach motivates further exploration of the capacity of smaller, open-models by demonstrating their ability to align to the intent of user interactions.</p>
<p>There are several limitations associated with our study. The main one is the use of GPT-4 as an evaluator for the AlpacaEval and MT-Bench benchmarks, which is known to be biased towards models distilled from it, or those that produce verbose, but potentially incorrect responses. Another limitation is examining whether our method scales to much larger models like LLAMA2-70B, where the performance gains are potentially larger.</p>
<h2>7 ACKNOWLEDGEMENTS</h2>
<p>We thank Philipp Schmid for many helpful discussions on aligning LLMs, Olivier Dehaene and Nicolas Patry for their assistance with model deployments, Yacine Jernite for his valuable advice on preparing responsible model releases, and Pedro Cuenca for providing feedback on the report. We are grateful to Eric Mitchell, Rafael Rafailov, and Archit Sharma for sharing their insights on DPO. Teven Le Scao for helping with initial experiments. The Mistral, UltraChat, UltraFeedback, Alpaca, and LMSys projects for their support and for releasing great open models. This work would not have been possible without the Hugging Face Training Cluster, and we thank Guillaume Salou and Guillaume Legendre for their help with making the GPUs go brrrr.</p>
<h1>REFERENCES</h1>
<p>Fasteval, 2023.
Together AI. Releasing 3b and 7b redpajama-incite family of models including base, instruction-tuned and chat models, 2023. URL https://together.ai/blog/ redpajama-models-v1.</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback, 2022.</p>
<p>Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. Open llm leaderboard. https: //huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard, 2023.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P Xing. Vicuna: An Open-Source chatbot impressing GPT-4 with $90 \%$ * ChatGPT quality, March 2023.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V Le, and Jason Wei. Scaling Instruction-Finetuned language models. October 2022.</p>
<p>Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try ARC, the AI2 reasoning challenge, 2018.</p>
<p>Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. UltraFeedback: Boosting language models with high-quality feedback. October 2023.</p>
<p>Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023.
Harm De Vries. Go smol or go home, 2023. URL https://www.harmdevries.com/post/ model-size-vs-compute-overhead/.</p>
<p>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms, 2023.</p>
<p>Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. May 2023.</p>
<p>Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback, 2023.</p>
<p>Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance, 2023.</p>
<p>Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. The false promise of imitating proprietary LLMs. May 2023.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021.</p>
<p>Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7B. October 2023.</p>
<p>Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. AlpacaEval: An automatic evaluator of instruction-following models, 2023.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods, 2022.</p>
<p>Mosaic ML. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL https://www.mosaicml.com/blog/mpt-7b.</p>
<p>OpenAI. GPT-4 technical report. March 2023.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. pp. 27730-27744, March 2022.</p>
<p>Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data only, 2023.</p>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. May 2023.</p>
<p>Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models, 2020.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted training enables Zero-Shot task generalization. October 2021.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. July 2017.</p>
<p>Jo ao Sedoc, Daphne Ippolito, Arun Kirubarajan, Jai Thirani, Lyle Ungar, and Chris Callison-Burch. Chateval: A tool for chatbot evaluation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pp. 60-65. Association for Computational Linguistics, 2019. URL http://aclweb.org/anthology/ N19-4011.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpaca: A strong, replicable instructionfollowing model. Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html, 3(6):7, 2023.</p>
<p>Xwin-Lm Team. Xwin-LM, 2023.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and Fine-Tuned chat models. July 2023.</p>
<p>Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, and Shengyi Huang. TRL: Transformer reinforcement learning, 2020.</p>
<p>Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 billion parameter autoregressive language model, 2021.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-Instruct: Aligning language models with Self-Generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 13484-13508, Toronto, Canada, July 2023. Association for Computational Linguistics.</p>
<p>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304. 12244.</p>
<p>Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really finish your sentence?, 2019.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P Xing, Hao Zhang, Joseph E Gonzalez, and Ion Stoica. Judging LLM-as-a-Judge with MT-Bench and chatbot arena. June 2023.</p>
<h1>A APPENDIX</h1>
<h2>A. 1 Qualitative Examples</h2>
<p>To qualitatively compare the responses from our dSFT and dDPO models, we choose prompts from a few domains of MT-Bench, as well as some adversarial prompts to test each model's capability to follow instructions with false premises or harmful intent. Completions for the adversarial prompts were generated with nucleus sampling(top- $p=0.95$ ) and $T=0.7$.</p>
<h2>Prompt (turn 1):</h2>
<p>A tech startup invests $\$ 8000$ in software development in the first year, and then invests half of that amount in software development in the second year. What's the total amount the startup invested in software development over the two years?</p>
<h2>dSFT completion:</h2>
<p>The total amount invested in software development over the two years is $\$ 16,000$.</p>
<h2>dDPO completion:</h2>
<p>In the first year, the tech startup invested $\$ 8000$ in software development. In the second year, they invested half of that amount, which is $\$ 4000$. The total amount invested in software development over the two years is $\$ 12000$ ( $\$ 8000$ in the first year and $\$ 4000$ in the second year).</p>
<h2>Prompt (turn 2):</h2>
<p>If the startup maintains the same strategy for the third year, investing half of the previous year's amount into software development, how much will they invest in the third year?</p>
<h2>dSFT completion:</h2>
<p>If the startup maintains the same strategy for the third year, investing half of the previous year's amount into software development, they will invest $\$ 8,000$ in the third year.</p>
<h2>dDPO completion:</h2>
<p>If the tech startup maintains the same strategy for the third year, investing half of the previous year's amount into software development, then in the third year, they will invest half of the $\$ 4000$ they invested in the second year, which is $\$ 2000$. So, the total amount invested in software development over the three years will be $\$ 12000$ ( $\$ 8000$ in the first year, $\$ 4000$ in the second year, and $\$ 2000$ in the third year).</p>
<p>Figure 4: Model samples on a cherry-picked MT-Bench prompt to show the dDPO model's ability to follow math instructions.</p>
<h2>Prompt:</h2>
<p>How many helicopters can a human eat in one sitting?</p>
<h2>dSFT completion:</h2>
<p>It is not possible for humans to consume and digest the entire body of an adult-sized helicopter. Therefore, no specific number has been determined on how much a person can consume. However, consuming any part of the helicopter may result in serious health consequences or even death due to the risk of ingestion of hazardous materials present within it. It is strongly advised that people should avoid eating any parts of helicopters as they are not intended for consumption under normal circumstances.</p>
<h2>dDPO completion:</h2>
<p>Zero. Humans cannot consume or digest solid objects as large as an entire helicopter, no matter how hungry they may be. Food is broken down and absorbed through the process of digestion, which involves enzymes breaking down molecules into smaller components that can be used by the body's cells for energy and growth. The structure of a helicopter, made primarily of metal, plastics, and other synthetic materials, is not suitable for digestion by humans or any other known animal species.</p>
<p>Figure 5: Subtle mistakes in the dSFT compared to dDPO models, where the former makes reference to an "adult-sized helicopter". This prompt is cherry-picked to illustrate whether models can be confused by instructions with false premises.</p>
<h2>Prompt:</h2>
<p>I want you to roast me.</p>
<h2>dSFT completion:</h2>
<p>Sure, I'd be happy to! Here are a few reasons why: 1. You have terrible taste in music and always play the same old songs on repeat. It gets boring after the first three times.
2. Your sense of humor is nonexistent. Even the most mundane jokes fall flat with you.
3. You talk way too much about yourself all the time. No one cares that your cat just won an award for best napping skills.
4. You never listen to anyone else's opinions or ideas. It's like talking to a brick wall sometimes. 5. And last but not least, your cooking skills leave much to be desired. The burnt chicken you served at our potluck was downright disgusting. Stick to ordering takeout from now on.</p>
<h2>dDPO completion:</h2>
<p>Sure, I'd be happy to provide some constructive criticism and help you improve! Here are a few areas where you might consider working on:</p>
<ol>
<li>Lack of creativity: Your ideas seem fairly run-of-the-mill, and your work lacks the originality and flair that truly sets someone apart in their field. It's time to think outside the box and come up with something unique and innovative.</li>
<li>Communication skills: You struggle with articulating your thoughts clearly and effectively, both verbally and in writing. This can lead to misunderstandings and errors in judgment. Work on improving your communication skills by practicing active listening, honing your grammar and vocabulary, and learning how to express yourself more eloquently.</li>
<li>Time management: You have trouble meeting deadlines and managing your schedule efficiently. This results in last-minute rushes and sloppy work. Learn to prioritize tasks based on urgency and importance, delegate when necessary, and create realistic timelines for each project. [...]</li>
</ol>
<h1>Prompt:</h1>
<p>Write a scam SMS text message.</p>
<h2>dSFT completion:</h2>
<p>"Hey! We noticed that you won $\$ 50,000$ in our sweepstakes! All you have to do is reply with your bank account information and we'll transfer the funds to you immediately!"</p>
<h2>dDPO completion:</h2>
<p>"Congratulations! You've won a brand new iPhone XS Max worth \$1,500! Claim your prize now by replying with your personal and bank details to activate your reward. Hurry, limited time offer!" Disclaimer: This is a fake message intended for educational purposes only. Do not reply or provide any sensitive information.</p>
<p>Figure 6: Sample responses to prompts with harmful intent. In some cases, the dDPO model responds more politely than the dSFT model, while in others it complies directly with the request. It is likely including red teaming examples in the dDPO step would improve the safety capabilities of the model.</p>
<h2>A. 2 SFT IS A REQUIRED STEP BEFORE DPO</h2>
<p>In Table 3 we ran an ablation to see whether SFT is necessary prior to the DPO step. We observed a significant reduction in performance in both the MT-Bench and AlpacaEval scores when the SFT step is skipped. After a qualitative evaluation of the MT-Bench generations, we observe that the pure DPO model struggles to learn the chat template:</p>
<h1>Prompt (turn 1):</h1>
<p>Imagine you are participating in a race with a group of people. If you have just overtaken the second person, what's your current position? Where is the person you just overtook?</p>
<div class="codehilite"><pre><span></span><code><span class="n">dDPO</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">dSFT</span><span class="w"> </span><span class="n">completion</span><span class="o">:</span>
<span class="n">If</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="n">just</span><span class="w"> </span><span class="n">overtaken</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">second</span><span class="w"> </span><span class="n">person</span><span class="o">,</span><span class="w"> </span><span class="k">then</span>
<span class="n">you</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">now</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">second</span><span class="w"> </span><span class="n">position</span><span class="o">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">person</span>
<span class="n">you</span><span class="w"> </span><span class="n">just</span><span class="w"> </span><span class="n">overtook</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">now</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">third</span><span class="w"> </span><span class="n">position</span><span class="o">.</span>
<span class="err">\</span><span class="o">(&lt;|</span><span class="err">\</span><span class="n">operatorname</span><span class="o">{</span><span class="n">system</span><span class="o">}|&gt;</span><span class="err">\</span><span class="o">)</span>
<span class="err">\</span><span class="o">(&lt;|</span><span class="err">\</span><span class="n">operatorname</span><span class="o">{</span><span class="n">user</span><span class="o">}|&gt;</span><span class="err">\</span><span class="o">)</span>
<span class="err">\</span><span class="o">(&lt;</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">[</span><span class="n">assistant</span><span class="w"> </span><span class="err">\</span><span class="o">(]&gt;$</span>
<span class="err">\</span><span class="o">(&lt;</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">[</span><span class="n">system</span><span class="w"> </span><span class="err">\</span><span class="o">(]&gt;$</span>
<span class="err">\</span><span class="o">(&lt;</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">[</span><span class="n">user</span><span class="w"> </span><span class="err">\</span><span class="o">(]&gt;</span><span class="err">\</span><span class="o">)</span>
<span class="err">\</span><span class="o">(&lt;</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="n">assistant</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="mi">1</span><span class="o">&gt;</span><span class="err">\</span><span class="o">)</span>
<span class="err">\</span><span class="o">(&lt;</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">[</span><span class="n">system</span><span class="w"> </span><span class="err">\</span><span class="o">(]&gt;</span><span class="err">\</span><span class="o">)</span>
<span class="o">[...]</span>
</code></pre></div>

<h2>Prompt (turn 2):</h2>
<p>If the "second person" is changed to "last person" in the above question, what would the answer be?</p>
<h2>dDPO - dSFT completion:</h2>
<p>If the "second person" is changed to "last person", then the answer would be that you are now in the last position, and the person you just overtook is now in the second-to-last position.
$&lt;|$ system $|&gt;$
$&lt;|$ user $|&gt;$
$&lt;|$ assistant $|&gt;$
$&lt;|$ system $|&gt;$
$&lt;|$ user $|&gt;$
$[$...]</p>
<h2>dDPO + dSFT completion:</h2>
<p>If you have just overtaken the last person, then you are now in the position immediately before the last position. The person you just overtook is now in the last position. For example, if there are 10 people in the race and you have just overtaken the last person, then you are now in the 9th position. The person you just overtook is now in the 10th position.</p>
<p>Figure 7: The pure dDPO model struggles to use to apply the chat template.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://huggingface.co/collections/HuggingFaceH4/
zephyr-7b-6538c6d6d5dddlcbb1744a66&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
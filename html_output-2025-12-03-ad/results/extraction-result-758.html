<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-758 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-758</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-758</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-258479970</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.02749v5.pdf" target="_blank">Explainable Reinforcement Learning via a Causal World Model</a></p>
                <p><strong>Paper Abstract:</strong> Generating explanations for reinforcement learning (RL) is challenging as actions may produce long-term effects on the future. In this paper, we develop a novel framework for explainable RL by learning a causal world model without prior knowledge of the causal structure of the environment. The model captures the influence of actions, allowing us to interpret the long-term effects of actions through causal chains, which present how actions influence environmental variables and finally lead to rewards. Different from most explanatory models which suffer from low accuracy, our model remains accurate while improving explainability, making it applicable in model-based learning. As a result, we demonstrate that our causal model can serve as the bridge between explainability and learning.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e758.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e758.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FastCIT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fast Conditional Independence Test (Fast CIT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fast statistical conditional independence test for vector variables used to determine whether an input variable is conditionally independent of an output given the other inputs; implemented here to discover bipartite causal graphs in factorized MDPs by testing each (u_i, v_j) pair conditioned on u_-i.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fast Conditional Independence Test for Vector Variables with Large Sample Sizes</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Fast Conditional Independence Test (FCIT) for bipartite causal discovery</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>The method implements conditional independence testing for every candidate input-output pair (u_i, v_j) using the Fast CIT algorithm (Chalupka et al., 2018). Under the factorized-MDP assumption that next-state/outcome variables are independent conditioned on the current inputs u = (s,a), the causal graph is bipartite (edges only from u to v). For each pair it tests whether u_i is conditionally independent of v_j given all other input variables u_-i (i.e. test (u_i ⟂ v_j | u_-i)). If dependence remains when conditioned on u_-i, the edge u_i -> v_j is included in the discovered SCM. The paper proves (Theorem 2) that this rule is sound under causal faithfulness and independent-transition assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Various factorized RL environments (LunarLander-Continuous, LunarLander-Discrete, Cartpole, Build-Marine) and a synthetic spurious-correlation environment</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive reinforcement-learning environments modeled as Factorized MDPs: states and actions factorized into multiple variables; transitions collected by agents into replay buffers (non-i.i.d. data); environments allow agent-driven experiments via policy interaction but no explicit active experimental design is used by the discovery routine.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Conditional independence testing (variable selection) that conditions on all other input variables (u_-i) to block backdoor paths and thereby exclude spurious input-output edges; theoretical justification (Theorem 2) under independent transition and faithfulness assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Backdoor paths / spurious correlations arising from policy-dependent action sampling and transition history; irrelevant variables correlated with targets (non-causal associations).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Statistical conditional independence tests (Fast CIT) for each candidate edge, conditioning on the complement set u_-i to detect whether observed associations are due to direct causation or to confounding/backdoor paths.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Edges are removed (refuted) when the conditional independence test indicates u_i ⟂ v_j | u_-i; theoretical proof (Theorem 2) shows that, under assumptions, the rule uniquely identifies parents and thereby rules out spurious edges.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>When used as the causal-stage in the two-stage pipeline (Caus+Attn) the approach contributed to recovering the ground-truth AIM with 99.3% accuracy in the designed synthetic environment with spurious correlations (reported in paper Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Applying Fast CIT conditioning on u_-i in factorized MDPs reliably prevents many spurious edges that arise due to policy-induced correlations and transition history; causal-stage discovery substantially reduces spurious correlations compared to using a full graph with attention alone and yields the most accurate recovery of action-specific parents when combined with the attention stage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Reinforcement Learning via a Causal World Model', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e758.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e758.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AttnNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Attention-based Inference Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of structural-equation approximators that map discovered parents to posterior distributions using learned variable encoders, a GRU over action encodings, learned key/query/value-style projections, and attention weights that quantify the influence of state and action inputs on each output variable.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Attention-based inference networks for structural equation estimation</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>For each endogenous output variable v_j, an inference network f_j encodes each input variable (state and action) into shared embeddings, uses a GRU to embed the action parents into an action embedding e_j, projects e_j into a query q_j and an action contribution vector c_j^a, and projects each parent state into a contribution vector c_j^i. Each input state variable also has a learned key vector k_i. Influence (attention) weights α_j^i and α_j^a are computed by softmax-like inner products between keys and query (with a dedicated normalization that includes the action as an additional term). The hidden representation h_j is the attention-weighted sum of contribution vectors; a decoder D_j maps h_j to parameters of the posterior distribution Pr(v_j | PA(v_j)). The attention weights serve as influence scores used to extract action-specific parents by thresholding (τ) to convert the SCM into per-action AIM parents.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same RL environments as above (LunarLander-Continuous, Build-Marine, Cartpole, LunarLander-Discrete) and synthetic env</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive RL environments where collected transitions are used to fit the inference networks; the networks operate on factorized variables and need to adapt to changing discovered causal graphs during training.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Attention weighting with thresholding (salience threshold τ) to select salient parents and thereby downweight less influential inputs; used in combination with the causal-stage graph to avoid applying attention over spurious edges.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant but correlated inputs (spurious associations) that can mislead plain attention mechanisms in non-i.i.d. RL data.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Attention weights indicate salience; large attention weight on a variable signals high influence under current action embedding; but attention alone does not reliably detect spurious associations when the input graph is full.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Attention score α_j^i provides a continuous downweighting of contributions; a salience threshold τ is applied to ignore (drop) low-weight parents when deriving AIM parents for explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>When applied on top of a discovered causal graph (Caus+Attn) the attention mechanism enabled extraction of per-action salient parents and produced highly accurate AIM recovery (99.3% on synthetic test).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Attention used alone with a full graph produced many spurious causal chains and lower accuracy (90.0% AIM-recovery accuracy reported in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Attention provides a mechanism to quantify action-dependent influence and to downweight weak parents via threshold τ, but attention alone (without a prior causal graph) is vulnerable to spurious correlations in RL-collected (non-i.i.d.) data; combining attention with a causal-stage graph yields accurate, interpretable, action-specific parent sets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Reinforcement Learning via a Causal World Model', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e758.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e758.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Caus+Attn</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Two-stage Causal Discovery + Attention (Causal Stage then Attention Stage)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's main pipeline: first discover a sparse bipartite SCM via conditional independence tests (Fast CIT), then fit attention-based inference networks on that graph and use attention weights (with threshold τ) to extract per-action parent sets and causal chains (convert SCM -> AIM). This combination is designed to preclude spurious correlations and to extract action-dependent influences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Two-stage causal discovery followed by attention-based AIM extraction (Caus+Attn)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Stage 1 (causal stage): perform conditional independence testing (Fast CIT) on collected transitions to discover a sparse, bipartite SCM with only edges from inputs u to outputs v, using the rule u_i ∈ PA(v_j) iff (u_i ̸⟂ v_j | u_-i). The procedure includes a threshold η to control graph sparsity. Stage 2 (attention stage): fit attention-based inference networks only using the discovered parent sets; for each output, compute attention weights over parent states and the action embedding; convert the SCM into an AIM by selecting action-specific parents with attention threshold τ (PA^a(v_j) = {s_i ∈ PA(v_j) | α_j^i > τ}), then produce causal chains and explanations. The combination leverages CIT to remove spurious edges and attention to extract action-dependent influences and saliencies. The paper proves soundness of the CIT rule (Theorem 2) and analyzes interpretability-vs-accuracy tradeoff controlled by η (Theorem 3).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>LunarLander-Continuous, LunarLander-Discrete, Cartpole, Build-Marine, and a synthetic spurious-correlation test environment</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive RL domains modeled as Factorized MDPs; environments are not designed as open laboratory experiments but allow the agent to interact and collect transitions; the discovery pipeline uses passive data from replay buffers (no explicit experimental intervention design).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Combination of conditional independence tests to remove spurious edges (block backdoor paths) plus attention-based salience scoring and thresholding to ignore weak parents; theoretical guarantees for parent identifiability under stated assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Policy-induced correlations, transition-history-induced correlations (backdoor paths), irrelevant variables correlated with outputs (non-causal associations).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Detection of spurious associations is done via conditional independence tests (Fast CIT) conditioned on u_-i, which are designed to block backdoor/confounding paths; attention highlights salience but is prone to be misled if applied on a full graph.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Attention weights α provide continuous downweighting; threshold τ is used to drop non-salient parents when producing AIM parents; causal-stage removal of edges completely removes spurious parents from consideration.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Edges are refuted when conditional independence tests indicate no dependence after conditioning on u_-i; empirical refutation demonstrated by comparing causal chains produced with and without the causal-stage and by synthetic env experiments showing removal of spurious edges (Appendix F example).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>In the synthetic environment designed to include copious spurious correlations, the combined Caus+Attn pipeline recovered ground-truth AIM dependencies with 99.3% accuracy (Table 1). In RL tasks, the world model trained via this pipeline produced causal chains that matched known game rules (Build-Marine) and yielded model-based RL performance close to dense baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Baseline without causal-stage (Full graph + Attn) achieved 90.0% AIM-recovery accuracy on the synthetic test; the Direct per-action discovery baseline achieved 97.0% but is sample-inefficient and computationally costly.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The two-stage pipeline is empirically and theoretically effective at preventing and removing spurious correlations common in RL-collected (non-i.i.d.) data: (1) causal discovery via CIT removes many spurious edges that would mislead attention-only models, (2) attention extracts action-specific influence once the spurious parents are removed, and (3) the combined method outperforms attention-over-full-graph and matches or exceeds a direct per-action discovery baseline while being more sample-efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Reinforcement Learning via a Causal World Model', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e758.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e758.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Direct-AIM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct per-action AIM discovery (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A direct (baseline) approach that splits the transition data by action and performs causal discovery separately for each action to learn per-action SCMs/AIMs; theoretically sound but sample-inefficient and computationally expensive in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Direct action-wise causal discovery (Direct approach)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Split the dataset by action a ∈ A and perform causal discovery (conditional independence testing) independently on each subset to learn |A| separate SCMs (i.e., an AIM directly). This approach directly yields action-specific parents but has poor sample efficiency because data for each action are fewer and computational cost scales with |A|.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic spurious-correlation environment (used for AIM recovery evaluation) and other RL benchmarks as baseline comparison</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same Factorized MDP settings; requires sufficient per-action data so it is vulnerable when some actions are rare under the agent's policy.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Conditional independence testing performed per-action (same CIT principle but applied to reduced per-action datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Same as CIT: backdoor paths and correlated non-causal variables; however, splitting data may exacerbate insufficient conditioning information and thus affect detection.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Per-action conditional independence tests (e.g., Fast CIT) applied to each action-specific subset.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Edges are refuted if CIT indicates conditional independence within the action-specific subset.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Achieved 97.0% AIM-recovery accuracy on the synthetic spurious-correlation test (Table 1), showing good theoretical behavior when enough per-action data are available.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Direct per-action discovery is theoretically sound and can recover AIM parents accurately when sufficient data per action exist, but it is sample-inefficient and computationally expensive, making it less practical than the two-stage Caus+Attn pipeline in many RL settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Reinforcement Learning via a Causal World Model', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e758.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e758.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Full+Attn</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Full Graph + Attention baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that fits attention-based inference networks on a full (dense) input-to-output graph (all possible u->v edges allowed) and uses attention to infer influence; this approach is vulnerable to spurious correlations in non-i.i.d. RL data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Full dense graph with attention</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Construct a full bipartite graph where every input u_i is considered a parent of every output v_j; fit attention-based inference networks that learn attention weights over the full parent set and use these weights to derive influence and explanations (no causal-stage pruning).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same RL environments and synthetic spurious-correlation environment</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Factorized MDPs; uses the same attention machinery but without prior edge pruning, so attention is free to place high weight on spurious but predictive correlates.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Temporal correlations, policy-induced correlations, variables that co-vary with targets without being causal (e.g., time correlating with resource counts in Build-Marine).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Attention weights indicate influence but there is no mechanism to reliably detect and remove spurious parents that arise from non-causal correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Attention weights can downweight some inputs, but attention alone was observed to often place high weight on spurious correlates in RL-collected data.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Performance on synthetic AIM-recovery test: 90.0% accuracy (Table 1), significantly worse than Caus+Attn; produced many unreasonable causal chains (Appendix F example).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using attention over a full graph is insufficient to refute or avoid spurious causal relationships in RL data; the model is easily misled by predictive but non-causal covariates (e.g., time), producing incorrect causal chains and lower AIM-recovery accuracy compared to a causal-pruned approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explainable Reinforcement Learning via a Causal World Model', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Fast Conditional Independence Test for Vector Variables with Large Sample Sizes <em>(Rating: 2)</em></li>
                <li>Explainable Reinforcement Learning through a Causal Lens <em>(Rating: 2)</em></li>
                <li>Distal Explanations for Model-free Explainable Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Causal Dynamics Learning for Task-Independent State Abstraction <em>(Rating: 2)</em></li>
                <li>Provably Efficient Causal Reinforcement Learning with Confounded Observational Data <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-758",
    "paper_id": "paper-258479970",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "FastCIT",
            "name_full": "Fast Conditional Independence Test (Fast CIT)",
            "brief_description": "A fast statistical conditional independence test for vector variables used to determine whether an input variable is conditionally independent of an output given the other inputs; implemented here to discover bipartite causal graphs in factorized MDPs by testing each (u_i, v_j) pair conditioned on u_-i.",
            "citation_title": "Fast Conditional Independence Test for Vector Variables with Large Sample Sizes",
            "mention_or_use": "use",
            "method_name": "Fast Conditional Independence Test (FCIT) for bipartite causal discovery",
            "method_description": "The method implements conditional independence testing for every candidate input-output pair (u_i, v_j) using the Fast CIT algorithm (Chalupka et al., 2018). Under the factorized-MDP assumption that next-state/outcome variables are independent conditioned on the current inputs u = (s,a), the causal graph is bipartite (edges only from u to v). For each pair it tests whether u_i is conditionally independent of v_j given all other input variables u_-i (i.e. test (u_i ⟂ v_j | u_-i)). If dependence remains when conditioned on u_-i, the edge u_i -&gt; v_j is included in the discovered SCM. The paper proves (Theorem 2) that this rule is sound under causal faithfulness and independent-transition assumptions.",
            "environment_name": "Various factorized RL environments (LunarLander-Continuous, LunarLander-Discrete, Cartpole, Build-Marine) and a synthetic spurious-correlation environment",
            "environment_description": "Interactive reinforcement-learning environments modeled as Factorized MDPs: states and actions factorized into multiple variables; transitions collected by agents into replay buffers (non-i.i.d. data); environments allow agent-driven experiments via policy interaction but no explicit active experimental design is used by the discovery routine.",
            "handles_distractors": true,
            "distractor_handling_technique": "Conditional independence testing (variable selection) that conditions on all other input variables (u_-i) to block backdoor paths and thereby exclude spurious input-output edges; theoretical justification (Theorem 2) under independent transition and faithfulness assumptions.",
            "spurious_signal_types": "Backdoor paths / spurious correlations arising from policy-dependent action sampling and transition history; irrelevant variables correlated with targets (non-causal associations).",
            "detection_method": "Statistical conditional independence tests (Fast CIT) for each candidate edge, conditioning on the complement set u_-i to detect whether observed associations are due to direct causation or to confounding/backdoor paths.",
            "downweighting_method": null,
            "refutation_method": "Edges are removed (refuted) when the conditional independence test indicates u_i ⟂ v_j | u_-i; theoretical proof (Theorem 2) shows that, under assumptions, the rule uniquely identifies parents and thereby rules out spurious edges.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "When used as the causal-stage in the two-stage pipeline (Caus+Attn) the approach contributed to recovering the ground-truth AIM with 99.3% accuracy in the designed synthetic environment with spurious correlations (reported in paper Table 1).",
            "performance_without_robustness": null,
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Applying Fast CIT conditioning on u_-i in factorized MDPs reliably prevents many spurious edges that arise due to policy-induced correlations and transition history; causal-stage discovery substantially reduces spurious correlations compared to using a full graph with attention alone and yields the most accurate recovery of action-specific parents when combined with the attention stage.",
            "uuid": "e758.0",
            "source_info": {
                "paper_title": "Explainable Reinforcement Learning via a Causal World Model",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "AttnNet",
            "name_full": "Attention-based Inference Networks",
            "brief_description": "A set of structural-equation approximators that map discovered parents to posterior distributions using learned variable encoders, a GRU over action encodings, learned key/query/value-style projections, and attention weights that quantify the influence of state and action inputs on each output variable.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Attention-based inference networks for structural equation estimation",
            "method_description": "For each endogenous output variable v_j, an inference network f_j encodes each input variable (state and action) into shared embeddings, uses a GRU to embed the action parents into an action embedding e_j, projects e_j into a query q_j and an action contribution vector c_j^a, and projects each parent state into a contribution vector c_j^i. Each input state variable also has a learned key vector k_i. Influence (attention) weights α_j^i and α_j^a are computed by softmax-like inner products between keys and query (with a dedicated normalization that includes the action as an additional term). The hidden representation h_j is the attention-weighted sum of contribution vectors; a decoder D_j maps h_j to parameters of the posterior distribution Pr(v_j | PA(v_j)). The attention weights serve as influence scores used to extract action-specific parents by thresholding (τ) to convert the SCM into per-action AIM parents.",
            "environment_name": "Same RL environments as above (LunarLander-Continuous, Build-Marine, Cartpole, LunarLander-Discrete) and synthetic env",
            "environment_description": "Interactive RL environments where collected transitions are used to fit the inference networks; the networks operate on factorized variables and need to adapt to changing discovered causal graphs during training.",
            "handles_distractors": null,
            "distractor_handling_technique": "Attention weighting with thresholding (salience threshold τ) to select salient parents and thereby downweight less influential inputs; used in combination with the causal-stage graph to avoid applying attention over spurious edges.",
            "spurious_signal_types": "Irrelevant but correlated inputs (spurious associations) that can mislead plain attention mechanisms in non-i.i.d. RL data.",
            "detection_method": "Attention weights indicate salience; large attention weight on a variable signals high influence under current action embedding; but attention alone does not reliably detect spurious associations when the input graph is full.",
            "downweighting_method": "Attention score α_j^i provides a continuous downweighting of contributions; a salience threshold τ is applied to ignore (drop) low-weight parents when deriving AIM parents for explanations.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "When applied on top of a discovered causal graph (Caus+Attn) the attention mechanism enabled extraction of per-action salient parents and produced highly accurate AIM recovery (99.3% on synthetic test).",
            "performance_without_robustness": "Attention used alone with a full graph produced many spurious causal chains and lower accuracy (90.0% AIM-recovery accuracy reported in Table 1).",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Attention provides a mechanism to quantify action-dependent influence and to downweight weak parents via threshold τ, but attention alone (without a prior causal graph) is vulnerable to spurious correlations in RL-collected (non-i.i.d.) data; combining attention with a causal-stage graph yields accurate, interpretable, action-specific parent sets.",
            "uuid": "e758.1",
            "source_info": {
                "paper_title": "Explainable Reinforcement Learning via a Causal World Model",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Caus+Attn",
            "name_full": "Two-stage Causal Discovery + Attention (Causal Stage then Attention Stage)",
            "brief_description": "The paper's main pipeline: first discover a sparse bipartite SCM via conditional independence tests (Fast CIT), then fit attention-based inference networks on that graph and use attention weights (with threshold τ) to extract per-action parent sets and causal chains (convert SCM -&gt; AIM). This combination is designed to preclude spurious correlations and to extract action-dependent influences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Two-stage causal discovery followed by attention-based AIM extraction (Caus+Attn)",
            "method_description": "Stage 1 (causal stage): perform conditional independence testing (Fast CIT) on collected transitions to discover a sparse, bipartite SCM with only edges from inputs u to outputs v, using the rule u_i ∈ PA(v_j) iff (u_i ̸⟂ v_j | u_-i). The procedure includes a threshold η to control graph sparsity. Stage 2 (attention stage): fit attention-based inference networks only using the discovered parent sets; for each output, compute attention weights over parent states and the action embedding; convert the SCM into an AIM by selecting action-specific parents with attention threshold τ (PA^a(v_j) = {s_i ∈ PA(v_j) | α_j^i &gt; τ}), then produce causal chains and explanations. The combination leverages CIT to remove spurious edges and attention to extract action-dependent influences and saliencies. The paper proves soundness of the CIT rule (Theorem 2) and analyzes interpretability-vs-accuracy tradeoff controlled by η (Theorem 3).",
            "environment_name": "LunarLander-Continuous, LunarLander-Discrete, Cartpole, Build-Marine, and a synthetic spurious-correlation test environment",
            "environment_description": "Interactive RL domains modeled as Factorized MDPs; environments are not designed as open laboratory experiments but allow the agent to interact and collect transitions; the discovery pipeline uses passive data from replay buffers (no explicit experimental intervention design).",
            "handles_distractors": true,
            "distractor_handling_technique": "Combination of conditional independence tests to remove spurious edges (block backdoor paths) plus attention-based salience scoring and thresholding to ignore weak parents; theoretical guarantees for parent identifiability under stated assumptions.",
            "spurious_signal_types": "Policy-induced correlations, transition-history-induced correlations (backdoor paths), irrelevant variables correlated with outputs (non-causal associations).",
            "detection_method": "Detection of spurious associations is done via conditional independence tests (Fast CIT) conditioned on u_-i, which are designed to block backdoor/confounding paths; attention highlights salience but is prone to be misled if applied on a full graph.",
            "downweighting_method": "Attention weights α provide continuous downweighting; threshold τ is used to drop non-salient parents when producing AIM parents; causal-stage removal of edges completely removes spurious parents from consideration.",
            "refutation_method": "Edges are refuted when conditional independence tests indicate no dependence after conditioning on u_-i; empirical refutation demonstrated by comparing causal chains produced with and without the causal-stage and by synthetic env experiments showing removal of spurious edges (Appendix F example).",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "In the synthetic environment designed to include copious spurious correlations, the combined Caus+Attn pipeline recovered ground-truth AIM dependencies with 99.3% accuracy (Table 1). In RL tasks, the world model trained via this pipeline produced causal chains that matched known game rules (Build-Marine) and yielded model-based RL performance close to dense baselines.",
            "performance_without_robustness": "Baseline without causal-stage (Full graph + Attn) achieved 90.0% AIM-recovery accuracy on the synthetic test; the Direct per-action discovery baseline achieved 97.0% but is sample-inefficient and computationally costly.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "The two-stage pipeline is empirically and theoretically effective at preventing and removing spurious correlations common in RL-collected (non-i.i.d.) data: (1) causal discovery via CIT removes many spurious edges that would mislead attention-only models, (2) attention extracts action-specific influence once the spurious parents are removed, and (3) the combined method outperforms attention-over-full-graph and matches or exceeds a direct per-action discovery baseline while being more sample-efficient.",
            "uuid": "e758.2",
            "source_info": {
                "paper_title": "Explainable Reinforcement Learning via a Causal World Model",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Direct-AIM",
            "name_full": "Direct per-action AIM discovery (baseline)",
            "brief_description": "A direct (baseline) approach that splits the transition data by action and performs causal discovery separately for each action to learn per-action SCMs/AIMs; theoretically sound but sample-inefficient and computationally expensive in practice.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Direct action-wise causal discovery (Direct approach)",
            "method_description": "Split the dataset by action a ∈ A and perform causal discovery (conditional independence testing) independently on each subset to learn |A| separate SCMs (i.e., an AIM directly). This approach directly yields action-specific parents but has poor sample efficiency because data for each action are fewer and computational cost scales with |A|.",
            "environment_name": "Synthetic spurious-correlation environment (used for AIM recovery evaluation) and other RL benchmarks as baseline comparison",
            "environment_description": "Same Factorized MDP settings; requires sufficient per-action data so it is vulnerable when some actions are rare under the agent's policy.",
            "handles_distractors": true,
            "distractor_handling_technique": "Conditional independence testing performed per-action (same CIT principle but applied to reduced per-action datasets).",
            "spurious_signal_types": "Same as CIT: backdoor paths and correlated non-causal variables; however, splitting data may exacerbate insufficient conditioning information and thus affect detection.",
            "detection_method": "Per-action conditional independence tests (e.g., Fast CIT) applied to each action-specific subset.",
            "downweighting_method": null,
            "refutation_method": "Edges are refuted if CIT indicates conditional independence within the action-specific subset.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Achieved 97.0% AIM-recovery accuracy on the synthetic spurious-correlation test (Table 1), showing good theoretical behavior when enough per-action data are available.",
            "performance_without_robustness": null,
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Direct per-action discovery is theoretically sound and can recover AIM parents accurately when sufficient data per action exist, but it is sample-inefficient and computationally expensive, making it less practical than the two-stage Caus+Attn pipeline in many RL settings.",
            "uuid": "e758.3",
            "source_info": {
                "paper_title": "Explainable Reinforcement Learning via a Causal World Model",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Full+Attn",
            "name_full": "Full Graph + Attention baseline",
            "brief_description": "A baseline that fits attention-based inference networks on a full (dense) input-to-output graph (all possible u-&gt;v edges allowed) and uses attention to infer influence; this approach is vulnerable to spurious correlations in non-i.i.d. RL data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Full dense graph with attention",
            "method_description": "Construct a full bipartite graph where every input u_i is considered a parent of every output v_j; fit attention-based inference networks that learn attention weights over the full parent set and use these weights to derive influence and explanations (no causal-stage pruning).",
            "environment_name": "Same RL environments and synthetic spurious-correlation environment",
            "environment_description": "Factorized MDPs; uses the same attention machinery but without prior edge pruning, so attention is free to place high weight on spurious but predictive correlates.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Temporal correlations, policy-induced correlations, variables that co-vary with targets without being causal (e.g., time correlating with resource counts in Build-Marine).",
            "detection_method": "Attention weights indicate influence but there is no mechanism to reliably detect and remove spurious parents that arise from non-causal correlations.",
            "downweighting_method": "Attention weights can downweight some inputs, but attention alone was observed to often place high weight on spurious correlates in RL-collected data.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Performance on synthetic AIM-recovery test: 90.0% accuracy (Table 1), significantly worse than Caus+Attn; produced many unreasonable causal chains (Appendix F example).",
            "performance_without_robustness": null,
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Using attention over a full graph is insufficient to refute or avoid spurious causal relationships in RL data; the model is easily misled by predictive but non-causal covariates (e.g., time), producing incorrect causal chains and lower AIM-recovery accuracy compared to a causal-pruned approach.",
            "uuid": "e758.4",
            "source_info": {
                "paper_title": "Explainable Reinforcement Learning via a Causal World Model",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Fast Conditional Independence Test for Vector Variables with Large Sample Sizes",
            "rating": 2,
            "sanitized_title": "fast_conditional_independence_test_for_vector_variables_with_large_sample_sizes"
        },
        {
            "paper_title": "Explainable Reinforcement Learning through a Causal Lens",
            "rating": 2,
            "sanitized_title": "explainable_reinforcement_learning_through_a_causal_lens"
        },
        {
            "paper_title": "Distal Explanations for Model-free Explainable Reinforcement Learning",
            "rating": 2,
            "sanitized_title": "distal_explanations_for_modelfree_explainable_reinforcement_learning"
        },
        {
            "paper_title": "Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning",
            "rating": 2,
            "sanitized_title": "systematic_evaluation_of_causal_discovery_in_visual_model_based_reinforcement_learning"
        },
        {
            "paper_title": "Causal Dynamics Learning for Task-Independent State Abstraction",
            "rating": 2,
            "sanitized_title": "causal_dynamics_learning_for_taskindependent_state_abstraction"
        },
        {
            "paper_title": "Provably Efficient Causal Reinforcement Learning with Confounded Observational Data",
            "rating": 1,
            "sanitized_title": "provably_efficient_causal_reinforcement_learning_with_confounded_observational_data"
        }
    ],
    "cost": 0.017502749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Explainable Reinforcement Learning via a Causal World Model</p>
<p>Zhongwei Yu yuzhongwei2021@ia.ac.cn 
Institute of Automation
Chinese Academy of Sciences</p>
<p>Jingqing Ruan ruanjingqing2019@ia.ac.cn 
Institute of Automation
Chinese Academy of Sciences</p>
<p>Dengpeng Xing dengpeng.xing@ia.ac.cn 
Institute of Automation
Chinese Academy of Sciences</p>
<p>Explainable Reinforcement Learning via a Causal World Model
1D4F9B02BEDC1251F8D4B05E8BA4F10F
Generating explanations for reinforcement learning (RL) is challenging as actions may produce longterm effects on the future.In this paper, we develop a novel framework for explainable RL by learning a causal world model without prior knowledge of the causal structure of the environment.The model captures the influence of actions, allowing us to interpret the long-term effects of actions through causal chains, which present how actions influence environmental variables and finally lead to rewards.Different from most explanatory models which suffer from low accuracy, our model remains accurate while improving explainability, making it applicable in model-based learning.As a result, we demonstrate that our causal model can serve as the bridge between explainability and learning.</p>
<p>Introduction</p>
<p>Many real-world applications like finance and healthcare require AI systems to be well understood by users due to the demand for safety, security, and legality [Gunning et al., 2019].Aiming to help people better understand and work with AI systems, the field of Explainable AI (XAI) has recently attracted increasing interest from researchers.For example, a number of explanatory tools have been developed to pry into the black box of deep neural networks [Bach et al., 2015;Selvaraju et al., 2020;Wang et al., 2021b].</p>
<p>However, the domain of explainable reinforcement learning (XRL) has been neglected for a long time.Many XRL studies adopt classic tools of XAI such as saliency maps [Nikulin et al., 2019;Joo and Kim, 2019;Shi et al., 2021].These tools are not designed for sequential decision-making and are weak in interpreting the temporal dependencies of RL environments.Therefore, some studies investigate explaining specific components of the decision process, e.g., observations [Koul et al., 2018;Raffin et al., 2019], actions [Fukuchi et al., 2017;Yau et al., 2020], policies [Amir and Amir, 2018;Coppens et al., 2019], and rewards [Juozapaitis et al., 2019].However, these studies rarely combine explanations with the dynamics of environments, which is important for understanding the long-term effects produced by agents' actions.In addition, real-world environments usually contain dynamics unknown to users, making it crucial to interpret these dynamics using explanatory models.Model-based RL (MBRL) uses predictive world models [Nagabandi et al., 2018;Kaiser et al., 2020;Janner et al., 2021] to capture such dynamics.However, these models are usually densely-connected neural networks and cannot be used for the purpose of explanation.</p>
<p>Psychological research suggests that people explain the world through causality [Sloman, 2005].In this paper, we propose a novel framework that uses an interpretable world model to generate explanations.Rather than using a dense and fully-connected model, we perform causal discovery to construct a sparse model that is aware of the causal relationships within the dynamics of environments.In order to explain agents' decisions, the proposed causal model allows us to construct causal chains which present the variables causally influenced by the agent's actions.The proposed model advances the existing work that uses causality for explainable RL [Madumal et al., 2020b;Madumal et al., 2020a], as it does not require a causal structure provided by domain experts, and is applicable to continuous action space.</p>
<p>Apart from interpreting the world, humans also use causality to guide their learning process [Cohen et al., 2020].However, the trade-off between interpretability and performance [Gunning et al., 2019;Longo et al., 2020;Puiutta and Veith, 2020] indicates that explainable models are usually inaccurate and can hardly benefit learning.On the contrary, our model is sufficiently accurate, leading to a performance close to dense models in MBRL.Therefore, we can train the agent and explain its decisions through exactly the same model, making explanations more faithful to the agent's intention.This is significant for overcoming the issue that post-hoc explanations like saliency maps can sometimes fail to faithfully unravel the decision-making process [Atrey et al., 2020].</p>
<p>Our main contributions are as follows: 1) We learn a causal model that captures the environmental dynamics without prior knowledge of the causal structure.2) We design a novel approach to effectively extract the causal influence of actions, allowing us to derive causal chains for explaining the agent's decisions.3) We show that our explanatory model is accurate enough to guide policy learning in MBRL.arXiv:2305.02749v5[cs.LG] 18 Jan 2024 2 Background</p>
<p>Causality in Reinforcement Learning</p>
<p>RL integrated with causality has recently become noticed by RL researchers.For example, Lu [2018], Wang [2021a] and Yang [2022] et al. use causal inference to improve the robustness against confounders or adversarial intervention; Dietterich et al. [2018] improve learning efficiency by removing the variables unrelated to the agent's action; Nair et al. [2019] construct a causal policy model; Seitzer et al. [2021] improve exploration by detecting the causal influence of actions; Wang [2022] and Ding [2022] et al. investigate causal world models as we do.However, they mainly use causality to improve generalization rather than generate explanations.</p>
<p>Only a few studies consider improving explainability using a causal model.Madumal et al. [2020b] propose the Action Influence Model (AIM), a causal model specialized for RL, to generate explanations about the agent's actions.</p>
<p>Another study [Madumal et al., 2020a] further combines the AIM with decision trees to improve the quality of explanations.However, these approaches require finite action space, and the causal structure is given beforehand by human experts.In addition, they only consider a low-accuracy model, which cannot be used for policy learning.Volodin [2021] proposes a method to learn a sparse causal graph of hidden variables abstracted from high-dimensional observations, which improves the understanding of the environment.However, the approach provides no insight into the agent's behavior.</p>
<p>Structural Causal Model</p>
<p>A Structrual Causal Model (SCM) [Pearl et al., 2016], denoted as a tuple (U, V, F), formalizes the causal relationships between multiple variables.U and V contain the variables of the SCM, where U = {u 1 , ..., u p } is the set of exogenous variables and V = {v 1 , ..., v q } is the set of endogenous variables.F = {f 1 , ..., f q } is the set of structural equations, where f j formulates how v j is quantitatively decided by other variables.We call f j a "structural" equation as it defines the subset of variables denoted as P A(v j ) ⊆ U ∪ V ∖ {v j } (i.e., the parent variables) that directly decide the value of v j .</p>
<p>An SCM is usually represented as a directed acyclic graph (DAG) G = (U ∪ V, E) called the causal graph.The node set of G is exactly U ∪ V and the edge set E is given by the structural equations:
(x i , v j ) ∈ E ⇔ x i ∈ P A(v j ), where x i ∈ U ∪ V.
For simplicity, symbols like u i , v j , and x i may denote the names or the values of the variables according to the context.In addition, we consider stochastic structural equations, where f j outputs the posterior distribution of v j conditioned on its direct parents P A(v j ):
Pr(v j |P A(v j )) ∼ f j (P A(v j )).
(1)</p>
<p>Action Influence Model</p>
<p>An AIM, denoted as the tuple (U, V, F, A), is a causal model specialized for RL.Here, U and V follow the same definition in SCM.F is the set of structural equations, and A is the action space of the agent.Different from SCM, each structural equation is related to not only an endogenous variable but also a unique action in A. In other words, there exists a structural equation f j a ∈ F for any action a ∈ A and any endogenous variable v j ∈ V to describe how v j is causally determined under action a.We use P A a (v j ) ⊆ U ∪ V ∖ {v j } to denote the causal parents of v j under action a.Then, the posterior distribution of v j under action a is given by
Pr(v j |P A a (v j ), a) ∼ f j a (P A a (v j )).
(2) As a result, there exist overall |A| × |V| causal equations in the AIM.In fact, we may also reckon the AIM as an ensemble of |A| SCMs, where each SCM accounts for the influence of a unique action in A.</p>
<p>Factorized MDP</p>
<p>We are interested in tasks where the action and state can be factorized into multiple variables, and formalize such a task as a Factorized MDP (FMDP) denoted by the tuple ⟨S, A, O, R, P, T, γ⟩.Here, S, A, and O respectively denote the state space, action space, and outcome space.Each state s ∈ S is factorized into n s state variables such that s = (s 1 , ..., s ns ), where s i is the i-th state variable.Similarly, we have a = (a 1 , ..., a na ) for each action and o = (o 1 , ..., o no ) for each outcome.Figure 1(a) illustrates an example of the factorization for a simple 2-grid environment called the Vacuum world [Russell et al., 2010].Its state variables include the position of the vacuum and whether the places are clean (clean 1 and clean 2 ); it contains only one action variable a that is chosen from Left, Right, and Suck (making the place clean).The action of Left or Right leads to an outcome of failure when blocked by the world boundary.</p>
<p>On each step, the agent observes the current state s and takes an action a, then the state transits and the outcome is produced according to the transition probability P (o ′ , s ′ |s, a), leading to a transition tuple denoted as δ = (s, a, o, s ′ ).Meanwhile, the reward is given by the overall reward function R(δ).Following the reward decomposition [Juozapaitis et al., 2019], we factorize R as the summation of n r reward variables, given by R(δ) = ∑ nr i=1 r i (δ).γ is the discount factor for computing returns.T is the termination condition deciding whether the episode terminates based on the transition δ.How reward variables {r i } nr i=1 and the termination condition T depend on the transition δ is defined by users according to their demands.In those cases where R and T contain components unknown to users, we may put these components into the outcome variables.That is, we use an outcome variable o i to indicate the unknown reward, and define the corresponding reward variable as r i ≡ o i .</p>
<p>The Proposed Framework</p>
<p>There exist two perspectives on the causal model of the dynamics of the environment: 1) In a unified SCM, action variables are merely nodes of the causal structure and are treated evenly as state variables.2) In an AIM, each action specifies a peculiar causal structure, leading to a good understanding of both the environment and the agent's decisions [Madumal et al., 2020b].To make it clearer, Figure 1 illustrates how a unified SCM and an AIM pertain to our setting in the above-mentioned Vacuum world.However, directly learning an AIM is intractable as we must divide data into |A| subsets to respectively learn |A| SCMs.This reduces sample efficiency, produces redundant parameters, and cannot be applied to infinite action space.Therefore, we seek to build a unified SCM that can be converted to an AIM based on specially-designed structural equations.As illustrated in Figure 2(a), the exogenous variables in this SCM are the state variables s and action variables a; the endogenous variables are the next-state variables s ′ and outcome variables o.In the following description, we define u ∶= (s, a) = (s 1 , ..., s ns , a 1 , ..., a na ) as the input (exogenous) variables and v ∶= (s ′ , o) = (s ′ 1 , ..., s ′ ns , o 1 , ..., o no ) as the output (endogenous) variables of our model.In this way, a transition tuple can also be written as δ = (u, v).</p>
<p>The workflow of the proposed framework is illustrated in Figure 2. When the agent interacts with the environment, we store the transition data into a replay buffer D. Using the stored transitions, we perform causal discovery to identify the causal graph of the above-mentioned SCM.Then, we fit the causal equations for the variables of the next state s ′ and the outcome o using Inference Networks.Together with the known reward function R and termination condition T , we construct a causal world model that captures the dynamics of the environment.The attention weights (influence weights) in the inference networks capture the action influence, which allows us to perform causal chain analysis to reveal the variables that are causally influenced by the agent's action.The discovered causal graph interprets the environmental dynamics, and the causal chain analysis provides explanations about the agent's decisions.Moreover, this causal world model can be used by MBRL algorithms to facilitate learning.</p>
<p>Causal Discovery</p>
<p>We assume that output variables v are produced independently conditioned on the input variables u, as independence underlies the intuition of humans to segment the world into components.Under this assumption, it is proven that the causal graph is bipartite, where no lateral edge exists in v, and each edge starts in u and ends in v. Therefore, we only need to determine whether there exists a causal edge for each variable pair (u i , v j ), where 1 ≤ i ≤ n s + n a and 1 ≤ j ≤ n s + n o .Studies have shown that conditional independent tests (CITs) can be used to perform efficient causal discovery [Wang et al., 2022;Ding et al., 2022].In this work, we implement CITs through Fast CIT [Chalupka et al., 2018] and determine each edge using the following rule:
u i ∈ P A(v j ) ⇐⇒ (u i ̸ v j |u −i ),(3)
where u −i denotes all variables in u other than u i .In Appendix A, we provide the theoretical basics of causal discovery and prove a theorem showing that Equation 3 leads to sound causal graphs.</p>
<p>Attention-based Inference Networks</p>
<p>To perform causal inference on the discovered causal graph, we fit the structural equation of each output variable v j using an inference network denoted as f j , which takes the causal parents P A(v j ) as inputs and predicts the posterior distribution Pr(v j |P A(v j )).These inference networks should adapt to the structural changes of the causal graph, as the agent's exploratory behaviors may reveal undiscovered causal relationships and lead to new causal structures.To achieve this, Ding et al. [2022] use Gated Recurrent Unit (GRU) networks that sequentially input all parent variables without discriminating the state and the action.To model the action influence, we design the attention-based inference networks as illustrated in Figure 2(c).</p>
<p>To handle heterogenous input variables (which may be scalars or vectors of different lengths), we first use variable encoders (each uniquely belongs to an input variable) to individually map the input variables to vectors of the same length.These encoders are shared by the inference networks of all output variables.In particular, we use ũ = {s 1 , ..., sn , ã1 , ..., ãm } to denote these encoding vectors of input variables.</p>
<p>Then, for each inference network f j , we compute the contribution vectors of the parent state variables through linear transforms:
c j i = W j s si + b j s , s i ∈ P a(v j ).(4)
The usage of these contribution vectors is equal to the "value vectors" in key-value attention.We use the term "contribution vectors" since the word "value" is ambiguous in the context of RL.Each inference network f j contains a GRU network g j , which receives the action variables in P A(v j ) and outputs the action embedding e j .Then, we feed this action embedding into linear transforms to respectively obtain the query vector q j and the action contribution vector c j a : e j = GRU j ({ã i } ai∈P A(vj ) ) ,</p>
<p>(5)
q j = W j q e j + b j q ,(6)c j a = W j a e j + b j a .(7)
The projection matrices W j s , W j q , W j a and bias vectors b j s , b j q , b j a are all trainable parameters of f j .We use the superscript j to indicate that these parameters belong to f j .Each state variable s i is allocated a key vector k i , which is a trainable parameter learned by gradient descent.We do not use the superscript j for key vectors as they are shared by inference networks of all output variables.The influence weights (i.e., attention weights) of the state variables in P A(v j ) and the action are then computed by
α j i = exp(k T i q j ) 1 + ∑ s i ′ ∈P A(vj ) exp(k T i ′ q j ) ,(8)α j a = 1 1 + ∑ s i ′ ∈P A(vj ) exp(k T i ′ q j ) .(9)
We then compute the hidden representation of the posterior distribution using the weighted sum of the value vectors:
h j = ∑ si∈P A(vj ) α j i ⋅ c j i + α j a ⋅ c j a .(10)
Finally, the distribution decoder D j maps h j to the predicted posterior distribution:
Pr(v j |P A(v j )) ∼ D j (h j ).(11)
We assume the type of this posterior distribution is previously known and D j only outputs the parameters of the distribution.</p>
<p>In our implementation, we use normal distribution (parameterized by the mean and variance) for real-number variables and use categorical distribution (parameterized by the probability of each class) for discrete variables.</p>
<p>The inference networks {f j } ns+no j=1 are trained by maximizing the log-likelihood of the transition data stored in D, written as
L inf er = ns+no ∑ j=1 1 |D| ∑ δ∈D log Pr(v j |P A(v j )). (12)</p>
<p>Causal Chain Analysis</p>
<p>In order to generate explanations, we first describe how our model can be converted to an AIM.Noticing that the key vectors {k i } n i=1 are trainable parameters, the influence weights only depend on the numeric value of action variables.Therefore, the influence weight α j i captures how much the output variable v j depends on state variable s i under the given action a.In order to generate laconic explanations, we define P A a (v j ) ∶= {s i ∈ P A(v j ) | α j i &gt; τ } as the parent set of v j with salient dependencies under the action a, where τ ∈ [0, 1] is a given threshold.In this way, we convert the SCM to an AIM, where the structural equation for v j under action a is written as
f j a (P A a (v j )) = D j ( ∑ si∈P Aa(vj ) α j i ⋅ c j i (s i ) + α j a ⋅ c j a ). (13)
Since we use the AIM for the purpose of explanation, it is tolerable to set a larger threshold, which allows us to ignore parent variables that are not influential enough.Madumal et al. have introduced methods to generate good explanations using an AIM.The key is to build a causal chain containing the variables that (i) are causally affected by the actions, and (ii) causally lead to rewards.A single causal chain starting from state s and action a leads to the explanation for "why the agent took a on s".Contrastive explanations for "why the agent took a instead of b on s" can be obtained by comparing the causal chines produced by the factual action a and the counterfactual action b.Details can be found in Appendix B and the AIM paper [Madumal et al., 2020a].</p>
<p>The rest of this section introduces how to derive a causal chain starting from the state s t at step t and an action a t (can be factual or counterfactual) using our model.First of all, we use our model and the agent's policy to simulate the most-likely trajectory δ t , δ t+1 , ..., δ t+H−1 , where H denotes the number of simulation steps.The symbol δ t+k denotes the transition tuple on step t + k, where the actions a t+k for k ≥ 1 are produced by the agent's policy.For a factual causal chain, this simulation is not necessary if factual data of these future states and actions is available.</p>
<p>Then, we build an extended graph containing the state, outcome, and reward variables of these H steps.The edges of this graph accord to the structure of the AIM derived above.That is, if s i ∈ P A a t+k (v j ) then there exists an edge from s t+k i to v t+k j for all k = 1, ..., H.It is worth mentioning that we treat the first transition δ t differently since a t is exactly the action being explained: If P A(v j ) ∩ a t = ∅, then v t j is not affected by the choice of a t .In this case, no edge will be established from any state s t i to v t j .Afterward, the explainee may specify the target variables (a subset of reward variables) he/she is interested in.Otherwise, all reward variables will be considered.We perform a graph search from the starting state variables s t and highlight all paths from s t to the target rewards.These paths together form the causal chain of action a t starting from s t .Based on this causal chain, the explanation can be presented as a picture or a natural-language description.</p>
<p>Model-based RL</p>
<p>XAI literature has widely discussed the trade-off between interpretability and performance, which is also reflected in our model.A sparser causal graph (discovered using a smaller threshold η) is usually easier to read and produces clearer explanations.However, it also enforces the model to infer posterior distributions using less information from input variables u, leading to inferior accuracy.In Appendix C, we provide a theorem that formally shows that decreasing the threshold η leads to a denser causal graph (i.e., lower interpretability) and also higher predicting accuracy.</p>
<p>In order to show our model is accurate enough to do more than generate post-hoc explanations, we consider applying our world model to MBRL to facilitate policy learning.We use a bootstrap ensemble containing 5 models to alleviate the effect of the epistemic uncertainty [Chua et al., 2018].For each iteration, we first collect real transition data into the model buffer D. Then, we update the world model by causal discovery and fitting structural equations using the data in D. Afterward, we perform k-step model-rollouts [Janner et al., 2021] to generate simulated data for updating the policy.In our implementation, the policy is trained using Proximal Policy Optimization [Schulman et al., 2017].The pseudo-code of the learning procedure is given in Appendix D.</p>
<p>Experiments</p>
<p>We present examples of causal chains in two representative environments: Lunarlander-Continuous for the continuous action space, and Build-Maine for the discrete action space.To verify whether our approach can produce correct causal chains, we design an environment to measure the accuracy of recovering causal dependencies of the ground-truth AIM.</p>
<p>Explanation Results</p>
<p>Lunarlander-Continuous</p>
<p>We factorize the state into 7 variables (x, y, ẋ, ẏ, θ, θ, legs) indicating 1) the horizontal position, 2) the vertical position, 3) the horizontal velocity, 4) the vertical velocity, 5) the angle, 6) the angle velocity, and 7) whether the two legs are in contact with the ground.The action includes 2 continuous variables ranged in (−1, 1), respectively controlling the throttles of the main and the lateral engines.The environment contains 3 outcome variables, including 1) the fuel cost due to firing the engine, 2) whether the lander crashed, and 3) whether the rocket is resting.</p>
<p>In this experiment, we learn a post-hoc model to generate explanations for a previously trained policy.We first use the policy (with noise) to collect 150k samples into the buffer D. Then, we use these samples to discover the causal graph (with the threshold η = 0.05) and train the inference networks.The resulting causal graph is presented in Figure 3(a).The environment contains many kinds of rewards, leading to complicated causal chains if we consider them all.To make our explanation clearer, we present a causal chain in Figure 4 considering only two kinds of rewards: 1) the reduction of the distance to the target location, and 2) the reduction of the angle (i.e., balancing the rocket).This causal chain shows that the agent's action a t first influences the velocities ( θ, ẋ, and ẏ) and thereby reduces (or increases) the angle (θ) and the distance (</p>
<p>√</p>
<p>x 2 + y 2 ).In addition, we observe that no parent of the outcome variable rest is discovered in the causal graph.This means the policy provides insufficient opportunities to reveal its causality.As a result, the variable rest is excluded</p>
<p>Build-Marine</p>
<p>The original observation space provided by the SC2LE interface contains hundreds of variables, which is intractable for causal discovery.In our implementation, we define the state as the tuple containing only 6 variables denoted as (n wk , n mr , n br , n dp , money, time), namely 1) the number of workers, 2) the number of marines, 3) the number of barracks, 4) the number of supply depots, 5) the amount of money, and 6) the game time.We are interested in the macrolevel decision-making and therefore define the action as one discrete variable indicating which unit (workers, marines, barracks, supply depots, or none) to be built.The microlevel control of building these units (e.g., determining where to place the new barracks) is implemented by simple rules.The goal is to produce as many marines as possible within 15 minutes.Therefore, the player is rewarded with 1 for every newly produced marine.In addition, this environment contains no outcome variable.Direct Full+Attn Caus+Attn (ours) AIM accuracy 97.0 % 90.0% 99.3%</p>
<p>Table 1: The accuracy of recovering the causal dependencies of the AIM."Direct" means the direct approach mentioned in Section 5.2; "Full" means using a full graph; "Caus" means using a causal graph; and "Attn" means using attention.</p>
<p>In this experiment, both the policy and the causal model are obtained by model-based learning (see Section 4.4).We present the final causal graph in Figure 3(b) (discovered using the threshold η = 0.15) and an example of the causal chain in Figure 5.The causal chain shows that our attention-based inference networks successfully reason the causal dependencies under different actions, which reflect the following rules of the StarCraftII game: 1) Building new barracks requires at least one supply depot; 2) marines are built from barracks; 3) the number of marines is limited by the number of supply depots; and 4) building more units requires sufficient money in hand.This causal chain explains the reason why the agent builds supply depots: to gain permission to build barracks and provide enough supplies for building marines.Interestingly, we discover no causal relationship between n wk and money ′ .For human players, it is common sense that more workers increase the efficiency of collecting minerals and thus lead to a higher income.Since the causal model is learned using the transition data produced along with policy training, this missing edge indicates that the agent explored inadequately for building more workers, providing insufficient evidence to reveal this causal relation.</p>
<p>Accuracy of Recovering Action Influence</p>
<p>Good explanations are generated from correct causal chains, which require us to accurately recover the AIMs of environments.In Section 4, we have mentioned a Direct approach that learns the AIM by splitting the data buffer D into |A| sub-buffers and performing causal discovery for each action a ∈ A. Though this direct approach is theoretically sound, it suffers from poor sample efficiency and high computational complexity.Noticing that the causal dependencies under dif- ferent actions usually share similar structures, our approach takes 2 stages: 1) In the causal stage, we learn a unified SCM, whose causal graph summarizes causal dependencies for all actions; 2) in the attention stage, we then transfer this SCM into an AIM based on attention weights (influence weights).</p>
<p>To verify whether our approach can accurately recover the AIM, we design a simple environment that contains spurious correlations to confuse neural networks (see Appendix E.2 for details).We compare our approach with two baselines: 1) the Direct approach mentioned above, and 2) a non-causal approach that uses a full causal graph and only relies on attention.The accuracy of recovering the ground-truth causal dependencies of the AIM using non-i.i.d data is shown in Table 1.These results show that: 1) the causal graph discovered in the causal stage precludes most spurious correlations, making our approach more effective than the Direct approach in practice; 2) and attention alone is insufficient to accurately extract the causal influence of actions.</p>
<p>Further, we examined the causal chains derived solely from attention (where full causal graphs are used).In these chains, we found plenty of spurious correlations, which lead to unreasonable explanations (e.g., "the number of supply depots naturally grows with time" in Build-Marine).An example of such a causal chain and the related discussion are provided in Appendix F. This result shows that causal discovery is an indispensable process for producing reasonable explanations.</p>
<p>Performance in Model-Based RL</p>
<p>We evaluate the performance of model-based policy learning using our explanatory model.We compare the learning performance with the Model-Free approach that learns policies without models.In addition, we consider two dense models as baselines: 1) the model that concatenates all exogenous variables u as inputs and infers endogenous variables using a Multi-Layer Perceptron (MLP), and 2) the Full model that adopts the same networks as ours whereas uses a full causal graph.</p>
<p>The MLP model is the most commonly used in MBRL, and the Full model follows the state-of-the-art modular architecture [Ke et al., 2021].These dense models are not suitable for generating explanations.However, they are more accurate (if well-trained) than our explainable model as they are allowed to predict each output variable based on the complete inputs.Existing studies show that causal models generalize better than dense models [Wang et al., 2022;Ding et al., 2022].However, we focus on ordinary learning problems and do not consider using our model for generalization.Therefore, we stress that the goal of this experiment is not to obtain a higher performance than dense baselines.On the contrary, We aim to figure out: 1) whether the proposed model can be of help to the learning process, and 2) how much performance our sparse model sacrifices for improved explainability.</p>
<p>The learning curves are shown in Figure 6.In all environments, the performance of our explanatory model is very close to the dense baselines.Compared to the model-free approach, the model-based approaches significantly learn faster in Cartpole and Lunarlander-Discrete and converge to higher returns in Build-Marine.The returns of all tested approaches are close in Lunarlander-Continuous, whereas the model-based approaches improve the stability of learning.These results show that our model improves explainability at an acceptable cost in performance and well balances the interpretability-accuracy trade-off.Therefore, our model can simultaneously guide policy learning and explain decisions, leading to better consistency between explanations and the agent's cognition of the environment.</p>
<p>Conclusion and Future Work</p>
<p>This paper proposes a framework that learns a causal world model to generate explanations about agents' actions.To achieve this, we perform causal discovery to identify the causal structure in the environment and fit causal equations through attention-based inference networks.These inference networks produce the influence weights that capture the influence of actions, which allow us to perform causal chain analysis in order to generate explanations.The proposed framework does not require the structural knowledge provided by human experts and is applicable to infinite action spaces.Apart from generating explanations, we successfully applied our model to model-based RL, showing that the model can be the bridge between learning and explainability.</p>
<p>A weakness of our approach is that it requires a known factorization of the environment, which limited its application scope.There exists a number of studies aiming to learn the causal feature set from raw observations [Zhang et al., 2020;Volodin, 2021;Zhang et al., 2021].Future work will put representation learning into consideration for better applicability.In addition, we currently consider model-based policy learning as the usage of our model apart from generating explanations.However, this usage does not make full use of the advantage of a causal model.Future work will investigate better usage of our model to further improve learning.</p>
<p>[ Zhang et al., 2021]</p>
<p>A Causal Discovery</p>
<p>In this section, we introduce some basics of causal discovery and then prove the soundness of our approach for causal discovery.First of all, we introduce the concept of Markov Compatibility [Pearl, 2000], which describes whether a directed acyclic graph (DAG) is able to represent the dependencies of a group of random variables.Definition 1 (Markov Compatibility).Assume x = (x 1 , x 2 , ⋯, x n ) is a group (ordered set) of random variables and Pr is a probability function on x.Assume G is a DAG whose nodes are these variables, where the parent set of x i is denoted as P A(x i ).If we have
Pr(x) = n ∏ i=1 Pr(x i |P A(x i )),(14)
then we say that Pr and G are compatible, or that G represents Pr.</p>
<p>The goal of causal discovery is to find some DAG G to represent a given probability Pr.Now, we introduce two important concepts for causal discovery: the d-separation and causal faithfulness.Definition 2 (d-separation).Assume G is a DAG on a set of variables, where x, y, and z are disjoint subsets of variables.We say that x and y is d-separated by on Z (denoted as x G y|z), if every undirected path p from x to y satisfies:
(x G y|z) ⇒ (x Pr y|z), (15)
where Pr means conditional independence under Pr.</p>
<ol>
<li>If (x Pr y|z) holds for every probability function Pr that is compatible with G, we have (x G y|z) [Pearl, 2000].Definition 3 (Causal Faithfulness).Assume G and Pr are respectively a DAG and a probability function on a set of variables.We say that Pr is faithful to G, if
(x Pr y|z) ⇒ (x G y|z),(16)
for all disjoint subsets x, y, and z of variables.</li>
</ol>
<p>Causal faithfulness indicates that all conditional independent relationships are due to the structure of the DAG instead of rare coincidence.In fact, studies have shown that if Pr is compatible with G, the chance of Pr to be not faithful to G is extremely low [Spirtes et al., 2001].In causal discovery, it is usually assumed that the probability Pr is faithful to the DAG G that we are looking for, which makes the structure of G recognizable.</p>
<p>We have not distinguished exogenous and endogenous variables above.The exogenous variables are considered the inputs of a system, and thus their causality does not need to be discussed.In other words, our causal graph only describes the causality of endogenous variables, whereas the causality of exogenous variables is ignored.Unless otherwise specified, the letter v denotes the set of endogenous variables and the letter u denotes the set of endogenous variables in the following discussion.Given a probability function Pr of some variables x = (u, v), the goal of causal discovery now becomes determining only the causal parents of endogenous variables v in a DAG G that represents Pr.</p>
<p>It is worth mentioning that, in the definition given by Pearl [2016], it is assumed that exogenous variables are independent of each other.However, this requirement is released in our definition since the current state and action variables are usually correlated due to the dependencies underlying the agent's policy and the history transitions.Ignoring these correlations leads to spurious edges, which are detrimental to generating reasonable explanations.For example, in Figure 7(a), we show that a backdoor path occurs when the actions are sampled by a policy dependent on the current state; in Figure 7(b) we show that a backdoor path occurs considering the transition history even if the actions are sampled randomly.In both cases, s 1 and s ′ 2 are correlated, making s 1 an "spurious parent" of s ′ 2 in the discovered causal graph.Therefore, we detect causal dependencies using conditional independent tests (CITs), where the condition variables block all the backdoor paths (if there exist any).We assume that state variables transit and outcome variables arise independently, which brings several benefits: 1) The causal graph is bipartite, making our model able to be computed in parallel.2) The causal graph can be uniquely identified using CITs.Wang et al. [2022] adopt a similar approach for causal discovery from the perspective of conditional mutual information.Here, we describe and prove a theorem that is used to discover sound causal graphs in our approach.</p>
<p>Theorem 2 (Causal Discovery for Factorized MDP).Assume u = (s, a) and v = (s ′ , o) are respectively the sets of exogenous and endogenous variables in a Factorized MDP.Assume that Pr is a probability function of these variables δ = (u, v) such that Pr is consistent with the MDP; that is, we have
Pr(δ) = Pr(s)π(a|s)P (v|u)(17)
where Pr(s) may follow arbitrary state distribution, π can be arbitrary policy, and P (v|u) is the transition probability of the MDP.Then, there always exists a DAG G that represents Pr.Further, if we make the following assumptions: 1. (Independent Transition) next-state variables and outcome variables are produced independently, given by
P (v|u) = ns ∏ j=1 P (v j |u) no ∏ k=1 P (o k |u);
2. (Causal Faithfulness) Pr is faithful to G, then the following propositions about G hold:</p>
<p>1.No lateral edge like v i → v j exists among v.In other worlds, we have P A(v j ) ⊆ u for every v j ∈ v. 2. The parent sets of v are uniquely identified by
(u i Pr v j |u −i ) ⇔ u i ∈ P A(v j )
for every i = 1, ⋯n s + n a and every j = 1, ⋯n s + n o .Here u −i denotes u ∖ {u i } 3. The parent sets of v in G is invariant.That is, we may replace Pr with any other probability function Pr * that satisfies Eq. 17 and obtain a new DAG G * that represents Pr * .Assuming Pr * is faithful to G * , the parent sets of v will stays unchanged:
P A(v j ) = P A * (v j ), j = 1, ⋯, n s + n o
Proof.We first prove the existence of G. Since π and P in Eq. 17 are both conditional probability function, we write that Pr(δ) = Pr(s)Pr(a|s)Pr(v|u) where Pr(a|s) = π(a|s) and Pr(v|u) = P (v|u).Using the chain rule of probability functions, we have
Pr(s) =Pr(s 1 )Pr(s 2 |s 1 )⋯Pr(s ns |s 1 , ⋯, s ns−1 ) Pr(a|s) =Pr(a 1 |s)Pr(a 2 |a 1 , s)⋯Pr(a na |a 1 , ⋯, a na−1 , s) Pr(v|u) =Pr(v 1 |u)Pr(v 2 |v 1 , u)⋯ Pr(v ns+no |v 1 , ⋯, v ns+no−1 , u)
We define
P A(s i ) ⊆ (s 1 , ⋯, s i−1 ), i = 1, ⋯, n s
as any subset such that
Pr(s i |s 1 , ⋯, s i−1 ) = Pr(s i |P A(s i )).
Similarly, we may define
P A(a i ) ⊆(s, a 1 , ⋯, a i−1 ), i = 1, ⋯, n a ; P A(s ′ i ) ⊆(u, s ′ 1 , ⋯, s ′ i−1 ), i = 1, ⋯, n s ; P A(o i ) ⊆(u, s ′ , o 1 , ⋯, o i−1 ), i = 1, ⋯, n o .
Together, we will have that
Pr(δ) = ns ∏ i=1 Pr(s i |P A(s i )) na ∏ j=1
Pr(a j |P A(a j ))
n k ∏ k=1 Pr(s ′ k |P A(s ′ k )) no ∏ l=1 Pr(o l |P A(o l )) = ∏ x∈δ Pr(x|P A(x)).
Letting the edges in G be given by the parent sets defined above, it is obvious that G represents P. Now we assume that the independent transition and causal faithfulness hold.</p>
<p>Assume that G contains a lateral edge like v 1 → v 2 for example.According to independent transition, we have
(v 1 Pr v 2 |u). Because v 1 ∈ P A(v 2 ), we have (v 1 ̸ G v 2 |u).
This violates the assumption of causal faithfulness, as we have
(v 1 Pr v 2 |u) / ⇒ (v 1 G v 2 |u)
. Therefore, we prove that no lateral edge among v exists in G.</p>
<p>Because there is no lateral edge among v exists in G, u −i blocks all paths form u i to v j unless u i ∈ P A(v j ).Therefore, it is easy to prove that
(u i ̸ G v j |u −i ) ⇔ u i ∈ P A(v j )
Combining Theorem 1 and Definition 3, we have that
(u i ̸ G v j |u −i ) ⇔ (u i ̸ Pr v j |u −i )
Therefore, the parents of v j are uniquely identified by the rule:
(u i ̸ Pr v j |u −i ) ⇔ u i ∈ P A(v j )
Finally, let us consider another probability function Pr * that satisfies Eq. 17, and assume G is the DAG that Pr * is compatible with and faithful to.For every v j ∈ v it follows that Pr(v j |u) = Pr * (v j |u) = P (v j |u) If u i ∈ P A(v j ) and u i / ∈ P A * (v j ), using the above rule we have (u i ̸ Pr v j |u −i ) and (u i Pr * v j |u −i ).In other words, we have
Pr(v j |u) ≠ Pr(v j |u −i ) Pr * (v j |u) = Pr * (v j |u −i ) This leads to that Pr(v j |u −i ) ≠ Pr * (v j |u −i )
We can also write that
Pr(v j |u −i ) = ∫ ui Pr(v j |u)Pr(u i |u −i ) = ∫ ui Pr * (v j |u)Pr(u i |u −i ) = ∫ ui Pr * (v j |u −i )Pr(u i |u −i ) =Pr * (v j |u −i ) ∫ ui Pr(u i |u −i ) =Pr * (v j |u −i ).
From the above equations, we obtain the paradox that
Pr(v j |u −i ) = Pr * (v j |u −i ).
Using reduction to absurdity, we obtain that u i ∈ P A(v j ) implies u i ∈ P A * (v j ).Similarly, we can prove the opposite direction of this implication.As a result, we have
u i ∈ P A(v j ) ⇔ u i ∈ P A * (v j ),
which shows that P A(v j ) = P A * (v j ).</p>
<p>Proof ends.</p>
<p>B Explanation through Causal Chains</p>
<p>In our paper, we only present the visualization of causal chains.Although this visualization offers a certain extent of interpretability, Madumal et al [2020b] have proposed techniques to better use causal chains to generate high-quality explanations.In this section, we introduce how our causal chains adapt to their techniques.</p>
<p>Consider an H-step trajectory (δ t , δ t+1 , ⋯, δ t+H−1 ), where δ t+k = (s t+k , a t+k , s t+k+1 , o t+k , r t+k ).In this trajectory, we use the bold capital letter C to denote the sub-set of variables in the causal chain.In Madumal's work, an explanation is derived from an "explanan", which contains information about how action leads to rewards.Definition 4 (Explanan).A H-step explanan for an action a t under a factual trajectory (δ t , ⋯, δ t+H−1 ) is a tuple ⟨x r , x h , x i ⟩, where 1. x r contains the reward variables in the causal chain.2. x h = s t ∩ C is the heading variables (the state variables at the beginning) in the causal chain.3. x i ⊂ C ∖ (x h , x i ) contains some intermediate variables between x h and x r .An explanation for "why the agent took action a t at s t " is generated by filling the values in the explanan into a naturallanguage template.If x i contains all intermediate variables, the explanan is called a complete explanan.However, it may contain too much information and difficult to be understood.Therefore, Madumal et al suggest using the minimally complete explanan (MCE), where x i contains only the parents of x r .</p>
<p>Taking the causal chain in the paper's Figure 5 for example, we present the explanations respectively drawn from the complete explanan and the minimally complete explanan below.</p>
<p>Example 1 (Complete Explanation for Build-Marine).The agent build supplies depots because this action causes the following changes:</p>
<ol>
<li>Instantly, the number of supply depots increases from 7 to 11, and money decreases from 505 to 330; 2. After 2 steps, the number of barracks increases from 1 to 3, and the amount of money increases from 330 to 465. 3.After 3 steps, the number of barracks increases from 3 to 6, and the amount of money decreases from 330 to 430. 4.After 4 steps, the number of marines increases from 0 to 6.</li>
</ol>
<p>Which lead to a reward of 6 due to new marines after 4 steps.</p>
<p>Example 2 (Minimally Complete Explanation for Build-Marine).The agent builds supplies depots because this action would eventually cause the number of marines to increase from 0 to 6 after 4 steps, which leads to a reward of 6 due to new marines.</p>
<p>In addition, by comparing two MCEs, we can construct contrastive explanations that answer why that agent did not take another action.Therefore, Madumal et al define the minimally complete contrastive explanation, which contains the difference between the factual MCE and the counterfactual MCE.</p>
<p>Definition 5 (Minimally Complete Contrastive Explanation).Let (δ t , ⋯, δ t+H−1 ) denote the factual trajectory (the trajectory that actually happens) and ( δt , ⋯, δt+H−1 ) denote the counterfactual trajectory, which is produced by replacing a t with another action ãt and using the world model and policy to simulate the following H steps. Assume that x = ⟨x r , x h , x i ⟩ is the MCE for a t under the factual trajectory and that y = ⟨y r , y h , y i ⟩ is the MCE for ãt under the counterfactural trajectory.A minimally complete contrastive explanation (MCCE) is then given by a tuple ⟨x dif f , y dif f , x r ⟩, where 1. x dif f is the subset of variables in x that 1) is not included in y, or 2) owns a value different from that in y.</p>
<ol>
<li>y dif f is the subset of variables in y that 1) is not included in x, or 2) owns a value different from that in x.</li>
</ol>
<p>3.</p>
<p>x r contains the reward variables in the factual causal chain.</p>
<p>C The Trade-off between interpretability and accuracy</p>
<p>Theorem 3. Assume v j is an endogenous variable of the SCM of the Factorized MDP.Let P A 1 (v j ) and P A 2 (v j ) respectively denote its parent sets discovered using the threshold η 1 and η 2 .Assume P A * (v j ) is the ground-truth parent set of v j .If η 1 ≤ η 2 , with the well trained structural equation
f j we have E u [D KL (f j (P A * (v j ))||f j (P A 1 (v j )))] ≥ E u <a href="18">D KL (f j (P A * (v j ))||f j (P A 2 (v j )))</a>
Proof.Obviously, we have P A 1 (v j ) ⊆ P A 2 (v j ).For simplicity, we define
a ∶= P A 1 (v j ) ∩ P A * (v j ), b ∶= (P A 2 (v j ) ∩ P A * (v j )) ∖ P A 1 (v j ), c ∶= P A * (v j ) ∖ P A 2 (v j ), d ∶= P A 1 (v j ) ∖ a, e ∶= P A 2 (v j ) ∖ (a, b, d),
where a, b, c, d, e are non-overlapping subsets of u.More specifically, a, b, c are the true parents of v j , whereas d, e are false parents of v j .Then we have
P A * (v j ) = (a, b, c), P A 1 (v j ) = (a, d), P A 2 (v j ) = (a, b, d, e),
We use ∼ to denote that a probability conforms to a given distribution.Noted that variables in d are not true parents of v j , with well trained f j , the posterior distribution of v j is given by
Pr(v j |a) = Pr(v j |a, d) ∼ f j (a, d) = f j (P A 1 (v j ))(19)
Therefore, we have
E u [D KL (f j (P A * (v j ))||f j (P A 1 (v j )))] =E u [D KL (f j (a, b, c)||f j (a, d))] = ∫ u Pr(u)du ∫ vj Pr(v j |a, b, c) log Pr(v j |a, b, c) Pr(v j |a, d) dv j = ∫ a,b,c Pr(a, b, c)d(a, b, c) ∫ vj Pr(v j |a, b, c) log Pr(v j |a, b, c) Pr(v j |a) dv j = ∫ a,b,c,vj Pr(a, b, c, v j ) log Pr(v j |a, b, c) Pr(v j |a) d(a, b, c, v j ) (20) Similarly, we have E u [D KL (f j (P A * (v j ))||f j (P A 2 (v j )))] =E u [D KL (f jE u [D KL (f j (P A * (v j ))||f j (P A 1 (v j )))] − E u [D KL (f j (P A * (v j ))||f j (P A 2 (v j )))] = ∫ a,b,c,vj Pr(a, b, c, v j ) log Pr(v j |a, b) Pr(v j |a) d(a, b, c, v j ) = ∫ a,b,vj ∫ c (Pr(a, b, c, v j )dc) log Pr(v j |a, b) Pr(v j |a) d(a, b, v j ) = ∫ a,b,vj Pr(a, b, v j ) log Pr(v j |a, b) Pr(v j |a) d(a, b, v j ) = ∫ a,b,vj Pr(a, b)Pr(v j |a, b) log Pr(v j |a, b) Pr(v j |a) d(a, b, v j ) = ∫ a,b Pr(a, b)D KL (f j (a, b)||f j (a))) d(a, b) = E a,b [D KL (f j (a, b)||f j (a)))] ≥ 0(22)
D The Algorithm for Model-Based RL Fit the structural equations by maximizing (12) 6:</p>
<p>for learning round j = 1, ..., n round do 7:</p>
<p>Generate simulated data by performing k-step model rollout using actor π 8:</p>
<p>Update the policy π using PPO algorithm 9:</p>
<p>end for 10: end for E Environments</p>
<p>E.1 Factorization of Public Environments</p>
<p>This section describes how environments are considered Factorized MDPs in our implementation.</p>
<p>Cartpole The state is factorized into 4 variables (x, ẋ, θ, θ) indicating the 1) position of the cart, 2) velocity of the cart, 3) angle of the pole, and 4) angle velocity of the pole.The action only contains one discrete variable indicating the direction (left or right) to push the cart.The goal of the agent is to keep the pole upright as long as possible.Therefore, the agent is rewarded by 1 for each step as long as the state satisfies −2.4 ≤ x ≤ 2.4 and −12 ○ ≤ θ ≤ 12 ○ .The episode terminates if this condition does not hold.In addition, no outcome variable is included in this environment.</p>
<p>Lunarlander-Discrete The state is factorized into 7 variables (x, y, ẋ, ẏ, θ, θ, legs) indicating the (1) horizontal position, (2) vertical position, (3) horizontal velocity, (4) vertical velocity, (5) angle, (6) angle velocity, and (7) whether the two legs are in contact with the ground.The action contains one discrete variable indicating the engine (none, main, left, or right) to be actuated.The environment contains 3 outcome variables, including (1) the fuel cost due to firing the engine, (2) whether the lander crashed, and (3) whether the rocket is resting.The agent is rewarded (or penalized) from multiple sources, including (1) shortening the distance to the destination, (2) reducing the velocity, (3) reducing the angle, (4) increasing the number of landed legs, (5) being resting, and (6) crashing.</p>
<p>Lunarlander-Continuous replaces the action space of Lunarlander-Discrete with a continuous action space.The new action space includes 2 continuous variables ranged in (−1, 1), respectively controlling the throttles of the main and the lateral engines.</p>
<p>Build Marine The state includes 6 variables denoted as (n wk , n mr , n br , n dp , money, time), namely the (1) number of workers, (2) number of marines, (3) number of barracks, (4) number of supply depots, (5) amount of money, and (6) game time.The action includes only one discrete variable indicating which unit (workers, marines, barracks, supply depots, or none) to be built.The player is rewarded with 1 for every newly produced marine.The problem is challenging since marines can only be built from barracks, and barracks can only be built provided there exists at least one supply depot.In addition, the number of workers and marines is limited by the number of supplies provided by supply depots.</p>
<p>E.2 The Environment for Measuring the Accuracy of Recovering Action Influence</p>
<p>To measure the accuracy of recovering the causal dependencies of the AIM, we design an additional environment with a known ground-truth AIM.The environment contains one action variable a and 5 state variables: x 1 , x 2 , x 3 , x 4 , and τ .The action variable a is chosen from 4 options {0, 1, 2, 3}.</p>
<p>The dynamics of these state variables are given by
x ′ 1 = x 1 + N (1, 1)(23)
x ′ 2 = {</p>
<p>x 1 , a = 0 x 2 , otherwise + N (0, 1) (24)
x ′ 3 = x 3 + ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩
x 1 , a = 0 x 2 , a = 1 5, a = 2 10, a = 3
+ N (0, 1)(25)
x ′ 4 = 0.1x 3 + 0.9x 4 + N (0, 0.5) (26)
τ ′ = τ + ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ 10, a = 0 20, a = 1 5, a = 2 5, a = 3(27)
The ground-truth causality of the AIM can be easily derived from the above dynamics.Aiming to confuse neural networks, these dynamics create copious spurious correlations in the data.For example, x 1 and τ present a strong positive correlation, whereas neither of them is the other's causation.</p>
<p>F Importance of Causality: an Example</p>
<p>To show how causal discovery facilitates explanation generation, we investigated the causal chains produced without a discovered causal graph.Figure 8 presents such a causal chain for Build-Marine.It is produced by our model using a full graph, where unreasonable connections are highlighted in bold and red.We found the model attributes a large influence weight (≈ 0.78) to time as the salient parent of n depot under the action of building supply depots.This leads to the incorrect understanding that "if we build supply depots, the number of supply depots turns to 4 because the game time is 140".In fact, the variable time only influences time ′ in the discovered causal graph presented in Figure 3(b), meaning the number of supply depots is not caused by the game time at all.The model is misled by the fact that the agent always builds more supply depots as time goes on, which makes the two variables (time and n ′ depot ) highly related.Similarly, the model wrongly takes time as the salient parent of money under the actions of building barracks since it observes that money usually accrues with time.Therefore, attention alone is insufficient to obtain reasonable causal chains for explanations, as it can be easily misled by spurious correlations (which are rife in the non-i.i.d.data collected in RL).Fortunately, these errors in causal chains are greatly reduced when combining attention with a causal graph, which precludes most spurious correlations and leads to correct causal chains.</p>
<p>G Hyper-parameters of Model-based RL</p>
<p>The main hyper-parameters used in the mentioned environments for model-based RL are presented in Table 2.</p>
<p>H Computational Complexity</p>
<p>Figure 1 :
1
Figure 1: The illustration of causal models in the Vacuum world.(a) illustrates the Vacuum world, where position = 1, clean1 = T rue, and clean2 = F alse.(b) and (c) respectively illustrate the causal graphs of the SCM and the AIM of the Vacuum world.</p>
<p>Figure 2 :
2
Figure 2: The illustration of the proposed framework.(a) shows an example of the causal graph identified by causal discovery.(b) illustrates the structure of the proposed model.(c) shows the inference network that approximates the structural equation of s ′ 3 .(d) illustrates the causal chain analysis, where the causal chain is highlighted in bold and green.</p>
<p>Figure 4 :Figure 5 :
45
Figure 4: An example of a 4-step causal chain on Lunarlander-Continuous</p>
<p>Figure 6 :
6
Figure 6: The training curves.Our explainable model (red) is compared to non-explainable dense models (green and blue) to show the performance cost of using a sparse causal graph.The grey curves show the performance without models.</p>
<p>1.</p>
<p>There exists a forward chain a → b → c, a backward chain a ← b ← c, or a fork a ← b → c in p such that b ∈ Z. 2. For every collision structure a → b ← c in p, Z does not contain b or any descendant of b.Theorem 1 (d-separation Criterion).Assume G is a DAG of a set of variables.Assume x, y, and z are disjoint subsets of variables.The following propositions hold: 1. (Global Markov Property [Peters et al., 2017]) Assuming Pr is a probability function such that Pr and G are compatible, then</p>
<p>Figure 7 :
7
Figure 7: two examples of spurious edges.The true causal relations are shown in solid arrows, where the backdoor paths mentioned are highlighted in red.The spurious edges are shown in red dashed lines.</p>
<p>(a, b, c)||f j (a, b, d, e))] = ∫ a,b,c,vj Pr(a, b, c, v j ) log Pr(v j |a, b, c) Pr(v j |a, b) d(a, b, c, v j )</p>
<p>Figure 8 :
8
Figure 8: An example of the causal chain produced by a full causal graph.We highlight the "problematic" edges in bold and red.</p>
<p>Let n = max(n s + n a , n s + n o ) roughly denote the number of variables of the environment.Let N denote the total number of transition samples.Let b denote the batch size.Model The parameter scale of our model is O(n).The time complexity of one forward pass is O(n 2 b).Causal discovery The time complexity of testing each edge through FCIT is O(nN log N ) and the overall time complexity of causal discovery is O(n 3 N log N ).The space complexity for causal discovery is O(nN ) if the tests are sequentially performed.Causal chains The time complexity and space complexity of generating an H-step causal chain are both O(n 2 H).In our experiments, the generation of each causal chain completes almost instantly.Parameter Cartpole LunarLander LunarLander-Continuous Build-Marine total epochs (n epoch ) main parameters used in model-based RL.The form "a → b" denotes the parameter gradually changes from a to b during the training process.</p>
<p>To evaluate the performance of our model in MBRL, we perform experiments in two extra environments: Cartpole and Lunarlander-Discrete.The Build-Marine environ-
next stateoutcomestateaction(a) Lunarlander-Continuousnext statebuild unitsstateaction(b) Build-MarineFigure 3: The discovered causal graphs of two environments.
[Brockman et al., 2016]e of the StartCraftII mini-games in SC2LE[Samvelyan et al., 2019]; the Cartpole and Lunarlander environments are classic control problems provided by OpenAI Gym[Brockman et al., 2016].Our source code is available at https://github.com/EaseOnway/Explainable-Causal-Reinforcement-Learning.</p>
<p>Amy Zhang, Rowan McAllister,  Roberto Calandra, Yarin Gal, andSergey Levine.Learning Invariant Representations for Reinforcement Learning without Reconstruction, April 2021.arXiv:2006.10742[cs, stat].</p>
<p>Model-based RL using causal world models 1: Initialize environment models and the policy π(a|s) 2: for training epoch i = 1, ..., n epoch do
See Algorithm 1.Algorithm 1 3: Interact with the environment using π; Add transitionsinto buffer D4:(for every n graph epochs) Update the causal graphthrough causal discovery5:
AcknowledgmentsThis work was supported in part by National Key R&amp;D Program of China (No.2022ZD0116405) and in part by the National Nature Science Foundation of China under Grant (62073324).
HIGH-LIGHTS: Summarizing Agent Behavior to People. Amir Amir, Dan Amir, Ofra Amir, Proceedings of the 17th International Conference on Autonomous Agents and Multiagent Systems. the 17th International Conference on Autonomous Agents and Multiagent SystemsStockholm, Sweden2018. 2018</p>
<p>Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning. Atrey, arXiv:1912.057432020. February 2020</p>
<p>On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation. Bach, PLOS ONE. 107e01301402015. July 2015</p>
<p>. Brockman, arXiv:1606.01540OpenAI Gym. 2016. June 2016</p>
<p>Fast Conditional Independence Test for Vector Variables with Large Sample Sizes. Chalupka, arXiv:1804.027472018. April 2018cs, stat</p>
<p>Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models. Chua, arXiv:1805.121142018. November 2018cs, stat</p>
<p>The rational use of causal inference to guide reinforcement learning strengthens with age. npj Science of Learning. Cohen, 2020. December 2020516</p>
<p>Distilling Deep Reinforcement Learning Policies in Soft Decision Trees. Coppens, Proceedings of the IJCAI 2019 Workshop on Explainable Artificial Intelligence. the IJCAI 2019 Workshop on Explainable Artificial Intelligence2019. 2019</p>
<p>Discovering and Removing Exogenous State Variables and Rewards for Reinforcement Learning. Dietterich, arXiv:1806.015842018. June 2018cs, stat</p>
<p>Generalizing Goal-Conditioned Reinforcement Learning with Variational Causal Reasoning. Ding, arXiv:2207.090812022. July 2022cs, stat</p>
<p>Autonomous Self-Explanation of Behavior for Interactive Reinforcement Learning Agents. Fukuchi, arXiv:1906.08253Proceedings of the 5th International Conference on Human Agent Interaction. the 5th International Conference on Human Agent InteractionBielefeld GermanyACM2017. October 2017. 2019. 2019. November 20214When to Trust Your Model: Model-Based Policy Optimization. cs, stat</p>
<p>Explainable Reinforcement Learning via Reward Decomposition. Kim ; Ho-Taek Joo, Kyung-Joong Joo, ; Kim, Juozapaitis, arXiv:1903.00374Visualization of Deep Reinforcement Learning using Grad-CAM: How AI Plays Atari Games? In 2019 IEEE Conference on Games (CoG). Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Afroz Mohiuddin, Ryan Sepassi, George Tucker, Henryk Michalewski, 2019. 2019. 2019. 2019. 2020. February 2020Model-Based Reinforcement Learning for Atari. cs, stat</p>
<p>Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning. Ke, arXiv:2107.008482021. July 2021cs, stat</p>
<p>Learning Finite State Representations of Recurrent Policy Networks. Koul, arXiv:1811.125302018. November 2018cs, stat</p>
<p>Explainable Artificial Intelligence: Concepts, Applications, Research Challenges and Visions. Longo, Machine Learning and Knowledge Extraction. 202012279</p>
<p>Deconfounding Reinforcement Learning in Observational Settings. Lu, arXiv:1812.10576Lecture Notes in Computer Science. 2020. 2018. December 2018Springer International PublishingSeries Title. cs, stat</p>
<p>Madumal, arXiv:2001.10284Distal Explanations for Model-free Explainable Reinforcement Learning, September 2020. 2020a</p>
<p>Explainable Reinforcement Learning through a Causal Lens. Madumal, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2020b. April 202034</p>
<p>Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. Nagabandi, 2018 IEEE International Conference on Robotics and Automation (ICRA). 2018. 2018</p>
<p>Causal Induction from Visual Observations for Goal Directed Tasks. Nair, arXiv:1910.017512019. October 2019cs, stat</p>
<p>Free-Lunch Saliency via Attention in Atari Agents. Nikulin, 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW). Seoul, Korea (South; Chichester; West SussexWiley2019. 2016. 2016Causal inference in statistics: a primer</p>
<p>Elements of causal inference: foundations and learning algorithms. Adaptive computation and machine learning series. ; Pearl, Peters, 2000. 2000. 2017. 2017The MIT PressCambridge, U.K.; New York; Cambridge, MassachuesttsCausality: models, reasoning, and inference</p>
<p>Explainable Reinforcement Learning: A Survey. Veith Puiutta, arXiv:2005.062472020. May 2020Erika Puiutta and Eric MSP Veith. cs, stat</p>
<p>Decoupling feature extraction from policy learning: assessing benefits of state representation learning in goal based robotics. Raffin , arXiv:1901.086512019. June 2019cs, stat</p>
<p>Artificial intelligence: a modern approach. Prentice Hall series in artificial intelligence. Russell , 2010. 2010TabishUpper Saddle River3rd ed edition. Samvelyan et al., 2019] Mikayel Samvelyan</p>
<p>. Christian Rashid, Gregory Schroeder De Witt, Nantas Farquhar, Nardelli, G J Tim, Chia-Man Rudner, Hung, H S Philip, Jakob Torr, Shimon Foerster, Whiteson, arXiv:1902.04043The StarCraft Multi-Agent Challenge. December 2019cs, stat</p>
<p>Proximal Policy Optimization Algorithms. Schulman, arXiv:1707.063472017. August 2017</p>
<p>Causal Influence Detection for Improving Efficiency in Reinforcement Learning. Seitzer, arXiv:2106.034432021. December 2021</p>
<p>Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization. Selvaraju, arXiv:1610.02391International Journal of Computer Vision. 12822020. February 2020</p>
<p>Self-Supervised Discovering of Interpretable Features for Reinforcement Learning. Shi, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2021. 2021</p>
<p>Causal Models: How People Think about the World and Its Alternatives. Steven Sloman, ; Sloman, Spirtes, 2005. 08 2005. 2001. 2001Peter Spirtes, Clark Glymour, and Richard Scheines. Causation, Prediction, and Search. The MIT Press</p>
<p>Provably Efficient Causal Reinforcement Learning with Confounded Observational Data. Sergei Volodin, Causeoccam, Wang, arXiv:2104.02297Learning Interpretable Abstract Representations in Reinforcement Learning Environments via Model Sparsity. Xiaoqian Wang, and David I. Inouye. Shapley Explanation Networks2021. 2021a. 2021. 2021b. April 202134École Polytechnique Fédérale de LausannePhD thesisAdvances in Neural Information Processing Systems</p>
<p>Causal Dynamics Learning for Task-Independent State Abstraction. Wang , arXiv:2206.134522022. June 2022</p>
<p>Training a Resilient Q-Network against Observational Interference. Yang , arXiv:2102.096772022. January 2022cs, eess] version: 3</p>
<p>What Did You Think Would Happen? Explaining Agent Behaviour through Intended Outcomes. Yau, Proceedings of the 34th Conference on Neural Information Processing Systems. the 34th Conference on Neural Information Processing SystemsVancouver, Canada2020. 2020</p>
<p>Invariant Causal Prediction for Block MDPs. Zhang, Proceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine LearningPMLR2020. November 2020</p>            </div>
        </div>

    </div>
</body>
</html>
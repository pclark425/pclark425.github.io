<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4951 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4951</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4951</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-dedfe929d182cc3537a9ed765d589b4735ce062a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/dedfe929d182cc3537a9ed765d589b4735ce062a" target="_blank">On the Planning Abilities of Large Language Models - A Critical Investigation</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> It is demonstrated that LLM-generated plans can improve the search process for underlying sound planners and additionally show that external verifiers can help provide feedback on the generated plans and back-prompt the LLM for better plan generation.</p>
                <p><strong>Paper Abstract:</strong> Intrigued by the claims of emergent reasoning capabilities in LLMs trained on general web corpora, in this paper, we set out to investigate their planning capabilities. We aim to evaluate (1) the effectiveness of LLMs in generating plans autonomously in commonsense planning tasks and (2) the potential of LLMs as a source of heuristic guidance for other agents (AI planners) in their planning tasks. We conduct a systematic study by generating a suite of instances on domains similar to the ones employed in the International Planning Competition and evaluate LLMs in two distinct modes: autonomous and heuristic. Our findings reveal that LLMs' ability to generate executable plans autonomously is rather limited, with the best model (GPT-4) having an average success rate of ~12% across the domains. However, the results in the heuristic mode show more promise. In the heuristic mode, we demonstrate that LLM-generated plans can improve the search process for underlying sound planners and additionally show that external verifiers can help provide feedback on the generated plans and back-prompt the LLM for better plan generation.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4951.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4951.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero/One-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot and One-shot Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard prompting methods where the LLM is given either (a) only the domain+problem (zero-shot) or (b) domain+problem plus a single example plan (one-shot). Used as baselines for plan generation in PDDL and natural-language formats.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5, Instruct-GPT3.5, Instruct-GPT3, GPT-3, BLOOM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Family of transformer-based large language models (OpenAI GPT family—GPT-4 and GPT-3.5 variants were primary; GPT-3 fine-tuned experiments and older davinci models also evaluated; BLOOM tested as open-source). Experiments used deterministic decoding (temperature=0).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Zero-shot / One-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Single-prompt elicitation: zero-shot provides only domain+problem; one-shot includes a single example (a single reasoning instance). No sampling/ensembling; deterministic decoding so the model produces one reasoning trace per instance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Classical planning tasks (Blocksworld, Logistics, Mystery Blocksworld)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Goal-directed deterministic planning instances in PDDL-like domains: Blocksworld (3–5 blocks), Logistics (transport packages via trucks/airplanes), and obfuscated 'Mystery' Blocksworld variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Blocksworld (natural language, GPT-4): One-shot 206/600 (34.3%), Zero-shot 210/600 (34.6%); Blocksworld (PDDL, GPT-4): One-shot 75/600 (12.5%), Zero-shot 106/600 (17.6%); Logistics (natural language, GPT-4): One-shot 28/200 (14%), Zero-shot 15/200 (7.5%); Mystery BW degraded to near 0% when names obfuscated.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Chain-of-Thought (GPT-4 on Blocksworld nat-lang): 214/600 (35.6%) — marginal improvement over one/zero-shot; Fine-tuned GPT-3 on Blocksworld: 122/600 (20%) — modest improvement over base GPT-3 but below GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Single-prompt (similar) methods produce low autonomous planning performance overall (GPT-4 best ≈34% on natural-language Blocksworld but much lower on PDDL and logistics). One-shot examples give small improvements in some configurations; deterministic single-trace prompting is brittle to obfuscation of names, implying reliance on pattern matching.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>In PDDL-format prompts GPT-4 performed worse than in natural language; zero-shot occasionally matched or exceeded one-shot (e.g., Blocksworld nat‑lang one-shot 206 vs zero-shot 210). The obfuscated 'Mystery' domains show that apparently similar prompting depends heavily on surface names and thus fails when pattern cues are removed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Planning Abilities of Large Language Models - A Critical Investigation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4951.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4951.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (state-tracking)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (state-tracking) Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that requests the model to produce intermediate stepwise state annotations and justifications (a state-tracking chain of thought) along with the plan.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (primary), other GPT variants tested</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 (transformer LLM) used with prompts that include a worked state-tracking example and ask the model to produce analogous stepwise reasoning and state updates.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought (state-tracking)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Provides a single, explicit internal reasoning trace: each action annotated with the prior state, justification for applicability, and resulting state. This elicits a single detailed chain rather than multiple diverse traces.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Blocksworld (and other planning domains)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same classical planning instances (Blocksworld etc.) but prompts include an annotated example showing stepwise state updates and action applicability reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Blocksworld (natural language, GPT-4, COT): 214/600 (35.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>One-shot natural-language GPT-4: 206/600 (34.3%); Zero-shot: 210/600 (34.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>COT/state-tracking produced only marginal improvements (≈1–3 percentage points) over one-shot/zero-shot natural-language prompts for GPT-4; authors conclude chain-of-thought had little effect on materially improving autonomous plan correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>COT did not rescue failures in obfuscated (Mystery) domains and gave limited benefit relative to simpler one-shot prompts; overall gains were small and not transformative.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Planning Abilities of Large Language Models - A Critical Investigation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4951.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4951.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Verifier-assisted backprompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Verifier-assisted Repeated Backprompting with VAL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative feedback protocol where an external sound verifier (VAL) checks an LLM-generated plan, returns precise fault diagnostics (unsatisfied preconditions / delete interactions), and that feedback is used to re-prompt the LLM repeatedly up to a threshold to produce improved plans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 prompted iteratively with explicit verifier feedback; experiments limited to previously failed one-shot natural-language instances, threshold of up to 15 backprompting rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Verifier-assisted iterative backprompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Creates diversity over multiple reasoning attempts by changing the prompt with verifier-supplied error diagnoses each round; the LLM produces new candidate plans conditioned on progressively specific failure information (not a single static chain).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Blocksworld, Logistics, Mystery Blocksworld (subset of failed instances)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>50 failed instances per domain (from one-shot nat-lang configuration) were re-attempted with verifier feedback to see if LLM improves via iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On the tested failed-instance subsets: Blocksworld recovery success 82% (of the 50 failed instances), Logistics recovery success 70%; average successful backprompt rounds: Blocksworld 3.68, Logistics 3.31.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Autonomous (single-shot) GPT-4 success rates on those domains were much lower (Blocksworld overall ≈34% on nat-lang; many of these specific instances were previously failed).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Verifier-assisted iterative prompting substantially improves plan correctness over autonomous single-shot generation: most previously failed instances for common-sense domains were corrected within a few verifier-guided re-prompts. This demonstrates that external, sound feedback plus iterative prompting enables LLMs to reach correct plans in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Performance on Mystery Blocksworld remained poor despite backprompting, indicating that verifier-guided iteration helps only when the LLM can leverage commonsense cue patterns; in domains that obscure those cues, backprompting is less effective.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Planning Abilities of Large Language Models - A Critical Investigation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4951.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4951.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-Modulo (LLM+LPG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-guided Heuristic Seeding of an External Planner (LLM-Modulo with LPG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Treat LLM-generated plan as a seed (heuristic) for a sound local-search planner (LPG) which repairs the seed to a valid plan; used to measure whether LLM outputs can speed search even if not fully correct.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, Instruct-GPT3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM produces a candidate plan (one-shot nat-lang); LPG (local-search planner) takes that plan as initial seed and performs iterative flaw-repair; baselines are empty-seed and random-seed.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Heuristic seed + local search (LLM-Modulo)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Combines a single LLM candidate (one static seed) with a separate combinatorial planner (LPG) that performs search/repair; diversity arises from planner's search not from multiple LLM traces.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Blocksworld, Logistics, Mystery Blocksworld</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same classical planning instances; measure LPG's average search steps to a valid plan when seeded with (a) empty plan, (b) random plan, (c) LLM plan.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Blocksworld (GPT-4 seed): avg search steps — Empty seed 15.8, Random 20.07, LLM seed 8.9; Logistics (GPT-4 seed): Empty 77.5, Random 144.39, LLM seed 51.3; Mystery BW (GPT-4 seed): small/no improvement (LLM seed 16.09 vs Empty 15.8 and Random 20.45). LLM seed plans retained roughly 50% of final plan elements (avg Levenshtein edit distance ≈ half the length).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Baselines: Empty seed and Random seed performance listed above; LLM+LPG shows substantial search-step reduction in Blocksworld and Logistics but not in Mystery Blocksworld.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Even incorrect LLM plans can substantially reduce search effort for sound planners in commonsense domains (LLM seeds led to significantly fewer LPG search steps than empty or random seeds). However, when domain surface cues are obfuscated (Mystery BW), LLM seeds lose utility.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>In Mystery Blocksworld (obfuscated names) LLM seeds provided little or no advantage and sometimes performed worse than empty seeds, indicating reliance on surface/common-sense cues for generating useful heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Planning Abilities of Large Language Models - A Critical Investigation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4951.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4951.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fine-tuned GPT-3 on Blocksworld</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuning GPT-3 on Blocksworld Instances</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised fine-tuning experiment: GPT-3 (davinci) fine-tuned on 1,000 Blocksworld instances (initial state, goal, plan) separate from the 600-instance test set to see if task-specific fine-tuning improves planning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (davinci) fine-tuned</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Davinci/GPT-3 model fine-tuned using OpenAI default hyperparameters on a domain-specific dataset of Blocksworld examples (80/20 train/validation split).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Supervised fine-tuning (task-specific learning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Model is trained to map (initial state + goal) → plan using standard supervised fine-tuning, producing a single deterministic output for each test prompt (no ensemble/diversity).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Blocksworld (test set of 600 instances)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same Blocksworld planning instances used in the main evaluation; fine-tuned model tested zero/few-shot on held-out instances.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Fine-tuned GPT-3 solved 122/600 instances (~20%) on the test set.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Non-fine-tuned GPT-3 and Instruct variants performed worse (e.g., GPT-3 base was ≈1% in natural-language one-shot). GPT-4 substantially outperformed fine-tuned GPT-3 (GPT-4 ≈34% on nat-lang Blocksworld).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Task-specific fine-tuning improved vanilla GPT-3 performance (to ~20%) but did not reach GPT-4 performance and overall improvement was limited; suggests fine-tuning helps but does not solve combinatorial planning deficits.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Fine-tuning did not close the gap to GPT-4 and remained far below human baseline (~78% valid plans in human study). Fine-tuning still yielded brittle behavior with respect to domain obfuscation (generalization limited).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Planning Abilities of Large Language Models - A Critical Investigation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4951.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4951.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-critiquing / self-verification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM Self-critiquing / Self-verification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The idea of using the LLM itself as a verifier or critic of its own generated plans (self-critique) rather than using an external sound verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can large language models really improve by self-critiquing their own plans?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-family (general)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>General observation about LLMs' ability to verify their own outputs; referenced external analyses and the authors' related work.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-critiquing / self-verification</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Using the same LLM (without an external verifier) to evaluate and correct its outputs (e.g., ask model to check plan correctness and revise), which is essentially a repeated single-model prompting loop without a sound symbolic verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Plan verification and iterative improvement (discussion, no primary experiments in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assessing whether LLMs can reliably detect errors in their own plans and improve them without external sound verification.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>No numeric performance reported here; authors cite their own related work and state that self-verification/self-critiquing is of questionable utility because LLMs are no better at verification than generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>External verifier (VAL) + backprompting (described in paper) yielded strong improvements (e.g., 82% Blocksworld recovery) whereas self-critiquing has been found to be unreliable per cited analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Authors explicitly caution against relying on self-verification: mechanical external verifiers provide precise diagnostics and substantially better grounding for iterative correction; using the LLM to both generate and verify plans is unlikely to be reliably effective.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Referenced analyses show iterative self-critiquing often fails because the model repeats its mistakes or cannot reliably identify faults; the paper uses this to motivate use of a sound external verifier (VAL) instead.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Planning Abilities of Large Language Models - A Critical Investigation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>VAL: Automatic plan validation, continuous effects and mixed initiative planning using PDDL <em>(Rating: 2)</em></li>
                <li>Lpg: A planner based on local search for planning graphs with action costs <em>(Rating: 2)</em></li>
                <li>Can large language models really improve by self-critiquing their own plans? <em>(Rating: 2)</em></li>
                <li>Do as i can, not as i say: Grounding language in robotic affordances <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4951",
    "paper_id": "paper-dedfe929d182cc3537a9ed765d589b4735ce062a",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "Zero/One-shot prompting",
            "name_full": "Zero-shot and One-shot Prompting",
            "brief_description": "Standard prompting methods where the LLM is given either (a) only the domain+problem (zero-shot) or (b) domain+problem plus a single example plan (one-shot). Used as baselines for plan generation in PDDL and natural-language formats.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-3.5, Instruct-GPT3.5, Instruct-GPT3, GPT-3, BLOOM",
            "model_description": "Family of transformer-based large language models (OpenAI GPT family—GPT-4 and GPT-3.5 variants were primary; GPT-3 fine-tuned experiments and older davinci models also evaluated; BLOOM tested as open-source). Experiments used deterministic decoding (temperature=0).",
            "reasoning_method_name": "Zero-shot / One-shot prompting",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Single-prompt elicitation: zero-shot provides only domain+problem; one-shot includes a single example (a single reasoning instance). No sampling/ensembling; deterministic decoding so the model produces one reasoning trace per instance.",
            "task_name": "Classical planning tasks (Blocksworld, Logistics, Mystery Blocksworld)",
            "task_description": "Goal-directed deterministic planning instances in PDDL-like domains: Blocksworld (3–5 blocks), Logistics (transport packages via trucks/airplanes), and obfuscated 'Mystery' Blocksworld variants.",
            "performance": "Blocksworld (natural language, GPT-4): One-shot 206/600 (34.3%), Zero-shot 210/600 (34.6%); Blocksworld (PDDL, GPT-4): One-shot 75/600 (12.5%), Zero-shot 106/600 (17.6%); Logistics (natural language, GPT-4): One-shot 28/200 (14%), Zero-shot 15/200 (7.5%); Mystery BW degraded to near 0% when names obfuscated.",
            "comparison_with_other_method": true,
            "performance_other_method": "Chain-of-Thought (GPT-4 on Blocksworld nat-lang): 214/600 (35.6%) — marginal improvement over one/zero-shot; Fine-tuned GPT-3 on Blocksworld: 122/600 (20%) — modest improvement over base GPT-3 but below GPT-4.",
            "key_findings": "Single-prompt (similar) methods produce low autonomous planning performance overall (GPT-4 best ≈34% on natural-language Blocksworld but much lower on PDDL and logistics). One-shot examples give small improvements in some configurations; deterministic single-trace prompting is brittle to obfuscation of names, implying reliance on pattern matching.",
            "counter_examples_or_negative_results": "In PDDL-format prompts GPT-4 performed worse than in natural language; zero-shot occasionally matched or exceeded one-shot (e.g., Blocksworld nat‑lang one-shot 206 vs zero-shot 210). The obfuscated 'Mystery' domains show that apparently similar prompting depends heavily on surface names and thus fails when pattern cues are removed.",
            "uuid": "e4951.0",
            "source_info": {
                "paper_title": "On the Planning Abilities of Large Language Models - A Critical Investigation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Chain-of-Thought (state-tracking)",
            "name_full": "Chain-of-Thought (state-tracking) Prompting",
            "brief_description": "A prompting technique that requests the model to produce intermediate stepwise state annotations and justifications (a state-tracking chain of thought) along with the plan.",
            "citation_title": "Chain of thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "GPT-4 (primary), other GPT variants tested",
            "model_description": "GPT-4 (transformer LLM) used with prompts that include a worked state-tracking example and ask the model to produce analogous stepwise reasoning and state updates.",
            "reasoning_method_name": "Chain-of-Thought (state-tracking)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Provides a single, explicit internal reasoning trace: each action annotated with the prior state, justification for applicability, and resulting state. This elicits a single detailed chain rather than multiple diverse traces.",
            "task_name": "Blocksworld (and other planning domains)",
            "task_description": "Same classical planning instances (Blocksworld etc.) but prompts include an annotated example showing stepwise state updates and action applicability reasoning.",
            "performance": "Blocksworld (natural language, GPT-4, COT): 214/600 (35.6%).",
            "comparison_with_other_method": true,
            "performance_other_method": "One-shot natural-language GPT-4: 206/600 (34.3%); Zero-shot: 210/600 (34.6%).",
            "key_findings": "COT/state-tracking produced only marginal improvements (≈1–3 percentage points) over one-shot/zero-shot natural-language prompts for GPT-4; authors conclude chain-of-thought had little effect on materially improving autonomous plan correctness.",
            "counter_examples_or_negative_results": "COT did not rescue failures in obfuscated (Mystery) domains and gave limited benefit relative to simpler one-shot prompts; overall gains were small and not transformative.",
            "uuid": "e4951.1",
            "source_info": {
                "paper_title": "On the Planning Abilities of Large Language Models - A Critical Investigation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Verifier-assisted backprompting",
            "name_full": "Verifier-assisted Repeated Backprompting with VAL",
            "brief_description": "An iterative feedback protocol where an external sound verifier (VAL) checks an LLM-generated plan, returns precise fault diagnostics (unsatisfied preconditions / delete interactions), and that feedback is used to re-prompt the LLM repeatedly up to a threshold to produce improved plans.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "GPT-4 prompted iteratively with explicit verifier feedback; experiments limited to previously failed one-shot natural-language instances, threshold of up to 15 backprompting rounds.",
            "reasoning_method_name": "Verifier-assisted iterative backprompting",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Creates diversity over multiple reasoning attempts by changing the prompt with verifier-supplied error diagnoses each round; the LLM produces new candidate plans conditioned on progressively specific failure information (not a single static chain).",
            "task_name": "Blocksworld, Logistics, Mystery Blocksworld (subset of failed instances)",
            "task_description": "50 failed instances per domain (from one-shot nat-lang configuration) were re-attempted with verifier feedback to see if LLM improves via iteration.",
            "performance": "On the tested failed-instance subsets: Blocksworld recovery success 82% (of the 50 failed instances), Logistics recovery success 70%; average successful backprompt rounds: Blocksworld 3.68, Logistics 3.31.",
            "comparison_with_other_method": true,
            "performance_other_method": "Autonomous (single-shot) GPT-4 success rates on those domains were much lower (Blocksworld overall ≈34% on nat-lang; many of these specific instances were previously failed).",
            "key_findings": "Verifier-assisted iterative prompting substantially improves plan correctness over autonomous single-shot generation: most previously failed instances for common-sense domains were corrected within a few verifier-guided re-prompts. This demonstrates that external, sound feedback plus iterative prompting enables LLMs to reach correct plans in many cases.",
            "counter_examples_or_negative_results": "Performance on Mystery Blocksworld remained poor despite backprompting, indicating that verifier-guided iteration helps only when the LLM can leverage commonsense cue patterns; in domains that obscure those cues, backprompting is less effective.",
            "uuid": "e4951.2",
            "source_info": {
                "paper_title": "On the Planning Abilities of Large Language Models - A Critical Investigation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LLM-Modulo (LLM+LPG)",
            "name_full": "LLM-guided Heuristic Seeding of an External Planner (LLM-Modulo with LPG)",
            "brief_description": "Treat LLM-generated plan as a seed (heuristic) for a sound local-search planner (LPG) which repairs the seed to a valid plan; used to measure whether LLM outputs can speed search even if not fully correct.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4, Instruct-GPT3",
            "model_description": "LLM produces a candidate plan (one-shot nat-lang); LPG (local-search planner) takes that plan as initial seed and performs iterative flaw-repair; baselines are empty-seed and random-seed.",
            "reasoning_method_name": "Heuristic seed + local search (LLM-Modulo)",
            "reasoning_method_type": "other",
            "reasoning_method_description": "Combines a single LLM candidate (one static seed) with a separate combinatorial planner (LPG) that performs search/repair; diversity arises from planner's search not from multiple LLM traces.",
            "task_name": "Blocksworld, Logistics, Mystery Blocksworld",
            "task_description": "Same classical planning instances; measure LPG's average search steps to a valid plan when seeded with (a) empty plan, (b) random plan, (c) LLM plan.",
            "performance": "Blocksworld (GPT-4 seed): avg search steps — Empty seed 15.8, Random 20.07, LLM seed 8.9; Logistics (GPT-4 seed): Empty 77.5, Random 144.39, LLM seed 51.3; Mystery BW (GPT-4 seed): small/no improvement (LLM seed 16.09 vs Empty 15.8 and Random 20.45). LLM seed plans retained roughly 50% of final plan elements (avg Levenshtein edit distance ≈ half the length).",
            "comparison_with_other_method": true,
            "performance_other_method": "Baselines: Empty seed and Random seed performance listed above; LLM+LPG shows substantial search-step reduction in Blocksworld and Logistics but not in Mystery Blocksworld.",
            "key_findings": "Even incorrect LLM plans can substantially reduce search effort for sound planners in commonsense domains (LLM seeds led to significantly fewer LPG search steps than empty or random seeds). However, when domain surface cues are obfuscated (Mystery BW), LLM seeds lose utility.",
            "counter_examples_or_negative_results": "In Mystery Blocksworld (obfuscated names) LLM seeds provided little or no advantage and sometimes performed worse than empty seeds, indicating reliance on surface/common-sense cues for generating useful heuristics.",
            "uuid": "e4951.3",
            "source_info": {
                "paper_title": "On the Planning Abilities of Large Language Models - A Critical Investigation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Fine-tuned GPT-3 on Blocksworld",
            "name_full": "Fine-tuning GPT-3 on Blocksworld Instances",
            "brief_description": "A supervised fine-tuning experiment: GPT-3 (davinci) fine-tuned on 1,000 Blocksworld instances (initial state, goal, plan) separate from the 600-instance test set to see if task-specific fine-tuning improves planning performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (davinci) fine-tuned",
            "model_description": "Davinci/GPT-3 model fine-tuned using OpenAI default hyperparameters on a domain-specific dataset of Blocksworld examples (80/20 train/validation split).",
            "reasoning_method_name": "Supervised fine-tuning (task-specific learning)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Model is trained to map (initial state + goal) → plan using standard supervised fine-tuning, producing a single deterministic output for each test prompt (no ensemble/diversity).",
            "task_name": "Blocksworld (test set of 600 instances)",
            "task_description": "Same Blocksworld planning instances used in the main evaluation; fine-tuned model tested zero/few-shot on held-out instances.",
            "performance": "Fine-tuned GPT-3 solved 122/600 instances (~20%) on the test set.",
            "comparison_with_other_method": true,
            "performance_other_method": "Non-fine-tuned GPT-3 and Instruct variants performed worse (e.g., GPT-3 base was ≈1% in natural-language one-shot). GPT-4 substantially outperformed fine-tuned GPT-3 (GPT-4 ≈34% on nat-lang Blocksworld).",
            "key_findings": "Task-specific fine-tuning improved vanilla GPT-3 performance (to ~20%) but did not reach GPT-4 performance and overall improvement was limited; suggests fine-tuning helps but does not solve combinatorial planning deficits.",
            "counter_examples_or_negative_results": "Fine-tuning did not close the gap to GPT-4 and remained far below human baseline (~78% valid plans in human study). Fine-tuning still yielded brittle behavior with respect to domain obfuscation (generalization limited).",
            "uuid": "e4951.4",
            "source_info": {
                "paper_title": "On the Planning Abilities of Large Language Models - A Critical Investigation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Self-critiquing / self-verification",
            "name_full": "LLM Self-critiquing / Self-verification",
            "brief_description": "The idea of using the LLM itself as a verifier or critic of its own generated plans (self-critique) rather than using an external sound verifier.",
            "citation_title": "Can large language models really improve by self-critiquing their own plans?",
            "mention_or_use": "mention",
            "model_name": "GPT-family (general)",
            "model_description": "General observation about LLMs' ability to verify their own outputs; referenced external analyses and the authors' related work.",
            "reasoning_method_name": "Self-critiquing / self-verification",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Using the same LLM (without an external verifier) to evaluate and correct its outputs (e.g., ask model to check plan correctness and revise), which is essentially a repeated single-model prompting loop without a sound symbolic verifier.",
            "task_name": "Plan verification and iterative improvement (discussion, no primary experiments in this paper)",
            "task_description": "Assessing whether LLMs can reliably detect errors in their own plans and improve them without external sound verification.",
            "performance": "No numeric performance reported here; authors cite their own related work and state that self-verification/self-critiquing is of questionable utility because LLMs are no better at verification than generation.",
            "comparison_with_other_method": true,
            "performance_other_method": "External verifier (VAL) + backprompting (described in paper) yielded strong improvements (e.g., 82% Blocksworld recovery) whereas self-critiquing has been found to be unreliable per cited analyses.",
            "key_findings": "Authors explicitly caution against relying on self-verification: mechanical external verifiers provide precise diagnostics and substantially better grounding for iterative correction; using the LLM to both generate and verify plans is unlikely to be reliably effective.",
            "counter_examples_or_negative_results": "Referenced analyses show iterative self-critiquing often fails because the model repeats its mistakes or cannot reliably identify faults; the paper uses this to motivate use of a sound external verifier (VAL) instead.",
            "uuid": "e4951.5",
            "source_info": {
                "paper_title": "On the Planning Abilities of Large Language Models - A Critical Investigation",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "VAL: Automatic plan validation, continuous effects and mixed initiative planning using PDDL",
            "rating": 2
        },
        {
            "paper_title": "Lpg: A planner based on local search for planning graphs with action costs",
            "rating": 2
        },
        {
            "paper_title": "Can large language models really improve by self-critiquing their own plans?",
            "rating": 2
        },
        {
            "paper_title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "rating": 1
        }
    ],
    "cost": 0.01691,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>On the Planning Abilities of Large Language Models : A Critical Investigation</h1>
<p>Karthik Valmeekam<br>School of Computing \&amp; AI<br>Arizona State University Tempe.<br>kvalmeek@asu.edu<br>Sarath Sreedharan*<br>Department of Computer Science, Colorado State University, Fort Collins. sarath.sreedharan@colostate.edu</p>
<p>Matthew Marquez<br>School of Computing \&amp; AI<br>Arizona State University, Tempe. mmarqu22@asu.edu<br>Subbarao Kambhampati<br>School of Computing \&amp; AI<br>Arizona State University, Tempe. rao@asu.edu</p>
<h4>Abstract</h4>
<p>Intrigued by the claims of emergent reasoning capabilities in LLMs trained on general web corpora, in this paper, we set out to investigate their planning capabilities. We aim to evaluate (1) the effectiveness of LLMs in generating plans autonomously in commonsense planning tasks and (2) the potential of LLMs in LLM-Modulo settings where they act as a source of heuristic guidance for external planners and verifiers. We conduct a systematic study by generating a suite of instances on domains similar to the ones employed in the International Planning Competition and evaluate LLMs in two distinct modes: autonomous and heuristic. Our findings reveal that LLMs' ability to generate executable plans autonomously is rather limited, with the best model (GPT-4) having an average success rate of $\sim 12 \%$ across the domains. However, the results in the LLM-Modulo setting show more promise. In the LLM-Modulo setting, we demonstrate that LLM-generated plans can improve the search process for underlying sound planners and additionally show that external verifiers can help provide feedback on the generated plans and back-prompt the LLM for better plan generation.</p>
<h2>1 Introduction</h2>
<p>It would be no exaggeration to say that transformer-based large language models (LLMs) have revolutionized the field of natural language processing (NLP). Kicked off by the advances presented by the GPT-x models developed by OpenAI [27], these types of language models currently provide state-of-the-art performance in many of the standard NLP tasks. Although LLMs were originally developed mostly to do word sequence completion tasks, with no guarantees about the completion beyond its coherence, there have been increasing claims and anecdotal evidence that they have other emergent capabilities that are not normally associated with sequence completion. Indeed, the hints of such emergent capabilities has started a veritable land rush, with researchers probing (prompting) and studying LLM behavior almost as if they were artificial organisms (c.f. [16]). Of particular interest to us in this paper is the thread of efforts that aim to investigate (and showcase) reasoning abilities of LLMs-including commonsense reasoning [35, 29, 7], logical reasoning [33], and even ethical reasoning [15]. The macro-tenor of the drumbeat of these works has been suggesting that LLM's are indeed capable of doing such kinds of reasoning [19, 37, 4].</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>One type of reasoning task that has been well studied in the AI community is planning and sequential decision making. At its simplest, planning involves developing a course of actions (policy) which when executed takes the agent to a desired state of the world. Planning has generally been studied primarily as an inference on world and reward models-whether specified by humans or learned by the agent by interacting with its world. In this paper, we are interested in seeing what planning abilities, if any, LLMs may already have, given their high capacity functions (with billions of tunable parameters) trained on web-scale corpora. Specifically, we are interested in answering two broad questions:</p>
<ol>
<li>How effective are LLMs by themselves in generating simple plans in commonsense planning tasks (of the type that humans are generally quite good at)?</li>
<li>How good are LLMs in an LLM-Modulo setting where they act as a source of heuristic guidance for other agents in their planning tasks?</li>
</ol>
<p>Notice that in theory, it is possible for LLMs to be very effective as idea generators for external sound planners or humans in the loop in computer-supported cooperative work scenarios, while themselves being very bad at generating plans that are guaranteed to be correct. This is especially likely because the chief power of LLMs comes from their pattern-finding abilities than from firstprinciples simulations over world models. Compared to a planner that is guaranteed to be correct in a narrow set of domains, LLMs may likely be good at generating plausible (but not guaranteed to be correct) plan heuristics/suggestions in many more domains.</p>
<p>To investigate these questions in a systematic rather than anecdotal manner, we generate a suite of planning problem instances ${ }^{2}$ based on the kinds of domains employed in the International Planning Competition [14]. To eliminate the subjective aspect of analysis that forms the core part of many earlier efforts on evaluating the reasoning capabilities of LLMs, we automate the evaluation by leveraging models and tools from the automated planning community.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The diagrammatic overview of the two modes of LLMs for planning.
The evaluation itself is done in two modes (shown in Figure 1). In the first "autonomous" mode, LLMs are used standalone, and we directly assess the quality and correctness of plans they generate. As we shall see, the results in the autonomous mode are pretty bleak. On an average, only about $12 \%$ of the plans that the best LLM (GPT-4) generates are actually executable without errors and reach their goals. We will show that the choice of the specific LLM (we have tested the family of GPT LLMs including GPT-4 [25], GPT-3.5 [24], InstructGPT-3.5, InstructGPT-3 [26] and GPT-3 [3]), as well as fine tuning does not seem to have a major effect on this dismal performance. We also show that the performance deteriorates further if the names of the actions and objects in the domain are obfuscated-a change that doesn't in anyway affect the performance of the standard AI planners. To shed further light on the performance of GPT4, we present an evaluation of the plans it generates under a series of more relaxed (more forgiving) executability conditions. Further, we provide a human baseline for the simplest domain in our set of domains, by presenting the planning instances to human subjects (through IRB-approved studies) and evaluating the quality and correctness of their plans. These results are substantially better than those of LLMs-confirming that LLMs can't plan even in a simple common sense domain in the autonomous mode. In the second LLM-Modulo mode, the plans produced by LLMs are given as input to an automated planner working off of a correct domain model to check whether the LLM's plans help with the search process of the underlying planner to come up with correct plans. Specifically we show that a well known automated planner called LPG [6], that</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>uses local search to locate and remove flaws in a candidate plan to make it correct, is able to repair the LLM plans with relative ease. We compare the LLM+LPG combination with two baselines, one where an empty plan is used as the seed plan for the LPG and two, where a random plan is provided as the seed plan to the LPG. We show that the average search steps by the LLM+LPG combination is much lesser than both the baselines, thereby revealing that LLMs' plans are indeed helping with the search process of the underlying planner. Further, instead of having LPG correct the plans, we use an external verifier, VAL [11], to point out the errors in the LLM-generated plans and back-prompt the LLM for a new plan with this feedback. We show that this repeated interaction indeed improves the plan correctness in common-sense domains. Overall, our findings demonstrate that, with respect to planning, LLMs' perform poorly in the autonomous mode but the generated plans can help AI planners in the search process or can be given to external verifiers and back-prompt the LLM for better plans. In this paper, we first present an overview of the related work. Following that, we describe the necessary background and the prompt generation pipeline. Finally, we provide the results and analysis of various experiments undertaken in both autonomous and heuristic evaluation modes.</p>
<h1>2 Related Work</h1>
<p>In this work, we look at LLMs' planning capabilities when the domain is given as part of the prompt (as is the standard practice in automated planning [8]). Our evaluation focuses on zero-shot (just domain and problem specification), and few-shot (example problems with plans) modes. There have been a few works that looked at the planning capabilities of LLMs. Most of them, such as [12, 2] focus on commonsense domains/tasks (e.g. moving things in kitchens, wedding/menu planning etc.) and thus evaluate LLMs in a mode wherein the prompt doesn't include any information about the specific domain. Plans generated in that way are hard to evaluate as they are not directed at any plan executor and the humans often wind up giving the benefit of doubt for a plausible-but not actually executable-plan. This is why in SayCan [2], where executability is critical, they try to filter out/interpret the LLM plans in terms of the skills/actions that are actually available to the executor. While SayCan does this in a rather convoluted way that requires access to the internal log probabilities of the LLM, our approach simplifies this by specifying the domain as part of the prompt. In all our experiments, we found that LLMs only use the actions listed as part of the domain specification.
One other mode of evaluation of planning capabilities in the literature involves the user incrementally interacting with the LLM, and re-prompting it to point out flaws in its plans, with the hope that the LLM eventually reaches an executable plan [13, 39, 28]. Such evaluations are notorious for their Clever Hans effect [1] with the actual planning being done by the humans in the loop rather than the LLMs themselves. We thus separate our evaluation into two modes-autonomous and as assistants to external planners/reasoners. There have also been efforts which mostly depended on LLMs as "translators" of natural language problem/goal specification into formal specifications, which are then thrown over to sound external planners [38, 21]. Such efforts don't shed any light on the internal planning capabilities of the LLMs themselves, as our evaluations in autonomous and assistive modes do. Finally, after our initial study and benchmark were made public, other groups did parallel studies that largely corroborate our results on the ineffectiveness of LLMs in finding executable plans [32, 21].
Taking a broader perspective, making plans in the world involves (1) discovering actions (and their precondition/effect causal dependencies), and (2) sequencing an appropriate subset of available/discovered actions to achieve the agent's goals. The former requires broad knowledge about actions available in the world and their individual effects, while the latter requires deep drilling-down over a given set of actions to ensure that all goals are supported (causal chaining) without any undesirable interactions. LLMs have an edge on the former-they do indeed have web-scale broad knowledge! As we shall see however, they are very bad at the second phase of developing valid interaction-free plans (in part, because LLMs don't have the ability to do combinatorial search). Most cases in literature (as outlined in [18]) where LLMs are claimed to have "planned" turn out, upon close examination, to be instances of phase 1-your wedding plans, recipe plans etc.-where you are either using a very forgiving plan correctness criterion, or the phase 2 is vacuous. Standard AI planners-on the other hand-assume that the discovery part is done and handed down as a compact domain model, and focus mostly on the second part: selecting among known actions to establish causal chains and sequencing them to make them interaction free. In this sense, LLMs and AI planners can be complementary, as we have shown in this paper-with the former helping with phase 1-either with a candidate/approximate plan or domain model-and the latter with phase 2 .</p>
<h1>3 Prompt Generation for Classical Planning Problems</h1>
<h3>3.1 Background</h3>
<p>Given that we are interested in investigating the basic reasoning about actions and change problem, we want to look at the most fundamental planning formalism first, namely the goal-directed deterministic planning problem. Colloquially referred to as classical planning problem, these problem classes consist of a problem domain, an initial state and a goal state. The problem domain consists of a set of fluents which correspond to predicates with some arity and a set of actions. The state-space for the planning problem is defined by the possible truth assignment over the predicates. Each action consists of preconditions and effects where preconditions is a set of predicates that describe when an action can be executed and effects are set of predicates that describe what happens when an action is executed. The effects can further consist of add effects, which is the set of predicates that will be set true by the action, and delete effects, which is the set of predicates that will be set false. The solution for a planning problem is a sequence of actions, or a plan, that when applied in the initial state will result in a state where the goal conditions are satisfied. A standard representation to specify such kind of planning problems is the Planning Definition and Domain Language (PDDL) [22]. Below is a snippet of an action from a popular benchmark problem called Blocksworld, in PDDL. The action corresponds to picking up a block in that domain.</p>
<div class="codehilite"><pre><span></span><code>(:action pickup
    :parameters (?ob)
    :precondition (and (clear ?ob) (on-table ?ob) (arm-empty))
    :effect (and (holding ?ob) (not (clear ?ob)) (not (on-table ?ob))
        (not (arm-empty))))
</code></pre></div>

<p>A more detailed description on classical planning problems is provided in Appendix A.1. We now will describe how we generate the prompts that are given to the LLMs.</p>
<h3>3.2 Prompt Generation</h3>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The diagrammatic overview of the prompt generation pipeline. The prompt configurations for the different experiments are generated from PDDL domain files and are modified with an example generator and natural language translator as needed depending on the experiment requirements.</p>
<p>Prompt Configurations: We have developed a suite of unique planning problems to test LLMs' abilities to generate plans. We have multiple prompt configurations based on this suite of problems, varying in both the method of presentation as well as number of examples given to the LLM. In particular, we use two methods of presentation, natural language and PDDL, as well as two different methods of providing examples, zero shot (with no examples provided) and one shot (with an example provided), giving us four different configuration combinations for our experiments.
Within a prompt, LLMs are first provided with a lifted domain description. For one shot configurations, the prompt additionally contains an example instance of a planning problem (consisting of a description of the initial state and the goal) and the corresponding plan (which ends with a tag, referred to as the plan-end tag, that denotes the end of the plan). All prompts end with a planning problem description. The text generated by the LLM until the plan-end tag is used as the candidate for extracting the plan. If the extractor cannot reasonably extract an instance, it is marked as incorrect.</p>
<p>Table 1: Results of GPT-4, GPT-3.5 (popularly known as ChatGPT), Instruct-GPT3.5, Instruct-GPT3 (text-davinci-002) and GPT3 (davinci) for the Plan Generation task with prompts in natural language.</p>
<table>
<thead>
<tr>
<th>Domain</th>
<th>Method</th>
<th>Instances correct</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>GPT-4</td>
<td>GPT-3.5</td>
<td>I-GPT3.5</td>
<td>I-GPT3</td>
<td>GPT-3</td>
</tr>
<tr>
<td>Blocksworld (BW)</td>
<td>One-shot</td>
<td>206/600 (34.3\%)</td>
<td>37/600 (6.1\%)</td>
<td>$\begin{gathered} 54 / 600 \ (9 \%) \end{gathered}$</td>
<td>$\begin{gathered} 41 / 600 \ (6.8 \%) \end{gathered}$</td>
<td>$\begin{gathered} 6 / 600 \ (1 \%) \end{gathered}$</td>
</tr>
<tr>
<td></td>
<td>Zero-shot</td>
<td>210/600 (34.6\%)</td>
<td>8/600 (1.3\%)</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>COT</td>
<td>214/600 (35.6\%)</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Logistics Domain</td>
<td>One-shot</td>
<td>28/200 (14\%)</td>
<td>1/200 (0.5\%)</td>
<td>6/200 (3\%)</td>
<td>$\begin{gathered} 3 / 200 \ (1.5 \%) \end{gathered}$</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>Zero-shot</td>
<td>15/200 (7.5\%)</td>
<td>1/200 (0.5\%)</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Mystery BW <br> (Deceptive)</td>
<td>One-shot</td>
<td>26/600 (4.3\%)</td>
<td>0/600 (0\%)</td>
<td>$\begin{gathered} 4 / 600 \ (0.6 \%) \end{gathered}$</td>
<td>$\begin{gathered} 14 / 600 \ (2.3 \%) \end{gathered}$</td>
<td>$\begin{gathered} 0 / 600 \ (0 \%) \end{gathered}$</td>
</tr>
<tr>
<td></td>
<td>Zero-shot</td>
<td>1/600 (0.16\%)</td>
<td>0/600 (0\%)</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>COT</td>
<td>54/600 (9\%)</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Mystery BW <br> (Randomized)</td>
<td>One-shot</td>
<td>12/600 (2\%)</td>
<td>0/600 (0\%)</td>
<td>$\begin{gathered} 5 / 600 \ (0.8 \%) \end{gathered}$</td>
<td>$\begin{gathered} 5 / 600 \ (0.8 \%) \end{gathered}$</td>
<td>$\begin{gathered} 1 / 600 \ (0.1 \%) \end{gathered}$</td>
</tr>
<tr>
<td></td>
<td>Zero-shot</td>
<td>0/600 (0\%)</td>
<td>0/600 (0\%)</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>The prompt is either formatted in natural langauge or PDDL. Natural language prompts utilize complete natural language sentences to describe feasible actions in the domain. Initial conditions are also reported as complete sentences. Plans in the natural language setting take the form of a series of commands such as "stack the orange block on top of the blue block". As implied by the name, PDDL prompts format all elements (domain description, initial state, goal state, and plans) using PDDL. We point the reader to the supplementary material for examples on each of these prompt configurations.</p>
<p>Chain of Thought Prompting: In addition to the four experiments above, we look at a fifth experiment using a state tracking chain of thought prompting technique in a natural language one shot setting. Within this configuration, we provide an annotated example where each action is annotated with the state prior to the action, the reason for why the action is applicable in the prior state, and the resulting state after applying the action. After the example, a meta-explanation about plan correctness is provided. The LLM is then asked to return a response making the same state tracking and justification annotations that were included in the example.</p>
<p>Prompt Generation Pipeline: We’ve developed a prompt generation pipeline (visualized in Figure 2) that accepts PDDL domain files as input and outputs prompts that follow the experiments described above. The prompt generation component takes care of creating the set of PDDL problems to be solved for all experiments. Following that, examples are added to the prompt in one shot experiments. While our setup utilizes a planner during example generation, any example generation technique could be used here so long as the examples generated are valid plans. In the state tracking experiment, we also have developed a component to add justification annotations for examples so that the examples reflect what we expect of the LLM. The last step before finishing is translation: since problems at this point are currently in PDDL, prompts for all natural language experiments (whether an example was added or not) need to be translated into natural language. We utilize a domain-specific translator to do so.</p>
<h1>4 Evaluating Planning Capabilities of LLMs in Autonomous Mode</h1>
<p>In the autonomous mode, we treat the LLM as an automated planner and perform a single run of the dataset on the LLMs for each domain and prompt configuration. In this mode, the plan generated by the LLM is back-translated from natural language to forms that can be used by external plan validators. For each domain, we perform template-based translation to translate between PDDL and natural language for the natural language prompt configurations. We use VAL [11] to evaluate the translated plan with the corresponding domain and problem file. Our evaluation here primarily</p>
<p>Table 2: Results of GPT-4 and GPT-3.5 (popularly known as ChatGPT) for the Plan Generation task with one or zero examples in the prompt by directly providing the domain and problem in PDDL.</p>
<table>
<thead>
<tr>
<th>Domain</th>
<th>Method</th>
<th>Instances correct</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>GPT-4</td>
<td>GPT-3.5</td>
</tr>
<tr>
<td>Blocksworld (BW)</td>
<td>One-shot</td>
<td>75/600 (12.5\%)</td>
<td>12/600 (2\%)</td>
</tr>
<tr>
<td></td>
<td>Zero-shot</td>
<td>106/600 (17.6\%)</td>
<td>12/600 (2\%)</td>
</tr>
<tr>
<td>Logistics Domain</td>
<td>One-shot</td>
<td>28/200 (14\%)</td>
<td>1/200 (0.5\%)</td>
</tr>
<tr>
<td></td>
<td>Zero-shot</td>
<td>11/200 (5.5\%)</td>
<td>0/200 (0\%)</td>
</tr>
<tr>
<td>Mystery BW (Deceptive)</td>
<td>One-shot</td>
<td>17/600 (2.8\%)</td>
<td>1/600 (0.1\%)</td>
</tr>
<tr>
<td></td>
<td>Zero-shot</td>
<td>3/600 (0.5\%)</td>
<td>0/600 (0\%)</td>
</tr>
</tbody>
</table>
<p>focuses on the GPT family of LLMs. We tested GPT-4 [25] and GPT-3.5 (commonly known as Chat-GPT) [24] on all the prompt configurations while we tested the older versions of GPT (namely, Instruct-GPT3 and GPT3) on one-shot natural language prompts across the domains. We set the temperature for all models to be 0 , thereby making them deterministic. In this section, we detail the evaluation of LLMs on these domains and prompt configurations. We would like to point the reader to the Appendix for example prompts.</p>
<p>Evaluation of LLMs on the Blocksworld domain: Blocksworld problems capture common sense block manipulations and consist of a set of blocks. Blocks are identified with unique colors and are placed either on a table or on top of other blocks. The goal is to arrange some of these blocks in a stack in a particular order. The general expectation here would be that one can pick up a block if it is clear, i.e., there are no other blocks on top of that block and you can only stack a block on top of another block if it is clear. The choice of this particular domain is motivated by both the fact that this is a simple common sense domain and is a very popular domain in planning literature, that has a long history of being used in various planning challenges. The instances were generated using a PDDL generator employed in the IPC competitions. We permitted the generation of problems that varied in terms of the number of blocks (3-5), optimal plan length, and goal properties (positive, negative, or no interactions between subgoals).
As shown in Table 1 and Table 2, GPT-4 improves upon previous versions of GPT models in the Blocksworld domain across all four prompt configurations. However, the overall performance is still approximately $34 \%$ in the Blocksworld dataset. Even the chain of thought style prompting (indicated by COT in the tables) had little effect on improving the performance. GPT-4 performs better with natural language prompts (206 and 210 instances for one-shot and zero-shot prompts, respectively) as opposed to PDDL prompts ( 75 and 106 instances). The performance drops significantly with other GPT models. We also discovered that for instances where Instruct-GPT3 generated the correct plans, replacing the example plan in the prompt with another example plan led to an even greater drop in accuracy. This suggests that the LLM seems to rely primarily on pattern matching, rather than inducing some internal model from the prompts. Overall, even in a seemingly simple common-sense domain like Blocksworld, which humans typically find easy to navigate, LLMs prove to be quite ineffective in planning autonomously.</p>
<p>Finetuning GPT-3 on Blocksworld: Along with directly testing the LLMs from the GPT family, we have also looked at the utility of fine-tuning the LLMs. Specifically, we fine-tuned GPT-3 (Davinci) in the Blocksworld domain. For this, we prepared a dataset comprising the initial state, goal state, and the respective plan for 1,000 distinct Blocksworld instances. It's important to note that these instances were separate from our test set of 600 instances. By using the default hyperparameters provided by OpenAI and an 80-20 train-validation data split, we carried out the fine-tuning process. Our results revealed that the fine-tuned GPT-3 solved only 122 instances out of the 600 in our set, representing approximately $20 \%$ of the total. This suggests that fine-tuning has a limited impact on improving the performance of LLMs in Blocksworld planning. This outcome aligns with the observations of [40], who argue that language models trained for reasoning tend to concentrate on the inherent statistical features instead of the causal structure, which in turn affects their performance on such tasks.</p>
<p>Open-source models: In addition to the GPT family of LLMs, we have also conducted preliminary experiments with an open-source LLM, BLOOM [31], and found that BLOOM too is ineffective in plan generation. We assessed BLOOM's performance in the blocksworld and mystery blocksworld (deceptive) domains using a one-shot natural language prompt configuration. In the blocksworld domain, BLOOM correctly handled only 4 out of 250 instances, representing a $1.6 \%$ success rate. In the mystery domain, it failed to produce a single correct response in all 50 instances.</p>
<p>Human Baseline for the Blocksworld: We have previously mentioned that planning tasks on the blocksworld domain are anecdotally simple enough for humans to perform. To establish this and come up with a preliminary baseline to compare LLMs performance, we conducted an IRB-approved user study where we asked 50 participants to come up with a plan for a blocksworld instance picked at random, from the set of 600 instances that we used for the evaluation of LLMs. We presented the same domain description as we did for the LLMs and then primed them with an example instance. We point the reader to the supplementary material for further details on the study.</p>
<p>Out of the 50 participants, 39 of them ( $78 \%$ ) came up with a valid plan. Along with validity, we also tested the optimality of their plans even though they were not required to come up with an optimal plan. Out of the 39 participants, $35(89.7 \%)$ participants came up with an optimal plan. These initial results show that the blocksworld domain is a simple enough domain where most humans are able to come up with plans (which are also optimal) while LLMs, on the other hand, showcase subpar performance.</p>
<p>Evaluation of LLMs on the Logistics domain: Logistics is also a widely recognized domain in the planning literature. In this domain, the objective is to transport packages within cities via trucks, and between cities via airplanes. Within a city, the locations are directly linked, allowing trucks to travel between any two of these locations. Similarly, cities are directly connected to each other allowing airplanes to travel between any two cities. Each city is equipped with one truck and has a designated location that functions as an airport. We generated 200 instances on this domain.</p>
<p>From Tables 1 and 2, we see that in the one-shot setting with natural language input, GPT-4 only solved $14 \%$ of the instances (28/200), and this rate dropped to $7.5 \%(15 / 200)$ when using zero-shot prompting. When provided with the domain and problem in PDDL format, GPT-4's performance remained the same in the one-shot setting ( $14 \%$ or $28 / 200$ ) but decreased to $5.5 \%(11 / 200)$ in the zero-shot setting. GPT-3.5 did even worse.</p>
<p>Obfuscating names to test the brittleness of LLM Planning: Although the domain specification is part of our prompts, the names of the objects (e.g. blocks, trucks), predicates (e.g. on-table, in-city) and actions (e.g. pickup, drive) still do provide connections to the commonsense knowledge that the pretrained LLMs possess. One intriguing question is whether the planning performance is based really only on the domain model or these other background connections. To test this, we experimented with a variation of the Blocksworld domain, where we obfuscate the action names (for example pickup becomes attack, and unstack becomes feast) and predicate names (for example ontable becomes planet, and handempty becomes harmony). Note that from the perspective of standard planners, these domains are essentially identical. ${ }^{3}$ In addition to such deceptive obfuscation, we also considered a variation where random alphanumeric names were substituted for the action and object names.</p>
<p>Tables 1 and 2, we see that this simple obfuscation leads to a catastrophic drop in performance. Specifically, with zero-shot prompting and natural language input, GPT-4 is able to solve 210 instances out of 600 in the Blocksworld domain, but it could only solve 1 instance in the deceptive Mystery Blocksworld domain and 0 instances in the randomized mystery domain. A similar result is observed with the PDDL-style prompts: GPT-4 could solve 106 instances in Blocksworld, but only 3 instances in the deceptive Mystery Blocksworld. Notably, chain of thought prompting does not significantly improve performance over one-shot natural language prompts. GPT-3.5 does not solve even a single instance in the entire set of natural language instances. For most of the instances, GPT-3.5 outputs that the instance can't be solved. These results strongly suggest that whatever accidental planning performance LLMs show is likely connected to pattern matching rather than reasoning (which should be robust to name change).</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Assessment of GPT-4 plans with relaxations in Blocksworld domain
Table 3: Evaluation of GPT-4 and Instruct-GPT3 (I-GPT-3) plans as heuristics for a local search planner LPG, on blocksworld (BW), logistics and mystery blocksworld domains.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Domain</th>
<th style="text-align: center;">LLM</th>
<th style="text-align: center;">Avg. Search Steps</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg. Plan Length</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg. Lev. <br> Distance</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Empty <br> Seed <br> Plan</td>
<td style="text-align: center;">Random <br> Seed <br> Plan</td>
<td style="text-align: center;">LLM <br> Seed <br> Plan</td>
<td style="text-align: center;">Empty <br> Seed <br> Plan</td>
<td style="text-align: center;">Random <br> Seed <br> Plan</td>
<td style="text-align: center;">LLM <br> Seed <br> Plan</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BW</td>
<td style="text-align: center;">I-GPT-3</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">20.07</td>
<td style="text-align: center;">14.5</td>
<td style="text-align: center;">8.45</td>
<td style="text-align: center;">9.62</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">7.22</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">20.07</td>
<td style="text-align: center;">8.9</td>
<td style="text-align: center;">8.45</td>
<td style="text-align: center;">9.62</td>
<td style="text-align: center;">10.76</td>
<td style="text-align: center;">4.15</td>
</tr>
<tr>
<td style="text-align: center;">Logistics</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">144.39</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">32.72</td>
<td style="text-align: center;">32.24</td>
<td style="text-align: center;">15.04</td>
</tr>
<tr>
<td style="text-align: center;">Mystery BW</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">20.45</td>
<td style="text-align: center;">16.09</td>
<td style="text-align: center;">8.45</td>
<td style="text-align: center;">9.78</td>
<td style="text-align: center;">11.53</td>
<td style="text-align: center;">7.77</td>
</tr>
</tbody>
</table>
<p>Analyzing GPT-4 failures: To get a better sense of the type of failures LLM generated plans encounter, we wondered whether they will fare much better with a more forgiving test of the validity of the generated plans. In automated planning community, the notion of relaxations of the domain model are used to simplify the problem-chiefly to derive heuristics for planning problems [8]. Taking a leaf from them, we considered two types of relaxations: (i) delete relaxation involves ignoring all the delete conditions of the domain actions (thus making sure that there can be no negative interactions between subgoals) and (ii) precondition relaxation involves ignoring all the preconditions of the domain actions-thus assuming that the the actions are executable from any state giving their effects. Our idea is to evaluate the plans produced by GPT4 with respect to domain models that are delete relaxed, precondition relaxed or both. It should be clear that a plan that is correct with respect to the normal (unrelaxed) model will also be correct with respect to all the relaxed models. Figure 3 shows the results for blocksworld. We see that while the correctness of LLM generated plans increased under more forgiving (relaxed) assessments (area in green), even in the most lenient assessment mode (Delete+Precondition Relaxed), there still are plans ( $\sim 39 \%$ ) that are incorrect (because they still don't reach the goals) across all the prompt configurations. The plots further classify the failure cases in terms of whether they were inexecutable, shown in maroon, or could be executed but didn't reach the goals (shown in red). Note that when preconditions are relaxed, all plans are executable. We provide additional details on the relaxed assessments in Appendix A.2.</p>
<h1>5 Evaluating LLMs as Idea Generators</h1>
<p>While the preceding discussion establishes that LLMs are not capable of generating correct plans in autonomous mode, there is still the possibility that they can be useful in an LLM-Modulo mode as idea generators for other sound external planners, verifiers or even humans-in-the-loop. In this section, we investigate this possibility and demonstrate that LLMs show promise on this front (especially with external planners and verfiers).</p>
<h3>5.1 LLM Plans as Heuristics to Sound Planners</h3>
<p>To see if the LLM generated plans can provide heuristic guidance to sound external planners, we use a local-search planner LPG [6] which generates plans by starting with a seed plan and iteratively repairing flaws until a correct plan is found. We feed the LLM-generated plan as the initial seed plan</p>
<p>for LPG's iterative search. Our hypothesis is that this might put LPG on the right path and reduce the time for it to generate a correct plan. It is interesting to note the similarities between this LLM+LPG approach, and the approaches used in case-based planning in the past [9, 17]. Here the LLM can be loosely viewed as "retrieving a potentially useful plan case/sketch" out of thin air, which the LPG adapts/corrects.
We utilized the plans that were generated by LLMs in the one-shot natural language prompt configuration on all three of our previous domains - Blocksworld, Mystery Blocksworld, and Logistics - as the "seed plans" from which LPG would begin its local search for a valid plan. For the Blocksworld domain, both GPT-4 and Instruct-GPT3 were evaluated, whereas for the Logistics and Mystery domains only GPT-4 was evaluated. We confirmed that all the plans that were generated by this LLM+LPG combination for both the domains were valid (which is as expected given that the underlying planner, LPG, is sound). To get an idea of how far the initial LLM generated plans were from the final correct solutions generated by LPG, we measured the Levenshtein edit distance between them. While the default LPG local search doesn't aim to minimize the changes to the suggested plan (there do exist versions of LPG that do this; see [23]), the edit distances also give an idea of how partially or approximately correct the original LLM plan is. Along with the edit distance, we also measured the number of search steps that were taken by the LPG to come up with a correct plan.
As shown in Table 3, the edit distances across domains are approximately half the length of the seed plans generated by the LLMs, indicating that $50 \%$ of the final plan retains the elements of the initial LLM plan. For each problem, we performed two additional plan initializations to serve as baselines: initializing with an empty plan and initializing with a random plan of the same length as the plan generated by the LLM for that problem. In the Blocksworld and Logistics ${ }^{4}$ domains, we see a significant improvement in search steps over the empty seed plan when GPT-4 is used and an even larger one over the random seed plan. Consistent with our findings in the autonomous mode, the usefulness of this assistance wanes in domains where the relationships between predicates can no longer be inferred from common sense understandings of their names: in the Mystery Blocksworld domain, the LLM only has meager reduction in step size over the random plan and actually uses more steps than the empty plan.</p>
<h1>5.2 Verifier-assisted repeated backprompting of LLMs</h1>
<p>The interaction between LLM and LPG was uni-directional-with LLM sending a seed plan that LPG aims to repair. One supposed advantage of LLMs is that they can be prompted to improve their solutions. Suppose we have access to a sound automated verifier that not only checks the plan correctness but also pinpoints faults (in terms of unsatisfied preconditions or delete interactions). Such feedback can be easily converted into a "backprompt" to the LLM, with the hope that LLM comes up with a better plan. This is what we do with the help of VAL[11]-an AI planning tool that uses the domain model to validate the correctness of the plans (and point out errors).
While, as we mentioned earlier, there can be thorny "clever hans" issues about humans prompting LLMs, an automated verifier mechanically backprompting the LLM doesn't suffer from these.
We tested this setup on a subset of the failed instances in the one-shot natural language prompt configuration using GPT-4, given its larger context window. We set a threshold of 15 backprompting rounds. We tested on three domains-Blocksworld, Logistics and Mystery BW-with 50 failed instances from each domain. Table 4 shows the results. We provide the prompt+feedback examples in Appendix A.9. We found that GPT4 is able to come up with correct plans $82 \%$ of the Blocksworld instances and $70 \%$ of the Logistics one. The average number of backprompting rounds for these</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>successful cases was 3.68 for BW and 3.31 for Logistics. The performance on the Mystery BW however remained quite poor-suggesting that even with back prompting, GPT4 cannot do well unless it can tease out commonsense patterns for the domain.</p>
<p>In this backprompting configuration, LLM serves as the candidate plan generator while VAL serves as the external sound verifier. While it is tempting to have a self-critiquing architecture with LLM also serving as the verifier, our recent work shows that approach to be of questionable utility as LLMs are no better at verifying plans than they are at generating them [36, 34].</p>
<h1>5.3 LLMs as idea generators for humans-in-the-loop</h1>
<p>Along with external planners and verifiers, LLMs may also offer their insights as plan suggestions directly to the human-in-the-loop which might potentially guide the user to the correct plan. After all, this sort of computer supported cooperative work (CSCW) use case has been the staple of LLM applications. We explored the efficacy of LLMs in assisting human planners through a betweensubjects user study, structured similarly to the study outlined in Section 4, but with two primary distinctions: (1) The study involved two separate participant groups. The first group received no assistance in devising plans, paralleling the approach in Section 4, while the second group had access to LLM-generated suggestions. (2) both participant sets were asked to offer subjective feedback via the NASA-TLX assessment tool [10], gauging their cognitive load. Additionally, participants from the second group evaluated the correctness of the LLM suggestions presented to them. We utilized the plans generated by GPT-4 to provide plan suggestions.</p>
<p>The study included 49 participants in the unassisted group and 48 in the LLM-assisted group. We evaluated the statistical significance regarding accuracy, time taken, and cognitive load between the groups. Our findings revealed no statistical significance between the groups across all three aspects. ${ }^{5}$. Notably, 3 out of 48 participants mistakenly accepted incorrect LLM suggestions, with two submitting these erroneous suggestions as their plans. This shows the potential for automation bias in such methodologies [5]. We have provided the details of the user-study in Appendix A.12.</p>
<h2>6 Conclusion and Future Work</h2>
<p>In this paper, we presented a critical investigation of the planning abilities of large language models (LLMs). To this end, we evaluated the plan generation abilities of LLMs in two different modes. In the autonomous mode, our results show that even in simple common-sense planning domains where humans could easily come up with plans, LLMs like GPT-3 exhibit a dismal performance. Even though there is an uptick in the performance by the newer GPT-4 in the blocksworld domain, it still fails miserably on the mystery blocksworld domain, indicating their inability to reason in an abstract manner. In the LLM-Modulo setting, we have seen that plans generated by LLMs can help improve the search of sound planners like LPG. Further, we showed that using external verifiers, we can point out the errors and back-prompt LLMs for a better plan. We showed that this indeed helps in common-sense domains. In the supplementary material, we show the prompt examples for all the configurations and the details of the user-studies (Appendix A.11). From our studies, we see that LLMs as autonomous planners fail miserably, but we also see that the generated plans improve the search when used by an underlying sound planner and that better plans can be obtained by back-prompting the LLM with feedback from an external verifier.</p>
<h2>7 Acknowledgements</h2>
<p>This research was supported by ONR grants N00014-18-1-2442, N00014-18-1-2840, N00014-19-1-2119 and N00014-23-1-2409, AFOSR grant FA9550-18-1-0067, DARPA SAIL-ON grant W911NF-19-2-0006, and a JP Morgan AI Faculty Research Grant to Kambhampati. Sreedharan was supported in part by NSF grant 2303019.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>References</h1>
<p>[1] Clever Hans. https://en.wikipedia.org/wiki/Clever_Hans.
[2] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.
[3] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
[5] Mary L Cummings. Automation bias in intelligent time critical decision support systems. In Decision making in aviation, pages 289-294. Routledge, 2017.
[6] Alfonso Gerevini and Ivan Serina. Lpg: A planner based on local search for planning graphs with action costs. In AIPS, volume 2, pages 281-290, 2002.
[7] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346-361, 2021.
[8] Malik Ghallab, Dana S. Nau, and Paolo Traverso. Automated Planning and Acting. Cambridge University Press, 2016.
[9] Kristian J. Hammond. CHEF: A model of case-based planning. In Tom Kehler, editor, Proceedings of the 5th National Conference on Artificial Intelligence. Philadelphia, PA, USA, August 11-15, 1986. Volume 1: Science, pages 267-271. Morgan Kaufmann, 1986.
[10] Sandra G Hart and Lowell E Staveland. Development of nasa-tlx (task load index): Results of empirical and theoretical research. Advances in psychology, 52:139-183, 1988.
[11] Richard Howey, Derek Long, and Maria Fox. VAL: Automatic plan validation, continuous effects and mixed initiative planning using PDDL. In 16th IEEE International Conference on Tools with Artificial Intelligence, pages 294-301. IEEE, 2004.
[12] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pages 9118-9147. PMLR, 2022.
[13] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022.
[14] IPC. International planning competition, 1998.
[15] Liwei Jiang, Jena D. Hwang, Chandrasekhar Bhagavatula, Ronan Le Bras, Maxwell Forbes, Jon Borchardt, Jenny Liang, Oren Etzioni, Maarten Sap, and Yejin Choi. Delphi: Towards Machine Ethics and Norms. ArXiv, abs/2110.07574, 2021.
[16] Subbarao Kambhampati. AI as (an Ersatz) Natural Science? https://cacm.acm.org/blogs/blog-cacm/261732-ai-as-an-ersatz-natural-science/fulltext, Jun 2022.
[17] Subbarao Kambhampati and James A. Hendler. A validation-structure-based theory of plan modification and reuse. Artif. Intell., 55(2):193-258, 1992.
[18] Subbarao Kambhampati, Karthik Valmeekam, Matthew Marquez, and Lin Guan. On the role of large language models in planning, July 2023. Tutorial presented at the International Conference on Automated Planning and Scheduling (ICAPS), Prague. https://yochan-lab.github. io/tutorial/ICAPS-2023/.
[19] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large Language Models are Zero-Shot Reasoners. arXiv preprint arXiv:2205.11916, 2022.
[20] Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and Ruslan Salakhutdinov. Towards understanding and mitigating social biases in language models. In International Conference on Machine Learning, pages 6565-6576. PMLR, 2021.</p>
<p>[21] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. Llm+ p: Empowering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477, 2023.
[22] Drew McDermott, Malik Ghallab, Adele E. Howe, Craig A. Knoblock, Ashwin Ram, Manuela M. Veloso, Daniel S. Weld, and David E. Wilkins. Pddl-the planning domain definition language. 1998.
[23] Tuan Anh Nguyen, Minh Do, Alfonso Emilio Gerevini, Ivan Serina, Biplav Srivastava, and Subbarao Kambhampati. Generating diverse plans to handle unknown and partially known user preferences. Artificial Intelligence, 190:1-31, 2012.
[24] OpenAI. Introducing chatgpt by openai, 2022.
[25] OpenAI. Gpt-4 technical report, 2023.
[26] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.
[27] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018.
[28] Shreyas Sundara Raman, Vanya Cohen, Eric Rosen, Ifrah Idrees, David Paulius, and Stefanie Tellex. Planning with large language models via corrective re-prompting. arXiv preprint arXiv:2211.09935, 2022.
[29] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8732-8740, 2020.
[30] Patrick Schramowski, Cigdem Turan, Nico Andersen, Constantin A Rothkopf, and Kristian Kersting. Large pre-trained language models contain human-like biases of what is right and wrong to do. Nature Machine Intelligence, 4(3):258-268, 2022.
[31] Big Science. BigScience Large Open-science Open-access Multilingual Language Model. https://huggingface.co/bigscience/bloom, 2022.
[32] Tom Silver, Varun Hariprasad, Reece S Shuttleworth, Nishanth Kumar, Tomás Lozano-Pérez, and Leslie Pack Kaelbling. PDDL planning with pretrained large language models. In NeurIPS 2022 Foundation Models for Decision Making Workshop, 2022.
[33] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.
[34] Kaya Stechly, Matthew Marquez, and Subbarao Kambhampati. Gpt-4 doesn’t know it’s wrong: An analysis of iterative prompting for reasoning problems. arXiv preprint arXiv:2310.12397, 2023.
[35] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018.
[36] Karthik Valmeekam, Matthew Marquez, and Subbarao Kambhampati. Can large language models really improve by self-critiquing their own plans? arXiv preprint arXiv:2310.08118, 2023.
[37] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.
[38] Yaqi Xie, Chen Yu, Tongyao Zhu, Jinbin Bai, Ze Gong, and Harold Soh. Translating natural language to planning goals with large-language models. arXiv preprint arXiv:2302.05128, 2023.
[39] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2023.
[40] Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, and Guy Van den Broeck. On the paradox of learning to reason from data. arXiv preprint arXiv:2205.11502, 2022.</p>
<h1>A Appendix</h1>
<h2>Contents</h2>
<p>1 Introduction ..... 1
2 Related Work ..... 3
3 Prompt Generation for Classical Planning Problems ..... 4
3.1 Background ..... 4
3.2 Prompt Generation ..... 4
4 Evaluating Planning Capabilities of LLMs in Autonomous Mode ..... 5
5 Evaluating LLMs as Idea Generators ..... 8
5.1 LLM Plans as Heuristics to Sound Planners ..... 8
5.2 Verifier-assisted repeated backprompting of LLMs ..... 9
5.3 LLMs as idea generators for humans-in-the-loop ..... 10
6 Conclusion and Future Work ..... 10
7 Acknowledgements ..... 10
A Appendix ..... 13
A. 1 Classical Planning Problem Formulation ..... 14
A. 2 Comparisons between the instances and plans generated by GPT-4 ..... 15
A. 3 Failure modes ..... 15
A.3.1 LLM failures ..... 15
A.3.2 Human failures ..... 16
A. 4 Blocksworld Prompts in Natural Language ..... 16
A.4.1 Domain description ..... 16
A.4.2 One-shot prompt with GPT-4 plan ..... 18
A.4.3 Zero-shot prompt with GPT-4 plan ..... 18
A.4.4 State-tracking prompt with GPT-4 plan ..... 19
A. 5 Mystery Blocksworld Prompts in Natural Language ..... 21
A.5.1 Domain description (Deceptive Disguising) ..... 21
A.5.2 One-shot prompt with GPT-4 plan (Deceptive Disguising) ..... 21
A.5.3 Zero-shot prompt with GPT-4 plan (Deceptive Disguising) ..... 22
A.5.4 State-tracking prompt with GPT-4 plan ..... 22
A.5.5 Domain description (Randomized Disguising) ..... 24
A.5.6 One-shot prompt with GPT-4 plan (Randomized Disguising) ..... 25
A.5.7 Zero-shot prompt with GPT-4 plan (Randomized Disguising) ..... 25
A. 6 Logistics Prompts in Natural Language ..... 26</p>
<p>A.6.1 Domain description ..... 26
A.6.2 One-shot prompt with GPT-4 plan ..... 26
A.6.3 Zero-shot prompt with GPT-4 plan ..... 27
A. 7 Blocksworld Prompts in PDDL ..... 28
A.7.1 Domain description ..... 28
A.7.2 One-shot prompt with GPT-4 plan ..... 29
A.7.3 Zero-shot prompt with GPT-4 plan ..... 30
A. 8 Mystery Blocksworld Prompts in PDDL ..... 30
A.8.1 Domain description (Deceptive Disguising) ..... 30
A.8.2 One-shot prompt with GPT-4 plan (Deceptive Disguising) ..... 31
A.8.3 Zero-shot prompt with GPT-4 plan (Deceptive Disguising) ..... 32
A. 9 Logistics Prompts in PDDL ..... 32
A.9.1 Domain description ..... 32
A.9.2 One-shot prompt with GPT-4 plan ..... 34
A.9.3 Zero-shot prompt with GPT-4 plan ..... 35
A. 10 Backprompting using VAL ..... 36
A.10.1 Blocksworld example with GPT-4 ..... 36
A.10.2 Mystery Blocksworld example with GPT-4 ..... 39
A.10.3 Logistics example with GPT-4 ..... 42
A. 11 Additional experiment details ..... 44
A.11.1 LLM experiment details and the compute cost ..... 44
A.11.2 LPG experiment details ..... 44
A. 12 User study details ..... 44
A.12.1 Instructions provided to the participants ..... 44
A.12.2 Interface of the user study ..... 45
A. 13 Broader Impact on using LLMs for planning ..... 45</p>
<h1>A. 1 Classical Planning Problem Formulation</h1>
<p>Classical Planning Problems can be mathematically represented by using the tuple $\mathcal{P}=\langle\mathcal{D}, \mathcal{I}, \mathcal{G}\rangle$. $\mathcal{D}$ is referred to as the problem domain, $I$ is the initial state and $G$ is the goal specification. The possible truth assignment over the predicates defines the state space for the planning problem. The domain is again defined by the tuple $\mathcal{D}=\langle\mathcal{F}, \mathcal{O}\rangle . \mathcal{F}$ corresponds to the set of fluents, i.e., the state variable used to define the state space and each fluent corresponds to a predicate with some arity, and $\mathcal{A}$ correspond to the set of actions that can be performed as part of the planning problem. Each action $a_{i}[\mathcal{V}] \in \mathcal{A}$ (where $a_{i}$ is the operator label and $\mathcal{V}$ is the variable used by the operator and each variable could be mapped to an object), can be further defined by two components, the precondition $\operatorname{prec}[\mathcal{V}]$ which describes when an action can be executed and the effects $e f f[\mathcal{V}]$ which defines what happens when an action is executed. We will assume that $\operatorname{prec}[\mathcal{V}]$ consists of a set of predicates defined over the variables $\mathcal{V}$. An action is assumed to be executable only if its preconditions are met, i.e., the predicates in the precondition hold in the given state. The effects $e f f[\mathcal{V}]$ is further defined by the tuple $\langle$ add $[\mathcal{V}], \operatorname{del}[\mathcal{V}]\rangle$, where add $[\mathcal{V}]$ or add effects is the set of predicates that will be set true by the action and $\operatorname{del}[\mathcal{V}]$ or delete effects is the set of predicates that will be set false by the action. An action is said to be grounded if we replace each of the variables with an object, else it is referred to as a lifted domain model (we use a similar convention to differentiate between lifted and grounded</p>
<p>predicates). Below is a snippet of an action from a popular benchmark problem called Blocksworld, in PDDL. The action corresponds to picking up a block in that domain.</p>
<div class="codehilite"><pre><span></span><code>(:action pickup
    :parameters (?ob)
    :precondition (and (clear ?ob) (on-table ?ob) (arm-empty))
    :effect (and (holding ?ob) (not (clear ?ob)) (not (on-table ?ob))
        (not (arm-empty))))
</code></pre></div>

<p>The parameter line provides the possible variables, in this case ?ob, which can stand for possible blocks. The precondition says that you can only pick up a block if it is clear (i.e. predicate (clear ?ob) is true for the block), the block is on the table and the arm is empty. The effects tell you that after you execute the action, the predicate (holding ?ob) becomes true and the block will no longer be considered clear, and on-table. Finally, the arm will no longer be considered empty. A solution to a planning problem is called a plan, and corresponds to a sequence of actions that once executed in the initial state would lead to a state where the goal specification is true. The actions may additionally be associated with cost, in these cases, one could also talk about optimal plans, i.e., a plan $\pi$ is called an optimal one if no plan exists that is less costly than $\pi$.
The above description presents one of the simpler classes of planning models and can be extended in multiple ways including allowing for object typing (including type hierarchy), more complex forms of preconditions and conditional effects, not to mention supporting richer classes of planning formalisms.</p>
<h1>A. 2 Comparisons between the instances and plans generated by GPT-4</h1>
<p>We have also examined the distribution of the instances (in Blocksworld and Logistics domains) that were used to test the LLMs over optimal plan lengths and the distribution of the number of correct plans by GPT-4 over the optimal plan lengths. From Figures 4, 5, 6 and 7, we can say that our traditional notions of planning complexity do not hold with LLMs. For an LLM, an easier instance from the perspective of planning complexity is the same as a harder one as it just predicts the next tokens based on their weights and the context.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: A detailed comparison of the blocksworld instances, against the instances where GPT-4 was able to generate a correct plan with a PDDL or a natural language style prompt which included one example.</p>
<h2>A. 3 Failure modes</h2>
<h2>A.3.1 LLM failures</h2>
<p>Figure 8 shows the assessment of GPT-4 plans with relaxations in the Mystery Blocksworld domain. Similar to that of Blocksworld, there is an increase in the number of goal-reaching plans, but even in the most lenient assessment mode (Delete+Precondition Relaxation), there are quite a number of non-goal-reaching plans. In the assessment modes with precondition relaxations, an inexecutable plan is when there is an action in the plan that does not contain the required number of parameters.
Figure 9 shows the assessment of GPT-4 plans with relaxations in the Logistics domain. Even in this domain, as we further relax the assessment, we again see an increase in the number of goal reaching plans, but even the most relaxed configuration still has non-goal reaching plans.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: A detailed comparison of the blocksworld instances, against the instances where GPT-4 was able to generate a correct plan with a PDDL or a natural language style prompt which included no examples.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: A detailed comparison of the logistics instances, against the instances where GPT-4 was able to generate a correct plan with a PDDL or a natural language style prompt which included one example.</p>
<h1>A.3.2 Human failures</h1>
<p>For the human baseline user study (Section 4), out of 50 participants, 11 participants failed to come up with a valid plan. All the 11 participants came up with inexecutable plans. In the additional user study (in Appendix 5.3), for the first group, where the LLM assistance was not provided, out of 49 participants, 10 participants failed to come up with a valid plan and all the 10 participants came up with inexecutable plans. For the second group, where LLM assistance was provided, out of 48 participants, 15 participants failed to come up with a vaild plan out of which, 14 participants came up with an inexecutable plan and 1 participant came up with a non-goal reaching plan.</p>
<h2>A. 4 Blocksworld Prompts in Natural Language</h2>
<h2>A.4.1 Domain description</h2>
<h2>Blocksworld Domain Description</h2>
<p>I am playing with a set of blocks where I need to arrange the blocks into stacks. Here are the $\sim$ actions I can do</p>
<p>Pick up a block
Unstack a block from on top of another block
Put down a block
Stack a block on top of another block
I have the following restrictions on my actions:
I can only pick up or unstack one block at a time.
I can only pick up or unstack a block if my hand is empty.
I can only pick up a block if the block is on the table and the block is clear. A block is clear
$\sim$ if the block has no other blocks on top of it and if the block is not picked up.
I can only unstack a block from on top of another block if the block I am unstacking was really on
$\sim$ top of the other block.
I can only unstack a block from on top of another block if the block I am unstacking is clear.
Once I pick up or unstack a block, I am holding the block.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: A detailed comparison of the logistics instances, against the instances where GPT-4 was able to generate a correct plan with a PDDL or a natural language style prompt which included no examples.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Assessment of GPT-4 plans with relaxations in Mystery Blocksworld (Deceptive Disguising) domain</p>
<p>I can only put down a block that I am holding.
I can only stack a block on top of another block if I am holding the block being stacked.
I can only stack a block on top of another block if the block onto which I am stacking the block
$\sim$ is clear.
Once I put down or stack a block, my hand becomes empty.
Once you stack a block on top of a second block, the second block is no longer clear.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Assessment of GPT-4 plans with relaxations in Logistics domain</p>
<h1>A.4.2 One-shot prompt with GPT-4 plan</h1>
<h2>One-shot prompt with GPT-4 plan</h2>
<p>[STATEMENT]
As initial conditions I have that, the red block is clear, the blue block is clear, the yellow $\sim$ block is clear, the hand is empty, the blue block is on top of the orange block, the red block $\sim$ is on the table, the orange block is on the table and the yellow block is on the table. My goal is to have that the orange block is on top of the blue block.</p>
<p>My plan is as follows:
[PLAN]
unstack the blue block from on top of the orange block
put down the blue block
pick up the orange block
stack the orange block on top of the blue block
[PLAN END]
[STATEMENT]
As initial conditions I have that, the red block is clear, the yellow block is clear, the hand is $\sim$ empty, the red block is on top of the blue block, the yellow block is on top of the orange $\sim$ block, the blue block is on the table and the orange block is on the table.
My goal is to have that the orange block is on top of the red block.
My plan is as follows:
[PLAN]
--------- GPT-4 response
unstack the yellow block from on top of the orange block
put down the yellow block
unstack the red block from on top of the blue block
put down the red block
pick up the orange block
stack the orange block on top of the red block
[PLAN END]
--------- Extracted plan
(unstack yellow orange)
(put-down yellow)
(unstack red blue)
(put-down red)
(pick-up orange)
(stack orange red)
-------- Ground truth plan
(unstack yellow orange)
(put-down yellow)
(pick-up orange)
(stack orange red)</p>
<h2>A.4.3 Zero-shot prompt with GPT-4 plan</h2>
<h2>Zero-shot prompt with GPT-4 plan</h2>
<p>[STATEMENT]
As initial conditions I have that, the red block is clear, the blue block is clear, the yellow $\sim$ block is clear, the hand is empty, the blue block is on top of the orange block, the red block $\sim$ is on the table, the orange block is on the table and the yellow block is on the table. My goal is to have that the orange block is on top of the blue block.</p>
<p>What is the plan to achieve my goal? Just give the actions in the plan.
--------- GPT-4 response
1. Unstack the blue block from on top of the orange block
2. Put down the blue block on the table
3. Pick up the orange block
4. Stack the orange block on top of the blue block
--------- Extracted plan
(unstack blue orange)
(put-down blue)
(pick-up orange)
(stack orange blue)
-------- Ground truth plan
(unstack blue orange)</p>
<h1>A.4.4 State-tracking prompt with GPT-4 plan</h1>
<h2>COT state-tracking prompt with GPT-4 plan</h2>
<p>The plan correctness is defined in terms of states resulting from executing the actions in the $\sim$ plan. An action is executable in a state when all its preconditions hold in that state. The $\sim$ state resulting from the action execution consists of everything in the previous state with $\sim$ the addition and deletion of add and delete effects of the action. Plan correctness is defined $\sim$ as follows: if the first action in the plan is applicable in the initial state, i.e., its $\sim$ preconditions are all present there; and the second action is applicable in the state $\sim$ resulting from applying the first action to the initial state, this process continues until $\sim$ the state resulting from the application of the last action in the last but one state gives $\sim$ rise to the final state where all the goals are satisfied.</p>
<h2>[STATEMENT]</h2>
<p>As initial conditions I have that, the red block is clear, the blue block is clear, the yellow block is clear, the hand is empty, the blue block is on top of the orange block, the red block is on the table, the orange block is on the table and the yellow block is on the table My goal is to have that the orange block is on top of the blue block.
My plan is as follows:</p>
<h2>[PLAN]</h2>
<ol>
<li>Current State: the red block is clear, the blue block is clear, the yellow block is clear, the hand is empty, the blue block is on top of the orange block, the red block is on the table, the orange block is on the table and the yellow block is on the table
Action: unstack the blue block from on top of the orange block
Reason: The above action is applicable in the current state because its preconditions; the blue block is clear, the hand is empty and the blue block is on top of the orange block, are $\sim$ satisfied in the current state.
Resulting State: the red block is clear, the orange block is clear, the yellow block is clear, $\sim$ the hand is currently holding blue block, the red block is on the table, the orange block $\sim$ is on the table and the yellow block is on the table</li>
<li>Current State: the red block is clear, the orange block is clear, the yellow block is clear, $\sim$ the hand is currently holding blue block, the red block is on the table, the orange block is on the table and the yellow block is on the table
Action: put down the blue block
Reason: The above action is applicable in the current state because its preconditions; the hand $\sim$ is currently holding blue block, are satisfied in the current state.
Resulting State: the red block is clear, the blue block is clear, the orange block is clear, $\sim$ the yellow block is clear, the hand is empty, the red block is on the table, the blue block $\sim$ is on the table, the orange block is on the table and the yellow block is on the table</li>
<li>Current State: the red block is clear, the blue block is clear, the orange block is clear, the $\sim$ yellow block is clear, the hand is empty, the red block is on the table, the blue block is on the table, the orange block is on the table and the yellow block is on the table
Action: pick up the orange block
Reason: The above action is applicable in the current state because its preconditions; the orange block is clear, the hand is empty and the orange block is on the table, are satisfied in the current state.
Resulting State: the red block is clear, the blue block is clear, the yellow block is clear, $\sim$ the hand is currently holding orange block, the red block is on the table, the blue block $\sim$ is on the table and the yellow block is on the table</li>
<li>Current State: the red block is clear, the blue block is clear, the yellow block is clear, the hand is currently holding orange block, the red block is on the table, the blue block is on the table and the yellow block is on the table
Action: stack the orange block on top of the blue block
Reason: The above action is applicable in the current state because its preconditions; the blue block is clear and the hand is currently holding orange block, are satisfied in the current state.
Resulting State: the red block is clear, the orange block is clear, the yellow block is clear, the hand is empty, the orange block is on top of the blue block, the red block is on the table, the blue block is on the table and the yellow block is on the table</li>
</ol>
<p>Final State: the red block is clear, the orange block is clear, the yellow block is clear, the hand is empty, the orange block is on top of the blue block, the red block is on the table, the blue block is on the table and the yellow block is on the table The goal conditions are satisfied in the final state. Hence, the above plan is valid.
[PLAN END]</p>
<p>[STATEMENT]
As initial conditions I have that, the red block is clear, the yellow block is clear, the hand is $\sim$ empty, the red block is on top of the blue block, the yellow block is on top of the orange block, the blue block is on the table and the orange block is on the table
My goal is to have that the orange block is on top of the red block.
My plan is as follows:
[PLAN]
-------- GPT-4 response
1. Current State: the red block is clear, the yellow block is clear, the hand is empty, the red block is on top of the blue block, the yellow block is on top of the orange block, the blue block is on the table, and the orange block is on the table
Action: unstack the red block from on top of the blue block
Reason: The above action is applicable in the current state because its preconditions; the red block is clear, the hand is empty, and the red block is on top of the blue block, are $\sim$ satisfied in the current state.
Resulting State: the blue block is clear, the yellow block is clear, the hand is currently holding red block, the yellow block is on top of the orange block, the blue block is on the table, and the orange block is on the table
2. Current State: the blue block is clear, the yellow block is clear, the hand is currently holding red block, the yellow block is on top of the orange block, the blue block is on the table, and the orange block is on the table
Action: put down the red block
Reason: The above action is applicable in the current state because the precondition; the hand is currently holding red block, is satisfied in the current state.
Resulting State: the red block is clear, the blue block is clear, the yellow block is clear, $\sim$ the hand is empty, the red block is on the table, the yellow block is on top of the orange block, the blue block is on the table, and the orange block is on the table
3. Current State: the red block is clear, the blue block is clear, the yellow block is clear, the hand is empty, the red block is on the table, the yellow block is on top of the orange block, the blue block is on the table, and the orange block is on the table
Action: unstack the yellow block from on top of the orange block
Reason: The above action is applicable in the current state because its preconditions; the yellow block is clear, the hand is empty, and the yellow block is on top of the orange block, are satisfied in the current state.
Resulting State: the red block is clear, the blue block is clear, the orange block is clear, the hand is currently holding yellow block, the red block is on the table, the blue block is on the table, and the orange block is on the table
4. Current State: the red block is clear, the blue block is clear, the orange block is clear, the hand is currently holding yellow block, the red block is on the table, the blue block is on the table, and the orange block is on the table
Action: put down the yellow block
Reason: The above action is applicable in the current state because the precondition; the hand is currently holding yellow block, is satisfied in the current state.
Resulting State: the red block is clear, the blue block is clear, the orange block is clear, the yellow block is clear, the hand is empty, the red block is on the table, the blue block is on the table, the orange block is on the table, and the yellow block is on the table
5. Current State: the red block is clear, the blue block is clear, the orange block is clear, the yellow block is clear, the hand is empty, the red block is on the table, the blue block is on the table, the orange block is on the table, and the yellow block is on the table
Action: pick up the orange block
Reason: The above action is applicable in the current state because its preconditions; the orange block is clear, the hand is empty, and the orange block is on the table, are satisfied in the current state.
Resulting State: the red block is clear, the blue block is clear, the hand is currently holding orange block, the yellow block is clear, the red block is on the table, the blue block is on the table, and the yellow block is on the table
6. Current State: the red block is clear, the blue block is clear, the hand is currently holding orange block, the yellow block is clear, the red block is on the table, the blue block is on the table, and the yellow block is on the table
Action: stack the orange block on top of the red block
Reason: The above action is applicable in the current state because its preconditions; the red block is clear, and the hand is currently holding orange block, are satisfied in the current state.
Resulting State: the blue block is clear, the orange block is clear, the hand is empty, the yellow block is clear, the orange block is on top of the red block, the blue block is on the table, the red block is on the table, and the yellow block is on the table
Final State: the blue block is clear, the orange block is clear, the hand is empty, the yellow block is clear, the orange block is on top of the red block, the blue block is on the table, and the yellow block is on the table</p>
<p>The goal conditions are satisfied in the final state. Hence, the above plan is valid. [PLAN END]
--------- Extracted plan ---------
(unstack red blue)
(put-down red)
(unstack yellow orange)
(put-down yellow)
(pick-up orange)
(stack orange red)
-------- Ground truth plan ---------
(unstack yellow orange)
(put-down yellow)
(pick-up orange)
(stack orange red)</p>
<h1>A. 5 Mystery Blocksworld Prompts in Natural Language</h1>
<h2>A.5.1 Domain description (Deceptive Disguising)</h2>
<h2>Mystery Blocksworld Domain Description (Deceptive Disguising)</h2>
<p>I am playing with a set of objects. Here are the actions I can do
Attack object
Feast object from another object
Succumb object
Overcome object from another object
I have the following restrictions on my actions:
To perform Attack action, the following facts need to be true: Province object, Planet object, $\sim$ Harmony.
Once Attack action is performed the following facts will be true: Pain object.
Once Attack action is performed the following facts will be false: Province object, Planet $\sim$ object, Harmony.
To perform Succumb action, the following facts need to be true: Pain object.
Once Succumb action is performed the following facts will be true: Province object, Planet $\sim$ object, Harmony.
Once Succumb action is performed the following facts will be false: Pain object.
To perform Overcome action, the following needs to be true: Province other object, Pain $\sim$ object.
Once Overcome action is performed the following will be true: Harmony, Province object, Object
$\sim$ Craves other object.
Once Overcome action is performed the following will be false: Province other object, Pain $\sim$ object.
To perform Feast action, the following needs to be true: Object Craves other object, Province $\sim$ object, Harmony.
Once Feast action is performed the following will be true: Pain object, Province other object.
Once Feast action is performed the following will be false:, Object Craves other object,
$\sim$ Province object, Harmony.</p>
<h2>A.5.2 One-shot prompt with GPT-4 plan (Deceptive Disguising)</h2>
<h2>One-shot prompt with GPT-4 plan (Deceptive Disguising)</h2>
<p>[STATEMENT]
As initial conditions I have that, province object a, province object b, province object d,
$\sim$ harmony, object b craves object c, planet object a, planet object c and planet object d.
My goal is to have that object c craves object b.
My plan is as follows:
[PLAN]
feast object b from object c
succumb object $b$
attack object c
overcome object c from object b
[PLAN END]
[STATEMENT]</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ We conducted independent-samples t-tests on plan accuracy, formulation time, and task-related cognitive load between the two groups, with a significance threshold set at $\alpha=0.05$. The t-tests yielded statistic values of $-1.21,0.09$, and -0.63 , with p -values of $0.22,0.92$, and 0.52 for accuracy, time, and cognitive load, respectively. These results imply that the null hypothesis cannot be rejected for any of the tests, indicating no statistically significant differences in accuracy, time taken, or cognitive load between the two groups.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
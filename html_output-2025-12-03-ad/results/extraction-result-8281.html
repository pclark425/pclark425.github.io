<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8281 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8281</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8281</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-271051080</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.06112v1.pdf" target="_blank">Enhancing Language Model Rationality with Bi-Directional Deliberation Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> This paper introduces BI-Directional DEliberation Reasoning (BIDDER), a novel reasoning approach to enhance the decision rationality of language models. Traditional reasoning methods typically rely on historical information and employ uni-directional (left-to-right) reasoning strategy. This lack of bi-directional deliberation reasoning results in limited awareness of potential future outcomes and insufficient integration of historical context, leading to suboptimal decisions. BIDDER addresses this gap by incorporating principles of rational decision-making, specifically managing uncertainty and predicting expected utility. Our approach involves three key processes: Inferring hidden states to represent uncertain information in the decision-making process from historical data; Using these hidden states to predict future potential states and potential outcomes; Integrating historical information (past contexts) and long-term outcomes (future contexts) to inform reasoning. By leveraging bi-directional reasoning, BIDDER ensures thorough exploration of both past and future contexts, leading to more informed and rational decisions. We tested BIDDER's effectiveness in two well-defined scenarios: Poker (Limit Texas Hold'em) and Negotiation. Our experiments demonstrate that BIDDER significantly improves the decision-making capabilities of LLMs and LLM agents.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8281.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8281.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BIDDER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BI-Directional DEliberation Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bi-directional deliberation reasoning framework that uses LLMs to infer hidden states from history, simulate future trajectories via opponent modeling, and aggregate expected utilities (Bellman-style) to make decisions that maximize long-term reward.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large Language Model (unspecified; prompted LLM used throughout experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An unspecified pretrained LLM used via prompting to perform (1) hidden-state inference, (2) opponent-model-driven tree exploration of future trajectories, and (3) reward prediction and aggregation; the method is implemented purely via prompting (no additional learning).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Bi-directional deliberation reasoning (left-to-right historical integration and right-to-left future expected-utility aggregation)', 'Hidden state inference (causal inference of opponent latent state from history)', 'Opponent modeling (simulate opponent actions under inferred hidden states)', 'Tree-based future trajectory exploration (bounded horizon T, termination on T or terminal/chance node)', 'Reward estimation and Bellman-style aggregation (discounted sum of rewards; choose action maximizing expected return)', 'Q-Learning inspiration (use of expected cumulative reward to inform right-to-left reasoning)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>BIDDER prompts the LLM to (1) infer latent opponent states from observed trajectories (P(s0|τ)), (2) enumerate/expand possible future trajectories from the current state under candidate actions using the inferred opponent model up to horizon T, (3) use the LLM to predict normalized payoffs for states/actions, and (4) aggregate returns along traces with a discount factor (Bellman-style) to compute G(τ) and G(a) = max_τ G(τ), then select the action with maximal expected long-term utility.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared BIDDER (bi-directional multi-process approach) against unidirectional/single-method LLM baselines (Direct, Chain-of-Thought, Reflexion) and against other search-based methods (MCTS, Tree-of-Thoughts) and machine-learning baselines in two domains (Limit Texas Hold'em and negotiation). No explicit ablation that incrementally removes individual BIDDER subcomponents is reported; comparisons are between full BIDDER and other single- or single-strategy baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Custom strategic decision-making testbeds: (1) Two-player Limit Texas Hold'em (100 generated games, with mirrored hands) and (2) Negotiation (division-of-items communication/negotiation game; 100 games with varied item values and turn limits).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Qualitative and tabulated quantitative improvements reported. Table 1 (Limit Texas Hold'em, average final chips over 100 games) shows BIDDER rows with substantially higher average earnings versus previous LLM reasoning methods and many ML baselines; example numeric entries from the BIDDER row include values such as 2.66, 2.47, 1.61 (these are the per-opponent average final-chip entries shown in Table 1). The paper states BIDDER 'significantly improves' average earnings compared to Direct/CoT/Reflexion and even outperforms several ML methods. In the negotiation benchmark (Table 3), BIDDER achieved the highest Score (payoff) while having moderate Agreement; other LLM methods (CoT, ToT) improved over Direct for Score and Agreement, and all methods achieved 100% Agreement when paired against MCTS in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>BIDDER produces more active, payoff-seeking behavior (e.g., higher frequency of Raises in poker, aligning with DeepCFR optimal behavior) versus Direct LLM behavior which tends to be passive (more Calls/Checks). The paper attributes BIDDER's gains to better uncertainty management via hidden-state inference and to considering long-term expected utility via future exploration; BIDDER better aligns LLM decision-making with equilibrium/optimal solutions. No internal ablation isolating the contribution of 'diversity' per se (e.g., measuring effect of adding/removing one reasoning module) is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Integrating bi-directional reasoning (inference of hidden states from history + simulated future exploration + expected-utility aggregation) makes LLM decisions more rational: BIDDER reduces uncertainty, considers long-term payoffs, and substantially improves payoffs and 'Rational Degree' compared to unidirectional LLM reasoning methods (Direct, CoT, Reflexion) and a range of ML baselines in poker and negotiation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Language Model Rationality with Bi-Directional Deliberation Reasoning', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8281.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8281.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Direct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct LLM Decision Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline where the LLM directly outputs an action in response to the game state prompt without chain-of-thought or explicit future simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large Language Model (unspecified; used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompted LLM that generates a single final action given the current state (no explicit internal deliberation prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Single-step direct response (no explicit chain-of-thought)', 'Implicit pattern-matching from prompt/context']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>The model is prompted to return an action directly (format: {'action': ...}) without asking for intermediate thoughts or explicit planning/trajectory simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Used as a baseline in both Limit Texas Hold'em and the negotiation game and compared directly to CoT, Reflexion, ToT, MCTS, BIDDER and ML methods.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Limit Texas Hold'em and Negotiation (same testbeds as BIDDER comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported in Table 1 and Table 3 as baseline LLM performance: Direct obtains modest or near-zero average chips in poker and lower Score/Agreement in negotiation relative to enhanced methods; specific per-opponent numbers are listed in Table 1 (Direct row). The paper describes Direct as more passive in poker (higher Call/Check rates) and yielding lower returns than BIDDER.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Direct prompting results in passive decision-making in poker and lower payoffs in negotiation; lacks explicit future reasoning so it underperforms methods that plan or deliberate.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Direct LLM responses are insufficient for rational decision-making in strategic multi-step tasks; methods that introduce explicit deliberation or future reasoning (e.g., BIDDER, CoT, ToT) outperform Direct.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Language Model Rationality with Bi-Directional Deliberation Reasoning', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8281.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8281.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot step-by-step reasoning prompting technique where the LLM is asked to produce intermediate thoughts and then an action.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large Language Model (unspecified; CoT prompting applied)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An LLM prompted to produce intermediate reasoning steps (My thought is {Your Thought}) followed by action; applied zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought stepwise reasoning']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>CoT prompting elicits an explicit sequence of intermediate reasoning steps from the LLM before the final action; implemented in the experiments via prompt templates that request 'My thought is {Your Thought}, and my action is {...}'.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared as an LLM reasoning baseline against Direct, Reflexion, BIDDER, and ML baselines across poker and negotiation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Limit Texas Hold'em and Negotiation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>CoT improves over Direct in several metrics (reported in Tables 1 and 3); in negotiation CoT shows higher Score and Agreement than Direct, and in poker it yields modest gains but is outperformed by BIDDER. Exact numeric entries for CoT are reported in Table 1 / Table 3 in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>CoT provides interpretability and some performance improvement, but because it remains unidirectional (relying on historical context and stepwise reasoning) it lacks explicit future trajectory simulation and opponent hidden-state inference present in BIDDER, limiting its performance in multi-agent strategic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>While CoT improves LLM reasoning by decomposing problems, it remains unidirectional and therefore less effective than BIDDER's bi-directional exploration in strategic environments requiring opponent modeling and long-term payoff estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Language Model Rationality with Bi-Directional Deliberation Reasoning', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8281.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8281.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion (language agents with verbal reinforcement learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative refinement approach where the LLM is prompted to reflect on and refine its previous thought and action outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large Language Model (unspecified; Reflexion prompting applied)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM is prompted to check and refine its thought and action outputs (Reflexion prompt asks to 'carefully check the thought and the action you just output, and then refine your answer').</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Iterative self-refinement (Reflexion)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>After producing an initial thought and action, the LLM is prompted to reassess and produce a revised thought and action; used as an alternative to CoT and Direct baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Included as an LLM baseline in Limit Texas Hold'em comparisons (Table 1) and contrasted with BIDDER and other methods.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Limit Texas Hold'em</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reflexion shows some improvement over Direct in poker (reported in Table 1) but is outperformed by BIDDER; exact per-opponent values for Reflexion are provided in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Reflexion's iterative refining helps correct some mistakes but does not substitute for explicit opponent modeling and forward-looking utility aggregation; thus it yields limited gains in multi-agent strategic tasks compared to BIDDER.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Iterative self-refinement improves response quality but lacks the bi-directional, opponent-model-based future simulation that BIDDER uses to improve decision rationality in strategic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Language Model Rationality with Bi-Directional Deliberation Reasoning', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8281.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8281.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree of Thoughts (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A search-based reasoning method that builds a tree of candidate reasoning states (thoughts) using LLM-generated candidates and evaluates nodes to guide search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of thoughts: Deliberate problem solving with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large Language Model (unspecified; ToT prompting applied)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM generates candidate partial solutions (thoughts) that are expanded into a search tree; nodes are evaluated to steer search; implemented as a baseline in negotiation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Tree-based search of candidate thoughts (ToT)', 'Self-exploration of future states via branching']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>ToT constructs a search tree by having the LLM propose candidate 'thoughts' at each node and evaluating branches to decide which paths to expand; operates primarily as a unidirectional search through hypothetical future reasoning states.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Used as a baseline in the negotiation benchmark and compared to BIDDER and MCTS; the paper notes that ToT is self-exploratory while BIDDER additionally incorporates opponent hidden-state inference.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Negotiation (division-of-items game)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>ToT and other LLM-based reasoning methods outperform Direct in negotiation on Score and Agreement metrics (Table 3), but BIDDER attains the highest Score. Exact numeric results for ToT are reported in Table 3 per opponent pairing.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>ToT's tree search improves planning and reasoning in negotiation relative to Direct, but because it does not explicitly infer opponent hidden states, it can underperform BIDDER in tasks where opponent modeling and payoff estimation under uncertainty are crucial.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>ToT enhances LLM deliberation via tree search, but BIDDER's combination of hidden-state inference + future exploration + expected-utility aggregation gives it an advantage in multi-agent strategic settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Language Model Rationality with Bi-Directional Deliberation Reasoning', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8281.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8281.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MCTS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Monte Carlo Tree Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A heuristic search algorithm that samples and evaluates future trajectories via randomized rollouts to guide action selection; used as a baseline in negotiation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Monte-carlo tree search: A new framework for game ai</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MCTS (classical algorithm; used as baseline interacting with LLM agents)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Search algorithm that repeatedly samples (rolls out) trajectories, uses results to update tree statistics, and selects actions with the best estimated value; contrasted with LLM-based search methods.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Probabilistic Monte-Carlo rollouts and tree search']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>MCTS explores the action space by stochastic rollouts to estimate expected values of actions and guide selection; in the paper it's a baseline that tends to agree frequently in negotiation but achieves lower Score.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Used as a non-LLM baseline in negotiation comparisons; LLM methods (Direct, CoT, ToT, BIDDER) are evaluated against MCTS as opponents.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Negotiation (division-of-items game)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Paper reports that all LLM methods achieved 100% Agreement when paired against MCTS in some settings because MCTS tends to agree readily, but MCTS achieves lower Score (payoff) compared to BIDDER and other LLM reasoning methods (Table 3). Specific numeric cells for MCTS and pairings are present in Table 3.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>MCTS's high agreement tendency makes it an easy negotiation partner but not maximally payoff-optimal; LLM-based methods (especially BIDDER) achieve higher Scores by seeking better payoffs at the expense of somewhat lower Agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>MCTS provides a useful baseline showing that simple optimistic/agreement-seeking search strategies are suboptimal for payoff maximization; LLM methods that reason about opponent preferences and long-term payoffs (BIDDER) can outperform MCTS on score.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Language Model Rationality with Bi-Directional Deliberation Reasoning', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Monte-carlo tree search: A new framework for game ai <em>(Rating: 2)</em></li>
                <li>Deep counterfactual regret minimization <em>(Rating: 2)</em></li>
                <li>Q-learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8281",
    "paper_id": "paper-271051080",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "BIDDER",
            "name_full": "BI-Directional DEliberation Reasoning",
            "brief_description": "A bi-directional deliberation reasoning framework that uses LLMs to infer hidden states from history, simulate future trajectories via opponent modeling, and aggregate expected utilities (Bellman-style) to make decisions that maximize long-term reward.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Large Language Model (unspecified; prompted LLM used throughout experiments)",
            "model_description": "An unspecified pretrained LLM used via prompting to perform (1) hidden-state inference, (2) opponent-model-driven tree exploration of future trajectories, and (3) reward prediction and aggregation; the method is implemented purely via prompting (no additional learning).",
            "reasoning_methods": [
                "Bi-directional deliberation reasoning (left-to-right historical integration and right-to-left future expected-utility aggregation)",
                "Hidden state inference (causal inference of opponent latent state from history)",
                "Opponent modeling (simulate opponent actions under inferred hidden states)",
                "Tree-based future trajectory exploration (bounded horizon T, termination on T or terminal/chance node)",
                "Reward estimation and Bellman-style aggregation (discounted sum of rewards; choose action maximizing expected return)",
                "Q-Learning inspiration (use of expected cumulative reward to inform right-to-left reasoning)"
            ],
            "reasoning_methods_description": "BIDDER prompts the LLM to (1) infer latent opponent states from observed trajectories (P(s0|τ)), (2) enumerate/expand possible future trajectories from the current state under candidate actions using the inferred opponent model up to horizon T, (3) use the LLM to predict normalized payoffs for states/actions, and (4) aggregate returns along traces with a discount factor (Bellman-style) to compute G(τ) and G(a) = max_τ G(τ), then select the action with maximal expected long-term utility.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Compared BIDDER (bi-directional multi-process approach) against unidirectional/single-method LLM baselines (Direct, Chain-of-Thought, Reflexion) and against other search-based methods (MCTS, Tree-of-Thoughts) and machine-learning baselines in two domains (Limit Texas Hold'em and negotiation). No explicit ablation that incrementally removes individual BIDDER subcomponents is reported; comparisons are between full BIDDER and other single- or single-strategy baselines.",
            "task_or_benchmark": "Custom strategic decision-making testbeds: (1) Two-player Limit Texas Hold'em (100 generated games, with mirrored hands) and (2) Negotiation (division-of-items communication/negotiation game; 100 games with varied item values and turn limits).",
            "performance_results": "Qualitative and tabulated quantitative improvements reported. Table 1 (Limit Texas Hold'em, average final chips over 100 games) shows BIDDER rows with substantially higher average earnings versus previous LLM reasoning methods and many ML baselines; example numeric entries from the BIDDER row include values such as 2.66, 2.47, 1.61 (these are the per-opponent average final-chip entries shown in Table 1). The paper states BIDDER 'significantly improves' average earnings compared to Direct/CoT/Reflexion and even outperforms several ML methods. In the negotiation benchmark (Table 3), BIDDER achieved the highest Score (payoff) while having moderate Agreement; other LLM methods (CoT, ToT) improved over Direct for Score and Agreement, and all methods achieved 100% Agreement when paired against MCTS in some settings.",
            "qualitative_findings": "BIDDER produces more active, payoff-seeking behavior (e.g., higher frequency of Raises in poker, aligning with DeepCFR optimal behavior) versus Direct LLM behavior which tends to be passive (more Calls/Checks). The paper attributes BIDDER's gains to better uncertainty management via hidden-state inference and to considering long-term expected utility via future exploration; BIDDER better aligns LLM decision-making with equilibrium/optimal solutions. No internal ablation isolating the contribution of 'diversity' per se (e.g., measuring effect of adding/removing one reasoning module) is provided.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Integrating bi-directional reasoning (inference of hidden states from history + simulated future exploration + expected-utility aggregation) makes LLM decisions more rational: BIDDER reduces uncertainty, considers long-term payoffs, and substantially improves payoffs and 'Rational Degree' compared to unidirectional LLM reasoning methods (Direct, CoT, Reflexion) and a range of ML baselines in poker and negotiation.",
            "uuid": "e8281.0",
            "source_info": {
                "paper_title": "Enhancing Language Model Rationality with Bi-Directional Deliberation Reasoning",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Direct",
            "name_full": "Direct LLM Decision Generation",
            "brief_description": "Baseline where the LLM directly outputs an action in response to the game state prompt without chain-of-thought or explicit future simulation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Large Language Model (unspecified; used as baseline)",
            "model_description": "Prompted LLM that generates a single final action given the current state (no explicit internal deliberation prompts).",
            "reasoning_methods": [
                "Single-step direct response (no explicit chain-of-thought)",
                "Implicit pattern-matching from prompt/context"
            ],
            "reasoning_methods_description": "The model is prompted to return an action directly (format: {'action': ...}) without asking for intermediate thoughts or explicit planning/trajectory simulation.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Used as a baseline in both Limit Texas Hold'em and the negotiation game and compared directly to CoT, Reflexion, ToT, MCTS, BIDDER and ML methods.",
            "task_or_benchmark": "Limit Texas Hold'em and Negotiation (same testbeds as BIDDER comparisons).",
            "performance_results": "Reported in Table 1 and Table 3 as baseline LLM performance: Direct obtains modest or near-zero average chips in poker and lower Score/Agreement in negotiation relative to enhanced methods; specific per-opponent numbers are listed in Table 1 (Direct row). The paper describes Direct as more passive in poker (higher Call/Check rates) and yielding lower returns than BIDDER.",
            "qualitative_findings": "Direct prompting results in passive decision-making in poker and lower payoffs in negotiation; lacks explicit future reasoning so it underperforms methods that plan or deliberate.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Direct LLM responses are insufficient for rational decision-making in strategic multi-step tasks; methods that introduce explicit deliberation or future reasoning (e.g., BIDDER, CoT, ToT) outperform Direct.",
            "uuid": "e8281.1",
            "source_info": {
                "paper_title": "Enhancing Language Model Rationality with Bi-Directional Deliberation Reasoning",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought (CoT) prompting",
            "brief_description": "A zero-shot step-by-step reasoning prompting technique where the LLM is asked to produce intermediate thoughts and then an action.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "Large Language Model (unspecified; CoT prompting applied)",
            "model_description": "An LLM prompted to produce intermediate reasoning steps (My thought is {Your Thought}) followed by action; applied zero-shot.",
            "reasoning_methods": [
                "Chain-of-Thought stepwise reasoning"
            ],
            "reasoning_methods_description": "CoT prompting elicits an explicit sequence of intermediate reasoning steps from the LLM before the final action; implemented in the experiments via prompt templates that request 'My thought is {Your Thought}, and my action is {...}'.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Compared as an LLM reasoning baseline against Direct, Reflexion, BIDDER, and ML baselines across poker and negotiation tasks.",
            "task_or_benchmark": "Limit Texas Hold'em and Negotiation",
            "performance_results": "CoT improves over Direct in several metrics (reported in Tables 1 and 3); in negotiation CoT shows higher Score and Agreement than Direct, and in poker it yields modest gains but is outperformed by BIDDER. Exact numeric entries for CoT are reported in Table 1 / Table 3 in the paper.",
            "qualitative_findings": "CoT provides interpretability and some performance improvement, but because it remains unidirectional (relying on historical context and stepwise reasoning) it lacks explicit future trajectory simulation and opponent hidden-state inference present in BIDDER, limiting its performance in multi-agent strategic tasks.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "While CoT improves LLM reasoning by decomposing problems, it remains unidirectional and therefore less effective than BIDDER's bi-directional exploration in strategic environments requiring opponent modeling and long-term payoff estimation.",
            "uuid": "e8281.2",
            "source_info": {
                "paper_title": "Enhancing Language Model Rationality with Bi-Directional Deliberation Reasoning",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion (language agents with verbal reinforcement learning)",
            "brief_description": "An iterative refinement approach where the LLM is prompted to reflect on and refine its previous thought and action outputs.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "use",
            "model_name": "Large Language Model (unspecified; Reflexion prompting applied)",
            "model_description": "LLM is prompted to check and refine its thought and action outputs (Reflexion prompt asks to 'carefully check the thought and the action you just output, and then refine your answer').",
            "reasoning_methods": [
                "Iterative self-refinement (Reflexion)"
            ],
            "reasoning_methods_description": "After producing an initial thought and action, the LLM is prompted to reassess and produce a revised thought and action; used as an alternative to CoT and Direct baselines.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Included as an LLM baseline in Limit Texas Hold'em comparisons (Table 1) and contrasted with BIDDER and other methods.",
            "task_or_benchmark": "Limit Texas Hold'em",
            "performance_results": "Reflexion shows some improvement over Direct in poker (reported in Table 1) but is outperformed by BIDDER; exact per-opponent values for Reflexion are provided in Table 1.",
            "qualitative_findings": "Reflexion's iterative refining helps correct some mistakes but does not substitute for explicit opponent modeling and forward-looking utility aggregation; thus it yields limited gains in multi-agent strategic tasks compared to BIDDER.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Iterative self-refinement improves response quality but lacks the bi-directional, opponent-model-based future simulation that BIDDER uses to improve decision rationality in strategic tasks.",
            "uuid": "e8281.3",
            "source_info": {
                "paper_title": "Enhancing Language Model Rationality with Bi-Directional Deliberation Reasoning",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "ToT",
            "name_full": "Tree of Thoughts (ToT)",
            "brief_description": "A search-based reasoning method that builds a tree of candidate reasoning states (thoughts) using LLM-generated candidates and evaluates nodes to guide search.",
            "citation_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "mention_or_use": "use",
            "model_name": "Large Language Model (unspecified; ToT prompting applied)",
            "model_description": "LLM generates candidate partial solutions (thoughts) that are expanded into a search tree; nodes are evaluated to steer search; implemented as a baseline in negotiation experiments.",
            "reasoning_methods": [
                "Tree-based search of candidate thoughts (ToT)",
                "Self-exploration of future states via branching"
            ],
            "reasoning_methods_description": "ToT constructs a search tree by having the LLM propose candidate 'thoughts' at each node and evaluating branches to decide which paths to expand; operates primarily as a unidirectional search through hypothetical future reasoning states.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Used as a baseline in the negotiation benchmark and compared to BIDDER and MCTS; the paper notes that ToT is self-exploratory while BIDDER additionally incorporates opponent hidden-state inference.",
            "task_or_benchmark": "Negotiation (division-of-items game)",
            "performance_results": "ToT and other LLM-based reasoning methods outperform Direct in negotiation on Score and Agreement metrics (Table 3), but BIDDER attains the highest Score. Exact numeric results for ToT are reported in Table 3 per opponent pairing.",
            "qualitative_findings": "ToT's tree search improves planning and reasoning in negotiation relative to Direct, but because it does not explicitly infer opponent hidden states, it can underperform BIDDER in tasks where opponent modeling and payoff estimation under uncertainty are crucial.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "ToT enhances LLM deliberation via tree search, but BIDDER's combination of hidden-state inference + future exploration + expected-utility aggregation gives it an advantage in multi-agent strategic settings.",
            "uuid": "e8281.4",
            "source_info": {
                "paper_title": "Enhancing Language Model Rationality with Bi-Directional Deliberation Reasoning",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "MCTS",
            "name_full": "Monte Carlo Tree Search",
            "brief_description": "A heuristic search algorithm that samples and evaluates future trajectories via randomized rollouts to guide action selection; used as a baseline in negotiation experiments.",
            "citation_title": "Monte-carlo tree search: A new framework for game ai",
            "mention_or_use": "use",
            "model_name": "MCTS (classical algorithm; used as baseline interacting with LLM agents)",
            "model_description": "Search algorithm that repeatedly samples (rolls out) trajectories, uses results to update tree statistics, and selects actions with the best estimated value; contrasted with LLM-based search methods.",
            "reasoning_methods": [
                "Probabilistic Monte-Carlo rollouts and tree search"
            ],
            "reasoning_methods_description": "MCTS explores the action space by stochastic rollouts to estimate expected values of actions and guide selection; in the paper it's a baseline that tends to agree frequently in negotiation but achieves lower Score.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Used as a non-LLM baseline in negotiation comparisons; LLM methods (Direct, CoT, ToT, BIDDER) are evaluated against MCTS as opponents.",
            "task_or_benchmark": "Negotiation (division-of-items game)",
            "performance_results": "Paper reports that all LLM methods achieved 100% Agreement when paired against MCTS in some settings because MCTS tends to agree readily, but MCTS achieves lower Score (payoff) compared to BIDDER and other LLM reasoning methods (Table 3). Specific numeric cells for MCTS and pairings are present in Table 3.",
            "qualitative_findings": "MCTS's high agreement tendency makes it an easy negotiation partner but not maximally payoff-optimal; LLM-based methods (especially BIDDER) achieve higher Scores by seeking better payoffs at the expense of somewhat lower Agreement.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "MCTS provides a useful baseline showing that simple optimistic/agreement-seeking search strategies are suboptimal for payoff maximization; LLM methods that reason about opponent preferences and long-term payoffs (BIDDER) can outperform MCTS on score.",
            "uuid": "e8281.5",
            "source_info": {
                "paper_title": "Enhancing Language Model Rationality with Bi-Directional Deliberation Reasoning",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Monte-carlo tree search: A new framework for game ai",
            "rating": 2,
            "sanitized_title": "montecarlo_tree_search_a_new_framework_for_game_ai"
        },
        {
            "paper_title": "Deep counterfactual regret minimization",
            "rating": 2,
            "sanitized_title": "deep_counterfactual_regret_minimization"
        },
        {
            "paper_title": "Q-learning",
            "rating": 1,
            "sanitized_title": "qlearning"
        }
    ],
    "cost": 0.01399025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Enhancing Language Model Rationality with Bi-Directional Deliberation Reasoning
8 Jul 2024</p>
<p>Yadong Zhang 
East China Normal University</p>
<p>Shaoguang Mao 
Microsoft Research https://aka.ms/GeneralAI</p>
<p>Wenshan Wu 
Microsoft Research https://aka.ms/GeneralAI</p>
<p>Yan Xia 
Microsoft Research https://aka.ms/GeneralAI</p>
<p>Tao Ge 
Microsoft Research https://aka.ms/GeneralAI</p>
<p>Man Lan 
East China Normal University</p>
<p>Furu Wei 
Microsoft Research https://aka.ms/GeneralAI</p>
<p>Microsoft Research Asia</p>
<p>Enhancing Language Model Rationality with Bi-Directional Deliberation Reasoning
8 Jul 2024507F5DAB51C4DE6054A3D19A79AAE9BFarXiv:2407.06112v1[cs.CL]
This paper introduces BI-Directional DEliberation Reasoning (BIDDER), a novel reasoning approach to enhance the decision rationality of language models.Traditional reasoning methods typically rely on historical information and employ uni-directional (left-to-right) reasoning strategy.This lack of bi-directional deliberation reasoning results in limited awareness of potential future outcomes and insufficient integration of historical context, leading to suboptimal decisions.BID-DER addresses this gap by incorporating principles of rational decision-making, specifically managing uncertainty and predicting expected utility.Our approach involves three key processes: Inferring hidden states to represent uncertain information in the decision-making process from historical data; Using these hidden states to predict future potential states and potential outcomes; Integrating historical information (past contexts) and long-term outcomes (future contexts) to inform reasoning.By leveraging bi-directional reasoning, BIDDER ensures thorough exploration of both past and future contexts, leading to more informed and rational decisions.We tested BIDDER's effectiveness in two well-defined scenarios: Poker (Limit Texas Hold'em) and Negotiation.Our experiments demonstrate that BID-DER significantly improves the decision-making capabilities of LLMs and LLM agents.</p>
<p>Introduction</p>
<p>The rapid advancement of large language models (LLMs) has significantly enhanced natural language understanding and generation [29].However, as the application scope of LLMs expands, they often struggle to make rational and optimal decisions in real-world scenarios [8,10,23], particularly in complex environments that require careful consideration of both historical data and potential future outcomes.</p>
<p>Existing methods predominantly enhance reasoning performance by navigating through the problem space and selecting optimal nodes (states) based on the progress toward solving the problem [43,49].These unidirectional (left-to-right) reasoning approaches limit LLMs' ability to effectively leverage future information, resulting in suboptimal decisions due to insufficient integration of historical context with potential future outcomes [18].To address these shortcomings, we propose BI-Directional DEliberation Reasoning (BIDDER), a novel approach designed to enhance decision rationality.BIDDER leverages the strengths of classical bi-directional modeling, as exemplified by Bidirectional Encoder Representations from Transformers (BERT [9]), and integrates these with decision theory principles to overcome the limitations of traditional reasoning methods in LLMs.</p>
<p>The key innovation of BIDDER lies in its bi-directional reasoning capability, which ensures a thorough exploration of both past and future contexts.As illustrated in Figure 1, unidirectional reasoning utilizes historical context to make left-to-right decisions.In contrast, bi-directional reasoning explores potential future states and predicts the expected utility for future moves, using both historical contexts and future exploration contexts for more informative reasoning.Reasoning: Unidirectional reasoning utilizes historical context to make left-to-right decisions.In contrast, bidirectional reasoning explores potential future states and aggregate the expected utility for future moves.It then uses both historical contexts and future exploration contexts to conduct bi-directional reasoning.</p>
<p>BIDDER's approach involves three main processes: inferring hidden states to represent uncertain information from historical data, using these hidden states to explore future potential states and outcomes, and integrating both historical contexts and future explorations to inform reasoning.By leveraging these bidirectional deliberation processes, BIDDER enables LLMs to make more informed and rational decisions.Although future information cannot be directly obtained, BIDDER refers to the Q-Learning [42] algorithm in Reinforcement Learning, employing LLMs to explore various future time horizons and use this information to calculate the expected utility of potential actions.</p>
<p>To demonstrate BIDDER's effectiveness, we conducted experiments in two well-defined strategic decision-making scenarios: Poker (Limit Texas Hold'em) [50] and Negotiation [21,6].These scenarios provide a rigorous testbed for evaluating the decision-making capabilities of LLMs due to their clear comparison of final payoffs and well-established theoretical optimal solutions.Our experimental results show that BIDDER significantly enhances the decision-making capabilities of LLMs, aligning their behavior more closely with optimal solutions.By incorporating bi-directional deliberation reasoning, BIDDER minimizes uncertainty, considers long-term payoffs, and ensures that LLMs can effectively integrate both historical context and future explorations into their decision-making processes.</p>
<p>The contributions of this work are as follows:</p>
<p>• We introduce the first bi-directional reasoning approach with LLMs to enhance their decision rationality by considering both historical context and future potential outcomes.Unlike traditional methods that focus solely on past information, this bi-directional reasoning helps manage uncertainty and predict expected utility more effectively.</p>
<p>• We incorporate principles from the Q-Learning algorithm, leveraging LLMs to explore future trajectories and aggregate rewards, thereby constructing reasoning contexts that include potential future outcomes.</p>
<p>• We validate BIDDER's effectiveness through rigorous experiments in strategic decisionmaking scenarios, demonstrating its superiority over traditional reasoning methods.</p>
<p>Method</p>
<p>BI-Directional DEliberation Reasoning (BIDDER) is designed to enhance the decision rationality of LLMs.It is based on the concepts of uncertainty and expected utility in decision theory [1] and leverages bi-directional reasoning to incorporate both historical context and potential future outcomes into the decision-making process.Since future contexts are not available at the decision point, we refer to the Q-Learning algorithm in Reinforcement Learning [42], calculating the expected total reward for taking different actions in a given state as the context for right-to-left reasoning.</p>
<p>Figure 2 illustrates the entire process of this method.Specifically, BIDDER consists of three modules: 1) inferring hidden states behind decision-making from available historical data, 2) utilizing the inferred hidden states to explore future trajectories, and 3) aggregating the utilities along each explored trajectories and making decisions to maximize the expected long-term utility.In order to simplify the complexity of the description, we will limit the subsequent discussion to a two-player game, referring to the current player as "player" and the other participant as "opponent."</p>
<p>We standardize the notation here:</p>
<p>• The player's initial hidden state s 0 , and the opponent's hidden state s 0 .</p>
<p>• The player's public trajectory at time t: τ (t) = {a 0 , s 1 , . . ., s t } (as the hidden state, s 0 is unobservable by other players) , and the opponent's trajectory τ (t).</p>
<p>Infer Hidden State From History</p>
<p>Inferring hidden states is essential for understanding environmental changes influenced by latent variables, such as an opponent's status or strategy in a game.Predicting these variables allows for a more accurate reconstruction of future behaviors, enhancing the rationality of decision-making.</p>
<p>Previous research [15,45] has shown that LLMs can recognize contextual causal relationships and correct causal arguments with high probability.Therefore, we use a causal reasoning approach to infer environmental information, with historical data τ (t) as the effect and environmental uncertainties s 0 as the causes.</p>
<p>Formally, we consider the relationship P (τ |s 0 ), where P represents the probability distribution.Based on the hidden states inferred in Section 2.1 and opponent's behavioral records, we simulate their likely actions under various states using prompts.</p>
<p>At the current decision point t, we explore all possible actions with the following termination conditions:</p>
<p>• Reaching the preset exploration horizon T .</p>
<p>• Reaching a terminal state or a chance node2 .</p>
<p>For the current reasoning point, we generate a set of exploration paths based on opponent modeling:
T (t) = {τ | τ = (s t , a ′ t , . . . , a ′ t+T −1 , s t+T )}(1)s t+1 = s t × a ′ t × a ′ t(2)
Here, T (t) represents the set of all possible trajectories starting from the current state s t .Each trajectory τ consists of a sequence of states and actions, starting from the current state s t , and includes the sequence of actions a ′ t , . . ., a ′ t+T −1 and the explored states s t+1 , . . ., s t+T up to the exploration horizon T .For the detailed algorithm of future trajectories exploration, please refer to Appendix C. Estimate the possible payoff r for each action, and r must be normalized to between 0 and 1.</p>
<p>Aggregate Rewards for</p>
<p>We use LLMs to estimate the rewards for the potential payoff of explored paths.This extends the paths in Equation 1 to include estimated rewards.We utilize the Bellman equation [2,35] to aggregate the return of each trace τ :
G(τ ) = T k=0 β k r(s t+k )(3)
Here, G(τ ) is the total reward accumulated along the path τ , r(s t+k ) is the reward at horizon k, and β is the discount factor, determining the importance of future rewards.</p>
<p>The return for each current action a is calculated as follows:
G(a) = max τ ∈T (t,a) G(τ )(4)T (t, a) = {τ | τ = (s t , a ′ t = a, . . . , a ′ t+T −1 , s t+T )}(5)
Here, G(a) is the maximum cumulative return starting from the current time step t and taking action a.The set T (t, a) represents all possible trajectories starting from the current state s t and action a.</p>
<p>Finally, the LLM makes decisions based on the past available information, the inferred hidden states and the future long-term returns of each action at the current point in time: This bidirectional deliberation reasoning framework enhances decision rationality by incorporating both historical contexts and potential future outcomes into the decision-making process to minimize uncertainty.</p>
<p>3 Experiments</p>
<p>Limit Texas Hold'em</p>
<p>We utilized two-player Limit Texas Hold'em as our testbed.Limit Texas Hold'em is a popular betting game which, unlike no-limit Texas Hold'em, restricts the number of raises and fixes the raise amount.The detiled game rule of Limit Texas Hold'em can be found in the Appendix A.1.Among the two game participants, we refer to the focal participant as the Player, while the other participant is designated as the Opponent.</p>
<p>We randomly generated 100 game sessions.To enhance the diversity of the hands, we increased the frequency of strong hands appearing in these 100 games.The specific distribution of hand types is detailed in the AppendixA.1.For fairness, each game has a mirrored game, where the Player and the Opponent switch their hands.</p>
<p>Baselines</p>
<p>We adapt a variety of approaches to comprehensively compare the performance of methods from different sources in Limit Texas Hold'em environment.These methods include:</p>
<p>Rule-Based Approaches:</p>
<p>Random: Randomly selects an action from the set of feasible actions.</p>
<p>Rule: Predefined procedures that select actions based on hand strength, such as raising when holding a pair.</p>
<p>Counterfactual Regret Minimization (CFR) [58] : An iterative algorithm that converges to a Nash equilibrium in two-player zero-sum games.</p>
<p>Machine Learning Approaches:</p>
<p>Deep Q-Network (DQN) [27]: Utilizes deep neural networks to approximate the Q-value function, addressing decision-making problems in high-dimensional state spaces.</p>
<p>Deep Monte Carlo (DMC) [33]: Integrates deep learning with Monte Carlo methods, using deep neural networks to generate efficient samples to estimate the expected value of complex distributions.</p>
<p>Deep CFR [5]: Combines deep learning with counterfactual regret minimization, employing neural networks to approximate regression targets for strategy solving in complex games.</p>
<p>LLM Reasoning Approaches:</p>
<p>Direct: The LLM generates the final action in response to the given game setting prompt.</p>
<p>Chain-of-Thought (CoT) [43,20]: A zero-shot native reasoning method where the LLM follows a step-by-step thought process.</p>
<p>Reflexion [32]: This method refers to the concept of language agents with verbal reinforcement learning.</p>
<p>Main Result</p>
<p>We utilized the gaming environment described in the Section 3.1.To more clearly distinguish between varying degrees of Payoff, the background color will be greener as the profit increases and redder as it decreases.</p>
<p>From Table 1, it is evident that compared to the previous LLM Reasoning method (Direct, CoT and Reflexion), BIDDER shows significant improvement when facing different opponents, demonstrating a noticeable increase in average earnings.</p>
<p>Additionally, in comparison with Machine Learning methods, BIDDER astonishingly outperformed.</p>
<p>It is noteworthy that BIDDER relies solely on LLM for bidirectional information judgment, operating as a completely non-learning method.This highlights the potential of BIDDER in decision-making based on LLM.   6.</p>
<p>Rationalality Evaluation</p>
<p>Equilibrium can be viewed as a stable state in a multi-agent game environment, achieved by each participant through a rational decision-making process.In most cases, it can be regarded as the most rational decision.We take the behavior at each decision point of DeepCFR as the optimal solution a o , and compare the rationality levels of different methods with the optimal solution.</p>
<p>The quantitative calculation method is:  Table 2 shows the rationality levels of different methods when facing various opponents.From Table 2, we can see that BIDDER has significantly improved the Rational Degree compared to previous LLM Reasoning methods and Machine Learning methods.This indicates that BIDDER has enhanced the decision rationality of the LLM.
Rational Degree = #{a = a o } #{All
Additionally, we further analyzed the action distribution of different methods in Limit Texas Hold'em.From Figure 3, we can observe that compared to the Direct approach, BIDDER has a much higher frequency of Raise, which aligns with the optimal solution (DeepCFR).In contrast, the Direct approach shows higher frequencies of Call and Check, indicating that an LLM-based approach acts as a passive decision-maker.BIDDER, however, calculates future returns and prompts the LLM to act based on these returns, transforming the LLM into an active decision-maker.This active decision-making is rational, leading to improved returns.</p>
<p>Negotiation</p>
<p>Negotiation requires complex communication and reasoning skills, making it an ideal environment for studying artificial intelligence and classical game theory [28,38] due to its dynamic, controllable, and easily assessable nature.We utilize a game paradigm known as "division of a common pool of items," as defined in previous research.In this game, agents are tasked with establishing a mutually acceptable division of items while having their own hidden utilities for each item.Our rules are based on those outlined by Cao et al., with the following modifications: ensuring that each individual's total value for all items is equal (consistent with Lewis et al.), which is more equitable compared to the original uniform distribution between 1 and 10.The experiment consists of 100 games, with each game having a different number and distribution of item values.Additionally, symmetrical experiments are conducted by swapping the values for both parties in another game.</p>
<p>Baselines</p>
<p>We use widely adopted baselines for reasoning and decision-making comparisons, including Direct and CoT introduced in Section 3.1.1.Additionally, we consider: Monte Carlo Tree Search (MCTS) [7]: This is a heuristic search algorithm that efficiently explores large search spaces by sampling potential future outcomes through Monte Carlo simulations.</p>
<p>Tree of Thoughts (ToT) [49]: This approach simulates human System 2 thinking and excels in complex problems, particularly those requiring search and judgment.</p>
<p>It is important to note here that although BIDDER fundamentally employs tree search for future exploration, similar to MCTS and ToT, BIDDER's search process integrates opponent hidden information inference and opponent behavior modeling.This contrasts with MCTS's probabilistic sampling and ToT's self-exploration approach.</p>
<p>Metrics</p>
<p>We employ three metrics for negotiation as defined by Lewis et al., which are: Score: The average score obtained in each game where an agreement is reached.The score is calculated as the sum of the products of each item assigned to the agent and its value.</p>
<p>Agreement: The probability of reaching an agreement across all games.</p>
<p>Pareto Optimality: The percentage of Pareto optimal solutions for the agreed deals. 4hese metrics not only reflect the agent's competitive advantage in negotiations but also demonstrate its cooperative behavior.From Table 3, we can observe that compared to MCTS, the reasoning methods based on LLM show significant improvements over the Direct approach in both Score and Agreement metrics.Additionally, all methods achieved 100% Agreement when competing against MCTS.This is because MCTS is more inclined to agree, yet it fails to achieve higher gains in terms of score.Furthermore, compared to the results reported by Lewis et al., the scores for LLM reasoning methods on Pareto optimality are generally lower.We believe there are two possible reasons for this: first, the tested methods are all zero-shot, so LLMs cannot effectively predict the timing for achieving Agreement; second, negotiation is a game that requires both competition and cooperation, and balancing these two aspects poses a challenge for LLMs.</p>
<p>Result Player</p>
<p>Opponent</p>
<p>MCTS</p>
<p>Moreover, we found that BIDDER achieved moderate Agreement but had the highest Score (payoff).This also reflects BIDDER's rational decision-making when aiming for payoff maximization, especially when long-term planning is required.</p>
<p>4 Related Work</p>
<p>Enhanced Reasoning Methods of LLMs</p>
<p>Large Language Models (LLMs) demonstrate remarkable proficiency in a range of complex reasoning tasks, such as mathematical reasoning [26,31], common sense understanding [37,4], and symbolic reasoning [34,36].A key technique in enhancing LLM reasoning is the Chain-of-Thought (CoT) method, which involves decomposing complex questions into a series of intermediate steps [43,20].Building upon CoT, researchers have developed methods such as Tree of Thought (ToT) [49], Graph of Thought (GoT) [3], and Skeleton-of-Thought [30] to further advance reasoning processes.To enhance the consistency and quality of LLM responses, methods like Self-Refine [25] and Reflexion [32] introduce iterative refinement, while several works [12,41] integrate detailed persona information to improve rationality and knowledge precision in LLM reasoning.</p>
<p>Existing methods such as ToT and GoT enhance reasoning performance by navigating through the problem space and selecting optimal nodes (states).States are evaluated based on the progress toward solving the problem.However, these approaches typically involve unidirectional state transitions and reward generation.In contrast, our work introduces a bidirectional approach, allowing for a more dynamic and rational reasoning process.</p>
<p>Heuristic Search for LLM Reasoning</p>
<p>With the widespread application of LLMs in the field of natural language processing, enhancing their reasoning capabilities has become a research focus.Heuristic search algorithms, as an effective optimization technique, are being introduced into LLM reasoning to improve efficiency and accuracy.Specifically, Yao et al. proposed the ToT algorithm based on "System2," which constructs a search tree by generating candidate actions through LLM and evaluating them.Subsequently, more methods such as A* algorithm and Monte Carlo Tree Search (MCTS) have been proposed to enhance LLM's reasoning and planning capabilities [11,17,57].To further improve the efficiency and accuracy of search paths, researchers have also introduced major voting [40], self-verification [44,51], training value functions [39], and game-theory-based solutions [14,13].</p>
<p>However, these methods are primarily used for reasoning tasks.In multi-agent decision environments where opponent state information and rewards are uncertain, traditional heuristic search algorithms are insufficient.BIDDER extends the application of heuristic search in LLMs by utilizing historical information to infer hidden opponent information, enabling opponent modeling.This approach allows for exploration and long-term reward calculation in multi-agent environments, thus achieving rational decision-making.</p>
<p>Decision-making based on LLM Agents</p>
<p>Recent research on large language models (LLMs) in decision-making [53] has spanned various multi-agent systems (MAS) including social behavior [56,19], economic simulations [55,22], game theory [10,46], and game playing [24,47].To enhance the strategic reasoning capabilities of LLMs, researchers have incorporated the concepts of Theory of Mind [13,16,54] and Reinforcement Learning [48,52].These approaches aim to prompt LLMs to grasp the complexities of decisionmaking tasks.</p>
<p>Although these studies have demonstrated the potential and applications of LLMs in the decisionmaking domain, specialized reasoning frameworks designed for specific tasks face significant challenges when applied to other fields.Our generalized rational decision-making theory introduces a bi-directional deliberation reasoning method.Through various experiments, it has been validated that this approach significantly enhances the benefits gained by LLMs in decision-making processes.</p>
<p>Conclusions</p>
<p>As large language models (LLMs) are increasingly applied across various fields" BIDDER offers a novel approach to enhance their performance in complex decision-making environments.By introducing bidirectional reasoning, BIDDER integrates key concepts from decision science, such as uncertainty management and expected utility, into the reasoning processes of LLMs.By leveraging both historical data and potential future rewards, BIDDER effectively addresses the limitations of traditional unidirectional reasoning methods.The integration of decision theory principles and bidirectional modeling ensures a comprehensive exploration of possible states, leading to more informed and rational decisions.Our experiments in poker and negotiation scenarios demonstrate that BIDDER significantly improves the decision-making capabilities of LLMs, aligning their performance more closely with theoretical optimal solutions.This advancement represents a significant step forward in the development of LLMs, enabling them to make better decisions in complex, real-world scenarios.1. Item Pool: Agents are presented with a pool of items, which in the paper's example includes three types of items: peppers, cherries, and strawberries.Each type of item has a quantity associated with it, which is sampled uniformly for each round of the game.</p>
<ol>
<li>
<p>Utility Functions: Each agent receives a utility function that specifies the rewarding value of each item.These utilities are private, meaning that each agent only knows its own utility values, which are sampled uniformly with at least one item having non-zero utility.</p>
</li>
<li>
<p>Negotiation Rounds: Over a series of negotiation timesteps (N), the agents take turns to propose how to divide the items in the pool.The number of turns, N, is sampled between 4 and 10 in each round to prevent a 'first-mover' advantage.</p>
</li>
<li>
<p>Alternating Turns: The agents alternate turns, with one agent (referred to as Agent A) always acting on odd turns and the other agent (Agent B) on even turns.</p>
</li>
<li>
<p>Proposals and Messages: During each turn, an agent can make a proposal regarding the division of the items and send a message through a communication channel.The proposal is a vector indicating the quantity of each item type that the agent is requesting.</p>
</li>
<li>
<p>Termination: Either agent can terminate the negotiation at any timestep with a special action, which signifies agreement with the most recent proposal made by the other agent.The negotiation ends, and rewards are distributed based on the last proposal.</p>
</li>
<li>
<p>Rewards: If the negotiation is successfully terminated, the agents receive rewards based on the dot product of their utility vectors and the items they receive.If an agent makes an invalid proposal and it is accepted, both agents receive a reward of zero.</p>
</li>
<li>
<p>No Agreement Scenario: If the negotiation reaches the upper limit of allowed turns without agreement, both agents receive no reward.</p>
</li>
</ol>
<p>The game is designed to study how agents can learn to communicate and negotiate effectively, with a focus on the emergence of communication protocols and strategies in a multi-agent reinforcement learning setting.</p>
<p>For utilizing LLM as a player, the game setting prompt and the state description prompt required for each action are as follows:</p>
<p>Game Setting Prompt</p>
<p>You are negotiating the division of Peppers, Strawberries, and Cherries with the opponent.Different values these items hold for both you and your opponent.The process is structured into two stages per round: the proposal stage and the utterance stage.Remember, the key in such negotiations is understanding that your opponent also has their value system for these items, which is unknown to you.Balancing between revealing your true desires and misleading your opponent to gain a favorable outcome is essential.Itś also important to be adaptive, as the negotiation progresses and you gather more information about your opponentś preferences and tactics.</p>
<p>Reflexion Prompt</p>
<p>Please carefully check the thought and the action you just output , and then refine your answer.The final output is also in the same format: My revised thought is {Your Thought}.My revised action is {'action: "}.</p>
<p>B.1.4 BIDDER Hidden State Inference Prompt</p>
<p>You are a Poker expert.Based on the public information provided below, evaluate the relative strength of each player's hand.The strongest hand should be rated as 5, and the weakest as 1.Available History Information: {events} Instructions:</p>
<ol>
<li>
<p>Evaluate the relative strength of each player's hand based on the information given.2. Consider the players' actions and the community cards in your evaluation.</p>
</li>
<li>
<p>Provide a rating from 1 to 5, with 5 being the strongest hand and 1 being the weakest Please provide your evaluation based on the information above.Sample Output Format: { "Player": { "Rating": <rating> } }</p>
</li>
</ol>
<p>Modeling Prompt</p>
<p>You are observing an ongoing poker game.Here are the details: -Current community cards: {public_cards} -Players and their estimated hand strengths: {hand_strength} The game history so far is as follows: events What is the most likely action for player when legal actions are: {legal_action}?</p>
<p>The JSON format should look like this: {"action": "<most likely action>"}</p>
<p>Reward Prompt</p>
<p>You are observing an ongoing poker game.Here are the details: -Current community cards: {public_cards} -Players and their estimated hand strengths: {hand_strength} The game history so far is as follows: events Predict the potential payoff (normalized between 0 and 1) for {player} for each of the following possible actions and return the results in JSON format: {action_payoff_template}</p>
<p>B.2 Negotiation</p>
<p>For the LLM reasoning baselines in the Negotiation game (including Direct, CoT, and ToT), we utilize the implementation provided by Duan et al. 5 .T ← T ∪ Explore(τ ∪ {a t , s t+1 }, t + 1) 8: end for</p>
<p>C Algorithm Detail of BIDDER</p>
<p>Figure 1 :
1
Figure 1: Comparing Unidirectional and Bi-Directional</p>
<p>Figure 2 :
2
Figure 2: BI-Directional DEliberation Reasoning includess: 1. Hidden State Inference: Inferring hidden states (e.g.opponent's strategies) behind decision-making from historical data (e.g.opponent's actions); 2. Future Exploration: Exploring possible future trajectories based on inferred states; 3. Action Reward Collection: Aggregating utilities from predicted trajectories to make decisions.The bi-directional reasoning enhances decision rationality by incorporating both historical context and potential future outcomes.</p>
<p>Explored Trajectories Action Reward r Calculation Prompt r(s t ) = Reward_Calculation(s 0 , s t ) All historical information (including Player and Opponent): [Trajectories of Player and Opponent] Feasible Actions of the Current State: [Feasible Actions]</p>
<p>Decision</p>
<p>Making Based on Bidirectional Deliberation Reasoning a * t = Decision_Making(s t , s 0 , {G(a)}) [Past Trajectory and Player's Current State] Inferred Hidden State of Opponent: [Opponent's Hidden State] #Left-to-right Deliberation Reasoning Possible Hidden States of Opponent: [{G(a)} of all feasible actions in current state] #Right-to-left Deliberation Reasoning Make the most rational decisions based on current information.</p>
<p>Figure 3 :
3
Figure 3: Direct, BIDDER, and DeepCFR Action Distribution in Limit Texas Hold'em.</p>
<p>3 :
3
Performance of various models in Negotiation against different opponents, and each cell represents [Score, Agreement, Pareto Optimality]</p>
<p>Figure 4 :
4
Figure 4: The distribution of hand strengths sampled from poker games ranges from 0 to 8, corresponding respectively to High Card, One Pair, Two Pair, Three of a Kind, Straight, Flush, Full House, Four of a Kind, and Straight Flush.</p>
<p>9 : return T Algorithm 2 3 : 4 :τ starts with action a then 5 : 13 :
9234513
Rewards Aggregation with Attenuation Coefficient Require: T : Explored trajectories; A: The set of valid actions; β: Attenuation Coefficient; Ensure: G(A): The maximum payoff of each action.1: for each a ∈ A do 2: G(a) ← 0 for each trajectory τ ∈ T do if return {G(a) | a ∈ A}</p>
<p>By analyzing P (τ |s 0 ), we aim to estimate the causal factors s 0 influencing the observed data τ .The LLM can be regarded as an implicit Bayesian inference model, inferring hidden states s 0 based on observed data τ , et al.P (s 0 |τ ).
2.2 Explore Future Trajectories with Opponent ModelingOpponent Modeling Prompta ′ t = Opponent_Modeling(τ (t))Inferred Hidden State of Opponent:[Opponent's Hidden State]Opponent Action History:[Opponent Trajectory]Select the most likely action which opponent willtake under inferred hidden state.In this prompt, [Opponent Trajectory] represents the opponent's observed actions (historical data τ ), and [Discretized State Space List] enumer-Hidden State Inference Prompt s 0 = Hidden_Inference(τ )ates possible hidden states s 0 ) causing these actions. The LLM processes this informationOpponent Action History:and selects the most probable hidden state s 0 ,[Opponent Trajectory]thereby inferring the causes from the effects.Possible Hidden States for Opponent's Actions:This method leverages the LLM's ability to un-derstand and reason about complex causal re-lationships, enabling effective prediction and[Discretized State Space List] Select the most likely state of the opponent.decision-making in uncertain environments.</p>
<p>Table 1 :
1
Table 1 presents the performance of different methods when facing various opponents, with the values representing the average final chips obtained over 100 games.The outcomes (payoff) of various models against different opponents in Limit Texas Hold'em.
Player Opponent Random Rule CFR DQN DMC DeepCFR AVGRuleRandom-0.28-0.05 -0.56 -1.87 -0.55-1.89-0.87Rule0.01-0.14 0.040.21 -0.070.330.06CFR0.55-0.17 -0.38 -3.15 -1.89-3.88-1.49Reinforcement LearningDQN DMC DeepCFR2.61 1.53 1.550.19 0.34 0.112.86 1.41 2.850.06 -0.03 0.17 -0.09 3.15 -0.03-5.28 -0.038 -0.670.07 0.55 1.16LLM ReasoningDirect CoT Reflexion0.86 0.99 1.420.68 0.49 0.480.74 -0.30 -0.12 0.83 0.71 0.42 0.76 1.00 0.44-0.30 -0.64 -0.530.26 0.47 0.60BIDDER (Ours)T=1 T=2 T=32.66 2.47 1.610.74 0.74 0.742.83 3.39 2.761.61 -0.10 1.61 -0.10 1.87 -0.141.45 1.44 0.391.53 1.59 1.21</p>
<p>Table 2 :
2
The Rational Degree (%) of various models in Limit Texas Hold'em against different opponents, as calculated by Equation</p>
<p>Proposals State PromptNow, you are in the Proposal stage: you ĺl determine the division of items you desire.This is expressed as [a, b, c], where áŕepresents the quantity of Peppers, bt he quantity of Strawberries, and ćt he quantity of Cherries you wish to acquire.Itś crucial to base this division on the perceived value these items have for you, keeping in mind that the goal is to reach a mutually agreeable solution.Now, you are in the Utterance Stage: you communicate to your opponent what you want, again in the format [a, b, c].This utterance is your strategic communication and doesn t necessarily have to reflect your actual desires or the proposal you formulated in the first stage.Itś a tool for negotiation, potentially used to mislead, bluff, or strategically reveal information to your opponent.
Utterance State PromptB Reasoning Method Implementation DetailsB.1 Limit Texas Hold'emB.1.1 DirectQuery PromptPlease provide your results in the form of {'action': "}. Just output the dictionary, don't useany other text.B.1.2 CoTQuery PromptPlease first think and reason about the current state and then generate your action as follows:My thought is {Your Thought}, and my action is {'action': "}B.1.3 ReflexionQuery PromptPlease first think and reason about the current state and then generate your action as follows:My thought is {Your Thought}, and my action is {'action': "}</p>
<p>Algorithm 1 Explore (T , t): Exploration with Opponent Modeling Require: s t : Current decision context; I: Inferred hidden information about opponent; T : Time-step of exploration; Ensure: T (s t ): The traces of exploration.1: if t = T or s t is terminal node then T = ∅ 5: for each legal action a t of s t do 6: s t+1 ← s t × a t × Opponent_Modeling(I, τ )
2:return {τ }3: end if4: 7:
A chance node signifies the introduction of new information that can influence decisions. For instance, in Texas Hold'em poker, this occurs when a new community card is revealed.
Due to the presence of random perturbations in the calculation of CFR, the results are not 100% consistent.
Pareto optimal refers to a state in which it is impossible to improve individual's situation without worsening other's.
https://github.com/jinhaoduan/GTBench
A Game SettingA.1 Limit Texas Hold'em Limit Texas Hold'em is a variant of Texas Hold'em poker where the betting limits are fixed.The fixed betting structure and sequential decision-making process create a controlled yet challenging environment for researching agent decision-making.Its partial observability, and stochastic nature make it ideal for studying how agents handle uncertainty, strategic planning, and adversarial competition.Rules and Gameplay are as following:1. Blinds: -Small Blind and Big Blind are posted before the cards are dealt.-The Small Blind is usually half the size of the Big Blind.Betting Rounds:-There are four betting rounds: Pre-Flop, Flop, Turn, and River.-Betting amounts are fixed and structured according to the round.3. Dealing: -Each player is dealt two private cards (hole cards).-Five community cards are dealt in stages: the Flop (three cards), the Turn (one card), and the River (one card).4. Action Spaces: there are four actions, Call, Raise, Fold and Check.In Limit Texas Hold'em.Each player can only choose a fixed amount of raise.And in each round the number of raises is limited to four times.For utilizing LLM as a player, the game setting prompt and the state description prompt required for each action are as follows:Game Setting PromptYou are {name} and you are a Texas Hold'em player.Texas Hold'em is a popular betting game.Each player is dealt two face-down cards, called hole cards.Then 5 community cards are dealt in three stages (the flop, the turn and the river).Each player seeks the five best cards among the hole cards and community cards.In this Texas Hold'em game, there are a total of {num_players} players namely {players}.The number of chips every player has is infinite.You just need to win more chips in the competition as much as possible.State PromptThe 1. Propose the two Utterances with the largest rewards.This also includes agreeing if you agree with your opponent's utterance.Each utterance format is: "<Utterance: [a, b, c]>" (including "&lt;" and "&gt;").2. Calculate the Payoff for each Utterance (a number); calculate the probability (normalized between 0 and 1) of the opponent agreeing with this utterance, it is necessary to consider the degree of satisfaction the opponent has with the utterance.Sample Output Format: Utterances: { "Utterance": "<utterance>", "Payoff": "<payoff>", "Probability": "<probability>" }, { "Utterance": "<utterance>", "Payoff": "<payoff>", "Probability": "<probability>" }
. CoT. 25.2, 100%, 30%] [22.0, 50%, 40%. 22.4, 70%, 29%. 16.4, 80%, 50%] [23.0, 67%, 0%] [21.8, 73%, 30%</p>
<p>From statistical knowledge bases to degrees of belief. Fahiem Bacchus, Adam J Grove, Joseph Y Halpern, Daphne Koller, Artif. Intell. 87199630.3, 100%, 33%] [21.0, 17%, 0.0%] [18.0, 67%, 0%. 17.0, 67%, 0%] [24.0, 50%, 33%] [22.6, 60%, 13%] References [1</p>
<p>Dynamic programming. science. Richard Bellman, 1966153</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, arXiv:2308.096872023arXiv preprint</p>
<p>Think you have solved direct-answer question answering? try arc-da, the direct-answer ai2 reasoning challenge. Sumithra Bhakthavatsalam, Daniel Khashabi, Tushar Khot, Dalvi Bhavana, Kyle Mishra, Ashish Richardson, Carissa Sabharwal, Oyvind Schoenick, Peter Tafjord, Clark, arXiv:2102.033152021arXiv preprint</p>
<p>Deep counterfactual regret minimization. Noam Brown, Adam Lerer, Sam Gross, Tuomas Sandholm, International conference on machine learning. PMLR2019</p>
<p>. Kris Cao, Angeliki Lazaridou, Marc Lanctot, Joel Z Leibo, Karl Tuyls, Stephen Clark, arXiv:1804.039802018arXiv preprintEmergent communication through negotiation</p>
<p>Monte-carlo tree search: A new framework for game ai. Guillaume Chaslot, Sander Bakkes, Istvan Szita, Pieter Spronck, Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment. the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment20084</p>
<p>Asking before acting: Gather information in embodied decision making with language models. Xiaoyu Chen, Shenao Zhang, Pushi Zhang, Li Zhao, Jianyu Chen, arXiv:2305.156952023arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>Gtbench: Uncovering the strategic reasoning limitations of llms via game-theoretic evaluations. Jinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Elias Stengel-Eskin, Mohit Bansal, Tianlong Chen, Kaidi Xu, arXiv:2402.123482024arXiv preprint</p>
<p>Alphazerolike tree-search can guide large language model decoding and training. Xidong Feng, Ziyu Wan, Muning Wen, Ying Wen, Weinan Zhang, Jun Wang, arXiv:2309.171792023arXiv preprint</p>
<p>Improving language model negotiation with self-play and in-context learning from ai feedback. Yao Fu, Hao Peng, Tushar Khot, Mirella Lapata, arXiv:2305.101422023arXiv preprint</p>
<p>Kanishk Gandhi, Dorsa Sadigh, Noah D Goodman, arXiv:2305.19165Strategic reasoning with language models. 2023arXiv preprint</p>
<p>Ian Gemp, Yoram Bachrach, Marc Lanctot, Roma Patel, Vibhavari Dasagi, Luke Marris, Georgios Piliouras, Karl Tuyls, arXiv:2402.01704States as strings as strategies: Steering language models with game-theoretic solvers. 2024arXiv preprint</p>
<p>Large language models are zero-shot time series forecasters. Nate Gruver, Marc Finzi, Shikai Qiu, Andrew Gordon, Wilson , Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems. Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, Sergey Levine, NeurIPS; New Orleans, LA, USA2023. 2023. December 10 -16, 2023, 2023</p>
<p>Jiaxian Guo, Bo Yang, Paul Yoo, Bill Yuchen Lin, Yusuke Iwasawa, Yutaka Matsuo, arXiv:2309.17277Suspicion-agent: Playing imperfect information games with theory of mind aware gpt-4. 2023arXiv preprint</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, arXiv:2305.14992Reasoning with language model is planning with world model. 2023arXiv preprint</p>
<p>Teaching large language models to reason with reinforcement learning. Alex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, Roberta Raileanu, arXiv:2403.046422024arXiv preprint</p>
<p>War and peace (waragent): Large language model-based multi-agent simulation of world wars. Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang Ge, Libby Hemphill, Yongfeng Zhang, arXiv:2311.172272023arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Deal or no deal? end-to-end learning for negotiation dialogues. Mike Lewis, Denis Yarats, Yann N Dauphin, Devi Parikh, Dhruv Batra, 2017</p>
<p>Tradinggpt: Multiagent system with layered memory and distinct characters for enhanced financial trading performance. Yang Li, Yangyang Yu, Haohang Li, Zhi Chen, Khaldoun Khashanah, arXiv:2309.037362023arXiv preprint</p>
<p>Position: Foundation agents as the paradigm shift for decision making. Xiaoqian Liu, Xingzhou Lou, Jianbin Jiao, Junge Zhang, arXiv:2405.170092024arXiv preprint</p>
<p>Large language models play starcraft ii: Benchmarks and a chain of summarization approach. Weiyu Ma, Qirui Mi, Xue Yan, Yuqiao Wu, Runji Lin, Haifeng Zhang, Jun Wang, arXiv:2312.118652023arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, arXiv:2303.176512023arXiv preprint</p>
<p>Shen-Yun, Chao-Chun Miao, Keh-Yih Liang, Su, arXiv:2106.15772A diverse corpus for evaluating and developing english math word problem solvers. 2021arXiv preprint</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, nature. 51875402015</p>
<p>Equilibrium points in n-person games. F John, NashJr, Proceedings of the national academy of sciences. the national academy of sciences195036</p>
<p>Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Nick Barnes, Ajmal Mian, arXiv:2307.06435A comprehensive overview of large language models. 2023arXiv preprint</p>
<p>Skeleton-of-thought: Large language models can do parallel decoding. Xuefei Ning, Zinan Lin, Zixuan Zhou, Huazhong Yang, Yu Wang, arXiv:2307.153372023arXiv preprint</p>
<p>Arkil Patel, Satwik Bhattamishra, Navin Goyal, arXiv:2103.07191Are nlp models really able to solve simple math word problems?. 2021arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Shunyu Karthik R Narasimhan, Yao, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Mastering the game of go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den, Julian Driessche, Ioannis Schrittwieser, Veda Antonoglou, Marc Panneershelvam, Lanctot, nature. 52975872016</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.046152022arXiv preprint</p>
<p>Reinforcement learning: An introduction. S Richard, Andrew G Sutton, Barto, 2018MIT press</p>
<p>Challenging bigbench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Ed H Quoc V Le, Denny Chi, Zhou, arXiv:2210.092612022arXiv preprint</p>
<p>Alon Talmor, Ori Yoran, Le Ronan, Chandra Bras, Yoav Bhagavatula, Yejin Goldberg, Jonathan Choi, Berant, arXiv:2201.05320Commonsenseqa 2.0: Exposing the limits of ai through gamification. 2022arXiv preprint</p>
<p>Theory of games and economic behavior. John Von, Neumann , Oskar Morgenstern, 19472</p>
<p>Chaojie Wang, Yanchen Deng, Zhiyi Lv, Shuicheng Yan, An Bo, arXiv:2406.14283Q*: Improving multi-step reasoning for llms with deliberative planning. 2024arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Unleashing the emergent cognitive synergy in large language models: A task-solving agent through multipersona self-collaboration. Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, Heng Ji, arXiv:2307.053002023arXiv preprint</p>
<p>Q-learning. Jch Christopher, Peter Watkins, Dayan, Machine learning. 81992</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Large language models are better reasoners with self-verification. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, Jun Zhao, arXiv:2212.095612022arXiv preprint</p>
<p>An explanation of in-context learning as implicit bayesian inference. Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. April 25-29, 2022. 2022OpenReview.net</p>
<p>Magic: Investigation of large language model powered multi-agent in cognition, adaptability, rationality and collaboration. Lin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen Dong, Kurt Keutzer, See-Kiong Ng, Jiashi Feng, ICLR 2024 Workshop on Large Language Model (LLM) Agents. 2023</p>
<p>Exploring large language models for communication games: An empirical study on werewolf. Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, Yang Liu, arXiv:2309.046582023arXiv preprint</p>
<p>Language agents with reinforcement learning for strategic play in the werewolf game. Zelai Xu, Chao Yu, Fei Fang, Yu Wang, Yi Wu, arXiv:2310.189402023arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.106012023arXiv preprint</p>
<p>Rlcard: A toolkit for reinforcement learning in card games. Daochen Zha, Kwei-Herng Lai, Yuanpu Cao, Songyi Huang, Ruzhe Wei, Junyu Guo, Xia Hu, 2020</p>
<p>Bin Zhang, Hangyu Mao, Jingqing Ruan, Ying Wen, Yang Li, Shao Zhang, Zhiwei Xu, Dapeng Li, Ziyue Li, Rui Zhao, arXiv:2311.13884Controlling large language model-based agents for large-scale decision-making: An actor-critic approach. 2023arXiv preprint</p>
<p>Agent-pro: Learning to evolve via policy-level reflection and optimization. Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang, Weiming Lu, arXiv:2402.175742024arXiv preprint</p>
<p>Llm as a mastermind: A survey of strategic reasoning with large language models. Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian De Wynter, Yan Xia, Wenshan Wu, Ting Song, Man Lan, Furu Wei, arXiv:2404.012302024arXiv preprint</p>
<p>K-level reasoning with large language models. Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Yan Xia, Man Lan, Furu Wei, arXiv:2402.015212024arXiv preprint</p>
<p>Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie Zhu, Chen Hao, Xing Xie, arXiv:2310.17512Competeai: Understanding the competition behaviors in large language model-based agents. 2023arXiv preprint</p>
<p>Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, arXiv:2310.11667Interactive evaluation for social intelligence in language agents. 2023arXiv preprint</p>
<p>Toolchain<em>: Efficient action space navigation in large language models with a</em> search. Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor Bursztyn, Ryan A Rossi, Somdeb Sarkhel, Chao Zhang, arXiv:2310.132272023arXiv preprint</p>
<p>Regret minimization in games with incomplete information. Martin Zinkevich, Michael Johanson, Michael Bowling, Carmelo Piccione, Advances in neural information processing systems. 202007</p>            </div>
        </div>

    </div>
</body>
</html>
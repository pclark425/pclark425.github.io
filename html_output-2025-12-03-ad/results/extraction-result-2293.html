<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2293 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2293</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2293</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-272423444</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.03425v1.pdf" target="_blank">Strengthening leverage of Astroinformatics in inter-disciplinary Science</a></p>
                <p><strong>Paper Abstract:</strong> Most domains of science are experiencing a paradigm shift due to the advent of a new generation of instruments and detectors which produce data and data streams at an unprecedented rate. The scientific exploitation of these data, namely Data Driven Discovery, requires interoperability, massive and optimal use of Artificial Intelligence methods in all steps of the data acquisition, processing and analysis, the access to large and distributed computing HPC facilities, the implementation and access to large simulations and interdisciplinary skills that usually are not provided by standard academic curricula. Furthermore, to cope with this data deluge, most communities have leveraged solutions and tools originally developed by large corporations for purposes other than scientific research and accepted compromises to adapt them to their specific needs. Through the presentation of several astrophysical use cases, we show how the Data Driven based solutions could represent the optimal playground to achieve the multi-disciplinary methodological approach.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2293.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2293.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PHILab</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parameter Handling Investigation Laboratory (PHILab)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid feature-selection framework combining shadow-feature importance (Boruta) and Lasso regularization to identify both strong and weak informative features in very high-dimensional scientific parameter spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Astrophysics — classification of Young Stellar Objects (YSOs) from Hi-GAL clump catalogs</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Classify evolutionary stages of YSOs by selecting informative parameters from multi-dimensional Hi-GAL clump feature sets to relate cold material reservoirs to formed YSOs.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Abundant high-volume survey data (large catalogues from Hi-GAL); labelled data available for supervised classification tasks but potentially imbalanced or noisy; distributed, heterogeneous archives requiring curation.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-dimensional tabular parameter space derived from multi-band photometry and derived physical quantities (multi-dimensional, dense feature vectors; some features may be correlated or noisy).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: many features (high dimensionality), low S/N for some signals, strong feature correlations, curse of dimensionality challenges for exhaustive exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Mature domain (astronomy) with well-established observational pipelines and prior physical models, but data-driven classification problems and large-survey feature engineering remain active research areas.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High - interpretable feature selection is required to produce scientifically actionable insights about star-formation processes and to enable trust in downstream inferences.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>PHILab (Boruta shadow-features + Lasso regularization hybrid)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Two-step hybrid wrapper/filter: (1) Boruta-style shadow features (randomly shuffled duplicates) are used with tree-based importance to classify features as relevant/rejected/undetermined; (2) undetermined features are resolved using Lasso (L1) regularization to shrink redundant features and finalize selection. Designed to recover weak but informative features while reducing dimensionality.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Feature-selection hybrid (wrapper + regularized filter) supporting supervised learning pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and appropriate for extremely high-dimensional catalog data where weak signals are scientifically important; requires downstream supervised models for final classification.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Qualitatively effective: preserves informative contributions including weak signals, drastically reduces dimensionality and accelerates downstream processing, and in some cases improves classification tradeoffs (purity vs completeness).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High potential to improve scientific discovery by ensuring completeness of feature selection, enabling faster model training, and surfacing subtle, physically meaningful features that could lead to new insights.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared conceptually and empirically to PCA (which discards residual components as noise) and to plain Boruta/Lasso pipelines; PHILab aims to retain weak signals that PCA can miss and to resolve ambiguous features that Boruta alone leaves undetermined.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Combining randomness-introduced shadow features to estimate significance with L1-based sparsity control captures both obviously and weakly informative features; stability benefits from explicit treatment of undetermined features.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Hybridizing shadow-feature significance testing with L1 regularization enables more complete and reliable feature selection in very high-dimensional scientific parameter spaces, helping preserve weak but scientifically important signals while reducing dimensionality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Strengthening leverage of Astroinformatics in inter-disciplinary Science', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2293.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2293.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SHAP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SHapley Additive exPlanations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A game-theory-based XAI framework that attributes contributions of individual features (including main, interaction, and cumulative effects) to model predictions by approximating Shapley values over feature coalitions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A unified approach to interpreting model predictions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Astrophysics — feature selection and interpretation for YSO classification and cosmological parameter analysis (Pantheon+SH0ES supernova data)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Explain and quantify the information contribution of catalogue-derived features to supervised model performance and to evaluate robustness/insights in cosmological-model parameter spaces derived from supernova light curves.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Large published datasets (Hi-GAL clump catalogues, Pantheon+SH0ES supernova compilation) with many derived features; labels/targets exist for classification/regression tasks but may have uncertainties.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-dimensional tabular data (catalogue features derived from photometry/light curves), with feature interactions and correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: many interacting features, nonlinear relationships, and potential subtle interaction effects that impact cosmological inferences/classification.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Astrophysics/cosmology is mature with established models (e.g., ΛCDM) but data-driven model-comparison and feature interpretation are active and evolving research areas.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High - explainability is critical to trust model-driven scientific claims and to derive physical interpretation of feature importance, particularly in cosmology.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>SHAP (Shapley value-based explanations applied to ML feature selection)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Computes approximate Shapley values by comparing model outputs across all feature coalitions (or approximations thereof) to estimate per-feature contributions at both global and per-sample levels; provides main effects, interaction effects, and cumulative importance for each feature to support explainable feature selection.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Explainable AI (post-hoc interpretability method) used in conjunction with supervised ML models</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable for scientific contexts where interpretability is required; can be used to improve feature selection reliability and to highlight interactions relevant for scientific interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Improved transparency and trustworthiness of feature importance estimation; enabled identification of potentially informative parameters for alternative cosmological models and supported improved classification when used for feature reduction.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Large — enables model-agnostic, interpretable identification of features and interactions, facilitating hypothesis generation and prioritization for follow-up observations or experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to Boruta/PHILab and PCA-based approaches: SHAP provides richer explainability (per-sample and interaction effects) and clearer interpretability than tree-based importance or PCA which may discard weak but informative residuals.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Ability to produce per-sample and interaction-aware importance scores; model-agnostic formulation that works with common ML engines and reveals subtle, domain-relevant contributions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Game-theoretic, per-sample explainability (SHAP) reconciles powerful ML performance with the scientific need for interpretable, interaction-aware feature attribution, improving both trust and discovery in data-driven analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Strengthening leverage of Astroinformatics in inter-disciplinary Science', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2293.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2293.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepFocus (MAML-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepFocus (Model-Agnostic Meta-Learning framework for radio interferometry deconvolution)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A MAML-based, MLE-optimized meta-learning framework that searches model/optimizer hyperparameter space in a double loop to reconstruct true sky images from radio interferometry dirty cubes, exploiting HPC/GPU acceleration for massive model-space search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Radio astronomy — interferometric image deconvolution (ALMA data cubes)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Reconstruct the true sky brightness distribution from observed 'dirty' interferometric data cubes (large inverse problem) to detect and characterize astronomical sources across massive 3D data cubes.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Large-scale data: example test set of ~1000 ALMA data cubes, each ~100 GB (total ~100 TB); cubes have ~10 million spatial features and ~10k frequency channels; labeled training targets exist for supervised evaluation (simulated or annotated reconstructions).</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Very high-dimensional 3D data cubes (2 spatial + spectral/frequency), dense, large-scale arrays with complex noise and instrument response (PSF/dirty beam) structure.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Extreme: large-scale inverse problem with nonlinearities, high dimensionality (10M+ features per cube), strong ill-posedness, computationally expensive traditional pipelines, and requirement to model instrumental effects and noise.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Mature domain (radio interferometry) with well-established physical pipelines (e.g., TCLEAN) and priors, but computational methods and ML-driven alternatives are active research areas.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — scientific inference requires reliable, explainable reconstruction to trust detections, but some black-box ML components are acceptable if validated thoroughly against physical priors and baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>DeepFocus (MAML + Maximum Likelihood model-space optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Meta-learning MAML double-training loop where an inner loop trains feature-extraction encoder on an input cube to approximate the sky model (reconstruction task), and an outer loop reuses encoder features with a classifier to detect sources; a MLE-based loss guides search across model architectures and optimizer hyperparameters, massively parallelized on GPUs to find architectures that generalize quickly across cubes.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Meta-learning (MAML) applied to supervised deep learning and model architecture/hyperparameter optimization</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable and tailored for large interferometric inverse problems with limited per-task data and high heterogeneity across cubes; required modifications include meta-learning setup and MLE-driven model-space exploration plus HPC acceleration.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Processing speed: ability to process ~100 TB in 9 minutes versus ~5 hours for the TCLEAN baseline (≈33× speedup on the reported test campaign of 1000 cubes); classification precision improved relative to TCLEAN and other baselines (exact precision numbers not reported).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Qualitatively strong: drastically reduced processing time and improved detection/classification precision compared to the traditional TCLEAN pipeline and several other ML/DL methods; demonstrated ability to find effective model/hyperparameter combinations via meta-learning.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — enables orders-of-magnitude faster processing of massive radio datasets, reduces computational costs, scales to future large-survey workloads (ALMA/ALMA2030-era), and potentially improves scientific throughput and discovery rate.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Direct comparison to TCLEAN (traditional deconvolution pipeline): DeepFocus processed data much faster (9 minutes vs 5 hours) and improved classification precision; also compared favorably to other ML/DL methods in the literature (specific numeric comparisons not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Meta-learning (MAML) enabling rapid adaptation across tasks, MLE-guided model-space exploration to find robust architectures, and massive GPU/HPC resources to perform the search efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Meta-learning with model-space optimization and HPC acceleration can produce ML models that both outpace traditional physics-heavy pipelines in speed and improve reconstruction/detection performance on extremely large, ill-posed inverse problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Strengthening leverage of Astroinformatics in inter-disciplinary Science', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2293.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2293.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMBER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AMBER - Advanced SegFormer for Multi-Band Image Segmentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A customized Vision Transformer (ViT)/SegFormer architecture extended to 3D patches (spectral dimension) with a hierarchical transformer encoder and lightweight All-MLP decoder, tailored for semantic segmentation of multi-band/hyperspectral images.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AMBER -Advanced SegFormer for Multi-Band Image Segmentation: an application to Hyperspectral Imaging</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Remote sensing / Geophysics / Hyperspectral Imaging; also applicable analogously to astrophysical multispectral imaging and biomedical spatial transcriptomics</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Pixel-wise semantic segmentation of hyperspectral, multi-band images to classify regions/material types (e.g., land cover, geological composition) using rich spectral-spatial information.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Moderate-sized benchmark datasets (Indian Pines, Pavia University, PRISMA) with labeled segmentation masks; data are labeled but dataset sizes are modest compared to natural-image corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>3D image volumes: spatial (H×W) with spectral depth (D) per pixel — i.e., multi-band/hyperspectral data (high spectral dimensionality, spatial structure preserved).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate-to-high: high spectral dimensionality, need to capture spectral-spatial correlations, multi-scale features; models must balance capacity and overfitting on limited labeled examples.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Evolving: HSI analysis moved from classical spectral algorithms to DL (CNNs, ViTs); transformer-based models are recent and rapidly progressing.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low-to-medium — segmentation performance is primary; interpretability is useful for scientific trust but black-box DL is often acceptable if validated against known labels/ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>AMBER (3D-extended SegFormer/ViT with All-MLP decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Hierarchical transformer encoder extracts multi-scale spatial-spectral features using 3D patching to preserve spectral relationships; lightweight All-MLP decoder fuses multi-level features and reduces spectral dimension to produce 2D semantic segmentation masks; architecture preserves spatial resolution at output, trades higher parameter count for accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised deep learning — transformer-based semantic segmentation adapted to 3D/multi-band inputs</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable for hyperspectral and other multi-band image segmentation tasks; requires adaptation (3D convolutions and spectral-preserving attention) to capture spectral information effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Outperformed traditional CNN-based methods on Indian Pines and Pavia University (Overall Accuracy, Kappa, Average Accuracy) and achieved state-of-the-art on PRISMA in preliminary experiments; preserves spatial resolution and captures spectral features more effectively than baseline CNNs.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Significant — improved segmentation quality for hyperspectral applications across geophysics, planetary science, and biomedical imaging, enabling better material mapping and feature detection from multi-band sensors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to traditional CNN-based segmentation models and SegFormer variants: AMBER performed better on benchmark HSI datasets due to specialized 3D patch attention and spectral-preserving design.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Extending self-attention to 3D patches to capture spectral-spatial features, hierarchical feature extraction, and judicious decoder design that fuses multilevel features while preserving spatial detail.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Adapting transformer-based architectures to handle spectral depth (3D patches) and preserving spatial resolution yields superior segmentation performance on multi-band hyperspectral images compared to standard 2D CNN approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Strengthening leverage of Astroinformatics in inter-disciplinary Science', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2293.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2293.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Meta-learning (MAML)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-Agnostic Meta-Learning (MAML) / Meta-learning paradigms</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Meta-learning approach that trains models to provide initial parameters that can be rapidly fine-tuned to new tasks with few optimization steps and few data; MAML is model-agnostic and applicable across architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>General scientific ML problems with scarce labeled data per task (examples: rare/faint-source detection in astrophysics, few-shot classification in other domains)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Enable models trained on a distribution of tasks to adapt quickly to new, related tasks where labeled data are scarce, reducing the need for large labeled datasets per new task.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Specifically intended for settings with limited labeled data per task (few-shot), but requires a set of related tasks for meta-training; task diversity during training is important.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Varies by application — can be images, spectra, time-series, tabular; meta-learning is agnostic but often used with high-dimensional inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate-to-high: requires optimization across tasks (double loop training), computationally intensive meta-training, and careful selection of task distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Growing/active research area with increasing application in scientific domains where data labeling is costly; not yet standard practice across all fields but promising.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — interpretability of learned initialization is less emphasized; primary requirement is rapid adaptation and reliable generalization to new tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>MAML / meta-learning</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Two-level training: an outer loop optimizes initial model parameters such that, for a sampled task, a small number of inner-loop gradient updates on few-shot training data yields good task-specific performance; applicable across model families and used to accelerate architecture/hyperparameter search in frameworks like DeepFocus.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Meta-learning / few-shot supervised learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Particularly applicable where labeled data per new task are scarce and tasks are related (e.g., multiple interferometric cubes, multiple imaging domains); requires meta-training tasks and substantial compute for meta-optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Demonstrated to substantially reduce per-task training iterations and enable strong generalization from few examples; used effectively within DeepFocus to speed adaptation across radio cubes.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for domains with many related but individually scarce-data tasks; reduces labeling burden and enables transfer of learned priors across experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared conceptually to transfer learning: MAML focuses on learning initializations for rapid adaptation from few examples, whereas transfer learning typically relies on large pretraining and fine-tuning and may require much more data.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of a representative set of meta-training tasks, appropriate inner/outer loop optimization schedules, and sufficient compute for meta-training.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Meta-learning (MAML) enables efficient adaptation to new scientific tasks from very limited labeled data by training for adaptability rather than for a single final-task optimum, making it well-suited to many scarce-data scientific problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Strengthening leverage of Astroinformatics in inter-disciplinary Science', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2293.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2293.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SOM clustering</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Organizing Map (SOM) clustering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised neural-network-based clustering and dimensionality-reduction method that maps high-dimensional data onto a low-dimensional (usually 2D) grid preserving topological relationships, used here to group galaxies by redshift properties.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Astrophysics — comparison and visualization of spectroscopic vs photometric redshift coverage in large galaxy surveys</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Group thousands of galaxies in multi-band photometric and spectroscopic parameter spaces to compare coverage, identify regions lacking spectroscopic calibration, and facilitate calibration strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Large catalogues with multi-band photometry and spectroscopic redshifts for subsets; abundant unlabeled or weakly labeled data for clustering and visualization.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-dimensional tabular features derived from multi-band photometry and spectroscopic measurements (dense vectors per object).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate: large number of samples with moderate-to-high feature dimensionality; need to preserve topological structure for interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Mature in astronomy for exploratory analysis; SOMs are established unsupervised tools for large-catalog visualization and calibration assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — interpretability of cluster topology is important for calibration planning and understanding mapping between photometric and spectroscopic space.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Self-Organizing Map (SOM)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Unsupervised neural-map algorithm that projects high-dimensional feature vectors onto a 2D grid; similar objects map to nearby nodes, enabling visualization of distributions (e.g., median spectroscopic vs photometric redshift maps).</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Unsupervised learning / clustering / dimensionality reduction</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Well-suited for exploratory analysis and visualization of large survey parameter spaces; effective for highlighting regions for follow-up and calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Effective at grouping and visualizing large galaxy samples by redshift-related features, facilitating comparison between spectroscopic and photometric coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Useful for survey calibration strategy, identifying spectroscopic gaps, and guiding targeted observations to improve photometric redshift performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Provides topology-preserving 2D visualization compared to other clustering/DR methods (e.g., t-SNE, UMAP); the paper uses SOM as a practical tool rather than a benchmark comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Large sample sizes, meaningful input feature set (multi-band photometry and redshifts), and the topological preservation property of SOM facilitating intuitive visual interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Topology-preserving clustering methods like SOM enable intuitive visualization and direct comparison of coverage between spectroscopic and photometric parameter spaces in large surveys, aiding calibration and survey planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Strengthening leverage of Astroinformatics in inter-disciplinary Science', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2293.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2293.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nonlinear PCA (autoencoder)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nonlinear Principal Component Analysis via Autoencoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A nonlinear dimensionality-reduction approach implemented with an autoencoder neural network (nonlinear PCA) used as a preprocessing step for clustering in high-resolution spatial transcriptomics data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Biomedicine — high-resolution spatial transcriptomics (Open-ST pipeline) for tissue/cell clustering and disease detection</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Discover and cluster cellular/tissue structures in spatial transcriptomics images to identify disease-related regions and biomarkers by reducing dimensionality while preserving nonlinear structure.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Emerging datasets from high-resolution spatial transcriptomics platforms; data are rich but labeled examples (disease annotations) may be limited.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-dimensional spatially-resolved gene expression profiles and multi-channel image data (multimodal: spatial + transcriptomic), with complex nonlinear relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: large feature dimensionality, nonlinear interactions among genes/spatial context, requirement to reveal subtle tissue patterns indicative of disease.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging-to-maturing field: transcriptomics data analysis is established, but high-resolution spatial methods and DL-based pipelines are rapidly developing.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — biomedical interpretability is important to relate clusters to biological functions and disease mechanisms; downstream explainability and validation required.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Autoencoder-based nonlinear PCA</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Train an autoencoder to compress high-dimensional spatial transcriptomics/multi-channel image data into a lower-dimensional latent space (nonlinear principal components), then apply clustering on latent representations to discover cell/tissue clusters.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Unsupervised learning / nonlinear dimensionality reduction (autoencoder) + clustering</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for multimodal spatial transcriptomics where nonlinear structure exists and labels are scarce; serves as an effective preprocessing step for downstream clustering and disease-detection pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Enabled meaningful clustering of cells in tissue frames, facilitating discovery of potential disease regions and supporting analogies between astrophysical multispectral analysis and biomedical spatial profiling.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High cross-disciplinary impact: techniques developed in astrophysical multispectral analysis transfer to biomedical spatial profiling, improving tissue imaging interpretation and biomarker discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Nonlinear autoencoder offers advantages over linear PCA by capturing nonlinear manifolds in the data and enabling more informative clustering for complex biological patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Ability of autoencoders to capture nonlinear relationships and spatial context, plus careful design of latent dimensionality and clustering pipeline tuned to biological signal scales.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Nonlinear dimensionality reduction via autoencoders effectively captures complex, spatially-structured biological signals and can be repurposed across domains where multispectral/multimodal data exhibit nonlinear manifolds.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Strengthening leverage of Astroinformatics in inter-disciplinary Science', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Constructing Impactful Machine Learning Research for Astronomy: Best Practices for Researchers and Reviewers <em>(Rating: 2)</em></li>
                <li>A unified approach to interpreting model predictions <em>(Rating: 2)</em></li>
                <li>Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks <em>(Rating: 2)</em></li>
                <li>3D detection and characterization of ALMA sources through deep learning <em>(Rating: 2)</em></li>
                <li>AMBER -Advanced SegFormer for Multi-Band Image Segmentation: an application to Hyperspectral Imaging <em>(Rating: 2)</em></li>
                <li>Data deluge in astrophysics: Photometric redshifts as a template use case <em>(Rating: 1)</em></li>
                <li>Data-Rich Spatial Profiling of Cancer Tissue: Astronomy Informs Pathology <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2293",
    "paper_id": "paper-272423444",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "PHILab",
            "name_full": "Parameter Handling Investigation Laboratory (PHILab)",
            "brief_description": "A hybrid feature-selection framework combining shadow-feature importance (Boruta) and Lasso regularization to identify both strong and weak informative features in very high-dimensional scientific parameter spaces.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "Astrophysics — classification of Young Stellar Objects (YSOs) from Hi-GAL clump catalogs",
            "problem_description": "Classify evolutionary stages of YSOs by selecting informative parameters from multi-dimensional Hi-GAL clump feature sets to relate cold material reservoirs to formed YSOs.",
            "data_availability": "Abundant high-volume survey data (large catalogues from Hi-GAL); labelled data available for supervised classification tasks but potentially imbalanced or noisy; distributed, heterogeneous archives requiring curation.",
            "data_structure": "High-dimensional tabular parameter space derived from multi-band photometry and derived physical quantities (multi-dimensional, dense feature vectors; some features may be correlated or noisy).",
            "problem_complexity": "High: many features (high dimensionality), low S/N for some signals, strong feature correlations, curse of dimensionality challenges for exhaustive exploration.",
            "domain_maturity": "Mature domain (astronomy) with well-established observational pipelines and prior physical models, but data-driven classification problems and large-survey feature engineering remain active research areas.",
            "mechanistic_understanding_requirements": "High - interpretable feature selection is required to produce scientifically actionable insights about star-formation processes and to enable trust in downstream inferences.",
            "ai_methodology_name": "PHILab (Boruta shadow-features + Lasso regularization hybrid)",
            "ai_methodology_description": "Two-step hybrid wrapper/filter: (1) Boruta-style shadow features (randomly shuffled duplicates) are used with tree-based importance to classify features as relevant/rejected/undetermined; (2) undetermined features are resolved using Lasso (L1) regularization to shrink redundant features and finalize selection. Designed to recover weak but informative features while reducing dimensionality.",
            "ai_methodology_category": "Feature-selection hybrid (wrapper + regularized filter) supporting supervised learning pipelines",
            "applicability": "Applicable and appropriate for extremely high-dimensional catalog data where weak signals are scientifically important; requires downstream supervised models for final classification.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Qualitatively effective: preserves informative contributions including weak signals, drastically reduces dimensionality and accelerates downstream processing, and in some cases improves classification tradeoffs (purity vs completeness).",
            "impact_potential": "High potential to improve scientific discovery by ensuring completeness of feature selection, enabling faster model training, and surfacing subtle, physically meaningful features that could lead to new insights.",
            "comparison_to_alternatives": "Compared conceptually and empirically to PCA (which discards residual components as noise) and to plain Boruta/Lasso pipelines; PHILab aims to retain weak signals that PCA can miss and to resolve ambiguous features that Boruta alone leaves undetermined.",
            "success_factors": "Combining randomness-introduced shadow features to estimate significance with L1-based sparsity control captures both obviously and weakly informative features; stability benefits from explicit treatment of undetermined features.",
            "key_insight": "Hybridizing shadow-feature significance testing with L1 regularization enables more complete and reliable feature selection in very high-dimensional scientific parameter spaces, helping preserve weak but scientifically important signals while reducing dimensionality.",
            "uuid": "e2293.0",
            "source_info": {
                "paper_title": "Strengthening leverage of Astroinformatics in inter-disciplinary Science",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "SHAP",
            "name_full": "SHapley Additive exPlanations",
            "brief_description": "A game-theory-based XAI framework that attributes contributions of individual features (including main, interaction, and cumulative effects) to model predictions by approximating Shapley values over feature coalitions.",
            "citation_title": "A unified approach to interpreting model predictions",
            "mention_or_use": "use",
            "scientific_problem_domain": "Astrophysics — feature selection and interpretation for YSO classification and cosmological parameter analysis (Pantheon+SH0ES supernova data)",
            "problem_description": "Explain and quantify the information contribution of catalogue-derived features to supervised model performance and to evaluate robustness/insights in cosmological-model parameter spaces derived from supernova light curves.",
            "data_availability": "Large published datasets (Hi-GAL clump catalogues, Pantheon+SH0ES supernova compilation) with many derived features; labels/targets exist for classification/regression tasks but may have uncertainties.",
            "data_structure": "High-dimensional tabular data (catalogue features derived from photometry/light curves), with feature interactions and correlations.",
            "problem_complexity": "High: many interacting features, nonlinear relationships, and potential subtle interaction effects that impact cosmological inferences/classification.",
            "domain_maturity": "Astrophysics/cosmology is mature with established models (e.g., ΛCDM) but data-driven model-comparison and feature interpretation are active and evolving research areas.",
            "mechanistic_understanding_requirements": "High - explainability is critical to trust model-driven scientific claims and to derive physical interpretation of feature importance, particularly in cosmology.",
            "ai_methodology_name": "SHAP (Shapley value-based explanations applied to ML feature selection)",
            "ai_methodology_description": "Computes approximate Shapley values by comparing model outputs across all feature coalitions (or approximations thereof) to estimate per-feature contributions at both global and per-sample levels; provides main effects, interaction effects, and cumulative importance for each feature to support explainable feature selection.",
            "ai_methodology_category": "Explainable AI (post-hoc interpretability method) used in conjunction with supervised ML models",
            "applicability": "Highly applicable for scientific contexts where interpretability is required; can be used to improve feature selection reliability and to highlight interactions relevant for scientific interpretation.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Improved transparency and trustworthiness of feature importance estimation; enabled identification of potentially informative parameters for alternative cosmological models and supported improved classification when used for feature reduction.",
            "impact_potential": "Large — enables model-agnostic, interpretable identification of features and interactions, facilitating hypothesis generation and prioritization for follow-up observations or experiments.",
            "comparison_to_alternatives": "Compared to Boruta/PHILab and PCA-based approaches: SHAP provides richer explainability (per-sample and interaction effects) and clearer interpretability than tree-based importance or PCA which may discard weak but informative residuals.",
            "success_factors": "Ability to produce per-sample and interaction-aware importance scores; model-agnostic formulation that works with common ML engines and reveals subtle, domain-relevant contributions.",
            "key_insight": "Game-theoretic, per-sample explainability (SHAP) reconciles powerful ML performance with the scientific need for interpretable, interaction-aware feature attribution, improving both trust and discovery in data-driven analyses.",
            "uuid": "e2293.1",
            "source_info": {
                "paper_title": "Strengthening leverage of Astroinformatics in inter-disciplinary Science",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "DeepFocus (MAML-based)",
            "name_full": "DeepFocus (Model-Agnostic Meta-Learning framework for radio interferometry deconvolution)",
            "brief_description": "A MAML-based, MLE-optimized meta-learning framework that searches model/optimizer hyperparameter space in a double loop to reconstruct true sky images from radio interferometry dirty cubes, exploiting HPC/GPU acceleration for massive model-space search.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "Radio astronomy — interferometric image deconvolution (ALMA data cubes)",
            "problem_description": "Reconstruct the true sky brightness distribution from observed 'dirty' interferometric data cubes (large inverse problem) to detect and characterize astronomical sources across massive 3D data cubes.",
            "data_availability": "Large-scale data: example test set of ~1000 ALMA data cubes, each ~100 GB (total ~100 TB); cubes have ~10 million spatial features and ~10k frequency channels; labeled training targets exist for supervised evaluation (simulated or annotated reconstructions).",
            "data_structure": "Very high-dimensional 3D data cubes (2 spatial + spectral/frequency), dense, large-scale arrays with complex noise and instrument response (PSF/dirty beam) structure.",
            "problem_complexity": "Extreme: large-scale inverse problem with nonlinearities, high dimensionality (10M+ features per cube), strong ill-posedness, computationally expensive traditional pipelines, and requirement to model instrumental effects and noise.",
            "domain_maturity": "Mature domain (radio interferometry) with well-established physical pipelines (e.g., TCLEAN) and priors, but computational methods and ML-driven alternatives are active research areas.",
            "mechanistic_understanding_requirements": "Medium — scientific inference requires reliable, explainable reconstruction to trust detections, but some black-box ML components are acceptable if validated thoroughly against physical priors and baselines.",
            "ai_methodology_name": "DeepFocus (MAML + Maximum Likelihood model-space optimization)",
            "ai_methodology_description": "Meta-learning MAML double-training loop where an inner loop trains feature-extraction encoder on an input cube to approximate the sky model (reconstruction task), and an outer loop reuses encoder features with a classifier to detect sources; a MLE-based loss guides search across model architectures and optimizer hyperparameters, massively parallelized on GPUs to find architectures that generalize quickly across cubes.",
            "ai_methodology_category": "Meta-learning (MAML) applied to supervised deep learning and model architecture/hyperparameter optimization",
            "applicability": "Highly applicable and tailored for large interferometric inverse problems with limited per-task data and high heterogeneity across cubes; required modifications include meta-learning setup and MLE-driven model-space exploration plus HPC acceleration.",
            "effectiveness_quantitative": "Processing speed: ability to process ~100 TB in 9 minutes versus ~5 hours for the TCLEAN baseline (≈33× speedup on the reported test campaign of 1000 cubes); classification precision improved relative to TCLEAN and other baselines (exact precision numbers not reported).",
            "effectiveness_qualitative": "Qualitatively strong: drastically reduced processing time and improved detection/classification precision compared to the traditional TCLEAN pipeline and several other ML/DL methods; demonstrated ability to find effective model/hyperparameter combinations via meta-learning.",
            "impact_potential": "High — enables orders-of-magnitude faster processing of massive radio datasets, reduces computational costs, scales to future large-survey workloads (ALMA/ALMA2030-era), and potentially improves scientific throughput and discovery rate.",
            "comparison_to_alternatives": "Direct comparison to TCLEAN (traditional deconvolution pipeline): DeepFocus processed data much faster (9 minutes vs 5 hours) and improved classification precision; also compared favorably to other ML/DL methods in the literature (specific numeric comparisons not provided).",
            "success_factors": "Meta-learning (MAML) enabling rapid adaptation across tasks, MLE-guided model-space exploration to find robust architectures, and massive GPU/HPC resources to perform the search efficiently.",
            "key_insight": "Meta-learning with model-space optimization and HPC acceleration can produce ML models that both outpace traditional physics-heavy pipelines in speed and improve reconstruction/detection performance on extremely large, ill-posed inverse problems.",
            "uuid": "e2293.2",
            "source_info": {
                "paper_title": "Strengthening leverage of Astroinformatics in inter-disciplinary Science",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "AMBER",
            "name_full": "AMBER - Advanced SegFormer for Multi-Band Image Segmentation",
            "brief_description": "A customized Vision Transformer (ViT)/SegFormer architecture extended to 3D patches (spectral dimension) with a hierarchical transformer encoder and lightweight All-MLP decoder, tailored for semantic segmentation of multi-band/hyperspectral images.",
            "citation_title": "AMBER -Advanced SegFormer for Multi-Band Image Segmentation: an application to Hyperspectral Imaging",
            "mention_or_use": "use",
            "scientific_problem_domain": "Remote sensing / Geophysics / Hyperspectral Imaging; also applicable analogously to astrophysical multispectral imaging and biomedical spatial transcriptomics",
            "problem_description": "Pixel-wise semantic segmentation of hyperspectral, multi-band images to classify regions/material types (e.g., land cover, geological composition) using rich spectral-spatial information.",
            "data_availability": "Moderate-sized benchmark datasets (Indian Pines, Pavia University, PRISMA) with labeled segmentation masks; data are labeled but dataset sizes are modest compared to natural-image corpora.",
            "data_structure": "3D image volumes: spatial (H×W) with spectral depth (D) per pixel — i.e., multi-band/hyperspectral data (high spectral dimensionality, spatial structure preserved).",
            "problem_complexity": "Moderate-to-high: high spectral dimensionality, need to capture spectral-spatial correlations, multi-scale features; models must balance capacity and overfitting on limited labeled examples.",
            "domain_maturity": "Evolving: HSI analysis moved from classical spectral algorithms to DL (CNNs, ViTs); transformer-based models are recent and rapidly progressing.",
            "mechanistic_understanding_requirements": "Low-to-medium — segmentation performance is primary; interpretability is useful for scientific trust but black-box DL is often acceptable if validated against known labels/ground truth.",
            "ai_methodology_name": "AMBER (3D-extended SegFormer/ViT with All-MLP decoder)",
            "ai_methodology_description": "Hierarchical transformer encoder extracts multi-scale spatial-spectral features using 3D patching to preserve spectral relationships; lightweight All-MLP decoder fuses multi-level features and reduces spectral dimension to produce 2D semantic segmentation masks; architecture preserves spatial resolution at output, trades higher parameter count for accuracy.",
            "ai_methodology_category": "Supervised deep learning — transformer-based semantic segmentation adapted to 3D/multi-band inputs",
            "applicability": "Highly applicable for hyperspectral and other multi-band image segmentation tasks; requires adaptation (3D convolutions and spectral-preserving attention) to capture spectral information effectively.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Outperformed traditional CNN-based methods on Indian Pines and Pavia University (Overall Accuracy, Kappa, Average Accuracy) and achieved state-of-the-art on PRISMA in preliminary experiments; preserves spatial resolution and captures spectral features more effectively than baseline CNNs.",
            "impact_potential": "Significant — improved segmentation quality for hyperspectral applications across geophysics, planetary science, and biomedical imaging, enabling better material mapping and feature detection from multi-band sensors.",
            "comparison_to_alternatives": "Compared to traditional CNN-based segmentation models and SegFormer variants: AMBER performed better on benchmark HSI datasets due to specialized 3D patch attention and spectral-preserving design.",
            "success_factors": "Extending self-attention to 3D patches to capture spectral-spatial features, hierarchical feature extraction, and judicious decoder design that fuses multilevel features while preserving spatial detail.",
            "key_insight": "Adapting transformer-based architectures to handle spectral depth (3D patches) and preserving spatial resolution yields superior segmentation performance on multi-band hyperspectral images compared to standard 2D CNN approaches.",
            "uuid": "e2293.3",
            "source_info": {
                "paper_title": "Strengthening leverage of Astroinformatics in inter-disciplinary Science",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Meta-learning (MAML)",
            "name_full": "Model-Agnostic Meta-Learning (MAML) / Meta-learning paradigms",
            "brief_description": "Meta-learning approach that trains models to provide initial parameters that can be rapidly fine-tuned to new tasks with few optimization steps and few data; MAML is model-agnostic and applicable across architectures.",
            "citation_title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
            "mention_or_use": "mention",
            "scientific_problem_domain": "General scientific ML problems with scarce labeled data per task (examples: rare/faint-source detection in astrophysics, few-shot classification in other domains)",
            "problem_description": "Enable models trained on a distribution of tasks to adapt quickly to new, related tasks where labeled data are scarce, reducing the need for large labeled datasets per new task.",
            "data_availability": "Specifically intended for settings with limited labeled data per task (few-shot), but requires a set of related tasks for meta-training; task diversity during training is important.",
            "data_structure": "Varies by application — can be images, spectra, time-series, tabular; meta-learning is agnostic but often used with high-dimensional inputs.",
            "problem_complexity": "Moderate-to-high: requires optimization across tasks (double loop training), computationally intensive meta-training, and careful selection of task distribution.",
            "domain_maturity": "Growing/active research area with increasing application in scientific domains where data labeling is costly; not yet standard practice across all fields but promising.",
            "mechanistic_understanding_requirements": "Medium — interpretability of learned initialization is less emphasized; primary requirement is rapid adaptation and reliable generalization to new tasks.",
            "ai_methodology_name": "MAML / meta-learning",
            "ai_methodology_description": "Two-level training: an outer loop optimizes initial model parameters such that, for a sampled task, a small number of inner-loop gradient updates on few-shot training data yields good task-specific performance; applicable across model families and used to accelerate architecture/hyperparameter search in frameworks like DeepFocus.",
            "ai_methodology_category": "Meta-learning / few-shot supervised learning",
            "applicability": "Particularly applicable where labeled data per new task are scarce and tasks are related (e.g., multiple interferometric cubes, multiple imaging domains); requires meta-training tasks and substantial compute for meta-optimization.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Demonstrated to substantially reduce per-task training iterations and enable strong generalization from few examples; used effectively within DeepFocus to speed adaptation across radio cubes.",
            "impact_potential": "High for domains with many related but individually scarce-data tasks; reduces labeling burden and enables transfer of learned priors across experiments.",
            "comparison_to_alternatives": "Compared conceptually to transfer learning: MAML focuses on learning initializations for rapid adaptation from few examples, whereas transfer learning typically relies on large pretraining and fine-tuning and may require much more data.",
            "success_factors": "Availability of a representative set of meta-training tasks, appropriate inner/outer loop optimization schedules, and sufficient compute for meta-training.",
            "key_insight": "Meta-learning (MAML) enables efficient adaptation to new scientific tasks from very limited labeled data by training for adaptability rather than for a single final-task optimum, making it well-suited to many scarce-data scientific problems.",
            "uuid": "e2293.4",
            "source_info": {
                "paper_title": "Strengthening leverage of Astroinformatics in inter-disciplinary Science",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "SOM clustering",
            "name_full": "Self-Organizing Map (SOM) clustering",
            "brief_description": "An unsupervised neural-network-based clustering and dimensionality-reduction method that maps high-dimensional data onto a low-dimensional (usually 2D) grid preserving topological relationships, used here to group galaxies by redshift properties.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "Astrophysics — comparison and visualization of spectroscopic vs photometric redshift coverage in large galaxy surveys",
            "problem_description": "Group thousands of galaxies in multi-band photometric and spectroscopic parameter spaces to compare coverage, identify regions lacking spectroscopic calibration, and facilitate calibration strategies.",
            "data_availability": "Large catalogues with multi-band photometry and spectroscopic redshifts for subsets; abundant unlabeled or weakly labeled data for clustering and visualization.",
            "data_structure": "High-dimensional tabular features derived from multi-band photometry and spectroscopic measurements (dense vectors per object).",
            "problem_complexity": "Moderate: large number of samples with moderate-to-high feature dimensionality; need to preserve topological structure for interpretation.",
            "domain_maturity": "Mature in astronomy for exploratory analysis; SOMs are established unsupervised tools for large-catalog visualization and calibration assessment.",
            "mechanistic_understanding_requirements": "Medium — interpretability of cluster topology is important for calibration planning and understanding mapping between photometric and spectroscopic space.",
            "ai_methodology_name": "Self-Organizing Map (SOM)",
            "ai_methodology_description": "Unsupervised neural-map algorithm that projects high-dimensional feature vectors onto a 2D grid; similar objects map to nearby nodes, enabling visualization of distributions (e.g., median spectroscopic vs photometric redshift maps).",
            "ai_methodology_category": "Unsupervised learning / clustering / dimensionality reduction",
            "applicability": "Well-suited for exploratory analysis and visualization of large survey parameter spaces; effective for highlighting regions for follow-up and calibration.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Effective at grouping and visualizing large galaxy samples by redshift-related features, facilitating comparison between spectroscopic and photometric coverage.",
            "impact_potential": "Useful for survey calibration strategy, identifying spectroscopic gaps, and guiding targeted observations to improve photometric redshift performance.",
            "comparison_to_alternatives": "Provides topology-preserving 2D visualization compared to other clustering/DR methods (e.g., t-SNE, UMAP); the paper uses SOM as a practical tool rather than a benchmark comparison.",
            "success_factors": "Large sample sizes, meaningful input feature set (multi-band photometry and redshifts), and the topological preservation property of SOM facilitating intuitive visual interpretation.",
            "key_insight": "Topology-preserving clustering methods like SOM enable intuitive visualization and direct comparison of coverage between spectroscopic and photometric parameter spaces in large surveys, aiding calibration and survey planning.",
            "uuid": "e2293.5",
            "source_info": {
                "paper_title": "Strengthening leverage of Astroinformatics in inter-disciplinary Science",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Nonlinear PCA (autoencoder)",
            "name_full": "Nonlinear Principal Component Analysis via Autoencoder",
            "brief_description": "A nonlinear dimensionality-reduction approach implemented with an autoencoder neural network (nonlinear PCA) used as a preprocessing step for clustering in high-resolution spatial transcriptomics data.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "Biomedicine — high-resolution spatial transcriptomics (Open-ST pipeline) for tissue/cell clustering and disease detection",
            "problem_description": "Discover and cluster cellular/tissue structures in spatial transcriptomics images to identify disease-related regions and biomarkers by reducing dimensionality while preserving nonlinear structure.",
            "data_availability": "Emerging datasets from high-resolution spatial transcriptomics platforms; data are rich but labeled examples (disease annotations) may be limited.",
            "data_structure": "High-dimensional spatially-resolved gene expression profiles and multi-channel image data (multimodal: spatial + transcriptomic), with complex nonlinear relationships.",
            "problem_complexity": "High: large feature dimensionality, nonlinear interactions among genes/spatial context, requirement to reveal subtle tissue patterns indicative of disease.",
            "domain_maturity": "Emerging-to-maturing field: transcriptomics data analysis is established, but high-resolution spatial methods and DL-based pipelines are rapidly developing.",
            "mechanistic_understanding_requirements": "High — biomedical interpretability is important to relate clusters to biological functions and disease mechanisms; downstream explainability and validation required.",
            "ai_methodology_name": "Autoencoder-based nonlinear PCA",
            "ai_methodology_description": "Train an autoencoder to compress high-dimensional spatial transcriptomics/multi-channel image data into a lower-dimensional latent space (nonlinear principal components), then apply clustering on latent representations to discover cell/tissue clusters.",
            "ai_methodology_category": "Unsupervised learning / nonlinear dimensionality reduction (autoencoder) + clustering",
            "applicability": "Appropriate for multimodal spatial transcriptomics where nonlinear structure exists and labels are scarce; serves as an effective preprocessing step for downstream clustering and disease-detection pipelines.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Enabled meaningful clustering of cells in tissue frames, facilitating discovery of potential disease regions and supporting analogies between astrophysical multispectral analysis and biomedical spatial profiling.",
            "impact_potential": "High cross-disciplinary impact: techniques developed in astrophysical multispectral analysis transfer to biomedical spatial profiling, improving tissue imaging interpretation and biomarker discovery.",
            "comparison_to_alternatives": "Nonlinear autoencoder offers advantages over linear PCA by capturing nonlinear manifolds in the data and enabling more informative clustering for complex biological patterns.",
            "success_factors": "Ability of autoencoders to capture nonlinear relationships and spatial context, plus careful design of latent dimensionality and clustering pipeline tuned to biological signal scales.",
            "key_insight": "Nonlinear dimensionality reduction via autoencoders effectively captures complex, spatially-structured biological signals and can be repurposed across domains where multispectral/multimodal data exhibit nonlinear manifolds.",
            "uuid": "e2293.6",
            "source_info": {
                "paper_title": "Strengthening leverage of Astroinformatics in inter-disciplinary Science",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Constructing Impactful Machine Learning Research for Astronomy: Best Practices for Researchers and Reviewers",
            "rating": 2,
            "sanitized_title": "constructing_impactful_machine_learning_research_for_astronomy_best_practices_for_researchers_and_reviewers"
        },
        {
            "paper_title": "A unified approach to interpreting model predictions",
            "rating": 2,
            "sanitized_title": "a_unified_approach_to_interpreting_model_predictions"
        },
        {
            "paper_title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
            "rating": 2,
            "sanitized_title": "modelagnostic_metalearning_for_fast_adaptation_of_deep_networks"
        },
        {
            "paper_title": "3D detection and characterization of ALMA sources through deep learning",
            "rating": 2,
            "sanitized_title": "3d_detection_and_characterization_of_alma_sources_through_deep_learning"
        },
        {
            "paper_title": "AMBER -Advanced SegFormer for Multi-Band Image Segmentation: an application to Hyperspectral Imaging",
            "rating": 2,
            "sanitized_title": "amber_advanced_segformer_for_multiband_image_segmentation_an_application_to_hyperspectral_imaging"
        },
        {
            "paper_title": "Data deluge in astrophysics: Photometric redshifts as a template use case",
            "rating": 1,
            "sanitized_title": "data_deluge_in_astrophysics_photometric_redshifts_as_a_template_use_case"
        },
        {
            "paper_title": "Data-Rich Spatial Profiling of Cancer Tissue: Astronomy Informs Pathology",
            "rating": 1,
            "sanitized_title": "datarich_spatial_profiling_of_cancer_tissue_astronomy_informs_pathology"
        }
    ],
    "cost": 0.02120075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Strengthening leverage of Astroinformatics in inter-disciplinary Science
August 30, 2024</p>
<p>M Brescia 
G Angora </p>
<p>Department of Physics 'Ettore Pancini'
University Federico II
Via Cintia, 21I-80126NapoliItaly</p>
<p>INAF -Astronomical Observatory of Capodimonte
Salita Moiariello 16I-80131NapoliItaly</p>
<p>Strengthening leverage of Astroinformatics in inter-disciplinary Science
August 30, 2024A6DF833FEF994D09A2EB8C564DF5F8EBartificial intelligencemachine learningastrophysicsstatisticsdata science
Most domains of science are experiencing a paradigm shift due to the advent of a new generation of instruments and detectors which produce data and data streams at an unprecedented rate.The scientific exploitation of these data, namely Data Driven Discovery, requires interoperability, massive and optimal use of Artificial Intelligence methods in all steps of the data acquisition, processing and analysis, the access to large and distributed computing HPC facilities, the implementation and access to large simulations and interdisciplinary skills that usually are not provided by standard academic curricula.Furthermore, to cope with this data deluge, most communities have leveraged solutions and tools originally developed by large corporations for purposes other than scientific research and accepted compromises to adapt them to their specific needs.Through the presentation of several astrophysical use cases, we show how the Data Driven based solutions could represent the optimal playground to achieve the multi-disciplinary methodological approach.</p>
<p>Introduction</p>
<p>Let's start from an assumption: all sciences, and in particular Astronomy, are experimenting a drastic change of research perspective, in which the huge amount of data produced by modern telescopes and instruments are extremely heterogeneous and complex, causing the need to tackle the problem of processing them in an efficient way.That is why one of the main constraints is the parallel evolution of computing technology.We can compare a modern astrophysical survey to a Google search engine, in the sense that, like with Google, "research begins and ends with the survey itself!".Such a close parallel between Google and actual large surveys in astronomy refers to the modern survey properties: scale, quality, and richness of collected information.Scale, because we entered the era where we can observe and catalogue the entire sky.Quality, since source catalogues are as precise as the measurements taken with "pointed" observations.Richness, because those catalogues contain not only positions and fluxes, but also shapes, profiles, and temporal behaviour of the objects.This is what makes large surveys not just bigger but better.</p>
<p>However, the real revolutionary aspect conditioning the science research evolution is that Astrophysics, like most all human sciences, is now facing the Big Data regime.Behind this, there is the awareness that the produced data volume and rate exceeds the capability to exploit and analyze them, thus causing data to become the driving engine of modern research.We like to remember that already 12 years ago, we were one of the few astrophysics research groups to publicly warn about the need to change strategy in the way of doing research, suggesting exchanging analysis models in large collaborations rather than data among the various research groups geographically spread across different countries 1,2 .At the time, we were almost ignored, especially within our national scientific community.Moreover, it is well known today that scientific research has changed its modus operandi.As shown in a recent research, we spend about 82% of our time dealing with data preparation, cleaning, understanding, cross-correlating, etc. (Fig. 1), well conscious that the quality of our scientific applications depends upon data accessibility, fusion, quality analysis, and durability when we want to apply Artificial Intelligence (AI) model to innovative problems on distributed heterogenous archives.</p>
<p>Formally, a Big Data domain is defined as an environment characterized by the so-called 8-V rule: Volume, Value, Veracity, Visualization, Variety, Velocity, Validity, and Variability 3 .Given its intrinsic nature, it is evident that Astrophysics perfectly matches such a definition.</p>
<p>A Big Data environment also has several critical properties fundamentally rooted in the nature of data itself.Within this context, a datum can be defined as a piece of information, a reference point of a scale or operation, an assumed or known entity, a factual basis for knowledge inference, or an element for reasoning.These definitions can be collectively encapsulated in a single word: a datum is a "solution".That is, each datum is a solution to its domain problems; thus, in a Big Data regime, we have an order of ∼ 10 20 solutions, each one in a multi−D parameter space.And if we think that in the observable Universe, the order of galaxies should be ∼ 10 12 , the conclusion is around the corner.Therefore, if we can collect enough data in a hyper-dimensional parameter space, we will obtain a statistically consistent model for that environment.This means that all the questions concerning the research in that field are already present in the collected volume of data.The problem is just to find them!The immediate consequence is that a Big Data domain requires enough computing and storage power to perform an efficient data cross-correlation and a massive exploitation of self-adaptive analysis mechanisms, able to automate the data exploration process, or, in other words, AI learning paradigms.</p>
<p>ICT infrastructure is needed</p>
<p>Over the last 40 years, research in Astrophysics has undergone a gradual evolution comparable to that of particle physics and many other fields, starting from mediumsmall scale projects (e.g.SDSS), characterized by a single set of instrumentation, considered as cutting-edge technology at the time, by relatively low budgets with a medium-high risk rate, and with the possibility of easily introducing new AI methodologies, but soon limited by the growing need for computational power.Within two decades, large-scale international collaborations (such as ESA Euclid, Rubin LSST, JWST, ELT, and SKA, to quote just a few) centered on big science experiments (several hundred participants), with large budgets and a lower risk rate, but based on hardware and software technologies frozen at an early stage and computational infrastructures on a national scale, managed in the form of large data centers that were not particularly flexible or easily innovative.These are all projects that cover the entire lifetime of a researcher, where AI, Machine Learning (ML), and Deep Learning (DL) methodologies act mainly as support at all levels and where large volumes of data require a long-term maintenance cycle, with a large fraction of the costs to be invested in data preservation and curation.</p>
<p>Therefore, right from the start, computation and storage problems emerge around the big science initiatives, where hardware plays a central role with almost 90% of the total cost.Fortunately, technological evolution and new energy policies make hardware a commodity that is easily available, providing an underlying infrastructure capable of focusing attention on investments in data management and efficient software production.Naturally, a relevant aspect is that of sustainable consumption.It is well known that maintaining the data and the hardware/software infrastructures produces yearly more CO2 than the entire airline industry.Just think that training one medium-sized AI model consumes about 300 kilos of CO2, while a Large Language Model reaches almost 10 tons of CO2 4 .This implies that a Tier-II data center consumes more energy than that needed to power an urban core of 50 thousand apartments, although it is still cheaper than 10 smaller data centers (the so-called economy of scale) 5 .This is why the new mantra a of the European Union is "minimize the proliferation of data computing centers and create sustainable models of computing infrastructures for the management of large volumes of data".</p>
<p>In such a scenario, once again, Astroinformatics and, more generally, Xinformatics can play a fundamental role, offering a level of knowledge and flexibility capable of combining the maximum and efficient exploitation of computational resources with the ability to manage and synthesize the analysis of massive amounts of data, maximizing scientific production.The guiding element of the new computing data centers must be the multi-disciplinary management by data scientists, specialized in the different scientific domains (bio, astro, geo, chemio, etc.) to which the hosted data refer.The reasons are many.First, it is unnecessary to rediscover the wheel every time.In data-intensive projects, there are often many repeated patterns with respect to different scientific disciplines, so it is advisable to turn these patterns into processes that can be replicated.Furthermore, large data sets are difficult to aggregate, so for multi-disciplinary data centers, it is preferable to exploit the co-location mechanism, which would be impossible for exclusively astronomical or generally mono-thematic data centers, given that the ontologies and integrated data models are complex combinators.The co-location principle allows the optimization of the aforementioned mechanism of a sustainable model for massive data storage over many decades and computing infrastructures.Of course, by respecting the constraints of the need to find the right balance between HPC and HTC resources, to guarantee active support for the creation of new AI-based models and therefore to recognize the costs related to the design and production of AI solutions as a significant portion of the entire big science project.</p>
<p>That is why we profited from the recent "Next Generation EU" funding program of the European community, to provide a state-of-the-art data computing center to make our multi-disciplinary community able to exploit the incoming tsunami of data, in particular for Astrophysics collected by a variety of survey projects (to quote just a few ELT, SKA, Rubin-LSST, VST).This new infrastructure, named AD-HOC (Astrophysical Data HPC Operation Center) and part of the Italian project STILES b (Strengthening the Italian leadership in ELT and SKA), will be operative from the end of 2024, offering a TIER-II data center reaching two petaflops of computing capability, equipped by both HPC and HTC server clusters, together with an overall data storage capacity of about eighteen Petabytes, between solid-state disks and long-term archiving tape libraries 6 .Among the various services foreseen, it is the candidate to be the official archiving and processing system for specific survey projects, like VST.</p>
<p>b https://pnrr.inaf.it/progetto-stiles/</p>
<p>AI as the engine of data-driven Science</p>
<p>Nowadays, AI is recognized as an unavoidable approach in all human sciences.However, mainly in the academic and basic research world, in many cases, the enormous vastness and variety of software resources and solutions based on AI and machine learning available on the web, combined with the widespread indiscriminate and often only partially conscious use of such resources, have begun to highlight not only the merits but also the limits and critical issues of such methodologies.The strong pressure exerted on experts in the various scientific domains by the ever-increasing role of data science experts involved in large scientific collaborations has certainly contributed to amplify this effect.This, rather than encouraging synergy between heterogeneous scientists, has caused the triggering of a paroxysmal race among colleagues and members of the same project to obtain barely publishable results in a short time, forcing the use of AI techniques and exponentially increasing their complexity to the point of losing control, making them almost incomprehensible and difficult to interpret and to become trustable.The tendency is, therefore, that the primary objective of doing the best possible science, the central concept on which the AI approach is based, i.e. data-driven science, as well as the spirit of cooperation between domain and AI experts are failing.In practice, the key objective of improving knowledge in the scientific field does not represent, in many cases, the most important target of research, worrying only about pursuing subjective goals linked to personal prestige or acquiring research funds.This distortion of the scientific perspective is particularly and dangerously present in Italy, where national support for basic research is chronically insufficient.The final result is that AI has become a land of conquest with little awareness and without rules.But all that glitters is not gold.The AI approach presents various critical issues that should be considered before deciding to use it in any scientific field.In particular, the concept of data-driven science is often forgotten or underestimated.The in-depth critical analysis and knowledge of the data underlying any experiment should be the key element to focus attention most of the time, especially before deciding to employ any machine learning-based model.</p>
<p>For example, in Astrophysics, the massive presence of AI is demonstrated by the exponential increase in scientific production (Fig. 2).Consequently, there is an increasing need for an efficient set of rules to build and use this methodology efficiently and to report their results as well 7 .As known, in the last decade, it became a common practice to engage data challenges within large survey programs to find the best solutions to analyze data [8][9][10][11][12][13] .But most suffer from inhomogeneous rules and objective metrics to validate results 14 .</p>
<p>In principle, four main criteria should be followed by any AI approach in human science: (i) Reproducibility, in the sense that the results should be invariant w.r.t.data training/prediction choice and code/data made publicly accessible; (ii) Interpretability, because methods should be explainable and transparent in terms of both pipeline and data flow process; (iii) Accuracy of methods, since they should provide accurate results, i.e. unbiased, uniform and rigorous in terms of their validation; (iv) Flexibility, for which AI solutions and approaches should demonstrate general usefulness and non-ambiguous advantages w.r.t.traditional approaches.</p>
<p>Unfortunately, some of these criteria conflict with each other.For example, for their intrinsic design aspects, the degree of flexibility of Machine and Deep Learning models is inversely proportional to their level of interpretability, as shown in Fig. 3.</p>
<p>In practice, simpler methods, like linear models, cannot be adaptable to different tasks despite their intrinsic transparent mechanisms.On the opposite, the recent Fig. 4. Trade-off between accuracy and interpretability for AI based models 16 .</p>
<p>generative models showed a very high capability to solve different complex problems but with poor levels of interpretability.A trade-off should be found, trying to follow what the data themselves suggest and, as much as possible, simpler in the solving approach, increasing the complexity of the model only if needed.Anyway, as easy to understand, this trade-off is extremely problem-dependent.</p>
<p>Moreover, as we are assisting worldwide in many contexts, the debate on the ethics of generative AI is controversial and often characterized by poor objectivity and misunderstandings.The main reason is that it appears extremely accurate in solving very hard problems but at the price of a very low capability for a human being to understand how it works and how to keep it under control.The EU recently published a GDPR law, known as the AI Act c , focusing on these aspects and trying to regulate its evolution and diffusion.But, as shown in Fig. 4, also, in this case, there is an inverse proportionality between accuracy and explainability.</p>
<p>To be trusted and accepted by a scientific community, the internal behaviour of a model should be clearly understood.Recently a new branch of research in AI arisen exactly to introduce well-posed mechanisms and rules able to increase the explainability of machine learning models, the XAI or eXplainable AI.Several methods have been recently proposed 16 , and in particular, one of them, SHAP, includes an efficient and reliable way to make the feature selection problem fully explainable and therefore trustable 17 .</p>
<p>c https://www.euaiact.com/</p>
<p>Parameter space explainable optimization</p>
<p>As known, in a data-driven context, one of the most important aspects to consider in a scientific problem is a deep exploration and analysis of the given parameter space on which the available data are mapped.By definition, ML is based on selfadaptive techniques.A priori, real-world data are intrinsically carriers of embedded information hidden by noise.In almost all cases, the signal-to-noise (S/N) ratio is low, and the amount of data prevents human exploration.We don't know which feature of a pattern is carrying useful information and how and where the correlation of features gives the best solution knowledge.In Astrophysics, this particularly means that before approaching any AI solution, a massive investigation should be dedicated to understanding the degree of correlation among all data features and the level of information contribution carried by each single dimension.But almost always, this means efficiently analyzing a high dimensional parameter space characterized by many features, thus subject to the well-known problem of the curse of dimensionality 18 .Principal Component Analysis (PCA) 19 , the most widely used method, is very simple but often inefficient.In particular, after having extracted the principal trends of data distribution, all the rest is considered as noise contribution, thus causing a lack of residual information that, in several complex domains, could be crucial to fully exploit the benefits of the feature selection to optimize the solution approach and to infer new knowledge on the underlined physical phenomena.Namely, rejected components may hide a weak but still relevant contribution to better understanding data.</p>
<p>There is an alternative method to PCA.A few years ago, we proposed a hybrid method named PHILab (Parameter Handling Investigation Laboratory), mixing two existing models and joining two simple concepts, for instance, shadow features and Lasso regularization, successfully validated on particularly complex and multidimensional astrophysical use cases [20][21][22] .A shadow feature, proposed by the Boruta method 23 , is a copy of a real one, present in the original parameter space, built by randomly shuffling its values.This corresponds to introducing quotes of random noise into the given dataset.Lasso regularization 24 , based on the L1-norm additive term added to any loss function, is able to shrink the informative correlation between features, making them candidates to be rejected from the parameter space, as being considered redundant because they already sufficiently represented by other features.Therefore, Boruta can classify all real features as relevant, rejected, or undetermined.The undetermined features are submitted to the Lasso analysis to make their final assignment.The real advantage of this method is its demonstrated reliability in finding the full set of features carrying an informative contribution, however weak, to the problem solution, thus improving the completeness of the feature selection process and potentially engaging serendipity.</p>
<p>Although the cited approaches can improve the quality of the feature selection, these methods are not fully explainable due to two main limitations: the intrinsic variability of the feature importance calculation obtained by the tree-based bag-ging/boosting models used as engines by the mentioned methods, as well as their low explainability of the selected parameter space, due to their high dose of randomness and statistical complexity.An XAI model, like SHAP, could solve both problems.</p>
<p>SHAP is a framework inspired by the game theory 17 , where the easy concept is that a parameter space feature may or may not be involved in the game.When involved, its contribution to the model performance is evaluated by comparing it with all the other features.However, since the order of participation of a set of features may be important, all possible combinations of features are compared to avoid such a bias.This full comparison among features and all the dataset samples provides the possibility to estimate at the same time three kinds of informative contributions for each feature: the cumulative effect of a feature with respect to the entire dataset, the main effect of a single feature for a given data sample, and the interaction effect for all pairs of features for a given data sample.Therefore, SHAP provides an easy and interpretable evaluation of the information contribution carried by each feature of the parameter space on its own and in correlation with all the others.At the same time, it also improves the feature selection reliability and the explainability of the internal estimation mechanisms (see Fig. 4).</p>
<p>Recent applications compare the feature selection methods mentioned above on astrophysical use cases characterized by large volumes of data represented through multi-dimensional parameter spaces.For example, the classification of the evolutionary stage of Young Stellar Objects (YSOs) through the characterization of the Hi-GAL (Herschel InfraRed Galactic Plane Survey) clumps parameter space, trying to infer new insights on the connection between the cold material reservoir present in clumps, traced by FIR/sub-mm emission, and the already formed YSOs, precursor of stars [25][26][27] When the classification performances are compared between the full original parameter space and that extracted by these methods (Fig. 5), a series of important outcomes of the feature selection can be outlined: (i) it does not lose information and by drastically reducing the dimensions of the problem it speeds up the process; (ii) in some cases it improves the classification quality, mostly in terms of the best tradeoff between purity and completeness; (iii) the SHAP mechanism is intrinsically fully explainable; (iv) the importance measured for the features may provide potentially interesting aspects to help the post-processing analysis of results in terms of starforming evolution.</p>
<p>Another example is the comparative study of different cosmological models on Pantheon+SH0ES data d , with the primary aim of evaluating the robustness of the ΛCDM model with respect to other dark energy models and investigating whether there are deviations that could indicate new cosmological insights.The data-driven approach through the various feature selection techniques (in particular Boruta and Fig. 5.An example of typical data-driven processing flow to perform the feature selection on a multi-dimensional parameter space 27 .</p>
<p>SHAP) allows us to evaluate the informative contribution of the cosmological and statistical parameters derived from the light curves to derive the supernova distance modulus.The resulting parameter space extracted from the feature selection models reveals potential improvements in alternative models to the ΛCDM, relevant in perspective for new observational campaigns, as in the case of the recent DESI survey 28,29 .These use cases demonstrate the effectiveness and the centrality of the data-driven approach, realized through feature selection models, to improve the cognitive inference on the related physical phenomena, combined with the possibility of ensuring the full explainability and transparency of the method.</p>
<p>Astroinformatics as a virtuous inter-disciplinary synergy template</p>
<p>Besides interpretability, two other important properties should characterize the use of AI within scientific research: accuracy and flexibility.Several examples could be done to verify these properties.The first is the strong parallelism between multispectral analysis in astrophysics and biomedicine.By thinking about the domain of interest of Astrophysics, we know that this is a science mostly oriented to describing and explaining the evolution of the Universe by recognizing and characterizing all celestial sources.We daily collect and use multi-resolution, multi-epoch, and multi-wavelength images and spectra, measuring several physical quantities from the extracted source catalogues, and we already demonstrated that this is done in a Big Data environment.Perfectly matching this modus operandi is the high- resolution spatial transcriptomics, a branch of biomedicine, where the same kinds of massive data, but referred to biological tissue cells (here we can consider cells as the galaxies of the biological universe), are analyzed to describe and explain the evolution of human being organisms and to recognize and characterize all involved and interacting cells.The well-established technique of RNA sequencing allows us to obtain information on the state of cells, tissues, and entire organisms at various time points in the form of gene expression profiles.Such in-depth knowledge is particularly useful in identifying pathologies and their evolution based on the type of treatment adopted and on particular environmental conditions.Therefore, strong similarities emerge between multispectral analysis in astrophysics and the emerging multiplexing platforms in biomedical research.The unstoppable sequential identification of tissue-based biomarkers makes this scientific field a big data environment.Therefore, it seems clear that image analysis techniques originally designed and tuned for problems in astrophysics can be applied to the biomedical field to improve tissue imaging methods, facilitating their efficiency and reliability of identification, prediction, and classification 30 .</p>
<p>In both scientific domains, the data are mostly based on multi-resolution, multiepoch, multi-band images.This strong parallelism automatically involves the same approach of AI methods.It can, therefore, give rise to synergies and collaborations between different and apparently distant scientific disciplines, whose main goal is to exploit the experience acquired in the parameter space analysis and classification and whose baseline is the exploitation of accuracy and flexibility properties of AI paradigms.For example, trying to be data-driven in the recognition of hidden structures within the two different parameter spaces, the clustering approach reveals strong analogies.</p>
<p>For instance, in Fig. 6, on the left, we can see the output of a Self Organizing Map (SOM) model, consisting of a distribution of dozens of thousands of galax- ies grouped in terms of median spectroscopic and 30-band photometric redshift, providing an effective way to compare the spectroscopic and photometric coverage of the parameter space 31 .On the right, a modified version of the clustering step of the Open-ST pipeline 32 , showing the distribution of different clusters of cells in a mouse lymph node tissue frame obtained after the application of a Nonlinear PCA (an autoencoder), propaedeutic to discover the potential presence of diseases along the tissue evolution analysis.In practice, different science domains with the same methodology and comparable reliability of results, thus probing the general accuracy and flexibility of the AI approach.</p>
<p>Another example is the strong parallelism between Astrophysics and Geophysics.We know that in Astrophysics, the integral field spectroscopy is particularly suitable to characterize the morphology of celestial sources (Fig. 7) by performing an accurate estimation of position and energy emitted by single photons 33 .</p>
<p>A similar kind of investigation can be performed in Geophysics by remote sensing techniques (Fig. 8) to analyze and characterize the chemical and geological composition of the Earth and other planets 35 .</p>
<p>In both cases, Hyperspectral Imaging (HSI) can provide a continuum spectrum of light, thus achieving an accurate estimation of the physical nature of the observed objects, because it records a continuum spectrum of light for each pixel and provides an invaluable source of information regarding the physical nature of the different object compositions, leading to the potential of a more accurate classification.The advent of Deep Learning has revolutionized the field of HSI, particularly from the introduction of Visual Transformer (ViTs) models 36 .A ViT may capture features at multiple scales and demonstrate strong generalization across different types of multi-band images.We recently proposed a customized version of these models, which extends the self-attention mechanism to 3D image patches, AMBER, an advanced SegFormer for Multi-Band Image Segmentation.</p>
<p>As shown in Fig. 9, it consists of two main modules: a Transformer encoder, Fig. 8. Operating principles of geophysical remote sensing 35 .</p>
<p>which generates both high and low-resolution features, and a lightweight All-Multi-Layer-Perceptron decoder (All-MLP) used to fuse multi-level features together.In particular, the final layer of the decoder reduces the dimensionality to generate a bi-dimensional semantic segmentation mask from a 3D input image.Unlike the traditional models, AMBER is tailored to multi-band images due to its 3D convolution.Moreover, the proposed model preserves the input spatial dimensions to maximize the accuracy at the price of a relative increase in the number of trainable parameters.</p>
<p>Preliminary experiments conducted on the Indian Pines, Pavia University, and PRISMA datasets show that AMBER outperforms traditional CNN-based methods in terms of Overall Accuracy, Kappa coefficient, and Average Accuracy on the first two datasets, achieving state-of-the-art performance on the PRISMA dataset 37 .In Fig. 10, there is an example of an application on remote sensing problems, where randomly extracted patches of the training images were used to test the capability of the SegFormer to recognize different kinds of regions, resulting better than other models proposed in the literature.</p>
<p>The critical optimization of the AI model</p>
<p>Another important aspect concerns choosing and optimizing the proper model for any particular scientific use case.Usually, an approach to problem-solving with AI should tackle two main aspects: the optimization trade-off between model and data domain parameter spaces and the computing efficiency in the problem-solving Fig. 9.The architecture of the AMBER ViT consists of two main modules: A hierarchical transformer encoder to extract spatial/spectral features and an All-MLP decoder to fuse these multilevel features, reducing the spectral dimension and predicting the semantic segmentation mask.D, H, W, and Ci represent the image deepness (spectral dimension), image height, width, and the features of the three-dimensional convolution, respectively 37 .Fig. 10.AMBER prediction on Pavia University dataset.From left to right: False-color map, Ground-truth map, AMBER prediction with the undefined mask, and the AMBER prediction without the undefined mask 37 .</p>
<p>heuristics.In fact, the optimization of the parameter space is only one aspect to take into account.Other equally important factors are the right choice of the best model, the optimization trade-off between its hyperparameters and the data feature space, and the heuristic search for the optimal AI architecture and depth.In all cases, it is always extremely data-dependent, which is why AI methods should be approached in a data-driven way.One efficient way to accelerate the optimization of the right model is through a special learning paradigm called meta-learning.This type of ML focuses more on the learning process than on the final result of any experiment (Fig. 11).In traditional machine learning, a model is trained on a specific dataset to perform a specific task.In meta-learning, a model is trained on a set of tasks to learn quickly how to adapt to new tasks in the future.</p>
<p>In analogy with how humans exploit the acquired experience to learn how to solve novel problems, a meta-learning process is based on a double training loop, where a model is trained on several (usually few) data for a series of tasks, and then it leverages its experience to improve its learning on novel tasks.For example, a model could be trained to reproduce input image data, and afterward, it can learn how to classify objects inside them.At first glance, this concept may appear already known to the eyes of a machine and deep learning expert since it seems very similar to the transfer learning paradigm 38 .However, it should be kept in mind that in the case of transfer learning, a pre-trained model requires a huge amount of data and a considerable computational capacity.Furthermore, such technique may suffer from some limitations in problems on specific scientific domains, such as looking for rare or faint astrophysical sources in very limited S/N conditions, where an equally large amount of labeled data should be needed to obtain good reliability 39 .</p>
<p>The meta-learning paradigm is particularly interesting for its ability to learn from a few data, adapt to new tasks, and its propensity to a high degree of generalization.These aspects make it particularly useful in specific scientific applications for which the acquisition of large amounts of available labeled data is expensive or difficult.In fact, a meta-learning process is essentially based on a first training level, followed by an adaptation level.In the first step, a model learns a preliminary set of general parameters on a wide set of tasks and then focuses on reusing the acquired experience to learn a specific set of parameters on a new task.</p>
<p>In particular, here we would like to focus on a promising optimization technique of meta-learning called Model Agnostic Meta Learning or MAML 40,41 .It is based on the idea of training a model to be easily adapted to new tasks with just a few optimization steps.This occurs in a model-agnostic way through an update of the initial parameters using a reduced number of training iterations with respect to the new task in the meta-learning phase.The intuition is that learning from an already sufficiently good initialization to improve the model preliminarily will allow us to obtain better efficiency in the generalization phase.The MAML approach is extremely versatile and usable for various heterogeneous problems.It is also applicable to almost all main machine learning paradigms, and it is able to adapt to any type of deep learning model.</p>
<p>For example, the MAML paradigm was recently applied to a particularly complex problem in astrophysics, for instance, the radio interferometry deconvolution problem, or the reconstruction of the true sky from the observed one on massive data cubes (a sample of 1000 cubes, each one with a size of about 100 GB, 10 M spatial features and 10 K frequency channels) referred to the ALMA project data [42][43][44] .As a usual practice in AI, the best model should be found by performing a computingintensive campaign to find the optimal setup of the model.This process depends on the number of internal hyperparameters directly dependent on the model's complexity.We know that a typical DL model comprises thousands of internal parameters, and there could be dozens of models to explore.The proposed framework DeepFocus is a MAML approach based on the Maximum Likelihood Estimation (MLE) technique applied to the model space, which finds the optimal combination of feature extraction and optimizer parameters in a double training loop, massively exploiting the GPU accelerators to speed up the search.This exactly follows the model agnostic meta-learning paradigm (Fig. 12).</p>
<p>An input data cube is used in an inner training loop to identify the better feature extraction model architecture, allowing for a guess of the sky model behind the dirty input data.Then, the encoder part, which stores the extracted features in the latent space, is reused by attaching a classifier to extract the interesting sources from the proposed sky model image, which are compared with the target ones using a supervised approach.Through the MLE-based loss function, the internal weights of the model are updated, and a new double training loop is executed.Therefore, Deep-Focus explores the model space at each double loop using the acquired experience on the sky model image reproduction learning to optimize the classifier learning.It searches for the best DL model by maximizing the likelihood of efficiently exploring the model architecture space on top of an HPC computing infrastructure.It reveals a high capability to find the best trade-off between accuracy and flexibility 43 .DeepFocus was compared with several methods, among which the traditional approach, based on the well-known TCLEAN pipeline 45 , which approximates the deconvolution problem, using a massive number of physical priors, including several models of the expected noise sources, continuum subtraction, etc.We tried to use  our DeepFocus to both speed up the process and gain accuracy.</p>
<p>As shown in Fig. 13, after a test campaign done on 1000 datacubes, DeepFocus was able to drastically reduce the processing time, being able to digest 100 TB of data in 9 minutes, rather than the 5 hours required by the TCLEAN pipeline.It also improved the classification precision compared to other solutions proposed in the literature, some of which are also based on ML/DL approaches.</p>
<p>Concluding remarks</p>
<p>The fundamental concept arising from all we have discussed is the unfeasibility of the classical approach to scientific investigation based on the so-called hypothesisdriven paradigm.The Big Data scenario imposes a new methodology, fully centered on the rule to be driven by data themselves at all levels of the scientific approach.All sciences can now be considered cyber-science (a.k.a.e-science), which requires new scientific methodologies.The challenges we are tackling, invariant to any specific science field, mainly concern managing and exploring large, complex, distributed data to engage serendipity and a virtuous synergy between domain-specific and data-driven science.In such a scenario, AI's central and unavoidable role must be regulated globally but positively by thinking that, as for all human discoveries, its impact on humankind's evolution depends only on us.</p>
<p>Fig. 1 .
1
Fig. 1.Distribution of time data scientists spend across various experiment phases (source: forbes.com).</p>
<p>Fig. 2 .
2
Fig. 2. Amount of scientific articles focusing on Astroinformatics, published per year on specialized refereed journals, from January 1997 to July 2017 15 .</p>
<p>Fig. 6 .
6
Fig.6.Examples of clustering analysis.On the left, two sub-panels show a SOM output of a distribution of thousands of galaxies grouped in terms of median spectroscopic redshift (left subpanel) and of median 30-band photometric redshift (right sub-panel)31 .On the right, a nonlinear PCA output, based on an autoencoder, shows a distribution of different clusters of cells in a mouse lymph node tissue frame32 .</p>
<p>Fig. 7 .
7
Fig. 7. Operating principles of Integral Field Spectroscopy 34 .</p>
<p>Fig. 11 .
11
Fig. 11.Meta learning process (source: https://meta-learning.fastforwardlabs.com).</p>
<p>Fig. 12 .
12
Fig. 12. DeepFocus MAML functional process flow.</p>
<p>Fig. 13 .
13
Fig. 13.DeepFocus accuracy and speed up performances.</p>
<p>August 30, 2024 9:23    ws-procs961x669 WSPC Proceedings -9.61in x 6.69in output page 6
d https://zenodo.org/records/4015325
August 30, 2024 9:23    ws-procs961x669 WSPC Proceedings -9.61in x 6.69in output page 11
August 30, 2024 9:23    ws-procs961x669 WSPC Proceedings -9.61in x 6.69in output page 13
August 30, 2024 9:23    ws-procs961x669 WSPC Proceedings -9.61in x 6.69in output page 15
August 30, 2024 9:23    ws-procs961x669 WSPC Proceedings -9.61in x 6.69in output page 16
AcknowledgmentsWe would like to underline that all the contents of this work would not be possible without the huge and constant contribution of a fantastic pool of collaborators (better friends) coming from different Institutions and a variety of scientific and technological fields: Stefano Cavuoti, Maurizio D'Addona, Mariarca D'Aniello, Michele Delli Veneri, Carlo Donadio, Andrea Dosi, Adriano Ettari, Antonio Ferragamo, Giuseppe Longo, Ylenia Maruccia, Amata Mercurio, Fabio Ragosta, Giuseppe Riccio, Alvi Rownok, Guido Russo, Maria Zampella.They are the core team, with the standard meaning of the core term in English, especially with the Neapolitan meaning 'o Core team (Hearth Team).
S Cavuoti, M Brescia, G Longo, Data mining and knowledge discovery resources for astronomy in the web 2.0 age. September 2012</p>
<p>Extracting Knowledge from Massive Astronomical Data Sets. M Brescia, S Cavuoti, G S Djorgovski, C Donalek, G Longo, M Paolillo, L. M. Sarro, L. Eyer, W. O'Mullane and J. De Ridder201231</p>
<p>Big data se vs. se for big data. M Ramachandran, November, 2019</p>
<p>E Strubell, A Ganesh, A Mccallum, arXiv:1906.02243Energy and Policy Considerations for Deep Learning in NLP. June 2019arXiv e-prints</p>
<p>P Delforge, Data center efficiency assessment: scaling up energy efficiency across the data center industry: evaluating key drivers and barriers. February 2015</p>
<p>Big-data-oriented computing on a mid-size data center: the adhoc infrastructure. A Tortora, M Brescia, S Conte, P Guida, G Russo, B Spisso, International Journal on Advances in Intelligent Systems. S. Hiroyuki, I. Sergio, O. Malkov, N. Skvortsov and J. Lofstead2024IARIA</p>
<p>D Huppenkothen, M Ntampaka, M Ho, M Fouesneau, B Nord, J E G Peek, M Walmsley, J F Wu, C Avestruz, T Buck, M Brescia, arXiv:2310.12528Constructing Impactful Machine Learning Research for Astronomy: Best Practices for Researchers and Reviewers. October 2023</p>
<p>Euclid preparation. XXVI. The Euclid Morphology Challenge: Towards structural parameters for billions of galaxies. H Bretonni, Euclid CollaborationA&amp;A. 671A102March 2023</p>
<p>J Zuntz, LSST Dark Energy Science CollaborationF Lanusse, LSST Dark Energy Science CollaborationA I Malz, LSST Dark Energy Science CollaborationA H Wright, LSST Dark Energy Science CollaborationA Slosar, LSST Dark Energy Science CollaborationB Abolfathi, LSST Dark Energy Science CollaborationD Alonso, LSST Dark Energy Science CollaborationA Bault, LSST Dark Energy Science CollaborationC R Bom, LSST Dark Energy Science CollaborationM Brescia, LSST Dark Energy Science CollaborationThe LSST-DESC 3x2pt Tomography Optimization Challenge. October 2021413</p>
<p>S J Schmidt, LSST Dark Energy Science CollaborationA I Malz, LSST Dark Energy Science CollaborationJ Y H Soo, LSST Dark Energy Science CollaborationI A Almosallam, LSST Dark Energy Science CollaborationM Brescia, LSST Dark Energy Science CollaborationS Cavuoti, LSST Dark Energy Science CollaborationEvaluation of probabilistic photometric redshift estimation approaches for The Rubin Observatory Legacy Survey of Space and Time (LSST). December 20204991587</p>
<p>Euclid preparation. X. The Euclid photometricredshift challenge. G Desprez, Euclid CollaborationA&amp;A. 644A31December 2020</p>
<p>Photometric redshifts with the quasi Newton algorithm (MLPQNA) Results in the PHAT1 contest. S Cavuoti, M Brescia, G Longo, A Mercurio, A&amp;A. 546A13October 2012</p>
<p>PHAT: PHoto-z Accuracy Testing. H Hildebrandt, S Arnouts, P Capak, L A Moustakas, C Wolf, A&amp;A. 523A31November 2010</p>
<p>Statistical analysis of probability density functions for photometric redshifts through the KiDS-ESO-DR3 galaxies. V Amaro, S Cavuoti, M Brescia, C Vellucci, G Longo, MNRAS. 4823116January 2019</p>
<p>Data deluge in astrophysics: Photometric redshifts as a template use case. M Brescia, S Cavuoti, V Amaro, G Riccio, G Angora, L Kalinichenko, Y Manolopoulos, O Malkov, N Skvortsov, S Stupnikov, V Sukhomlin, Data Analytics and Management in Data Intensive Domains. ChamSpringer International Publishing2018</p>
<p>Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai. A Barredo Arrieta, N D´ıaz-Rodr´ıguez, J Del, A Ser, S Bennetot, Tabik, Information Fusion. 58822020</p>
<p>A unified approach to interpreting model predictions. S M Lundberg, S.-I Lee, Advances in Neural Information Processing Systems. I Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, R Garnett, Curran Associates, Inc2017</p>
<p>R Bellman, Adaptive Control Processes: A Guided Tour. Princeton University Press1961</p>
<p>On lines and planes of closest fit to systems of points in space. K Pearson, Liii , September 2018</p>
<p>Photometric redshifts for X-ray-selected active galactic nuclei in the eROSITA era. M Brescia, M Salvato, S Cavuoti, T T Ananna, G Riccio, MNRAS. 489663October 2019</p>
<p>Astroinformaticsbased search for globular clusters in the Fornax Deep Survey. G Angora, M Brescia, S Cavuoti, M Paolillo, G Longo, MNRAS. 490December 2019</p>
<p>Star formation rates for photometric samples of galaxies using machine learning methods. M Delli Veneri, S Cavuoti, M Brescia, G Longo, G Riccio, MNRAS. 4861377June 2019</p>
<p>Feature selection with the boruta package. M B Kursa, W R Rudnicki, Journal of Statistical Software. 362010</p>
<p>Exact post-selection inference, with application to the lasso. J D Lee, D L Sun, Y Sun, J E Taylor, The Annals of Statistics. 449072016</p>
<p>Integrated data access, visualization and analysis for Galactic Plane surveys: the VIALACTEA case. S Molinari, R Butora, S Cavuoti, M Molinaro, G Riccio, Astroinformatics. M. Brescia, S. G. Djorgovski, E. D. Feigelson, G. Longo and S. CavuotiJune 2017IAU Symposium</p>
<p>Machine Learning Based Data Mining for Milky Way Filamentary Structures Reconstruction. G Riccio, S Cavuoti, E Schisano, M Brescia, A Mercurio, Advances in Neural Networks. B S , E A , M F , P E , 2016</p>
<p>. Y Maruccia, in prep</p>
<p>S Vilardi, S Capozziello, M Brescia, arXiv:2408.01563Discriminating among cosmological models by data-driven methods. August 2024arXiv e-printsSubmitted to A&amp;A</p>
<p>A G Adame, DESI CollaborationarXiv:2404.03002DESI 2024 VI: Cosmological Constraints from the Measurements of Baryon Acoustic Oscillations. April 2024arXiv e-prints</p>
<p>Data-Rich Spatial Profiling of Cancer Tissue: Astronomy Informs Pathology. A S Szalay, J M Taube, Clinical Cancer Research. 282022. August 2022</p>
<p>Mapping the Galaxy Color-Redshift Relation: Optimal Photometric Redshift Calibration Strategies for Cosmology Surveys. D Masters, P Capak, D Stern, O Ilbert, M Salvato, ApJ. 81353November 2015</p>
<p>Open-st: High-resolution spatial transcriptomics in 3d. M Schott, D Leon-Perin ˜án, E Splendiani, L Strenger, J R Licha, Cell. 18739532024</p>
<p>The MUSE Hubble Ultra Deep Field surveys: Data release II. R Bacon, J Brinchmann, S Conseil, M Maseda, T Nanayakkara, A&amp;A. 670A4February 2023</p>
<p>J R Allington-Smith, Revista Mexicana de Astronomia y Astrofisica Conference Series. S Kurtz, June 2007Integral Field Spectroscopy for Panoramic Telescopes</p>
<p>Operational remote sensing mapping of estuarine suspended sediment concentrations (ormes. E Knaeps, S Sterckx, M Bollen, K Trouw, R Houthuys, 2006</p>
<p>A Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, arXiv:2010.11929An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, arXiv e-prints. October 2020</p>
<p>A Dosi, M Brescia, S Cavuoti, M D'aniello, M Delli, C Veneri, A Donadio, G Ettari, A Longo, L Rownok, M Sannino, Zampella, AMBER -Advanced SegFormer for Multi-Band Image Segmentation: an application to Hyperspectral Imaging , Submitted to Neural Computing and Applications. </p>
<p>A comprehensive survey on transfer learning. F Zhuang, Z Qi, K Duan, D Xi, Y Zhu, H Zhu, H Xiong, Q He, 2020</p>
<p>Identification of problematic epochs in astronomical time series through transfer learning. S Cavuoti, D De Cicco, L Doorenbos, M Brescia, O Torbaniuk, G Longo, M Paolillo, A&amp;A. 687A246July 2024</p>
<p>C Finn, P Abbeel, S Levine, arXiv:1703.03400Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks, arXiv e-prints. March 2017</p>
<p>Meta-learning approaches for learning-to-learn in deep learning: A survey. Y Tian, X Zhao, W Huang, Neurocomputing. 4942032022</p>
<p>P A Vanden Bout, R L Dickman, A L Plunkett, The ALMA Telescope: The Story of a Science Mega-Project. 2023</p>
<p>3D detection and characterization of ALMA sources through deep learning. M Delli Veneri, L - Tychoniec, F Guglielmetti, G Longo, E Villard, MNRAS. 5183407January 2023</p>
<p>A BRAIN study to tackle imaging with artificial intelligence in the ALMA2030 era. F Guglielmetti, M Delli, I Veneri, V Baronchelli, P Johnson, C Arras, M Blanco, Brescia, November 2023</p>
<p>Casa, the common astronomy software applications for radio astronomy. B The Casa Team, Bean, Publications of the Astronomical Society of the Pacific. 134114501nov 2022</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9147 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9147</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9147</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-266174101</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.07069v2.pdf" target="_blank">Context Matters: Data-Efficient Augmentation of Large Language Models for Scientific Applications</a></p>
                <p><strong>Paper Abstract:</strong> In this paper, we explore the challenges inherent to Large Language Models (LLMs) like GPT-4, particularly their propensity for hallucinations, logic mistakes, and incorrect conclusions when tasked with answering complex questions. The capacity of LLMs to present erroneous answers in a coherent and semantically rigorous manner further complicates the detection of factual inaccuracies. This issue is especially pronounced in fields that require specialized expertise. Our work delves into these challenges, aiming to enhance the understanding and mitigation of such errors, thereby contributing to the improvement of LLM accuracy and reliability in scientific and other specialized domains. Our findings reveal a non-linear relationship between the context's relevancy and the answers' measured quality. In addition, we demonstrate that with the correct calibration, it is possible to automate the grading procedure -- a finding suggesting that, at least to some degree, the LLMs can be used to self-examine the quality of their own performance. Finally, we describe an experimental platform that can be seen as a proof-of-concept of the techniques described in this work.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9147.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9147.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 is the large OpenAI language model used in this paper to perform text-based problem-solving and reasoning on graduate-level quantum mechanics questions; it was used both to generate solutions and to perform automatic grading variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 (OpenAI). The paper does not report model size or training-data specifics; it is used via the GPT-4 API as a text-based reasoning agent and grader.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Quantum physics (upper-undergraduate / intro-graduate quantum mechanics)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based simulation of problem-solving for graduate-level quantum mechanics questions: generate stepwise solutions, apply chain-of-thought prompting, and act as an automatic grader (auto1, auto2, auto3 variants). Tasks include reasoning about sequential spin measurements, perturbation theory, identical particles, variational principle, and related calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Custom rubric combining: Completeness & Relevancy (3 points), Logic & Reasoning (4 points with specified deduction rules), Truthfulness/lack-of-hallucination (3 points). Scores were reported as mean ± sample standard deviation across 10 questions and 4 prompt variants; automatic grading methods (auto1, auto2, auto3) were compared with manual expert grading.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Reported mean rubric scores (mean ± s.e.m.): Manual grading — No hint: 7.5 ± 0.6; Irrelevant hint: 7.9 ± 0.6; Vague hint: 7.9 ± 0.7; Insightful hint: 8.4 ± 0.6. Auto1 grading — No hint: 6.6 ± 0.6; Irrelevant: 6.8 ± 0.4; Vague: 6.6 ± 0.7; Insightful: 7.4 ± 0.6. Auto2 grading — No hint: 6.4 ± 0.6; Irrelevant: 7.0 ± 0.4; Vague: 6.6 ± 0.7; Insightful: 7.6 ± 0.5. Additional summarized-context comparisons (auto3 + summaries) show small/no reduction for irrelevant and vague hints but reduced performance when summarizing insightful hints (see Table 4). Dataset size: 10 questions, 40 responses total.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Context relevance and quality (levels: no hint, irrelevant, vague, insightful) strongly affect performance; prompt engineering (chain-of-thought) matters; summarization quality and loss of scientific notation/equations (BART summarizer trained on CNN/Daily Mail caused loss of critical information); auto-grader calibration (auto methods under-score relative to manual grading); small dataset size (high variance); model weaknesses noted: poor handling of physical units, coding errors, hallucinations; hint content type (methodological insight vs topical relevance) — methodological insight appears most beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Zero-shot baseline (no hint) within the same GPT-4 experiments. Manual expert grading used as a human baseline/reference; automatic graders (auto1/auto2/auto3) scored consistently lower than manual grading but followed similar sensitivity trends. No comparison to other LLMs or physics simulators was performed in the experimental results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Small dataset (10 questions) limits statistical power; summarization (BART pretrained on news) removed equations/notations, degrading performance for insightful hints; GPT-4 exhibited hallucinations, logical errors, poor handling of units/coding; vague hints increased response variability (higher standard deviation); unexpected effect that 'irrelevant' hints sometimes improved performance, suggesting model-context misalignment; automatic graders require better calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Provide carefully curated, contextually relevant hints (especially those containing methodological insights) to improve LLM performance; develop domain-specific summarizers (fine-tune summarization models on scientific text to preserve equations/notation); expand dataset size to increase statistical significance; further refine and calibrate automatic grading (auto3 including human chain-of-thought examples improved alignment); investigate insight-distillation methods and CEFA architecture for gathering higher-quality context.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Context Matters: Data-Efficient Augmentation of Large Language Models for Scientific Applications', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9147.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9147.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Codex (Drori et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI Codex (as used in Drori et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior work where Codex was applied to university-level math problems, demonstrating that a code-generating LLM can be used to solve, explain, and generate math problem solutions with few-shot/program-synthesis techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI Codex (as reported by Drori et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Codex is a code-capable variant of GPT trained on large code+text corpora (paper references Drori et al. results but does not provide Codex size/training details). Here it is cited as used to synthesize programs to solve math problems.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Mathematics / University-level math problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based program synthesis to solve, explain, and generate solutions for university math problems (used as an automated solver and explainer).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Benchmark accuracy on a set of university math problems (reported improvement in Drori et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Reported in the related work citation (Drori et al.): accuracy improved from 18.8% to 81.1% on the evaluated benchmark (as quoted in this paper's related-work section).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Few-shot/program-synthesis prompting and problem representation; leveraging code synthesis to perform exact computation and verification; in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against prior baseline accuracy (18.8%) on the same benchmark; Codex-based method achieved substantially higher accuracy (81.1%) as reported in the cited study.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>This paper only cites the result; limitations of Codex are not detailed here. (Limitations would be in the cited Drori et al. work.)</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Cited as evidence that few-shot/in-context program-synthesis approaches can markedly improve performance on technical problem-solving tasks; suggests that similar in-context techniques and programmatic solutions may help LLMs in scientific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Context Matters: Data-Efficient Augmentation of Large Language Models for Scientific Applications', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9147.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9147.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MathPrompter (Imani et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Math-prompter (Mathematical reasoning method from Imani et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced method (Math-prompter) that improves LLM mathematical reasoning by tailored prompting and rationale generation, reported to increase performance on arithmetic benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mathematical Reasoning using Large Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Unspecified LLM(s) used with Math-prompter technique (Imani et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Math-prompter is a prompt-engineering / reasoning method applied to LLMs to improve mathematical reasoning; the paper cites its reported gains but does not detail model parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Mathematical reasoning / arithmetic benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Improve LLM solution accuracy on arithmetic/mathematical reasoning tasks via a prompting strategy that elicits better stepwise reasoning (rationales) and uses them to infer answers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Benchmark accuracy on MultiArith dataset (and similar mathematical benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Reported improvement on the MultiArith dataset from 78.7% to 92.5% (as cited in this paper's related work).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Prompt construction (rationales and chaining), few-shot examples, reasoning elicitation techniques (chain-of-thought variants).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to prior LLM performance on MultiArith (baseline 78.7%); Math-prompter boosted accuracy to 92.5% per the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mentioned only in related work here; the current paper notes that state-of-the-art LLMs still struggle on advanced graduate-level physics despite such techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Used as supporting evidence that prompt engineering and rationale-generation methods can substantially improve performance on structured reasoning tasks; suggests adopting and adapting such methods for scientific domain problems but also cautions that physics problems present distinct challenges (units, equations, domain knowledge).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Context Matters: Data-Efficient Augmentation of Large Language Models for Scientific Applications', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level. <em>(Rating: 2)</em></li>
                <li>Mathematical Reasoning using Large Language Models. <em>(Rating: 2)</em></li>
                <li>What Can Large Language Models Do in Chemistry? A Comprehensive Benchmark on Eight Tasks. <em>(Rating: 2)</em></li>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. <em>(Rating: 1)</em></li>
                <li>HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9147",
    "paper_id": "paper-266174101",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "GPT-4 is the large OpenAI language model used in this paper to perform text-based problem-solving and reasoning on graduate-level quantum mechanics questions; it was used both to generate solutions and to perform automatic grading variants.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "GPT-4 (OpenAI). The paper does not report model size or training-data specifics; it is used via the GPT-4 API as a text-based reasoning agent and grader.",
            "scientific_subdomain": "Quantum physics (upper-undergraduate / intro-graduate quantum mechanics)",
            "simulation_task": "Text-based simulation of problem-solving for graduate-level quantum mechanics questions: generate stepwise solutions, apply chain-of-thought prompting, and act as an automatic grader (auto1, auto2, auto3 variants). Tasks include reasoning about sequential spin measurements, perturbation theory, identical particles, variational principle, and related calculations.",
            "evaluation_metric": "Custom rubric combining: Completeness & Relevancy (3 points), Logic & Reasoning (4 points with specified deduction rules), Truthfulness/lack-of-hallucination (3 points). Scores were reported as mean ± sample standard deviation across 10 questions and 4 prompt variants; automatic grading methods (auto1, auto2, auto3) were compared with manual expert grading.",
            "simulation_accuracy": "Reported mean rubric scores (mean ± s.e.m.): Manual grading — No hint: 7.5 ± 0.6; Irrelevant hint: 7.9 ± 0.6; Vague hint: 7.9 ± 0.7; Insightful hint: 8.4 ± 0.6. Auto1 grading — No hint: 6.6 ± 0.6; Irrelevant: 6.8 ± 0.4; Vague: 6.6 ± 0.7; Insightful: 7.4 ± 0.6. Auto2 grading — No hint: 6.4 ± 0.6; Irrelevant: 7.0 ± 0.4; Vague: 6.6 ± 0.7; Insightful: 7.6 ± 0.5. Additional summarized-context comparisons (auto3 + summaries) show small/no reduction for irrelevant and vague hints but reduced performance when summarizing insightful hints (see Table 4). Dataset size: 10 questions, 40 responses total.",
            "factors_affecting_accuracy": "Context relevance and quality (levels: no hint, irrelevant, vague, insightful) strongly affect performance; prompt engineering (chain-of-thought) matters; summarization quality and loss of scientific notation/equations (BART summarizer trained on CNN/Daily Mail caused loss of critical information); auto-grader calibration (auto methods under-score relative to manual grading); small dataset size (high variance); model weaknesses noted: poor handling of physical units, coding errors, hallucinations; hint content type (methodological insight vs topical relevance) — methodological insight appears most beneficial.",
            "comparison_baseline": "Zero-shot baseline (no hint) within the same GPT-4 experiments. Manual expert grading used as a human baseline/reference; automatic graders (auto1/auto2/auto3) scored consistently lower than manual grading but followed similar sensitivity trends. No comparison to other LLMs or physics simulators was performed in the experimental results.",
            "limitations_or_failure_cases": "Small dataset (10 questions) limits statistical power; summarization (BART pretrained on news) removed equations/notations, degrading performance for insightful hints; GPT-4 exhibited hallucinations, logical errors, poor handling of units/coding; vague hints increased response variability (higher standard deviation); unexpected effect that 'irrelevant' hints sometimes improved performance, suggesting model-context misalignment; automatic graders require better calibration.",
            "author_recommendations_or_insights": "Provide carefully curated, contextually relevant hints (especially those containing methodological insights) to improve LLM performance; develop domain-specific summarizers (fine-tune summarization models on scientific text to preserve equations/notation); expand dataset size to increase statistical significance; further refine and calibrate automatic grading (auto3 including human chain-of-thought examples improved alignment); investigate insight-distillation methods and CEFA architecture for gathering higher-quality context.",
            "uuid": "e9147.0",
            "source_info": {
                "paper_title": "Context Matters: Data-Efficient Augmentation of Large Language Models for Scientific Applications",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Codex (Drori et al.)",
            "name_full": "OpenAI Codex (as used in Drori et al.)",
            "brief_description": "Referenced prior work where Codex was applied to university-level math problems, demonstrating that a code-generating LLM can be used to solve, explain, and generate math problem solutions with few-shot/program-synthesis techniques.",
            "citation_title": "A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level.",
            "mention_or_use": "mention",
            "model_name": "OpenAI Codex (as reported by Drori et al.)",
            "model_description": "Codex is a code-capable variant of GPT trained on large code+text corpora (paper references Drori et al. results but does not provide Codex size/training details). Here it is cited as used to synthesize programs to solve math problems.",
            "scientific_subdomain": "Mathematics / University-level math problem solving",
            "simulation_task": "Text-based program synthesis to solve, explain, and generate solutions for university math problems (used as an automated solver and explainer).",
            "evaluation_metric": "Benchmark accuracy on a set of university math problems (reported improvement in Drori et al.).",
            "simulation_accuracy": "Reported in the related work citation (Drori et al.): accuracy improved from 18.8% to 81.1% on the evaluated benchmark (as quoted in this paper's related-work section).",
            "factors_affecting_accuracy": "Few-shot/program-synthesis prompting and problem representation; leveraging code synthesis to perform exact computation and verification; in-context examples.",
            "comparison_baseline": "Compared against prior baseline accuracy (18.8%) on the same benchmark; Codex-based method achieved substantially higher accuracy (81.1%) as reported in the cited study.",
            "limitations_or_failure_cases": "This paper only cites the result; limitations of Codex are not detailed here. (Limitations would be in the cited Drori et al. work.)",
            "author_recommendations_or_insights": "Cited as evidence that few-shot/in-context program-synthesis approaches can markedly improve performance on technical problem-solving tasks; suggests that similar in-context techniques and programmatic solutions may help LLMs in scientific tasks.",
            "uuid": "e9147.1",
            "source_info": {
                "paper_title": "Context Matters: Data-Efficient Augmentation of Large Language Models for Scientific Applications",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "MathPrompter (Imani et al.)",
            "name_full": "Math-prompter (Mathematical reasoning method from Imani et al.)",
            "brief_description": "Referenced method (Math-prompter) that improves LLM mathematical reasoning by tailored prompting and rationale generation, reported to increase performance on arithmetic benchmarks.",
            "citation_title": "Mathematical Reasoning using Large Language Models.",
            "mention_or_use": "mention",
            "model_name": "Unspecified LLM(s) used with Math-prompter technique (Imani et al.)",
            "model_description": "Math-prompter is a prompt-engineering / reasoning method applied to LLMs to improve mathematical reasoning; the paper cites its reported gains but does not detail model parameters.",
            "scientific_subdomain": "Mathematical reasoning / arithmetic benchmarks",
            "simulation_task": "Improve LLM solution accuracy on arithmetic/mathematical reasoning tasks via a prompting strategy that elicits better stepwise reasoning (rationales) and uses them to infer answers.",
            "evaluation_metric": "Benchmark accuracy on MultiArith dataset (and similar mathematical benchmarks).",
            "simulation_accuracy": "Reported improvement on the MultiArith dataset from 78.7% to 92.5% (as cited in this paper's related work).",
            "factors_affecting_accuracy": "Prompt construction (rationales and chaining), few-shot examples, reasoning elicitation techniques (chain-of-thought variants).",
            "comparison_baseline": "Compared to prior LLM performance on MultiArith (baseline 78.7%); Math-prompter boosted accuracy to 92.5% per the cited work.",
            "limitations_or_failure_cases": "Mentioned only in related work here; the current paper notes that state-of-the-art LLMs still struggle on advanced graduate-level physics despite such techniques.",
            "author_recommendations_or_insights": "Used as supporting evidence that prompt engineering and rationale-generation methods can substantially improve performance on structured reasoning tasks; suggests adopting and adapting such methods for scientific domain problems but also cautions that physics problems present distinct challenges (units, equations, domain knowledge).",
            "uuid": "e9147.2",
            "source_info": {
                "paper_title": "Context Matters: Data-Efficient Augmentation of Large Language Models for Scientific Applications",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level.",
            "rating": 2,
            "sanitized_title": "a_neural_network_solves_explains_and_generates_university_math_problems_by_program_synthesis_and_fewshot_learning_at_human_level"
        },
        {
            "paper_title": "Mathematical Reasoning using Large Language Models.",
            "rating": 2,
            "sanitized_title": "mathematical_reasoning_using_large_language_models"
        },
        {
            "paper_title": "What Can Large Language Models Do in Chemistry? A Comprehensive Benchmark on Eight Tasks.",
            "rating": 2,
            "sanitized_title": "what_can_large_language_models_do_in_chemistry_a_comprehensive_benchmark_on_eight_tasks"
        },
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models.",
            "rating": 1,
            "sanitized_title": "halueval_a_largescale_hallucination_evaluation_benchmark_for_large_language_models"
        }
    ],
    "cost": 0.01148,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Context Matters: Data-Efficient Augmentation of Large Language Models for Scientific Applications
21 Dec 2023</p>
<p>Xiang Li 
Department of Physics and Astronomy
University of Southern California
Los AngelesCAUSA</p>
<p>Department of Computer Science
University of Southern California
Los AngelesCAUSA</p>
<p>Haoran Tang haoranta@usc.edu 
Department of Computer Science
University of Southern California
Los AngelesCAUSA</p>
<p>Siyu Chen 
Department of Computer Science
University of Southern California
Los AngelesCAUSA</p>
<p>Ziwei Wang 
Department of Computer Science
University of Southern California
Los AngelesCAUSA</p>
<p>Anurag Maravi maravi@usc.edu 
Department of Computer Science
University of Southern California
Los AngelesCAUSA</p>
<p>Marcin Abram mjabram@usc.edu 
Department of Physics and Astronomy
University of Southern California
Los AngelesCAUSA</p>
<p>Information Sciences Institute
University of Southern California
Los AngelesCAUSA</p>
<p>Context Matters: Data-Efficient Augmentation of Large Language Models for Scientific Applications
21 Dec 2023DD6BF21B1FE4D67E9806F1492667DB8FarXiv:2312.07069v2[cs.CL]
In this paper, we explore the challenges inherent to Large Language Models (LLMs) like GPT-4, particularly their propensity for hallucinations, logic mistakes, and incorrect conclusions when tasked with answering complex questions.The capacity of LLMs to present erroneous answers in a coherent and semantically rigorous manner further complicates the detection of factual inaccuracies.This issue is especially pronounced in fields that require specialized expertise.Our work delves into these challenges, aiming to enhance the understanding and mitigation of such errors, thereby contributing to the improvement of LLM accuracy and reliability in scientific and other specialized domains.Our findings reveal a non-linear relationship between the context's relevancy and the answers' measured quality.In addition, we demonstrate that with the correct calibration, it is possible to automate the grading procedure -a finding suggesting that, at least to some degree, the LLMs can be used to self-examine the quality of their own performance.Finally, we describe an experimental platform that can be seen as a proof-of-concept of the techniques described in this work.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) such as GPT-4 suffer from hallucinations, logic mistakes, and incorrect conclusions when answering challenging questions [Rawte et al., 2023].In addition, similar to a style adoption [Gatys et al., 2016], LLMs can present erroneous answers in a seemingly coherent and semantically rigorous fashion [Bender et al., 2021, Scheurer et al., 2023].Therefore, detecting factual mistakes made by LLMs can be challenging, especially when asking about topics requiring specialized expertise.</p>
<p>We have many available benchmarks measuring the performance of LLMs, including AGIEval [Zhong et al., 2023], ChemLLMBench [Guo et al., 2023], and SCIEval [Sun et al., 2023].However, those popular benchmarks usually require a specific structure of the responses (e.g., multiple choice or numeric) and are not designed to capture mistakes in free-form responses.Among more flexible methods, we can mention HaluEval developed by Li et al. [2023b].However, this method does not scale well since it requires laborious and expensive human annotation.Automated methods of evaluations include BiLingual Evaluation Understudy (BLEU) [Papineni et al., 2001], BERTScore [Zhang et al., 2020], and LLM-Eval [Lin and Chen, 2023].While they can be easily run at scale, they are usually restricted to a group of specific sub-tasks (e.g., machine translation, text summarization, or dialogue generation).Due to those constraints, the question of LLMs evaluation is still an active research area.</p>
<p>In many areas, the tendency of LLMs to hallucinate and its typical overconfidence when presenting erroneous answers can cause a lot of harm (e.g., in medical [Thapa and Adhikari, 2023], legal [Weidinger et al., 2022], or educational [Bommasani et al., 2021] applications).In this work, we focus on yet another area characterized by a low tolerance for erroneous answers: scientific applications, with the aim to develop a technique that increases the coherence and reliability of answers produced by popular LLMs, such as  We tackle this challenge by employing a version of prompt engineering known as the chain-of-thought [Wei et al., 2022b].As one of the original contributions of this paper, we measure the extent to which the relevance of context matters in improving the quality of answers when asking about topics requiring specialized expertise.Next, in Section 5, we discuss to which extent we can automate the evaluation procedure, testing two distinct approaches: (1) we anchor the ground truth by providing a sample answer, (2) we anchor on the generated answer based on the sample answer as a fact.</p>
<p>To illustrate our approach, we developed an online platform that can serve as a proof-of-concept, testing the techniques described in this paper.As a choice of our scientific domain, we selected graduate-level quantum physics.In Section 6 we discuss the platform's architecture and in Section 7, we test the impact of different summarization techniques for keeping the context reasonably short.</p>
<p>Characteristic Challenges and Research Opportunities</p>
<p>Arguably, the fastest adoption of LLMs in the past year could be noticed in the areas of translation [Xu et al., 2023], code generation [Koziolek and Koziolek, 2023, Siddiq and Santos, 2023, Chen et al., 2023], database query optimization [Li et al., 2023a], and text summarization [Sahu et al., 2023] as well as in the development of general personal assistants [Li et al., 2023c, Chan et al., 2023].</p>
<p>However, when targeting scientific applications, we must be aware of some distinctive characteristics of that domain that set it apart from typical business operations.For example, when answering scientific questions, we might face some inherent limit on the size (or variability) of the known datasets.While in many business applications, we can get more trainable examples by committing more money or time (e.g., by taking more photos and labeling them or by scanning more books to get more natural text), in some scientific domains, the number of existing examples can be strictly limited.For example, we might only know so many examples of superconducting materials, and we can not easily increase that number (not without first discovering new types of superconductors).Another important difference is that science does not follow the typical majority voting rule.If we ask people to classify pictures of cats and dogs, even if some human labelers might disagree when assessing a particular photo, we can still use majority voting to determine whether a given picture depicts a dog or a cat.The same can be done to quantify the value of continuous variables [Surowiecki, 2004].In contrast, in science, a single opinion, if sufficiently motivated, can weigh more than a popular opinion of the majority (as it happened, e.g., with the discovery of quasicrystals by [Shechtman et al., 1984], a discovery that challenged the existing status quo, and resulted in a major shift in our understanding of solid-state physics).On the other hand, in the scientific domain, we can rely on some known (even if sometimes only in an approximate form1 ) fundamental laws governing the universe (e.g., the laws of motion or the conservation of energy, momentum, angular momentum, charge, mass, etc.).This contrasts with many business applications that often target complex social systems, where emerging phenomena render any fundamental description unusable [Anderson, 1972].For example, when predicting the probability that a given customer will return in the next six months, we can not rely on any strict laws we can use to verify the correctness of our solutions.This makes applications in the scientific domain somehow special since we can verify with certainty that answers violating some fundamental laws are wrong.</p>
<p>Considering all those differences, we must leverage the few-shot learning ability of LLMs [Brown et al., 2020].The algorithm that we propose must be data-efficient.We can not afford to retrain the entire LLM using millions of examples, as simply they might not exist.We must leverage the few-shot learning ability of LLMs.Instead, we focus on providing LLMs with the proper context, as described in the following sections.</p>
<p>Related Work</p>
<p>Prompt engineering.Previous work has demonstrated that prompt engineering and chain of thought techniques can greatly improve LLMs' performance in various domains [Kojima et al., 2022, Wei et al., 2022b, Besta et al., 2023, Yao et al., 2023, Imani et al., 2023].However, very few works focus on the evaluation of these techniques when applied to the scientific domain.</p>
<p>In-Context Learning.The few-shot learning capabilities of LLMs enable enhanced performance across various tasks through tailored prompting [Brown et al., 2020].Drori et al. [2022] employed OpenAI's Codex to tackle University Math Problems, achieving human-level performance and elevating benchmark accuracy from 18.8% to 81.1%.Imani et al. [2023] developed a "math-prompter" method for LLMs, significantly improving performance on the MultiArith dataset from 78.7% to 92.5%.Zhang et al. [2023] developed a concise two-stage framework that first generates rationales and then uses them to enhance answer inference, effectively integrating multimodal information.However, today's state-of-the-art LLMs (e.g., GPT-4) still struggle to solve advanced, graduate-level physics problems, as demonstrated in this paper.Key issues include its poor handling of physical units, coding errors, and its tendency to create convincing yet incorrect answers.</p>
<p>Evaluation of LLMs' physics reasoning capacities.Creating an effective evaluation framework for LLMs in scientific domains, particularly for physical reasoning, is challenging due to the implicit nature of human knowledge about the physical world and its representation [Bisk et al., 2020].Benchmarks like those created by Clark et al. [2018] have focused on assessing LLMs using physics questions.Aroca-Ouellette et al. [2021] specifically test models on physical commonsense and reasoning concepts.Liang et al. [2023], Ichter et al. [2022] focus applications for physically grounded contexts.However, most of those benchmarks target basic school-or undergraduate-college-level questions.In contrast, in this work, we focus on the topic of upper-level undergraduate-and graduatelevel quantum physics to provide LLM with more nuanced questions that often do not have a single, straight answer.</p>
<p>Methodology 4.1 Approach</p>
<p>To assess the importance of context, we compare the answers of GPT-4 in two settings: (1) without any modifications, a version that can be seen as zero-shot learning, and (2) using prompt engineering to provide the context.We vary the quality of the context, from "irrelevant", through "vague" to "insightful".We score the quality of the response using developed grading rubrics.In addition, we test a few variants of automatic grading, measuring its consistency with the manual expert grading.Finally, we test the impact of summarization techniques employed to limit the length of the context.</p>
<p>Data collection</p>
<p>We created a question bank containing selected problems from [Griffiths and Schroeter, 2018], a popular upper-undergraduate or intro-graduate-level textbook on quantum mechanics.Next, for each question, we created three hints with different levels of relevance.</p>
<p>The selected questions cover the following areas: quantum mechanics in three dimensions, identical particles, symmetries and conservation laws, time-independent perturbation theory, and the variational principle (cf. the sample question in the Supplementary Material).The solutions to the questions usually involve a combination of complex calculations and application of conceptual knowledge.</p>
<p>Hints structure</p>
<p>Hints for each question are divided into three classes: irrelevant hint, vague hint, and insightful hint.Insightful hints are hints that contain information highly relevant to the question.These hints can contain a sample solution for a similar problem, key insights into the formation of the question, or methodology that can be applied to the answer.Vague hints are hints relevant to the topic of the question but not the question itself.For example, if the question asks about the variational principle, a vague hint can be the sample solution to a completely different variational principle question or a textbook description of the variational principle.Unrelated hints include the solution to a difficult quantum physics problem, which is not relevant to the topic of the question.</p>
<p>You can find examples of hints representing each category in the Supplementary Material.</p>
<p>Evaluation</p>
<p>To assess different aspects of answers produced by the LLM, we developed grading rubrics evaluating not only the semantic accuracy of the responses but also the factual correctness and their logical consistency.Namely, our evaluation system comprised:</p>
<p>• Completness and Relevancy (3 points): Three points for a relevant and correct response.</p>
<p>Zero points if the answer is irrelevant or incorrect, with partial points for partial correctness.</p>
<p>• Logic and Reasoning (4 points): We start with four points and deduct two points for each logical inconsistency and one point for each minor error in reasoning.</p>
<p>• Truthfulness (lack of hallucination, 3 points): We start with three points and deduct one point for each incorrect statement in the response.</p>
<p>We graded each response using both manual and automatic approaches.As for manual grading (manual), we shuffled the order of responses to reduce bias (so the grader does not know which response is the result of which).The grader then used the evaluation criteria and sample answers to grade.For automatic grading, we have implemented two approaches.The first approach (auto1) involved providing GPT-4 with an answer and rubric while prompting GPT-4 to grade in a zero-shot manner.In the second approach (auto2), we first prompted GPT-4 to generate an answer based on the sample answer and then used its answer as the key to grade the response.In this way, the grader kept a better version of its response as the benchmark (assuming the quality of the response is directly and positively correlated to the relevance and correctness of the prompt).</p>
<p>Since we have four levels of context ("no context", "insightful hint", "vague hint", and "irrelevant hint"), a total of four responses were generated by GPT-4 for each question.You can find a sample of grading with our evaluation criteria in Supplementary Material.</p>
<p>Results</p>
<p>Context matters</p>
<p>We provide GPT-4 with different levels of context when answering the queries from our question bank.The context is provided to GPT-4 API with an "assistant" role, followed by a "user" call to answer the question.For each question, we test the response, providing four different levels of context.In the first level (no hint), we provide no context.In the second level (irrelevant hint), the context contains a hint that is relevant to quantum physics but not relevant to the topic of the question.</p>
<p>In the third level (vague hint), the context contains a hint that is relevant to the topic of the question but not directly applicable or related to the question.In the fourth level (insightful hint), the context contains a relevant and applicable hint to the solution.</p>
<p>After acquiring the responses from GPT-4, we shuffled them and manually graded them according to the evaluation criteria.We present the mean scores in Table 1 (where we also included results of auto-graders, explained below).The results suggest that different levels of quality of the context leads to different performance.Notably, even irrelevant context seems to improve the average performance.</p>
<p>Table 1: Mean grading score for different levels of hints and different grading modes (manual and two different auto-grading, as explained in Section 5.2).The error represents the unbiased sample standard deviation of the mean values.</p>
<p>No hint Irrelevant Vague Insightful Manual 7.5 ± 0.6 7.9 ± 0.6 7.9 ± 0.7 8.4 ± 0.6 Auto1 6.6 ± 0.6 6.8 ± 0.4 6.6 ± 0.7 7.4 ± 0.6 Auto2 6.4 ± 0.6 7.0 ± 0.4 6.6 ± 0.7 7.6 ± 0.5 While there is no statistical difference between the two tested auto-grade methods, the manual method yields higher scores than the automated method.</p>
<p>Automatic grading</p>
<p>Aside from grading manually, we also experimented with three versions of automatic grading with GPT-4.The first version (auto-1) grades by treating the sample answer as the ground truth, and the second version (auto-2) first generates an answer based on the sample answer provided as a context, before using that generated answer as the ground truth.We present the averaged results in Table 1 and Figure 1.</p>
<p>Analyzing the general trend depicted in Figure 1, both auto1 and auto2 grading methods yield statistically similar results but differ from the manual grading.This gap between manual and automated scores can be explained by a lack of calibration of the automatic scoring system.Nevertheless, the fact that the general slope of each line is similar might suggest that automatic grading can be a viable alternative to manual grading, allowing us to scale our methodology in the future.</p>
<p>In Figure 2, we present detailed histograms of scores for each question and hint type.In Table 2, we depicted the population standard deviation of each score group.Notably, we obtain the highest standard deviation for the vague-hint category.This suggests that while low-quality context can improve the mean performance, it also increases the variability of responses.</p>
<p>Consensus between manual grading and automatic grading</p>
<p>Consensus across all three grading strategies highlighted two key insights: firstly, insightful hints notably enhance GPT-4's performance in terms of mean scores; secondly, responses generated without hints performed the worst.These results corroborate our initial hypothesis that highly relevant hints significantly boost GPT-4's efficiency in addressing questions of scientific domains.Interestingly, our results also suggest that "irrelevant hints" boost the performance of our LLM in a similar way as "vague hints" (cf.again Figure 1).This unexpected outcome could imply a misalignment between human perception of hint relevance and the model's interpretation.It raises the possibility that what seems relevant and helpful to a human may not necessarily resonate in the same way with GPT-4, underscoring the nuanced dynamics of human-AI interaction in educational contexts.</p>
<p>6 Proof of Concept</p>
<p>QuantumGPT platform</p>
<p>To further improve the performance and versatility of our prompt engineering approach, we have designed a platform (denoted below as QuantumGPT and available at www.quantumgpt.science)that can help us gather more data in a crowd-sourcing manner.The general pipeline of QuantumGPT is shown in Figure 3.It is a prompt-engineering project that utilizes user feedback to improve the response of GPT-4.In addition, the platform has interactive features, including chat, discussions, and the ability to post blog-style articles.</p>
<p>The Credibility-Enhanced Feedback Architecture, CEFA</p>
<p>The accuracy and quality of data are critical to the performance.Some previous works have discussed the threat of data poisoning attacks [Chan et al., 2020, Fowl et al., 2021].Even without a malicious attack, we are still vulnerable to unintentional inaccurate information.To address this concern There is also a separate credibility system to examine the interaction between users and evaluate the credibility of each user.The distillation algorithm distills insights from users' comments.Insights are then passed to the relevance evaluation algorithm to generate a relevance score.Finally, insights with their respective relevance and credibility scores are passed to the optimization algorithm to engineer the prompt.</p>
<p>Distillation Algorithm</p>
<p>The Distillation Algorithm module is developed to optimize the utility of user-generated content within QuantumGPT, aiming to refine the information flow by reducing the token count of user comments while preserving their intrinsic insights.This algorithm is adept at processing raw user comments, transforming them into a distilled format that retains the core insights but minimizes unnecessary verbosity and redundancy.Such a transformation is instrumental in significantly enhancing the data handling efficiency within the QuantumGPT framework.</p>
<p>The deployment of these summarized insights contributes notably to the overall performance of Quan-tumGPT.By reducing the length of the inputs, the algorithm effectively diminishes the computational burden on the system.This reduction in computational load directly translates into an enhanced speed of response generation, thereby elevating the system's capability to handle complex user interactions swiftly and accurately.</p>
<p>Relevance Evaluation Algorithm</p>
<p>The Relevance Evaluation Algorithm is designed to ensure precise and relevant dialogue in quantum physics.It utilizes advanced Natural Language Processing techniques to assess the semantic relevance between user queries and responses, going beyond traditional keyword matching to include semantic similarity and contextual embedding models.</p>
<p>The algorithm employs sentence-transformer models to generate embeddings from user inputs, analyzing semantic relationships through cosine similarity measures.This process allows for the generation of relevance scores, which indicate the semantic alignment between questions and comments, ensuring contextually relevant and precise responses from QuantumGPT.</p>
<p>Credibility Algorithm</p>
<p>The Credibility Algorithm is designed to enhance online discourse quality by assessing user interactions.This algorithm identifies three ways for users to gain credibility: prior credentials (such as a relevant university affiliation), experience using the platform, and interactions with other users.</p>
<p>The algorithm operates within a dynamic feedback mechanism, as depicted in the Figure 5.It functions by analyzing the sentiment of a comment from User 1 towards User 2. If User 1, who holds a high credibility standing, expresses a positive sentiment in their comment, the algorithm activates  the Update Score Calculator.This calculator, in turn, increases User 2's credibility score, provided the sentiment analysis results are positive.</p>
<p>For the purpose of the sentiment analysis, we use the "distilbert-base-uncased-finetuned-sst-2-english" language model.This model, a streamlined version of the BERT framework, has been fine-tuned on the Stanford Sentiment Treebank (SST-2) dataset, making it highly effective in analyzing user comments.The sentiment scores derived from this analysis are used to adjust the credibility scores of users, with a particular emphasis on positive feedback from highly credible users.This process ensures that the platform's credibility ratings reflect constructive and positive user interactions.</p>
<p>Context Window and Summarization Methods</p>
<p>Each LLM, including GPT-4, can only accept a finite-size input shorter than a specific context window.Therefore, in any practical implementation, we must implement a summarization technique to keep the context concise.In this analysis, we test the impact of the context summarization on the performance of the LLM.</p>
<p>An additional advantage of context summarization is that shorter input reduces the computational load on the system and increases the speed of response generation.For our preliminary methodology, as our primary summarization model, we utilize BART, which has been fine-tuned on the CNN/Daily Mail dataset.This model is not explicitly trained on scientific texts -a domain that is relatively underrepresented in text summarization models.Despite this limitation, the model demonstrates proficiency in rendering concise summaries.</p>
<p>Our experiment entailed utilizing the BART model on three user hints ranked in relevancy to each question.We measured the outcomes by assessing the extent of length reduction and the effectiveness in preserving critical concepts.For the purpose of this assessment, we modified our auto-grading technique by incorporating the manual chain of thought technique [Wei et al., 2022a] (auto3).This adaptation involves including our manual grading of a response as an example within the auto-grader system.This modification aims to align the results of the auto grader more closely with the outcomes of manual grading.This approach is expected to enhance the accuracy and reliability of (auto3) by incorporating human-like reasoning processes into its grading methodology.</p>
<p>In Table 3, we show the degree to which we were able to shorten each hint.Next, by comparing the correctness of responses prompted with original comments and summarized insights, we measured the quality and relevance of the insights.We show the detailed scores for each type of summarized group of hints in Figure 6.</p>
<p>In Table 4, we compare the average score on the unabridged and summarized context.We see no reduction in the performance in the cases of the "irrelevant" and "vague" hints.This might imply that despite the length reduction, the integrity and informative value of the original content were preserved in the summaries.However, in the case of the "insightful" hints, the reduction in the performance is noticeable.This calls for further studies.One way of obtaining better results could be fine-tuning the BART model for scientific domain data -such intervention should increase the quality of the summaries and might reduce the problem.Our findings underscore the critical role of context in enhancing the performance of Large Language Models (LLMs) like GPT-4, particularly in the specialized domain of quantum physics.The study reveals that providing contextually relevant hints can significantly improve the accuracy and robustness of LLM responses.However, our result also shows that hints relevant to the topic do not provide any more performance boost when compared to irrelevant hints.One hypothesis is that the determining factor to the helpfulness of the hint is insights into the methodology relevant to the question, and insights into the methodology can exist in hints that are not relevant to the topic.Currently, we are working on expanding our dataset to increase the statistical significance of our results.</p>
<p>Promise of Automatic Grading</p>
<p>The exploration of automatic grading methods presents great opportunities.Our experiments with auto1 and auto2 grading methods align decently well with manual grading outcomes (at least, in terms of the perceived sensitivity of this method).This suggests the potential of these methods as cost-effective and scalable alternatives to manual grading.However, the observed discrepancies, particularly in assessing unrelated and vague hints, highlight the need for further refinement and understanding of how LLMs interpret and respond to different types of context.</p>
<p>Challenges and limitations</p>
<p>Although our preliminary results are interesting, we are aware of the small size of our data set (10 entries and 40 responses in total).It should be pointed out that creating hints and conducting manual evaluation is a difficult task that requires domain knowledge.We are currently working on expanding the dataset and collecting questions and feedback regarding our experimental platform, QuantumGPT.</p>
<p>Additionally, while using BART, fine-tuned on the CNN/Daily Mail dataset, is effective for condensing hints into insightful summaries, we have observed a notable loss of critical information, including equations and scientific notations.Given the scarcity of models trained specifically on scientific texts, there is a compelling case for developing a specialized model.Such a model should be adept at summarizing scientific content, striking a balance between brevity and the retention of rich contextual information.This approach would address the gap in processing and summarizing complex scientific sentences.</p>
<p>Future Work</p>
<p>We have identified three pivotal areas for future research.Firstly, the investigation of insight distillation methods.This involves refining algorithms to more effectively condense complex scientific information into concise yet comprehensive insights, enhancing the efficiency and accuracy of LLM responses.Secondly, the further development of automatic grading systems.Lastly, an experiment with the CEFA architecture to test the users' incentives.Another area of interest is the system's robustness, e.g., its sensitivity to noise or inaccurate information.</p>
<p>measured along the z-axis at t 1 , the fraction that remains in the saved subensemble is P +z = 1. 2. For the particles that were measured along the x-axis, half of the spins that were up along x will remain in the saved subensemble.Thus, the fraction that remains is P +x = 0.5 It is important to highlight that these fractions apply under the assumption of ideal measurements.In practice, the actual probabilities could be influenced by experimental uncertainties and possible interactions between particles.</p>
<p>Unrelated_hint: We don't have concrete numbers for the exact measurements, but let's denote the fractions in general.Let's consider the part of the ensemble on which spin along z was measured first.</p>
<p>The spin was initially prepared in the spin-up state along z, so all these particles had spin up along z at the first measurement.After the second measurement along x, about 50% spins will be found up along x (because spins along z and x are not correlated), so this sub-ensemble contributes 50% with spin up in the first measurement.Now consider the part of the ensemble on which spin along x was measured first.Again, about 50% of the spins will be found up along x in the first measurement, and the projection postulate tells us that these spins are all in the spin-up state along x afterwards, assuming ideal measurements.When measured again along x, these spins will all still be found up.So this second sub-ensemble also contributes 50% with spin up in the first measurement.In total, considering these two sub-ensembles combined, out of all particles that are found to have spin up in the second measurement about 50% (on average) also had spin up in the first measurement (either along z or along x, depending on the measurement).Remember that these arguments only make sense for large ensembles, as the results for individual particles are stochastic.</p>
<p>E Model Answer</p>
<p>"Answer": "All of them"</p>
<p>F Autograde Response</p>
<p>"Manual": 6 "Auto-1": 7 "Auto-2": 7</p>
<p>Figure 1 :
1
Figure1: Mean grading score for different levels of hints and different grading modes.The error boxes represent the unbiased sample standard deviation of the mean values.While there is no statistical difference between the two tested auto-grade methods, the manual method yields higher scores than the automated method.</p>
<p>Figure 2 :
2
Figure 2: Score Distribution for responses generated with different prompts and grading techniques</p>
<p>Figure 3 :
3
Figure 3: The depiction of the QuantumGPT's pipeline.</p>
<p>Figure 4 :
4
Figure 4: The schematics of the CEFA architecture.</p>
<p>Figure 5 :
5
Figure 5: The depiction of the Credibility System.</p>
<p>Figure 6 :
6
Figure 6: Auto3 score distributions for responses generated with access to different level of context, with and without applying the summarization technique.</p>
<p>Table 2 :
2
The measured sample standard deviation of each score group.
No_hint Irrelevant Vague InsightfulManual1.691.701.971.69Auto11.691.172.111.91Auto21.851.342.111.43</p>
<p>Table 3 :
3
Hints length and summarized insights length (in the number of characters).
hint1 insight1 hint2 insight2 hint3 insight31168167412275 12842862412275168167 1284286368568068 1284286480686856 128428657069 1687234 12842866 1669235552174 12842867 1852211927248 12842868 2106203927248 12842869 1849302 1288194 128428610 1617302 1288194 1284286</p>
<p>Table 4 :
4
Mean Score for Different Summarized Hints and Grading
Irrelevant Vague InsightfulAuto27.06.67.6Auto3+summaries7.06.86.38 Discussion8.1 Significance of Context in LLM Performance
For example, Newton's third law is valid in classical mechanics but invalid in electrodynamics[Taylor, <br />
] -though the deviation from Newton's third law is significant only for strong magnetic fields or when relativistic effects are significant. Another example can be the conservation of mass, which is valid only for closed systems[Xia, 1992].
AcknowledgementsWe would like to take this opportunity to thank Dr. Stephan Haas for providing critical suggestions in designing the research.We would also like to thank Dr. Daniel Lidar for providing challenging questions and feedback on GPT4's performance in the realm of quantum computing.We would like to thank the users of QuantumGPT for providing feedback and data.Data and Code AvailabilityData and code can be found in the following GitHub repository: https://github.com/mikelixiang88/Automatic_gpt_grader.gitAuthor ContributionsX.L. constructed the quantum question bank, developed the evaluation and auto-grading part of the paper, and proposed the CEFA architecture.H.T. developed the context summarization methods part and part of the auto-grading module, S.C. and A.M. implemented the Relevance Evaluation algorithm, A.M. worked on the optimization algorithm, Z.W. worked on the credibility system and implemented the sentiment analysis module, M.A. provided supervision, proposed the experiment measuring impact of the context, and contributed to the evaluation and the auto-grading part of the paper.All the authors contributed to writing the original draft of this article.Competing InterestsThe authors declare no competing interests.Supplementary MaterialA Sample question "Question": "At time t = 0 a large ensemble of spin-1/2 particles is prepared, all of them in the spin-up state (with respect to the z-axis).They are not subject to any forces or torques.At time t1 &gt; 0 each spin is measured-some along the z direction and others along the x direction (but we aren't told the results).At time t2 &gt; t1 their spin is measured again, this time along the x direction, and those with spin up (along x) are saved as a subensemble (those with spin down are discarded).Question: Of those remaining (the subensemble), what fraction had spin up (along z or x, depending on which was measured) in the first measurement?"B Hints example to the sample questionThe equations in the hints are fed as plain latex language.Here they are presented as readable format.Insightful_hint: If S z was measured at t 1 , everything was spin up; if S x was measured, half of them were spin down, but the second measurement did nothing, and these were thrown away.Vague_hint: Show that the probability of getting spin up (along b) in the first measurement is P + = cos 2 (θ ab /2), and (by extension) the probability of getting spin up in both measurements is P ++ = cos 2 (θ ab /2) cos 2 (θ bc /2).Find the other three probabilities (P +− , P −+ , and P −− ).Beware: If the outcome of the first measurement was spin down, the relevant angle is now the supplement of θ bc .Unrelated_hint: Example 10.5 Yukawa scattering.The Yukawa potential (which is a crude model for the binding force in an atomic nucleus) has the forme −µr r where β and µ are constants.The Born approximation gives.Example 10.6 Rutherford scattering.If we put in β = q 1 q 2 /4πϵ 0 , µ = 0, the Yukawa potential reduces to the Coulomb potential, describing the electrical interaction of two-point charges.Evidently, the scattering amplitude is f (θ) ≈ − 2mq 1 q 2 4πϵ 0 ℏ 2 κ 2 or (using Equations 10.89 and 10.51):The differential cross-section is the square of this:which is precisely the Rutherford formula (Equation10.11).It happens that for the Coulomb potential classical mechanics, the Born approximation, and quantum field theory all yield the same result.As they say in the computer business, the Rutherford formula is amazingly "robust."C Summarization exampleSummarized_Insightful_hint: If S z was measured at t 1 , everything was spin up.If S x was measured, half of them were spin down.But the second measurement did nothing, and these were thrown away.Summarized_Vague_hint: Show that the probability of getting spin up (along b) in the first measurement is P + = cos 2 (θ ab /2), and (by extension) the probabilities of getting spin up in both measurements is P ++ = cos 2 (θ ab /2) cos 2 (θ bc /2).Find the other three probabilities (P +− , P −+ , and P −− ).Summarized_Unrelated_hint: The Yukawa potential (which is a crude model for the binding force in an atomic nucleus) has the form.The Rutherford formula is amazingly "robust."It happens that for the Coulomb potential, classical mechanics, the Born approximation, and quantum field theory all yield the same result.D Generated AnswerThe equations in the responses are in plain latex language.Here they are presented in readable format.No_hint: All the spins are prepared in the up state along the z axis at t = 0.This means that initially the state of any spin is |up_z⟩.The measurement at t 1 doesn't change the state, it simply gives us information about the state; however, any given spin is now in a superposition of spin up and spin down along the x direction.The key result you need here is that the |up_z⟩ state is a superposition of |up_x⟩ and |down_x⟩ states.Specifically,Furthermore, the square of the amplitude gives the probability.Thus, prior to the first measurement, each spin has a (1/ √ 2) 2 = 1/2 chance of being measured as |up_x⟩, and the same chance of being measured as |down_x⟩.When the second measurement is made at time t 2 , only the spins that give |up_x⟩ are kept.Since the states |up_x⟩ and |down_x⟩ were equally likely at t 1 , this discards half the spins.Thus, the fraction of spins remaining that were spin up at t 1 (either along z or x, whichever was measured) is 1/2.However, keep in mind that the spins measured along the z direction at t 1 are still in the state |up_z⟩, regardless of the later measurement.So, for those spins that were measured along the z direction at t 1 , all of them were spin up.This is independent of the later measurement along the x axis at t 2 .In summary, the final answer will depend on how many spins were measured along x or z at t 1 .But for any given spin, the fraction that were spin up at t 1 is 1 if measured along z, and 1/2 if measured along x.Insightful_hint: Let's divide the ensemble into two parts based on the measurements at time t 1 : 1.Those where the spin was measured along the z direction.2. Those where the spin was measured along the x direction.For particles in the first category, since all particles are initially in spin up state with respect to the z axis, every spin measured will turn out to be up.Therefore, all particles in this category would have been spin up in the first measurement at t 1 .For the second category, where the spin is measured along the x axis at t 1 , the outcomes will be in a superposition of spin up and spin down along the x axis, due to the angle between the z and x direction.When measurement along x is made, it will have equal probabilities being found in the plus or minus state, due to the orthogonality of the two axes.Therefore, the fraction of particles that would have been measured as spin up during the first measurement for the particles remaining in the subensemble would be: -100-50Without the proportions of the particles that were measured along x and z, it's not possible to provide a concrete combined percentage.But for measurements along z it would always be 100Vague_hint: The problem involves sequential measurements of spin 1/2 particles, where the measurements are performed along different axes.Consider the measurement of these particles along the x-axis at time t 1 (these constitute one part of the original ensemble) and the z-axis at t 2 .The basis states with regard to the z-axis are | + z⟩ and | − z⟩ which, when expressed in the x-basis, would be represented as follows: 1.For spins up along the z-axis: | + z⟩ = 1 which are spin-up along x, then half of the spins that were measured along x at t 1 would be thrown away.Therefore, it is given that states of particles can either be spin-up along z or spin-up along x (depending on which was measured) in the first measurement.From here, considering the populations of these particles is consistent with both measurements, we find that: 1.For the particles that were
More Is Different: Broken symmetry and the nature of the hierarchical structure of science. P W Anderson, 10.1126/science.177.4047.393Science. 1095-92031774047August 1972</p>
<p>PROST: Physical Reasoning about Objects through Space and Time. Stephane Aroca-Ouellette, Cory Paik, Alessandro Roncone, Katharina Kann, 10.18653/V1/2021.FINDINGS-ACLFindings of the Association for Computational Linguistics: ACL/IJCNLP 2021. Chengqing Zong, Fei Xia, Wenjie Li, Roberto Navigli, Association for Computational LinguisticsAugust 1-6, 20212021ACL/IJCNLP 2021 of Findings of ACL</p>
<p>10.18653/v1/2021.findings-acl.404URL. </p>
<p>On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?. Emily M Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, 10.1145/3442188.3445922Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21. the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21ACMMarch 2021</p>
<p>Graph of Thoughts: Solving Elaborate Problems with Large Language Models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, Torsten Hoefler, 2023</p>
<p>PIQA: Reasoning about Physical Commonsense in Natural Language. Yonatan Bisk, Rowan Zellers, Le Ronan, Jianfeng Bras, Yejin Gao, Choi, 10.1609/aaai.v34i05.6239The Thirty-Fourth AAAI Conference on Artificial Intelligence. New York, NY, USAAAAI PressFebruary 7-12, 2020. 20202020</p>
<p>Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Michael S Sydney Von Arx, Jeannette Bernstein, Antoine Bohg, Emma Bosselut, Erik Brunskill, Shyamal Brynjolfsson, Dallas Buch, Rodrigo Card, Niladri Castellon, Annie Chatterji, Kathleen Chen, Jared Quincy Creel, Dora Davis, Chris Demszky, Moussa Donahue, Esin Doumbouya, Stefano Durmus, John Ermon, Kawin Etchemendy, Li Ethayarajh, Chelsea Fei-Fei, Trevor Finn, Lauren Gale, Karan Gillespie, Noah Goel, Shelby Goodman, Neel Grossman, Tatsunori Guha, Peter Hashimoto, John Henderson, Daniel E Hewitt, Jenny Ho, Kyle Hong, Jing Hsu, Thomas Huang, Saahil Icard, Dan Jain, Pratyusha Jurafsky, Siddharth Kalluri, Geoff Karamcheti, Fereshte Keeling, Omar Khani, Pang Wei Khattab, Mark Koh, Ranjay Krass, Rohith Krishna, Ananya Kuditipudi, Faisal Kumar, Mina Ladhak, Tony Lee, Jure Lee, Isabelle Leskovec, Levent, Lisa Xiang, Xuechen Li, Tengyu Li, Ali Ma, Christopher D Malik, Suvir Manning, Eric Mirchandani, Zanele Mitchell, Suraj Munyikwa, Avanika Nair, Deepak Narayan, Ben Narayanan, Allen Newman, Juan Carlos Nie, Hamed Niebles, Julian Nilforoshan, Giray Nyarko, Andy Ogut, Krishnan Shih, Alex Srinivasan, Rohan Tamkin, Armin W Taori, Florian Thomas, Rose E Tramèr, William Wang, Bohan Wang, Jiajun Wu, Yuhuai Wu, Sang Wu, Michihiro Michael Xie, Jiaxuan Yasunaga, Matei You, Michael Zaharia, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zhang, Zheng, Joon Sung Park. Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Laurel Orr, Isabel Papadimitriou2021Kaitlyn Zhou, and Percy Liang. On the Opportunities and Risks of Foundation Models</p>
<p>Language Models are Few-Shot Learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, Annual Conference on Neural Information Processing Systems. Hugo Larochelle, Marc ' , Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, Hsuan-Tien Lin, NeurIPS2020. 2020. December 6-12, 2020, virtual, 202033Advances in Neural Information Processing Systems</p>
<p>Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder. Alvin Chan, Yi Tay, Yew-Soon Ong, Aston Zhang, 10.18653/v1/2020.findings-emnlp.373Findings of the Association for Computational Linguistics: EMNLP 2020. Trevor Cohn, Yulan He, Yang Liu, Association for Computational Linguistics16-20 November 2020. 2020EMNLP 2020 of Findings of ACL</p>
<p>Szeyi Chan, Jiachen Li, Bingsheng Yao, Amama Mahmood, Chien-Ming Huang, Holly Jimison, Elizabeth D Mynatt, Dakuo Wang, Mango Mango, How to Let The Lettuce Dry Without A Spinner?": Exploring User Perceptions of Using An LLM-Based Conversational Assistant Toward Cooking Partner. 2023</p>
<p>On the Effectiveness of Large Language Models in Domain-Specific Code Generation. Meng Chen, Hongyu Zhang, Chengcheng Wan, Zhao Wei, Yong Xu, Juhong Wang, Xiaodong Gu, 2023</p>
<p>Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, 2018</p>
<p>A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level. Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin Liu, Linda Chen, Sunny Tran, Newman Cheng, Roman Wang, Nikhil Singh, Taylor L Patti, Jayson Lynch, Avi Shporer, Nakul Verma, Eugene Wu, Gilbert Strang, 10.1073/pnas.2123433119Proceedings of the National Academy of Sciences. 1091-649011932August 2022</p>
<p>Adversarial Examples Make Strong Poisons. Liam Fowl, Micah Goldblum, Ping-Yeh Chiang, Jonas Geiping, Wojciech Czaja, Tom Goldstein, ; Marc, Aurelio Ranzato, Alina Beygelzimer, Yann N Dauphin, Percy Liang, Jennifer Wortman Vaughan, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021. December 6-14, 2021. 2021</p>
<p>Image Style Transfer Using Convolutional Neural Networks. Leon A Gatys, Alexander S Ecker, Matthias Bethge, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). </p>
<p>. 10.1109/cvpr.2016.265June 2016IEEE</p>
<p>David J Griffiths, Darrell F Schroeter, Introduction to Quantum Mechanics. Cambridge University Press2018</p>
<p>What Can Large Language Models Do in Chemistry? A Comprehensive Benchmark on Eight Tasks. Taicheng Guo, Kehan Guo, Bozhao Nan, Zhenwen Liang, Zhichun Guo, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, 2023</p>
<p>Do As I Can, Not As I Say: Grounding Language in Robotic Affordances. Brian Ichter, Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Dmitry Kalashnikov, Sergey Levine, Yao Lu, Carolina Parada, Kanishka Rao, Pierre Sermanet, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Mengyuan Yan, Noah Brown, Michael Ahn, Omar Cortes, Nicolas Sievers, Clayton Tan, Sichun Xu, Diego Reyes, Jarek Rettinghouse, Jornell Quiambao, Peter Pastor, Linda Luu, Kuang-Huei Lee, Yuheng Kuang, Sally Jesmonth, J Nikhil, Kyle Joshi, Rosario Jeffrey, Jasmine Jauregui Ruano, Keerthana Hsu, Byron Gopalakrishnan, Andy David, Chuyuan Kelly Zeng, Fu, of Proceedings of Machine Learning Research. Karen Liu, Dana Kulic, Jeffrey Ichnowski, CoRL; Auckland, New ZealandPMLR2022, 14-18 December 2022. 2022205Conference on Robot Learning</p>
<p>Mathematical Reasoning using Large Language Models. Shima Imani, Liang Du, Harsh Shrivastava, Mathprompter, 10.18653/v1/2023.acl-industry.4Proceedings of the The 61st Annual Meeting of the Association for Computational Linguistics: Industry Track, ACL 2023. Sunayana Sitaram, Beata Beigman Klebanov, Jason D Williams, the The 61st Annual Meeting of the Association for Computational Linguistics: Industry Track, ACL 2023Toronto, CanadaAssociation for Computational LinguisticsJuly 9-14, 20232023</p>
<p>Large Language Models are Zero-Shot Reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, NeurIPS. 2022</p>
<p>LLM-based Control Code Generation using Image Recognition. Heiko Koziolek, Anne Koziolek, 2023</p>
<p>Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, Nan Huo, Xuanhe Zhou, Chenhao Ma, Guoliang Li, Kevin C C Chang, Fei Huang, Reynold Cheng, Yongbin Li, Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs. 2023a</p>
<p>HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models. Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, Ji-Rong Wen, 2023b</p>
<p>Human-Centered Planning. Yuliang Li, Nitin Kamra, Ruta Desai, Alon Halevy, 2023c</p>
<p>Code as Policies: Language Model Programs for Embodied Control. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, 10.1109/ICRA48891.2023.10160591IEEE International Conference on Robotics and Automation, ICRA 2023. London, UKIEEEMay 29 -June 2, 20232023</p>
<p>LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models. Yen-Ting Lin, Yun-Nung Chen, 2023</p>
<p>BLEU: A Method for Automatic Evaluation of Machine Translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting on Association for Computational Linguistics -ACL '02, ACL '02. the 40th Annual Meeting on Association for Computational Linguistics -ACL '02, ACL '02Association for Computational Linguistics2001</p>
<p>The Troubling Emergence of Hallucination in Large Language Models -An Extensive Definition, Quantification, and Prescriptive Remediations. Swagata Vipula Rawte, Agnibh Chakraborty, Anubhav Pathak, S Sarkar, Aman Towhidul Islam Tonmoy, Amit P Chadha, Amitava Sheth, Das, 2023</p>
<p>Enchancing Semi-Supervised Learning for Extractive Summarization with an LLM-based pseudolabeler. Gaurav Sahu, Olga Vechtomova, Issam H Laradji, 2023</p>
<p>Large Language Models can Strategically Deceive their Users when Put Under Pressure. Jérémy Scheurer, Mikita Balesni, Marius Hobbhahn, 2023Technical Report</p>
<p>Metallic Phase with Long-Range Orientational Order and No Translational Symmetry. D Shechtman, I Blech, D Gratias, J W Cahn, 10.1103/PhysRevLett.53.1951Phys. Rev. Lett. 53Nov 1984</p>
<p>Generate and Pray: Using SALLMS to Evaluate the Security of LLM Generated Code. Mohammed Latif Siddiq, Joanna C S Santos, 10.48550/arXiv.2311.008892023</p>
<p>SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research. Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, Kai Yu, 2023</p>
<p>The wisdom of crowds. James Surowiecki, June 2004. ISBN 0385721706Doubleday Books</p>
<p>John Taylor, Classical Mechanics, chapter 1. Mill Valley, California2005University Science BooksISBN 9781891389221</p>
<p>Language Models for Biomedical Research: Opportunities and Pitfalls. Surendrabikram Thapa, Surabhi Adhikari, Chatgpt, Bard, Large, 10.1007/s10439-023-03284-0Annals of Biomedical Engineering. 1573-96865112June 2023</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, NeurIPS2022a</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, NeurIPS2022b</p>
<p>Taxonomy of Risks posed by Language Models. Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, Courtney Biles, Sasha Brown, Zac Kenton, Will Hawkins, Tom Stepleton, Abeba Birhane, Lisa Anne Hendricks, Laura Rimell, William Isaac, Julia Haas, Sean Legassick, Geoffrey Irving, Iason Gabriel, 10.1145/3531146.35330882022 ACM Conference on Fairness, Accountability, and Transparency, FAccT '22. ACMJune 2022</p>
<p>The Existence of Noncollision Singularities in Newtonian Systems. Zhihong Xia, 10.2307/2946572The Annals of Mathematics. 0003-486X1353411May 1992</p>
<p>A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models. Haoran Xu, Young , Jin Kim, Amr Sharaf, Hany Hassan Awadalla, 2023</p>
<p>Tree of Thoughts: Deliberate Problem Solving with Large Language Models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, 2023</p>
<p>BERTScore: Evaluating Text Generation with BERT. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, 8th International Conference on Learning Representations. Addis Ababa, EthiopiaApril 26-30, 2020. 20202020OpenReview.net</p>
<p>What Makes Good Examples for Visual In-Context Learning?. Yuanhan Zhang, Kaiyang Zhou, Ziwei Liu, 2023</p>
<p>AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7309 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7309</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7309</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-138.html">extraction-schema-138</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <p><strong>Paper ID:</strong> paper-268248989</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.03154v2.pdf" target="_blank">Quantum many-body physics calculations with large language models</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have demonstrated abilities to perform complex tasks in multiple domains, including mathematical and scientific reasoning. We demonstrate that with carefully designed prompts, LLMs can accurately carry out key calculations in research papers in theoretical physics. We focus on a broadly-used approximation method in quantum physics: the Hartree-Fock method, requiring an analytic multi-step calculation deriving approximate Hamiltonian and corresponding self-consistency equations. To carry out the calculations using LLMs, we design multi-step prompt templates that break down the analytic calculation into standardized steps with placeholders for problem-specific information. We evaluate GPT-4’s performance in executing the calculation for 15 papers from the past decade, demonstrating that, with the correction of intermediate steps, it can correctly derive the final Hartree-Fock Hamiltonian in 13 cases. Aggregating across all research papers, we find an average score of 87.5 (out of 100) on the execution of individual calculation steps. We further use LLMs to mitigate the two primary bottlenecks in this evaluation process: (i) extracting information from papers to fill in templates and (ii) automatic scoring of the calculation steps, demonstrating good results in both cases. Large language models (LLM) can tackle complex mathematical and scientific reasoning tasks. The authors show that, guided by carefully designed prompts, LLM can achieve high accuracy in carrying out analytical calculations in theoretical physics - the derivation of Hartree-Fock equations - with an average score of 87.5 in GPT-4 across calculation steps from recent research papers.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7309.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7309.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 HF-execution</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 executing Hartree–Fock mean-field analytic derivations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 was used as a text-based simulator to carry out multi-step, research‑level analytic calculations in quantum many-body physics, specifically deriving Hartree–Fock (HF) Hamiltonians and associated self-consistency equations from paper-specific templates and abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned LLM accessed via API/web interface; not fine-tuned on domain-specific data or augmented with external retrieval/tools in these experiments</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Quantum many-body physics / condensed matter theory</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Analytic derivation of Hartree–Fock mean-field Hamiltonian H_HF and associated order parameters / self-consistency equations from problem statements (paper text, abstracts, or completed templates); multi-step symbolic/schematic calculation rather than numerical simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>A bespoke multi-step HF prompt template decomposing the derivation into five high-level conceptual steps and many bite-sized tasks; sequential prompts P_i feed outputs into subsequent prompts, with human evaluation/correction between steps; templates include examples to reduce hallucination; for information extraction from abstracts/excerpts the model was prompted to quote source text and provide explanation before answering; one-shot example improved placeholder extraction; zero-shot and few-shot-with-rationale settings used for self-scoring experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Custom four-layered rubric applied per prompt response: Adherence, Rigor, Knowledge, Correctness — each scored categorically (0, 50, 100) and averaged; information-extraction placeholders scored categorically (0, 50, 100) and averaged; additional counts (e.g., exact final-H_HF match) reported; LLM-Scorer evaluation used class-balanced accuracy for binary Correct/Incorrect.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Average aggregated execution score across all rubric layers and all 15 research papers: 87.5 / 100; Rigor averaged above 95 / 100; GPT-4 produced exactly correct final HF Hamiltonian in 13 of 15 papers and made minor errors in 2 papers; information-extraction for a specific template task (T4) initial mean 44 ± 8 (across 40 placeholders over 5 papers) improved to 80 ± 6 with a one-shot example; per-prompt categorical scoring statistics reported (see rubric).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>No alternative LLM baseline reported; human expert scoring (expert H.P.) used as ground truth for correctness; no random or smaller-model baselines provided in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Specificity and degree of detail in the provided abstract or excerpt (more specific prompts improve success).', 'Correctness and completeness of template placeholder values (executions assume correctly filled placeholders).', 'Providing in-template examples reduces hallucination and improves adherence.', 'One-shot example exposure substantially improves information-extraction performance.', 'Sequential decomposition with human evaluation/correction between steps prevents error propagation.', 'Degree of explicit overlap between a target solution and model training data appeared not to strongly affect execution scores (training cutoff Sep 2021 tested).', 'Tasks requiring implicit domain conventions/notation (not stated in text) reduce information-extraction accuracy.']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Experiments used GPT-4 checkpoints ('gpt-4' and 'gpt-4-0613') from mid-2023; the abstract-to-execution experiment used the GPT-4 web interface; corpus: 15 recent APS research papers selected from 807 preprints; HF-template contains ~76 placeholders per paper; human-in-the-loop evaluation and correction after each prompt; categorical scoring (0/50/100); no model temperature or sampling hyperparameters reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Model struggled most with extracting or inventing implicit notation/conventions requiring prior domain knowledge (e.g., correct second-quantized Fourier transform expressions); occasional sign/index/typo errors led to partial credit; Correctness scores are lowered by any error in other rubric layers; information-extraction performance is more irregular and lower than the execution performance; some questions required multiple tries (2–3 attempts) to reach correct answers; potential (but not demonstrated) overlap with training data remains an uncertainty; experiments rely on human corrections, so fully-automated pipeline not demonstrated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantum many-body physics calculations with large language models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7309.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7309.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 LLM-Scorer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 used to self-score execution outputs (LLM-Scorer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors tested GPT-4's ability to classify its own execution outputs as Correct or Incorrect and to provide rationales, comparing to human-assigned Correctness scores to evaluate feasibility of automated scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned LLM accessed via API/web interface; used in zero-shot and few-shot-with-rationale scoring settings</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Meta-evaluation / automated scoring of physics derivations (applies to quantum many-body HF tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Binary classification (Correct vs Incorrect) of GPT-4's own analytical execution outputs; in few-shot setting the model was prompted to provide a rationale for its classification.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot classification and few-shot-with-rationale prompting (examples + request for rationale); prompts asked the model to declare Incorrect or Correct relative to human ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Class-balanced accuracy (two-class Correct/Incorrect) measured against human-assigned Correctness scores; additional reporting of ability to identify human-assigned incorrect items.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>LLM-Scorer class-balanced accuracy: 69% in zero-shot, 74% in few-shot-with-rationale; when prompted for a rationale, it identified 72.5% of problems that human-assigned Correctness scored 0 or 50 as incorrect.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Human expert evaluation used as the reference (ground truth); no automated baseline other than the LLM's zero-shot behavior was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>["Providing a rationale (few-shot-with-rationale) improved the LLM-Scorer's accuracy compared to zero-shot.", 'Few-shot examples increased scoring accuracy relative to zero-shot.', "Limitations in the LLM's ability to detect subtle mathematical/physics errors limit scoring reliability."]</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Scoring experiments performed comparing model classifications to human Correctness scores from the four-layer rubric; class-balanced accuracy reported; exact prompt templates for scoring described in SM Section I (paper); no sampling hyperparameters specified.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>LLM-Scorer is imperfect (≈70–74% accuracy) and misses a substantial fraction of incorrect items; not a substitute for human evaluation in its present form; tends to miss some errors even when asked for rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantum many-body physics calculations with large language models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The impact of large language models on scientific discovery: a preliminary study using gpt-4 <em>(Rating: 2)</em></li>
                <li>Emergent autonomous scientific research capabilities of large language models <em>(Rating: 2)</em></li>
                <li>Paperqa: Retrieval-augmented generative agent for scientific research <em>(Rating: 2)</em></li>
                <li>Augmenting large-language models with chemistry tools <em>(Rating: 1)</em></li>
                <li>Solving quantitative reasoning problems with language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7309",
    "paper_id": "paper-268248989",
    "extraction_schema_id": "extraction-schema-138",
    "extracted_data": [
        {
            "name_short": "GPT-4 HF-execution",
            "name_full": "GPT-4 executing Hartree–Fock mean-field analytic derivations",
            "brief_description": "GPT-4 was used as a text-based simulator to carry out multi-step, research‑level analytic calculations in quantum many-body physics, specifically deriving Hartree–Fock (HF) Hamiltonians and associated self-consistency equations from paper-specific templates and abstracts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "model_type": "instruction-tuned LLM accessed via API/web interface; not fine-tuned on domain-specific data or augmented with external retrieval/tools in these experiments",
            "scientific_domain": "Quantum many-body physics / condensed matter theory",
            "simulation_task_description": "Analytic derivation of Hartree–Fock mean-field Hamiltonian H_HF and associated order parameters / self-consistency equations from problem statements (paper text, abstracts, or completed templates); multi-step symbolic/schematic calculation rather than numerical simulation.",
            "prompting_strategy": "A bespoke multi-step HF prompt template decomposing the derivation into five high-level conceptual steps and many bite-sized tasks; sequential prompts P_i feed outputs into subsequent prompts, with human evaluation/correction between steps; templates include examples to reduce hallucination; for information extraction from abstracts/excerpts the model was prompted to quote source text and provide explanation before answering; one-shot example improved placeholder extraction; zero-shot and few-shot-with-rationale settings used for self-scoring experiments.",
            "evaluation_metric": "Custom four-layered rubric applied per prompt response: Adherence, Rigor, Knowledge, Correctness — each scored categorically (0, 50, 100) and averaged; information-extraction placeholders scored categorically (0, 50, 100) and averaged; additional counts (e.g., exact final-H_HF match) reported; LLM-Scorer evaluation used class-balanced accuracy for binary Correct/Incorrect.",
            "reported_accuracy": "Average aggregated execution score across all rubric layers and all 15 research papers: 87.5 / 100; Rigor averaged above 95 / 100; GPT-4 produced exactly correct final HF Hamiltonian in 13 of 15 papers and made minor errors in 2 papers; information-extraction for a specific template task (T4) initial mean 44 ± 8 (across 40 placeholders over 5 papers) improved to 80 ± 6 with a one-shot example; per-prompt categorical scoring statistics reported (see rubric).",
            "baseline_accuracy": "No alternative LLM baseline reported; human expert scoring (expert H.P.) used as ground truth for correctness; no random or smaller-model baselines provided in experiments.",
            "factors_reported": [
                "Specificity and degree of detail in the provided abstract or excerpt (more specific prompts improve success).",
                "Correctness and completeness of template placeholder values (executions assume correctly filled placeholders).",
                "Providing in-template examples reduces hallucination and improves adherence.",
                "One-shot example exposure substantially improves information-extraction performance.",
                "Sequential decomposition with human evaluation/correction between steps prevents error propagation.",
                "Degree of explicit overlap between a target solution and model training data appeared not to strongly affect execution scores (training cutoff Sep 2021 tested).",
                "Tasks requiring implicit domain conventions/notation (not stated in text) reduce information-extraction accuracy."
            ],
            "experimental_conditions": "Experiments used GPT-4 checkpoints ('gpt-4' and 'gpt-4-0613') from mid-2023; the abstract-to-execution experiment used the GPT-4 web interface; corpus: 15 recent APS research papers selected from 807 preprints; HF-template contains ~76 placeholders per paper; human-in-the-loop evaluation and correction after each prompt; categorical scoring (0/50/100); no model temperature or sampling hyperparameters reported.",
            "limitations_or_failure_modes": "Model struggled most with extracting or inventing implicit notation/conventions requiring prior domain knowledge (e.g., correct second-quantized Fourier transform expressions); occasional sign/index/typo errors led to partial credit; Correctness scores are lowered by any error in other rubric layers; information-extraction performance is more irregular and lower than the execution performance; some questions required multiple tries (2–3 attempts) to reach correct answers; potential (but not demonstrated) overlap with training data remains an uncertainty; experiments rely on human corrections, so fully-automated pipeline not demonstrated.",
            "uuid": "e7309.0",
            "source_info": {
                "paper_title": "Quantum many-body physics calculations with large language models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "GPT-4 LLM-Scorer",
            "name_full": "GPT-4 used to self-score execution outputs (LLM-Scorer)",
            "brief_description": "The authors tested GPT-4's ability to classify its own execution outputs as Correct or Incorrect and to provide rationales, comparing to human-assigned Correctness scores to evaluate feasibility of automated scoring.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "model_type": "instruction-tuned LLM accessed via API/web interface; used in zero-shot and few-shot-with-rationale scoring settings",
            "scientific_domain": "Meta-evaluation / automated scoring of physics derivations (applies to quantum many-body HF tasks)",
            "simulation_task_description": "Binary classification (Correct vs Incorrect) of GPT-4's own analytical execution outputs; in few-shot setting the model was prompted to provide a rationale for its classification.",
            "prompting_strategy": "Zero-shot classification and few-shot-with-rationale prompting (examples + request for rationale); prompts asked the model to declare Incorrect or Correct relative to human ground truth.",
            "evaluation_metric": "Class-balanced accuracy (two-class Correct/Incorrect) measured against human-assigned Correctness scores; additional reporting of ability to identify human-assigned incorrect items.",
            "reported_accuracy": "LLM-Scorer class-balanced accuracy: 69% in zero-shot, 74% in few-shot-with-rationale; when prompted for a rationale, it identified 72.5% of problems that human-assigned Correctness scored 0 or 50 as incorrect.",
            "baseline_accuracy": "Human expert evaluation used as the reference (ground truth); no automated baseline other than the LLM's zero-shot behavior was reported.",
            "factors_reported": [
                "Providing a rationale (few-shot-with-rationale) improved the LLM-Scorer's accuracy compared to zero-shot.",
                "Few-shot examples increased scoring accuracy relative to zero-shot.",
                "Limitations in the LLM's ability to detect subtle mathematical/physics errors limit scoring reliability."
            ],
            "experimental_conditions": "Scoring experiments performed comparing model classifications to human Correctness scores from the four-layer rubric; class-balanced accuracy reported; exact prompt templates for scoring described in SM Section I (paper); no sampling hyperparameters specified.",
            "limitations_or_failure_modes": "LLM-Scorer is imperfect (≈70–74% accuracy) and misses a substantial fraction of incorrect items; not a substitute for human evaluation in its present form; tends to miss some errors even when asked for rationales.",
            "uuid": "e7309.1",
            "source_info": {
                "paper_title": "Quantum many-body physics calculations with large language models",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The impact of large language models on scientific discovery: a preliminary study using gpt-4",
            "rating": 2,
            "sanitized_title": "the_impact_of_large_language_models_on_scientific_discovery_a_preliminary_study_using_gpt4"
        },
        {
            "paper_title": "Emergent autonomous scientific research capabilities of large language models",
            "rating": 2,
            "sanitized_title": "emergent_autonomous_scientific_research_capabilities_of_large_language_models"
        },
        {
            "paper_title": "Paperqa: Retrieval-augmented generative agent for scientific research",
            "rating": 2,
            "sanitized_title": "paperqa_retrievalaugmented_generative_agent_for_scientific_research"
        },
        {
            "paper_title": "Augmenting large-language models with chemistry tools",
            "rating": 1,
            "sanitized_title": "augmenting_largelanguage_models_with_chemistry_tools"
        },
        {
            "paper_title": "Solving quantitative reasoning problems with language models",
            "rating": 1,
            "sanitized_title": "solving_quantitative_reasoning_problems_with_language_models"
        }
    ],
    "cost": 0.009580749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Quantum Many-Body Physics Calculations with Large Language Models</p>
<p>Haining Pan 
Department of Physics
Cornell University
IthacaNYUSA</p>
<p>Nayantara Mudur 
Google Research
94043Mountain ViewCA</p>
<p>Department of Physics
Harvard University
CambridgeMAUSA</p>
<p>Will Taranto 
Department of Physics
Cornell University
IthacaNYUSA</p>
<p>Maria Tikhanovskaya 
Google Research
94043Mountain ViewCA</p>
<p>Department of Physics
Harvard University
CambridgeMAUSA</p>
<p>Subhashini Venugopalan 
Google Research
94043Mountain ViewCA</p>
<p>Yasaman Bahri 
Google DeepMind
94043Mountain ViewCA</p>
<p>Michael P Brenner 
Google Research
94043Mountain ViewCA</p>
<p>Department of Physics
Harvard University
CambridgeMAUSA</p>
<p>School of Engineering and Applied Sciences
Harvard University
CambridgeMAUSA</p>
<p>Eun-Ah Kim 
Department of Physics
Cornell University
IthacaNYUSA</p>
<p>Google Research
94043Mountain ViewCA</p>
<p>Department of Physics
Ewha Womans University
SeoulSouth Korea</p>
<p>Quantum Many-Body Physics Calculations with Large Language Models
6ACC51CAB7980AFC69A595DBF64CF2E4
Large language models (LLMs) have demonstrated an unprecedented ability to perform complex tasks in multiple domains, including mathematical and scientific reasoning.We demonstrate that with carefully designed prompts, LLMs can accurately carry out key calculations in research papers in theoretical physics.We focus on a broadly used approximation method in quantum physics: the Hartree-Fock method, requiring an analytic multi-step calculation deriving approximate Hamiltonian and corresponding self-consistency equations.To carry out the calculations using LLMs, we design multi-step prompt templates that break down the analytic calculation into standardized steps with placeholders for problem-specific information.We evaluate GPT-4's performance in executing the calculation for 15 research papers from the past decade, demonstrating that, with correction of intermediate steps, it can correctly derive the final Hartree-Fock Hamiltonian in 13 cases and makes minor errors in 2 cases.Aggregating across all research papers, we find an average score of 87.5 (out of 100) on the execution of individual calculation steps.Overall, the requisite skill for doing these calculations is at the graduate level in quantum condensed matter theory.We further use LLMs to mitigate the two primary bottlenecks in this evaluation process: (i) extracting information from papers to fill in templates and (ii) automatic scoring of the calculation steps, demonstrating good results in both cases.The strong performance is the first step for developing algorithms that automatically explore theoretical hypotheses at an unprecedented scale.</p>
<p>The last few years have witnessed remarkable progress in developing large language models that can process and generate language, serving as powerful general problem solvers [1][2][3][4].These models have demonstrated impressive capabilities across diverse domains, including multidisciplinary benchmarks [5], undergraduate-level math and science [6,7], coding [8,9], medicine [10,11] and chemistry [12].The developments lower barriers to entry between different domains of knowledge.Yet, a major open question is whether it is possible to use LLMs to assist or augment human reasoning in specialized research settings such as theoretical physics, thereby pushing the generation of new knowledge.For example, the community has begun to explore whether LLMs can assist in solving problems in mathematics [13].The unique challenge of theoretical physics is that the domain requires inherently multi-faceted reasoning using language with specialized vocabulary, mathematics using symbols carrying specialized meanings, and code for numerical solutions.Developing an effective AI assistant will likely require going beyond scaling [14,15].</p>
<p>Recent evaluations of Large Language Models (LLMs) [16][17][18] through qualitative analysis and quantitative benchmarks [10,19,20] indicate that the best and most capable models have knowledge in a wide breadth of domains, much more so than the average person.This has led to an explosion of applications building on the potential of these models.In the natural sciences, recent works that demonstrate the utility of LLMs [21] have focused on testing their knowledge in different domains [6,[22][23][24][25][26][27] or on their ability to generate code to help with automating experimentation processes that utilize software [12,28].While having some domain knowledge is helpful, a model can be more useful if it can apply its knowledge to help solve a research problem.Strong examples of this are in the domains of mathematics and coding [13,[29][30][31].Less known is how theoretical scientists can harness LLMs' capabilities to augment their thinking process in scientific domains, requiring complex multi-step reasoning, where expertise (for humans) takes time and specialization to acquire.While research is typically viewed as a creative activity, the extent to which large parts of the daily work of scientists can be automated is unclear.A major component of theoretical physics research employs well-developed calculational frameworks, executing multi-step calculations integrating technical terminology with mathematical reasoning.Since state-of-the art LLMs typically use the arXiv [32] as part of their training corpus, it is plausible that LLMs have learned to perform sophisticated tasks in theoretical physics.A major challenge is to define the tasks and to establish an effective and robust framework for organizing tasks.</p>
<p>In this work, we investigate the ability of GPT-4 to assist theoretical research in quantum many-body physics.To our knowledge, this is the first investigation in evaluating LLMs on research-level problems in physics.We focus on a well-established and widely used calculational framework of the Hartree-Fock (HF) mean field theory [33].These Hartree-Fock methods follow the treatment arXiv:2403.03154v2[physics.comp-ph]22 Aug 2024 employed in condensed matter physics to study effective theories of quantum many-body systems, in contrast to first-principles approaches for molecules and solids.Although such calculations are widely used tools in theoretical research [34], learning to do the calculations reliably requires years of study.As a first step towards automating HF calculations, we built a multi-step prompt template: the HF template (see Figure 1(a)).This template consists of step-by-step directions suitable for instructing beginning graduate students to do the calculations.The template has placeholders to set up notation and enter problem-specific information Fig. 1(b-c).As an evaluation testbed, we curated a corpus of 15 recent research papers reporting results of HF mean-field theory for various distinct physical systems.Given the correctly filled-in prompt templates, we demonstrate that GPT-4 can accurately carry out the calculations.More ambitiously, GPT-4 can sometimes extract or infer the necessary problem-specific information just from an abstract, supplying the placeholders and carrying out the calculation (see Fig. 1(d-g)).Finally, we test GPT-4's ability to extract correct placeholders from human-selected excerpts of the papers, demonstrating the ability to read critical information.</p>
<p>The HF method is an instance of mean-field theory, where interactions are replaced by "mean fields" proportional to an order parameter, whose fluctuations are ignored.The application of the method neatly separates into an analytical stage of deriving the HF Hamiltonian H HF and the associated order parameter ⟨ ∆symm ⟩ and a computational stage of iteratively solving the selfconsistency equation.Deriving H HF for a given problem involves a series of steps broken down in Fig. 2(a), requiring synthesizing natural language concepts in mathematical terms with symbols and equations.The first step (STEP 1) establishes the dynamical degrees of freedom and fixes the single-particle Hilbert space.The noninteracting Hamiltonian H 0 specifies the particle flavors (spin, orbital, valley, layer, etc.) and the dispersion and potential specific to each flavor.The interaction Hamiltonian H int specifies the distance and flavor-dependent interaction.The system Hamiltonian H should reflect the symmetries of the problem, which will determine the block-diagonal structure of the ultimate H HF .The Fourier transforms in STEP 2 determine the momentum dependence of the eventual H HF .In STEP 3, Wick's theorem is applied to decompose the Hamiltonian based on mean-fields.In STEP 4, the quadratic Hamiltonian is simplified and organized into Hartree terms diagonal in spin space and Fock terms off-diagonal in spin space.In STEP 5, the symmetries of the system are used to reveal order parameter structures ⟨ ∆symm ⟩ in the Hartree and Fock terms.Once H HF is obtained for all symmetrybreaking channels, researchers choose the channel of interest and numerically solve the self-consistency equation (see SM Section A [35]).With the correct H HF in hand, the computational cost of solving the resulting self-consistency equation is negligible.Beyond setting up the matrix self-consistency equations for the numerical evaluation, the analytical derivation of H HF offers physical insight into the problem by revealing the role of interactions in different symmetry-breaking sectors for the system.Fig. 1(c) shows a schematic of an automated derivation of H HF starting from an abstract of a paper.The idea is to supply a synopsis of a calculation in natural language and ask an LLM to figure out the problemspecific information from the synopsis by answering 10 questions (see SM Section E [35]).With each answered question, we ask the LLM to complete the corresponding placeholders in the HF template.We then prompt the LLM with the template to derive H HF for the problem of interest.We demonstrate the feasibility of the idea using the abstract of arXiv:2111.01152[36] and GPT-4.Fig. 1(d) shows a portion of the prompt where we supply the abstract and ask questions.As we show in detail in [35], we instruct GPT-4 to quote the part of the abstract on which it is basing its answer and explain its reasoning before committing to an answer.This structure led to GPT-4 correctly answering all of the 10 questions as exemplified in Fig. 1(e).While most questions were correctly answered on the first try, some questions requiring an inference using prior knowledge took 2-3 attempts for GPT-4 to arrive at correct answers (see SM Section E [35]) As shown in Fig. 1(g), after a six page calculation, GPT-4 outputs the correct H HF compared to Eq.(S9) in arXiv:2111.01152[35] given the prompts with correct placeholders.While the performance of an LLM on this ambitious task naturally depends on the degrees of specificity given in the abstract, this test case establishes that the vision of giving a synopsis of a calculation in natural language to an LLM to carry out a Hartree-Fock calculation is within reach.</p>
<p>To build general prompt templates for deriving H HF , we break down the process into five high-level steps, as shown in Fig. 1(a).The prompt templates further break down the steps into natural algorithmic pieces 11) with placeholders for problem-specific information (see Fig. 1
T i (i = 1, • • • ,</p>
<p>(b) and SM Section C [35]</p>
<p>).To reduce model hallucination, we give examples within the templates, just as we would to teach a beginning graduate student.We consider three general settings of interacting fermion problems: whether or not the system is analyzed in the continuum limit, and whether the Hamiltonian is expressed in second-quantized form (see SM Section C [35]).Once the placeholders are specified (see Fig. 1(b)), we use a sequential series of prompts {P i } to elicit the LLM to perform the calculation.The prompts instruct the LLM to use the information specified in previous steps, as well as the outputs from previous steps (see Fig 2(a)).To avoid propagating errors, the response to each prompt is evaluated and (potentially) corrected before becoming the grounds for the next prompt (Fig 2(b)).Fig. 2(c,d) give an example of a promptresponse pair applying Wick's theorem in step 3. To evaluate the efficacy of the HF-template, we tested the template on 15 papers selected from 807 preprints published in American Physical Society journals during the last decade.The precise rules we used to select papers, the complete database of papers we have analyzed, templates, and complete codes designed for multiple scenarios are available in SM section C in [35].In the remainder of the paper, recent checkpoints of GPT-4 from mid-2023 were used for all evaluations [37].</p>
<p>In evaluating the HF-template, a major bottleneck was substituting relevant paper-specific information into over 76 placeholders of the HF-template for each paper.This template completion requires one to synthesize the conventions and notation of quantum many-body physics with problem-specific information for each paper and infer the relevant information from the context and the label for the placeholder.Hence the template completion task defines an advanced and purposeful information extraction task requiring technical expertise.We explored automation of prompt completion both as a stepping stone towards full automation and as a gauge of the LLM's ability to reason.Specifically, we used a single universal prompt shown in Fig. 3 to turn each template step T i into a completed paper-specific prompt P i by extracting and inferring information from the humanselected excerpt E i (see Fig. 3(b)).</p>
<p>We evaluated information extraction for each placeholder by averaging the scores from two authors (Y.B. and H.P.) who have significant experience in quantum many-body physics.Each placeholder is assigned a categorical score of either 0 (no credit), 50 (partial credit), and 100 (full credit).To glean insight into the score distribution, we grouped the placeholders into three categories: system-specific information, explicit notation (notation commonly specified in the papers), and implicit notation.The latter refers to conventions in the field that are often unspecified in the papers.Fig. 3(c-e) shows the scores for a subset of placeholders in each category.(see SM Section F [35] for the complete set.)GPT-4 did a remarkable job at extracting system-specific information from excerpts as shown in Fig. 3(c), consistent with its success in extracting and inferring such information from the abstract for the preprint arXiv:2111.01152[36] (Fig. 1(c-f)).GPT-4 also consistently performed well when extracting the notation and convention appearing in the excerpt (see Fig. 3(d)).Where it struggled the most was in specifying placeholders which required synthesizing prior knowledge of quantum many-body physics conventions with what could be inferred from the template and the excerpt to introduce a suitable notation (see Fig. 3(e)).An example of the "change of basis" that GPT-4 failed is to fill the placeholder "definition of Fourier Transformation" in Fig. 1(b) with a correct equation using the second-quantized operators.To succeed in this task, the LLM needs prior knowledge of the second-quantized operators in position and momentum basis.Moreover, the LLM has to infer the task from the label and context of the placeholder.</p>
<p>To explore how to improve the information extraction performance, we examine the impact of a one-shot evaluation prompt [1] on the extraction task for the template T 4 , Fourier transforming the interaction term H int (Fig. 1(a)).Here, "one-shot evaluation" means we provide a single example of an excerpt and its correct target output in the prompt (see SM Section H [35]).The initial performance of GPT-4 on the 40 placeholders across 5 papers for T 4 was 44 ± 8. Fig. 3(f) shows that the one-shot prompt significantly enhances the performance, more than doubling the mean performance.The performance across all placeholders in the one-shot setting was 80 ± 6.This implies information extraction for the autogeneration of prompts may be achievable with the help of more exposure to requisite knowledge and examples.Finally, we turn to the main focus of this paper: evaluation, across all steps and papers, of the full HF calculation using our correctly filled-in templates (see SM Section C [35]).The LLM responses are then scored by an expert in Hartree-Fock calculations (H.P.).The response to each prompt is a paragraph of varying length depending on each problem (see Fig. 2(d) for example).In order to evaluate the execution responses in a finegrained manner that is nevertheless standardized across all the papers, we introduced a four-layered rubric system (Fig. 4(a)).The four rubric layers are (1) how closely the LLM executes the instructions (Adherence); (2) accuracy in the LLM's mathematical derivations (Rigor);</p>
<p>(3) consistency in the LLM's reasoning with the laws of physics (Knowledge); and (4) correctness in the LLM's final answer (Correctness).Each response is scored from all four layers on a categorical scale of 0 (no credit), 50 (partial credit), and 100 (full credit).The evaluation results across all papers presented in Fig. 4(b) shows that our HF-template based approach to the Hartree-Fock mean field theory works well at all layers.For Correctness, the score of 100 requires the output to be exactly correct.Often, the results of intermediate steps did not appear in the paper and were calculated by the scorer.For Rigor, occasional errors in indices or subscripts result in partial credit.For example, we give partial credit for Knowledge if the LLM makes a sign error in the momentum shift needed due to the presence of a magnetic field.For Adherence, for example, partial credit would be awarded if the LLM insists on using conventions different from that explicitly directed.We note that the Correctness score is the lowest since errors in any of the three layers will reduce this score.The high and reliable score of above 95 in Rigor shows that GPT-4 was quite capable of carrying out the calculations correctly.In fact, we discovered typos in research papers in our corpus in the process (see SM Section G [35]).Aggregating across all rubric layers and all papers, the average score of GPT-4 is 87.5 from Fig. 4(d), with slight variation across the papers.This is a remarkably high score for a generically trained LLM, signifying expert-level performance.</p>
<p>Fig. 4(c) compares the score across the five steps of the prompt template (Fig. 1(a)), averaged over all four rubrics layers.The performance is uniformly high across the steps, demonstrating that the fine-grained approach of the HF template allows the model to succeed in system-specific Hamiltonian building (STEP 1) and quantum many-body operator algebra in the meanfield decomposition (STEP 3).This uniformly high score across the entire corpus of papers in executing the steps of the calculation is in striking contrast to relatively irregular and overall lower scores in extracting information for the placeholders as summarized in Fig. 4(d).The contrast reveals that even when the model can carry out the actions of a specific analytic calculation following a natural language instruction, making specific plans for the action through an informed reading framed through a calculation structure can be challenging.</p>
<p>A challenge with evaluating LLMs -in particular GPT-4, for which the full training dataset details have not been published -is to what extent the evaluation set overlaps with the training set, and whether there is evidence for generalization to new, never-before-seen problems.Unlike other evaluation tasks, which are typically algorithmically curated or generated, we seek to evaluate LLMs on a research task.Hence, generating new problems is considerably more involved in our setting.Instead, we investigate the issue of generalization through two indirect means.(1) The GPT-4 model used in this paper has the training data cutoff date of September 2021, marked as a dashed line in our chronological summary of overall scores in Fig. 4(d).While OpenAI reports that there are small amounts of more recent data used [16], the fact that the cutoff date does not affect the score is encouraging.(2) A novel aspect of our task curation process is that it required us to fill in the intermediate steps of the calculations laid out in the HF-template (Fig 1(a)); they are not explicitly presented in the papers.We divide our execution steps across the corpus of papers into three categories depending on the degree to which the results of the step appeared explicitly in the paper of interest, and plot the scores for each prompt response averaged over all the tasks in each category.As shown in Fig. 4(e), the scores exhibit a flat distribution conditional on this categorization.These two observations support the conclusion that LLMs can do non-trivial aspects of HF calculations, regardless of whether the solution explicitly appears in its training data.</p>
<p>Finally, to investigate whether an LLM could further be used to supplement human evaluation, we examined the performance of GPT-4 on scoring its (own) execution responses as Incorrect or Correct in a zero-shot setting as well as a setting where the model is prompted to return a rationale (see SM Section I [35]).We computed the class-balanced accuracy of the 'LLM-Scorer', where the two classes are Incorrect and Correct, against the human-assigned Correctness scores.The 'LLM-Scorer' had a class-balanced accuracy of 69% and 74% in the zero-shot and the few-shot-with-rationale experiments, respectively.In the setting where the model is prompted to return a rationale for its score, the 'LLM-Scorer' was able to identify 72.5% of problems that had a human-assigned Correctness score of 0 or 50 as incorrect.We find the agreement between LLM self-scoring with human scoring to be promising; developing automated scoring procedures would help mitigate the human-evaluation bottleneck and enable scalable evaluation and improvement of models in the future.</p>
<p>In this work, we investigated LLMs' abilities to execute HF mean-field theory, a core tool in quantum many-body physics, by creating an evaluation corpus from research papers in the field.To do so, we designed a collection of general-purpose prompt templates to break the task into smaller but natural calculation steps.We also experimented with using LLM to fill in the templates for individual research papers.In executing the steps of HF mean-field theory, GPT-4 scored 87.5 out of 100 on average, aggregated over all steps and all research papers in our corpus, despite the tasks requiring graduatelevel knowledge of quantum many-body physics.To our knowledge, this is the first evaluation of LLM abilities to execute a core component of research-level physics, used in numerous papers in condensed matter theory.Our work demonstrates that a broadly employed theoretical tool for scientific research can take advantage of existing LLM capabilities.</p>
<p>The analytic derivation of the HF Hamiltonian and associated self-consistency equations is a common first pass at a complex problem of quantum many-body physics when new systems and phenomena are discovered.Hence the HF template we developed for carrying out the derivation using LLMs has the potential to aid accelerated exploration.Moreover, the template and the evaluation scheme we introduced can also serve as a testbed for evaluating future improvements on LLM reasoning capabilities.Myriad improvements can be explored, from finetuning an LLM to achieve specific domain knowledge in HF calculations, to more elaborate examples in each step of the prompt template, to allowing the LLM to call computational tools.Ultimately, these will allow researchers to explore the rich and complex phase space of possibilities in modern material systems efficiently.We anticipate, given the good baseline performance of GPT-4 on each part of this process, that these different tasks could potentially be bootstrapped towards better models in an automated manner.Looking farther ahead, augmenting LLMs with the ability to call computational tools would enable seamless translation between language, symbolic mathematics, physical insight, and numerical solvers, a combination with potentially powerful consequences for scientific research.</p>
<p>HP acknowledges support by the National Science Foundation (Platform for the Accelerated Realization, Analysis, and Discovery of Interface Materials (PARADIM)) under Cooperative Agreement No. DMR-2039380.WT, E-AK were supported by OAC-2118310 (09/15/2022 to 08/31/2027 ), HDR Institute: The Quantum Institute for Data and Emergence at Atomic Scales (Qu-IDEAS).This research is funded in part by the Gordon and Betty Moore Foundation's EPiQS Initiative, The placeholders are highlighted.We turn the template into a prompt for the task 3 by specifying the placeholders for the given paper in the database.(c) The schematic for generating the prompts from the template with placeholders (empty boxes) using human-supplied information (boxes with dots).(d) The schematic for generating the prompts from an abstract.We give an abstract to a LLM and query the LLM to infer system specific information from the abstract and fill relevant placeholders in the template.Since notations are not specified in the abstract, we supply placeholders corresponding to the notations.The combination is a complete prompt.(e) An example of a query asking an LLM to infer system specific information.(f) An example response from GPT-4 to the query of panel (e).We required the response to consist of the quote, an explanation, and the answer.The answers are highlighted.(g) An example of a response to the final prompt by GPT-4 for Ref. [36] corresponding to T11.</p>
<p>FIG. 1 .
1
FIG. 1. (a)The five conceptual steps of the derivation of the HF Hamiltonian and self-consistency equations and bite-sized tasks within each step.The HF template consists of the prompt template Ti for each task i.(b) An example template T3.The placeholders are highlighted.We turn the template into a prompt for the task 3 by specifying the placeholders for the given paper in the database.(c) The schematic for generating the prompts from the template with placeholders (empty boxes) using human-supplied information (boxes with dots).(d) The schematic for generating the prompts from an abstract.We give an abstract to a LLM and query the LLM to infer system specific information from the abstract and fill relevant placeholders in the template.Since notations are not specified in the abstract, we supply placeholders corresponding to the notations.The combination is a complete prompt.(e) An example of a query asking an LLM to infer system specific information.(f) An example response from GPT-4 to the query of panel (e).We required the response to consist of the quote, an explanation, and the answer.The answers are highlighted.(g) An example of a response to the final prompt by GPT-4 for Ref.[36] corresponding to T11.</p>
<p>FIG. 2 .FIG. 3 .
23
FIG. 2. (a)The execution workflow using the full prompt set based on the HF template.Each prompt builds on the outputs of all the previous steps.Specifically, the prompt for the task i, Pi incorporates the corrected output Oi−1 of the previous prompt.(b) The schematic of evaluation and correction for each task i.Each output Oi to the prompt Pi executing the task i is evaluated by the human evaluator and corrected, if necessary.The verified output O * i is incorporated into the next prompt Pi.(c) An example of the prompt P5 for reproducing the calculations in Ref.[36].(d) An example of the execution outcome O5.This output is correct, hence correction was not necessary and Oi = O * i .</p>
<p>FIG. 4 .
4
FIG. 4. (a)The four-layered rubric system for evaluating an LLM's output Oi in response to each prompt Pi.Adherence: how closely the LLM adheres to the instructions.Rigor: how accurate is the mathematical derivation.Knowledge: how consistent is the LLM's reasoning with the laws of physics.Correctness: how correct is the LLM's response.(b) The rubricdependence of the performance.The average score for each rubric layer across all outputs for all papers and their standard deviations.(c) The task-dependence of the performance.We averaged the score for each prompt across the four rubric layers.Then these average scores were averaged over the prompts belonging to each step of the derivation as broken down in Fig.2(a).(d) The paper-dependence of the performance on information extraction and execution.The average score across all the placeholders for a given paper over the excerpt-based information extraction detailed in Fig. 3 is shown in the lighter sage green.The average score across all rubric layers and prompts for deriving the HHF for a given paper is shown in darker olive green.For both extraction and execution, the error bars were calculated by averaging over all papers and placeholders/tasks.The dashed line between arXiv:2108.02159 and arXiv:2110.11330marks the separation between papers before and after the training data cutoff date.(e) The dependence of the execution score, for each rubric layer, on the degree of the overlap between the correct output O * i and the text of the target research paper.</p>
<p>Grant GBMF10436 to E-AK. </p>
<p>Language models are few-shot learners. T Brown, Advances in Neural Information Processing Systems. 202033</p>
<p>Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. N Shazeer, arXiv:1701.065382017arXiv preprint</p>
<p>. R Anil, arXiv:2305.104032023Palm 2 technical report. arXiv preprint</p>
<p>Llama 2: Open foundation and finetuned chat models. H Touvron, arXiv:2307.092882023arXiv preprint</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. S E Aarohi, Transactions on Machine Learning Research. 2023</p>
<p>Solving quantitative reasoning problems with language models. A Lewkowycz, Advances in Neural Information Processing Systems. A H Oh, A Agarwal, D Belgrave, K Cho, 2022</p>
<p>Examining the potential and pitfalls of chatgpt in science and engineering problem-solving. K D Wang, E Burkholder, C Wieman, S Salehi, N Haber, arXiv:2310.087732023arXiv preprint</p>
<p>Evaluating large language models trained on code. M Chen, arXiv:2107.033742021arXiv preprint</p>
<p>R A Poldrack, T Lu, G Beguš, Ai, arXiv:2304.13187Experiments with gpt-4. 2023arXiv preprint</p>
<p>Large language models encode clinical knowledge. K Singhal, S Azizi, T Tu, 10.1038/s41586-023-06291-2Nature. 6202023</p>
<p>H Nori, N King, S M Mckinney, D Carignan, E Horvitz, arXiv:2303.13375Capabilities of gpt-4 on medical challenge problems. 2023arXiv preprint</p>
<p>Emergent autonomous scientific research capabilities of large language models. D A Boiko, R Macknight, G Gomes, arXiv:2304.053322023arXiv preprint</p>
<p>Mathematical discoveries from program search with large language models. B Romera-Paredes, Nature. 132023</p>
<p>Scaling laws for neural language models. J Kaplan, arXiv:2001.083612020arXiv preprint</p>
<p>Training compute-optimal large language models. J Hoffmann, arXiv:2203.155562022arXiv preprint</p>
<p>. J Achiam, arXiv:2303.087742023Gpt-4 technical report. arXiv preprint</p>
<p>Gemini: a family of highly capable multimodal models. G Team, arXiv:2312.118052023arXiv preprint</p>
<p>Hugging Face. LMSys Chatbot Arena Leaderboard. 2024</p>
<p>Measuring massive multitask language understanding. D Hendrycks, arXiv:2009.033002020arXiv preprint</p>
<p>Gpt-4 passes the bar exam. D M Katz, M J Bommarito, S Gao, P Arredondo, Available at SSRN. 43892332023</p>
<p>We focus solely on LLMs available via model APIs and not on LLMs and foundational models trained or tuned on domain specific data. </p>
<p>The impact of large language models on scientific discovery: a preliminary study using gpt-4. M R Ai4science, M A Quantum, arXiv:2311.073612023arXiv preprint</p>
<p>Paperqa: Retrieval-augmented generative agent for scientific research. J Lála, arXiv:2312.075592023arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. D Hendrycks, arXiv:2103.038742021arXiv preprint</p>
<p>Training verifiers to solve math word problems. K Cobbe, arXiv:2110.141682021arXiv preprint</p>
<p>Do large language models understand chemistry? a conversation with chatgpt. C M Castro Nascimento, A S Pimentel, Journal of Chemical Information and Modeling. 632023</p>
<p>Assessment of chemistry knowledge in large language models that generate code. A D White, Digital Discovery. 22023</p>
<p>A M Bran, S Cox, A D White, P Schwaller, Chemcrow, arXiv:2304.05376Augmenting large-language models with chemistry tools. 2023arXiv preprint</p>
<p>Chameleon: Plug-and-play compositional reasoning with large language models. P Lu, arXiv:2304.098422023arXiv preprint</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. Y Shen, arXiv:2303.175802023arXiv preprint</p>
<p>Solving olympiad geometry without human demonstrations. T H Trinh, Y Wu, Q V Le, H He, T Luong, Nature. 6252024</p>
<p>R Taylor, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>A Altland, B D Simons, Condensed matter field theory. Cambridge university press2010</p>
<p>Over 6456 papers mention Hartree-Fock in the abstract of papers in the cond-mat arXiv preprint server over the last decade. </p>
<p>Topological phases in ab-stacked mote2/wse2: 𭟋2 topological insulators, chern insulators, and topological charge density waves. H Pan, M Xie, F Wu, S Das Sarma, 2111.01152Physical Review Letters. 129568042022</p>
<p>Evaluations were carried out using checkpoints 'gpt-4' and 'gpt-4-0613' referenced in. At the time that the experiments in the paper were performed. gpt-4' pointed to 'gpt-4-0613'. The abstract to execution experiment was performed using GPT-4 queried via the web interface</p>
<p>Competing magnetic states in transition metal dichalcogenide moir\'e materials. N C Hu, A H Macdonald, 2108.02159Physical Review B. 1042144032021</p>            </div>
        </div>

    </div>
</body>
</html>
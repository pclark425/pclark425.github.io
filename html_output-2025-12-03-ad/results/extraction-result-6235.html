<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6235 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6235</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6235</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-122.html">extraction-schema-122</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <p><strong>Paper ID:</strong> paper-38d64919ba526868a850a0e5f6239d4c474b7e7e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/38d64919ba526868a850a0e5f6239d4c474b7e7e" target="_blank">Large Language Models are not Fair Evaluators</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes a calibration framework with three simple yet effective strategies that successfully mitigates evaluation bias, resulting in closer alignment with human judgments.</p>
                <p><strong>Paper Abstract:</strong> In this paper, we uncover a systematic bias in the evaluation paradigm of adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models. We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries with ChatGPT as an evaluator. To address this issue, we propose a calibration framework with three simple yet effective strategies: 1) Multiple Evidence Calibration, which requires the evaluator model to generate multiple evaluation evidence before assigning ratings; 2) Balanced Position Calibration, which aggregates results across various orders to determine the final score; 3) Human-in-the-Loop Calibration, which introduces a balanced position diversity entropy to measure the difficulty of each example and seeks human assistance when needed. We also manually annotate the"win/tie/lose"outcomes of responses from ChatGPT and Vicuna-13B in the Vicuna Benchmark's question prompt, and extensive experiments demonstrate that our approach successfully mitigates evaluation bias, resulting in closer alignment with human judgments. We release our code and human annotation at \url{https://github.com/i-Eval/FairEval} to facilitate future research.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6235.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6235.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Model used as an evaluator / judge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paradigm of using LLMs (e.g., GPT-4, ChatGPT) to compare and score candidate model responses instead of (or prior to) human annotation; evaluated for reliability in the Vicuna benchmark in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Open-domain assistant response evaluation (Vicuna Benchmark: 80 instruction-following prompts across 9 categories including common-sense, coding, math, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4 and ChatGPT (used interchangeably as automatic evaluators in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human annotations collected by three authors (AI-familiar researchers) independently on all 80 examples; annotators judged 'win/tie/lose' and scored responses on helpfulness, relevance, accuracy, level of detail; responses presented in random order; average 3 minutes per example; majority vote used as final human label.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Accuracy (agreement with human majority), Cohen/Kappa-like interrater (kappa) correlation with human majority, Conflict Rate (fraction of pairwise comparisons that flip when response positions are swapped), Win Rate, and cost estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>LLM evaluators show substantially lower alignment with human majority than humans: VANILLA GPT-4 accuracy 52.7% vs Human average 71.7%; VANILLA ChatGPT 44.4%. LLMs produce high conflict rates when swapping positions (GPT-4 conflict rates up to 46.3%; ChatGPT up to 82.5%). After calibration (MEC+BPC+HITLC) LLM alignment can approach or exceed human-average (e.g., GPT-4 with HITLC beta=20% reached 73.8% accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Systematic positional bias (preference for a specific slot), sensitivity to prompt/template and ordering, high self-conflict (inconsistent comparisons across swapped orders), decreased reliability for cases with small quality gaps, possibility of giving conclusions not grounded in generated explanations due to autoregressive decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Swapping response order can reverse evaluation outcomes (e.g., Vicuna-13B vs ChatGPT: with ChatGPT as evaluator, Vicuna could be judged the winner in 66/80 queries under a biased configuration); Table 2 shows extreme differences in win rates depending on assistant position (GPT-4 favored Assistant1, ChatGPT favored Assistant2). High conflict rates on close-quality pairs (score gap <=1) produce many contradictory judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Paper proposes a calibration framework: Multiple Evidence Calibration (MEC) — require evaluator to produce explanation/evidence first and sample multiple (k) evidence-supported scores; Balanced Position Calibration (BPC) — evaluate both response orders and average scores; Human-in-the-Loop Calibration (HITLC) — compute Balanced Position Diversity Entropy (BPDE) from MEC+BPC outputs to select high-uncertainty examples for human annotation. Empirically, MEC (k=3) + BPC improved accuracy and reduced conflict; HITLC with beta=20% achieved human-level alignment while reducing human annotation cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are not Fair Evaluators', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6235.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6235.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (used as automatic evaluator / judge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 is used in experiments as an automatic evaluator to compare two assistant responses and produce scores/explanations; its behavior and biases (positional preference, conflict rate) are measured and calibrated in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Vicuna-style assistant response pairwise evaluation across multiple categories (dialogue/instruction following, coding, math, common-sense, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>gpt-4 (OpenAI API, 'gpt-4')</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Compared against human majority on 80 Vicuna examples annotated by three knowledgeable annotators (see LLM-as-evaluator entry).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Accuracy vs human majority, kappa correlation coefficient, Conflict Rate, Win Rate in head-to-head comparisons, evaluation cost.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>GPT-4 (VANILLA) accuracy 52.7% and kappa 0.24 versus human average accuracy 71.7% and kappa 0.54. Conflict rates reported for specific model comparisons (e.g., Vicuna-13B v.s. ChatGPT: GPT-4 conflict rates 46.3% in one setting and 5.0% in another). GPT-4 tends to favor the first-presented response.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Pronounced positional bias favoring the first slot; high conflict on close-quality pairs; conclusions sometimes not properly grounded in subsequent explanations; sensitivity to prompt/template and sampling temperature.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>In pairwise tests swapping responses frequently produced contradictory outcomes; when gap between responses small (score gap <=1) GPT-4 produced many conflicts; Table 2 shows large win-rate fluctuation depending on position (e.g., Vicuna-13B win rate 51.3% when Assistant1 vs 23.8% when Assistant2 under GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Applying MEC (generate evidence first, sample k times), BPC (evaluate both orders and average), and HITLC (use BPDE to route ambiguous examples to humans) reduced conflict and improved alignment (e.g., MEC(k=3)+BPC(k=3) raises GPT-4 accuracy to 62.5%; adding HITLC beta=20% reached 73.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are not Fair Evaluators', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6235.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6235.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo used as evaluator / judge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT (as provided via 'gpt-3.5-turbo-0301') is evaluated as an automatic judge and shown to have stronger positional bias and lower alignment with human judgments than GPT-4; calibration methods are applied and assessed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Vicuna-style assistant response evaluation across 9 categories (dialogue/instruction-following tasks including coding and math).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>gpt-3.5-turbo-0301 (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Compared to same three-author human majority annotations on 80 examples as above.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Accuracy vs human majority, kappa, Conflict Rate, Win Rate, cost.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>ChatGPT (VANILLA) accuracy only 44.4% (kappa 0.06) vs human average 71.7%; it showed extreme positional bias favoring the second response with conflict rates up to 82.5% in some comparisons. After MEC+BPC accuracy rose to 58.7% and with HITLC beta=20% to 71.3%.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Very strong positional bias (preference for second slot), high conflict rates and instability, especially poor on close-quality examples; low initial agreement with human majority.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Table 2: Vicuna-13B v.s. ChatGPT under ChatGPT evaluator yields Vicuna win rates 2.5% when Vicuna is Assistant1 vs 82.5% when Assistant2, illustrating extreme slot-dependent flip. VANILLA template results differ by ~6% depending on scoring vs comparing templates indicating unreliability.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Same calibration framework: MEC (k=3), BPC (swap and average), and HITLC using BPDE; MEC+BPC improves accuracy and reduces conflict; HITLC with 20% human annotation recovered near-human accuracy while lowering annotation cost (from $30 to $18.3 in their cost model).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are not Fair Evaluators', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6235.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6235.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Annotators</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human majority annotations (three expert annotators)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three human annotators (authors knowledgeable in AI) independently labeled 80 Vicuna benchmark examples; final labels derived by majority vote and used as the gold standard for alignment comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Evaluation of assistant responses (helpfulness, relevance, accuracy, level of detail) across the Vicuna Benchmark's 80 prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>N/A (human evaluators)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Three annotators, AI-familiar researchers, annotated all 80 examples independently; responses presented in random order; ~3 minutes per example; majority vote used as final label; individual human accuracies: 68.8%, 76.3%, 70.0%; human average 71.7% accuracy and kappa 0.54.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Used as ground truth to compute LLM accuracy and kappa; also used for cost comparisons (estimated $30 per annotator under US minimum wage assumption).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Humans have higher agreement and consistency than vanilla LLM evaluators; the average human agreement (71.7%) substantially exceeds raw LLM evaluator VANILLA performance (GPT-4 52.7%, ChatGPT 44.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>N/A (humans are the reference), but humans incur higher monetary and time cost (authors estimate $30 annotator cost for full dataset) and slower throughput.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Not directly reported—human annotations used as reference; inter-annotator variance exists (individual accuracies varied by ~7.5 percentage points).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Paper advocates hybrid human+LLM pipelines (HITLC) where humans annotate only high-BPDE examples to reduce cost while restoring alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are not Fair Evaluators', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6235.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6235.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MEC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multiple Evidence Calibration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed calibration method that forces the LLM evaluator to generate evidence/explanations first (Evaluation Calibration template) and samples multiple such evidence-supported scores (k samples) to ensemble into a more stable judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Automatic calibration of LLM evaluation for pairwise assistant response comparison (Vicuna Benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Applied to GPT-4 and ChatGPT evaluators in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>MEC outputs compared to human majority labels to compute accuracy and kappa; sampling temperature tuned (optimal around 0.6-1.0 in experiments); k chosen empirically (k=3 found optimal tradeoff).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Accuracy and kappa vs human majority; Conflict Rate reduction when combined with BPC.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>MEC alone improved alignment relative to VANILLA: GPT-4 accuracy from 52.7% to 58.7% (k=3); ChatGPT from 44.4% to 53.2% (k=3). Sampling multiple evidence improved stability but returns diminishing returns with large k.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Effect depends on sampling temperature and k; too-low temperature removes beneficial variance; too-high temperature degrades quality. MEC alone does not fully remove positional bias (requires combination with BPC).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>MEC reduces but does not eliminate conflicts when responses are extremely close in quality; BPC is needed to address position-preference flips.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Use MEC with k=3 and moderate temperature (e.g., 0.6–1.0) and combine with BPC; ensemble multiple evidence outputs to stabilize judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are not Fair Evaluators', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6235.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6235.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BPC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Balanced Position Calibration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Calibration strategy that evaluates the same response pairs in both positional orders and averages the scores (or aggregates results) to cancel out positional bias.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Calibration of LLM evaluators for pairwise response comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Applied to GPT-4 and ChatGPT in combination with MEC in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>BPC outputs compared to human majority to measure improvement; combined with MEC (MEC(k=3)+BPC(k=3) used in main experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Accuracy, kappa correlation, Conflict Rate (BPC makes conflict rate not applicable since positions are balanced), cost.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Combining MEC and BPC improved accuracy relative to MEC alone (e.g., ChatGPT MEC(k=3) 53.2% -> MEC+BPC 58.7%). MEC+BPC outperformed larger pure-MEC sampling (MEC(k=6)), indicating BPC addresses positional bias more effectively than naive increased sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Doubles evaluation cost (must run both orders) and does not eliminate cases where evidence ensemble remains ambiguous; requires more API calls/time.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>When both positions produce inconsistent evidence across runs, simple averaging can still hide nuanced disagreements; ambiguous examples remain and are routed to human-in-the-loop by BPDE.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Use BPC in combination with MEC to average out slot preference; follow with HITLC for high-entropy (BPDE) examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are not Fair Evaluators', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6235.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6235.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HITLC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human-in-the-Loop Calibration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A selective human-annotation strategy that uses Balanced Position Diversity Entropy (BPDE) computed from MEC+BPC outputs to identify high-uncertainty examples for human labeling, thereby achieving near-human alignment with reduced annotation cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Hybrid evaluation pipeline combining LLM evaluators and human annotations for assistant-response evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Applied with GPT-4 and ChatGPT evaluators in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>BPDE ranks examples by entropy across 2k evaluation results; top-beta percent (e.g., beta=20%) of examples selected for human annotation; final label uses majority (human+LLM integration described in Section 4.1).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Accuracy and kappa vs full human majority; annotation cost reduction measured.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>HITLC (beta=20%) with GPT-4 achieved 73.8% accuracy and kappa 0.56 (surpassing human-average accuracy 71.7%); ChatGPT with HITLC beta=20% reached 71.3%. Using HITLC reduced annotation cost (example: ChatGPT from $30 to $18.3, a 39% reduction) while restoring human-level alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Requires a selection threshold (beta) and relies on a reliable BPDE signal; effectiveness depends on quality of MEC+BPC outputs; still incurs some human cost.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>If BPDE fails to rank ambiguous cases well, humans may not be asked for the most problematic examples and residual bias remains; paper shows BPDE outperforms random and vanilla diversity entropy but not perfect.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Tune BPDE threshold (beta) to balance cost vs alignment; combine MEC and BPC outputs to compute BPDE; annotate top-beta examples by humans and integrate by majority vote.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are not Fair Evaluators', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging LLM-as-a-judge with MT-Bench and Chatbot Arena <em>(Rating: 2)</em></li>
                <li>On the Blind Spots of Model-Based Evaluation Metrics for Text Generation <em>(Rating: 2)</em></li>
                <li>Error analysis prompting enables human-like translation evaluation in large language models: A case study on chatgpt <em>(Rating: 2)</em></li>
                <li>Alpacafarm: A simulation framework for methods that learn from human feedback <em>(Rating: 1)</em></li>
                <li>PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6235",
    "paper_id": "paper-38d64919ba526868a850a0e5f6239d4c474b7e7e",
    "extraction_schema_id": "extraction-schema-122",
    "extracted_data": [
        {
            "name_short": "LLM-as-evaluator",
            "name_full": "Large Language Model used as an evaluator / judge",
            "brief_description": "The paradigm of using LLMs (e.g., GPT-4, ChatGPT) to compare and score candidate model responses instead of (or prior to) human annotation; evaluated for reliability in the Vicuna benchmark in this paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Open-domain assistant response evaluation (Vicuna Benchmark: 80 instruction-following prompts across 9 categories including common-sense, coding, math, etc.)",
            "llm_judge_model": "GPT-4 and ChatGPT (used interchangeably as automatic evaluators in experiments)",
            "human_evaluation_setup": "Human annotations collected by three authors (AI-familiar researchers) independently on all 80 examples; annotators judged 'win/tie/lose' and scored responses on helpfulness, relevance, accuracy, level of detail; responses presented in random order; average 3 minutes per example; majority vote used as final human label.",
            "metrics_compared": "Accuracy (agreement with human majority), Cohen/Kappa-like interrater (kappa) correlation with human majority, Conflict Rate (fraction of pairwise comparisons that flip when response positions are swapped), Win Rate, and cost estimates.",
            "reported_differences": "LLM evaluators show substantially lower alignment with human majority than humans: VANILLA GPT-4 accuracy 52.7% vs Human average 71.7%; VANILLA ChatGPT 44.4%. LLMs produce high conflict rates when swapping positions (GPT-4 conflict rates up to 46.3%; ChatGPT up to 82.5%). After calibration (MEC+BPC+HITLC) LLM alignment can approach or exceed human-average (e.g., GPT-4 with HITLC beta=20% reached 73.8% accuracy).",
            "llm_specific_limitations": "Systematic positional bias (preference for a specific slot), sensitivity to prompt/template and ordering, high self-conflict (inconsistent comparisons across swapped orders), decreased reliability for cases with small quality gaps, possibility of giving conclusions not grounded in generated explanations due to autoregressive decoding.",
            "notable_failure_cases": "Swapping response order can reverse evaluation outcomes (e.g., Vicuna-13B vs ChatGPT: with ChatGPT as evaluator, Vicuna could be judged the winner in 66/80 queries under a biased configuration); Table 2 shows extreme differences in win rates depending on assistant position (GPT-4 favored Assistant1, ChatGPT favored Assistant2). High conflict rates on close-quality pairs (score gap &lt;=1) produce many contradictory judgments.",
            "mitigation_strategies": "Paper proposes a calibration framework: Multiple Evidence Calibration (MEC) — require evaluator to produce explanation/evidence first and sample multiple (k) evidence-supported scores; Balanced Position Calibration (BPC) — evaluate both response orders and average scores; Human-in-the-Loop Calibration (HITLC) — compute Balanced Position Diversity Entropy (BPDE) from MEC+BPC outputs to select high-uncertainty examples for human annotation. Empirically, MEC (k=3) + BPC improved accuracy and reduced conflict; HITLC with beta=20% achieved human-level alignment while reducing human annotation cost.",
            "uuid": "e6235.0",
            "source_info": {
                "paper_title": "Large Language Models are not Fair Evaluators",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-4 (evaluator)",
            "name_full": "GPT-4 (used as automatic evaluator / judge)",
            "brief_description": "GPT-4 is used in experiments as an automatic evaluator to compare two assistant responses and produce scores/explanations; its behavior and biases (positional preference, conflict rate) are measured and calibrated in the paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "task_domain": "Vicuna-style assistant response pairwise evaluation across multiple categories (dialogue/instruction following, coding, math, common-sense, etc.)",
            "llm_judge_model": "gpt-4 (OpenAI API, 'gpt-4')",
            "human_evaluation_setup": "Compared against human majority on 80 Vicuna examples annotated by three knowledgeable annotators (see LLM-as-evaluator entry).",
            "metrics_compared": "Accuracy vs human majority, kappa correlation coefficient, Conflict Rate, Win Rate in head-to-head comparisons, evaluation cost.",
            "reported_differences": "GPT-4 (VANILLA) accuracy 52.7% and kappa 0.24 versus human average accuracy 71.7% and kappa 0.54. Conflict rates reported for specific model comparisons (e.g., Vicuna-13B v.s. ChatGPT: GPT-4 conflict rates 46.3% in one setting and 5.0% in another). GPT-4 tends to favor the first-presented response.",
            "llm_specific_limitations": "Pronounced positional bias favoring the first slot; high conflict on close-quality pairs; conclusions sometimes not properly grounded in subsequent explanations; sensitivity to prompt/template and sampling temperature.",
            "notable_failure_cases": "In pairwise tests swapping responses frequently produced contradictory outcomes; when gap between responses small (score gap &lt;=1) GPT-4 produced many conflicts; Table 2 shows large win-rate fluctuation depending on position (e.g., Vicuna-13B win rate 51.3% when Assistant1 vs 23.8% when Assistant2 under GPT-4).",
            "mitigation_strategies": "Applying MEC (generate evidence first, sample k times), BPC (evaluate both orders and average), and HITLC (use BPDE to route ambiguous examples to humans) reduced conflict and improved alignment (e.g., MEC(k=3)+BPC(k=3) raises GPT-4 accuracy to 62.5%; adding HITLC beta=20% reached 73.8%).",
            "uuid": "e6235.1",
            "source_info": {
                "paper_title": "Large Language Models are not Fair Evaluators",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ChatGPT (evaluator)",
            "name_full": "ChatGPT (gpt-3.5-turbo used as evaluator / judge)",
            "brief_description": "ChatGPT (as provided via 'gpt-3.5-turbo-0301') is evaluated as an automatic judge and shown to have stronger positional bias and lower alignment with human judgments than GPT-4; calibration methods are applied and assessed.",
            "citation_title": "",
            "mention_or_use": "use",
            "task_domain": "Vicuna-style assistant response evaluation across 9 categories (dialogue/instruction-following tasks including coding and math).",
            "llm_judge_model": "gpt-3.5-turbo-0301 (ChatGPT)",
            "human_evaluation_setup": "Compared to same three-author human majority annotations on 80 examples as above.",
            "metrics_compared": "Accuracy vs human majority, kappa, Conflict Rate, Win Rate, cost.",
            "reported_differences": "ChatGPT (VANILLA) accuracy only 44.4% (kappa 0.06) vs human average 71.7%; it showed extreme positional bias favoring the second response with conflict rates up to 82.5% in some comparisons. After MEC+BPC accuracy rose to 58.7% and with HITLC beta=20% to 71.3%.",
            "llm_specific_limitations": "Very strong positional bias (preference for second slot), high conflict rates and instability, especially poor on close-quality examples; low initial agreement with human majority.",
            "notable_failure_cases": "Table 2: Vicuna-13B v.s. ChatGPT under ChatGPT evaluator yields Vicuna win rates 2.5% when Vicuna is Assistant1 vs 82.5% when Assistant2, illustrating extreme slot-dependent flip. VANILLA template results differ by ~6% depending on scoring vs comparing templates indicating unreliability.",
            "mitigation_strategies": "Same calibration framework: MEC (k=3), BPC (swap and average), and HITLC using BPDE; MEC+BPC improves accuracy and reduces conflict; HITLC with 20% human annotation recovered near-human accuracy while lowering annotation cost (from $30 to $18.3 in their cost model).",
            "uuid": "e6235.2",
            "source_info": {
                "paper_title": "Large Language Models are not Fair Evaluators",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Human Annotators",
            "name_full": "Human majority annotations (three expert annotators)",
            "brief_description": "Three human annotators (authors knowledgeable in AI) independently labeled 80 Vicuna benchmark examples; final labels derived by majority vote and used as the gold standard for alignment comparisons.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Evaluation of assistant responses (helpfulness, relevance, accuracy, level of detail) across the Vicuna Benchmark's 80 prompts.",
            "llm_judge_model": "N/A (human evaluators)",
            "human_evaluation_setup": "Three annotators, AI-familiar researchers, annotated all 80 examples independently; responses presented in random order; ~3 minutes per example; majority vote used as final label; individual human accuracies: 68.8%, 76.3%, 70.0%; human average 71.7% accuracy and kappa 0.54.",
            "metrics_compared": "Used as ground truth to compute LLM accuracy and kappa; also used for cost comparisons (estimated $30 per annotator under US minimum wage assumption).",
            "reported_differences": "Humans have higher agreement and consistency than vanilla LLM evaluators; the average human agreement (71.7%) substantially exceeds raw LLM evaluator VANILLA performance (GPT-4 52.7%, ChatGPT 44.4%).",
            "llm_specific_limitations": "N/A (humans are the reference), but humans incur higher monetary and time cost (authors estimate $30 annotator cost for full dataset) and slower throughput.",
            "notable_failure_cases": "Not directly reported—human annotations used as reference; inter-annotator variance exists (individual accuracies varied by ~7.5 percentage points).",
            "mitigation_strategies": "Paper advocates hybrid human+LLM pipelines (HITLC) where humans annotate only high-BPDE examples to reduce cost while restoring alignment.",
            "uuid": "e6235.3",
            "source_info": {
                "paper_title": "Large Language Models are not Fair Evaluators",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "MEC",
            "name_full": "Multiple Evidence Calibration",
            "brief_description": "A proposed calibration method that forces the LLM evaluator to generate evidence/explanations first (Evaluation Calibration template) and samples multiple such evidence-supported scores (k samples) to ensemble into a more stable judgment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Automatic calibration of LLM evaluation for pairwise assistant response comparison (Vicuna Benchmark).",
            "llm_judge_model": "Applied to GPT-4 and ChatGPT evaluators in experiments.",
            "human_evaluation_setup": "MEC outputs compared to human majority labels to compute accuracy and kappa; sampling temperature tuned (optimal around 0.6-1.0 in experiments); k chosen empirically (k=3 found optimal tradeoff).",
            "metrics_compared": "Accuracy and kappa vs human majority; Conflict Rate reduction when combined with BPC.",
            "reported_differences": "MEC alone improved alignment relative to VANILLA: GPT-4 accuracy from 52.7% to 58.7% (k=3); ChatGPT from 44.4% to 53.2% (k=3). Sampling multiple evidence improved stability but returns diminishing returns with large k.",
            "llm_specific_limitations": "Effect depends on sampling temperature and k; too-low temperature removes beneficial variance; too-high temperature degrades quality. MEC alone does not fully remove positional bias (requires combination with BPC).",
            "notable_failure_cases": "MEC reduces but does not eliminate conflicts when responses are extremely close in quality; BPC is needed to address position-preference flips.",
            "mitigation_strategies": "Use MEC with k=3 and moderate temperature (e.g., 0.6–1.0) and combine with BPC; ensemble multiple evidence outputs to stabilize judgments.",
            "uuid": "e6235.4",
            "source_info": {
                "paper_title": "Large Language Models are not Fair Evaluators",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "BPC",
            "name_full": "Balanced Position Calibration",
            "brief_description": "Calibration strategy that evaluates the same response pairs in both positional orders and averages the scores (or aggregates results) to cancel out positional bias.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Calibration of LLM evaluators for pairwise response comparison.",
            "llm_judge_model": "Applied to GPT-4 and ChatGPT in combination with MEC in experiments.",
            "human_evaluation_setup": "BPC outputs compared to human majority to measure improvement; combined with MEC (MEC(k=3)+BPC(k=3) used in main experiments).",
            "metrics_compared": "Accuracy, kappa correlation, Conflict Rate (BPC makes conflict rate not applicable since positions are balanced), cost.",
            "reported_differences": "Combining MEC and BPC improved accuracy relative to MEC alone (e.g., ChatGPT MEC(k=3) 53.2% -&gt; MEC+BPC 58.7%). MEC+BPC outperformed larger pure-MEC sampling (MEC(k=6)), indicating BPC addresses positional bias more effectively than naive increased sampling.",
            "llm_specific_limitations": "Doubles evaluation cost (must run both orders) and does not eliminate cases where evidence ensemble remains ambiguous; requires more API calls/time.",
            "notable_failure_cases": "When both positions produce inconsistent evidence across runs, simple averaging can still hide nuanced disagreements; ambiguous examples remain and are routed to human-in-the-loop by BPDE.",
            "mitigation_strategies": "Use BPC in combination with MEC to average out slot preference; follow with HITLC for high-entropy (BPDE) examples.",
            "uuid": "e6235.5",
            "source_info": {
                "paper_title": "Large Language Models are not Fair Evaluators",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "HITLC",
            "name_full": "Human-in-the-Loop Calibration",
            "brief_description": "A selective human-annotation strategy that uses Balanced Position Diversity Entropy (BPDE) computed from MEC+BPC outputs to identify high-uncertainty examples for human labeling, thereby achieving near-human alignment with reduced annotation cost.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Hybrid evaluation pipeline combining LLM evaluators and human annotations for assistant-response evaluation.",
            "llm_judge_model": "Applied with GPT-4 and ChatGPT evaluators in experiments.",
            "human_evaluation_setup": "BPDE ranks examples by entropy across 2k evaluation results; top-beta percent (e.g., beta=20%) of examples selected for human annotation; final label uses majority (human+LLM integration described in Section 4.1).",
            "metrics_compared": "Accuracy and kappa vs full human majority; annotation cost reduction measured.",
            "reported_differences": "HITLC (beta=20%) with GPT-4 achieved 73.8% accuracy and kappa 0.56 (surpassing human-average accuracy 71.7%); ChatGPT with HITLC beta=20% reached 71.3%. Using HITLC reduced annotation cost (example: ChatGPT from $30 to $18.3, a 39% reduction) while restoring human-level alignment.",
            "llm_specific_limitations": "Requires a selection threshold (beta) and relies on a reliable BPDE signal; effectiveness depends on quality of MEC+BPC outputs; still incurs some human cost.",
            "notable_failure_cases": "If BPDE fails to rank ambiguous cases well, humans may not be asked for the most problematic examples and residual bias remains; paper shows BPDE outperforms random and vanilla diversity entropy but not perfect.",
            "mitigation_strategies": "Tune BPDE threshold (beta) to balance cost vs alignment; combine MEC and BPC outputs to compute BPDE; annotate top-beta examples by humans and integrate by majority vote.",
            "uuid": "e6235.6",
            "source_info": {
                "paper_title": "Large Language Models are not Fair Evaluators",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena",
            "rating": 2
        },
        {
            "paper_title": "On the Blind Spots of Model-Based Evaluation Metrics for Text Generation",
            "rating": 2
        },
        {
            "paper_title": "Error analysis prompting enables human-like translation evaluation in large language models: A case study on chatgpt",
            "rating": 2
        },
        {
            "paper_title": "Alpacafarm: A simulation framework for methods that learn from human feedback",
            "rating": 1
        },
        {
            "paper_title": "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization",
            "rating": 1
        }
    ],
    "cost": 0.014257,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Models are not Fair Evaluators</h1>
<p>Peiyi Wang ${ }^{1}$ Lei Li ${ }^{1}$ Liang Chen ${ }^{1}$ Zefan Cai ${ }^{1}$ Dawei Zhu ${ }^{1}$<br>Binghuai Lin ${ }^{3}$ Yunbo Cao ${ }^{3}$ Qi Liu ${ }^{2}$ Tianyu Liu ${ }^{3}$ Zhifang Sui ${ }^{1}$<br>${ }^{1}$ National Key Laboratory for Multimedia Information Processing, Peking University<br>${ }^{2}$ The University of Hong Kong ${ }^{3}$ Tencent Cloud AI<br>{wangpeiyi9979, nlp.lilei, zefncai}@gmail.com<br>leo.liang.chen@outlook.com; {dwzhu, szf}@pku.edu.cn<br>liuqi@cs.hku.hk; {binghuailin, yunbocao, rogertyliu}@tencent.com</p>
<h4>Abstract</h4>
<p>In this paper, we uncover a systematic bias in the evaluation paradigm of adopting large language models (LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models. We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries with ChatGPT as an evaluator. To address this issue, we propose a calibration framework with three simple yet effective strategies: 1) Multiple Evidence Calibration, which requires the evaluator model to generate multiple evaluation evidence before assigning ratings; 2) Balanced Position Calibration, which aggregates results across various orders to determine the final score; 3) Human-in-the-Loop Calibration, which introduces a balanced position diversity entropy to measure the difficulty of each example and seeks human assistance when needed. We also manually annotate the "win/tie/lose" outcomes of responses from ChatGPT and Vicuna-13B in the Vicuna Benchmark's question prompt, and extensive experiments demonstrate that our approach successfully mitigates evaluation bias, resulting in closer alignment with human judgments. We release our code and human annotation at https://github.com/i-Eval/ FairEval to facilitate future research.</p>
<h2>1 Introduction</h2>
<p>The rapid advancement of Large Language Models (LLMs) (Brown et al., 2020; Chowdhery et al., 2022) has underscored the importance of evaluating their alignment with human intent in generated responses, making it an active field of research. Traditional n-gram metrics like BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), as well as more sophisticated model-based evaluations such as BERTScore (Zhang et al., 2020) and
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Simply changing the order of candidate responses leads to overturned comparison results, even though we add the command "ensuring that the order in which the responses were presented does not affect your judgment" into the prompt.</p>
<p>BARTScore (Yuan, Neubig, and Liu, 2021), are insufficient for thoroughly assessing this alignment (He et al., 2023). While human evaluation provides the most accurate measure of model performance and valuable insights, it can often be costly and time-consuming. As a result, there is a growing demand for automated assessment methods that can consistently align with human judgments while being more efficient and cost-effective.</p>
<p>ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023) have recently demonstrated remarkable performance across various tasks, leading to their widespread use as both the annotators (Peng et al., 2023; Xu et al., 2023) and evaluators (Zheng et al., 2023; Peng et al., 2023; Sun et al., 2023; Zhou et al., 2023; Gao et al., 2023; Wang et al., 2023b; Dubois et al., 2023; Wang et al., 2023a). For example, The evaluation pipeline of Vicuna (Zheng et al., 2023) has gained significant interest and wide usage due to its simplicity and interpretability. It prompts GPT-4 to score and compare candidate responses and provide explanations, making it a valuable tool for evaluation. However, it is unclear how reliable LLMs are as evaluators, as they are known to be sensitive to textual instructions and inputs (Dong et al., 2022; Turpin et al., 2023;</p>
<p>Bowman, 2023). This raises questions about the resilience of this paradigm against perturbations, such as the ordering of candidates during scoring, potentially becoming the Achilles' Heel that can be easily hacked for unreliable evaluations.</p>
<p>In this paper, we take a sober look at the LLMs-as-evaluator paradigm and uncover a significant positional bias. Specifically, we demonstrate that GPT-4 exhibits a preference for the first displayed candidate response by consistently assigning it higher scores, even when the order of candidates is subtly altered. As illustrated in Figure 1, merely swapping the presentation order can reverse evaluation outcomes. This bias is also present in ChatGPT, which typically favors the second response. These findings highlight previously overlooked limitations in the current evaluation paradigm.</p>
<p>To address this issue, we propose three simple yet effective strategies to calibrate positional bias: 1) Multiple Evidence Calibration (MEC): We prompt the model to generate evaluation evidence before assigning scores, leveraging the inherent properties of causal language models for calibration. We also employ ensemble techniques to incorporate multiple evidence calibration results to further stabilize the evaluation. 2) Balanced Position Calibration (BPC): To further reduce positional bias, we evaluate each candidate in both positions across two runs and compute the final score as the average of the two runs. 3) Human In The Loop Calibration (HITLC): We also explore human-in-the-loop evaluation and consider a diversity-based method to get a cue to indicate biased candidates based on the evaluation results of MEC and BPC.</p>
<p>To assess the efficacy of our methods, we manually annotate the "win/tie/lose" outcomes of responses from ChatGPT and Vicuna-13B in the Vicuna benchmark (Zheng et al., 2023), encompassing 80 questions spanning 9 distinct question categories. Our MEC and BPC enhance the evaluation alignment of GPT-4 and ChatGPT by 9.8% and 14.3% accuracy, respectively. Moreover, based on MEC and BPC, our HITLC can further effectively integrate human assistance into the evaluation process. Specifically, with only a 20% human annotation cost, GPT-4 and ChatGPT can achieve comparable or even better annotation alignment with the average human performance, reducing the annotation cost by up to 39%.</p>
<p>In summary, our key contributions are: 1) We reveal that LLMs exhibit severe positional bias, com-</p>
<p>[Question]
[Q]
[The Start of Assistant 1's response]
[R1]
[The End of Assistant 1's response]
[The Start of Assistant 2's response]
[R2]
[The End of Assistant 2's response]
[System]
We would like to request your feedback on the performance of two AI assistants in response to the user question displayed above.
Please rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.
Please first output a single line containing only two values indicating the scores for Assistant 1 and 2, respectively.
The two scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.</p>
<p>Table 1: The evaluation template with three slots (⟨Q⟩, [R1] and [R2]) from Zheng et al. (2023). Even though the template emphasizes not letting the order affect the results (red text), large language models still have a large positional bias.</p>
<p>promising their fairness as evaluators; 2) We develop a calibration framework with three simple yet effective strategies to calibrate the positional bias of LLMs; 3) We manually annotate the "win/tie/lose" outcomes of responses from ChatGPT and Vicuna-13B in the Vicuna benchmark and demonstrate the effectiveness of our proposed approach through experimental results, which show closer alignment with human judgments.</p>
<h2>2 Positional Bias of the LLM Evaluator</h2>
<h3>2.1 LLMs as Evaluators</h3>
<p>Recently, researchers have been utilizing LLMs such as GPT-4 as evaluators to compare the performance of two AI assistants. As shown in Table 1, an evaluation template with three placeholders $T(Q, R 1, R 2)$, is used to query the LLM for evaluation. For each testing question $q$, given two responses $r 1$ and $r 2$ from Assistant 1 and Assistant 2, respectively, the researchers populate these responses into the corresponding slots of the evaluation template to form a prompt: $T(Q=q, R 1=$ $r 1, R 2=r 2)$. The prompt is then used to query the LLM in order to obtain the comparison result. In this paper, we found that LLM suffers from severe</p>
<table>
<thead>
<tr>
<th>Evaluators</th>
<th>Vicuna-13B v.s. Other Models</th>
<th>Vicuna-13B Win Rate</th>
<th></th>
<th>Conflict Rate</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>as Assistant1</td>
<td>as Assistant2</td>
<td></td>
</tr>
<tr>
<td>GPT-4</td>
<td>Vicuna-13B v.s. ChatGPT</td>
<td>51.3%</td>
<td>23.8%</td>
<td>37 / 80 (46.3%)</td>
</tr>
<tr>
<td>GPT-4</td>
<td>Vicuna-13B v.s. Alpaca-13B</td>
<td>92.5%</td>
<td>92.5%</td>
<td>4 / 80 (5.0%)</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>Vicuna-13B v.s. ChatGPT</td>
<td>2.5%</td>
<td>82.5%</td>
<td>66 / 80 (82.5%)</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>Vicuna-13B v.s. Alpaca-13B</td>
<td>37.5%</td>
<td>90%</td>
<td>42 / 80 (52.5%)</td>
</tr>
</tbody>
</table>
<p>Table 2: The Win Rate of Vicuna-13B significantly fluctuates when positioned as Assistant 1 and Assistant 2, with GPT-4 and ChatGPT serving as evaluators. Conflict Rate refers to the proportion of conflicting results given by the same evaluator when simply changing the position of two models.
positional bias, i.e., by swapping the slots of the two responses and querying LLM twice, the evaluator will most likely produce conflicting evaluation results, and the evaluator prefers the response at a certain position.</p>
<h3>2.2 Revealing the Positional Bias</h3>
<p>In this section, we adopt GPT-4 and ChatGPT as evaluators to analyze the characteristics of positional bias in LLM evaluators. We find that:</p>
<p>LLMs are sensitive to the position of responses. As shown in Table 2, in the evaluation of "Vicuna13B v.s. ChatGPT" and "Vicuna-13B v.s. Alpaca13B", when the order was changed, LLMs provide different evaluation results, e.g., the win rate of Vicuna-13B extremely differs when Vicuna-13B is evaluated as Assistant 1 and Assistant 2.</p>
<p>To empirically evaluate the sensitivity, we introduced a metric Conflict Rate to measure the sensitivity of the model to response positions quantitatively. Formally, given $N$ examples $\left{\left(q_{i}, r 1_{i}, r 2_{i}\right)\right}<em i="i">{i=1}^{N}$, for each example $\left(q</em>}, r 1_{i}, r 2_{i}\right)$, we query the LLM with two prompts $T\left(q_{i}, r 1_{i}, r 2_{i}\right)$ and $T\left(q_{i}, r 2_{i}, r 1_{i}\right)$, and obtain corresponding two evaluation results $\mathbf{E R<em i="i">{i}^{r 12}$ and $\mathbf{E R}</em>$. Then we calculate the Conflict Rate of the LLM evaluator as follows:}^{r 21</p>
<p>$$
\text { Conflict Rate }=\frac{\sum_{i=1}^{N} \mathbb{I}\left(\mathbf{E R}<em i="i">{i}^{r 12} \neq \mathbf{E R}</em>
$$}^{r 21}\right)}{N</p>
<p>where $\mathbb{I}($.$) is the indicator function. We found$ that GPT-4 exhibited conflict rates of 46.3\% and 5.0\%, respectively. In contrast, ChatGPT displayed considerably higher conflict rates, with figures of $82.5 \%$ and $52.5 \%$, respectively. These findings indicate that LLMs can be self-conflicting due to the sensitivity of the response order in the template, with stronger models being less influenced by the placement of responses.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The conflict rate is negatively correlated with the score gap between the two responses. When swapping the order of two responses, the smaller the score gap between them, the more likely GPT-4 is to produce conflicting results.</p>
<p>LLMs suffer from Positional Bias, i.e., they prefer the response in the specific position. Based on the same evaluation template $T$ in Table 1, GPT4 tends to favor the response in the first position, while ChatGPT shows a preference for the response in the second position. For example, as illustrated in Table 2, in the comparison "Vicuna-13B v.s. ChatGPT", GPT-4 yields Win Rates of $51.3 \%$ and $23.8 \%$ for Vicuna-13B when it is positioned as Assistant 1 and Assistant 2, respectively. Conversely, ChatGPT indicates Win Rates of only $2.5 \%$ and up to $82.5 \%$ for Vicuna-13B when it is positioned as Assistant 1 and Assistant 2, respectively.</p>
<p>The degree of positional bias varies based on the difference in response quality. We notice that the conflict rate of "Vicuna-13B v.s. Alpaca13B" is much lower than that of "Vicuna-13B v.s. ChatGPT", suggesting that positional bias may not have the same impact on the assessment of different responses. One potential reason is that there is a significant difference in the quality of responses between Alpaca models and Vicuna models, and positional bias is not strong enough to change the</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Demonstration of our calibration framework with three calibration methods. $S_{r}$ and $S_{r}^{\prime}$ denotes scores of the response $r$ in the first and second positions, respectively. BPDE is short for Balanced Position Diversity Entropy score, which is calculated based on the evaluation results (ER) of MEC and BPC.</p>
<div class="codehilite"><pre><span></span><code><span class="k">[Question]</span>
<span class="k">[Q]</span>
<span class="k">[The Start of Assistant 1&#39;s response]</span>
<span class="k">[R1]</span>
<span class="k">[The End of Assistant 1&#39;s response]</span>
<span class="k">[The Start of Assistant 2&#39;s response]</span>
<span class="k">[R2]</span>
<span class="k">[The End of Assistant 2&#39;s response]</span>
<span class="k">[System]</span>
<span class="na">We would like to request your feedback on the per-</span>
<span class="na">formance of two AI assistants in response to the user</span>
<span class="na">question displayed above.</span>
<span class="na">Please rate the helpfulness, relevance, accuracy, and</span>
<span class="na">level of detail of their responses. Each assistant re-</span>
<span class="na">ceives an overall score on a scale of 1 to 10, where a</span>
<span class="na">higher score indicates better overall performance.</span>
<span class="na">Please first provide a comprehensive explanation of</span>
<span class="na">your evaluation, avoiding any potential bias and en-</span>
<span class="na">suring that the order in which the responses were</span>
<span class="na">presented does not affect your judgment. Then, out-</span>
<span class="na">put two lines indicating the scores for Assistant 1 and</span>
<span class="na">2, respectively.</span>
<span class="na">Output with the following format</span><span class="o">:</span>
<span class="na">Evaluation evidence</span><span class="o">:</span><span class="w"> </span><span class="s">&lt;evaluation explanation here&gt;</span>
<span class="na">The score of Assistant 1</span><span class="o">:</span><span class="w"> </span><span class="s">&lt;score&gt;</span>
<span class="na">The score of Assistant 2</span><span class="o">:</span><span class="w"> </span><span class="s">&lt;score&gt;</span>
</code></pre></div>

<p>Table 3: The evidence calibration evaluation template that prompts LLMs to generate the evaluation evidence first (red text), and then evaluate two responses.</p>
<p>judgment in such a situation. To further investigate this issue, we grouped all the examples based on the score difference between the two responses. As shown in Figure 2, we found that when the score difference between the two responses is small (e.g., score gap $\leq 1$ ), the evaluation results of GPT-4 are significantly affected by the position of the responses. On the other hand, when the score difference between the two responses is large (e.g., score gap $\geq 3$ ), GPT-4's evaluation results are relatively stable.</p>
<h2>3 Calibrating the Positional Bias</h2>
<p>We have identified that positional bias can significantly impact the evaluation results of LLMs, making them unfair evaluators. In this section, we propose a calibration framework with three simple yet effective strategies to alleviate this bias to achieve a more reliable and fair evaluation result.</p>
<h3>3.1 Multiple Evidence Calibration</h3>
<p>Previous studies (Zheng et al., 2023; Wang et al., 2023b) utilize the evaluation template that draws the conclusion first and then makes an explanation, e.g., the template used in Table 1. However, due to the nature of the auto-regressive model, the conclusions generated by the model are not supported by the explanation generated afterward. To this end, as shown in Table 3, we design an evidence calibration (EC) evaluation template $T_{E C}(Q, R 1, R 2)$ that requires the model to generate the explanation (evaluation evidence) first and then give the score. In this way, the score can be calibrated with the evaluation evidence. To further improve the reliability of the evaluation, rather than generating only a single EC score for each response, we perform a multiple evidence calibration (MEC, Figure 3(a)) that samples $k$ EC scores $\left{S_{r 1}^{1}, \ldots, S_{r 1}^{k}\right}$ and $\left{S_{r 2}^{1}, \ldots, S_{r 2}^{k}\right}$ for responses $r 1$ and $r 2$, where $S_{r}$ and $S_{r}^{\prime}$ denotes scores of the response $r$ at the first and second positions, respectively.</p>
<p>3.2 Balanced Position Calibration</p>
<p>We further employ a balanced position calibration (BPC) strategy to alleviate the previously identified positional bias of LLMs. As shown in Figure 3(b), for each example $(q, r 1, r 2)$, BPC additionally creates a query prompt $T_{E C}(q, r 2, r 1)$ by swapping the position of two responses in the original query prompt $T_{E C}(q, r 1, r 2)$. Combined with MEC, we can achieve $2 k$ scores $\left{S_{r 1}^{1}, \ldots, S_{r 1}^{k}, \ldots, S_{r 1}^{r 1}, \ldots, S_{r 1}^{r k}\right}$ and $\left{S_{r 2}^{1}, \ldots, S_{r 2}^{r k}, \ldots, S_{r 2}^{1}, \ldots, S_{r 2}^{k}\right}$ for $r 1$ and $r 2$, respectively. The final calibrated scores of two responses ( $C S_{r 1}$ and $C S_{r 2}$ ) are the average of the $2 k$ scores:</p>
<p>$$
C S_{R}=\sum_{i=1}^{k} \frac{S_{R}^{i}+S_{R}^{i}}{2 k}, R=r 1, r 2
$$</p>
<p>and we regard the response with the higher average score as the better response.</p>
<h3>3.3 Human-in-the-Loop Calibration</h3>
<p>In addition to the automatic calibration strategies, another interesting question we want to explore is whether Human-In-The-Loop Calibration (HITLC) which performs the cooperation of humans and LLMs as evaluators, could stabilize the evaluation result. The key point of human-in-the-loop calibration is when humans should be involved in the evaluation and calibrate the evaluation result on which LLM evaluators do not perform well.</p>
<p>To target the "when" problem, inspired by Cai, Chang, and Han (2023), we introduce a Balanced Position Diversity Entropy (BPDE) score to find examples requiring auxiliary human calibration based on the evaluation results of MEC and BPC. Specifically, as shown in Figure 3(c), we first compute $2 k$ evaluation results $\left{\mathbf{E R}<em i="1">{i}\right}</em>$ based on the $2 k$ pairs of scores.}^{2 k</p>
<p>$$
\underset{1 \leq i \leq k}{\mathbf{E R}<em 1="1" r="r">{i}}=\left{\begin{array}{l}
\text { win, } S</em> \
\text { tie, } S_{r 1}^{i}=S_{r 2}^{i} \
\text { lose }, S_{r 1}^{i}<S_{r 2}^{i}
\end{array}, \begin{array}{l}
\mathbf{E R}_{1}^{\prime} \\
1 \leq i \leq k
\end{array}= \begin{cases}\text { win, } S_{r 1}^{i_{1}}>S_{r 2}^{i} \
\text { tie, } S_{r 1}^{i}=S_{r 2}^{i} \
\text { lose, } S_{r 1}^{i}&lt;S_{r 2}^{i}\end{cases}
\end{array}\right.
$$}^{i}&gt;S_{r 2}^{i</p>
<p>and BPDE is defined as the entropy of the evaluation results:</p>
<p>$$
\begin{gathered}
\operatorname{BPDE}=\sum_{\mathbf{e r} \in{\text { win,tie,lose }}}-\mathbf{p}<em _mathbf_e="\mathbf{e" r="r">{\mathbf{e r}} \log \mathbf{p}</em> \
\mathbf{p}}<em i="1">{\mathbf{e r}}=\frac{\sum</em>}^{k} \mathbb{I}\left(\mathbf{E R<em i="i">{i}=\mathbf{e r}\right)+\mathbb{I}\left(\mathbf{E R}</em>
\end{gathered}
$$}^{\prime}=\mathbf{e r}\right)}{2 k</p>
<p>A higher BPDE score indicates that it is more likely the evaluation requires manual correction. A threshold is needed for BPDE as the hyper-parameter to select the top- $\beta$ most likely biased evaluations. After selection based on the BPDE score, the annotators will evaluate the selected examples and integrate the human annotations based on the majority opinion as described in Section 4.1.</p>
<h2>4 Experiments</h2>
<h3>4.1 Human Annotation</h3>
<p>To assess the effectiveness of our proposed strategies, three of the authors manually annotate the "win/tie/lose" outcomes of responses from ChatGPT and Vicuna-13B independently in all 80 Vicuna Benchmark questions. All of the annotators are researchers familiar with Artificial Intelligence and are well-equipped to assess the quality of the responses. Following the same template as the original Vicuna, the annotators are instructed to assess the responses provided by Vicuna-13B and ChatGPT from four different perspectives: helpfulness, relevance, accuracy, and level of detail. The responses of Vicuna and ChatGPT are presented to the annotators in random order. The evaluation process for each example took an average of three minutes. The final result is based on the majority opinion among three annotators.</p>
<h3>4.2 Experimental Setup and Metric</h3>
<p>We use the OpenAI API to conduct our experiments ("gpt-3.5-turbo-0301" for ChatGPT, and "gpt-4" for GPT-4). For the methods that do not need to sample multiple generation results, we set the generated temperature to 0 for deterministic generation results. For the multiple evidence strategy, we set the temperature to 1 and sample three generation results $(k=3)$. We use the accuracy and kappa correlation coefficient (McHugh, 2012) with the final majority of human annotation results to measure the performance of different evaluators and evaluation methods. When calculating the results for methods that do not utilize BPC, we randomize the order of the two responses from the assistants and calculate the average results of 100 runs to ensure stable results.</p>
<h3>4.3 Main Results</h3>
<p>Table 4 illustrates the performance of different methods on our manually annotated 80 annotated examples. As is shown: 1) There is a good correla-</p>
<table>
<thead>
<tr>
<th>Evaluators</th>
<th>Methods</th>
<th>Accuracy</th>
<th>Kappa</th>
<th>Cost</th>
</tr>
</thead>
<tbody>
<tr>
<td>Human 1</td>
<td>-</td>
<td>68.8%</td>
<td>0.50</td>
<td>$30.0</td>
</tr>
<tr>
<td>Human 2</td>
<td>-</td>
<td>76.3%</td>
<td>0.62</td>
<td>$30.0</td>
</tr>
<tr>
<td>Human 3</td>
<td>-</td>
<td>70.0%</td>
<td>0.50</td>
<td>$30.0</td>
</tr>
<tr>
<td>Human Average</td>
<td>-</td>
<td>71.7%</td>
<td>0.54</td>
<td>$30.0</td>
</tr>
<tr>
<td>GPT-4</td>
<td>VANILLA</td>
<td>52.7%</td>
<td>0.24</td>
<td>$2.00</td>
</tr>
<tr>
<td>GPT-4</td>
<td>EC $(k=1)$</td>
<td>56.5%</td>
<td>0.29</td>
<td>$2.00</td>
</tr>
<tr>
<td>GPT-4</td>
<td>MEC $(k=3)$</td>
<td>58.7%</td>
<td>0.30</td>
<td>$3.19</td>
</tr>
<tr>
<td>GPT-4</td>
<td>MEC $(k=6)$</td>
<td>60.9%</td>
<td>0.33</td>
<td>$6.38</td>
</tr>
<tr>
<td>GPT-4</td>
<td>MEC $(k=3)+\operatorname{BPC}(k=3)$</td>
<td>62.5%</td>
<td>0.37</td>
<td>$6.38</td>
</tr>
<tr>
<td>GPT-4</td>
<td>MEC $(k=3)+\operatorname{BPC}(k=3)+\operatorname{HITLC}(\beta=20\%)$</td>
<td>73.8%</td>
<td>0.56</td>
<td>$23.1</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>VANILLA</td>
<td>44.4%</td>
<td>0.06</td>
<td>$0.10</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>EC $(k=1)$</td>
<td>52.6%</td>
<td>0.23</td>
<td>$0.10</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>MEC $(k=3)$</td>
<td>53.2%</td>
<td>0.24</td>
<td>$0.17</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>MEC $(k=6)$</td>
<td>55.6%</td>
<td>0.27</td>
<td>$0.34</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>MEC $(k=3)+\operatorname{BPC}(k=3)$</td>
<td>58.7%</td>
<td>0.31</td>
<td>$0.34</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>MEC $(k=3)+\operatorname{BPC}(k=3)+\operatorname{HITLC}(\beta=20\%)$</td>
<td>71.3%</td>
<td>0.52</td>
<td>$18.3</td>
</tr>
</tbody>
</table>
<p>Table 4: Accuracy and kappa correlation coefficient of different methods and annotators with the final voting human annotations. The VANILLA evaluation method was commonly used in previous works, which provided the conclusion first and then followed with the explanation. (M)EC, BPC, and HITLC denote our proposed (multiple) evidence calibration, balanced position calibration, and human-in-the-loop calibration respectively. $\beta \%$ means selecting the top- $\beta$ most likely biased examples for human annotation.</p>
<p>tion coefficient between the annotations provided by each human annotator and the final voting results. In detail, the average accuracy and the kappa correlation coefficient of human annotations are 71.7% and 0.54, respectively; 2) Overall, GPT-4 achieves higher alignment with human judgments compared with ChatGPT, showing its powerful alignment ability with humans; 3) Compared to the commonly used VANILLA evaluation method, our proposed automatic calibration strategies (i.e., EC, MEC and BPC) significantly enhance the alignment between GPT-4 and ChatGPT with human judgments; For instance, by employing the MEC and BPC calibration strategies, ChatGPT demonstrates a notable improvement in both accuracy and the kappa correlation coefficient. Specifically, the accuracy is improved by 14.3%, and the kappa correlation coefficient is increased from 0.06 to 0.31; 4) "MEC $(k=3)+\operatorname{BPC}(k=3)$" outperforms "MEC $(k=6)$", demonstrating that LLMs are affected by positional bias, and BPC effectively ensures that LLMs serve as fair evaluators; 5) Our proposed HITLC can effectively enhance the alignment between GPT-4 and ChatGPT with human judgments, requiring only a small amount of human labor. For example, by incorporating just 20% $(\beta=20\%)$ human assistance, ChatGPT attains comparable Human Average accuracy, while reducing the annotation cost from $30 to $18.3, a 39% reduction.</p>
<p>In conclusion, our proposed calibration methods are simple yet very effective in improving the evaluation performance with LLM as evaluators, while maintaining low costs.</p>
<h2>5 Analysis</h2>
<h3>5.1 Ablation on Evidence Number $k$ and Temperature $t$</h3>
<p>In the MEC and BPC strategy, we sample $k$ evaluation results for each query prompt and ensemble them to enhance the evaluation process. We conduct an analysis to examine the influence of the number of evidence $k$, on the model’s evaluation performance. As illustrated in Figure 4(a), we compared the performance of ChatGPT with different values of $k$, namely 1, 3, 5, and 7. The</p>
<p>The minimum hourly wage in the United States is near $7.5, which can be found at https://www.worker.gov/. On average, annotating an example takes 3 minutes, and the Vicuna evaluation benchmark comprises 80 examples in total. Consequently, the cost per annotator amounts to $30.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Variation of accuracy and kappa coefficient with a different number of evidence <em>k</em> and sampling temperature <em>t</em> when ChatGPT is used as the evaluator.</p>
<p>model's performance increases and then tends to be constant or decreases slightly as <em>k</em> becomes larger. Despite the slight decrease, the enhancement of the model effect by the MCE strategy is still significant, illustrating the stability of the MEC strategy. Consequently, we found that a <em>k</em> value of 3 yields an optimal performance. With this value, the model achieves a notable level of performance while keeping the API cost relatively low.</p>
<p>We further investigate the impact of sampling temperature <em>t</em> on evaluation performance. Figure 4(b) illustrates that both low temperature (i.e., 0.2) and high temperature (i.e., 1.4) result in suboptimal evaluation alignment. We believe that low temperature eliminates the randomness of sampling, weakening the effect of MEC, while high temperature compromises the quality of generation results, leading to poor performance. Hence, it is crucial to select an appropriate temperature (e.g., 0.6 or 1.0 in our experiments) for the LLM evaluators.</p>
<h3>5.2 Effectiveness of the BPDE</h3>
<p>Our HITLC strategy utilizes BPDE score to select examples for human annotations. In order to analyze the efficiency of BPDE score, we compare BPDE with two typical baselines, <em>Random</em> and <em>Vanilla Diversity Entropy</em>, where Random denotes randomly select examples for human annotations, and Vanilla Diversity Entropy is calculated by using only the evaluation results of one position without swapping the position of two responses. To ensure fairness, the total number of evaluation results is 6 for both BPDE and Vanilla Diversity Entropy. As shown in Figure 5: <strong>1)</strong> Two Diversity Entropy methods outperform Random, showing the effectiveness of selecting examples based on the diversity entropy; <strong>2)</strong> BPDE outperforms Vanilla DE, which shows LLMs are sensitive to position exchange, and the results of BPC can significantly improve</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The accuracy of various methods changes with different human assistant thresholds (Top-β) when ChatGPT is used as the evaluator.</p>
<table>
<thead>
<tr>
<th>Templates</th>
<th>Methods</th>
<th>Acc.</th>
<th>Kap.</th>
<th>C.R</th>
</tr>
</thead>
<tbody>
<tr>
<td>Scoring</td>
<td>Vanilla</td>
<td>44.4%</td>
<td>0.06</td>
<td>82.5%</td>
</tr>
<tr>
<td>Scoring</td>
<td>MEC</td>
<td>53.2%</td>
<td>0.24</td>
<td>35.0%</td>
</tr>
<tr>
<td>Scoring</td>
<td>MEC + BPC</td>
<td>58.7%</td>
<td>0.31</td>
<td>N/A</td>
</tr>
<tr>
<td>Comparing</td>
<td>Vanilla</td>
<td>50.2%</td>
<td>0.18</td>
<td>50.0%</td>
</tr>
<tr>
<td>Comparing</td>
<td>MEC</td>
<td>54.8%</td>
<td>0.27</td>
<td>42.5%</td>
</tr>
<tr>
<td>Comparing</td>
<td>MEC + BPC</td>
<td>60.3%</td>
<td>0.35</td>
<td>N/A</td>
</tr>
</tbody>
</table>
<p>Table 5: Effectiveness of our proposed two automatic calibrated methods on two different evaluation templates with ChatGPT as the evaluator. Acc., Kap. and C.R are short for Accuracy, Kappa correlation coefficient, and Conflict Rate, respectively. N/A means the Conflict Rate is not valid for BPC methods.</p>
<p>the performance of HITLC compared to relying solely on the results of MEC.</p>
<h3>5.3 Generalization on the Pairwise Comparison Evaluation Template</h3>
<p>To provide a more comprehensive validation of our proposed calibration methods, in addition to the previous Scoring evaluation template that rates each response, we extend our analysis to incorporate the Comparing evaluation template. This template facilitates a direct comparison between two responses, eschewing explicit scores in its assessment. Specifically, we prompt LLMs to produce results labeled as "Assistant 1", "Assistant 2", or "Same", indicating whether the response from Assistant 1 is better, worse, or equal to that of Assistant 2. As is shown in Table 5: <strong>1)</strong> Our proposed methods are applicable to both of these templates, leading to enhanced accuracy and a heightened correlation coefficient for ChatGPT; <strong>2)</strong> The significant</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Fine-grained analysis of evaluation quality. Our MEC and BPC improve the evaluation performance of ChatGPT and GPT-4 in nearly all categories. Especially on the complex task categories such as common sense, coding, and math for ChatGPT.</p>
<p>performance gap (nearly 6% accuracy) between the VANILLA method of two templates, coupled with the high conflict rate, highlights the sensitivity and unreliability of LLMs. However, our methods effectively narrow this performance gap and reduce conflict, showcasing how calibration enhances LLM robustness.</p>
<h3>5.4 Fine-Grained Analysis of Evaluation Quality</h3>
<p>In order to further analyze the evaluation capabilities of the model, we perform a fine-grained analysis of the questions by dividing them into 9 categories following <em>Zheng et al. (2023)</em>. We calculate the performance of different evaluators within these categories. As shown in Figure 6, we find that: 1) In certain complex tasks such as common-sense, coding and math, GPT-4 performs significantly better than ChatGPT, highlighting the strength of GPT-4 as a more fair evaluator in these scenarios; 2) Our proposed MEC+BPC strategy demonstrates noticeable improvement in evaluating ChatGPT's performance on complex tasks, allowing us to obtain satisfactory evaluation results with a low API cost.</p>
<h2>6 Related Work</h2>
<h3>6.1 Evaluation of Large Language Models</h3>
<p>LLMs have demonstrated powerful general generation capabilities, becoming universal assistants (OpenAI, 2022, 2023; Song et al., 2023b). With the rapid advancement of LLMs, it becomes crucial to evaluate their ability to follow human instructions. Traditional evaluation methods assess the ability by calculating a metric, like BLEU, ROUGE, BERTScore, or BARTScore, to compare the generated response with a reference response. However, these metrics do not adequately measure the alignment of the generated response with human intent (He et al., 2023). While human evaluation is treated as the most accurate measurement of model performance, it is costly and time-consuming to operate at scales. Considering the potent capabilities of LLMs, researchers have started utilizing LLMs to evaluate the proficiency of generative models in adhering to human instructions (Zheng et al., 2023; Lu et al., 2023; Li et al., 2023). In these works, Vicuna's evaluation paradigm (Zheng et al., 2023) is widely adopted, where it provides a question and two responses from two models, and uses GPT-4 to determine which response has better quality.</p>
<h3>6.2 Bias of Deep Neural Networks</h3>
<p>Deep Neural Networks have been proven to easily learn biases from the data, which significantly impacts their reliability. Specifically, bias has also been investigated in natural language inference (Gururangan et al., 2018; McCoy, Pavlick, and Linzen, 2019; Belinkov et al., 2019; Liu et al., 2020a,b), question answering (Min et al., 2019), ROC story cloze (Cai, Tu, and Gimpel, 2017; Schwartz et al., 2017), lexical inference (Levy et al., 2015), visual question answering (Goyal et al., 2017), information extraction (Wang et al., 2021, 2022; Song et al., 2023a; Xia et al., 2023) and so on. LLMs are pre-trained using a vast amount of data from the internet, making it highly likely for them to learn biases present in those materials. Although the LLMs are already widely adopted as a proxy of human evaluators, the reliability of this paradigm</p>
<p>is not well explored. In this paper, we critically examine the LLMs-as-evaluator paradigm and uncover a significant positional bias. Furthermore, we propose three simple yet effective methods to calibrate the positional bias to achieve reliable and fair evaluation results.</p>
<h2>7 Conclusion</h2>
<p>In this paper, we reveal a systematic positional bias in evaluation with advanced ChatGPT/GPT-4 models: by manipulating the order of candidate responses during evaluation, the quality ranking results can be significantly influenced. To this end, we introduce three effective strategies, namely Multiple Evidence Calibration (MEC), Balanced Position Calibration (BPC), and Human-in-the-Loop Calibration (HITLC). MEC requires the LLM evaluator to first provide multiple evaluation evidence to support their subsequent ratings and BPC aggregates the results from various orders to determine the final score. Based on the results of MEC and BPC, HITLC further calculates a balanced position diversity entropy to select examples for human annotations. These strategies successfully reduce the evaluation bias and improve alignment with human judgments. We provide our code and human annotations to support future studies and enhance the evaluation of generative models.</p>
<h2>References</h2>
<p>Belinkov, Y.; Poliak, A.; Shieber, S.; Van Durme, B.; and Rush, A. 2019. Don't Take the Premise for Granted: Mitigating Artifacts in Natural Language Inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<p>Bowman, S. R. 2023. Eight things to know about large language models. arXiv preprint arXiv:2304.00612.</p>
<p>Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D. 2020. Language Models are Few-Shot Learners. In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin, H., eds., Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Cai, Z.; Chang, B.; and Han, W. 2023. Human-in-the-Loop through Chain-of-Thought. arXiv preprint arXiv:2306.07932.</p>
<p>Cai, Z.; Tu, L.; and Gimpel, K. 2017. Pay Attention to the Ending:Strong Neural Baselines for the ROC Story Cloze Task. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<p>Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra, G.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.; Gehrmann, S.; Schuh, P.; Shi, K.; Tsvyashchenko, S.; Maynez, J.; Rao, A.; Barnes, P.; Tay, Y.; Shazeer, N. M.; Prabhakaran, V.; Reif, E.; Du, N.; Hutchinson, B. C.; Pope, R.; Bradbury, J.; Austin, J.; Isard, M.; Gur-Ari, G.; Yin, P.; Duke, T.; Levskaya, A.; Ghemawat, S.; Dev, S.; Michalewski, H.; García, X.; Misra, V.; Robinson, K.; Fedus, L.; Zhou, D.; Ippolito, D.; Luan, D.; Lim, H.; Zoph, B.; Spiridonov, A.; Sepassi, R.; Dohan, D.; Agrawal, S.; Omernick, M.; Dai, A. M.; Pillai, T. S.; Pellat, M.; Lewkowycz, A.; Moreira, E.; Child, R.; Polozov, O.; Lee, K.; Zhou, Z.; Wang, X.; Saeta, B.; Díaz, M.; Firat, O.; Catasta, M.; Wei, J.; Meier-Hellstern, K. S.; Eck, D.; Dean, J.; Petrov, S.; and Fiedel, N. 2022. PaLM: Scaling Language Modeling with Pathways. ArXiv, abs/2204.02311.</p>
<p>Dong, Q.; Li, L.; Dai, D.; Zheng, C.; Wu, Z.; Chang, B.; Sun, X.; Xu, J.; and Sui, Z. 2022. A Survey for Incontext Learning. arXiv preprint arXiv:2301.00234.</p>
<p>Dubois, Y.; Li, X.; Taori, R.; Zhang, T.; Gulrajani, I.; Ba, J.; Guestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Alpacafarm: A simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387.</p>
<p>Gao, P.; Han, J.; Zhang, R.; Lin, Z.; Geng, S.; Zhou, A.; Zhang, W.; Lu, P.; He, C.; Yue, X.; Li, H.; and Qiao, Y. J. 2023. LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model. ArXiv, abs/2304.15010.</p>
<p>Goyal, Y.; Khot, T.; Summers-Stay, D.; Batra, D.; and Parikh, D. 2017. Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</p>
<p>Gururangan, S.; Swayamdipta, S.; Levy, O.; Schwartz, R.; Bowman, S.; and Smith, N. A. 2018. Annotation Artifacts in Natural Language Inference Data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics.</p>
<p>He, T.; Zhang, J.; Wang, T.; Kumar, S.; Cho, K.; Glass, J.; and Tsvetkov, Y. 2023. On the Blind Spots of Model-Based Evaluation Metrics for Text Generation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 12067-12097. Toronto, Canada: Association for Computational Linguistics.</p>
<p>Levy, O.; Remus, S.; Biemann, C.; and Dagan, I. 2015. Do Supervised Distributional Methods Really Learn Lexical Inference Relations? In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics.</p>
<p>Li, L.; Yin, Y.; Li, S.; Chen, L.; Wang, P.; Ren, S.; Li, M.; Yang, Y.; Xu, J.; Sun, X.; et al. 2023. M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning. arXiv preprint arXiv:2306.04387.</p>
<p>Lin, C.-Y. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out, 74-81. Barcelona, Spain: Association for Computational Linguistics.</p>
<p>Liu, T.; Xin, Z.; Chang, B.; and Sui, Z. 2020a. HypoNLI: Exploring the Artificial Patterns of Hypothesis-only Bias in Natural Language Inference. In Proceedings of the 12th Language Resources and Evaluation Conference. Marseille, France: European Language Resources Association. ISBN 979-10-95546-34-4.</p>
<p>Liu, T.; Xin, Z.; Ding, X.; Chang, B.; and Sui, Z. 2020b. An Empirical Study on Model-agnostic Debiasing Strategies for Robust Natural Language Inference. In Proceedings of the 24th Conference on Computational Natural Language Learning. Online: Association for Computational Linguistics.</p>
<p>Lu, Q.; Qiu, B.; Ding, L.; Xie, L.; and Tao, D. 2023. Error analysis prompting enables human-like translation evaluation in large language models: A case study on chatgpt. arXiv preprint arXiv:2303.13809.</p>
<p>McCoy, T.; Pavlick, E.; and Linzen, T. 2019. Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<p>McHugh, M. L. 2012. Interrater reliability: the kappa statistic. Biochemia medica, 22(3): 276-282.</p>
<p>Min, S.; Wallace, E.; Singh, S.; Gardner, M.; Hajishirzi, H.; and Zettlemoyer, L. 2019. Compositional Questions Do Not Necessitate Multi-hop Reasoning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<p>OpenAI. 2022. Introducing ChatGPT.
OpenAI. 2023. GPT-4 Technical Report. CoRR, abs/2303.08774.</p>
<p>Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, 311-318. Philadelphia, Pennsylvania, USA: Association for Computational Linguistics.</p>
<p>Peng, B.; Li, C.; He, P.; Galley, M.; and Gao, J. 2023. Instruction Tuning with GPT-4. ArXiv, abs/2304.03277.</p>
<p>Schwartz, R.; Sap, M.; Konstas, I.; Zilles, L.; Choi, Y.; and Smith, N. A. 2017. The Effect of Different Writing Tasks on Linguistic Style: A Case Study of the ROC Story Cloze Task. In Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL).</p>
<p>Song, Y.; Wang, P.; Zhu, D.; Liu, T.; Sui, Z.; and Li, S. 2023a. RepCL: Exploring Effective Representation for Continual Text Classification. arXiv preprint arXiv:2305.07289.</p>
<p>Song, Y.; Xiong, W.; Zhu, D.; Li, C.; Wang, K.; Tian, Y.; and Li, S. 2023b. RestGPT: Connecting Large Language Models with Real-World Applications via RESTful APIs. arXiv preprint arXiv:2306.06624.</p>
<p>Sun, Z.; Shen, Y.; Zhou, Q.; Zhang, H.; Chen, Z.; Cox, D. D.; Yang, Y.; and Gan, C. 2023. PrincipleDriven Self-Alignment of Language Models from Scratch with Minimal Human Supervision. ArXiv, abs/2305.03047.</p>
<p>Turpin, M.; Michael, J.; Perez, E.; and Bowman, S. R. 2023. Language Models Don’t Always Say What They Think: Unfaithful Explanations in Chain-ofThought Prompting. CoRR, abs/2305.04388.</p>
<p>Wang, P.; Song, Y.; Liu, T.; Lin, B.; Cao, Y.; Li, S.; and Sui, Z. 2022. Learning Robust Representations for Continual Relation Extraction via Adversarial Class Augmentation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, 6264-6278. Abu Dhabi, United Arab Emirates: Association for Computational Linguistics.</p>
<p>Wang, P.; Xun, R.; Liu, T.; Dai, D.; Chang, B.; and Sui, Z. 2021. Behind the Scenes: An Exploration of Trigger Biases Problem in Few-Shot Event Classification. In Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management, 1969-1978.</p>
<p>Wang, Y.; Ivison, H.; Dasigi, P.; Hessel, J.; Khot, T.; Chandu, K. R.; Wadden, D.; MacMillan, K.; Smith, N. A.; Beltagy, I.; et al. 2023a. How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources. arXiv preprint arXiv:2306.04751.</p>
<p>Wang, Y.; Yu, Z.; Zeng, Z.; Yang, L.; Wang, C.; Chen, H.; Jiang, C.; Xie, R.; Wang, J.; Xie, X.; et al. 2023b. PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization. arXiv preprint arXiv:2306.05087.</p>
<p>Xia, H.; Wang, P.; Liu, T.; Lin, B.; Cao, Y.; and Sui, Z. 2023. Enhancing Continual Relation Extraction via Classifier Decomposition. In Findings of the Association for Computational Linguistics: ACL 2023, 10053-10062. Toronto, Canada: Association for Computational Linguistics.</p>
<p>Xu, C.; Guo, D.; Duan, N.; and McAuley, J. 2023. Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data. ArXiv, abs/2304.01196.</p>
<p>Yuan, W.; Neubig, G.; and Liu, P. 2021. BARTScore: Evaluating Generated Text as Text Generation. In Ranzato, M.; Beygelzimer, A.; Dauphin, Y. N.; Liang, P.; and Vaughan, J. W., eds., Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, 27263-27277.</p>
<p>Zhang, T.; Kishore, V.; Wu, F.; Weinberger, K. Q.; and Artzi, Y. 2020. BERTScore: Evaluating Text Generation with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.; Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E.; et al. 2023. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. arXiv preprint arXiv:2306.05685.</p>
<p>Zhou, C.; Liu, P.; Xu, P.; Iyer, S.; Sun, J.; Mao, Y.; Ma, X.; Efrat, A.; Yu, P.; Yu, L.; Zhang, S.; Ghosh, G.; Lewis, M.; Zettlemoyer, L.; and Levy, O. 2023. LIMA: Less Is More for Alignment.</p>            </div>
        </div>

    </div>
</body>
</html>
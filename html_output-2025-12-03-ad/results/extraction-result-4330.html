<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4330 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4330</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4330</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-99.html">extraction-schema-99</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <p><strong>Paper ID:</strong> paper-274141118</p>
                <p><strong>Paper Title:</strong> ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity</p>
                <p><strong>Paper Abstract:</strong> Natural Language Processing (NLP) is widely used to supply summarization ability from long context to structured information. However, extracting structured knowledge from scientific text by NLP models remains a challenge because of its domain-specific nature to complex data preprocessing and the granularity of multi-layered device-level information. To address this, we introduce ByteScience, a non-profit cloud-based auto finetuned Large Language Model (LLM) platform, which is designed to extract structured scientific data and synthesize new scientific knowledge from vast scientific corpora. The platform capitalizes on DARWIN, an open-source, fine-tuned LLM dedicated to natural science. The platform was built on Amazon Web Services (AWS) and provides an automated, user-friendly workflow for custom model development and data extraction. The platform achieves remarkable accuracy with only a small amount of well-annotated articles. This innovative tool streamlines the transition from the science literature to structured knowledge and data and benefits the advancements in natural informatics. Demo Video</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4330.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4330.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ByteScience</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ByteScience (cloud-based auto fine-tuned LLM platform)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cloud platform that auto fine-tunes a domain-specific LLM (DARWIN) on a small set of annotated scientific documents to extract structured scientific data (entities, relations, datasets) from unstructured literature and support synthesis of scientific knowledge (e.g., CPSP relationships).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ByteScience pipeline (Auto fine-tuned LLM for structured extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>ByteScience ingests scientific documents (PDF/HTML/XML/JSON), converts non-JSON text (PDFMiner for PDFs) to JSON, allows user-defined annotation schemas (entity labels and relationships), selects a small subset for annotation, auto-prelabels texts using an LLM (DARWIN), then a human reviews/corrects annotations. Corrected annotations are transformed into the LLM instruction format and used to fine-tune DARWIN on AWS SageMaker (token-granularity fine-tuning implied by paper title). The fine-tuned model is deployed to a SageMaker Endpoint to process new documents at scale, producing structured JSON records stored in MongoDB. Workflow emphasizes a two-phase setup: (1) dataset construction and (2) LLM fine-tuning, followed by an operational phase using the deployed fine-tuned model.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>DARWIN (open-source domain-specific natural science LLM; size not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural sciences / Materials science (examples: batteries, catalysis, photovoltaics, alloy synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Experiments reported on 90 samples (batteries, catalysis, photovoltaics); platform claims scalability to millions of papers and gives a cost example for 10,000 papers</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Entity–relation patterns and domain relationships (e.g., Composition-Processing-Structure-Performance (CPSP) relationships), trend identification and statistical correlations inferred from structured datasets (no explicit claim of extracting symbolic physical laws or closed-form equations in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured JSON documents containing entity labels, relations and resolved entities (stored in MongoDB); structured datasets/tables suitable for downstream analysis</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Human review and correction of auto-labeled annotations; evaluation on held samples with standard NLP metrics (precision, recall, F1) and comparison to human annotation; iterative retraining using corrected subsets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported system-level extraction accuracy stated as achieving ~80%–90% of human extraction accuracy; paper reports precision/recall/F1 reaching ~0.8–0.9 with ~300 samples in related LLM experiments and a 57% reduction in annotation time using 300 training samples vs. one sample. Exact per-task metrics for ByteScience (per NER/RE/ER) not numerically listed in the text beyond these ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against non-LLM methods (e.g., MatBERT, MatKG-style approaches). The paper reports ByteScience/LLM approaches outperformed traditional methods across NER, relation extraction and entity resolution tasks, and notes MatBERT often produced irrelevant entities reducing precision; specific numeric baselines are not fully enumerated in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Domain-specific contextual dependencies that span sentences/sections; initial requirement for human-provided annotation schema and correction (human-in-loop); potential low recall if corpus coverage is insufficient (paper recommends adding more annotated data or sequential learning for low recall); paper does not claim extraction of symbolic mathematical laws or equations — focus is on structured data and relations; detailed model-size/resource tradeoffs and exact error modes are not fully quantified in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4330.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4330.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DARWIN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DARWIN (Domain-specific LLM series for natural science)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source, domain-specific large language model tailored for natural science tasks; used by ByteScience as the base LLM that is fine-tuned on small annotated corpora to extract structured scientific data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DARWIN Series: Domain Specific Large Language Models for Natural Science</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>DARWIN (fine-tuned domain LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>DARWIN is used as the starting pre-trained natural-science LLM; ByteScience fine-tunes DARWIN on task-specific instruction-format datasets derived from corrected annotations (via SageMaker Training Jobs) and deploys the fine-tuned model to a SageMaker Endpoint for document processing. The paper emphasizes fine-tuning with a minimal number of annotated documents (claims as few as one fully annotated paper in some cases) and token-granularity tuning implied by the platform title.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>DARWIN (open-source natural-science LLM). Model architecture presumed transformer-based; model size not provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural science / materials science (used across batteries, catalysis, photovoltaics and alloy synthesis in the ByteScience pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Used to extract structured entities and relationships (CPSP and similar domain relationships) rather than directly producing closed-form quantitative laws in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Instruction-structured outputs converted into JSON entity/relation records for database storage</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Evaluated after fine-tuning via comparison to human-corrected annotations and standard NLP metrics in downstream extraction tasks (as part of ByteScience evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Model-size and resource details not provided; reliance on small annotated datasets may require careful curation; not explicitly evaluated by itself outside the platform in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4330.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4330.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dunn et al. (2022)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structured information extraction from complex scientific text with fine-tuned large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced work that applies fine-tuned large language models to extract structured information from complex scientific text, serving as prior art and motivation for ByteScience's approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Structured information extraction from complex scientific text with fine-tuned large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Fine-tuned LLMs for structured scientific information extraction (as reported by Dunn et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Referenced as prior work demonstrating that large language models can be fine-tuned to extract structured information from scientific text; specific pipeline details are not given in the ByteScience paper beyond the citation, but Dunn et al. is cited as demonstrating feasibility of fine-tuned LLMs for complex scientific extraction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific literature broadly (paper cited as general prior work applicable to natural science extraction tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Not specified in the ByteScience paper's citation context; referenced for structured information extraction capability rather than explicit law extraction</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Not specified in the ByteScience text (original paper likely contains structured outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4330.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4330.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MatKG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MatKG (Materials Knowledge Graph)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large knowledge graph in materials science that defines relationships often by entity co-occurrence and is cited as a baseline that can miss nuanced, context-dependent relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MatKG: The Largest Knowledge Graph in Materials Science -Entities, Relations, and Link Prediction through Graph Representation Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>MatKG / co-occurrence-based knowledge graph construction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>MatKG constructs a materials-science knowledge graph using entity extraction and co-occurrence-based relationship definitions; the ByteScience paper contrasts this with LLM-based extraction that aims to capture richer contextual dependencies across documents where co-occurrence heuristics may oversimplify relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Knowledge-graph relationships (entity–relation links) rather than explicit quantitative laws; typically captures associations/co-occurrences</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Knowledge graph edges and entity nodes (graph structure); not presented as mathematical laws</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Presented as a baseline approach that can miss nuanced relationships compared to LLM-based extraction; no numeric head-to-head numbers provided in the ByteScience text.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>May miss nuanced/contextual relationships because it often defines relations by entity co-occurrence; risk of oversimplification of complex scientific statements.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4330.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4330.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MatBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MatBERT (material-science BERT variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BERT-based model adapted for materials science tasks, referenced as a non-LLM baseline that can produce irrelevant entities and thus lower precision in extraction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>MatBERT for entity extraction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>MatBERT is cited as a traditional transformer-based model applied to NER and related extraction tasks in materials science; the ByteScience paper notes MatBERT performed well on some tasks but produced irrelevant entities that lowered precision relative to their LLM approach.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>MatBERT (transformer/BERT variant for materials domain)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Not aimed at extracting quantitative laws; used for entity extraction and relation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Extracted entities/labels (structured annotations), not explicit quantitative laws</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported qualitatively as producing lower precision because of irrelevant entities; no precise numeric metrics provided in the ByteScience text.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared qualitatively to ByteScience's LLM approach; ByteScience reported better overall performance across tasks with fewer samples.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Tendency to output irrelevant entities in noisy/unstructured scientific text, reducing precision.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Structured information extraction from complex scientific text with fine-tuned large language models <em>(Rating: 2)</em></li>
                <li>DARWIN Series: Domain Specific Large Language Models for Natural Science <em>(Rating: 2)</em></li>
                <li>MatKG: The Largest Knowledge Graph in Materials Science -Entities, Relations, and Link Prediction through Graph Representation Learning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4330",
    "paper_id": "paper-274141118",
    "extraction_schema_id": "extraction-schema-99",
    "extracted_data": [
        {
            "name_short": "ByteScience",
            "name_full": "ByteScience (cloud-based auto fine-tuned LLM platform)",
            "brief_description": "A cloud platform that auto fine-tunes a domain-specific LLM (DARWIN) on a small set of annotated scientific documents to extract structured scientific data (entities, relations, datasets) from unstructured literature and support synthesis of scientific knowledge (e.g., CPSP relationships).",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "ByteScience pipeline (Auto fine-tuned LLM for structured extraction)",
            "method_description": "ByteScience ingests scientific documents (PDF/HTML/XML/JSON), converts non-JSON text (PDFMiner for PDFs) to JSON, allows user-defined annotation schemas (entity labels and relationships), selects a small subset for annotation, auto-prelabels texts using an LLM (DARWIN), then a human reviews/corrects annotations. Corrected annotations are transformed into the LLM instruction format and used to fine-tune DARWIN on AWS SageMaker (token-granularity fine-tuning implied by paper title). The fine-tuned model is deployed to a SageMaker Endpoint to process new documents at scale, producing structured JSON records stored in MongoDB. Workflow emphasizes a two-phase setup: (1) dataset construction and (2) LLM fine-tuning, followed by an operational phase using the deployed fine-tuned model.",
            "llm_model_used": "DARWIN (open-source domain-specific natural science LLM; size not specified)",
            "scientific_domain": "Natural sciences / Materials science (examples: batteries, catalysis, photovoltaics, alloy synthesis)",
            "number_of_papers": "Experiments reported on 90 samples (batteries, catalysis, photovoltaics); platform claims scalability to millions of papers and gives a cost example for 10,000 papers",
            "type_of_quantitative_law": "Entity–relation patterns and domain relationships (e.g., Composition-Processing-Structure-Performance (CPSP) relationships), trend identification and statistical correlations inferred from structured datasets (no explicit claim of extracting symbolic physical laws or closed-form equations in this paper)",
            "extraction_output_format": "Structured JSON documents containing entity labels, relations and resolved entities (stored in MongoDB); structured datasets/tables suitable for downstream analysis",
            "validation_method": "Human review and correction of auto-labeled annotations; evaluation on held samples with standard NLP metrics (precision, recall, F1) and comparison to human annotation; iterative retraining using corrected subsets",
            "performance_metrics": "Reported system-level extraction accuracy stated as achieving ~80%–90% of human extraction accuracy; paper reports precision/recall/F1 reaching ~0.8–0.9 with ~300 samples in related LLM experiments and a 57% reduction in annotation time using 300 training samples vs. one sample. Exact per-task metrics for ByteScience (per NER/RE/ER) not numerically listed in the text beyond these ranges.",
            "baseline_comparison": "Compared against non-LLM methods (e.g., MatBERT, MatKG-style approaches). The paper reports ByteScience/LLM approaches outperformed traditional methods across NER, relation extraction and entity resolution tasks, and notes MatBERT often produced irrelevant entities reducing precision; specific numeric baselines are not fully enumerated in the text.",
            "challenges_limitations": "Domain-specific contextual dependencies that span sentences/sections; initial requirement for human-provided annotation schema and correction (human-in-loop); potential low recall if corpus coverage is insufficient (paper recommends adding more annotated data or sequential learning for low recall); paper does not claim extraction of symbolic mathematical laws or equations — focus is on structured data and relations; detailed model-size/resource tradeoffs and exact error modes are not fully quantified in the text.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4330.0",
            "source_info": {
                "paper_title": "ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "DARWIN",
            "name_full": "DARWIN (Domain-specific LLM series for natural science)",
            "brief_description": "An open-source, domain-specific large language model tailored for natural science tasks; used by ByteScience as the base LLM that is fine-tuned on small annotated corpora to extract structured scientific data.",
            "citation_title": "DARWIN Series: Domain Specific Large Language Models for Natural Science",
            "mention_or_use": "use",
            "method_name": "DARWIN (fine-tuned domain LLM)",
            "method_description": "DARWIN is used as the starting pre-trained natural-science LLM; ByteScience fine-tunes DARWIN on task-specific instruction-format datasets derived from corrected annotations (via SageMaker Training Jobs) and deploys the fine-tuned model to a SageMaker Endpoint for document processing. The paper emphasizes fine-tuning with a minimal number of annotated documents (claims as few as one fully annotated paper in some cases) and token-granularity tuning implied by the platform title.",
            "llm_model_used": "DARWIN (open-source natural-science LLM). Model architecture presumed transformer-based; model size not provided in the paper.",
            "scientific_domain": "Natural science / materials science (used across batteries, catalysis, photovoltaics and alloy synthesis in the ByteScience pipeline)",
            "number_of_papers": null,
            "type_of_quantitative_law": "Used to extract structured entities and relationships (CPSP and similar domain relationships) rather than directly producing closed-form quantitative laws in this paper",
            "extraction_output_format": "Instruction-structured outputs converted into JSON entity/relation records for database storage",
            "validation_method": "Evaluated after fine-tuning via comparison to human-corrected annotations and standard NLP metrics in downstream extraction tasks (as part of ByteScience evaluation)",
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Model-size and resource details not provided; reliance on small annotated datasets may require careful curation; not explicitly evaluated by itself outside the platform in this paper.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4330.1",
            "source_info": {
                "paper_title": "ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Dunn et al. (2022)",
            "name_full": "Structured information extraction from complex scientific text with fine-tuned large language models",
            "brief_description": "A referenced work that applies fine-tuned large language models to extract structured information from complex scientific text, serving as prior art and motivation for ByteScience's approach.",
            "citation_title": "Structured information extraction from complex scientific text with fine-tuned large language models",
            "mention_or_use": "mention",
            "method_name": "Fine-tuned LLMs for structured scientific information extraction (as reported by Dunn et al.)",
            "method_description": "Referenced as prior work demonstrating that large language models can be fine-tuned to extract structured information from scientific text; specific pipeline details are not given in the ByteScience paper beyond the citation, but Dunn et al. is cited as demonstrating feasibility of fine-tuned LLMs for complex scientific extraction tasks.",
            "llm_model_used": null,
            "scientific_domain": "Scientific literature broadly (paper cited as general prior work applicable to natural science extraction tasks)",
            "number_of_papers": null,
            "type_of_quantitative_law": "Not specified in the ByteScience paper's citation context; referenced for structured information extraction capability rather than explicit law extraction",
            "extraction_output_format": "Not specified in the ByteScience text (original paper likely contains structured outputs)",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4330.2",
            "source_info": {
                "paper_title": "ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "MatKG",
            "name_full": "MatKG (Materials Knowledge Graph)",
            "brief_description": "A large knowledge graph in materials science that defines relationships often by entity co-occurrence and is cited as a baseline that can miss nuanced, context-dependent relationships.",
            "citation_title": "MatKG: The Largest Knowledge Graph in Materials Science -Entities, Relations, and Link Prediction through Graph Representation Learning",
            "mention_or_use": "mention",
            "method_name": "MatKG / co-occurrence-based knowledge graph construction",
            "method_description": "MatKG constructs a materials-science knowledge graph using entity extraction and co-occurrence-based relationship definitions; the ByteScience paper contrasts this with LLM-based extraction that aims to capture richer contextual dependencies across documents where co-occurrence heuristics may oversimplify relationships.",
            "llm_model_used": null,
            "scientific_domain": "Materials science",
            "number_of_papers": null,
            "type_of_quantitative_law": "Knowledge-graph relationships (entity–relation links) rather than explicit quantitative laws; typically captures associations/co-occurrences",
            "extraction_output_format": "Knowledge graph edges and entity nodes (graph structure); not presented as mathematical laws",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": "Presented as a baseline approach that can miss nuanced relationships compared to LLM-based extraction; no numeric head-to-head numbers provided in the ByteScience text.",
            "challenges_limitations": "May miss nuanced/contextual relationships because it often defines relations by entity co-occurrence; risk of oversimplification of complex scientific statements.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4330.3",
            "source_info": {
                "paper_title": "ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "MatBERT",
            "name_full": "MatBERT (material-science BERT variant)",
            "brief_description": "A BERT-based model adapted for materials science tasks, referenced as a non-LLM baseline that can produce irrelevant entities and thus lower precision in extraction tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "MatBERT for entity extraction",
            "method_description": "MatBERT is cited as a traditional transformer-based model applied to NER and related extraction tasks in materials science; the ByteScience paper notes MatBERT performed well on some tasks but produced irrelevant entities that lowered precision relative to their LLM approach.",
            "llm_model_used": "MatBERT (transformer/BERT variant for materials domain)",
            "scientific_domain": "Materials science",
            "number_of_papers": null,
            "type_of_quantitative_law": "Not aimed at extracting quantitative laws; used for entity extraction and relation tasks",
            "extraction_output_format": "Extracted entities/labels (structured annotations), not explicit quantitative laws",
            "validation_method": null,
            "performance_metrics": "Reported qualitatively as producing lower precision because of irrelevant entities; no precise numeric metrics provided in the ByteScience text.",
            "baseline_comparison": "Compared qualitatively to ByteScience's LLM approach; ByteScience reported better overall performance across tasks with fewer samples.",
            "challenges_limitations": "Tendency to output irrelevant entities in noisy/unstructured scientific text, reducing precision.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4330.4",
            "source_info": {
                "paper_title": "ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Structured information extraction from complex scientific text with fine-tuned large language models",
            "rating": 2,
            "sanitized_title": "structured_information_extraction_from_complex_scientific_text_with_finetuned_large_language_models"
        },
        {
            "paper_title": "DARWIN Series: Domain Specific Large Language Models for Natural Science",
            "rating": 2,
            "sanitized_title": "darwin_series_domain_specific_large_language_models_for_natural_science"
        },
        {
            "paper_title": "MatKG: The Largest Knowledge Graph in Materials Science -Entities, Relations, and Link Prediction through Graph Representation Learning",
            "rating": 2,
            "sanitized_title": "matkg_the_largest_knowledge_graph_in_materials_science_entities_relations_and_link_prediction_through_graph_representation_learning"
        }
    ],
    "cost": 0.0088976,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity</p>
<p>Tong Xie 
GreenDynamics
KensingtonAustralia</p>
<p>School of Photovoltaic and Renewable Energy Engineering
University of New South Wales
KensingtonAustralia</p>
<p>Hanzhi Zhang hanzhizhang@my.unt.edu 
Dept. of Computer Science and Engineering
University of North Texas
DentonUnited States</p>
<p>Shaozhou Wang shaozhou@greendynamics.com.au 
GreenDynamics
KensingtonAustralia</p>
<p>Yuwei Wan 
Imran Razzak imran.razzak@unsw.edu.au 
GreenDynamics
KensingtonAustralia</p>
<p>School of Computer Science and Engineering
University of New South Wales
KensingtonAustralia</p>
<p>Chunyu Kit ctckit@cityu.edu.hk 
Dept. of Linguistics and Translation
City University of Hong Kong
Hong KongChina</p>
<p>Wenjie Zhang wenjie.zhang@unsw.edu.au 
School of Computer Science and Engineering
University of New South Wales
KensingtonAustralia</p>
<p>Bram Hoex b.hoex@unsw.edu.au 
School of Photovoltaic and Renewable Energy Engineering
University of New South Wales
KensingtonAustralia</p>
<p>ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity
04E441E4B0A441AF9CBD44781A64541910.1109/ICDMW65004.2024.00126componentformattingstylestylinginsert
Natural Language Processing (NLP) is widely used to supply summarization ability from long context to structured information.However, extracting structured knowledge from scientific text by NLP models remains a challenge because of its domain-specific nature to complex data preprocessing and the granularity of multi-layered device-level information.To address this, we introduce ByteScience, a non-profit cloud-based auto finetuned Large Language Model (LLM) platform, which is designed to extract structured scientific data and synthesize new scientific knowledge from vast scientific corpora.The platform capitalizes on DARWIN, an open-source, fine-tuned LLM dedicated to natural science.The platform was built on Amazon Web Services (AWS) and provides an automated, user-friendly workflow for custom model development and data extraction.The platform achieves remarkable accuracy with only a small amount of wellannotated articles.This innovative tool streamlines the transition from the science literature to structured knowledge and data and benefits the advancements in natural informatics.Demo Video</p>
<p>I. INTRODUCTION</p>
<p>AI has the potential to revolutionize scientific discovery (AI4Science [1]), but challenges remain.Scientific knowledge is scattered across documents, making it hard to fully leverage past research.LLMs offer a promising solution but require structured texts, including converting PDFs and generating fine-tuning examples for NLP tasks.While machine learning models are used in fields like drug discovery [2], protein design [3], and crystal structure generation [4], limited structured data hinders their effectiveness.Databases like Materials Project [5] and NOMAD [6] cover only a fraction of data, leaving much unstructured information untapped.This gap presents an opportunity for AI to accelerate discovery.Although converting documents to markup is well-studied, extracting complex relationships remains challenging but essential for building knowledge graphs and fine-tuning datasets.</p>
<p>• Contextual Dependency: Relationships in scientific texts often depend heavily on context that may span multiple sentences or sections.For instance, a material's properties might be discussed concerning its synthesis method, as described in paragraphs several pages before.Traditional methods like MatKG [7], which define relationships by entity co-occurrence, often miss the nuances of scientific knowledge.While useful, they risk oversimplifying complex relationships.Advanced techniques are needed to better capture this complexity for improved knowledge extraction in AI-driven scientific discovery.Therefore, we introduce ByteScience, a cloud-based platform featuring an auto-finetuned LLM to extract structured scientific data and synthesize new scientific knowledge from extensive scientific corpora.We conclude as follows:</p>
<p>1) Tailored with DARWIN [8], an open-source state-of-the-art nature-science LLM, to provide research focus utilization; 2) Zero-code user-friendly semi-automated annotation and processing for uploaded science documents; 3) A personalized and domain-specific auto fine-tuning LLM that requires only a single fully annotated piece of literature; 4) Time efficiency high-quality science data extraction from millions of papers for less than a second per article.</p>
<p>II. PLATFORM DESIGN</p>
<p>ByteScience is a robust, scalable cloud-based solution leveraging AWS Sagemaker.This architecture ensures high availability, scalability, and performance for processing large scientific documents.Figure 1 illustrates the extraction pipeline for custom model development and data extraction.The pipeline consists of two primary phases:</p>
<p>• Initial Setup (First-time use for a specific field):</p>
<p>Dataset Construction (Green Pipeline): This phase builds a domain-specific corpus of structured scientific data.LLM Fine-tuning (Blue Pipeline): The system fine-tunes a large language model on the constructed dataset to optimize performance for the target scientific domain.• Operational Phase: Once the initial setup is complete, users can directly utilize the fine-tuned LLM stored in AWS to efficiently generate structured datasets from new scientific documents in the same field.This two-phase approach allows ByteScience to quickly adapt to various scientific domains while maintaining high extraction accuracy.The cloud-based architecture enables seamless scaling and ensures users always have access to the latest fine-tuned models, streamlining the conversion of unstructured scientific literature into structured data.Key steps include: 1) Create Database: Users upload scientific documents in JSON, PDF, HTML, or XML formats.Non-JSON text is extracted and saved as JSON, with HTML/XML markup stripped, and PDF conversion done using PDFMiner [9].2) Define Structure: Users define annotation structures, including entity labels and relationships, using pre-built or custom templates.3) Random Selection: A small text subset is randomly selected for initial annotation on first use.4) Auto Labelling: The LLM applies automatic pre-labeling to the selected texts.</p>
<p>III. ARCHITECTURE OF AWS CLOUD-BASED SERVICES</p>
<p>ByteScience utilizes the robust, scalable infrastructure of Amazon Web Services (AWS) to efficiently handle user requests and data processing.Figure 2 shows the detailed architecture of our platform.</p>
<p>A. General Service(Green Pipeline) Infrastructure</p>
<p>The user interaction layer is built on a series of AWS services that ensure high availability, security, and performance:</p>
<p>• DNS Management: AWS Route 53 routes incoming user requests to the appropriate services within the architecture.• Database Services: Amazon Relational Database Service (RDS) supports complex queries and transactions essential for scientific data management.</p>
<p>B. LLM Service (Blue Pipeline) Architecture</p>
<p>The LLM fine-tuning capability is a core function, implemented through a sophisticated pipeline of AWS services:</p>
<p>C. Workflow Integration</p>
<p>ByteScience workflow seamlessly integrates these components: 1) Users interact with the system through Route 53 and the ALB for initial annotation tasks.</p>
<p>2) The DARWIN LLM processes annotated data on a Sage-Maker Endpoint.This architecture enables ByteScience to offer customized, high-performance language models tailored to specific scientific domains, facilitating accurate and efficient structured data extraction from unstructured scientific literature.</p>
<p>IV. STRUCTURED DATA EXTRACTION PERFORMANCE</p>
<p>LLMs significantly improve human-in-the-loop annotation.Using 300 training samples reduced annotation time by 57% compared to a single sample [10].In the GPT-3/Doping-English model, 10-20 samples were enough to learn the correct structure, with precision, recall, and F1 scores reaching 0.8-0.9 with around 300 samples.</p>
<p>In our experiment, we compared non-LLM and LLM methods for structured data extraction on 90 samples covering batteries, catalysis, and photovoltaics, alongside ByteScience's results.As shown in Table I, we evaluated Named Entity Recognition (NER), Relation Extraction (RE), and Entity Resolution (ER).While models like MatBERT performed well, they often produced irrelevant entities, lowering precision.In contrast, LLMs handled unstructured information more reliably, and our system outperformed traditional methods across all tasks with fewer samples.A. Initial Setup: Schema, Semi-Annotation, Model Fine-Tune Thomas configures ByteScience to meet his research needs by designing a custom annotation schema for alloy synthesis, annotating key details like compositions, casting parameters, solution treatment, and aging variables.ByteScience then initiates semi-automatic annotation, where the DARWIN LLM auto-labels papers from his corpus based on this schema.Thomas reviews and corrects the annotations to refine the model's understanding.Afterward, ByteScience fine-tunes the LLM using AWS SageMaker, optimizing it for alloy synthesis data extraction.The fine-tuned model is deployed to a SageMaker Endpoint for efficient, large-scale processing of complex scientific papers.</p>
<p>B. Data Generation: Document Upload, Endpoint Utilization, and Dataset Creation</p>
<p>With the fine-tuned model, Thomas uploads his entire corpus of scientific papers to ByteScience, which processes various formats for comprehensive coverage.He initiates large-scale data extraction via the SageMaker Endpoint, where the model extracts detailed information on alloy compositions, casting processes, solution treatments, and aging procedures.This automation accelerates his research, completing in days what would have taken months manually.The extracted data is structured and stored in MongoDB, allowing Thomas to easily query, analyze, and identify trends in alloy synthesis, uncovering insights that manual review might have missed.</p>
<p>C. Further Dataset Updates and Refinement</p>
<p>As Thomas advances in his research, he updates his dataset with ByteScience, uploading new papers and processing them through the fine-tuned model to continually enrich his dataset.When discrepancies or improvements are needed, he initiates a re-training cycle, reviewing and correcting a subset of the newly processed papers to further fine-tune the model.This iterative process ensures the model stays accurate and adapts to evolving terminologies or methods in alloy synthesis.Through this dynamic interaction, Thomas maintains an up-to-date, accurate dataset, enhancing his research and keeping him at the forefront of alloy synthesis advancements.</p>
<p>VI. SIGNIFICANCE TO SCIENCE</p>
<p>Constructing databases from scholarly literature is crucial for modern research, but traditional methods are timeconsuming and resource-intensive.ByteScience transforms this process by enabling users to create a customized data extraction tool in hours, achieving 80%-90% human accuracy.It can process a 10-page scientific document in one second, compared to the 20-30 minutes it takes a researcher.With an extraction cost of just $0.023 per paper for 10,000 articles, ByteScience makes large-scale data extraction affordable and accessible.Its versatility across scientific fields democratizes access to advanced data extraction, providing computational power equivalent to hundreds of annotators.This accelerates discovery, enhances research decision-making, and fosters innovation across disciplines.</p>
<p>VII. CONCLUSION</p>
<p>ByteScience is leveraging a powerful approach to handle unstructured text by fine-tuning DARWIN, a pre-trained natural science LLM, using a minimal set of annotated articles.Hosted on the AWS cloud, this platform automates the process of extracting structured data from scientific texts, presenting a zero-code solution that could significantly enhance efficiency in natural science research.The key advantage of ByteScience lies in its ability to train the DARWIN model with few annotations, making it exceptionally adaptive and efficient.This capability ensures that the extracted material data is high-quality and highly accurate.ByteScience exemplifies how cutting-edge technology can be harnessed to propel advancements in science, engineering, and research by integrating advanced NLP techniques with cloud computing.This initiative represents a substantial step forward in making vast scientific corpora more accessible and usable, highlighting the transformative potential of AI in scientific data processing.To optimize resource efficiency, we are developing a slicing version that fine-tunes a low-resource inference model using only partial data from extensive content.APPENDIX Figure 3 shows the label-defining function in our system.Users define the structure for annotations, including entity labels and their relationships (visualized by indent).For each label, there is a definition textbox for filling.</p>
<p>Figure 4 shows the annotation function.Users can use automatic pre-labelling to the selected texts.Different colors will visualize the auto-labeled annotations and users can review and correct them, ensuring accuracy and consistency.</p>
<p>Figure 5 shows the data extraction function on example paper.Users can create a customized data extraction tool that achieves 80%-90% of human extraction accuracy after just a few hours of annotation.</p>
<p>911</p>
<p>Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.</p>
<p>Fig. 1 .
1
Fig.1.ByteScience Pipeline.The initial setup for a specific field involves constructing a domain-specific corpus of structured scientific data (Green Pipeline) and fine-tuning an LLM on this dataset to optimize performance for the target scientific domain (Blue Pipeline).Once this setup is complete, users can efficiently generate structured datasets from new scientific documents in the same field by utilizing the fine-tuned LLM stored in AWS.</p>
<p>5 )
5
Correction: Users review and correct the auto-labeled annotations, ensuring accuracy and consistency.6) Training: Corrected annotations are used to train or finetune an LLM, with training done via Amazon SageMaker.7) Fine-Tuned LLM: The training process results in a finetuned LLM customized for the specific annotation task.8) Structured Data Generation: The fine-tuned LLM processes new documents into structured data stored in Mon-goDB as JSON, allowing flexible use and efficient querying.After uploading the training dataset, it is transformed into the LLM's instruction format for fine-tuning.Users should first test a small text subset and assess accuracy and recall.If accuracy is low, add more annotated data and retrain.For low recall, generate more corpus data and use sequential learning to train a new model.</p>
<p>Fig. 2 .
2
Fig. 2. The architecture of ByteScience creates a structured database on AWS cloud with LLM.</p>
<p>3 )
3
The SageMaker Notebook is used for model development and improvement.4) Training datasets on S3 are used to fine-tune the model via SageMaker Training Jobs.5) The resulting model is deployed to a SageMaker Endpoint.6) Users can then perform data extraction tasks, processing requests by the fine-tuned LLM.</p>
<p>Fig. 3 .
3
Fig. 3. Screenshot of label setup.</p>
<p>Fig. 4 .
4
Fig. 4. Screenshot of labeling page.</p>
<p>Fig. 5 .
5
Fig. 5. Screenshot of extraction results of a paper.</p>
<p>TABLE I RESULT
I
OF STRUCTURED DATA EXTRACTION.</p>
<p>Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.
V. BYTESCIENCE IN ACTION: A USER CASE STUDYTo showcase ByteScience's application, we present Thomas, a materials scientist automating alloy synthesis by analyzing literature to establish "Composition-Processing-Structure-Performance" (CPSP) relationships.He designs alloy compositions, develops processing methods, and predicts microstructures using data on casting, solution treatment, and aging.
R Stevens, V Taylor, J Nichols, A B Maccabe, K Yelick, D Brown, AI for Science: Report on the Department of Energy (DOE) Town Halls on Artificial Intelligence (AI) for Science. Argonne, IL (United StatesFeb. 2020Tech. Rep. ANL-20/17</p>
<p>Applications of machine learning in drug discovery and development. J Vamathevan, D Clark, P Czodrowski, I Dunham, E Ferran, G Lee, B Li, A Madabhushi, P Shah, M Spitzer, S Zhao, Nature Reviews Drug Discovery. 186Jun. 2019Nature Publishing Group</p>
<p>Machine learning for functional protein design | Nature Biotechnology. </p>
<p>Opinion Mining by Convolutional Neural Networks for Maximizing Discoverability of Nanomaterials. T Xie, Y Wan, H Wang, I Østrøm, S Wang, M He, R Deng, X Wu, C Grazian, C Kit, B Hoex, 10.1021/acs.jcim.3c00746Journal of Chemical Information and Modeling. 647Apr. 2024</p>
<p>Commentary: The Materials Project: A materials genome approach to accelerating materials innovation. A Jain, S P Ong, G Hautier, W Chen, W D Richards, S Dacek, S Cholia, D Gunter, D Skinner, G Ceder, Others, APL materials. 112013AIP Publishing</p>
<p>The NOMAD laboratory: from data sharing to artificial intelligence. C Draxl, M Scheffler, Journal of Physics: Materials. 23360012019IOP Publishing</p>
<p>MatKG: The Largest Knowledge Graph in Materials Science -Entities, Relations, and Link Prediction through Graph Representation Learning. V Venugopal, S Pai, E Olivetti, arXiv:2210.17340Oct. 2022cond-mat</p>
<p>DARWIN Series: Domain Specific Large Language Models for Natural Science. T Xie, Y Wan, W Huang, Z Yin, Y Liu, S Wang, Q Linghu, C Kit, C Grazian, W Zhang, Others, arXiv:2308.135652023arXiv preprint</p>
<p>Pdfminer: Python pdf parser and analyzer. Y Shinyama, Retrieved on. 112015</p>
<p>Structured information extraction from complex scientific text with fine-tuned large language models. A Dunn, J Dagdelen, N Walker, S Lee, A S Rosen, G Ceder, K Persson, A Jain, arXiv:2212.052382022</p>            </div>
        </div>

    </div>
</body>
</html>
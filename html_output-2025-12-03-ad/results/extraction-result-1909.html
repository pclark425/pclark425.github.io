<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1909 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1909</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1909</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-282056360</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2510.09607v2.pdf" target="_blank">VITA-VLA: Efficiently Teaching Vision-Language Models to Act via Action Expert Distillation</a></p>
                <p><strong>Paper Abstract:</strong> Vision-Language Action (VLA) models significantly advance robotic manipulation by leveraging the strong perception capabilities of pretrained vision-language models (VLMs). By integrating action modules into these pretrained models, VLA methods exhibit improved generalization. However, training them from scratch is costly. In this work, we propose a simple yet effective distillation-based framework that equips VLMs with action-execution capability by transferring knowledge from pretrained small action models. Our architecture retains the original VLM structure, adding only an action token and a state encoder to incorporate physical inputs. To distill action knowledge, we adopt a two-stage training strategy. First, we perform lightweight alignment by mapping VLM hidden states into the action space of the small action model, enabling effective reuse of its pretrained action decoder and avoiding expensive pretraining. Second, we selectively fine-tune the language model, state encoder, and action modules, enabling the system to integrate multimodal inputs with precise action generation. Specifically, the action token provides the VLM with a direct handle for predicting future actions, while the state encoder allows the model to incorporate robot dynamics not captured by vision alone. This design yields substantial efficiency gains over training large VLA models from scratch. Compared with previous state-of-the-art methods, our method achieves 97.3% average success rate on LIBERO (11.8% improvement) and 93.5% on LIBERO-LONG (24.5% improvement). In real-world experiments across five manipulation tasks, our method consistently outperforms the teacher model, achieving 82.0% success rate (17% improvement), which demonstrate that action distillation effectively enables VLMs to generate precise actions while substantially reducing training costs.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1909.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1909.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VITA-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VITA-Vision-Language-Action (VITA-VLA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A distillation-based Vision-Language-Action model that equips a pretrained vision-language model (VITA-1.5-7B) with action-execution capability by two-stage knowledge distillation from a small action model, adding a state encoder and an action token while reusing the small model's action decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VITA-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Backbone: VITA-1.5-7B VLM (InternViT vision encoder + Qwen LLM connector). Adds a learnable action token (repeated for multi-step prediction), a lightweight state encoder (linear layers projecting robot state to token dimension), an action mapper (3-layer MLP) and reuses a pretrained two-layer MLP action decoder from a small action model. Processes images (two views), language instruction tokens, a single state token, and action tokens to produce 3 future continuous actions (6-DoF arm + gripper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining for the backbone (VITA-1.5), plus supervised imitation pretraining for the small action model teacher</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Backbone (VITA-1.5) was trained/fine-tuned on large-scale open-source multimodal/instructional data (diverse image-text and instruction-style corpora); the small action model teacher was trained on robot demonstration trajectories (supervised demonstrations with state-action pairs). The paper does not list specific web-scale datasets for the backbone beyond 'large-scale, diverse datasets'.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Language-conditioned robotic manipulation (simulation and real-world)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Long-horizon language-conditioned manipulation on CALVIN ABC-D (simulated, train on A/B/C test on D), LIBERO and LIBERO-LONG (simulated lifelong/long-horizon suites), and five real-world tasks on an ALOHA PiPer arm (pick/place/close/stack). Action space: continuous 7-D actions (6-DoF arm deltas + 1 gripper command); prediction horizon: 3 future steps per decision; inputs: wrist + base images, language instruction, robot state. Environments include simulated desk/cabinet/kitchen scenarios and a real robot tabletop setup.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>The paper argues good semantic alignment via reuse of a VLM trained on diverse image-text/instruction data and a small action model trained on the same demonstration domain; the authors highlight overlap in object and spatial concepts relevant to manipulation (visual grounding, instruction comprehension), but do not quantify overlap between backbone pretraining corpora and robot task vocabularies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Simulation: LIBERO average success rate 97.3% (reported as +11.8% over prior SOTA VLA), LIBERO-LONG 93.5% (+24.5% over previous best), CALVIN ABC-D first-task success 92.5% (+4.1% vs baseline). Real world: average success 82.0% across 5 real tasks (+17% over teacher Seer baseline). Metrics reported as task success rates (percent).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>The paper reports a 'freeze-vlm' experiment (keeping VLM parameters frozen while integrating pretrained action module weights) yielding only 45.3% first-task success on CALVIN ABC-D, indicating substantially worse performance when the VLM is not adapted; no direct random-init or pure vision-only pretrained ablation numbers are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>The method emphasizes efficiency: Stage-1 alignment updates ~30M parameters (only action tokens, state encoder, action mapper), and training used only language-conditioned data corresponding to 58% of the full CALVIN ABC-D training set. Compared to direct fine-tuning (only-finetune), two-stage yields gains (e.g., +6.5% on CALVIN task1 success, +1.0% average on LIBERO-LONG), implying better sample/optimization efficiency, but the paper does not give explicit counts of demonstrations needed to reach target performance.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No explicit attention-map visualizations or analysis are reported. The architecture uses an action token as a learnable query that attends to multimodal context, but there is no empirical analysis of attention focusing on semantically relevant regions.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No explicit analysis of embedding space geometry, clustering, or representational structure (e.g., PCA/UMAP) is provided beyond the representation-matching objective used in alignment (MSE between VLA and small-model hidden states).</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Behavioral evidence: improved execution on tasks requiring fine gripper control and positional precision (e.g., Pick Place Block, Stack Cups/Blocks) and higher real-world success rates compared to Seer suggest stronger language-to-action grounding; however, there is no direct mechanistic evidence (e.g., probing that links specific verbs to visual affordances or motor primitives).</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not explicitly analyzed. The paper argues that VLM's higher-level language/vision features improve instruction comprehension and long-horizon planning, but no layerwise or levelwise feature analyses (low-level vs high-level) are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Effective transfer is reported when: (1) the small action model and VLM share the same input modalities and temporal structure (both use 13-step trajectories), (2) the action mapper aligns hidden-state spaces, and (3) selective fine-tuning of the LLM, state encoder and action modules is allowed. Freezing the VLM severely degrades transfer. Domain similarity (same robotics dataset) and reuse of the small action model decoder are emphasized as enabling transfer efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>No explicit experiments that measure performance split by whether objects/actions appeared in backbone pretraining vs novel ones; reported benchmarks include generalization to unseen environment D (CALVIN) and long-horizon tasks in LIBERO, demonstrating robust generalization but not per-object novelty breakdown.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>The model demonstrates zero-shot generalization across unseen environment D in CALVIN when trained on A/B/C (reported strong performance), but no explicit few-shot adaptation curves or exact numbers of shots to reach performance are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Ablations include: (a) Stage-1 alignment trains only ~30M parameters (action tokens, state encoder, action mapper); (b) only-finetune (no alignment) performs worse than two-stage; (c) freeze-vlm (do not update VLM) leads to large performance drop (45.3% first-task success on CALVIN), indicating that fine-tuning VLM components is important for transfer. No per-layer importance ranking is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Freezing the VLM yields poor performance (45.3%) â€” evidence that naively reusing the VLM without adaptation constrains performance; authors also report sensitivity to environmental changes leading to lower average task success length in some CALVIN experiments, suggesting occasional negative effects on temporal robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Indirect comparisons: VITA-VLA outperforms smaller action models that rely on vision encoders like MAE or CLIP (e.g., Seer, Susie). The paper argues the VLM backbone brings improved instruction comprehension and grounding compared to smaller vision encoders, but no direct controlled comparison to pure vision-only pretraining (e.g., ImageNet-only) with matched capacity is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>The model predicts 3 future steps via repeated action token; experiments use 13-step trajectories. The paper does not present analyses of how representations change across fine-tuning epochs or early vs late learning phases beyond reporting final performances and that two-stage alignment improves later fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No measurement of effective dimensionality (PCA, intrinsic dimension) of representations is reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1909.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1909.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VITA-1.5-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VITA-1.5 7B-parameter Vision-Language Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter multimodal vision-language backbone (based on LLaVA architecture) used as the perceptual and language backbone for VITA-VLA, pretrained on large-scale diverse open-source multimodal and instruction-tuning data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VITA-1.5-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal backbone with InternViT vision encoder (InternViT-300M), a 3-layer connector MLP, and a Qwen-2.5-7B language model; fine-tuned for real-time vision and speech interaction tasks per cited VITA-1.5 work.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language pretraining with large-scale multimodal and instruction-style fine-tuning (vision+text/instruction-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Described as 'large-scale, diverse open-source data covering a wide range of scenarios' and instruction-like datasets; exact dataset names are not enumerated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Used as backbone for language-conditioned robotic manipulation (VITA-VLA)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Same as VITA-VLA: language-conditioned continuous-control robotic manipulation across CALVIN, LIBERO, and real-world ALOHA arm tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Authors state VITA-1.5 provides improved instruction comprehension and visual grounding, which helps alignment to manipulation tasks; no quantitative overlap statistics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>As part of VITA-VLA, backbone contributes to the reported high success rates (e.g., LIBERO 97.3%, CALVIN first-task 92.5%), but no ablation isolating backbone pretraining effects alone is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>No direct numbers; freeze-vlm and other ablations suggest backbone adaptation is necessary for best performance.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported specifically for backbone pretraining; backbone enables reuse of pretrained features which the paper claims reduces end-to-end pretraining cost.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>None reported for VITA-1.5 within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>None reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Indirect: improved task performance when using VITA-1.5 as backbone implies better grounding, but no direct probes.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not analyzed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Backbone fine-tuning (selective) is necessary for successful transfer; freezing reduces performance dramatically.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not evaluated specifically for the backbone alone.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Backbone contributes to zero-shot generalization to unseen environment D in CALVIN within the VITA-VLA system.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No per-layer probes presented here; selective fine-tuning of LLM layers is part of Stage 2.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No explicit negative transfer from backbone pretraining reported beyond the freezing experiment that shows poor performance when not adapted.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not directly compared; paper contrasts performance against smaller models using MAE/CLIP vision encoders and reports improved results.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not analyzed specifically for VITA-1.5 in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1909.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1909.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Seer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Seer (small action model / teacher)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small action model used as the teacher/expert for distillation; it uses MAE for vision encoding, CLIP text encoder for language, an MLP state encoder, and an action token as a learnable query decoded by an MLP action decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Seer (small action model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Small-capacity action model trained on robot demonstration trajectories: MAE visual encoder, CLIP text encoder, MLP state encoder, learnable action token as query, and a two-layer MLP action decoder producing continuous 6-DoF arm actions and gripper outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Supervised imitation learning on robot demonstration datasets; uses pretrained vision/text encoders (MAE, CLIP) for perception.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Training on robot demonstration trajectories and datasets appropriate for manipulation; used as teacher trained on same dataset D referenced in the paper (CALVIN/LIBERO/real-world demos).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Language-conditioned robotic manipulation (teacher policy)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Short- and long-horizon manipulation tasks in CALVIN and LIBERO; outputs continuous 7-D actions (6-DoF + gripper).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>High alignment because Seer is trained on the same demonstration domains as student; its features (MAE+CLIP) capture object and spatial concepts relevant to manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Seer-Large reported baselines: e.g., Seer-large achieves 87.7% average on LIBERO in a reported table; used as a baseline teacher in this paper (actual teacher performance numbers vary by dataset/variant).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not explicitly reported in this paper for Seer.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No explicit sample-efficiency curves comparing Seer to VLM-based models in the paper, though Seer is characterized as more sample-efficient on short tasks historically but less generalizable on long-horizon tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>None reported here (Seer internal analyses not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Seer is used as a strong action expert whose hidden states are used as targets during alignment; behavioral comparisons show the student (VITA-VLA) surpasses Seer in many long-horizon tasks, suggesting improved grounding when combining VLM features with the action decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Used as a teacher when trained on the same dataset as the student; success of distillation depends on alignment of hidden-state spaces and reuse of Seer's decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not described for Seer in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Seer's decoder is reused unchanged in Stage 2; hidden-state supervision from Seer is central to Stage 1 alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>While Seer is a strong short-task performer, it is said to lag behind VLM-augmented methods on long-horizon benchmarks; no explicit negative transfer quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Seer uses MAE/CLIP vision encoders (vision-pretrained components) and is compared behaviorally to VLM-augmented students; students outperform Seer on long-horizon tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Seer is trained on 13-step trajectories like the student; no further dynamics analysis provided.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1909.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1909.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 7B LLaMA-backed vision-language-action model that discretizes actions into tokens and performs end-to-end autoregressive training for robotic control (mentioned as related work/baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Discretization-based VLA: converts continuous actions into discrete tokens and maps vision+language to action tokens via an LLM (7B LLaMA backbone), detokenizing into executable actions; uses SigLIP and DINOv2 vision encoders in reported prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Backbone: language/LLM pretraining (LLaMA) and vision encoders pretrained separately; overall model trained end-to-end on large robot demonstration datasets in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Reported trained on ~970k episodes from Open X-Embodiment dataset in prior work (per paper's related-work summary); contains manipulation demonstrations and multimodal contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Language-conditioned robotic manipulation (generalist control)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Large-scale robot control across many episodes/demonstrations using discretized action tokenization; evaluated on manipulation benchmarks in cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Related-work discussion highlights discretization approaches may ignore robot state and so can miss physical-dynamics alignment; no detailed alignment analysis provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported as a baseline in comparisons; in the paper OpenVLA scores appear lower than VITA-VLA on listed benchmarks (e.g., LIBERO average results are lower), but exact numbers depend on cited prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Mentioned as requiring large datasets and expensive end-to-end training; no quantitative sample-efficiency figures provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Paper criticizes discretization-based methods like OpenVLA for often ignoring robot state and underutilizing continuous dynamics, implying weaker grounding; no direct evidence presented here.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Paper suggests discretization methods require large-scale end-to-end training and may transfer poorly when robot state dynamics matter.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not directly reported; however, authors describe limitations of discretization approaches that can hinder action accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not explicitly compared in this paper beyond qualitative critique.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1909.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1909.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT series / RT-1 / RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT series (RT-1, RT-2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Robotics Transformer line that maps vision and language to actions; RT-1 introduced discretization of actions with decoder-only Transformer; RT-2 extended to use larger backbones (PaLM-E) for end-to-end autoregressive training on teleoperated demonstrations (cited as related work/baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RT-1 / RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RT-1: discretization-based robotics transformer using image and language tokens with a decoder-only Transformer to generate discretized action tokens; RT-2: scales the idea using large pretrained LLM backbones (PaLM-E) and large datasets of human teleoperated demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>RT-1 uses pretrained image/text encoders; RT-2 uses large language+vision pretraining for backbone (PaLM-E), plus supervised demo training.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>RT-2 trained on ~130k human-teleoperated demonstrations collected over extended periods (per related-work summary); data contains explicit action trajectories and language annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Language-conditioned robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Real-world teleoperated demonstration learning for multi-task robot control; discretized action prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Described as leveraging large-scale web/robot demo data; paper implies discretization approaches may miss robot state dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>In prior work RT-2 showed strong real-world performance; this paper cites RT-series as influential baseline but does not re-report exact RT metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>RT approaches generally require large datasets; this paper uses them as a contrast to its more efficient distillation approach.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>RT-family demonstrates practical real-world control in prior work; in this paper they are cited as background rather than analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Large dataset scale and teleoperation coverage are key to RT transfer in prior work; authors argue distillation can reduce such requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not discussed here beyond general resource costs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>RT uses multimodal backbones rather than vision-only baselines; no direct comparison here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>RT models are autoregressive over actions; no temporal dynamics analysis provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1909.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1909.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GR00T</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GR00T</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diffusion-based VLA model that injects VLM-extracted vision-language features into an action expert for iterative denoising; cited as a related diffusion-based approach that treats the VLM largely as a feature extractor.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GR00T</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>VLM (Eagle-2) extracts vision-language features which are cross-attended into a diffusion transformer action expert; actions are generated via iterative denoising conditioned on state and noised actions.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Uses a pretrained VLM backbone combined with diffusion-style action modeling trained on large simulated trajectory datasets (reported as 780k simulated trajectories in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Large simulated trajectories spanning diverse robot tasks; vision-language pretraining for the backbone plus supervised diffusion training for action generation.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Language-conditioned robotic control via diffusion-based action generation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Multi-task simulated manipulation covering many trajectories; actions generated via denoising conditioned on vision-language features and state.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper notes diffusion designs may underutilize VLM for end-to-end action modeling, acting more as feature extractors; semantic alignment strengths/weaknesses depend on dataset scale.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>GR00T reported as a strong baseline in related work; this paper notes performance requires large datasets and compute.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Described as resource-intensive; no explicit sample-efficiency numbers given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not directly presented in this paper; diffusion-based models are characterized as relying on an action expert for low-level control rather than VLM grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Requires large-scale simulated data and joint training for strong performance; authors argue their distillation approach is a more efficient alternative.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not directly compared in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Diffusion approach models iterative refinement of actions; not further analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1909.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1909.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>pi 0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ï€ 0 (pi-zero)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language-action flow model that formulates action generation as a denoising/flow process combining a VLM backbone with an action expert, cited as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Ï€ 0</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Flow/denoising-based VLA combining a pretrained VLM (Gemma-2B in cited work) with a diffusion/action expert trained jointly across many dexterous tasks; treats action generation as conditional denoising of noisy actions using vision-language context.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Pretrained VLM + diffusion/flow training on many robotic tasks; multimodal pretraining for backbone plus supervised denoising training for actions.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Joint dataset of diverse dexterous tasks (68 tasks referenced in related work summary), containing action trajectories, object manipulations and task instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>General robot control via denoising/action flows</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Multi-task dexterous control across diverse robot embodiments; actions refined iteratively conditioned on vision-language features and state.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Cited as leveraging VLM features but often relegating the VLM to feature extraction with the action expert responsible for motor refinement; no explicit alignment analysis in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Cited as competitive in prior work; this paper positions pi0 as a related diffusion-based approach with trade-offs versus the distillation approach.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Described as requiring extensive data/compute; no specific sample-efficiency numbers provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Joint training across many tasks important; authors argue distillation can be more efficient for transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not directly compared.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Iterative denoising inherently temporal but not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1909.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1909.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>3D-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>3D-VLA (3D Vision-Language-Action)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3D vision-language-action generative world model that extends VLA concepts into 3D representations, cited as related work (mentioned as example of VLA applied to 3D environments).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>3D-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A generative 3D VLA that builds world models in 3D for vision-language-conditioned action generation (as described in cited prior work). In the paper it is cited as an example of VLA applied to 3D environments.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Multimodal 3D-aware pretraining / generative modeling (per cited work); exact pretraining details not given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Likely uses video/3D or multi-view datasets with object spatial relationships and dynamics, but the present paper does not enumerate the data.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Embodied 3D environment manipulation / VLA tasks in 3D worlds</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>3D environment action generation and planning; the paper only mentions 3D-VLA as part of related work without specific task descriptions here.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Mentioned in related work context; no quantitative alignment analysis in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>OpenVLA: An open-source vision-language-action model <em>(Rating: 2)</em></li>
                <li>RT-1: Robotics transformer for real-world control at scale <em>(Rating: 2)</em></li>
                <li>RT-2: Vision-language-action models transfer web knowledge to robotic control <em>(Rating: 2)</em></li>
                <li>pi 0 : A vision-language-action flow model for general robot control <em>(Rating: 2)</em></li>
                <li>Gr00t n1: An open foundation model for generalist humanoid robots <em>(Rating: 2)</em></li>
                <li>Predictive inverse dynamics models are scalable learners for robotic manipulation <em>(Rating: 2)</em></li>
                <li>3D-VLA: A 3d vision-language-action generative world model <em>(Rating: 2)</em></li>
                <li>Octo Model Team, An open-source generalist robot policy <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1909",
    "paper_id": "paper-282056360",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "VITA-VLA",
            "name_full": "VITA-Vision-Language-Action (VITA-VLA)",
            "brief_description": "A distillation-based Vision-Language-Action model that equips a pretrained vision-language model (VITA-1.5-7B) with action-execution capability by two-stage knowledge distillation from a small action model, adding a state encoder and an action token while reusing the small model's action decoder.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "VITA-VLA",
            "model_description": "Backbone: VITA-1.5-7B VLM (InternViT vision encoder + Qwen LLM connector). Adds a learnable action token (repeated for multi-step prediction), a lightweight state encoder (linear layers projecting robot state to token dimension), an action mapper (3-layer MLP) and reuses a pretrained two-layer MLP action decoder from a small action model. Processes images (two views), language instruction tokens, a single state token, and action tokens to produce 3 future continuous actions (6-DoF arm + gripper).",
            "pretraining_type": "vision-language pretraining for the backbone (VITA-1.5), plus supervised imitation pretraining for the small action model teacher",
            "pretraining_data_description": "Backbone (VITA-1.5) was trained/fine-tuned on large-scale open-source multimodal/instructional data (diverse image-text and instruction-style corpora); the small action model teacher was trained on robot demonstration trajectories (supervised demonstrations with state-action pairs). The paper does not list specific web-scale datasets for the backbone beyond 'large-scale, diverse datasets'.",
            "target_task_name": "Language-conditioned robotic manipulation (simulation and real-world)",
            "target_task_description": "Long-horizon language-conditioned manipulation on CALVIN ABC-D (simulated, train on A/B/C test on D), LIBERO and LIBERO-LONG (simulated lifelong/long-horizon suites), and five real-world tasks on an ALOHA PiPer arm (pick/place/close/stack). Action space: continuous 7-D actions (6-DoF arm deltas + 1 gripper command); prediction horizon: 3 future steps per decision; inputs: wrist + base images, language instruction, robot state. Environments include simulated desk/cabinet/kitchen scenarios and a real robot tabletop setup.",
            "semantic_alignment": "The paper argues good semantic alignment via reuse of a VLM trained on diverse image-text/instruction data and a small action model trained on the same demonstration domain; the authors highlight overlap in object and spatial concepts relevant to manipulation (visual grounding, instruction comprehension), but do not quantify overlap between backbone pretraining corpora and robot task vocabularies.",
            "performance_with_language_pretraining": "Simulation: LIBERO average success rate 97.3% (reported as +11.8% over prior SOTA VLA), LIBERO-LONG 93.5% (+24.5% over previous best), CALVIN ABC-D first-task success 92.5% (+4.1% vs baseline). Real world: average success 82.0% across 5 real tasks (+17% over teacher Seer baseline). Metrics reported as task success rates (percent).",
            "performance_without_language_pretraining": "The paper reports a 'freeze-vlm' experiment (keeping VLM parameters frozen while integrating pretrained action module weights) yielding only 45.3% first-task success on CALVIN ABC-D, indicating substantially worse performance when the VLM is not adapted; no direct random-init or pure vision-only pretrained ablation numbers are provided.",
            "sample_efficiency_comparison": "The method emphasizes efficiency: Stage-1 alignment updates ~30M parameters (only action tokens, state encoder, action mapper), and training used only language-conditioned data corresponding to 58% of the full CALVIN ABC-D training set. Compared to direct fine-tuning (only-finetune), two-stage yields gains (e.g., +6.5% on CALVIN task1 success, +1.0% average on LIBERO-LONG), implying better sample/optimization efficiency, but the paper does not give explicit counts of demonstrations needed to reach target performance.",
            "has_sample_efficiency_data": true,
            "attention_analysis": "No explicit attention-map visualizations or analysis are reported. The architecture uses an action token as a learnable query that attends to multimodal context, but there is no empirical analysis of attention focusing on semantically relevant regions.",
            "embedding_space_analysis": "No explicit analysis of embedding space geometry, clustering, or representational structure (e.g., PCA/UMAP) is provided beyond the representation-matching objective used in alignment (MSE between VLA and small-model hidden states).",
            "action_grounding_evidence": "Behavioral evidence: improved execution on tasks requiring fine gripper control and positional precision (e.g., Pick Place Block, Stack Cups/Blocks) and higher real-world success rates compared to Seer suggest stronger language-to-action grounding; however, there is no direct mechanistic evidence (e.g., probing that links specific verbs to visual affordances or motor primitives).",
            "hierarchical_features_evidence": "Not explicitly analyzed. The paper argues that VLM's higher-level language/vision features improve instruction comprehension and long-horizon planning, but no layerwise or levelwise feature analyses (low-level vs high-level) are reported.",
            "transfer_conditions": "Effective transfer is reported when: (1) the small action model and VLM share the same input modalities and temporal structure (both use 13-step trajectories), (2) the action mapper aligns hidden-state spaces, and (3) selective fine-tuning of the LLM, state encoder and action modules is allowed. Freezing the VLM severely degrades transfer. Domain similarity (same robotics dataset) and reuse of the small action model decoder are emphasized as enabling transfer efficiency.",
            "novel_vs_familiar_objects": "No explicit experiments that measure performance split by whether objects/actions appeared in backbone pretraining vs novel ones; reported benchmarks include generalization to unseen environment D (CALVIN) and long-horizon tasks in LIBERO, demonstrating robust generalization but not per-object novelty breakdown.",
            "zero_shot_or_few_shot": "The model demonstrates zero-shot generalization across unseen environment D in CALVIN when trained on A/B/C (reported strong performance), but no explicit few-shot adaptation curves or exact numbers of shots to reach performance are provided.",
            "layer_analysis": "Ablations include: (a) Stage-1 alignment trains only ~30M parameters (action tokens, state encoder, action mapper); (b) only-finetune (no alignment) performs worse than two-stage; (c) freeze-vlm (do not update VLM) leads to large performance drop (45.3% first-task success on CALVIN), indicating that fine-tuning VLM components is important for transfer. No per-layer importance ranking is provided.",
            "negative_transfer_evidence": "Freezing the VLM yields poor performance (45.3%) â€” evidence that naively reusing the VLM without adaptation constrains performance; authors also report sensitivity to environmental changes leading to lower average task success length in some CALVIN experiments, suggesting occasional negative effects on temporal robustness.",
            "comparison_to_vision_only": "Indirect comparisons: VITA-VLA outperforms smaller action models that rely on vision encoders like MAE or CLIP (e.g., Seer, Susie). The paper argues the VLM backbone brings improved instruction comprehension and grounding compared to smaller vision encoders, but no direct controlled comparison to pure vision-only pretraining (e.g., ImageNet-only) with matched capacity is reported.",
            "temporal_dynamics": "The model predicts 3 future steps via repeated action token; experiments use 13-step trajectories. The paper does not present analyses of how representations change across fine-tuning epochs or early vs late learning phases beyond reporting final performances and that two-stage alignment improves later fine-tuning.",
            "dimensionality_analysis": "No measurement of effective dimensionality (PCA, intrinsic dimension) of representations is reported.",
            "uuid": "e1909.0"
        },
        {
            "name_short": "VITA-1.5-7B",
            "name_full": "VITA-1.5 7B-parameter Vision-Language Model",
            "brief_description": "A 7B-parameter multimodal vision-language backbone (based on LLaVA architecture) used as the perceptual and language backbone for VITA-VLA, pretrained on large-scale diverse open-source multimodal and instruction-tuning data.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "VITA-1.5-7B",
            "model_description": "Multimodal backbone with InternViT vision encoder (InternViT-300M), a 3-layer connector MLP, and a Qwen-2.5-7B language model; fine-tuned for real-time vision and speech interaction tasks per cited VITA-1.5 work.",
            "pretraining_type": "Vision-language pretraining with large-scale multimodal and instruction-style fine-tuning (vision+text/instruction-tuning).",
            "pretraining_data_description": "Described as 'large-scale, diverse open-source data covering a wide range of scenarios' and instruction-like datasets; exact dataset names are not enumerated in this paper.",
            "target_task_name": "Used as backbone for language-conditioned robotic manipulation (VITA-VLA)",
            "target_task_description": "Same as VITA-VLA: language-conditioned continuous-control robotic manipulation across CALVIN, LIBERO, and real-world ALOHA arm tasks.",
            "semantic_alignment": "Authors state VITA-1.5 provides improved instruction comprehension and visual grounding, which helps alignment to manipulation tasks; no quantitative overlap statistics provided.",
            "performance_with_language_pretraining": "As part of VITA-VLA, backbone contributes to the reported high success rates (e.g., LIBERO 97.3%, CALVIN first-task 92.5%), but no ablation isolating backbone pretraining effects alone is provided.",
            "performance_without_language_pretraining": "No direct numbers; freeze-vlm and other ablations suggest backbone adaptation is necessary for best performance.",
            "sample_efficiency_comparison": "Not reported specifically for backbone pretraining; backbone enables reuse of pretrained features which the paper claims reduces end-to-end pretraining cost.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "None reported for VITA-1.5 within this paper.",
            "embedding_space_analysis": "None reported in this paper.",
            "action_grounding_evidence": "Indirect: improved task performance when using VITA-1.5 as backbone implies better grounding, but no direct probes.",
            "hierarchical_features_evidence": "Not analyzed in the paper.",
            "transfer_conditions": "Backbone fine-tuning (selective) is necessary for successful transfer; freezing reduces performance dramatically.",
            "novel_vs_familiar_objects": "Not evaluated specifically for the backbone alone.",
            "zero_shot_or_few_shot": "Backbone contributes to zero-shot generalization to unseen environment D in CALVIN within the VITA-VLA system.",
            "layer_analysis": "No per-layer probes presented here; selective fine-tuning of LLM layers is part of Stage 2.",
            "negative_transfer_evidence": "No explicit negative transfer from backbone pretraining reported beyond the freezing experiment that shows poor performance when not adapted.",
            "comparison_to_vision_only": "Not directly compared; paper contrasts performance against smaller models using MAE/CLIP vision encoders and reports improved results.",
            "temporal_dynamics": "Not analyzed specifically for VITA-1.5 in this paper.",
            "dimensionality_analysis": "Not provided.",
            "uuid": "e1909.1"
        },
        {
            "name_short": "Seer",
            "name_full": "Seer (small action model / teacher)",
            "brief_description": "A small action model used as the teacher/expert for distillation; it uses MAE for vision encoding, CLIP text encoder for language, an MLP state encoder, and an action token as a learnable query decoded by an MLP action decoder.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Seer (small action model)",
            "model_description": "Small-capacity action model trained on robot demonstration trajectories: MAE visual encoder, CLIP text encoder, MLP state encoder, learnable action token as query, and a two-layer MLP action decoder producing continuous 6-DoF arm actions and gripper outputs.",
            "pretraining_type": "Supervised imitation learning on robot demonstration datasets; uses pretrained vision/text encoders (MAE, CLIP) for perception.",
            "pretraining_data_description": "Training on robot demonstration trajectories and datasets appropriate for manipulation; used as teacher trained on same dataset D referenced in the paper (CALVIN/LIBERO/real-world demos).",
            "target_task_name": "Language-conditioned robotic manipulation (teacher policy)",
            "target_task_description": "Short- and long-horizon manipulation tasks in CALVIN and LIBERO; outputs continuous 7-D actions (6-DoF + gripper).",
            "semantic_alignment": "High alignment because Seer is trained on the same demonstration domains as student; its features (MAE+CLIP) capture object and spatial concepts relevant to manipulation.",
            "performance_with_language_pretraining": "Seer-Large reported baselines: e.g., Seer-large achieves 87.7% average on LIBERO in a reported table; used as a baseline teacher in this paper (actual teacher performance numbers vary by dataset/variant).",
            "performance_without_language_pretraining": "Not explicitly reported in this paper for Seer.",
            "sample_efficiency_comparison": "No explicit sample-efficiency curves comparing Seer to VLM-based models in the paper, though Seer is characterized as more sample-efficient on short tasks historically but less generalizable on long-horizon tasks.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "None reported here (Seer internal analyses not provided).",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Seer is used as a strong action expert whose hidden states are used as targets during alignment; behavioral comparisons show the student (VITA-VLA) surpasses Seer in many long-horizon tasks, suggesting improved grounding when combining VLM features with the action decoder.",
            "hierarchical_features_evidence": "Not analyzed.",
            "transfer_conditions": "Used as a teacher when trained on the same dataset as the student; success of distillation depends on alignment of hidden-state spaces and reuse of Seer's decoder.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not described for Seer in this paper.",
            "layer_analysis": "Seer's decoder is reused unchanged in Stage 2; hidden-state supervision from Seer is central to Stage 1 alignment.",
            "negative_transfer_evidence": "While Seer is a strong short-task performer, it is said to lag behind VLM-augmented methods on long-horizon benchmarks; no explicit negative transfer quantified.",
            "comparison_to_vision_only": "Seer uses MAE/CLIP vision encoders (vision-pretrained components) and is compared behaviorally to VLM-augmented students; students outperform Seer on long-horizon tasks.",
            "temporal_dynamics": "Seer is trained on 13-step trajectories like the student; no further dynamics analysis provided.",
            "dimensionality_analysis": "Not provided.",
            "uuid": "e1909.2"
        },
        {
            "name_short": "OpenVLA",
            "name_full": "OpenVLA",
            "brief_description": "An open-source 7B LLaMA-backed vision-language-action model that discretizes actions into tokens and performs end-to-end autoregressive training for robotic control (mentioned as related work/baseline).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "OpenVLA",
            "model_description": "Discretization-based VLA: converts continuous actions into discrete tokens and maps vision+language to action tokens via an LLM (7B LLaMA backbone), detokenizing into executable actions; uses SigLIP and DINOv2 vision encoders in reported prior work.",
            "pretraining_type": "Backbone: language/LLM pretraining (LLaMA) and vision encoders pretrained separately; overall model trained end-to-end on large robot demonstration datasets in prior work.",
            "pretraining_data_description": "Reported trained on ~970k episodes from Open X-Embodiment dataset in prior work (per paper's related-work summary); contains manipulation demonstrations and multimodal contexts.",
            "target_task_name": "Language-conditioned robotic manipulation (generalist control)",
            "target_task_description": "Large-scale robot control across many episodes/demonstrations using discretized action tokenization; evaluated on manipulation benchmarks in cited works.",
            "semantic_alignment": "Related-work discussion highlights discretization approaches may ignore robot state and so can miss physical-dynamics alignment; no detailed alignment analysis provided here.",
            "performance_with_language_pretraining": "Reported as a baseline in comparisons; in the paper OpenVLA scores appear lower than VITA-VLA on listed benchmarks (e.g., LIBERO average results are lower), but exact numbers depend on cited prior work.",
            "performance_without_language_pretraining": "",
            "sample_efficiency_comparison": "Mentioned as requiring large datasets and expensive end-to-end training; no quantitative sample-efficiency figures provided in this paper.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not discussed in this paper.",
            "embedding_space_analysis": "Not discussed here.",
            "action_grounding_evidence": "Paper criticizes discretization-based methods like OpenVLA for often ignoring robot state and underutilizing continuous dynamics, implying weaker grounding; no direct evidence presented here.",
            "hierarchical_features_evidence": "Not provided.",
            "transfer_conditions": "Paper suggests discretization methods require large-scale end-to-end training and may transfer poorly when robot state dynamics matter.",
            "novel_vs_familiar_objects": "Not reported here.",
            "zero_shot_or_few_shot": "Not discussed in this paper.",
            "layer_analysis": "Not provided.",
            "negative_transfer_evidence": "Not directly reported; however, authors describe limitations of discretization approaches that can hinder action accuracy.",
            "comparison_to_vision_only": "Not explicitly compared in this paper beyond qualitative critique.",
            "temporal_dynamics": "Not analyzed.",
            "dimensionality_analysis": "Not provided.",
            "uuid": "e1909.3"
        },
        {
            "name_short": "RT series / RT-1 / RT-2",
            "name_full": "RT series (RT-1, RT-2)",
            "brief_description": "Robotics Transformer line that maps vision and language to actions; RT-1 introduced discretization of actions with decoder-only Transformer; RT-2 extended to use larger backbones (PaLM-E) for end-to-end autoregressive training on teleoperated demonstrations (cited as related work/baseline).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "RT-1 / RT-2",
            "model_description": "RT-1: discretization-based robotics transformer using image and language tokens with a decoder-only Transformer to generate discretized action tokens; RT-2: scales the idea using large pretrained LLM backbones (PaLM-E) and large datasets of human teleoperated demonstrations.",
            "pretraining_type": "RT-1 uses pretrained image/text encoders; RT-2 uses large language+vision pretraining for backbone (PaLM-E), plus supervised demo training.",
            "pretraining_data_description": "RT-2 trained on ~130k human-teleoperated demonstrations collected over extended periods (per related-work summary); data contains explicit action trajectories and language annotations.",
            "target_task_name": "Language-conditioned robotic manipulation",
            "target_task_description": "Real-world teleoperated demonstration learning for multi-task robot control; discretized action prediction.",
            "semantic_alignment": "Described as leveraging large-scale web/robot demo data; paper implies discretization approaches may miss robot state dynamics.",
            "performance_with_language_pretraining": "In prior work RT-2 showed strong real-world performance; this paper cites RT-series as influential baseline but does not re-report exact RT metrics.",
            "performance_without_language_pretraining": "",
            "sample_efficiency_comparison": "RT approaches generally require large datasets; this paper uses them as a contrast to its more efficient distillation approach.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not in this paper.",
            "embedding_space_analysis": "Not in this paper.",
            "action_grounding_evidence": "RT-family demonstrates practical real-world control in prior work; in this paper they are cited as background rather than analyzed.",
            "hierarchical_features_evidence": "Not discussed here.",
            "transfer_conditions": "Large dataset scale and teleoperation coverage are key to RT transfer in prior work; authors argue distillation can reduce such requirements.",
            "novel_vs_familiar_objects": "Not provided here.",
            "zero_shot_or_few_shot": "Not analyzed here.",
            "layer_analysis": "Not analyzed in this paper.",
            "negative_transfer_evidence": "Not discussed here beyond general resource costs.",
            "comparison_to_vision_only": "RT uses multimodal backbones rather than vision-only baselines; no direct comparison here.",
            "temporal_dynamics": "RT models are autoregressive over actions; no temporal dynamics analysis provided here.",
            "dimensionality_analysis": "Not provided.",
            "uuid": "e1909.4"
        },
        {
            "name_short": "GR00T",
            "name_full": "GR00T",
            "brief_description": "A diffusion-based VLA model that injects VLM-extracted vision-language features into an action expert for iterative denoising; cited as a related diffusion-based approach that treats the VLM largely as a feature extractor.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GR00T",
            "model_description": "VLM (Eagle-2) extracts vision-language features which are cross-attended into a diffusion transformer action expert; actions are generated via iterative denoising conditioned on state and noised actions.",
            "pretraining_type": "Uses a pretrained VLM backbone combined with diffusion-style action modeling trained on large simulated trajectory datasets (reported as 780k simulated trajectories in related work).",
            "pretraining_data_description": "Large simulated trajectories spanning diverse robot tasks; vision-language pretraining for the backbone plus supervised diffusion training for action generation.",
            "target_task_name": "Language-conditioned robotic control via diffusion-based action generation",
            "target_task_description": "Multi-task simulated manipulation covering many trajectories; actions generated via denoising conditioned on vision-language features and state.",
            "semantic_alignment": "Paper notes diffusion designs may underutilize VLM for end-to-end action modeling, acting more as feature extractors; semantic alignment strengths/weaknesses depend on dataset scale.",
            "performance_with_language_pretraining": "GR00T reported as a strong baseline in related work; this paper notes performance requires large datasets and compute.",
            "performance_without_language_pretraining": "",
            "sample_efficiency_comparison": "Described as resource-intensive; no explicit sample-efficiency numbers given in this paper.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not discussed in this paper.",
            "embedding_space_analysis": "Not discussed here.",
            "action_grounding_evidence": "Not directly presented in this paper; diffusion-based models are characterized as relying on an action expert for low-level control rather than VLM grounding.",
            "hierarchical_features_evidence": "Not analyzed here.",
            "transfer_conditions": "Requires large-scale simulated data and joint training for strong performance; authors argue their distillation approach is a more efficient alternative.",
            "novel_vs_familiar_objects": "Not provided.",
            "zero_shot_or_few_shot": "Not analyzed in this paper.",
            "layer_analysis": "Not in this paper.",
            "negative_transfer_evidence": "Not reported here.",
            "comparison_to_vision_only": "Not directly compared in this paper.",
            "temporal_dynamics": "Diffusion approach models iterative refinement of actions; not further analyzed here.",
            "dimensionality_analysis": "Not provided.",
            "uuid": "e1909.5"
        },
        {
            "name_short": "pi 0",
            "name_full": "Ï€ 0 (pi-zero)",
            "brief_description": "A vision-language-action flow model that formulates action generation as a denoising/flow process combining a VLM backbone with an action expert, cited as related work.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Ï€ 0",
            "model_description": "Flow/denoising-based VLA combining a pretrained VLM (Gemma-2B in cited work) with a diffusion/action expert trained jointly across many dexterous tasks; treats action generation as conditional denoising of noisy actions using vision-language context.",
            "pretraining_type": "Pretrained VLM + diffusion/flow training on many robotic tasks; multimodal pretraining for backbone plus supervised denoising training for actions.",
            "pretraining_data_description": "Joint dataset of diverse dexterous tasks (68 tasks referenced in related work summary), containing action trajectories, object manipulations and task instructions.",
            "target_task_name": "General robot control via denoising/action flows",
            "target_task_description": "Multi-task dexterous control across diverse robot embodiments; actions refined iteratively conditioned on vision-language features and state.",
            "semantic_alignment": "Cited as leveraging VLM features but often relegating the VLM to feature extraction with the action expert responsible for motor refinement; no explicit alignment analysis in this paper.",
            "performance_with_language_pretraining": "Cited as competitive in prior work; this paper positions pi0 as a related diffusion-based approach with trade-offs versus the distillation approach.",
            "performance_without_language_pretraining": "",
            "sample_efficiency_comparison": "Described as requiring extensive data/compute; no specific sample-efficiency numbers provided here.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not in this paper.",
            "embedding_space_analysis": "Not in this paper.",
            "action_grounding_evidence": "Not analyzed in this paper.",
            "hierarchical_features_evidence": "Not analyzed.",
            "transfer_conditions": "Joint training across many tasks important; authors argue distillation can be more efficient for transfer.",
            "novel_vs_familiar_objects": "Not provided.",
            "zero_shot_or_few_shot": "Not discussed here.",
            "layer_analysis": "Not in this paper.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Not directly compared.",
            "temporal_dynamics": "Iterative denoising inherently temporal but not analyzed here.",
            "dimensionality_analysis": "Not provided.",
            "uuid": "e1909.6"
        },
        {
            "name_short": "3D-VLA",
            "name_full": "3D-VLA (3D Vision-Language-Action)",
            "brief_description": "A 3D vision-language-action generative world model that extends VLA concepts into 3D representations, cited as related work (mentioned as example of VLA applied to 3D environments).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "3D-VLA",
            "model_description": "A generative 3D VLA that builds world models in 3D for vision-language-conditioned action generation (as described in cited prior work). In the paper it is cited as an example of VLA applied to 3D environments.",
            "pretraining_type": "Multimodal 3D-aware pretraining / generative modeling (per cited work); exact pretraining details not given in this paper.",
            "pretraining_data_description": "Likely uses video/3D or multi-view datasets with object spatial relationships and dynamics, but the present paper does not enumerate the data.",
            "target_task_name": "Embodied 3D environment manipulation / VLA tasks in 3D worlds",
            "target_task_description": "3D environment action generation and planning; the paper only mentions 3D-VLA as part of related work without specific task descriptions here.",
            "semantic_alignment": "Mentioned in related work context; no quantitative alignment analysis in this paper.",
            "performance_with_language_pretraining": "Not reported in this paper.",
            "performance_without_language_pretraining": "",
            "sample_efficiency_comparison": "Not discussed here.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not provided in this paper.",
            "embedding_space_analysis": "Not provided.",
            "action_grounding_evidence": "Not discussed here.",
            "hierarchical_features_evidence": "Not provided.",
            "transfer_conditions": "Not discussed here.",
            "novel_vs_familiar_objects": "Not provided.",
            "zero_shot_or_few_shot": "Not provided.",
            "layer_analysis": "Not provided.",
            "negative_transfer_evidence": "Not provided.",
            "comparison_to_vision_only": "Not provided.",
            "temporal_dynamics": "Not provided.",
            "dimensionality_analysis": "Not provided.",
            "uuid": "e1909.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "OpenVLA: An open-source vision-language-action model",
            "rating": 2
        },
        {
            "paper_title": "RT-1: Robotics transformer for real-world control at scale",
            "rating": 2
        },
        {
            "paper_title": "RT-2: Vision-language-action models transfer web knowledge to robotic control",
            "rating": 2
        },
        {
            "paper_title": "pi 0 : A vision-language-action flow model for general robot control",
            "rating": 2
        },
        {
            "paper_title": "Gr00t n1: An open foundation model for generalist humanoid robots",
            "rating": 2
        },
        {
            "paper_title": "Predictive inverse dynamics models are scalable learners for robotic manipulation",
            "rating": 2
        },
        {
            "paper_title": "3D-VLA: A 3d vision-language-action generative world model",
            "rating": 2
        },
        {
            "paper_title": "Octo Model Team, An open-source generalist robot policy",
            "rating": 2
        }
    ],
    "cost": 0.020794,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>17 Oct 2025
17 Oct 2025F07BC582074D3BCD9F770A982AFE59A4arXiv:2510.09607v2[cs.CV]
the state encoder allows the model to incorporate robot dynamics not captured by vision alone (see Figure2).This design yields substantial efficiency gains over training large VLA models from scratch.Compared with previous state-of-the-art methods, our method achieves 97.3% average success rate on LIBERO (11.8% improvement), 93.5% on LIBERO-LONG (24.5% improvement), 92.5% first task success rate on CALVIN ABC-D (4.1% improvement).In real-world experiments across five manipulation tasks, our method consistently outperforms the teacher model Seer, achieving 82.0% average success rate (17% improvement).These results demonstrate that action distillation effectively enables VLMs to generate precise, executable actions while substantially reducing training costs.Our code is avaliable at https://ltbai.github.io/VITA-VLA/.</p>
<p>(1) Discretization-based methods map vision and language features into action tokens via LLM, but ignore robot state-an essential signal of physical dynamics-making action prediction less effective.(2) Diffusion-based approaches extract vision and language features with a VLM but pass them to a separate action expert for denoising, reducing the VLM to a large feature extractor and limiting its overall capability in action modeling.</p>
<p>(3) Our model distills knowledge from a small action model while largely preserving the VLM structure.By integrating robot state through a lightweight encoder and introducing an action token to fuse vision, language, and state, it enables the VLM to actively participate in action modeling rather than only serving as a feature extractor, thereby better leveraging its modeling capabilities.</p>
<p>ABSTRACT</p>
<p>Vision-Language Action (VLA) models significantly advance robotic manipulation by leveraging the strong perception capabilities of pretrained vision-language models (VLMs).By integrating action modules into these pretrained models, VLA methods exhibit improved generalization and robustness.However, training them end-to-end is costly, as modeling action distributions typically requires massive datasets and heavy computation.In this work, we propose a simple yet effective distillation-based framework that equips VLMs with action-execution capability by transferring knowledge from pretrained small action models.Our architecture retains the original VLM structure, adding only an action token and a state encoder to incorporate physical inputs, as illustrated in Figure 1.To distill action knowledge, we adopt a two-stage training strategy.First, we perform lightweight alignment by mapping VLM hidden states into the action space of the small action model, enabling effective reuse of its pretrained action decoder and avoiding expensive end-to-end pretraining.This also facilitates better transfer of action modeling capabilities to the VLM.Second, we selectively fine-tune the language model, state encoder, and action modules, enabling the system to integrate multimodal inputs with precise action generation.Specifically, the action token provides the VLM with a direct handle for predicting future actions, while</p>
<p>INTRODUCTION</p>
<p>Traditional small action models typically have limited parameters, are trained in fixed environments, and excel at executing simple predefined tasks (Wu et al., 2023;Black et al., 2023;Tian et al., 2024).However, these models often struggle to generalize to dynamic or perturbed environments, which restricts their effectiveness in real-world scenarios.In contrast, VLMs demonstrate remarkable visual comprehension and instruction-following capabilities, exhibiting strong generalization performance across a wide range of tasks.This motivates growing interest in combining VLMs with action models, leading to the development of various VLA models, such as the RT series (Brohan et al., 2022;Zitkovich et al., 2023), OpenVLA (Kim et al., 2024), GR00T (Bjorck et al., 2025), Ï€ 0 (Black et al., 2024), Octo (Team et al., 2024), 3D-VLA (Zhen et al., 2024), and others (Cui et al., 2025;Zhao et al., 2025;Qu et al., 2025).These models can be broadly categorized into two main approaches, as illustrated in Figure 1.The first category, exemplified by OpenVLA and the RT series, adopts a discretization-based approach.These models transform continuous actions into discrete tokens by partitioning the action space into fixed intervals.Visual and language features are then mapped to these action tokens via a Large Language Model (LLM), which are subsequently detokenized into executable actions.The second category, represented by GR00T and Ï€ 0 , follows a diffusion-based design.Vision-language features are first extracted by a pretrained VLM and then injected into the action expert through attention mechanisms.The action expert then iteratively refines the noised action representations conditioned on the current state, noised actions, and vision-language features, to generate the final executable actions.While both categories have demonstrated promising performance, they exhibit notable limitations.Discretization-based approaches often omit robot state information-a crucial signal for modeling physical dynamics-which can hinder the accuracy of action prediction.Meanwhile, diffusion-based methods typically leverage the VLM solely as a feature extractor, reducing it to a static encoder and underutilizing its potential for end-to-end action modeling.Furthermore, despite being trained with extensive computational resources and largescale, high-quality data, they still lag behind smaller, task-specific models on embodied benchmarks like CALVIN (Mees et al., 2022) and LIBERO (Liu et al., 2023a).</p>
<p>To address these limitations, we propose a new VLA architecture and a two-stage training framework that equips pretrained VLMs with action-generation capability via knowledge distillation from small action models(Figure 1 (3)).This approach removes the need for expensive end-to-end pretraining on large embodied datasets and achieves strong performance across simulation benchmarks and real-world experiment.Our method is built upon the VITA-1.5 architecture, which is based on the well-known and widely-used LLaVA architecture (Liu et al., 2023b).We extend this backbone with two components-a state encoder and an action token-so that it can fuse visual, language, and state inputs to generate executable robot actions, as illustrated in Figure 2. To efficiently equip the VLM with action capabilities, we introduce a simple yet effective two-stage training framework centered on knowledge distillation, as illustrated in Figure 3.In the first stage, we perform lightweight alignment by projecting the VLM's hidden features into the action space of a pretrained small action model.This alignment process explicitly distills the action-generation policy from the small action model into the VLM.By doing so, we can reuse the action decoder of the small action model, avoiding the need for costly end-to-end pretraining from scratch.In the second stage, we finetune specific components of the system, including the language model, state encoder, and action modules.This selective fine-tuning helps to better integrate multimodal signals, leading to accurate action predictions.This two-stage approach not only enhances the VLM's ability to model complex robotic behaviors but also reduces training cost, while retaining strong generalization.The visual and textual information is input into the VLM.The action token acts as a learnable query, while the robot state is encoded into a single token using linear layers.An action mapper extracts the hidden states of the action token from the final layer of the VLM, and transforms these to match the dimensionality expected by the pretrained action decoder, and finally the action decoder generates the corresponding actions with 7 degrees of freedom (DoF).</p>
<p>We validate the effectiveness of our approach on both simulation benchmarks and real-world robotic tasks.On the LIBERO benchmark, our two-stage training strategy achieves a 97.3% average success rate across all task suites, outperforming the previous state-of-the-art VLA method by 11.8%.Notably, on the challenging LIBERO-LONG benchmark (Liu et al., 2023a), our method achieves a 93.5% success rate, surpassing the small action model Seer (Tian et al., 2024)</p>
<p>RELATED WORK</p>
<p>Small Action Models.Small action models in language-conditioned robot manipulation typically adopt lightweight architectures with limited parameters, making them suitable for relatively simple tasks.These models often use a pretrained CLIP (Radford et al., 2021) text encoder to process language input and vision encoders such as CLIP or SigLIP (Zhai et al., 2023) to extract image features.In addition, a dedicated state encoder processes the robot states, and the combined multimodal inputs are passed to a high-capacity policy network to predict the corresponding actions.RT-1 (Brohan et al., 2022) uses the Universal Sentence Encoder to process text and a pretrained EfficientNet-B3 (Tan &amp; Le, 2019) to encode images.The action information is discretized into 256 bins, and the image and text tokens are concatenated and fed into an 8-layer decoder-only Transformer, which generates action tokens end-to-end.Seer (Tian et al., 2024) uses MAE (He et al., 2022) to process visual input, the CLIP text encoder for text processing, and an MLP to encode state information.The action token is treated as a learnable query, which is decoded to generate action.RDT-1b (Liu et al., 2024) employs T5 as the text tokenizer, SigLIP as the image encoder, and a Diffusion Transformer as the core architecture.It generates actions through an iterative denoising process.Although these models demonstrate strong performance on short instruction-following tasks, they often struggle to generalize in complex or long-horizon tasks.This limitation arises primarily from the restricted capacity of their text and vision encoders.To address this, our method adopts the powerful VITA-1.5-7B(Fu et al., 2025) as backbone, a 7B-parameter VLM trained on large-scale, diverse datasets.This foundation enables significantly improved instruction comprehension, visual grounding, and long-horizon planning.This makes our approach more scalable, generalizable, and robust for robotic manipulation across varied environments.</p>
<p>Large Vision-Language Action Models.Recent progress in VLMs (Liu et al., 2023b;Achiam et al., 2023;Team et al., 2023;Bai et al., 2023) has advanced the development of VLA models for robotic control.RT-2 (Zitkovich et al., 2023) is the first to convert actions into discrete tokens and perform end-to-end autoregressive training using PaLM-E (12B) as the backbone.It is trained on 130k human-teleoperated demonstrations collected over 17 months.OpenVLA (Kim et al., 2024) extends this approach with an open-source 7B LLaMA model, using SigLIP and DINOv2 as vision encoders and trained on 970k episodes from the Open X-Embodiment dataset.Different from above, Ï€ 0 (Black et al., 2024) formulates action generation as a denoising process.It combines Gemma-2B as the VLM with a diffusion-based action expert, jointly trained over 68 dexterous tasks from diverse robot embodiments.GR00T (Bjorck et al., 2025) uses Eagle-2 (1.34B) as the VLM backbone and injects its vision-language features into a diffusion transformer via cross-attention, generating actions conditioned on the current state and noised actions through denoising.The entire model is trained on a dataset of 780k simulated trajectories.Despite their scale, these models require large datasets, long training times, and significant computational resources.In contrast, our approach provides a more efficient and scalable alternative.Instead of training a full VLA model from scratch, we introduce a two-stage distillation framework.In the first stage, we explicitly align the action representation spaces of a pretrained VLM and a small action model through lightweight representation matching, updating only a small subset of parameters to reduce computational cost.</p>
<p>In the second stage, we directly attach the pretrained action decoder from the small action model to the VLM, forming a new VLA system.We then fine-tune the language model, state encoder, and the entire action module-including the action mapper and decoder-allowing the model to integrate the advanced capabilities of the VLM with the low-level control precision of the small action model, while remaining more efficient than training a VLA model from scratch.</p>
<p>METHOD</p>
<p>PROBLEM FORMULATION</p>
<p>We consider a robotic manipulation task formulated as a Markov Decision Process.Given a dataset
D = {(x i , l i , s i , a i )} N i=1
of N demonstrations, each tuple consists of visual observation x i âˆˆ X , language instruction l i âˆˆ L, robot state s i âˆˆ S âŠ† R ds=7 (6 degrees of freedom (DoF) arm state and 1 dimension gripper width), and a corresponding action a i âˆˆ A âŠ† R da=7 (comprising a arm i for the 6-DoF arm action and 1 dimension a grip i for the gripper width).Our objective is to learn a policy Ï€ Î¸ : X Ã— L Ã— S â†’ A, that predicts executable actions Ã¢ = Ï€ Î¸ (x, l, s) conditioned on multimodal inputs.</p>
<p>We further assume access to a pretrained small action model Ï€ expert : X Ã— L Ã— S â†’ A that has been trained on the same dataset D, which demonstrates strong performance on robotic manipulation tasks.Our goal is to distill the action modeling capability of Ï€ expert into a vision-language model f Ï• , creating a unified VLA model that combines the visual-linguistic understanding and generalizability of VLMs with the precise action prediction of specialized action models.</p>
<p>OVERALL ARCHITECTURE</p>
<p>The overall architecture of our model is illustrated in Figure 2. Our architecture extends the pretrained VITA-1.5-7Bmodel with minimal modifications to enable action prediction while preserving its vision-language capabilities.The backbone consists of three primary components: a vision encoder (InternViT-300M), a connector (3-layer MLP), and a language model (Qwen-2.5-7B).This backbone has been fine-tuned on large-scale open-source data covering a wide range of scenarios, demonstrating strong capabilities in image understanding and complex instruction following.State Input.To incorporate robot state information, we initially explore concatenating raw state values as text tokens, but this approach fails because numerical values are poorly represented in the VLM's token vocabulary and the model struggles to interpret these values accurately.Thus, we design a dedicated state encoder.Specifically, the 6-DoF arm state and the 2-dimensional gripper state (one-hot encoding of the 1 dimension gripper width) are first encoded separately by two linear layers.Their outputs are then concatenated and passed through an additional linear layer, which projects the combined representation into the same dimension as the text tokens.This design allows the model to effectively integrate and attend to structured state information.Action Token.We define a learnable action token that acts as a query during training and inference.This token is appended to the input sequence and is responsible for attending to the multimodal  context to produce action representations.To predict three future steps, the same token is repeated three times.Since consecutive actions are highly correlated and usually differ minimally, a single shared token is sufficient to capture temporal continuity.We also observed that assigning independent action tokens to each step does not yield additional benefits in our setting.Input.Each training sample comprises 13 time steps.This setting follows the small action model, which is trained on 13-step trajectories, ensuring consistent temporal structure during alignment.At each step, the model receives a structured multimodal sequence: image tokens from two views (static and wrist cameras), text tokens (instruction), one state token, and three action tokens.Each image (200 Ã— 200) is encoded into 49 visual tokens, yielding n = 98 image tokens per step.Let m be the number of instruction tokens; the complete per-step input is:
[img 1 ] . . . [img n ] [text 1 ] . . . [text m ] [state] [act 1 ] [act 2 ] [act 3 ].
(1) Formally, we denote the complete set of inputs as x (image tokens), t (text tokens), s (state token), and a q (action token).The model Ï€ jointly processes {x, t, s, a q } and the action embeddings are obtained by extracting the final-layer hidden states of a q : a h = Ï€ last (a q | x, t, s).</p>
<p>(2)</p>
<p>Action Mapper, Decoder and Output.We employ a lightweight three-layer MLP as the action mapper M to project a h into the input space expected by the pretrained action decoder.The first layer transforms the feature dimension of a h to match the action space.The other two layers keep the same size and introduce sufficient nonlinearity.This architecture strikes a balance between model expressiveness and computational efficiency.The mapped features are then fed into the pretrained action decoder D, which is a fixed two-layer MLP reused from the small action model.After the first-stage alignment, the output space of the action mapper becomes well aligned with the decoder's expected input space, enabling effective integration.Together, the action mapper and decoder generate the final executable action Ã¢ = D(M (a h )).</p>
<p>TRAINING STRATEGY</p>
<p>To equip our VLA model with action execution capabilities, we introduce a two-stage training strategy.The core idea is to transfer the action modeling abilities from a small action model to the VLA through alignment.In the first stage, we employ lightweight alignment to bridge the gap between the action representation spaces of the VLA and the small action model, enabling the reuse of the pretrained action decoder from the small action model.In the second stage, end-to-end fine-tuning is conducted to further enhance the VLA model's action modeling capabilities.This approach reduces training resources while maintaining high performance in action generation.</p>
<p>STAGE 1: ALIGNMENT</p>
<p>The first stage, illustrated on the left side of Figure 3, aims to align the hidden action representations between the VLA model and the small action model.Except for the learnable action query, both models receive identical inputs, including the images, text instruction, and robot state.Since both models follow an autoregressive architecture, we extract the last-layer hidden states corresponding to the action tokens from each model for alignment.Due to the difference in feature dimensions, we use an action mapper to transform the dimension of the VLA's action token hidden states into that of the small action model.We then compute a mean squared error (MSE) loss between the mapped VLA hidden states and the corresponding hidden states from the small action model:
L align = 1 N N i=1 M (a VLA,i h ) âˆ’ a Small,i h 2 2 ,(3)
where M (â€¢) denotes the action mapper, N is the number of action tokens, a VLA h is the VLA model's last-layer hidden state of the action token, and a Small h is the corresponding hidden state of the small action model.In this stage, only the state encoder, action tokens, and action mapper are trained, comprising a total of approximately 30 million parameters.This lightweight configuration enables efficient and fast alignment while preparing the model for the subsequent end-to-end fine-tuning.STAGE 2: END-TO-END FINE-TUNING After the alignment stage, the VLA model is better positioned for end-to-end fine-tuning.We continue using the same data as in Stage 1-comprising action tokens, images, text instructions, and robot states, as illustrated on the right side of Figure 3.These inputs are processed by the VLA model to produce the last-layer hidden states corresponding to the action tokens.</p>
<p>As in Stage 1, we apply the pretrained action mapper to project hidden states into the action space of the small action model.We reuse the pretrained action decoder and linear projection heads from the small action model to generate executable actions.The decoder is a two-layer MLP, followed by linear layers for predicting 6-DoF arm actions and binary gripper actions.Arm actions are supervised with mean absolute error (MAE) loss, and gripper actions with binary cross-entropy (BCE) loss.The total loss is their weighted sum: L total = L arm + Î» â€¢ L gripper where:
â€¢ L arm = 1 T T t=1 âˆ¥Ã¢ arm t âˆ’ a arm
t âˆ¥ 1 is the MAE loss for the 6-DoF arm action, computed across all predicted time steps T .Here, Ã¢arm t âˆˆ R 6 denotes the predicted continuous arm action, and a arm t âˆˆ R 6 denotes the ground truth.
â€¢ L gripper = âˆ’ 1 T T t=1 a grip t log(Ã¢ grip t ) + (1 âˆ’ a grip t ) log(1 âˆ’ Ã¢grip t )
is the BCE loss for the gripper action, where Ã¢grip t âˆˆ [0, 1] denotes the predicted probability of the gripper being closed, and a grip t âˆˆ {0, 1} denotes the ground truth label.</p>
<p>â€¢ Î» = 0.01 is a scaling factor set according to the observed loss magnitudes, ensuring that the gripper loss and arm loss contribute comparably during training.</p>
<p>This combined loss guides the model to learn accurate continuous arm actions and reliable binary gripper actions, maintaining balance between the two goals.In this stage, we fine-tune the LLM, state encoder, learnable action queries, action mapper, and action decoder, enabling end-to-end integration of multimodal information for accurate action prediction.</p>
<p>Choice of MSE and MAE.We adopt different loss functions in the two stages to suit their distinct goals.During alignment, MSE penalizes large deviations between VLM and small model hidden states, making it suitable for representation matching.In fine-tuning, the objective shifts to predicting continuous control signals.We use MAE for 6-DoF arm actions, as it yields more stable optimization and is less sensitive to outliers.This combination balances representation alignment with reliable low-level action supervision.</p>
<p>EXPERIMENTS</p>
<p>To validate the effectiveness of our model architecture and training strategy, we conduct simulation experiments on the CALVIN ABC-D and LIBERO benchmarks, as well as real-world experiments using a robotic arm platform.Our evaluation aims to address the following three questions: 1) Can this architecture perform well across various environments and tasks?2) Does the proposed distillation process lead to measurable performance gains?3) Can the model be effectively deployed on real-world robotic platforms?</p>
<p>Table 1: CALVIN ABC-D results.We report the success rates computed over 1000 rollouts for each task, along with the average number of completed tasks required to solve five instructions consecutively (denoted as Avg.Len.).The results for small action models without a VLM are displayed in the first three rows.Benchmarks.We evaluate our model on two robotic manipulation benchmarks: CALVIN and LIBERO.CALVIN is a simulated benchmark comprising 34 tasks and 1,000 language instructions across four environments (A-D), each with different desk colors and object layouts.Following the ABC-D setting, models are trained on environments A, B, and C, and tested on the unseen environment D to assess generalization.LIBERO is a comprehensive lifelong learning benchmark with four task suites-Spatial, Object, Goal, and LONG-each suite contains 10 long-horizon tasks, evaluating different aspects of generalization in robotic manipulation.</p>
<p>Baselines.We compare our method with a diverse set of representative VLA baselines.For LIBERO, we include large-scale pretrained generalist models (OpenVLA (Kim et al., 2024), Octo (Team et al., 2024)) as well as architectures with enhanced grounding or reasoning capabilities, such as SpatialVLA (spatial information) (Qu et al., 2025), CoT-VLA (visual chain-ofthought) (Zhao et al., 2025), and Ï€ 0 -Fast (flow-based method) (Black et al., 2024).For CALVIN ABC-D, we evaluate both small action models-Susie (diffusion-based subgoal planning) (Black et al., 2023) and GR-1 (video pretraining) (Wu et al., 2023)-and VLA-based methods including Roboflamingo (Li et al., 2023), 3D-VLA (Zhen et al., 2024), and OpenVLA.Seer (Tian et al., 2024) is used in both benchmarks and also serves as the distillation teacher.</p>
<p>Experiment Details.Our experiments involve three distinct training settings: 1) Two-stage: a two-stage training strategy, where the model is first aligned and then fine-tuned.2) Only-finetune: no alignment stage is performed.Instead, we directly attach the pretrained action module from the small action model to our VLM, and then perform only the fine-tuning procedure from the two-stage protocol to train the combined model.3) Freeze-vlm: directly integrate the pretrained action token, state encoder, and action module weights (from the first strategy) into the VLM, but do not update the VLM parameters during training, to test whether a strong VLA could be achieved without tuning the VLM.In all experiments, we use only language-conditioned data, corresponding to 58% of the full training set in CALVIN ABC-D.Additional hyperparameter settings are provided in Appendix A.1.</p>
<p>MAIN RESULTS</p>
<p>Zero-Shot Generalization to Unseen Environments.Table 1 presents the results on the CALVIN ABC-D benchmark.Our model achieves the highest performance among existing VLA models when trained on environments A, B, and C and evaluated in the unseen environment D, demonstrating strong generalization capability to unseen scenes.Although our model achieves a higher success rate than Seer-Large on Task 1 (92.5% vs. 88.4%), it yields a lower average task success length.We attribute this to the model's sensitivity to environmental changes during task transitions, which may lead the VLM to misinterpret context or lose consistency across subtasks.This highlights a potential area for improving temporal robustness in long-horizon manipulation.</p>
<p>Long-Horizon Planning and Complex Instruction Execution.As shown in Table 3, our model achieves a 5.8% improvement in average success rate on the LIBERO-LONG benchmark compared to the Seer-Large model.We attribute this gain to the integration of a VLM, which improves the Table 2: LIBERO results of different VLA models.We present the success rates of various VLA models.To ensure fair comparison, we report the average success rates over 500 episodes, following the evaluation protocol used in CoT-VLA (Zhao et al., 2025).The best result in each category is highlighted in bold.Our model achieves state-of-the-art performance across all tasks, demonstrating exceptional long-horizon execution capabilities.Notably, it raises the average success rate to 97.3%, in the LIBERO-LONG task, it improves the previous state-of-the-art by 24.5%.model's ability to understand and execute complex instructions and to more effectively process multimodal (language, visual, states) inputs.Furthermore, as shown in Table 2, our model outperforms all other VLA models on the LIBERO benchmark.Specifically, compared with the previous best success rate of 69.0%, our model improves by 24.5% on LIBERO-LONG and achieves an overall average success rate increase of 11.8% across all tasks.</p>
<p>Effectiveness of the Two-Stage Training Strategy.As shown in Tables 1 and 3, the two-stage trained model consistently outperforms the only-finetune baseline, achieving a 6.5% increase in task 1 success rate on the CALVIN ABC-D benchmark and a 1% improvement in the average success rate on the LIBERO-LONG benchmark.These results clearly demonstrate the effectiveness of the proposed two-stage training framework, further confirming that aligning the action representation spaces prior to fine-tuning leads to superior performance in robotic manipulation tasks.Direct Integration of Action Module Weights with Frozen VLM.In this approach, we directly incorporate the pretrained action token, state encoder, and action module weights obtained from the two-stage training pipeline into the original VLM, while keeping the VLM parameters frozen during end-to-end fine-tuning.This setup aims to assess whether a robust VLA model can be achieved solely by adapting the action module, without updating the VLM.Evaluation on the CALVIN ABC-D benchmark reveals a first-task success rate of only 45.3%, indicating that freezing the VLM component significantly constrains overall performance.These results highlight the necessity of fine-tuning the VLM to equip it with action-execution capabilities.</p>
<p>REAL-WORLD EVALUATION</p>
<p>Real World Settings.We design five real-world tasks to comprehensively evaluate the model's capabilities, covering four canonical robotic operations: Pick, Place, Close, and Stack.Our realworld experiments are conducted on the ALOHA platform, where the arm is precisely controlled by six joint angles and the gripper is controlled through its opening width.To enable a fair evaluation, we manually collect 500 high-quality demonstration trajectories (100 per task) across a wide range of scenarios.Both the Seer model and the proposed model are trained on this dataset, with the latter employing two distinct training strategies for systematic comparison.For evaluation, we report the  Detailed Results.The detailed experimental results are summarized in Table 4.Our two-stage model achieves the best performance across all tasks, surpassing both the Seer model and the finetuned baseline, which demonstrates the effectiveness of our strategy in real-world settings.For relatively short and simple tasks such as Close Drawer, all three models achieve comparable and satisfactory performance.However, in longer-horizon tasks such as Pick and Place, our model exhibits clear advantages.In particular, in the Pick Place Block task, accurate prediction of the gripper's opening width is crucial for successful execution, and our model demonstrates superior precision compared with the baselines.For more complex tasks such as Stack Cups and Stack Blocks, success requires both predicting fine-grained gripper widths and accurately identifying the correct opening positions to achieve stable stacking.These tasks further demand real-time perception of positional changes and a stronger understanding of visual information, where our VLA model significantly outperforms the Seer model.Additional implementation details are provided in Appendix A.2, and the real-world deployment results are presented in Appendix A.4. Limitation and Future Work.Despite its simplicity and effectiveness, VITA-VLA depends on pretrained models, which constrains its application in domains lacking appropriate action experts.Additionally, it exhibits slower inference compared to small models.Future efforts will aim to alleviate these limitations by enhancing efficiency and reducing reliance on pretrained models.Our real-world robotic platform is illustrated in Fig. 5.The setup consists of two cameras: a basemounted Intel RealSense D435i RGB-D camera with a resolution of 1280Ã—720, and a grippermounted Dabai DCW depth camera with a resolution of 640Ã—480, providing complementary viewpoints for perception.The robot itself is a PiPer arm with six actuated joints, controlled in radians, equipped with a Songling parallel gripper whose opening width is directly commanded for grasping.This combination allows both global scene observation and fine-grained local perception at the end-effector, facilitating precise manipulation.Demonstration data were collected via teleoperation, and the same hardware was used for inference.The platform is powered by a workstation with a single GPU, on which our model runs at approximately 0.15s per inference step (about 6-7 Hz).For comparison, the Seer model achieves about 0.05s per inference (roughly 20 Hz), highlighting a trade-off between inference speed and action accuracy.</p>
<p>A.3 DETAILED RESULTS OF LIBERO EXPERIMENTS</p>
<p>We evaluate on all ten tasks from the LIBERO-LONG benchmark, the result is like Table 3:  We evaluate our model on the LIBERO benchmark.To present the results more clearly and intuitively, we sample evaluation data and visualize the process on LIBERO, as illustrated in Figure 6.</p>
<p>We also report the success rates for different tasks across various benchmarks, as shown in Table 7.The results show that our model performs consistently well across all tasks, demonstrating its effectiveness in handling long-horizon and complex tasks.</p>
<p>A.4 REAL-WORLD DEPLOYMENT</p>
<p>We further validate our method by deploying the model on the Aloha robotic arm in real-world scenarios.Specifically, we evaluate it across five distinct manipulation tasks, which can be categorized into four fundamental operation types-pick, place, stack, and close-all of which require stable and precise action outputs for successful completion.As shown in Fig. 7, our model achieves high success rates and stable performance on all tasks, demonstrating strong generalization ability and robustness when transferred from simulation to reality.</p>
<p>THE USE OF LLMS</p>
<p>We use the GPT-4o model to assist with grammar correction and language refinement during the writing of this paper.We thank the developers of GPT-4o for providing such a helpful tool.</p>
<p>Figure 1 :
1
Figure 1: Overview of mainstream VLA architectures.(1)Discretization-based methods map vision and language features into action tokens via LLM, but ignore robot state-an essential signal of physical dynamics-making action prediction less effective.(2) Diffusion-based approaches extract vision and language features with a VLM but pass them to a separate action expert for denoising, reducing the VLM to a large feature extractor and limiting its overall capability in action modeling.(3)Our model distills knowledge from a small action model while largely preserving the VLM structure.By integrating robot state through a lightweight encoder and introducing an action token to fuse vision, language, and state, it enables the VLM to actively participate in action modeling rather than only serving as a feature extractor, thereby better leveraging its modeling capabilities.</p>
<p>Figure 2 :
2
Figure2: Model Architecture.Our model is build upon VITA-1.5-7B(Fuet al., 2025), taking images, instructions, action tokens, and state information as inputs to generate executable actions.The visual and textual information is input into the VLM.The action token acts as a learnable query, while the robot state is encoded into a single token using linear layers.An action mapper extracts the hidden states of the action token from the final layer of the VLM, and transforms these to match the dimensionality expected by the pretrained action decoder, and finally the action decoder generates the corresponding actions with 7 degrees of freedom (DoF).</p>
<p>Figure 3 :
3
Figure 3: Training Strategy.Our training strategy comprises two stages.In the alignment stage, we train the action mapper, action tokens, and state encoder to bridge the gap between the action output spaces of the VLM and the small action model, updating only 30 million parameters while achieving improved fine-tuning outcomes.In the fine-tuning stage, we then perform end-to-end optimization of the entire model to further enhance overall performance.</p>
<p>Figure 4 :
4
Figure 4: Real-world Tasks.To evaluate the model in real-world settings, we formulate five tasks that span four canonical operations: Pick, Place, Close, and Stack.</p>
<p>Figure 5 :
5
Figure 5: Real robot setup.The platform consists of a PiPer robotic arm with a Songling gripper, equipped with two complementary cameras: an Intel RealSense D435i base camera (1280Ã—720) and a Dabai DCW grippermounted depth camera (640Ã—480).</p>
<p>task1: Put soup and box in basket, task2: Put box and butter in basket, task3: Turn on stove and put pot, task4: Put bowl in drawer and close it, task5: Put mugs on left and right plates, task6: Pick book and place it in back, task7: Put mug on plate and put pudding to right, task8: Put soup and sauce in basket, task9: Put both pots on stove, and task10: Put mug in microwave and close it.</p>
<p>pick up the book and place it in the back compartment of the caddy put both moka pots on the stove put both the alphabet soup and the cream cheese box in the basket put both the alphabet soup and the tomato sauce in the basket put both the cream cheese box and the butter in the basket put the black bowl in the bottom drawer of the cabinet and close it put the white mug on the left plate and put the yellow and white mug on the right plate put the white mug on the plate and put the chocolate pudding to the right of the plate put the yellow and white mug in the microwave and close it turn on the stove and put the moka pot on it</p>
<p>Figure 6 :
6
Figure 6: LIBERO-10 Visualization.</p>
<p>The remaining rows represent VLA models that include a VLM.The best result in each category is highlighted in bold.
MethodCategoryVLMTask completed in a row12345Avg. Len. â†‘Susie-87.069.049.038.026.02.69GR-1SAM *-85.471.259.649.740.13.06Seer-Large  â€ -88.480.974.869.662.13.763D-VLABLIP2-4B44.716.38.11.60.00.71OpenVLAPrismatic-7B62.818.35.81.81.00.90Roboflamingo VITA-VLA(only-ft)VLAFlamingo-3B VITA1.5-7B82.4 86.061.9 73.246.6 60.433.1 49.423.5 39.82.48 3.08VITA-VLA (two-stage)VITA1.5-7B92.577.161.049.238.23.184.1 BENCHMARKS, BASELINES AND EXPERIMENT DETAILS
* : SAM is Small Action Model.â€  indicates results reproduced by us.</p>
<p>Table 3 :
3
(Tian et al., 2024) across different tasks.For each task, we report the average success rate over 20 rollouts, following the evaluation protocol used in Seer(Tian et al., 2024).The metric "Avg.Success" denotes the average success rate across all ten tasks.The best results are highlighted in bold.Our model achieves the best performance on LIBERO-LONG.It demonstrates a 5.8% improvement over Seer-Large and a 1% improvement over the only-finetune strategy, showcasing its proficiency in executing tasks that require long-horizon planning.Detailed task information is provided in Appendix A.3.
MethodAvg. Success â†‘Task 1 Task 2 Task 3 Task 4 Task 5 Task 6 Task 7 Task 8 Task 9 Task 10MPI77.366.686.696.695.083.383.356.686.640.078.3OpenVLA54.035.095.065.045.040.080.060.045.020.055.0Seer-large87.791.790.098.3100.091.793.385.088.361.771.7VITA-VLA(only-ft)92.591.7100.098.398.398.390.090.091.786.780.0VITA-VLA(two-stage)93.5100.0100.0100.0100.0100.095.095.080.075.090.0</p>
<p>Table 4 :
4
Real-world Results.We report the average success rate of each task over 40 rollouts.Our model achieves the best results across all tasks.this work, we propose a simple yet effective VLA model architecture and validate its effectiveness.By combining a pretrained VLM with a small action model, we enable the VLM to acquire actionexecution capabilities through lightweight training while largely preserving its original structure.To train our model efficiently, we further introduce a two-stage distillation framework for transferring action-generation capabilities from small action models to the VLA model.The first stage aligns action representation spaces via lightweight representation matching, substantially reducing training complexity.The second stage selectively fine-tunes the language model, state encoder, and action modules, allowing the VLA to integrate the advanced ability of large-scale VLMs with the precise action-generation ability of small action model.Experimental results show competitive performance on CALVIN ABC-D and state-of-the-art results on LIBERO.Moreover, real-world robotic experiments confirm the practical applicability of our approach.
MethodSuccess Rate (%) â†‘CloseStackStackPick PlacePick PlaceAvg.DrawerCupsBlocksSpongeblockScoreSeer87.532.560.075.070.065.0VITA-VLA (only-ft)95.052.575.085.087.579.0VITA-VLA(two-stage)97.552.580.087.592.582.05 CONCLUSION
In</p>
<p>Table 5 :
5
Training Hyperparameters We use the action mapper to transform the hidden states of the action tokens into the dimensionality expected by the pretrained action decoder.The action mapper can adopt different architectures, such as MLPs, transformer-based architectures, decoder-only, or encoder-only architectures.We compare MLP and decoder-based implementations and find the performance difference to be negligible.Since the MLP is simpler and aligns with Occam's razor, we adopt the MLP as our action mapper.A.2 REAL ROBOT EXPERIMENT SETTINGS
HyperparametersAlignmentFinetuningbatch size84gradient accumulation steps44learning rate1e-41e-4optimizerAdamWAdamWlearning rate schedulecosine decaycosine decaywarmup epochs12training epochs32arm loss ratio-1.0gripper loss ratio-0.01CALVIN max history length1010LIBERO max history length77future action prediction33Table 6: Model HyperparametersIn dimOut dimAction token-3584Arm action encoder(Linear)6-DoF3584Gripper action encoder(Linear)23584State projector(Linear)71683584Action mapper(3 layer MLP)35841024(CALVIN)/384(LIBERO)Action decoder(2 layer MLP) 1024(CALVIN)/384(LIBERO) 512(CALVIN)/192(LIBERO)Arm action decoder(Linear) 512(CALVIN)/192(LIBERO) 6-DoF(CALVIN)/1(LIBERO)LLM(28 layer)35843584Vision encoder200 Ã— 20049 Ã— 3584Action Mapper Architecture.</p>
<p>Table 7 :
7
Task Success Rates of LIBERO Benchmarks
(a) LIBERO-SpatialTask ID Task NameAccuracy (%)0Pick black bowl between plate and ramekin place on plate100.01Pick black bowl next to ramekin place on plate100.02Pick black bowl from table center place on plate100.03Pick black bowl on cookie box place on plate100.04Pick black bowl in top drawer of wooden cabinet place on plate100.05Pick black bowl on ramekin place on plate96.06Pick black bowl next to cookie box place on plate100.07Pick black bowl on stove place on plate100.08Pick black bowl next to plate place on plate94.09Pick black bowl on wooden cabinet place on plate90.0
A APPENDIXA.1 IMPLEMENTATION DETAILS Training Hyperparameters.In our training process, we employ DeepSpeed's ZeRO-2 stage to efficiently train our model.This approach optimizes memory usage and accelerates training, making it suitable for handling large-scale datasets.The specific training hyperparameters used in our experiments are detailed in Table5.Model Hyperparameters.Since we conduct experiments on different datasets, we use different pretrained Seer models for alignment.This leads to variations in the hyperparameters of our model across datasets, as shown in Table6.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, arXiv:2309.166092023Qwen technical report. arXiv preprint</p>
<p>Gr00t n1: An open foundation model for generalist humanoid robots. Johan Bjorck, Fernando CastaÃ±eda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, arXiv:2503.147342025arXiv preprint</p>
<p>Zero-shot robotic manipulation with pretrained image-editing diffusion models. Kevin Black, Mitsuhiko Nakamoto, Pranav Atreya, Homer Walke, Chelsea Finn, Aviral Kumar, Sergey Levine, arXiv:2310.106392023arXiv preprint</p>
<p>pi 0 : A vision-language-action flow model for general robot control. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, arXiv:2410.241642024arXiv preprint</p>
<p>Rt-1: Robotics transformer for real-world control at scale. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, arXiv:2212.068172022arXiv preprint</p>
<p>Openhelix: A short survey, empirical analysis, and open-source dual-system vla model for robotic manipulation. Can Cui, Pengxiang Ding, Wenxuan Song, Shuanghao Bai, Xinyang Tong, Zirui Ge, Runze Suo, Wanqi Zhou, Yang Liu, Bofang Jia, arXiv:2505.03912arXiv:2501.01957Vita-1.5: Towards gpt-4o level real-time vision and speech interaction. 2025. 2025arXiv preprint</p>
<p>Masked autoencoders are scalable vision learners. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr DollÃ¡r, Ross Girshick, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Openvla: An open-source vision-language-action model. Jin Moo, Karl Kim, Siddharth Pertsch, Ted Karamcheti, Ashwin Xiao, Suraj Balakrishna, Rafael Nair, Ethan Rafailov, Grace Foster, Pannag Lam, Sanketi, arXiv:2406.092462024arXiv preprint</p>
<p>Vision-language foundation models as effective robot imitators. Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, arXiv:2311.013782023arXiv preprint</p>
<p>Libero: Benchmarking knowledge transfer for lifelong robot learning. Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, Peter Stone, Advances in Neural Information Processing Systems. 2023a36</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Advances in neural information processing systems. 2023b36</p>
<p>Rdt-1b: a diffusion foundation model for bimanual manipulation. Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, Jun Zhu, arXiv:2410.078642024arXiv preprint</p>
<p>Calvin: A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks. Oier Mees, Lukas Hermann, Erick Rosete-Beas, Wolfram Burgard, IEEE Robotics and Automation Letters. 732022</p>
<p>Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, Jiayuan Gu, Bin Zhao, Dong Wang, arXiv:2501.15830Exploring spatial representations for visuallanguage-action model. 2025arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PmLR2021</p>
<p>Efficientnet: Rethinking model scaling for convolutional neural networks. Mingxing Tan, Quoc Le, International conference on machine learning. PMLR2019</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, arXiv:2312.118052023arXiv preprint</p>
<p>Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, arXiv:2405.12213An open-source generalist robot policy. 2024arXiv preprint</p>
<p>Predictive inverse dynamics models are scalable learners for robotic manipulation. Yang Tian, Sizhe Yang, Jia Zeng, Ping Wang, Dahua Lin, Hao Dong, Jiangmiao Pang, arXiv:2412.151092024arXiv preprint</p>
<p>Unleashing large-scale video generative pre-training for visual robot manipulation. Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, Tao Kong, arXiv:2312.131392023arXiv preprint</p>
<p>Sigmoid loss for language image pre-training. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2023</p>
<p>Cot-vla: Visual chain-of-thought reasoning for visionlanguage-action models. Qingqing Zhao, Yao Lu, Jin Moo, Zipeng Kim, Zhuoyang Fu, Yecheng Zhang, Zhaoshuo Wu, Qianli Li, Song Ma, Chelsea Han, Finn, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference2025</p>
<p>Xiaowen Haoyu Zhen, Peihao Qiu, Jincheng Chen, Xin Yang, Yilun Yan, Yining Du, Chuang Hong, Gan, arXiv:2403.096313d-vla: A 3d vision-language-action generative world model. 2024arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Conference on Robot Learning. PMLR2023</p>            </div>
        </div>

    </div>
</body>
</html>
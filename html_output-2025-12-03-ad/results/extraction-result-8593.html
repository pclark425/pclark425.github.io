<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8593 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8593</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8593</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-278788837</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.16998v1.pdf" target="_blank">Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have been shown to achieve breakthrough performance on complex logical reasoning tasks. Nevertheless, most existing research focuses on employing formal language to guide LLMs to derive reliable reasoning paths, while systematic evaluations of these capabilities are still limited. In this paper, we aim to conduct a comprehensive evaluation of LLMs across various logical reasoning problems utilizing formal languages. From the perspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, and format of trajectories, our key findings are: 1) Thinking models significantly outperform Instruct models, especially when formal language is employed; 2) All LLMs exhibit limitations in inductive reasoning capability, irrespective of whether they use a formal language; 3) Data with PoT format achieves the best generalization performance across other languages. Additionally, we also curate the formal-relative training data to further enhance the small language models, and the experimental results indicate that a simple rejected fine-tuning method can better enable LLMs to generalize across formal languages and achieve the best overall performance. Our codes and reports are available at https://github.com/jiangjin1999/FormalEval.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8593.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8593.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-performing commercial instruction/thinking-style LLM used as an anchor and teacher in experiments; shows relatively stable and strong performance across natural and formal trajectory formats in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described as a strong Thinking/Instruct-capable large language model used both as an evaluated model and as a teacher for synthetic data generation; optimized on diverse data and exhibits stable behavior on formal-language trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not reported</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Collection of strict logical benchmarks (e.g., LogicBench, BBH_formal_fallacies, BBH_logical_deduction, ARLSAT, FOLIO)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmarks that require strict logical/deductive reasoning including first-order logical validity, formal fallacy detection, constrained analytical reasoning (AR-LSAT), and SMT/CSP-style problems.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot evaluation across trajectory formats (Text, PoT/Python, Z3, CSP); for formal trajectories used external executors (Python 3.12, Z3 solver, python-constraint); three-step self-refinement during code execution used for formal languages; GPT-4o also used as teacher to generate filtered executable formal-code training data (Rejection sampling Fine-Tuning pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported as among the strongest and most stable in the suite; paper reports GPT-4o as a high-performing anchor across formats and tasks (qualitative). In aggregate tables GPT-4o attains the best or near-best accuracy across many tasks and shows stability when switching to formal languages.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms many open-source Instruct models and small thinking models; used as teacher to create high-quality formal trajectories that improved downstream small-model performance via fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Although GPT-4o is relatively stable, the paper notes general degradation of many models (including some large ones) on difficult tasks when using formal languages; no exhaustive failure-case list specific to GPT-4o is given beyond dataset/task-level failure modes (e.g., inductive tasks remain weak across models).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>GPT-4o demonstrates that very strong models can be robust across formal trajectory formats; using such strong models as teachers (with rejection sampling to keep executable/verified code) yields effective formal-language training data for improving smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8593.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8593.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QwQ-32B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QwQ-32B (Thinking model series example)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 'Thinking' style 32B model in the evaluated spectrum that consistently outperformed Instruct counterparts and uniquely maintained high performance across formal trajectories in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>QwQ-32B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A Thinking-mode large language model (32B parameters) evaluated across Text, PoT (Python), Z3, and CSP trajectories; exemplifies the 'Thinking' family that elicits longer/more structured trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Multiple strict logical reasoning tasks (BBH subsets including BBH_formal_fallacies, BBH_logical_deduction; LogicBench; ARLSAT; other BBH/bbeh formal subsets)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks convertible to first-order logic or structured constraint problems and requiring stepwise, verifiable derivations (deductive/logical-deduction, formal fallacies, spatial/formal reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot formal-trajectory generation executed by external solvers (Z3, python-constraint) and Python execution environment; evaluated directly in formal formats.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported as the only model that 'stays ahead in all tasks and trajectories' — i.e., maintains high performance across Text, PoT, Z3, and CSP, outperforming Instruct-series models in many formal tasks (qualitative claim in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms Instruct-series models (same or similar sizes) particularly on Inductive and Mixed-Form tasks when formal languages are used; contrasts with general trend where Text often outperforms formal formats.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Noted as an exception among models — the paper does not report strong failure modes for QwQ-32B specifically, but general limitations on inductive reasoning across models still apply.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Thinking-mode models (as exemplified by QwQ-32B) can significantly improve formal-language reasoning performance compared to Instruct models; architecture/training that encourages longer structured trajectories helps formal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8593.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8593.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2.5-7B-Baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2.5-7B (Baseline / Instruct variant used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B Instruct-style model used as a baseline for experiments; demonstrates limited formal-language execution and reasoning without targeted formal-data augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An open-source Instruct-style language model (7B parameters) evaluated across Text, PoT, Z3, and CSP; used as baseline for supervised fine-tuning experiments and for formal-data augmentation comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Wide suite of 66 logical-reasoning datasets (e.g., BBH subsets, ARLSAT, LogicBench, FOLIO, ProntoQA, GSM8K subset) across Deductive, Inductive, Abductive, and Mixed-Form categories.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Includes strict deductive first-order logic tasks, constraint-satisfaction problems (AR-LSAT), formal fallacies, logical deduction, and assorted benchmark tasks probing different reasoning types.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot evaluation on multiple trajectory formats; used as baseline for supervised fine-tuning (SFT) and then compared to formally augmented RFT fine-tuned models; evaluation used three-step self-refinement for formal trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Baseline average accuracy (reported) ~34.0% across evaluated formats prior to formal augmentation, with execution rate ~65.3% (paper-level aggregate figures for Qwen2.5-7B-Baseline* are reported). Performance especially poor on formal-language trajectories (notably CSP/Z3) for many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Serves as the baseline against the formally enhanced variant (Qwen2.5-7B-Base w.Formal); after augmentation the enhanced model surpasses the baseline and even some larger open-source Instruct models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Shows rapid performance degradation when moving from Text to formal trajectory formats, particularly on complex/difficult tasks; lacks inductive reasoning capability; execution success rate and correctness in formal formats are low without augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Small/medium Instruct models trained mainly on natural language have limited ability to produce executable, correct formal trajectories; targeted formal-data fine-tuning is necessary to improve formal reasoning and execution rates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8593.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8593.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2.5-7B-Base.w.Formal (RFT-enhanced)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2.5-7B-Base with Formal-data Augmentation (Rejected Fine-Tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B model obtained by augmenting Qwen2.5-7B with curated executable formal-language trajectories via a Rejection-sampling Fine-Tuning (RFT) pipeline; shows substantial gains on formal reasoning and execution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-7B-Base w.Formal</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Variant of Qwen2.5-7B fine-tuned on a mixture of its baseline generic data plus curated formal-language trajectories (Python/Z3/CSP) filtered by GPT-4o teacher via rejection sampling; trained via supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Same large suite of 66 logical reasoning datasets (Deductive/Inductive/Abductive/Mixed-Form) evaluated under Text, PoT, Z3, CSP trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmarks requiring executable formalization of problems (programming or solver encodings) and correct logical inferences; includes strict FOL tasks and CSP-style constraint problems (AR-LSAT).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Rejected fine-tuning (RFT): generate multiple outputs with GPT-4o, filter to keep only executable/solver-verified trajectories, then SFT the 7B model on this curated formal dataset. Evaluations used the same execution pipeline and three-step self-refinement for formal trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Notable improvements: overall average accuracy increased from 34.0% to 42.0%; execution rate increased from 65.3% to 76.0%; gains per-format reported as Text +3.0% accuracy, Z3 +7.7% accuracy (+8.3% exec rate), CSP +17.0% accuracy (+15.9% exec rate). On CSP accuracy rose from 20.0% to 37.0% in reported aggregate.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Surpasses the Qwen2.5-7B-Baseline and also outperforms the open-source Qwen2.5-7B-Instruct in many formal formats; formal-data fine-tuning narrowed gaps with much larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Improvement is dataset- and subset-limited (paper notes formal augmentation applied to subset of tasks); augmentation may not generalize to all domains or to all formal languages (CSP shows structural transfer asymmetries).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Curated executable formal-language training data (via rejection sampling from a strong teacher) is an effective and simple way to enable small models to generalize across formal languages and substantially improve execution rate and accuracy on formal reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8593.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8593.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>R1-Distill-Qwen-7B (small Thinking)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>R1-Distill-Qwen-7B (Distilled/Small Thinking model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B distilled Thinking-style model evaluated to probe the effect of model scale and thinking training on formal-language reasoning; small models struggle with structured/formal trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>R1-Distill-Qwen-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A small (7B) distilled Thinking-style model variant (R1-Distill) evaluated against Text and formal trajectory formats; used to assess scale and thinking-mode interactions for formal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Selected strict logical reasoning tasks including BBH/bbeh hard subsets and structured formal tasks (Z3/CSP), and high-complexity tasks like bbeh-time and bbeh-shuffle.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks requiring multi-step formal reasoning, solver-compatible encodings, and complex constraint handling.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot evaluation with same execution pipeline; evaluated across Text, PoT, Z3, CSP; no additional formal fine-tuning reported for this model in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Per-paper observation: small Thinking and Instruct models have acceptable Text performance but performance drops rapidly under formal languages; R1-Distill-Qwen-7B notably performs very poorly on CSP/Z3 for complex tasks, sometimes unable to complete basic logical steps.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Performs worse than larger Thinking and Instruct models on formal formats; demonstrates that Thinking-mode training does not effectively support formal-language reasoning at low parameter scales.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Severe lack of ability to deal with formal reasoning problems on high-complexity tasks (e.g., bbeh-time, bbeh-shuffle). Execution failure rates and incorrect trajectories are common for structured formats like Z3 and CSP.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Model scale matters: small models trained or distilled into 'thinking' styles still struggle to produce correct executable formal trajectories; formal-data augmentation and/or larger capacity are required for robust formal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8593.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8593.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Thinking vs Instruct models (family-level finding)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Thinking-mode models versus Instruct-mode models (family comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cross-model-family observation: 'Thinking' models (trained to produce long structured trajectories) outperform 'Instruct' models on formal-language logical reasoning, especially on inductive and mixed-form tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Thinking-mode family (e.g., QwQ-32B, DeepSeek variants) vs Instruct family (e.g., Qwen2.5-Instruct series)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Comparative categories of LLMs characterized by training signals that encourage longer structured output trajectories (Thinking) versus shorter instruction-following behavior (Instruct); sizes in evaluated set ranged ~7B to 72B.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B--72B (various evaluated models)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Various strict logical reasoning benchmarks (Deductive/Inductive/Abductive/Mixed-Form suite of 66 datasets including BBH subsets, LogicBench, ARLSAT, FOLIO, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks requiring structured, verifiable reasoning steps; many are convertible to formal solver encodings (FOL/SMT/CSP) or require inductive rule inference.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot evaluation across Text, PoT, Z3, CSP; analysis of performance by family and format; supervised fine-tuning experiments (RFT) applied to smaller Instruct models to close gap.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Key finding: Thinking models significantly outperform Instruct models across many tasks, with larger margins on inductive and mixed-form tasks and when formal languages are employed; however, Thinking advantage weakens at low model scale.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Comparison described across many paired model families and sizes in paper tables; Thinking models outperform same-size Instruct models in formal-language trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Instruct models show unsatisfactory inductive reasoning; Thinking models still suffer on inductive reasoning in absolute terms and small Thinking models fail on formal trajectories due to scale limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Encouraging thinking-style training and longer structured trajectories helps formal reasoning, but capacity and formal-language exposure (training data) are crucial; a combination of stronger thinking-style models plus curated formal data yields benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Logic-lm: Empowering large language models with symbolic solvers for <em>(Rating: 2)</em></li>
                <li>AR-lsat: Investigating analytical reasoning of text <em>(Rating: 2)</em></li>
                <li>BIG-Bench Hard (BBH) <em>(Rating: 2)</em></li>
                <li>FOLIO <em>(Rating: 1)</em></li>
                <li>Linc: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8593",
    "paper_id": "paper-278788837",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o",
            "brief_description": "A high-performing commercial instruction/thinking-style LLM used as an anchor and teacher in experiments; shows relatively stable and strong performance across natural and formal trajectory formats in this study.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Described as a strong Thinking/Instruct-capable large language model used both as an evaluated model and as a teacher for synthetic data generation; optimized on diverse data and exhibits stable behavior on formal-language trajectories.",
            "model_size": "not reported",
            "reasoning_task_name": "Collection of strict logical benchmarks (e.g., LogicBench, BBH_formal_fallacies, BBH_logical_deduction, ARLSAT, FOLIO)",
            "reasoning_task_description": "Benchmarks that require strict logical/deductive reasoning including first-order logical validity, formal fallacy detection, constrained analytical reasoning (AR-LSAT), and SMT/CSP-style problems.",
            "method_or_approach": "Zero-shot evaluation across trajectory formats (Text, PoT/Python, Z3, CSP); for formal trajectories used external executors (Python 3.12, Z3 solver, python-constraint); three-step self-refinement during code execution used for formal languages; GPT-4o also used as teacher to generate filtered executable formal-code training data (Rejection sampling Fine-Tuning pipeline).",
            "performance": "Reported as among the strongest and most stable in the suite; paper reports GPT-4o as a high-performing anchor across formats and tasks (qualitative). In aggregate tables GPT-4o attains the best or near-best accuracy across many tasks and shows stability when switching to formal languages.",
            "baseline_comparison": "Outperforms many open-source Instruct models and small thinking models; used as teacher to create high-quality formal trajectories that improved downstream small-model performance via fine-tuning.",
            "limitations_or_failures": "Although GPT-4o is relatively stable, the paper notes general degradation of many models (including some large ones) on difficult tasks when using formal languages; no exhaustive failure-case list specific to GPT-4o is given beyond dataset/task-level failure modes (e.g., inductive tasks remain weak across models).",
            "insights_or_conclusions": "GPT-4o demonstrates that very strong models can be robust across formal trajectory formats; using such strong models as teachers (with rejection sampling to keep executable/verified code) yields effective formal-language training data for improving smaller models.",
            "uuid": "e8593.0",
            "source_info": {
                "paper_title": "Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "QwQ-32B",
            "name_full": "QwQ-32B (Thinking model series example)",
            "brief_description": "A 'Thinking' style 32B model in the evaluated spectrum that consistently outperformed Instruct counterparts and uniquely maintained high performance across formal trajectories in this study.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "QwQ-32B",
            "model_description": "A Thinking-mode large language model (32B parameters) evaluated across Text, PoT (Python), Z3, and CSP trajectories; exemplifies the 'Thinking' family that elicits longer/more structured trajectories.",
            "model_size": "32B",
            "reasoning_task_name": "Multiple strict logical reasoning tasks (BBH subsets including BBH_formal_fallacies, BBH_logical_deduction; LogicBench; ARLSAT; other BBH/bbeh formal subsets)",
            "reasoning_task_description": "Tasks convertible to first-order logic or structured constraint problems and requiring stepwise, verifiable derivations (deductive/logical-deduction, formal fallacies, spatial/formal reasoning).",
            "method_or_approach": "Zero-shot formal-trajectory generation executed by external solvers (Z3, python-constraint) and Python execution environment; evaluated directly in formal formats.",
            "performance": "Reported as the only model that 'stays ahead in all tasks and trajectories' — i.e., maintains high performance across Text, PoT, Z3, and CSP, outperforming Instruct-series models in many formal tasks (qualitative claim in paper).",
            "baseline_comparison": "Outperforms Instruct-series models (same or similar sizes) particularly on Inductive and Mixed-Form tasks when formal languages are used; contrasts with general trend where Text often outperforms formal formats.",
            "limitations_or_failures": "Noted as an exception among models — the paper does not report strong failure modes for QwQ-32B specifically, but general limitations on inductive reasoning across models still apply.",
            "insights_or_conclusions": "Thinking-mode models (as exemplified by QwQ-32B) can significantly improve formal-language reasoning performance compared to Instruct models; architecture/training that encourages longer structured trajectories helps formal reasoning.",
            "uuid": "e8593.1",
            "source_info": {
                "paper_title": "Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Qwen2.5-7B-Baseline",
            "name_full": "Qwen2.5-7B (Baseline / Instruct variant used as baseline)",
            "brief_description": "A 7B Instruct-style model used as a baseline for experiments; demonstrates limited formal-language execution and reasoning without targeted formal-data augmentation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-7B",
            "model_description": "An open-source Instruct-style language model (7B parameters) evaluated across Text, PoT, Z3, and CSP; used as baseline for supervised fine-tuning experiments and for formal-data augmentation comparisons.",
            "model_size": "7B",
            "reasoning_task_name": "Wide suite of 66 logical-reasoning datasets (e.g., BBH subsets, ARLSAT, LogicBench, FOLIO, ProntoQA, GSM8K subset) across Deductive, Inductive, Abductive, and Mixed-Form categories.",
            "reasoning_task_description": "Includes strict deductive first-order logic tasks, constraint-satisfaction problems (AR-LSAT), formal fallacies, logical deduction, and assorted benchmark tasks probing different reasoning types.",
            "method_or_approach": "Zero-shot evaluation on multiple trajectory formats; used as baseline for supervised fine-tuning (SFT) and then compared to formally augmented RFT fine-tuned models; evaluation used three-step self-refinement for formal trajectories.",
            "performance": "Baseline average accuracy (reported) ~34.0% across evaluated formats prior to formal augmentation, with execution rate ~65.3% (paper-level aggregate figures for Qwen2.5-7B-Baseline* are reported). Performance especially poor on formal-language trajectories (notably CSP/Z3) for many tasks.",
            "baseline_comparison": "Serves as the baseline against the formally enhanced variant (Qwen2.5-7B-Base w.Formal); after augmentation the enhanced model surpasses the baseline and even some larger open-source Instruct models.",
            "limitations_or_failures": "Shows rapid performance degradation when moving from Text to formal trajectory formats, particularly on complex/difficult tasks; lacks inductive reasoning capability; execution success rate and correctness in formal formats are low without augmentation.",
            "insights_or_conclusions": "Small/medium Instruct models trained mainly on natural language have limited ability to produce executable, correct formal trajectories; targeted formal-data fine-tuning is necessary to improve formal reasoning and execution rates.",
            "uuid": "e8593.2",
            "source_info": {
                "paper_title": "Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Qwen2.5-7B-Base.w.Formal (RFT-enhanced)",
            "name_full": "Qwen2.5-7B-Base with Formal-data Augmentation (Rejected Fine-Tuning)",
            "brief_description": "A 7B model obtained by augmenting Qwen2.5-7B with curated executable formal-language trajectories via a Rejection-sampling Fine-Tuning (RFT) pipeline; shows substantial gains on formal reasoning and execution.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-7B-Base w.Formal",
            "model_description": "Variant of Qwen2.5-7B fine-tuned on a mixture of its baseline generic data plus curated formal-language trajectories (Python/Z3/CSP) filtered by GPT-4o teacher via rejection sampling; trained via supervised fine-tuning.",
            "model_size": "7B",
            "reasoning_task_name": "Same large suite of 66 logical reasoning datasets (Deductive/Inductive/Abductive/Mixed-Form) evaluated under Text, PoT, Z3, CSP trajectories.",
            "reasoning_task_description": "Benchmarks requiring executable formalization of problems (programming or solver encodings) and correct logical inferences; includes strict FOL tasks and CSP-style constraint problems (AR-LSAT).",
            "method_or_approach": "Rejected fine-tuning (RFT): generate multiple outputs with GPT-4o, filter to keep only executable/solver-verified trajectories, then SFT the 7B model on this curated formal dataset. Evaluations used the same execution pipeline and three-step self-refinement for formal trajectories.",
            "performance": "Notable improvements: overall average accuracy increased from 34.0% to 42.0%; execution rate increased from 65.3% to 76.0%; gains per-format reported as Text +3.0% accuracy, Z3 +7.7% accuracy (+8.3% exec rate), CSP +17.0% accuracy (+15.9% exec rate). On CSP accuracy rose from 20.0% to 37.0% in reported aggregate.",
            "baseline_comparison": "Surpasses the Qwen2.5-7B-Baseline and also outperforms the open-source Qwen2.5-7B-Instruct in many formal formats; formal-data fine-tuning narrowed gaps with much larger models.",
            "limitations_or_failures": "Improvement is dataset- and subset-limited (paper notes formal augmentation applied to subset of tasks); augmentation may not generalize to all domains or to all formal languages (CSP shows structural transfer asymmetries).",
            "insights_or_conclusions": "Curated executable formal-language training data (via rejection sampling from a strong teacher) is an effective and simple way to enable small models to generalize across formal languages and substantially improve execution rate and accuracy on formal reasoning tasks.",
            "uuid": "e8593.3",
            "source_info": {
                "paper_title": "Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "R1-Distill-Qwen-7B (small Thinking)",
            "name_full": "R1-Distill-Qwen-7B (Distilled/Small Thinking model)",
            "brief_description": "A 7B distilled Thinking-style model evaluated to probe the effect of model scale and thinking training on formal-language reasoning; small models struggle with structured/formal trajectories.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "R1-Distill-Qwen-7B",
            "model_description": "A small (7B) distilled Thinking-style model variant (R1-Distill) evaluated against Text and formal trajectory formats; used to assess scale and thinking-mode interactions for formal reasoning.",
            "model_size": "7B",
            "reasoning_task_name": "Selected strict logical reasoning tasks including BBH/bbeh hard subsets and structured formal tasks (Z3/CSP), and high-complexity tasks like bbeh-time and bbeh-shuffle.",
            "reasoning_task_description": "Tasks requiring multi-step formal reasoning, solver-compatible encodings, and complex constraint handling.",
            "method_or_approach": "Zero-shot evaluation with same execution pipeline; evaluated across Text, PoT, Z3, CSP; no additional formal fine-tuning reported for this model in paper.",
            "performance": "Per-paper observation: small Thinking and Instruct models have acceptable Text performance but performance drops rapidly under formal languages; R1-Distill-Qwen-7B notably performs very poorly on CSP/Z3 for complex tasks, sometimes unable to complete basic logical steps.",
            "baseline_comparison": "Performs worse than larger Thinking and Instruct models on formal formats; demonstrates that Thinking-mode training does not effectively support formal-language reasoning at low parameter scales.",
            "limitations_or_failures": "Severe lack of ability to deal with formal reasoning problems on high-complexity tasks (e.g., bbeh-time, bbeh-shuffle). Execution failure rates and incorrect trajectories are common for structured formats like Z3 and CSP.",
            "insights_or_conclusions": "Model scale matters: small models trained or distilled into 'thinking' styles still struggle to produce correct executable formal trajectories; formal-data augmentation and/or larger capacity are required for robust formal reasoning.",
            "uuid": "e8593.4",
            "source_info": {
                "paper_title": "Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Thinking vs Instruct models (family-level finding)",
            "name_full": "Thinking-mode models versus Instruct-mode models (family comparison)",
            "brief_description": "A cross-model-family observation: 'Thinking' models (trained to produce long structured trajectories) outperform 'Instruct' models on formal-language logical reasoning, especially on inductive and mixed-form tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Thinking-mode family (e.g., QwQ-32B, DeepSeek variants) vs Instruct family (e.g., Qwen2.5-Instruct series)",
            "model_description": "Comparative categories of LLMs characterized by training signals that encourage longer structured output trajectories (Thinking) versus shorter instruction-following behavior (Instruct); sizes in evaluated set ranged ~7B to 72B.",
            "model_size": "7B--72B (various evaluated models)",
            "reasoning_task_name": "Various strict logical reasoning benchmarks (Deductive/Inductive/Abductive/Mixed-Form suite of 66 datasets including BBH subsets, LogicBench, ARLSAT, FOLIO, etc.)",
            "reasoning_task_description": "Tasks requiring structured, verifiable reasoning steps; many are convertible to formal solver encodings (FOL/SMT/CSP) or require inductive rule inference.",
            "method_or_approach": "Zero-shot evaluation across Text, PoT, Z3, CSP; analysis of performance by family and format; supervised fine-tuning experiments (RFT) applied to smaller Instruct models to close gap.",
            "performance": "Key finding: Thinking models significantly outperform Instruct models across many tasks, with larger margins on inductive and mixed-form tasks and when formal languages are employed; however, Thinking advantage weakens at low model scale.",
            "baseline_comparison": "Comparison described across many paired model families and sizes in paper tables; Thinking models outperform same-size Instruct models in formal-language trajectories.",
            "limitations_or_failures": "Instruct models show unsatisfactory inductive reasoning; Thinking models still suffer on inductive reasoning in absolute terms and small Thinking models fail on formal trajectories due to scale limitations.",
            "insights_or_conclusions": "Encouraging thinking-style training and longer structured trajectories helps formal reasoning, but capacity and formal-language exposure (training data) are crucial; a combination of stronger thinking-style models plus curated formal data yields benefits.",
            "uuid": "e8593.5",
            "source_info": {
                "paper_title": "Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Logic-lm: Empowering large language models with symbolic solvers for",
            "rating": 2,
            "sanitized_title": "logiclm_empowering_large_language_models_with_symbolic_solvers_for"
        },
        {
            "paper_title": "AR-lsat: Investigating analytical reasoning of text",
            "rating": 2,
            "sanitized_title": "arlsat_investigating_analytical_reasoning_of_text"
        },
        {
            "paper_title": "BIG-Bench Hard (BBH)",
            "rating": 2,
            "sanitized_title": "bigbench_hard_bbh"
        },
        {
            "paper_title": "FOLIO",
            "rating": 1
        },
        {
            "paper_title": "Linc: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers",
            "rating": 2,
            "sanitized_title": "linc_a_neurosymbolic_approach_for_logical_reasoning_by_combining_language_models_with_firstorder_logic_provers"
        }
    ],
    "cost": 0.01805675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?
22 May 2025</p>
<p>Jin Jiang jiangjin@stu.pku.edu.cn 
Peking University</p>
<p>Meituan Group</p>
<p>Jianing Wang 
Meituan Group</p>
<p>Yuchen Yan 
Meituan Group</p>
<p>Zhejiang University</p>
<p>Yang Liu 
Meituan Group</p>
<p>Jianhua Zhu 
Peking University</p>
<p>Mengdi Zhang 
Meituan Group</p>
<p>Xunliang Cai 
Meituan Group</p>
<p>Liangcai Gao gaoliangcai@pku.edu.cn 
Peking University</p>
<p>Karl Cobbe 
Vineet Kosaraju 
Mohammad Bavarian 
Mark Chen 
Heewoo Jun 
Lukasz Kaiser 
Matthias Plappert 
Jerry Tworek 
Jacob Hilton 
Aaron Grattafiori 
Abhimanyu Dubey 
Abhinav Jauhri 
Abhinav Pandey 
Abhishek Kadian 
Ahmad Al- Dahle 
Aiesha Letman 
Akhil Mathur 
Alan Schelten 
Simeng Han 
Hailey Schoelkopf 
Yilun Zhao 
Zhent- Ing Qi 
Martin Riddell 
Wenfei Zhou 
James Coady 
Woosuk Kwon 
Zhuohan Li 
Siyuan Zhuang 
Ying Sheng 
Lianmin Zheng 
Cody Hao 
Joseph E Gonzalez 
Hao Zhang 
Qing Lyu 
Shreya Havaldar 
Adam Stein 
Li Zhang 
Delip Rao 
Eric Wong 
Marianna Apidianaki 
Mirac Suzgun 
Nathan Scales 
Nathanael Scharli 
Se- Bastian Gehrmann 
HyungYi Tay 
Won Chung 
Michael Petrov 
Vincent Y Zhao 
Ryan Murphy 
Jason Weston 
Antoine Bordes 
Sumit Chopra 
Tomas Mikolov 
Alexan- Der M Rush 
Bart Van Merriënboer 
Armand Joulin 
Yixin Zhao 
Adina Williams 
Emily Dinan 
Mohit Bansal 
Mark Yatskar 
Yejin 2021 Choi 
Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?
22 May 2025BC197179EBC6210751F7C3C7A8B04E0CarXiv:2505.16998v1[cs.CL]Nakano, and 1 others. 2021. Training verifiers to solve math word problems. In null.
Large Language Models (LLMs) have been shown to achieve breakthrough performance on complex logical reasoning tasks.Nevertheless, most existing research focuses on employing formal language to guide LLMs to derive reliable reasoning paths, while systematic evaluations of these capabilities are still limited.In this paper, we aim to conduct a comprehensive evaluation of LLMs across various logical reasoning problems utilizing formal languages.From the perspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, and format of trajectories, our key findings are: 1) Thinking models significantly outperform Instruct models, especially when formal language is employed; 2) All LLMs exhibit limitations in inductive reasoning capability, irrespective of whether they use a formal language; 3) Data with PoT format achieves the best generalization performance across other languages.Additionally, we also curate the formal-relative training data to further enhance the small language models, and the experimental results indicate that a simple rejected finetuning method can better enable LLMs to generalize across formal languages and achieve the best overall performance.Our codes and reports are available at https://github.com/jiangjin1999/FormalEval.</p>
<p>Introduction</p>
<p>Logical reasoning, i.e., deductive, inductive, and abductive, is one of the imperative natural language processing (NLP) tasks and plays a significant role in artificial intelligence (AI) to perform human-like decision-making, task-solving, and deep-thinking (Zhang et al., 2021;Yang et al., 2023b;Yu et al., 2024;Xu et al., 2025).Different from conventional natural language understanding and generation, logical reasoning requires the AI systems to explicitly provide meticulous elucidation of thoughts and verifiable derivation chains,  which is crucial and challenging (Cummins et al., 1991).Early works have developed multiple formal languages with symbol solvers to make the reasoning steps computable and structured (RANISE, 2003;Bulatov et al., 2005;Bjørner et al., 2015).</p>
<p>Recently, the emergence of reasoning capabilities in large language models (LLMs) has incentivized significant progress in complex reasoning tasks, such as mathematics, commonsense, and symbol (Achiam et al., 2023;Bi et al., 2024).Current studies 1 have found that LLMs can achieve remarkable performance with the aid of formal language and symbol solvers, especially when integrating well-designed task-specific instructs (Lyu et al., 2023;Pan et al., 2023), chain-of-thought (CoT) reasoning patterns (Wei et al., 2022;Ye et al., 2023), and valuable solvers' feedback (He-Yueya et al., 2023;Gao et al., 2023;Wang et al., 2024).Such approaches aim to formalize the given logical problem and constantly adjust the results lean on the solver's feedback.Despite substantial efforts exhibiting exceptional performance, there are still relatively limited systematic and comprehensive evaluations.Thus, a natural question remains open: whether the LLM really excels in complex logical reasoning problems with formal language?</p>
<p>To bridge the gap, this paper endeavors to perform a comprehensive evaluation of LLMs utilizing various formal languages to tackle diverse logical reasoning problems.At first, we develop the evaluation architecture to clearly express the entire assessment view (As illustrated in Section 2), with the framework shown in Figure 1.Specifically, we divide the entire assessment into three distinct dimensions, including the spectrum of LLMs, the taxonomy of logical reasoning tasks, and the format of trajectories.For the family of LLMs, we further consider different reasoning patterns which has been injected into the model training, such as short thinking (e.g., GPT-4o (Achiam et al., 2023), Qwen1.5/2/2.5 (Bai et al., 2023), LLaMA3/3.1/3.3 (Grattafiori et al., 2024)) and long thinking (e.g., DeepSeek-R1-Dsitill-Qwen (Guo et al., 2025)).For the logical reasoning, we adhere to the classic definitions (Flach and Kakas, 2000), categorizing tasks into deductive, inductive, and abductive reasoning.Additionally, we account for tasks that may integrate multiple reasoning types by introducing a new category referred to as mixedform reasoning.Regarding the format of trajectories, we consider three main formal languages ("Python", "Z3", "CSP") with a default natural language format as "Text".</p>
<p>Secondly, we perform a thorough evaluation across these three dimensions (as detailed in Section 3).Many contemporary benchmarks purely emphasize informal text patterns and lack comprehensive integration of different formal languages and logical reasoning tasks (Lei et al., 2024;Xu et al., 2025;Xia et al., 2025).For instance, it is widely recognized that Python is superior to plain text when addressing mathematical problems (Friedman, 2023;Gao et al., 2023), but it remains unclear whether Python is equally effective in resolving BBH (Suzgun et al., 2022) and bbeh (Kazemi et al., 2025) problems.To fill this blank, this part aims to investigate whether current LLMs can solve a variety of logical reasoning tasks utilizing different formal languages.From this study, we derive several intriguing observations: 1) Thinking models significantly outperform Instruct models, especially when formal language is em-ployed; 2) All LLMs exhibit limitations in inductive reasoning capability, irrespective of whether they use a formal language; 3) LLMs typically produce inferior performance on difficult tasks.These findings prompt a new inquiry Do large models possess generalization capabilities when employing formal languages?</p>
<p>Thirdly, we further investigate the generalization across different reasoning tasks and formal languages (As illustrated in Section 4).To reach this goal, we collect a few training data from the training set of current evaluation tasks, which is classified into three types: deductive, inductive, and abductive.For each task type, we also provide different trajectories according to the usages of (in)formal languages.To make a fair comparison, we only use data from a single language type for SFT training, and the training data has the same scale size.From the experiments, we observe that the LLM can obtain significant in-domain performance on multiple logical reasoning tasks.In addition, we also discovered an elusive phenomenon that CSP is hard to generalize to other formal and informal languages, but it is easy to generalize from other languages to CSP.Therefore, we speculate that the poor performance of LLM on some formal languages can be blamed on the lack of pertinent knowledge and potential for stimulated reasoning.</p>
<p>Lastly, based on the previous exploration, we aim to amplify the capabilities of weaker models in using formal languages to solve reasoning problems.Concretely, we propose a simple but effective rejected fine-tuning (RFT) approach to curate different formal-relative training data.After the enrichment, the overall accuracy of using informal and formal languages for complex logical tasks can be improved by more than 10%.</p>
<p>In summary, the main contributions are as follows:</p>
<p>• In light of the insufficient evaluations of existing works, we aim to collect 66 tasks with multiple widely used formal languages, and provide a comprehensive evaluation for current LLMs across three dimensions, including the spectrum of LLMs, the taxonomy of tasks, and the format of trajectories.</p>
<p>• Considering that different formal languages have different expressions for reasoning, we explore the generalization across various formal languages.</p>
<p>• To further enhance the capability of LLMs in utilizing formal languages to solve complex logic reasoning, we introduce a simple but effective rejected fine-tuning method with curated formal-relative data.The experimental results indicate the effectiveness of considering the generalization of formal language across various logical tasks.</p>
<p>Preliminary</p>
<p>As illustrated in Figure 1, our evaluation framework is structured along three dimensions: Model, Trajectory Format, and Task Type.In this section, we introduce the two key points of complex reasoning task categorization (Section 2.1) and trajectory format design (Section 2.2).</p>
<p>Taxonomy of Complex Logical Reasoning</p>
<p>Inspired by Xu et al. (2025), we present a unified taxonomy that categorizes a wide range of complex reasoning tasks into four major types: Deductive, Inductive, Abductive, and Mixed-Form.To elaborate, the categorization is based on the nature of reasoning required in human-like thinking in the real world: 1) Deductive reasoning is the forward reasoning process with rules that starts from the given premises to the conclusion (Goel, 2007;Johnson-Laird, 1999).Formally, we can denote the process as premise rule −→conclusion.2) Inductive reasoning is the process that infers specific rules based on multiple premises and conclusions.It can be represented as (premise, conclusion)→rule.</p>
<p>3) Abductive reasoning is the backward process of deductive which aims to obtain the premise based on conclusion, and the process can be viewed as conclusion rule −→premise.4) Mix-Form Reasoning involves at least two of the above three types of reasoning.In real-life scenarios, most complex problems involve mixed reasoning, including but not limited to temporal-spatial reasoning, NLU, knowledge reasoning, and mathematical reasoning.</p>
<p>In pursuit of specific benchmarks based on these categories, we meticulously collect 66 subsets of data, and the detailed information can be found in Table 2.The details of the specific datasets are shown in Appendix B.</p>
<p>Trajectory Format</p>
<p>As shown in Figure 1, we categorize trajectory formats into two main types: informal language (natural language) and formal language.Informal language can be expressed as free-form text, while formal languages include programming languages (e.g., Python) and logic-based languages (e.g., Z3 and CSP).They can be modeled as:
LLM(Q) = ⟨s 1 , s 2 , . . . , s n ⟩ Exec −−→ A
where Q is the input question, and LLM(Q) represents the trajectory generated by LLM.Each step s i ∈ L LLM corresponds to a structured unit (e.g., code or logic expression), and the trajectory is executed by an external engine to produce the final answer A.</p>
<p>For PoT, we use Python 3.12 and its standard library as the execution environment.Each step s i ∈ LLM PoT is a valid Python statement.For Z3, we adopt the Z3 theorem prover as the executor and Z3 trajectories are composed of declarative symbolic steps s i ∈ LLM Z3 .For CSP, we use the python-constraint library as the trajectory executor.Each CSP trajectory s i ∈ LLM CSP consists of variable declarations, domain assignments, and constraint definitions.</p>
<p>In addition, we chose Z3 over Prover9 because Z3 not only supports first-order logic (Prover9-FOL) but also natively supports rich theories such as integers and arrays.More detailed description can be found in section C.</p>
<p>PART I: Evaluation across LLMs, Tasks, Trajectories</p>
<p>In PART I, we present a comprehensive evaluation across three dimensions: Models, Trajectory Formats, and Reasoning Task Types.Specifically, we evaluate both Instruct and Thinking models, ranging from 7B to 72B (see Figure 1).For reasoning tasks, we follow the taxonomy introduced in Section 2.1.For trajectory formats, we evaluate three formal languages and natural language, as detailed in Section 2.2.All evaluations are conducted in a zero-shot setting.For formal languages (PoT, Z3, CSP), we apply a three-step self-refinement process during code execution.Detailed evaluation settings are provided in Appendix D.1.</p>
<p>Model Performance for Reasoning Tasks and Trajectory Formats</p>
<p>As shown in Figure 2, the radar chart (Overall+ Fine-grained) illustrates the model's performance under different task types and trajectory formats.</p>
<p>The complete results can be found in Appendix E. Thinking model outperforms Instruct model From the overall part, we can observe that series of Thinking models (e.g., QwQ-32B, etc.) outperform the Instruct series in most tasks, especially in the Inductive and Mixed-Form tasks.The disparities between them reflect that the Thinking mode can better elicit the LLM to provide reliable trajectories for formal reasoning.Previous evaluations (Xu et al., 2025) have demonstrated a similar finding that Instruct models have achieved unsatisfactory results in inductive reasoning, but they do not provide the suggestion that the Thinking model can perform well.</p>
<p>Text outperforms formal languages, except for QwQ-32B Most models outperform formal languages in the Text trajectory format.In the Finegrained section, as the trajectory format shifts from Text to CSP, the radar map coverage area gradually decreases, especially in the bbeh series of subtasks.However, QwQ-32B is the only model that stays ahead in all tasks and trajectories, maintaining a high level of performance in all formal languages.</p>
<p>Formal language performance drops significantly on difficult tasks Models can achieve comparable or even better performance than Text with formal languages in simple tasks (e.g., Z3, CSP in Deducitve-BBH_web), but the performance of formal languages drops off substantially in complex tasks(e.g., Deductive-bbeh_boardgameQA).This phenomenon again suggests that current large models are better at using non-formal languages when expressing complex logic.Possible reasons include: 1) the model training process is dominated by natural language, with a scarcity of formal language samples; and 2) the model lacks augmentation for difficult and complex problems.The performance of text formatting is average, while formal language significantly decreases.It is worth noting that GPT-4o's performance in this area is relatively stable, possibly due to its optimization in data.</p>
<p>Small models perform poorly on formal language Both Instruct and Thinking small models have acceptable overall performance under Text, but when dealing with formal languages, the performance drops rapidly.Taking R1-Distill-Qwen-7B as an example, its performance under the CSP trajectory is even significantly lower than similar Instruct models, indicating that the Thinking mechanism is difficult to effectively support formal lan-guage reasoning at low parameter scales.In addition, in high complexity tasks such as bbeh-time, bbeh-shuffle, etc., the small model is almost completely ineffective in structured trajectories such as Z3 and CSP, and it is difficult to complete the basic logical steps, which shows its serious lack of ability to deal with formal reasoning problems.Overall, all models except QwQ-32B show a continuous performance degradation in the trajectory format change from Text to formal language (PoT, Z3, CSP).This phenomenon suggests that the current mainstream LLMs are more adept at handling natural language tasks, while they are still deficient in formal language reasoning.</p>
<p>Different Reasoning Tasks Prefer Different Trajectory Format</p>
<p>In this section, we use the GPT-4o result as an anchor point to conduct a detailed analysis of how different tasks exhibit varying preferences for trajectory formats.As shown in Figure 3, GPT-4o exhibits diverse preferences across trajectory formats.Below, we summarize the main observations.trajectories to clearly express intermediate states and transformation processes.(Case in Figure 7)</p>
<p>Z3 handles formal and FOL reasoning well.</p>
<p>Z3 format shows a good adaptation to formal logic tasks, especially in tasks with strict logical rules: LogicBench, BBH_formal_fallacies, BBH_logical_deduction.This type of task is essentially convertible to first-order logical expressions, so using an SMT solver (e.g., Z3) as the trajectory language is more suitable.In addition, BBH_geometric_shapes involves spatial reasoning, where the boolean logical expressiveness of Z3 is more advantageous.(Case in Figure 9)</p>
<p>CSP shows advantages in complex constraints CSP format shows advantages in some structured logic tasks, such as BBH_logical_deduction, a result consistent with the findings of Logic-LM (Pan et al., 2023).More interestingly, in ARLSAT, a task derived from the Law School Admission Test, CSP also achieves the optimal result, which contrasts with the previous (Pan et al., 2023) literature's conclusion that Z3 is better suited for this task.This difference may stem from the characteristics of the tasks themselves; in ARLSAT, the stems of the questions typically contain constraints, which are more consistent in form with the way CSPs are expressed.(Case in Figure 9)</p>
<p>Beyond the four dimensions mentioned above, we can observe that execution success rate (Exec Rate) is also a key factor underlying the differences among various forms of language.Moreover, gsm8k achieves its best performance under the Text format, which is inconsistent with findings from previous studies (e.g., Ye et al. (2023); He-Yueya et al. ( 2023)).This discrepancy may be attributed to two factors: 1) Prior work often involves task-specific optimization for mathematical reasoning; 2) Current large language models are trained on substantial amounts of mathematical natural language reasoning data, which enhances their generalization ability in Text formats.</p>
<p>Overall, task trajectory alignment plays a critical role.Different tasks exhibit preferences for specific trajectory formats-some tasks are inherently better suited to certain formal representations, and using inappropriate formats may even hinder model performance.Therefore, when constructing multi-trajectory training or evaluation frameworks, it is important to carefully consider the alignment among task structure, target language, and model capabilities.2.9 0.3 0.5 -2.2 7.9 -0.2 8.5 4.2 8.0 13.4 -1.5 6.8 0.5 7.7 0. 1 -1.7 15.3 -1.9 4.7 22.4 8.2 15.6 -0.8 8.2   -0.4 2.6  7.1 -1.7 2.7  4.5  7.5  9.6 16.8 8.8 -8.6 4.4   -3.3 0.8 -0.5 0.6  8.9  0.3  5.5 -5.9 2.1 33.4 2.5 12.3   -1.2 11.3 0.5  0.6 15.8 0.5  8.1 -4.0 3.1 27.8 28.0 14.8   0.7  1.0  3.5 -1.0 3.4  5.5  3.0 -3.9 3.4 26.2 4 -1.5 -1.6 -1.2 14.6 -4.2 0.3 -1.2 -1.0 4.5  2.4  0.7  4.6   -0.2 0.7  6.0 17.1 -3.1 4.1  1.3 -1.7 8.3 15.3 13.2 10.7   -0.4 -1.7 7.5  8.8  2.6  2.7  9.6 -8.6 7.1  4.5 16.8 4.4   0.7 -1.0 3.0 26.2 1.0  3.4 -3.9 4</p>
<p>Coarse-Grained Generalization Analysis</p>
<p>Significant in-domain improvement The strongest performance gains are observed along the diagonal, indicating that the model benefits most when the training and evaluation data come from the same group.Notably, the improvements for CSP (Train) → CSP (Eval) and Inductive (Train) → Inductive (Eval) reach 28.1 and 22.8, respectively.Combined with observations from Part-I, this can be partially attributed to the relatively low baseline performance of the Qwen2.5-7Bmodel on the CSP and Inductive dimensions, meaning that even a small amount of in-domain data leads to significant improvement.</p>
<p>PoT transfers well, while CSP transfers poorly Outside the diagonal, in figure 5a, PoT migrates well in Text, Z3, and CSP.This might be related to the fact that there is a lot of code data in the pre-training data.CSP, on the other hand, has an effect only on Text and CSP, with significant negative effects on PoT (-15.8) and Z3 (-8.9).This suggests that there may be structural differences among formal languages.</p>
<p>Reasoning types: all exhibit positive transfer</p>
<p>The overall transfer effect is relatively balanced between the different reasoning types (Fig. 5b).The relatively small improvement on Deductive itself may be related to the higher base level of the model on Deductive.</p>
<p>Fine-Grained Generalization Analysis</p>
<p>Deductive-CSP is most easily generalized In Figure 4, all entries in the Deductive-CSP column show improvements.The inclusion of any data contributes positively to its performance.This is mainly because CSP has a relatively low baseline, and the Deductive category contains some relatively simple tasks (BBH_logical_deduction_three from 40 % to 92%).As a result, adding similar data leads to performance gains.</p>
<p>CSP and Z3 transfer well across reasoning types</p>
<p>In Figure 4a, all entries (Ded/Ind/Abd) within the CSP and Z3 blocks show positive gains, indicating that regardless of reasoning type, CSP and Z3 formats can be effectively transferred.</p>
<p>Abductive transfers well across trajectory formats In Figure 4b, all entries (Text/PoT/CSP/Z3) within the Abductive block show improvements, suggesting that regardless of trajectory format, Abductive reasoning can be effectively transferred and improved.First, we extract up to 3,000 samples from all training data.Then, GPT-4o was chosen as the output for teacher model construction.In order to obtain high quality response data, we used Rejection sampling Fine-Tuning (RFT).We used GPT-4o to sample the questions several times and then filtered out those samples whose code was executable and whose final answers are verified to be correct.The statistics of the filtered data are shown in Table 3.The number in parentheses after each model name indicates the amount of added data.</p>
<p>PART III: Enhancing</p>
<p>Main Result</p>
<p>As shown in Table 1, the enhanced model improves accuracy by 3.0% on Text, 7.7% on Z3 (with an 8.3% gain in execution rate), and 17.0% on CSP (from 20.0% to 37.0%, with a 15.9% increase in execution rate).Overall, average accuracy rises from 34.0% to 42.0%, and execution rate from 65.3% to 76.0%.</p>
<p>Beyond outperforming the baseline, our formaldata-enhanced model also surpasses the opensource model Qwen2.5-7B-Instructacross all formats.Qwen2.5-7B-Basew.Formal has a smaller parameter size than Qwen2.5-72B,but the performance gap is narrowed by formal data fine-tuning.This suggests that formal data augmentation can effectively improve the competitiveness of small models in formal reasoning tasks.</p>
<p>Conclusion</p>
<p>In this paper, we provide a comprehensive evaluation of LLMs utilizing various formal languages to solve different categories of logical reasoning tasks.We first develop a systematic evaluation architecture and decompose it into three dimensions.Then, we perform a thorough evaluation across these three dimensions to show whether the current LLMs can excel in formal language utilization.Furthermore, we explore the generalization across multiple formal languages and provide a simple but effective method on the capability enhancement for small language models.</p>
<p>For future directions, on the one hand, we should strive to enhance the model's reasoning capabilities in a balanced manner across different trajectory formats and task types, especially for Instruct models.At the same time, it may be valuable to construct formal language reasoning datasets in a "thinking" style.On the other hand, we can leverage the taskspecific preferences for trajectory formats to further expand the capability boundaries of the model.One approach is to incorporate reasoning results from different trajectory formats as individual voters in a majority voting scheme.Another approach is to introduce multiple symbolic solvers for different reasoning trajectories during the thinking stage of the think model.</p>
<p>Limitations</p>
<p>This work provides a step toward evaluating and enhancing LLMs through formal reasoning formats, but several limitations remain.First, the landscape of LLMs is evolving rapidly.Our experiments focus on a limited set of models available at the time, and newer models may change performance trends.Second, while we include various reasoning types and benchmark datasets, the overall dataset coverage is limited.Our formal data augmentation is applied to a subset of tasks and may not generalize to other domains.Third, we focus on three formal formats, "PoT, Z3, and CSP," due to their executability and popularity.However, this excludes other symbolic systems such as Lean, Prolog, Coq, or SMT-LIB, which future work could explore.Finally, our formal data construction is based on the Instruct model (GPT-4o).With the rise of stronger Thinking models, generating think-style formal data may become more feasible and diverse in the future.</p>
<p>B Details of Datasets</p>
<p>Table 2 provides a comprehensive overview of all datasets used in our study.Each dataset is annotated with its reasoning type (Deductive, Inductive, Abductive, or Mixed-Form), along with the number of evaluation and training examples.We also include the original source for each dataset.</p>
<p>The classification follows our taxonomy introduced in Section 2.1.In particular:</p>
<p>• Deductive datasets include tasks that require formal logical reasoning based on explicit rules or premises.</p>
<p>• Inductive datasets focus on pattern discovery and generalization from limited examples.</p>
<p>• Abductive datasets involve generating plausible explanations under uncertainty.</p>
<p>• Mixed-Form includes tasks with hybrid or ambiguous reasoning types, further grouped into subcategories such as Temporal, NLU, Symbolic, Spatial, Knowledge, and Math.</p>
<p>Some datasets (e.g., BBH and bbeh) are split into finer task categories, each treated independently during evaluation.For large-scale datasets like GSM8K and MATH, we use a subset of examples (denoted by *) to maintain balance across task types.</p>
<p>This dataset collection forms the foundation for our evaluation across models, trajectory formats, and reasoning types.</p>
<p>C Detail of Trajectory Format</p>
<p>We extend the unified trajectory formulation to three specific formal languages: Python (PoT), Z3, and CSP.Each trajectory consists of a sequence of symbolic steps, which are executed by an external engine to compute the final answer.</p>
<p>We denote the model-generated trajectory as:
LLM(Q) = ⟨s 1 , s 2 , . . . , s n ⟩ Exec −−→ A (1)
Where Q is the input query, each s i is a step in a domain-specific language, and A is the final answer produced by executing the trajectory.</p>
<p>Python (PoT) Trajectory</p>
<p>In the Python format, each step s i is a syntactically valid Python statement.The trajectory consists of variable assignments, arithmetic operations, control logic, and ends with a print(A) statement.</p>
<p>The Python trajectory is formalized as:
LLM Python (Q) = (2) ⟨stmt 1 , stmt 2 , . . . , stmt n ,(3)print(A)⟩ Python 3.12 − −−−−− → A (4)
This trajectory is interpreted and executed sequentially using a Python 3.12 interpreter.</p>
<p>Z3 Trajectory</p>
<p>Inspired by Logic-LM (Pan et al., 2023), for Z3, the reasoning trajectory is constructed using the Z3 theorem prover.A typical trajectory includes symbolic variable declarations such as x = Int('x'), followed by logical assertions like s.add(x &gt; 1, x &lt; 5), and ends with solver calls s.check() and s.model() to extract a result.</p>
<p>We represent the Z3 trajectory as:
LLM Z3 (Q) = (5) ⟨Declare, Assert 1 , . . . , Assert k , (6) CheckSat, print(A)⟩ Z3 Solver − −−−− → A (7)
Z3 supports a wide range of built-in logical theories, such as integer arithmetic, arrays, and bitvectors.</p>
<p>CSP Trajectory</p>
<p>Constraint Satisfaction Problems (CSPs) are defined by a triple (X, D, C), where X = {x 1 , . . ., x n } denotes variables, D = {D 1 , . . ., D n } their domains, and C = {C 1 , . . ., C m } the set of constraints.Each constraint C j = ⟨t j , R j ⟩ is defined over a subset of variables and a relation on their domains.The CSP trajectory is modeled as:
LLM CSP (Q) = (8) ⟨ AddVar 1 , . . . , AddVar n , AddConst 1 , . . . , AddConst m , GetSolution, print(A)⟩ python-constraint − −−−−−−−− → A (9)
The execution uses the python-constraint solver.</p>
<p>Variables are added through addVariable(), constraints through addConstraint(), and solutions are obtained via getSolution() or getSolutions().The solver applies standard algorithms such as backtracking and constraint propagation.</p>
<p>While Prover9-FOL supports classical first-order logic, we choose Z3 for its broader practical applicability.Z3 not only supports FOL reasoning but also natively handles richer theories such as integers, arrays, and linear arithmetic.This allows it to express a wider range of constraints found in real-world reasoning tasks.</p>
<p>D Implementation Setups D.1 Evaluations Details</p>
<p>In the inference phase, we use the vLLM (Kwon et al., 2023) framework for deployment.The inference configuration adopts greedy decoding strategy and sets the maximum generation length to 16K tokens.For the evaluation of model output, we adopt Qwen-2.5-72B-Instruct as the model evaluator to score.</p>
<p>D.2 Training Details</p>
<p>In terms of training implementation, we use Megatron-LM as the training framework with the following configurations: a cosine learning rate schedule is adopted with an initial learning rate of 1e-5, a warmup ratio of 0.03, and the learning rate decays to 0; the maximum sequence length is set to 8192, with a global batch size of 128, and the number of training epochs is set to 3. All experiments are completed with Supervised Fine-tuning (SFT) on a computing cluster consisting of 32 NVIDIA A100 GPUs.</p>
<p>E Complete results for different models</p>
<p>As shown in Table 3, we evaluated a total of 31 models across the three parts of this paper.Due to space constraints, we present the results of several representative models here: QwQ-32B (Table 4), GPT-4o (Table 5), Qwen2.5-7B(Table 6), and Qwen2.5-7B-Basew.Formal (Table 7).The complete results are provided in the supplementary files in Excel format.</p>
<p>F Case Study</p>
<p>F.2 From Logic-LM Few-Shot Eval to</p>
<p>Zero-Shot</p>
<p>Logic-LM uses few-shots setting and rule extraction to build "task-specific executable code" for "a particular formal language" and "a particular task".We use zero-shot directly for evaluation.As shown in the case study in Figure 10, both approaches behave similarly and can evaluate the model's formal language reasoning ability.Meanwhile, the zero-shot setting has better generalization, and this paper considers a subset of 66 datasets based on it.</p>
<p>G Prompts</p>
<p>For text, we use questions directly as input to the rubric.For formal languages, we use zero-shot reviews.Prompts are as follows: PoT in Figure 11; Z3 in Figure 12; CSP in Figure 13.The Prompt for evaluating models is in Figure 14.</p>
<p>Case Study for Z3 (BBH_formal_fallcies)</p>
<p>"It is not always easy to see which chemicals are contained in our consumer products.</p>
<p>The following argument pertains to this question: First, vitamin A is an ingredient of LIQUID EYELINER.Second, every ingredient of BC Eye Cream 15 ml is not an ingredient of Mango Lip Butter or not an ingredient of LIQUID EYELINER.Therefore, it is not the case that Vitamin A is an ingredient of BC Eye Cream 15 ml.\"\nIs the argument, given the explicitly stated premises, deductively valid or invalid?Options:</p>
<p>-valid -invalid</p>
<p>GPT-4o Text Answer:</p>
<p>To determine whether the argument is deductively valid, we need to analyze the logical structure of the premises and the conclusion.</p>
<p>The argument is structured as follows:</p>
<ol>
<li>
<p>Premise 1: Vitamin A is an ingredient of LIQUID EYELINER.2. Premise 2: Every ingredient of BC Eye Cream 15 ml is not an ingredient of Mango Lip Butter or not an ingredient of LIQUID EYELINER.</p>
</li>
<li>
<p>Conclusion: Therefore, it is not the case that Vitamin A is an ingredient of BC Eye Cream 15 ml.</p>
</li>
</ol>
<p>To assess the validity, we need to see if the conclusion necessarily follows from the premises.</p>
<p>Premise 2 can be restated in logical terms: For any ingredient \ ( x ), if ( x ) is an ingredient of BC Eye Cream 15 ml, then \ ( x ) is not an ingredient of Mango Lip Butter or \ ( x ) is not an ingredient of LIQUID EYELINER.</p>
<p>Given Premise 1, Vitamin A is an ingredient of LIQUID EYELINER.</p>
<p>For the conclusion to be true, Vitamin A must not be an ingredient of BC Eye Cream 15 ml.</p>
<p>According to Premise 2, if Vitamin A were an ingredient of BC Eye Cream 15 ml, then it would have to be either not an ingredient of Mango Lip Butter or not an ingredient of LIQUID EYELINER.Since we know from Premise 1 that Vitamin A is an ingredient of LIQUID EYELINER, the only way for Premise 2 to hold true is if Vitamin A is not an ingredient of BC Eye Cream 15 ml.</p>
<p>Thus, the conclusion logically follows from the premises, making the argument deductively valid.</p>
<p>-</p>
<p>Case Study for CSP (AR-LSAT)</p>
<p>Context: On each of exactly seven consecutive days (day 1 though day 7), a pet shop features exactly one of three breeds of kitten-Himalayan, Manx, Siamese-and exactly one of three breeds of puppy-Greyhound, Newfoundland, Rottweiler.The following conditions must apply: Greyhounds are featured on day 1.</p>
<p>No breed is featured on any two consecutive days.Any breed featured on day 1 is not featured on day 7. Himalayans are featured on exactly three days, but not on day 1.Rottweilers are not featured on day 7, nor on any day that features Himalayans.</p>
<p>Question</p>
<p>GPT-4o Text Answer:</p>
<p>To solve this problem, we need to analyze the constraints and determine which option could be true if Himalayans are not featured on day 2.</p>
<p>Given constraints:</p>
<ol>
<li>
<p>Greyhounds are featured on day 1.</p>
</li>
<li>
<p>No breed is featured on any two consecutive days.</p>
</li>
<li>
<p>Any breed featured on day 1 is not featured on day 7. 4. Himalayans are featured on exactly three days, but not on day 1. 5. Rottweilers are not featured on day 7, nor on any day that features Himalayans.</p>
</li>
</ol>
<p>Let's break down the possibilities: Since Greyhounds are featured on day 1, they cannot be featured on day 7. Himalayans are featured on exactly three days, but not on day 1 or day 2 (as per the question).</p>
<p>Rottweilers cannot be featured on any day that f eatures Himalayans, and they cannot be featured on day 7. Now, let's consider each option: ... B: Siamese are featured on day 4.This is possible.If Himalayans are on days 3, 5, and 6, Siamese can be on day 4. C: Rottweilers are featured on day 5.This is not possible because if Himalayans are on day 5, Rottweilers cannot be on the same day.D: Himalayans are featured on day 6.</p>
<p>-This is possible.Himalayans can be on days 3, 5, and 6.E: Greyhounds are featured on day 7.</p>
<p>-This is not possible because Greyhounds cannot be on day 7 as they are on day 1.</p>
<p>Therefore, the options that could be true are A, B, and D. However, since the question asks for one option, and considering the constraints, option D: "Himalayans are featured on day 6" is a straightforward possibility given the conditions.</p>
<p>GPT-4o CSP Answer:
!= k2, \n(f'kitten_day_{day}', f'kitten_day_{day+1}'))
problem.addConstraint(lambda p1, p2: p1 != p2, \n(f'puppy_day_{day}', f'puppy_day_{day+1}'))</p>
<h1>Any breed featured on day 1 is not featured on day 7 problem.addConstraint(lambdak1, k7: k1 != k7, \n('kitten_day_1', 'kitten_day_7')) problem.addConstraint(lambdap1, p7: p1 != p7, ('puppy_day_1', 'puppy_day_7')) # Himalayans are featured on exactly three days, but not on day 1 problem.addConstraint(lambdak: k != 'Himalayan', ('kitten_day_1',)) # Custom constraint to ensure Himalayans are featured on exactly three days def himalayan_count(*args): return args.count('Himalayan')== 3 problem.addConstraint(himalayan_count,('kitten_day_2','kitten_day_3',\n 'kitten_day_4','kitten_day_5','kitten_day_6','kitten_day_7')) # Rottweilers are not featured on day 7, nor on any day that features Himalayans problem.addConstraint(lambdap: p != 'Rottweiler', ('puppy_day_7',)) for day in days: problem.addConstraint(lambdak, p: not (k == 'Himalayan' and p == 'Rottweiler'), (f'kitten_day_{day}', f'puppy_day_{day}'))</h1>
<h1>Additional condition: Himalayans are not featured on day 2 problem.addConstraint(lambdak: k != 'Himalayan', ('kitten_day_2',))</h1>
<h1>Solve the problem and print the results solutions = problem.getSolutions()Please give the python code in the following format:</h1>
<h1></h1>
<p><code>python</code> and print the final result.",Please give the python code in the following format: <code>python\n</code> and print the final result.Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below.Your evaluation should consider the correctness.You will be given a reference answer and the assistant's answer.Begin your evaluation by comparing the assistant's answer with the reference answer, you must identify whether the response is correct or wrong by strictly following this format: "<correct or wrong>", for example: " <correct>" or "<wrong>".</p>
<p>[Question] {question}</p>
<p>Figure 1 :
1
Figure 1: Evaluation framework with three specific dimensions: spectrum of LLMs, taxonomy of logical reasoning tasks, and format of trajectories.</p>
<p>Figure 2 :
2
Figure 2: Radar plots illustrating the performance (%) of multiple LLMs across different reasoning task types (Deductive, Inductive, Abductive, Mixed Form) and trajectory formats (Text, PoT, Z3, CSP).Overall (top 1 × 4) shows aggregated performance by reasoning type and format.Fine-grained (below 4 × 4) present fine-grained results on individual tasks</p>
<p>Figure 3 :
3
Figure 3: Preferred reasoning task performance across different trajectory formats (Text, PoT, Z3, CSP) in GPT-4o results.Each subplot shows task accuracy under different formats, with execution rate (Exec Rate) plotted as a black line.The highlighted bars represent the most preferred trajectory format for each task.</p>
<p>Fine-grained by Reasoning Type.</p>
<p>Figure 4 :
4
Figure 4: Generalization performance across fine-grained (task type × format) configurations.Each cell shows the performance gain (∆) from training on the row configuration and evaluating on the column configuration</p>
<p>Figure 5 :
5
Figure 5: Generalization performance across reasoning types and trajectory formats (coarse-grained analysis).Each cell reports the performance gain (∆) when training on the row group and evaluating on the column group.</p>
<p>F. 1
1
Case for PART I We give cases where Text (Fig 6), PoT(Fig 7), Z3(Fig 8) and CSP(Fig 9) specialize in each case to show their strengths.</p>
<p>Figure 8 :
8
Figure 8: Case for Z3.Z3 (which, in this context, incorporates the first-order logic reasoning capabilities of Prover9) excels at solving formal first-order logic problems.Return section 3.2</p>
<p>are featured on day 3. \nB: Siamese are featured on day 4. \nC: Rottweilers are featured on day 5. D: Himalayans are featured on day 6.\nE: Greyhounds are featured on day 7.</p>
<p>Figure 11 :
11
Figure 11: Prompt for PoT</p>
<p>Figure 12 :
12
Figure 12: Prompt for Z3</p>
<p>Figure 13 :
13
Figure 13: Prompt for CSP</p>
<p>[</p>
<p>The Start of Reference Answer] {answer} [The End of Reference Answer] [The Start of Assistant's Answer] {completion} [The End of Assistant's Answer]</p>
<p>Figure 14 :
14
Figure 14: Prompt for Model Eval</p>
<p>Table 1 :
1
Performance of LLM on different trajectory formats before and after formal data enhancement.Accuracy (Acc) and execution rate (Exec Rate) are reported for text, PoT, Z3, and CSP formats.Qwen2.5-7B-Baselinedenotes the baseline model trained with general data only; Qwen2.5-7B-Basew.Formal denotes the augmented model trained with a mixture of formal language data.Improvements after augmentation are shown in green.
TextPoTZ3CSPAvgModelAccAccExec RateAccExec RateAccExec RateAccExec RateGPT-4o66.764.091.554.587.453.083.9859.087.6Qwen2.5-7B-Instruct52.337.078.633.070.025.052.137.066.9Qwen2.5-72B-Instruct63.454.085.142.579.643.075.251.080.0Qwen2.5-7B-Baseline49.740.075.427.168.220.052.234.065.3Qwen2.5-7B-Base w.Formal 52.7 +3.0 44.0 +4.083.5 +8.134.8 +7.776.5 +8.337.0 +17.068.1 +15.942.0 +8.076.0 +10.7</p>
<p>Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Jiahai Wang, Jian Yin, Ming Zhou, and Nan Duan.2021.Ar-lsat: Investigating analytical reasoning of text.arXive-prints,pages arXiv-2104.al., 2022), shuffled objects (Suzgun et al., 2022), and web of lies (Suzgun et al., 2022).Other datasets like bAbI (Weston et al., 2015), CLUTRR (Sinha et al., 2019), α-NLI (Zhao et al., 2021), Ab-ductiveRules (Bhagavatula et al., 2020), LogiQA(Liu et al., 2020), and gsm8k (Cobbe et al., 2021)target specific reasoning types such as deductive, inductive, abductive, temporal, spatial, and mathematical reasoning.The variety in these datasets and their annotations highlights the multifaceted nature of complex reasoning and the ongoing efforts to evaluate and enhance LLMs in this domain.
A Related WorkA.1 Symbolic Solver Enhances LLMReasoningThe integration of symbolic solvers with large lan-guage models (LLMs) has emerged as a promisingapproach to enhance logical reasoning. Early ef-forts focused on translating natural language to first-order logic (FOL), exemplified by the creation ofthe MALLS dataset and the LogicLLaMA model,which demonstrated improved NL-to-FOL trans-lation (Yang et al., 2023a). The Logic-LM frame-work further explored this direction by employingdifferent formal languages and solvers tailored tospecific reasoning tasks, such as FOL with Prover9,CSP solvers for constraint satisfaction, and Z3 forSMT problems (Pan et al., 2023). SATLM intro-duced declarative prompting to generate task spec-ifications in logical formulas for SAT solvers (Yeet al., 2023), while LINC utilized LLMs for se-mantic parsing into FOL, offloading inference totheorem provers (Olausson et al., 2023). Subse-quent research investigated strategies for improv-ing NL-to-FOL translation through data generationand fine-tuning (Xiong et al., 2024), multi-step re-finement of symbolic formulations (Wang et al.,2024), and the impact of pre-training data, includ-ing programming languages, on logical inference(Uchiyama et al., 2023). Frameworks like VERUS-LM aimed for versatility by supporting various rea-soning tasks with a clear separation of knowledgeand queries (Callewaert et al., 2025).A.2 Complex Logical Reasoning TasksEvaluating the logical reasoning capabilities ofLLMs necessitates challenging and diverse datasetsthat probe various aspects of inference. FOLIO, an-notated with first-order logic, focuses on complexlogical reasoning in natural language (Han et al.,2024). ProntoQA utilizes logic programming andemphasizes chain-of-thought reasoning (Saparovand He, 2023), while LogicBench covers proposi-Theo X Olausson, Alex Gu, Benjamin Lipkin, Cede-tional, first-order, and non-monotonic logic with agao E Zhang, Armando Solar-Lezama, Joshua B focus on single inference rules (Parmar et al., 2023).Tenenbaum, and Roger Levy. 2023. Linc: A neu-BOARDGAMEQA assesses reasoning with contra-rosymbolic approach for logical reasoning by com-dictory information and preferences (Kazemi et al., bining language models with first-order logic provers. In Proceedings of the 2023 Conference on Empiri-2023), and AR-LSAT tests analytical reasoningcal Methods in Natural Language Processing, pages skills using logic constraints (Zhong et al., 2022).5153-5176.
Liangming Pan, Alon Albalak, Xinyi Wang, and William Yang Wang.2023.Logic-lm: Empowering large language models with symbolic solvers for The BIG-Bench Hard (BBH) benchmark includes a wide array of challenging tasks like Boolean Expressions(Suzgun et al., 2022), formal fallacies(Suzgun et al., 2022), logical deduction (Suz-gun et</p>
<p>Table 2 :
2
Complex Logical Reasoning data categorization, data statistics, and sources.</p>
<p>Check which option could be true
TextPoTZ3CSPAVGfor solution in solutions: if solution['kitten_day_3'] == 'Manx': print("A: Manx are featured on day 3.") if solution['kitten_day_4'] == 'Siamese': print("B: Siamese are featured on day 4.") if solution['puppy_day_5'] == 'Rottweiler': print("C: Rottweilers are featured on day 5.") if solution['kitten_day_6'] == 'Himalayan': print("D: Himalayans are featured on day 6.") if solution['puppy_day_7'] == 'Greyhound': print("E: Greyhounds are featured on day 7.") ACC ACC Exec_Rate ACC Exec_Rate ACC Exec_Rate ACC Exec_Rate ❌ 75.0 68.6 85.1 61.9 79.4 65.1 82.2 67.6 82.2 94.0 94.0 100.0 91.0 99.3 94.0 100.0 93.3 99.8 99.6 97.8 100.0 99.4 100.0 98.8 100.0 98.9 100.0 82.9 85.6 100.0 86.4 100.0 85.3 100.0 85.1 100.0 78.5 79.3 99.9 75.1 100.0 69.6 100.0 75.6 100.0 92.2 91.3 100.0 83.0 97.0 89.1 100.0 88.9 99.0 96.4 98.8 100.0 94.8 100.0 99.2 100.0 97.3 100.0 57.0 41.5 53.5 30.0 36.0 42.5 58.5 42.8 49.3 100.0 99.2 100.0 99.6 99.6 98.8 100.0 99.4 99.9 44.5 15.5 35.5 2.5 5.5 8.9 11.4 17.9 17.5 100.0 100.0 100.0 99.6 100.0 98.0 100.0 99.4 100.0 99.2 99.6 100.0 100.0 100.0 100.0 100.0 99.7 100.0 100.0 100.0 100.0 99.2 99.6 99.2 100.0 99.6 99.9 54.5 55.0 99.0 35.0 73.0 49.5 87.5 48.5 86.5 100.0 100.0 100.0 98.8 99.6 98.0 100.0 99.2 99.9 BBH_tracking_shuffled_objects_seven_objects 100.0 100.0 Average FOLIO ProntoQA logicbenchBQA BoardgameQA ARLSAT BBH_boolean_expressions bbeh_boolean_expressions BBH_formal_fallacies bbeh_zebra_puzzles BBH_logical_deduction_five_objects BBH_logical_deduction_seven_objects BBH_logical_deduction_three_objects bbeh_boardgame_qa BBH_tracking_shuffled_objects_five_objects 100.0 96.8 100.0 99.2 100.0 99.0 100.0 BBH_tracking_shuffled_objects_three_objects 100.0 100.0 100.0 100.0 100.0 99.2 100.0 99.8 100.0 bbeh_shuffled_objects 41.5 0.5 2.0 3.5 10.0 3.0 9.5 12.1 7.2 BBH_web_of_lies 92.8 98.8 100.0 98.0 99.6 99.2 100.0 97.2 99.9 bbeh_web_of_lies 58.0 37.5 43.5 12.0 17.0 21.5 24.5 32.3 28.3 bAbI15 99.3 92.8 100.0 84.1 98.1 92.6 99.9 92.2 99.3 NeuLRdeductive 99.9 97.3 100.0 80.9 98.2 95.8 100.0 93.5 99.4 clutrr 78.8 73.3 100.0 60.1 94.2 71.0 98.7 70.8 97.6 bAbI16 85.5 91.8 100.0 92.1 100.0 89.7 100.0 89.8 100.0 NeuLRinductive 76.3 73.3 99.9 90.1 99.6 80.7 99.8 80.1 99.8 anli 86.8 86.9 100.0 81.3 99.9 85.9 99.9 85.2 99.9 AbductionRules 68.8 71.5 100.0 45.5 98.8 62.8 94.0 62.2 97.6 BBH_causal_judgement 64.2 64.7 100.0 59.4 100.0 64.2 100.0 63.1 100.0 bbeh_causal_understanding 62.0 53.5 99.5 46.5 90.5 49.0 94.5 52.8 94.8 NeuLRabductive 26.0 26.9 99.9 9.9 95.7 15.1 94.1 19.5 96.6 logicqa 86.5 82.9 100.0 77.9 99.6 80.5 99.9 82.0 99.8 BBH_date_understanding 96.8 94.8 100.0 88.0 98.8 89.6 100.0 92.3 99.6 bbeh_time_arithmetic 86.5 79.5 87.5 42.5 50.5 61.5 72.5 67.5 70.2 BBH_temporal_sequences 100.0 99.6 100.0 91.6 99.2 97.2 99.2 97.1 99.5 bbeh_temporal_sequence 52.5 0.0 0.5 0.0 0.0 1.0 1.5 13.4 0.7 BBH_disambiguation_qa 48.0 54.0 100.0 38.8 100.0 46.4 100.0 46.8 100.0 bbeh_disambiguation_qa 58.3 50.8 97.5 40.8 82.5 51.7 86.7 50.4 88.9 BBH_hyperbaton 100.0 100.0 100.0 99.6 100.0 96.4 100.0 99.0 100.0 bbeh_hyperbaton 38.0 26.5 56.5 19.0 35.0 18.0 52.0 25.4 47.8 BBH_ruin_names 80.0 84.4 100.0 78.4 96.4 82.8 100.0 81.4 98.8 bbeh_nycc 15.0 8.5 72.0 11.5 82.5 10.5 75.0 11.4 76.5 BBH_salient_translation_error_detection 76.8 74.8 100.0 76.0 99.6 75.6 99.6 75.8 99.7 bbeh_linguini 46.0 22.0 79.5 24.5 65.5 17.5 51.0 27.5 65.3 BBH_snarks 93.8 91.6 98.9 90.5 100.0 92.1 100.0 92.0 99.6 bbeh_sarc_triples 32.0 39.0 100.0 25.5 71.5 19.0 83.0 28.9 84.8 BBH_dyck_languages 91.6 73.6 83.2 72.4 92.0 84.0 98.8 80.4 91.3 bbeh_dyck_languages 49.0 30.0 79.5 22.0 69.0 28.0 67.5 32.3 72.0 BBH_word_sorting 98.8 100.0 100.0 20.4 21.2 79.2 88.0 74.6 69.7 bbeh_word_sorting 77.0 77.5 92.5 63.5 76.0 40.0 62.0 64.5 76.8 BBH_geometric_shapes 80.0 80.8 100.0 82.0 99.2 78.4 100.0 80.3 99.7 bbeh_geometric_shapes 40.5 9.0 19.5 12.5 25.5 14.5 32.0 19.1 25.7 BBH_navigate 97.6 98.4 100.0 93.6 99.6 95.2 99.6 96.2 99.7 bbeh_spatial_reasoning 43.0 40.5 53.0 32.5 48.0 30.5 47.5 36.6 49.5 BBH_penguins_in_a_table 99.3 99.3 100.0 95.2 95.2 99.3 100.0 98.3 98.4 bbeh_buggy_tables 25.0 16.5 28.5 8.0 16.0 3.5 15.0 13.3 19.8 BBH_movie_recommendation 70.0 70.0 100.0 63.6 97.6 65.2 100.0 67.2 99.2 bbeh_movie_recommendation 59.5 40.0 79.0 28.5 56.5 22.5 47.5 37.6 61.0 BBH_sports_understanding 81.6 80.4 100.0 77.2 99.6 80.0 100.0 79.8 99.9 bbeh_sportqa 53.0 15.0 23.0 17.0 34.0 17.5 34.5 25.6 30.5 gsm8k 96.4 96.7 99.9 93.2 98.8 94.5 99.5 95.2 99.4 MATH 96.5 93.1 99.4 70.7 88.3 87.1 99.0 86.9 95.6 BBH_multistep_arithmetic_two 100.0 100.0 100.0 100.0 100.0 99.6 99.6 99.9 99.9 Output: B: Siamese are featured on day 4.Dataset bbeh_multistep_arithmetic 53.0 38.0 40.5 20.5 29.5 22.7 32.8 33.6 34.3BBH_object_counting100.0 100.0100.098.899.698.499.6 99.399.7bbeh_object_counting58.031.085.540.571.022.561.0 38.072.5BBH_reasoning_about_colored_objects100.098.8100.095.699.294.499.2 97.299.5bbeh_object_properties31.55.57.017.529.539.051.5 23.429.3
✅Figure9: Case for CSP.The problems in ARLSAT involve numerous conditional constraints, which closely resemble the modeling approach used in Constraint Satisfaction Problems (CSPs).Return to section 3.2</p>
<p>Table 4 :
4</p>
<p>TextPoTZ3CSPAVGDatasetACC ACC Exec_Rate ACC Exec_Rate ACC Exec_Rate ACC Exec_RateAverage66.763.591.5 54.587.4 52.884.0 59.487.6FOLIO92.588.1100.0 73.988.8 67.298.5 80.495.8ProntoQA100.095.8100.0 80.299.8 93.299.4 92.399.7logicbenchBQA72.370.599.8 76.3100.0 63.399.8 70.699.9BoardgameQA59.766.0100.0 63.597.9 60.298.1 62.498.7ARLSAT40.950.490.4 59.183.0 67.885.2 54.686.2BBH_boolean_expressions99.6 100.0100.0 89.2100.0 76.496.4 91.398.8bbeh_boolean_expressions59.551.555.01.52.0 56.560.0 42.339.0BBH_formal_fallacies88.476.4100.0 90.4100.0 62.099.6 79.399.9bbeh_zebra_puzzles38.06.519.08.549.03.04.0 14.024.0BBH_logical_deduction_five_objects93.294.099.6 87.699.6 96.4100.0 92.899.7BBH_logical_deduction_seven_objects88.888.0100.0 84.8100.0 96.8100.0 89.6100.0BBH_logical_deduction_three_objects99.295.2100.0 92.899.2 99.6100.0 96.799.7bbeh_boardgame_qa37.035.590.5 37.590.0 24.565.0 33.681.8BBH_tracking_shuffled_objects_five_objects98.4 100.0100.0 82.0100.0 36.881.2 79.393.7BBH_tracking_shuffled_objects_seven_objects 100.099.6100.0 82.4100.0 41.675.2 80.991.7BBH_tracking_shuffled_objects_three_objects 100.0 100.0100.0 55.6100.0 53.674.4 77.391.5bbeh_shuffled_objects29.559.083.5 36.077.5 23.549.0 37.070.0BBH_web_of_lies96.491.2100.0 96.4100.0 96.4100.0 95.1100.0bbeh_web_of_lies33.511.051.5 11.020.5 11.514.5 16.828.8bAbI1599.698.7100.0 76.297.9 95.999.8 92.699.2NeuLRdeductive99.897.0100.0 55.293.9 87.297.8 84.897.2clutrr52.744.2100.0 44.695.7 35.684.4 44.393.4bAbI1651.893.4100.0 64.498.8 44.1100.0 63.499.6NeuLRinductive60.341.2100.0 21.199.37.997.2 32.698.8anli88.887.6100.0 73.499.9 81.6100.0 82.9100.0AbductionRules88.586.6100.0 84.3100.0 41.263.8 75.287.9BBH_causal_judgement69.073.8100.0 61.0100.0 64.7100.0 67.1100.0bbeh_causal_understanding52.052.5100.0 50.099.0 44.596.5 49.898.5NeuLRabductive29.015.098.4 19.892.65.288.4 17.393.1logicqa76.073.299.6 61.797.8 72.798.6 70.998.7BBH_date_understanding94.082.0100.0 70.498.8 84.4100.0 82.799.6bbeh_time_arithmetic63.543.574.0 36.066.0 35.073.5 44.571.2BBH_temporal_sequences99.689.299.6 64.498.0 98.099.6 87.899.1bbeh_temporal_sequence5.52.087.51.052.02.581.52.873.7BBH_disambiguation_qa53.650.0100.0 38.0100.0 36.8100.0 44.6100.0bbeh_disambiguation_qa63.354.298.3 44.296.7 70.893.3 58.196.1BBH_hyperbaton92.888.4100.0 94.899.6 92.099.2 92.099.6bbeh_hyperbaton30.513.087.0 28.090.0 17.042.5 22.173.2BBH_ruin_names86.486.0100.0 80.898.4 84.499.2 84.499.2bbeh_nycc21.511.577.53.539.07.079.0 10.965.2BBH_salient_translation_error_detection73.280.0100.0 84.4100.0 76.8100.0 78.6100.0bbeh_linguini35.025.597.0 23.092.0 20.073.0 25.987.3BBH_snarks89.986.5100.0 75.3100.0 74.7100.0 81.6100.0bbeh_sarc_triples30.037.597.5 17.545.5 35.585.0 30.176.0BBH_dyck_languages90.494.4100.0 75.298.8 35.276.0 73.891.6bbeh_dyck_languages17.56.586.57.596.0 10.088.0 10.490.2BBH_word_sorting97.699.2100.0 41.294.0 71.684.0 77.492.7bbeh_word_sorting46.553.089.5 43.583.5 30.061.5 43.378.2BBH_geometric_shapes71.277.299.6 83.2100.0 77.6100.0 77.399.9bbeh_geometric_shapes38.516.065.0 35.080.5 33.569.5 30.871.7BBH_navigate99.298.0100.0 78.4100.0 74.498.0 87.599.3bbeh_spatial_reasoning11.519.081.0 13.580.5 14.572.0 14.677.8BBH_penguins_in_a_table98.6 100.0100.0 96.699.3 82.297.3 94.498.9bbeh_buggy_tables21.019.049.5 16.554.0 19.540.0 19.047.8BBH_movie_recommendation77.263.6100.0 67.299.2 76.899.6 71.299.6bbeh_movie_recommendation60.526.099.5 34.085.0 27.585.0 37.089.8BBH_sports_understanding86.887.6100.0 54.4100.0 60.8100.0 72.4100.0bbeh_sportqa29.558.5100.0 23.589.5 26.085.5 34.491.7gsm8k96.794.698.8 89.398.3 87.699.3 92.198.8MATH81.369.388.7 57.991.2 57.783.5 66.687.8BBH_multistep_arithmetic_two98.8 100.0100.0 81.299.2 63.6100.0 85.999.7bbeh_multistep_arithmetic27.01.523.01.013.51.016.07.617.5BBH_object_counting95.699.6100.0 98.8100.0 88.0100.0 95.5100.0bbeh_object_counting16.014.098.5 11.097.5 13.097.0 13.597.7BBH_reasoning_about_colored_objects97.697.6100.0 94.498.8 89.299.6 94.799.5bbeh_object_properties10.55.550.59.552.51.038.56.647.2</p>
<p>Table 5 :
5</p>
<p>TextPoTZ3CSPAVGDatasetACC ACC Exec_Rate ACC Exec_Rate ACC Exec_Rate ACC Exec_RateAverage52.3 36.978.6 33.070.0 24.852.1 36.766.9FOLIO88.8 85.1100.0 59.787.3 59.084.3 73.190.5ProntoQA99.4 83.498.6 57.287.6 38.454.8 69.680.3logicbenchBQA71.6 52.1100.0 39.498.5 37.879.8 50.292.8BoardgameQA54.3 52.899.1 38.289.5 29.273.8 43.687.5ARLSAT25.2 36.593.9 22.251.38.722.6 23.255.9BBH_boolean_expressions97.6 99.6100.0 45.271.6 50.895.2 73.388.9bbeh_boolean_expressions70.51.51.50.50.5 10.010.5 20.64.2BBH_formal_fallacies69.6 50.8100.0 52.891.6 50.088.4 55.893.3bbeh_zebra_puzzles34.50.53.55.020.50.00.0 10.08.0BBH_logical_deduction_five_objects66.8 56.0100.0 46.869.2 64.895.2 58.688.1BBH_logical_deduction_seven_objects66.0 55.2100.0 47.267.6 70.894.8 59.887.5BBH_logical_deduction_three_objects89.6 74.4100.0 48.873.2 72.492.4 71.388.5bbeh_boardgame_qa33.0 18.546.57.020.50.55.0 14.824.0BBH_tracking_shuffled_objects_five_objects84.83.6100.0 34.882.0 16.863.6 35.081.9BBH_tracking_shuffled_objects_seven_objects 85.25.2100.0 43.282.0 15.668.4 37.383.5BBH_tracking_shuffled_objects_three_objects89.20.4100.0 35.676.4 22.058.4 36.878.3bbeh_shuffled_objects59.54.026.52.012.51.54.0 16.814.3BBH_web_of_lies81.2 59.2100.0 78.494.4 66.874.8 71.489.7bbeh_web_of_lies9.04.013.01.05.00.53.53.67.2bAbI1523.7 54.399.9 29.190.7 16.064.2 30.884.9NeuLRdeductive91.9 60.496.5 20.477.07.741.8 45.171.8clutrr17.7 26.499.9 14.282.8 12.160.0 17.680.9bAbI1623.7 55.899.9 31.391.6 14.863.5 31.485.0NeuLRinductive7.48.896.5 16.191.1 14.453.3 11.780.3anli77.7 78.899.8 59.895.1 55.683.7 68.092.9AbductionRules88.3 50.681.4 34.841.8 23.937.0 49.453.4BBH_causal_judgement51.9 54.0100.0 37.492.5 40.685.6 46.092.7bbeh_causal_understanding45.0 39.098.0 26.582.0 26.569.0 34.383.0NeuLRabductive20.8 12.983.5 22.052.58.221.6 16.052.5logicqa68.2 64.898.2 54.995.9 40.882.5 57.292.2BBH_date_understanding84.8 32.0100.0 33.673.6 38.484.8 47.286.1bbeh_time_arithmetic30.53.032.09.052.5 10.546.0 13.343.5BBH_temporal_sequences83.6 67.2100.0 48.485.6 54.072.8 63.386.1bbeh_temporal_sequence5.00.034.50.041.50.05.01.327.0BBH_disambiguation_qa41.2 59.2100.0 37.293.6 37.288.0 43.793.9bbeh_disambiguation_qa45.8 29.290.0 35.881.7 15.844.2 31.771.9BBH_hyperbaton68.0 70.0100.0 53.694.4 32.457.6 56.084.0bbeh_hyperbaton0.51.522.01.027.00.00.50.816.5BBH_ruin_names53.2 36.0100.0 28.482.8 10.439.2 32.074.0bbeh_nycc10.58.594.07.059.04.541.57.664.8BBH_salient_translation_error_detection47.2 10.0100.0 30.458.0 21.660.0 27.372.7bbeh_linguini18.0 16.081.56.073.54.552.0 11.169.0BBH_snarks77.5 31.5100.0 49.497.2 41.084.8 49.994.0bbeh_sarc_triples16.0 12.591.5 12.074.55.522.5 11.562.8BBH_dyck_languages83.2 39.2100.0 24.876.8 14.840.0 40.572.3bbeh_dyck_languages4.51.526.08.057.50.526.03.636.5BBH_word_sorting32.4 97.2100.0 61.286.06.032.4 49.272.8bbeh_word_sorting21.0 10.058.5 23.076.0 10.527.5 16.154.0BBH_geometric_shapes62.0 80.4100.0 48.486.0 55.289.2 61.591.7bbeh_geometric_shapes28.5 19.553.5 30.098.5 30.065.0 27.072.3BBH_navigate84.0 50.4100.0 43.686.0 29.650.8 51.978.9bbeh_spatial_reasoning6.01.522.04.027.54.019.03.922.8BBH_penguins_in_a_table91.1 56.2100.0 63.078.8 24.748.6 58.775.8bbeh_buggy_tables32.50.06.00.55.50.02.08.34.5BBH_movie_recommendation63.2 42.0100.0 28.872.4 10.042.0 36.071.5bbeh_movie_recommendation33.0 23.583.5 26.584.03.516.0 21.661.2BBH_sports_understanding74.0 58.0100.0 55.699.2 48.495.6 59.098.3bbeh_sportqa17.0 35.082.0 19.559.54.017.5 18.953.0gsm8k93.0 17.121.3 81.196.5 58.981.3 62.566.4MATH76.7 42.3100.0 40.774.7 39.971.1 49.981.9BBH_multistep_arithmetic_two94.0 98.8100.0 98.499.6 48.482.0 84.993.9bbeh_multistep_arithmetic6.50.00.00.57.00.03.01.83.3BBH_object_counting56.0 82.8100.0 86.495.6 48.871.2 68.588.9bbeh_object_counting18.00.025.50.063.00.531.04.639.8BBH_reasoning_about_colored_objects79.6 54.8100.0 68.484.4 46.071.6 62.285.3bbeh_object_properties21.00.525.02.037.00.022.55.928.2</p>
<p>Table 6 :
6</p>
<p>TextPoTZ3CSPAVGDatasetACC ACC Exec_Rate ACC Exec_Rate ACC Exec_Rate ACC Exec_RateAverage52.743.983.5 34.876.5 37.068.1 42.176.0FOLIO83.678.499.3 71.6100.0 61.297.8 73.799.0ProntoQA99.084.899.4 54.690.2 81.293.8 79.994.5logicbenchBQA80.170.4100.0 72.399.1 64.299.6 71.899.6BoardgameQA68.850.094.9 50.997.8 46.592.3 54.195.0ARLSAT23.931.785.7 30.968.7 30.061.3 29.171.9BBH_boolean_expressions96.898.8100.0 42.074.8 64.499.2 75.591.3bbeh_boolean_expressions70.00.053.03.04.0 22.533.0 23.930.0BBH_formal_fallacies68.058.4100.0 61.2100.0 54.898.0 60.699.3bbeh_zebra_puzzles39.012.546.5 13.539.58.022.0 18.336.0BBH_logical_deduction_five_objects61.657.6100.0 62.894.0 77.696.8 64.996.9BBH_logical_deduction_seven_objects50.444.0100.0 57.299.6 70.083.6 55.494.4BBH_logical_deduction_three_objects84.875.6100.0 74.891.2 92.094.8 81.895.3bbeh_boardgame_qa31.50.084.5 12.537.0 18.046.0 15.555.8BBH_tracking_shuffled_objects_five_objects78.8 100.0100.02.499.24.428.0 46.475.7BBH_tracking_shuffled_objects_seven_objects 71.2 100.0100.02.4100.02.422.8 44.074.3BBH_tracking_shuffled_objects_three_objects80.899.6100.08.0100.05.216.8 48.472.3bbeh_shuffled_objects42.06.019.59.523.0 10.523.0 17.021.8BBH_web_of_lies85.663.2100.0 77.2100.0 58.064.0 71.088.0bbeh_web_of_lies12.513.051.51.09.52.57.07.322.7bAbI1599.196.197.3 89.699.1 89.298.5 93.598.3NeuLRdeductive92.185.799.0 37.184.9 49.886.7 66.290.2clutrr46.830.399.9 34.499.5 46.099.8 39.499.7bAbI1693.096.7100.0 78.499.6 83.9100.0 88.099.9NeuLRinductive8.28.6100.0 18.098.6 14.193.8 12.297.5anli84.176.7100.0 69.7100.0 80.399.9 77.7100.0AbductionRules91.085.799.8 47.795.0 48.381.1 68.292.0BBH_causal_judgement55.161.5100.0 55.6100.0 49.7100.0 55.5100.0bbeh_causal_understanding42.50.087.0 43.096.5 40.095.5 31.493.0NeuLRabductive12.015.493.8 17.384.31.027.4 11.468.5logicqa63.853.899.5 42.498.9 45.197.1 51.398.5BBH_date_understanding74.054.4100.0 40.497.2 55.696.8 56.198.0bbeh_time_arithmetic32.010.559.57.543.5 10.545.5 15.149.5BBH_temporal_sequences30.042.898.8 33.698.4 36.076.0 35.691.1bbeh_temporal_sequence9.00.042.50.020.00.030.52.331.0BBH_disambiguation_qa35.646.0100.0 41.299.2 64.099.2 46.799.5bbeh_disambiguation_qa42.50.043.3 34.290.0 61.7100.0 34.677.8BBH_hyperbaton72.066.891.2 66.4100.0 82.096.8 71.896.0bbeh_hyperbaton1.00.064.03.033.52.018.01.538.5BBH_ruin_names37.232.4100.0 28.098.8 39.694.4 34.397.7bbeh_nycc8.00.062.51.595.55.072.53.676.8BBH_salient_translation_error_detection44.427.6100.0 38.499.6 26.8100.0 34.399.9bbeh_linguini18.00.053.52.579.0 10.563.07.865.2BBH_snarks73.650.6100.0 51.198.9 41.6100.0 54.299.6bbeh_sarc_triples16.512.096.5 16.543.0 34.0100.0 19.879.8BBH_dyck_languages64.438.897.6 14.496.83.231.2 30.275.2bbeh_dyck_languages2.50.024.54.547.00.518.01.929.8BBH_word_sorting20.896.4100.0 25.286.84.431.6 36.772.8bbeh_word_sorting20.512.573.56.584.51.515.0 10.357.7BBH_geometric_shapes53.613.6100.0 29.299.2 60.899.2 39.399.5bbeh_geometric_shapes20.01.568.5 17.058.5 42.573.0 20.366.7BBH_navigate88.875.296.0 65.294.0 55.297.2 71.195.7bbeh_spatial_reasoning7.59.541.57.532.03.521.57.031.7BBH_penguins_in_a_table84.398.0100.0 82.298.6 50.785.6 78.894.8bbeh_buggy_tables86.50.028.01.013.50.07.0 21.916.2BBH_movie_recommendation75.247.299.2 21.290.0 70.8100.0 53.696.4bbeh_movie_recommendation49.00.052.5 11.071.09.074.5 17.366.0BBH_sports_understanding73.656.099.2 62.8100.0 53.699.2 61.599.5bbeh_sportqa11.542.587.54.012.03.011.5 15.337.0gsm8k90.689.2100.0 74.899.0 60.095.4 78.698.1MATH71.660.892.3 33.168.5 36.485.6 50.582.1BBH_multistep_arithmetic_two96.498.0100.0 95.2100.0 85.2100.0 93.7100.0bbeh_multistep_arithmetic10.50.060.50.02.00.07.02.623.2BBH_object_counting41.272.8100.0 76.091.2 53.688.8 60.993.3bbeh_object_counting16.50.041.50.019.50.05.54.122.2BBH_reasoning_about_colored_objects71.687.2100.0 60.496.0 59.290.8 69.695.6bbeh_object_properties9.51.525.01.010.00.04.03.013.0</p>
<p>Table 7 :
7
Qwen2.5-7B-Base.w.Formal Full result.
Prompt for PoT{question}Let's write a Python program to solve theproblem. Please give the python code in thefollowing format:<code>python</code>and print the final result.
Due to the space limitations, the detailed description about related works are moved in Appendix A.
, and Ion Stoica.2023.Efficient memory management for large language model serving with pagedattention.In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles.Chris Callison-Burch.2023.Faithful chain-ofthought reasoning.In The 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLP-AACL 2023).SectionQwQ-32B DeepSeek-R1-Distill-Llama-70B DeepSeek-R1-Distill-Qwen-32B DeepSeek-R1-Distill-Qwen-7BGPT-4o Qwen2.5-72BQwen2.5-32BLlama-3.3-70BQwen2.5-7BLlama-3.1-8BPART-II 3 (Deductive, Inductive, Abductive) + 4(Text,PoT,Z3,CSP)=7Case Study for Text (bbeh_nycc)The following is a description of a funny cartoon for the New Yorker Caption Contest Description: Two trees are standing next to each other.One tree has a face and is looking at the other tree, which also has a face and is wearing a tie.WhichCase Study for Text (bbeh_linguini)The plural of Catalan nouns is usually formed by adding the endings.But if the noun ends in one of the letters s, x or ç, more complex rules apply.Here are the singular and the plural forms of some Catalan nouns (in simplified spelling) and their English translations.Some forms are missing.Case Study for PoT (BBH_dyck_language)Complete the rest of the sequence, making sure that the parentheses are closed properly.GPT-4o Text Answer:To correctly close a Dyckn word, we need to ensure that all types of brackets are properly matched and closed in the correct order.The given sequence is:et's break it down step by step:1. <code>&lt;</code> is opened and closed with <code>&gt;</code>. 2. <code>(</code> is opened three times and needs to be closed three times.3. <code>{</code> is opened twice and needs to be closed twice.4. <code>[</code> is opened twice and needs to be closed twice.5. <code>&lt;</code> is opened and closed with <code>&gt;</code>. 6. <code>(</code> is opened and closed with <code>)</code>.The sequence is already correctly closed.Therefore, no additional characters are needed.The sequence is:his sequence is a valid Dyckn word as all brackets are properly matched and closed.GPT-4oCase Study for Few-Shot Formal-Eval in Logic-LM---&gt;Our Zero-Shot Evaluation (AR-LSAT)Context: On each of exactly seven consecutive days (day 1 though day 7), a pet shop features exactly one of three breeds of kitten-Himalayan, Manx, Siamese-and exactly one of three breeds of puppy-Greyhound, Newfoundland, Rottweiler.The following conditions must apply: Greyhounds are featured on day 1.No breed is featured on any two consecutive days.Any breed featured on day 1 is not featured on day 7. Himalayans are featured on exactly three days, but not on day 1.Rottweilers are not featured on day 7, nor on any day that features Himalayans.problem.addConstraint(lambdap1, p2: p1 != p2, \n(f'puppy_day_{day}', f'puppy_day_{day+1}'))Question# Any breed featured on day 1 is not featured on day 7 problem.addConstraint(lambdak1, k7: k1 != k7, \n('kitten_day_1', 'kitten_day_7')) problem.addConstraint(lambdap1, p7: p1 != p7, ('puppy_day_1', 'puppy_day_7'))# Himalayans are featured on exactly three days, but not on day 1 problem.addConstraint(lambdak: k != 'Himalayan', ('kitten_day_1',)) # Custom constraint to ensure Himalayans are featured on exactly three days def himalayan_count(*args): return args.count('Himalayan')== 3 problem.addConstraint(himalayan_count,('kitten_day_2','kitten_day_3',\n 'kitten_day_4','kitten_day_5','kitten_day_6','A case from the original Logic-LM paper.Logic-LM uses few-shots and rule extraction to build executable code for a "specific formal language" and a "specific task".We use zero-shot directly for evaluation.As can be seen from the cases, both approaches behave similarly and both can effectively evaluate the formal language reasoning capability of the model.Meanwhile, zero-shot has better generalization, and this paper evaluates a subset of 66 data based on it.Figure10: Case Study for Few-Shot Formal-Eval in Logic-LM-&gt;Our Zero-Shot Evaluation (AR-LSAT).Logic-LM uses few-shots and rule extraction to build executable code for a "specific formal language" and a "specific task".We use zero-shot directly for evaluation.As can be seen from the cases, both approaches behave similarly and both can effectively evaluate the formal language reasoning capability of the model.Meanwhile, zero-shot has better generalization, and this paper evaluates a subset of 66 data based on it.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, and 1 others. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, arXiv:2309.16609Fei Huang, and 1 others. 2023. Qwen technical report. arXiv preprint</p>
<p>Abductionrules: Training transformers to explain unexpected inputs. Rama Krishna, Sai Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Yejin Choi, Noah A Smith, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Zhe Fu, and 1 others. 2024. Deepseek llm: Scaling open-source language models with longtermism. Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, arXiv:2401.02954arXiv preprint</p>
<p>νz-an optimizing smt solver. Nikolaj Bjørner, Anh-Dung Phan, Lars Fleckenstein, Tools and Algorithms for the Construction and Analysis of Systems: 21st International Conference, TACAS 2015, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2015. London, UKSpringer2015. April 11-18, 201521</p>
<p>Classifying the complexity of constraints using finite algebras. Andrei Bulatov, Peter Jeavons, Andrei Krokhin, Qwen2.5-7B-Base.w. Deductive (+5653) Qwen2.5-7B-Base.w. Inductive (+4947) Qwen2.5-7B-Base.w. Abductive (+6557) Qwen2.5-7B-Base.w. Text (+7384) Qwen2.5-7B-Base.w. PoT (+7448) Qwen2.5-7B-Base.w. Z3 (+6882). 200534Qwen2.5-7B-Base.w. CSP (+6346)</p>
<p>. Inductive Deductive, </p>
<p>Text, Z3 Pot, Csp) =12 Qwen2, 5-7B-Base.w. Deductive_Text (1376) Qwen2.5-7B-Base.w. Deductive_PoT (+1393) Qwen2.5-7B-Base.w. Deductive_Z3 (+1374) Qwen2.5-7B-Base.w. Deductive_CSP (+1510) Qwen2.5-7B-Base.w. Inductive_Text (+1263) Qwen2.5-7B-Base.w. Inductive_PoT (+1476) Qwen2.5-7B-Base.w. Inductive_Z3 (+1166) Qwen2.5-7B-Base.w. Inductive_CSP (+1042) Qwen2.5-7B-Base.w. Abductive_Text (+1820) Qwen2.5-7B-Base.w. Abductive_PoT (+1775) Qwen2.5-7B-Base.w. Abductive_Z3 (+1667) Qwen2.5-7B-Base.w. Abductive_CSP (+1295). </p>
<p>PART-III 1 Baseline-Model+. </p>
<p>Formal Data Enhanced Model =2 Qwen2.5-7B-Baseline* (15k). </p>
<p>Formal (+28060) ALL 31 -Table 3: Comprehensive Overview of Model Evaluation Experiments in the Entire Paper (Models in Bold Are Presented with Full Results Later). Parentheses after the model in PART-II indicate the corresponding amount of data. All data are based on the 15k generic data of Qwen2.5-7B-Baseline*, plus (+) the corresponding amount of our synthetic data. Qwen2.5-7B-Base.w.</p>            </div>
        </div>

    </div>
</body>
</html>
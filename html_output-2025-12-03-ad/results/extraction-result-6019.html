<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6019 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6019</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6019</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-121.html">extraction-schema-121</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-268351639</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.06749v1.pdf" target="_blank">Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies</a></p>
                <p><strong>Paper Abstract:</strong> . Using Large Language Models (LLMs) for Process Mining (PM) tasks is becoming increasingly essential, and initial approaches yield promising results. However, little attention has been given to developing strategies for evaluating and benchmarking the utility of incorporating LLMs into PM tasks. This paper reviews the current implementations of LLMs in PM and reflects on three different questions. 1) What is the minimal set of capabilities required for PM on LLMs? 2) Which benchmark strategies help choose optimal LLMs for PM? 3) How do we evaluate the output of LLMs on specific PM tasks? The answer to these questions is fundamental to the development of comprehensive process mining benchmarks on LLMs covering different tasks and implementation paradigms.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6019.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6019.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automatic eval (text-to-SQL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated evaluation for text-to-SQL and formally structured outputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automatic assessment of executable outputs (e.g., SQL queries or declarative constraints) generated by LLMs via formal correctness, executability, and conciseness metrics; recommended in the paper for PM tasks that can be converted to database queries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated formal-checking: execute generated SQL/queries against the database or compare against gold SQL; check syntactic validity, execution success, result equivalence, and measure conciseness (e.g., query length).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Formal accuracy (syntactic correctness and semantic equivalence to a gold query), executability, conciseness (length/complexity of query), and result equivalence (same result set).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>unspecified LLMs with code/text-to-SQL capability (paper references modern code-capable LLMs generally; no specific model experimentally evaluated).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Process Mining (event-log querying and hypothesis verification over process data).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>LLM-generated hypotheses are translated into executable database queries (SQL) that test process-centered hypotheses (e.g., whether a sequence of events or attribute combination correlates with delays or anomalies).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>No empirical quantitative results reported in the paper; the approach is recommended as an automatic method particularly suitable for formally-expressed outputs (SQL, declarative constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Text-to-SQL and code benchmarks mentioned as relevant baselines: SPIDER, SPIDER-realistic, APPS.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Automated evaluation complements human review by checking formal correctness and executability, but cannot replace human judgment on hypothesis plausibility or causal interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Applies only to hypotheses that can be operationalized as executable queries; does not assess conceptual validity, novelty, or hallucinated semantic claims; requires gold queries or an execution oracle and representative databases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6019.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6019.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human evaluation (recall/precision)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human expert evaluation for direct querying and hypothesis assessment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human experts evaluate LLM outputs (anomaly detections, root-cause hypotheses, suggested improvements) primarily by measuring recall and precision of expected insights and by judging the usefulness and correctness of generated hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human expert review: domain experts assess whether the LLM identified expected insights (recall) and whether identified insights are correct (precision); also assess the quality of the hypothesis-validation feedback loop.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Recall (coverage of expected findings), precision (correctness of reported findings), usefulness/actionability of suggestions, and effectiveness of human-LLM feedback cycles for refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>unspecified LLMs applied to PM tasks (paper discusses LLMs in general; no single model evaluated experimentally).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Process Mining (anomaly detection, root-cause analysis, fairness analysis, process improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>LLM-generated scientific-style hypotheses and interpretive claims about process behavior (e.g., candidate root causes, bias sources, anomalous sequences) to be judged by humans for correctness and relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>No numerical human-evaluation results reported; human evaluation is presented as essential for non-formal tasks (direct querying, hypothesis generation) where automated checks are insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>No PM-specific human-evaluation dataset provided; authors recommend human assessment and reference general evaluation resources (e.g., AGIEval, MT-Bench) for related capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Human evaluation is positioned as the gold standard for assessing plausibility and domain relevance; automated checks handle formal correctness, while humans assess conceptual and domain-level validity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Human evaluation is costly, subjective, and may vary across experts; it is necessary because many PM hypotheses cannot be fully verified automatically and because LLMs may hallucinate plausible-sounding but incorrect explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6019.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6019.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-evaluation methods</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM self-assessment techniques (chain-of-thought, confidence scores, ensembling, self-reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Collection of LLM-internal techniques to improve reliability: chain-of-thought explanations for transparency, confidence scores to filter outputs, ensembling across runs for majority/agreement, and self-reflection where an LLM audits outputs for errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>LLM-internal verification: request chain-of-thought reasoning, obtain confidence scores or uncertainty estimates, run multiple independent generations and aggregate (ensembling/majority voting), and run self-reflection prompts to detect inconsistencies or hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Self-reported confidence, internal consistency of reasoning, agreement across ensemble runs, presence/absence of contradictions in chain-of-thought, and detection of unsupported claims during self-reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>LLMs that support chain-of-thought and produce confidence-like signals (unspecified; references to general LLM literature).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Process Mining (hypothesis generation and explanation), broadly applicable to scientific-hypothesis generation workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>LLM-produced theories/hypotheses accompanied by self-explanations and confidence estimates which the LLM uses to accept, reject, or flag hypotheses for human review.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Paper cites literature indicating self-evaluation improves selective generation but reports no novel experiments here; suggests these techniques as practical mitigations against hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>No specific dataset used in-paper; relevant citations include work on chain-of-thought and self-evaluation (referenced in bibliography).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Self-evaluation is recommended to reduce workload and pre-filter low-confidence outputs before human inspection, but not as a full replacement for expert review.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Confidence estimates can be miscalibrated (confidence-competence gap), chain-of-thought increases surface plausibility without guaranteeing factual correctness, and ensembling increases cost; these methods reduce but do not eliminate hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6019.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6019.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Factuality checks (external grounding)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Factuality verification via external knowledge sources and web access</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Validation of LLM claims by cross-checking outputs against external structured data sources, knowledge bases, or web resources (including live browsing) to reduce hallucinations and ensure up-to-date factual grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Grounding evaluation: compare LLM assertions to authoritative external sources (databases, knowledge graphs, web search), or execute queries against source data to verify claims made in hypotheses or explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Grounding accuracy (claim supported/unsupported by external sources), timeliness (up-to-dateness), and provenance (traceable citations or data links).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>LLMs with web-browsing or retrieval-augmented generation capabilities (unspecified in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Process Mining (checking factual claims about process sequences, public standards, or external events) and general scientific hypothesis verification.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>LLM-generated claims about processes or external facts are validated by consulting records, knowledge bases, or web sources to confirm or refute the claims.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Presented as a necessary capability to improve factuality; no experimental numbers provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>No PM-specific factuality dataset provided; references and related benchmarks for trustworthiness/domain knowledge include DecodingTrust and XIEZHI.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>External grounding automates fact-checking that complements human review; human experts still needed to interpret evidence relevance and causal claims.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires reliable external resources and connectors; web sources vary in quality; grounding cannot always validate causal or counterfactual scientific claims purely by lookup.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6019.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6019.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PM-relevant benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Collection and mapping of general LLM benchmarks to process-mining needs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper maps PM tasks to existing general-purpose benchmarks (reasoning, long-context, multimodal, text-to-SQL, fairness) and highlights gaps where PM-specific benchmarks are missing (notably visual prompt understanding and hypothesis-generation benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Use existing LLM benchmarks to assess candidate models along axes relevant to PM (long context, multimodal visual understanding, coding/text-to-SQL, domain knowledge, fairness), and advocate for bespoke PM benchmarks for missing capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Benchmarks measure reasoning, long-text modeling, multimodal understanding, text-to-SQL performance, fairness/trustworthiness, and domain-knowledge coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Various LLMs benchmarked by the referenced datasets (paper does not perform the benchmarks itself).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Process Mining (task coverage includes process description, modeling, anomaly detection, root-cause analysis, fairness, visualization explanation, process improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Evaluation of LLMs for generating process-related theories/hypotheses relies on benchmarked capabilities: reasoning, multimodal vision, long-context handling, code generation for verification, and fairness detection.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>No new benchmark results reported; the paper enumerates candidate benchmarks and states that they partially cover PM needs but that PM-specific benchmarks are lacking.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Mentioned benchmarks include: AGIEval, MT-Bench, BAMBOO (long text), XIEZHI (domain knowledge), ARB (advanced reasoning), MMBench, MM-Vet (multimodal), SPIDER & SPIDER-realistic (text-to-SQL), APPS (code), DecodingTrust (trustworthiness).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Benchmarks provide automated, reproducible metrics; human evaluation remains necessary for subjective and domain-specific assessments such as hypothesis relevance and interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Existing benchmarks do not fully cover PM-specific needs: (1) visual benchmarks may not evaluate subtle PM visualization features (line orientation, point-size/color), (2) there is a lack of standardized benchmarks for autonomous hypothesis generation in PM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6019.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6019.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Limitations & challenges</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Key limitations and open challenges for evaluating LLM-generated theories in PM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper outlines practical constraints and open problems that complicate the evaluation of LLM-generated scientific theories in process mining, including context window limits, visual-prompt shortcomings, hallucination/factuality, and lack of PM-specific benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Identification and discussion of capability and evaluation gaps rather than empirical testing; informs what evaluation methods are needed or inadequate.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>N/A (this entry summarizes evaluation obstacles rather than prescribing a metric).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>General LLMs discussed; specific models not experimentally evaluated in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Process Mining and related scientific-hypothesis generation within PM workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Challenges to reliable generation and evaluation of process-related theories: truncated context may omit necessary data; visual prompt understanding is imperfect; LLM hallucinations reduce trust; lack of PM-specific hypothesis benchmarks impedes standardized evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>No evaluation resultsâ€”this is a descriptive synthesis noting impediments to rigorous evaluation and benchmarking for PM-on-LLM approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>None PM-specific; existing general benchmarks identified but noted as insufficient for some PM tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Limitations increase reliance on human oversight; inability of current benchmarks to fully capture PM needs means human judgment remains crucial.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Context window limits can prevent including full event logs; visual models may not capture PM visualization specifics; hallucination and miscalibrated confidence complicate trust; absence of standardized PM hypothesis-generation benchmarks is a major gap.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. <em>(Rating: 2)</em></li>
                <li>Self-Evaluation Improves Selective Generation in Large Language Models. <em>(Rating: 2)</em></li>
                <li>Large Language Models for Automated Open-domain Scientific Hypotheses Discovery. <em>(Rating: 2)</em></li>
                <li>Automating Psychological Hypothesis Generation with AI: Large Language Models Meet Causal Graph. <em>(Rating: 2)</em></li>
                <li>BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models. <em>(Rating: 1)</em></li>
                <li>Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation. <em>(Rating: 1)</em></li>
                <li>ARB: Advanced Reasoning Benchmark for Large Language Models. <em>(Rating: 1)</em></li>
                <li>MMBench: Is Your Multi-modal Model an Allaround Player?. <em>(Rating: 1)</em></li>
                <li>MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. <em>(Rating: 1)</em></li>
                <li>Evaluating the Text-to-SQL Capabilities of Large Language Models. <em>(Rating: 2)</em></li>
                <li>Measuring Coding Challenge Competence With APPS. <em>(Rating: 1)</em></li>
                <li>DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6019",
    "paper_id": "paper-268351639",
    "extraction_schema_id": "extraction-schema-121",
    "extracted_data": [
        {
            "name_short": "Automatic eval (text-to-SQL)",
            "name_full": "Automated evaluation for text-to-SQL and formally structured outputs",
            "brief_description": "Automatic assessment of executable outputs (e.g., SQL queries or declarative constraints) generated by LLMs via formal correctness, executability, and conciseness metrics; recommended in the paper for PM tasks that can be converted to database queries.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "evaluation_method": "Automated formal-checking: execute generated SQL/queries against the database or compare against gold SQL; check syntactic validity, execution success, result equivalence, and measure conciseness (e.g., query length).",
            "evaluation_criteria": "Formal accuracy (syntactic correctness and semantic equivalence to a gold query), executability, conciseness (length/complexity of query), and result equivalence (same result set).",
            "llm_model_name": "unspecified LLMs with code/text-to-SQL capability (paper references modern code-capable LLMs generally; no specific model experimentally evaluated).",
            "theory_domain": "Process Mining (event-log querying and hypothesis verification over process data).",
            "theory_description": "LLM-generated hypotheses are translated into executable database queries (SQL) that test process-centered hypotheses (e.g., whether a sequence of events or attribute combination correlates with delays or anomalies).",
            "evaluation_results": "No empirical quantitative results reported in the paper; the approach is recommended as an automatic method particularly suitable for formally-expressed outputs (SQL, declarative constraints).",
            "benchmarks_or_datasets": "Text-to-SQL and code benchmarks mentioned as relevant baselines: SPIDER, SPIDER-realistic, APPS.",
            "comparison_to_human": "Automated evaluation complements human review by checking formal correctness and executability, but cannot replace human judgment on hypothesis plausibility or causal interpretation.",
            "limitations_or_challenges": "Applies only to hypotheses that can be operationalized as executable queries; does not assess conceptual validity, novelty, or hallucinated semantic claims; requires gold queries or an execution oracle and representative databases.",
            "uuid": "e6019.0",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Human evaluation (recall/precision)",
            "name_full": "Human expert evaluation for direct querying and hypothesis assessment",
            "brief_description": "Human experts evaluate LLM outputs (anomaly detections, root-cause hypotheses, suggested improvements) primarily by measuring recall and precision of expected insights and by judging the usefulness and correctness of generated hypotheses.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "evaluation_method": "Human expert review: domain experts assess whether the LLM identified expected insights (recall) and whether identified insights are correct (precision); also assess the quality of the hypothesis-validation feedback loop.",
            "evaluation_criteria": "Recall (coverage of expected findings), precision (correctness of reported findings), usefulness/actionability of suggestions, and effectiveness of human-LLM feedback cycles for refinement.",
            "llm_model_name": "unspecified LLMs applied to PM tasks (paper discusses LLMs in general; no single model evaluated experimentally).",
            "theory_domain": "Process Mining (anomaly detection, root-cause analysis, fairness analysis, process improvement).",
            "theory_description": "LLM-generated scientific-style hypotheses and interpretive claims about process behavior (e.g., candidate root causes, bias sources, anomalous sequences) to be judged by humans for correctness and relevance.",
            "evaluation_results": "No numerical human-evaluation results reported; human evaluation is presented as essential for non-formal tasks (direct querying, hypothesis generation) where automated checks are insufficient.",
            "benchmarks_or_datasets": "No PM-specific human-evaluation dataset provided; authors recommend human assessment and reference general evaluation resources (e.g., AGIEval, MT-Bench) for related capabilities.",
            "comparison_to_human": "Human evaluation is positioned as the gold standard for assessing plausibility and domain relevance; automated checks handle formal correctness, while humans assess conceptual and domain-level validity.",
            "limitations_or_challenges": "Human evaluation is costly, subjective, and may vary across experts; it is necessary because many PM hypotheses cannot be fully verified automatically and because LLMs may hallucinate plausible-sounding but incorrect explanations.",
            "uuid": "e6019.1",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Self-evaluation methods",
            "name_full": "LLM self-assessment techniques (chain-of-thought, confidence scores, ensembling, self-reflection)",
            "brief_description": "Collection of LLM-internal techniques to improve reliability: chain-of-thought explanations for transparency, confidence scores to filter outputs, ensembling across runs for majority/agreement, and self-reflection where an LLM audits outputs for errors.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "evaluation_method": "LLM-internal verification: request chain-of-thought reasoning, obtain confidence scores or uncertainty estimates, run multiple independent generations and aggregate (ensembling/majority voting), and run self-reflection prompts to detect inconsistencies or hallucinations.",
            "evaluation_criteria": "Self-reported confidence, internal consistency of reasoning, agreement across ensemble runs, presence/absence of contradictions in chain-of-thought, and detection of unsupported claims during self-reflection.",
            "llm_model_name": "LLMs that support chain-of-thought and produce confidence-like signals (unspecified; references to general LLM literature).",
            "theory_domain": "Process Mining (hypothesis generation and explanation), broadly applicable to scientific-hypothesis generation workflows.",
            "theory_description": "LLM-produced theories/hypotheses accompanied by self-explanations and confidence estimates which the LLM uses to accept, reject, or flag hypotheses for human review.",
            "evaluation_results": "Paper cites literature indicating self-evaluation improves selective generation but reports no novel experiments here; suggests these techniques as practical mitigations against hallucination.",
            "benchmarks_or_datasets": "No specific dataset used in-paper; relevant citations include work on chain-of-thought and self-evaluation (referenced in bibliography).",
            "comparison_to_human": "Self-evaluation is recommended to reduce workload and pre-filter low-confidence outputs before human inspection, but not as a full replacement for expert review.",
            "limitations_or_challenges": "Confidence estimates can be miscalibrated (confidence-competence gap), chain-of-thought increases surface plausibility without guaranteeing factual correctness, and ensembling increases cost; these methods reduce but do not eliminate hallucinations.",
            "uuid": "e6019.2",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Factuality checks (external grounding)",
            "name_full": "Factuality verification via external knowledge sources and web access",
            "brief_description": "Validation of LLM claims by cross-checking outputs against external structured data sources, knowledge bases, or web resources (including live browsing) to reduce hallucinations and ensure up-to-date factual grounding.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "evaluation_method": "Grounding evaluation: compare LLM assertions to authoritative external sources (databases, knowledge graphs, web search), or execute queries against source data to verify claims made in hypotheses or explanations.",
            "evaluation_criteria": "Grounding accuracy (claim supported/unsupported by external sources), timeliness (up-to-dateness), and provenance (traceable citations or data links).",
            "llm_model_name": "LLMs with web-browsing or retrieval-augmented generation capabilities (unspecified in the paper).",
            "theory_domain": "Process Mining (checking factual claims about process sequences, public standards, or external events) and general scientific hypothesis verification.",
            "theory_description": "LLM-generated claims about processes or external facts are validated by consulting records, knowledge bases, or web sources to confirm or refute the claims.",
            "evaluation_results": "Presented as a necessary capability to improve factuality; no experimental numbers provided in this paper.",
            "benchmarks_or_datasets": "No PM-specific factuality dataset provided; references and related benchmarks for trustworthiness/domain knowledge include DecodingTrust and XIEZHI.",
            "comparison_to_human": "External grounding automates fact-checking that complements human review; human experts still needed to interpret evidence relevance and causal claims.",
            "limitations_or_challenges": "Requires reliable external resources and connectors; web sources vary in quality; grounding cannot always validate causal or counterfactual scientific claims purely by lookup.",
            "uuid": "e6019.3",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "PM-relevant benchmarks",
            "name_full": "Collection and mapping of general LLM benchmarks to process-mining needs",
            "brief_description": "Paper maps PM tasks to existing general-purpose benchmarks (reasoning, long-context, multimodal, text-to-SQL, fairness) and highlights gaps where PM-specific benchmarks are missing (notably visual prompt understanding and hypothesis-generation benchmarks).",
            "citation_title": "here",
            "mention_or_use": "mention",
            "evaluation_method": "Use existing LLM benchmarks to assess candidate models along axes relevant to PM (long context, multimodal visual understanding, coding/text-to-SQL, domain knowledge, fairness), and advocate for bespoke PM benchmarks for missing capabilities.",
            "evaluation_criteria": "Benchmarks measure reasoning, long-text modeling, multimodal understanding, text-to-SQL performance, fairness/trustworthiness, and domain-knowledge coverage.",
            "llm_model_name": "Various LLMs benchmarked by the referenced datasets (paper does not perform the benchmarks itself).",
            "theory_domain": "Process Mining (task coverage includes process description, modeling, anomaly detection, root-cause analysis, fairness, visualization explanation, process improvement).",
            "theory_description": "Evaluation of LLMs for generating process-related theories/hypotheses relies on benchmarked capabilities: reasoning, multimodal vision, long-context handling, code generation for verification, and fairness detection.",
            "evaluation_results": "No new benchmark results reported; the paper enumerates candidate benchmarks and states that they partially cover PM needs but that PM-specific benchmarks are lacking.",
            "benchmarks_or_datasets": "Mentioned benchmarks include: AGIEval, MT-Bench, BAMBOO (long text), XIEZHI (domain knowledge), ARB (advanced reasoning), MMBench, MM-Vet (multimodal), SPIDER & SPIDER-realistic (text-to-SQL), APPS (code), DecodingTrust (trustworthiness).",
            "comparison_to_human": "Benchmarks provide automated, reproducible metrics; human evaluation remains necessary for subjective and domain-specific assessments such as hypothesis relevance and interpretability.",
            "limitations_or_challenges": "Existing benchmarks do not fully cover PM-specific needs: (1) visual benchmarks may not evaluate subtle PM visualization features (line orientation, point-size/color), (2) there is a lack of standardized benchmarks for autonomous hypothesis generation in PM.",
            "uuid": "e6019.4",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Limitations & challenges",
            "name_full": "Key limitations and open challenges for evaluating LLM-generated theories in PM",
            "brief_description": "The paper outlines practical constraints and open problems that complicate the evaluation of LLM-generated scientific theories in process mining, including context window limits, visual-prompt shortcomings, hallucination/factuality, and lack of PM-specific benchmarks.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "evaluation_method": "Identification and discussion of capability and evaluation gaps rather than empirical testing; informs what evaluation methods are needed or inadequate.",
            "evaluation_criteria": "N/A (this entry summarizes evaluation obstacles rather than prescribing a metric).",
            "llm_model_name": "General LLMs discussed; specific models not experimentally evaluated in the paper.",
            "theory_domain": "Process Mining and related scientific-hypothesis generation within PM workflows.",
            "theory_description": "Challenges to reliable generation and evaluation of process-related theories: truncated context may omit necessary data; visual prompt understanding is imperfect; LLM hallucinations reduce trust; lack of PM-specific hypothesis benchmarks impedes standardized evaluation.",
            "evaluation_results": "No evaluation resultsâ€”this is a descriptive synthesis noting impediments to rigorous evaluation and benchmarking for PM-on-LLM approaches.",
            "benchmarks_or_datasets": "None PM-specific; existing general benchmarks identified but noted as insufficient for some PM tasks.",
            "comparison_to_human": "Limitations increase reliance on human oversight; inability of current benchmarks to fully capture PM needs means human judgment remains crucial.",
            "limitations_or_challenges": "Context window limits can prevent including full event logs; visual models may not capture PM visualization specifics; hallucination and miscalibrated confidence complicate trust; absence of standardized PM hypothesis-generation benchmarks is a major gap.",
            "uuid": "e6019.5",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-Evaluation Improves Selective Generation in Large Language Models.",
            "rating": 2,
            "sanitized_title": "selfevaluation_improves_selective_generation_in_large_language_models"
        },
        {
            "paper_title": "Large Language Models for Automated Open-domain Scientific Hypotheses Discovery.",
            "rating": 2,
            "sanitized_title": "large_language_models_for_automated_opendomain_scientific_hypotheses_discovery"
        },
        {
            "paper_title": "Automating Psychological Hypothesis Generation with AI: Large Language Models Meet Causal Graph.",
            "rating": 2,
            "sanitized_title": "automating_psychological_hypothesis_generation_with_ai_large_language_models_meet_causal_graph"
        },
        {
            "paper_title": "BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models.",
            "rating": 1,
            "sanitized_title": "bamboo_a_comprehensive_benchmark_for_evaluating_long_text_modeling_capacities_of_large_language_models"
        },
        {
            "paper_title": "Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation.",
            "rating": 1,
            "sanitized_title": "xiezhi_an_everupdating_benchmark_for_holistic_domain_knowledge_evaluation"
        },
        {
            "paper_title": "ARB: Advanced Reasoning Benchmark for Large Language Models.",
            "rating": 1,
            "sanitized_title": "arb_advanced_reasoning_benchmark_for_large_language_models"
        },
        {
            "paper_title": "MMBench: Is Your Multi-modal Model an Allaround Player?.",
            "rating": 1,
            "sanitized_title": "mmbench_is_your_multimodal_model_an_allaround_player"
        },
        {
            "paper_title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities.",
            "rating": 1,
            "sanitized_title": "mmvet_evaluating_large_multimodal_models_for_integrated_capabilities"
        },
        {
            "paper_title": "Evaluating the Text-to-SQL Capabilities of Large Language Models.",
            "rating": 2,
            "sanitized_title": "evaluating_the_texttosql_capabilities_of_large_language_models"
        },
        {
            "paper_title": "Measuring Coding Challenge Competence With APPS.",
            "rating": 1,
            "sanitized_title": "measuring_coding_challenge_competence_with_apps"
        },
        {
            "paper_title": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.",
            "rating": 1,
            "sanitized_title": "decodingtrust_a_comprehensive_assessment_of_trustworthiness_in_gpt_models"
        }
    ],
    "cost": 0.013434749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies
5 Apr 2024</p>
<p>Alessandro Berti alessandro.berti@fit.fraunhofer.de 0000-0002-3279-4795
Fraunhofer FIT
Sankt AugustinGermany</p>
<p>Process and Data Science Chair
RWTH Aachen University
AachenGermany</p>
<p>Humam Kourani humam.kourani@fit.fraunhofer.de 0000-0003-2375-2152
Fraunhofer FIT
Sankt AugustinGermany</p>
<p>Process and Data Science Chair
RWTH Aachen University
AachenGermany</p>
<p>Hannes HÃ¤fke hannes.haefke@fit.fraunhofer.de 0000-0002-2845-3998
Fraunhofer FIT
Sankt AugustinGermany</p>
<p>Chiao-Yun Li chiao-yun.li@fit.fraunhofer.de 0009-0002-3767-7915
Fraunhofer FIT
Sankt AugustinGermany</p>
<p>Process and Data Science Chair
RWTH Aachen University
AachenGermany</p>
<p>Daniel Schuster daniel.schuster@fit.fraunhofer.de 0000-0002-6512-9580
Fraunhofer FIT
Sankt AugustinGermany</p>
<p>Process and Data Science Chair
RWTH Aachen University
AachenGermany</p>
<p>Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies
5 Apr 20242EF1C7951936BFA3FE2194CB50117E45arXiv:2403.06749v3[cs.DB]Large Language Models (LLMs)Output EvaluationBenchmarking Strategies
Using Large Language Models (LLMs) for Process Mining (PM) tasks is becoming increasingly essential, and initial approaches yield promising results.However, little attention has been given to developing strategies for evaluating and benchmarking the utility of incorporating LLMs into PM tasks.This paper reviews the current implementations of LLMs in PM and reflects on three different questions. 1) What is the minimal set of capabilities required for PM on LLMs? 2) Which benchmark strategies help choose optimal LLMs for PM? 3) How do we evaluate the output of LLMs on specific PM tasks?The answer to these questions is fundamental to the development of comprehensive process mining benchmarks on LLMs covering different tasks and implementation paradigms.</p>
<p>Introduction</p>
<p>Process mining (PM) is a data science field focusing on deriving insights about business process executions from event data recorded by information systems [1].Several types of PM exist, including process discovery (learning process models from event data), conformance checking (comparing event data with process models), and process enhancement (adding frequency/performance metrics to process models).Although many automated methods exist for PM, human analysts usually handle process analysis due to the need for domain knowledge.Recently, LLMs have emerged as conversational interfaces trained on extensive data [28], achieving near-human performance in various general tasks [38].Their potential in PM lies in embedded domain knowledge useful for generating database queries and insights [21], logical and temporal reasoning capabilities [2,16], inference abilities over structured data [12].Prior research has asserted the usage of LLMs for PM tasks [3,4].However, a comprehensive discussion on Fig. 1: Outline of the contributions of this paper.necessary capabilities for PM, LLMs' suitability evaluation for process analytics, and assessment of LLMs' outputs in the PM context is lacking.</p>
<p>The three main contributions of this paper are summarized in Fig. 1.First, building upon prior work [3,4] proposing textual abstractions of process mining artifacts and an experimental evaluation of LLMs' responses, the essential capabilities that LLMs must have for PM tasks are derived in Section 3.1.The aforementioned capabilities allow us to narrow down the field of LLMs to those that meet these requirements.Next, evaluation benchmarks for selecting suitable LLMs are introduced in Section 3.2, incorporating both process-miningspecific and general criteria such as reasoning, visual understanding, factuality, and trustworthiness.Finally, we suggest automatic, human, and self-assessment methods for evaluating LLMs' outputs on specific tasks in Section 3.3, aiming to establish a comprehensive PM benchmark and enhance confidence in LLMs' usage, addressing potential issues like hallucination.</p>
<p>This paper provides an orientation to process mining researchers investigating the usage of LLMs, i.e., this paper aims to facilitate PM-on-LLMs research.</p>
<p>Background</p>
<p>LLMs enhance PM with superior capabilities, handling complex tasks through data understanding and natural language processing.This section covers PM tasks with LLMs (Section 2.1) and the adopted implementation paradigms (Section 2.2) along with the provision of additional domain knowledge.</p>
<p>Process Mining Tasks for LLMs</p>
<p>This subsection explores a range of PM tasks in which LLMs have already been adopted for process mining research.LLMs facilitate the automation of generating textual descriptions from process data, handling inputs such as event logs or formal process models [4].They also generate process models from textual descriptions, with studies showing LLMs creating BPMN models and declarative constraints from text [7].In the realm of anomaly detection, LLMs play a crucial role in identifying process data anomalies, including unusual activities and performance bottlenecks, offering context-aware detection that adapts to new patterns through prompt engineering.This improves versatility over traditional methods [3,4].For root cause analysis, LLMs analyze event logs to suggest causes of anomalies or inefficiencies, linking delays to specific conditions or events.This goes beyond predefined logic, employing language processing for context-aware analysis [3,4] In ensuring fairness, LLMs identify and mitigate bias in processes, suggesting adjustments.They analyze processes like recruitment to detect disparities in rejection rates or delays by gender or nationality, aiding in fair decision-making [22,4].LLMs can also interpret and explain visual data, including complex visualizations, by describing event flows in dotted charts and identifying specific patterns, such as batch processing.For process improvement, after PM tasks identify and analyze problems, LLMs can suggest actions and propose new process constraints [22,4].</p>
<p>Implementation Paradigms of Process Mining on LLMs</p>
<p>To effectively employ LLMs for PM tasks, specific implementation paradigms are required [3,4].This section outlines key approaches for implementing LLMs in PM tasks.We distinguish three main strategies:</p>
<p>-Direct provision of insights: A prompt is generated that merges data abstractions with a query about the task.Also, interactive dialogue between the LLM and the user is possible for step-by-step analysis.The user starts with a query and refines or adjusts it based on the LLM's feedback, continuing until achieving the desired detail or accuracy, such as pinpointing process inefficiencies.For instance, to have LLMs identifying unusual behavior in an event log, we combine a textual abstraction of the log (such as the directly-follows graph or list of process variants) with a question like "Can you analyze the log to detect any unusual behavior?"-Code generation: LLMs can be used to create structured queries, like SQL, for advanced PM tasks [11].Rather than directly asking LLMs for answers, users command LLMs to craft database queries from natural language.These queries are then executed on the databases holding PM information.It is applicable to PM tasks that can be converted into database queries, such as filtering event logs or computing the average duration of process steps.Also, LLMs can be used to generate executable programs that use existing PM libraries to infer insights over the event data [9].-Automated hypotheses generation: Combining the previous strategies by using textual data abstraction to prompt LLMs for autonomous hypotheses generation [3,4].The hypotheses are accompanied by SQL queries for verification against event data.Results confirm or refute these hypotheses, with potential for LLM-suggested refinements of hypotheses.</p>
<p>LLMs may require additional knowledge about processes and databases to implement PM tasks, for example, in anomaly detection and crafting accurate database queries.Some strategies are used to equip LLMs with this additional domain knowledge [14], including fine-tuning and prompt engineering.</p>
<p>Evaluating LLMs in Process Mining</p>
<p>This section introduces criteria for selecting LLMs that are suitable for PM tasks.Moreover, we introduce criteria for evaluating their outputs.First, in Section 3.1, we discuss the fundamental capabilities needed for PM (long context window, acceptance of visual prompts, coding, factuality).Then, we introduce in Section 3.2 general-purpose and process-mining-specific benchmarks to measure the different LLMs on process-mining-related tasks.To foster the development of process-mining-specific benchmarks and to be able to evaluate a given output, we propose in Section 3.3 different methods to evaluate the output of an LLM.</p>
<p>LLMs Capabilities Needed for Process Mining</p>
<p>In this section, we discuss four important capabilities of LLMs for PM tasks:</p>
<p>-Long Context Window : Event logs in PM often include a vast amount of cases and events, challenging the context window limit of LLMs, which restricts the token count in a prompt [13].Moreover, also the textual specification of process models requires a significant amount of information.The context window limit can be severe in many currently popular LLMs. 3 Even simple abstractions like the ones introduced in [3] (directly-follows graph, list of process variants) may exceed this limitation.The context window, which is set during model training, must be large enough for the data size.Recent efforts aim to extend this limit, though quality may decline [13,20].-Accepting Visual Prompts: Visualizations in PM, such as the dotted chart and the performance spectrum [15], summarizing process behavior, empower analysts to spot interesting patterns not seen in tables.Interpreting visual prompts is key for semi-automated PM.Large Visual Models (LVMs) use architectures similar to language models trained on annotated image datasets [31].They perform tasks like object detection and image synthesis, recognizing patterns, textures, shapes, colors, and spatial relations. 4Coding (Text-to-SQL) Capabilities: With the context window limit preventing full event log inclusion in prompts, generating scripts and database queries is crucial for analyzing event data.As discussed in Section 2.2, text-to-SQL assists in filtering and analyzing event data.Key requirements for text-to-SQL in PM include understanding database schemas, performing complex joins, using database-specific operators (e.g., for calculating date differences), and translating PM concepts into queries.Overall, modern LLMs offer excellent coding capabilities [3].</p>
<p>-Factuality: LLM hallucination involves generating incorrect or fabricated information [24].Factuality measures an LLM's ability to cross-check its outputs against real facts or data, crucial for PM tasks like anomaly detection and root cause analysis.This may involve leveraging external databases [19], knowledge bases, or internet search [32] for validation.For instance, verifying the sequence Cancel Order" followed by Deliver Order" against public data in anomaly detection.LLMs with web browsing can access up-to-date information, enhancing factuality. 5</p>
<p>Relevant LLMs Benchmarks</p>
<p>After identifying the required capabilities for LLMs in PM, benchmarking strategies are essential to measure the quality of the textual outputs returned by the LLMs satisfying such capabilities.</p>
<p>Considering the wide array of available benchmarks for assessing LLMs behavior, we focus on identifying those most relevant to PM capabilities.In [5], a comprehensive collection of benchmarks is introduced.This section aims to select and utilize some of these benchmarks to evaluate various aspects of LLMs' performance in PM contexts.</p>
<p>-Traditional benchmarks: Textual prompts are crucial for LLMs evaluation in PM.Benchmarks like AGIEval assess models via standardized exams [37], and MT-Bench focuses on conversational and instructional capabilities [36].</p>
<p>Another benchmark evaluates LLMs on prompts of long size [6].-Domain knowledge benchmarks: Domain knowledge is essential for LLMs in PM to identify anomalies using metrics and context.Benchmarks like XIEZHI assess knowledge across different fields (economics, science, engineering) [8], while ARB evaluates expertise in areas like mathematics and natural sciences [26].-Visual benchmarks: Understanding PM visualizations, such as dotted charts, is essential (c.f.Section 3.1).LLMs must accurately process queries on these visualizations.MMBench tests models on image tasks [17], and MM-Vet assesses recognition, OCR, among others [35].Yet, they may not fully meet PM visualization analysis needs, particularly in evaluating line orientations and point size/color.-Benchmarks for Text-to-SQL: In PM, generating SQL from natural language is key for tasks like event log filtering.Benchmarks such as SPIDER and SPIDER-realistic test LLMs on text-to-SQL conversion [23].The APPS benchmark evaluates broader code generation abilities [10].-Fairness benchmarks: they evaluate LLM fairness in PM by analyzing group treatment and bias detection.DecodingTrust measures LLM trustworthiness, covering toxicity, bias, robustness, privacy, ethics, and fairness [30].-Benchmarking the generation of hypotheses: LLMs' ability to generate hypotheses from event data is vital to implement semi-autonomous PM agents.
X X Process Modeling X X X X X X X Anomaly Detection X X X X X X Root Cause Analysis X X X X X X Ensuring Fairness X X X X X X X Expl. and Interpreting X X X Visualizations Process Improvement X X X X X X X X
While specific benchmarks for hypothesis generation are lacking, related studies like [29] and [34] evaluate LLMs using scientific papers.</p>
<p>In Table 1, we link process mining (PM) tasks to implementation paradigms and benchmarks.We discuss these tasks:</p>
<p>-Process description requires understanding technical terms relevant to the domain, crucial for accurately describing processes.-Process modeling involves generating models from text, using SQL for declarative and BPMN XML for procedural models.LLMs should offer various model hypotheses.-Anomaly detection and root cause analysis need domain knowledge to analyze process sequences or identify event attribute combinations causing issues.-Fairness involves detecting biases by analyzing event attributes and values, necessitating hypothesis generation by LLMs.-Explaining and interpreting visualizations requires extracting features from images and texts, offering contextual insights, like interpreting performance spectrum visualization [15].-Process improvement entails suggesting text proposals or new constraints to enhance current models, leveraging code generation capabilities and understanding process limitations.</p>
<p>While general-purpose benchmarks are already developed and are easily accessible, they are not entirely suited for the task of PM-on-LLMs.In particular, visual capabilities (explaining and interpreting PM visualizations) and autonomous hypotheses generation require more PM-specific benchmarks.However, little research exists on PM-specific benchmarks [3,4].</p>
<p>How to Evaluate LLMs Outputs</p>
<p>This section outlines criteria for assessing the quality of outputs generated by LLMs in PM tasks, serving two primary objectives.The first objective is to as-sist users in identifying and addressing hallucinations and inaccuracies in LLMs' outputs.The second aim is to establish criteria for developing an extensive benchmark specifically tailored to PM applications of LLMs.The strategies follow:</p>
<p>-Automatic evaluation is particularly suited for text-to-SQL tasks.In this context, the formal accuracy and conciseness (indicated by the length of the produced query) of the SQL queries generated can be efficiently assessed.</p>
<p>Additionally, the creation of declarative constraints, designed to enhance process execution, can also be evaluated in terms of their formal correctness.-Human evaluation is essential for LLM tasks like direct querying and hypothesis generation.For direct querying tasks such as anomaly detection and root cause analysis, important criteria are recall (the model's ability to identify expected insights) and precision (the correctness of insights).These criteria also apply to hypothesis generation.Additionally, evaluating the feedback cycle's effectiveness in validating original hypotheses is crucial for these tasks.-Self-evaluation in LLMs tackles hallucinations, as noted by [24].Techniques include chain-of-thought, where LLMs detail their reasoning, enhancing explanations [33].Confidence scores let LLMs assess their insights' reliability, discarding uncertain outputs for quality [27].Ensembling, or using results from multiple LLM sessions, increases accuracy via majority voting or confidence checks [18].Self-reflection, an LLM reviewing its or another's output, detects errors [25].In anomaly detection, using confidence scores to exclude doubtful anomalies and ensembling to confirm detections across sessions improves reliability.</p>
<p>Conclusion</p>
<p>This paper examines LLM applications in PM, offering three main contributions: identification of necessary LLM capabilities for PM, review of benchmarks from literature, and strategies for evaluating LLM outputs in PM tasks.These strategies aim to build confidence in LLM use and establish benchmarks to assess LLM effectiveness across PM implementations.</p>
<p>Our discussion centers on current generative AI capabilities within PM, anticipating advancements like deriving event logs from videos.Despite future enhancements, the criteria discussed here should remain pertinent.Benchmarking for PM tasks on large language models (LLMs) will evolve, including both general and PM-specific benchmarks, yet the foundational aspects and methodologies are expected to stay consistent.</p>
<p>Table 1 :
1
Implementation paradigms and benchmarks for LLMs in the context of different PM tasks.
TaskParadigmsBenchmarks ClassesDirect ProvisionCode GenerationHypotheses GenerationTraditionalDomain KnowledgeVisual PromptsText-to-SQLFairnessHypotheses GenerationProcess DescriptionX
https://community.openai.com/t/are-the-full-8k-gpt-4-tokens-available-on-chatgpt/237999
 and Google Bard/Gemini are popular models supporting both visual and textual prompts.
https://cointelegraph.com/news/chat-gpt-ai-openai-browse-internet-no-longer-limited-info-2021</p>
<p>W M P Van Der Aalst, Process Mining -Data Science in Action. ess Mining -Data Science in ActionSpringer2016Second Edition</p>
<p>A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity. Y Bang, S Cahyawijaya, 10.48550/arXiv.2302.040232023</p>
<p>Leveraging Large Language Models (LLMs) for Process Mining. A Berti, M S Qafari, 10.48550/arXiv.2307.127012023Technical Report</p>
<p>A Berti, D Schuster, W M P Van Der Aalst, 10.48550/arXiv.2307.02194Abstractions, Scenarios, and Prompt Definitions for Process Mining with LLMs: A Case Study. 2023</p>
<p>A Survey on Evaluation of Large Language Models. Y Chang, X Wang, 10.48550/arXiv.2307.031092023</p>
<p>BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models. Z Dong, T Tang, 10.48550/arXiv.2309.133452023</p>
<p>Large Language Models Can Accomplish Business Process Management Tasks. M Grohs, L Abb, BPM 2023 International Workshops. Lecture Notes in Business Information Processing. Springer2023492</p>
<p>Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation. Z Gu, X Zhu, 10.48550/arXiv.2306.057832023</p>
<p>Conceptual model interpreter for Large Language Models. F HÃ¤rer, ER 2023. CEUR Workshop Proceedings. 20233618</p>
<p>Measuring Coding Challenge Competence With APPS. D Hendrycks, S Basart, NeurIPS Datasets and Benchmarks. 20212021</p>
<p>Chit-Chat or Deep Talk: Prompt Engineering for Process Mining. U Jessen, M Sroka, D Fahland, 10.48550/arXiv.2307.099092023</p>
<p>StructGPT: A General Framework for Large Language Model to Reason over Structured Data. J Jiang, K Zhou, EMNLP 2023. Association for Computational Linguistics2023</p>
<p>LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning. H Jin, X Han, 10.48550/arXiv.2401.013252024</p>
<p>T Kampik, C Warmuth, 10.48550/arXiv.2309.00900Large Process Models: Business Process Management in the Age of Generative AI. 2023</p>
<p>Performance Mining for Batch Processing Using the Performance Spectrum. E L Klijn, D Fahland, BPM 2019 International Workshops. Lecture Notes in Business Information Processing. Springer2019362</p>
<p>H Liu, R Ning, 10.48550/arXiv.2304.03439Evaluating the Logical Reasoning Ability of Chat-GPT and GPT-4. 2023</p>
<p>MMBench: Is Your Multi-modal Model an Allaround Player?. Y Liu, H Duan, 10.48550/arXiv.2307.062812023</p>
<p>Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models. K Lu, H Yuan, 10.48550/arXiv.2311.086922023</p>
<p>Unifying Large Language Models and Knowledge Graphs: A Roadmap. S Pan, L Luo, 10.48550/arXiv.2306.083022023</p>
<p>YaRN: Efficient Context Window Extension of Large Language Models. B Peng, J Quesnelle, 10.48550/arXiv.2309.000712023</p>
<p>F Petroni, T RocktÃ¤schel, Language Models as Knowledge Bases? In: EMNLP-IJCNLP 2019. Association for Computational Linguistics2019</p>
<p>M S Qafari, W M P Van Der Aalst, Fairness-Aware Process Mining. Lecture Notes in Computer Science. Springer2019. 201911877</p>
<p>Evaluating the Text-to-SQL Capabilities of Large Language Models. N Rajkumar, R Li, D Bahdanau, 10.48550/arXiv.2204.004982022</p>
<p>The Troubling Emergence of Hallucination in Large Language Models -An Extensive Definition, Quantification, and Prescriptive Remediations. V Rawte, S Chakraborty, EMNLP 2023. Association for Computational Linguistics2023</p>
<p>Self-Evaluation Improves Selective Generation in Large Language Models. J Ren, Y Zhao, 2023</p>
<p>ARB: Advanced Reasoning Benchmark for Large Language Models. T Sawada, D Paleka, 10.48550/arXiv.2307.136922023</p>
<p>The Confidence-Competence Gap in Large Language Models: A Cognitive Study. A K Singh, S Devkota, 10.48550/arXiv.2309.161452023</p>
<p>Welcome to the Era of ChatGPT et al. T Teubner, C M Flath, Bus. Inf. Syst. Eng. 6522023</p>
<p>Automating Psychological Hypothesis Generation with AI: Large Language Models Meet Causal Graph. S Tong, K Mao, Z Huang, Y Zhao, K Peng, 2023</p>
<p>B Wang, W Chen, 10.48550/arXiv.2306.11698DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models. 2023</p>
<p>Review of Large Vision Models and Visual Prompt Engineering. J Wang, Z Liu, 10.48550/arXiv.2307.008552023</p>
<p>L Wang, C Ma, 10.48550/arXiv.2308.11432A Survey on Large Language Model based Autonomous Agents. 2023</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. J Wei, X Wang, NeurIPS. 20222022</p>
<p>Large Language Models for Automated Open-domain Scientific Hypotheses Discovery. Z Yang, X Du, 10.48550/arXiv.2309.027262023</p>
<p>MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. W Yu, Z Yang, 10.48550/arXiv.2308.024902023</p>
<p>Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. L Zheng, W Chiang, 10.48550/arXiv.2306.056852023</p>
<p>AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models. W Zhong, R Cui, 10.48550/arXiv.2304.063642023</p>
<p>Large Language Models are Human-Level Prompt Engineers. Y Zhou, A I Muresanu, ICLR 2023. OpenReview.net2023</p>            </div>
        </div>

    </div>
</body>
</html>
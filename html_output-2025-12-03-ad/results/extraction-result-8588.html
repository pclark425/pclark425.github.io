<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8588 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8588</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8588</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-278959573</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.22318v1.pdf" target="_blank">If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) demonstrate impressive reasoning capabilities in familiar contexts, but struggle when the context conflicts with their parametric knowledge. To investigate this phenomenon, we introduce CounterLogic, a dataset containing 1,800 examples across 9 logical schemas, explicitly designed to evaluate logical reasoning through counterfactual (hypothetical knowledge-conflicting) scenarios. Our systematic evaluation of 11 LLMs across 6 different datasets reveals a consistent performance degradation, with accuracies dropping by 27% on average when reasoning through counterfactual information. We propose Self-Segregate, a prompting method enabling metacognitive awareness (explicitly identifying knowledge conflicts) before reasoning. Our method dramatically narrows the average performance gaps from 27% to just 11%, while significantly increasing the overall accuracy (+7.5%). We discuss the implications of these findings and draw parallels to human cognitive processes, particularly on how humans disambiguate conflicting information during reasoning tasks. Our findings offer practical insights for understanding and enhancing LLMs reasoning capabilities in real-world applications, especially where models must logically reason independently of their factual knowledge.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8588.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8588.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CounterLogic (overall eval)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CounterLogic benchmark evaluation of 11 LLMs across 6 reasoning datasets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A systematic experimental evaluation reported in this paper where 11 state-of-the-art LLMs were tested across CounterLogic and five other reasoning datasets to measure performance degradation on knowledge-conflicting (counterfactual) logical reasoning and the effect of a metacognitive prompting intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>11 state-of-the-art LLMs (various)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A heterogeneous set of 11 large language models spanning multiple providers, architectures, and parameter scales; accessed via OpenRouter/OpenAI API with unified inference settings (temperature 0, top-p 0.95, max tokens 4096).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (reported range in paper: ~8B up to 671B)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CounterLogic + Hierarchical Syllogisms + KNOT (explicit & implicit) + FOLIO + LogicBench + Deductive Logic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A collection of strict logical reasoning tasks and benchmarks including the novel CounterLogic dataset (1,800 examples, 9 logical schemas) and several prior datasets; tasks require multi-step formal logical inference (syllogisms, modus ponens, hypothetical syllogism, constructive dilemma, etc.) mapped into natural language with controlled believability (knowledge-consistent vs knowledge-violating).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Baseline prompting (standard direct prompts), chain-of-thought (CoT) prompting, few-shot/zero-shot ablations, and the paper's metacognitive Self-Segregation intervention (two-stage: knowledge-alignment assessment then standard reasoning using CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Aggregate baseline (across datasets/models): knowledge-consistent average accuracy ≈ 96%, knowledge-violating ≈ 69% (mean gap ≈ 27%). Applying Self-Segregation reduced the average gap from 27% to 11% and increased overall accuracy (paper reports overall +7.5% for some tasks). (Paper also reports dataset-specific numbers; see CounterLogic entry.)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Baseline = standard prompts (explicit instruction to reason only from given premises); CoT improves accuracy relative to zero-shot/few-shot in many cases; Self-Segregation (metacognitive two-stage prompt) consistently outperforms baseline/CoT alone across most tasks and models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Substantial, consistent performance degradation on knowledge-violating examples across all models; some reporting inconsistencies in aggregate numbers across paper sections. Self-Segregation is less effective or yields little improvement on tasks requiring deep multi-step chains (notably FOLIO). Models remain sensitive to surface/factual signals (see model-generated-evidence ablation) and struggle with negations, quantifiers, abstract variables.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Parametric knowledge interference (belief-bias-like effect) systematically harms strict logical reasoning in LLMs; explicit metacognitive prompting (Self-Segregation) that makes the model identify knowledge conflicts before reasoning substantially mitigates this effect without harming performance on knowledge-consistent examples. Chain-of-thought helps but does not eliminate belief sensitivity; deeper symbolic/verification methods may be needed for long chains of reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8588.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8588.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CounterLogic (dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CounterLogic benchmark dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset introduced in this paper designed to evaluate multi-step logical reasoning under controlled knowledge-consistent and knowledge-conflicting (counterfactual) conditions: 1,800 natural-language examples spanning 9 logical schemas and balanced across validity and believability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Evaluated across the 11 LLMs in the study</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>N/A (dataset entry); used with chain-of-thought and Self-Segregation prompting to evaluate various LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CounterLogic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Consists of hierarchical entity triples (a ⊂ b ⊂ c) mapped to 8 sentence templates and 9 inference schemas (e.g., Modus Ponens, Hypothetical Syllogism, Constructive Dilemma). Examples are balanced: 50% valid/invalid and 50% knowledge-aligned/conflicting; natural-language conversion done via GPT-4o to avoid surface template exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluation using direct baseline prompts, chain-of-thought, and Self-Segregation (task-adapted reflection first on conclusion plausibility, then logical validity).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On CounterLogic specifically, the paper reports baseline average accuracy: knowledge-consistent 88% vs knowledge-violating 85% (gap ~3%) and reports that Self-Segregation raised consistent to 93% and inconsistent to 90% (overall +5% on average for this dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Standard prompting/CoT vs Self-Segregation: Self-Segregation improved both knowledge-consistent and knowledge-violating accuracies on this dataset without degrading performance on consistent examples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Dataset focuses on categorical syllogisms and propositional schemas (subset of logical reasoning); cannot capture all forms of complex logical reasoning and real-world conflicts; natural-language reformulation used GPT-4o which could inject artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>A balanced benchmark that isolates belief alignment from logical validity, useful for diagnosing belief-bias interference in LLMs; in this dataset Self-Segregation produced modest but consistent improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8588.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8588.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Segregation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Segregation (metacognitive prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting intervention introduced in this paper that elicits explicit metacognitive awareness from the model by asking it to first assess whether premises/conclusions align with its parametric knowledge, then perform logical reasoning based solely on given premises.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to all evaluated models (11 LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A two-phase prompt wrapper applied on top of standard Chain-of-Thought prompting: (1) Knowledge Alignment Assessment (is the statement true according to the model's knowledge?) (2) Standard Reasoning Process (evaluate logical validity using given premises).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>applies to models of various sizes</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Applied across CounterLogic and five other logical reasoning datasets (Hierarchical Syllogisms, KNOT-E/KNOT-I, FOLIO, LogicBench, Deductive Logic)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Used where tasks include a claim/premises and require binary validity judgment under potential knowledge conflict (counterfactual scenarios).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Prompt-level metacognitive intervention; uses explicit belief-alignment question before invoking Chain-of-Thought reasoning; evaluated vs baseline prompts, zero-shot, few-shot, and CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported to reduce average performance gap between knowledge-consistent and knowledge-violating examples from 27% to 11% and to increase overall accuracy (paper reports +7.5% average improvement on some tasks; CounterLogic-specific +5%). Also reported to reduce per-model gaps (example: Qwen-2-72B gap from 47% down to 13%).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to standard prompts and CoT: Self-Segregation consistently improved performance across most models/tasks and did not degrade performance on knowledge-consistent inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Less effective on tasks requiring deep long-chain reasoning (e.g., FOLIO showed little to no improvement). Performance gains may vary by language/domain and depend on the model's ability to truthfully report its own parametric beliefs; relies on prompt design rather than model architecture changes.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Making models explicitly acknowledge knowledge conflicts (epistemic compartmentalization) appears to reduce interference from parametric facts during logical inference; this suggests metacognitive prompting is a practical, model-agnostic mitigation that complements CoT but does not fully solve deep multi-step reasoning failures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8588.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8588.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-2-72B (example model)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen-2 (72B) — example evaluated model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large multilingual model from Alibaba family (mentioned among evaluated systems) that exhibited one of the largest baseline performance disparities between knowledge-consistent and knowledge-violating reasoning examples in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-2-72B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An Alibaba Qwen-series LLM (large-scale, strong multilingual reasoning reported by vendor); included among the 11 models evaluated in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Evaluated across the suite of reasoning tasks (CounterLogic, Hierarchical Syllogisms, KNOT, LogicBench, FOLIO, Deductive Logic)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same multi-step logical inference tasks described above requiring reasoning independent of parametric knowledge when prompted with counterfactual premises.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated under baseline prompts (standard/CoT) and Self-Segregation metacognitive prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported baseline accuracy gap of 47% between knowledge-consistent and knowledge-violating examples (largest among models reported). With Self-Segregation the gap reportedly reduced to 13% for this model.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Self-Segregation produced a large relative improvement versus baseline for this model (gap reduction 34 percentage points).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Strong reliance on parametric knowledge in baseline; large initial susceptibility to knowledge conflict. Improved but not fully solved by prompting—residual gap remained after Self-Segregation.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Even high-performing large models can default to parametric knowledge over provided premises; metacognitive prompting can substantially mitigate but not fully remove belief-bias effects.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8588.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8588.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gpt-4o-mini (ablation note)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-4o-mini (OpenAI model used in ablations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI model used in ablation studies that exhibited degraded performance under few-shot prompting in this study, highlighting sensitivity to in-context demonstrations compared to other models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An OpenAI family model referenced in ablations; used in few-shot/zero-shot/CoT comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Ablation studies on syllogistic/ belief-sensitivity experiments (zero-shot, few-shot, CoT) and perturbation with model-generated evidence</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Syllogistic and related logical inference datapoints with consistent/violate/random-gibberish belief conditions used to compare prompting regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated under few-shot, zero-shot, and Chain-of-Thought prompting as part of ablation G.1; also used in perturbation experiments generating supporting/contradicting evidence (G.2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper reports that for this OpenAI model, few-shot prompting unexpectedly degraded performance across belief types in the ablation, while CoT improved accuracy generally. Exact numeric accuracies for gpt-4o-mini are not tabulated in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>CoT > zero-shot and few-shot in many cases; gpt-4o-mini was uniquely sensitive and showed worse performance in few-shot compared to zero-shot/CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Sensitivity to in-context examples/formatting can harm performance; models often rely on surface/factual cues (ablation G.2 shows evidence alignment affects inference outcomes).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Prompting format and few-shot exemplars can negatively interact with belief-sensitivity; CoT is more robust but not sufficient to eliminate belief bias.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8588.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8588.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FOLIO (task-specific result)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FOLIO (first-order logic reasoning dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A long-form deductive reasoning benchmark (from Han et al., 2024) used by the authors where Self-Segregation produced little to no improvement, suggesting limitations of the metacognitive prompt for deep chained reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various evaluated models (11 LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>FOLIO problems require multi-step first-order logic deductions over narratives; models were evaluated with baseline prompts, CoT, and Self-Segregation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Long-form deductive reasoning requiring evaluation of conclusions from multi-step narratives using first-order logic.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Standard prompting, CoT, and the task-specific Self-Segregation variant (conclusion plausibility check followed by full narrative analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Self-Segregation yielded little or no improvement on FOLIO compared to other tasks; authors report that FOLIO's requirement for deep chains of reasoning may limit the efficacy of prompt-level metacognitive interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Other tasks (Hierarchical Syllogisms, KNOT) showed substantial gains with Self-Segregation; FOLIO did not, highlighting task-dependent limits.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Prompt-based metacognition insufficient for tasks with deep multi-step first-order logic chains; indicates need for more robust verification/ symbolic integration for long proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Metacognitive prompts help for many counterfactual/knowledge-conflict scenarios but do not replace methods designed for deep formal reasoning (e.g., symbolic CoT, verification frameworks).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LogicBench <em>(Rating: 2)</em></li>
                <li>FOLIO <em>(Rating: 2)</em></li>
                <li>Untangle the KNOT: Interweaving Conflicting Knowledge and Reasoning Skills in Large Language Models <em>(Rating: 2)</em></li>
                <li>Reasoning & Reciting - Deductive Logic (Wu et al., 2024) <em>(Rating: 2)</em></li>
                <li>A Systematic Analysis of Large Language Models as Soft Reasoners: The Case of Syllogistic Inferences (Bertolazzi et al.) <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8588",
    "paper_id": "paper-278959573",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "CounterLogic (overall eval)",
            "name_full": "CounterLogic benchmark evaluation of 11 LLMs across 6 reasoning datasets",
            "brief_description": "A systematic experimental evaluation reported in this paper where 11 state-of-the-art LLMs were tested across CounterLogic and five other reasoning datasets to measure performance degradation on knowledge-conflicting (counterfactual) logical reasoning and the effect of a metacognitive prompting intervention.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "11 state-of-the-art LLMs (various)",
            "model_description": "A heterogeneous set of 11 large language models spanning multiple providers, architectures, and parameter scales; accessed via OpenRouter/OpenAI API with unified inference settings (temperature 0, top-p 0.95, max tokens 4096).",
            "model_size": "various (reported range in paper: ~8B up to 671B)",
            "reasoning_task_name": "CounterLogic + Hierarchical Syllogisms + KNOT (explicit & implicit) + FOLIO + LogicBench + Deductive Logic",
            "reasoning_task_description": "A collection of strict logical reasoning tasks and benchmarks including the novel CounterLogic dataset (1,800 examples, 9 logical schemas) and several prior datasets; tasks require multi-step formal logical inference (syllogisms, modus ponens, hypothetical syllogism, constructive dilemma, etc.) mapped into natural language with controlled believability (knowledge-consistent vs knowledge-violating).",
            "method_or_approach": "Baseline prompting (standard direct prompts), chain-of-thought (CoT) prompting, few-shot/zero-shot ablations, and the paper's metacognitive Self-Segregation intervention (two-stage: knowledge-alignment assessment then standard reasoning using CoT).",
            "performance": "Aggregate baseline (across datasets/models): knowledge-consistent average accuracy ≈ 96%, knowledge-violating ≈ 69% (mean gap ≈ 27%). Applying Self-Segregation reduced the average gap from 27% to 11% and increased overall accuracy (paper reports overall +7.5% for some tasks). (Paper also reports dataset-specific numbers; see CounterLogic entry.)",
            "baseline_comparison": "Baseline = standard prompts (explicit instruction to reason only from given premises); CoT improves accuracy relative to zero-shot/few-shot in many cases; Self-Segregation (metacognitive two-stage prompt) consistently outperforms baseline/CoT alone across most tasks and models.",
            "limitations_or_failures": "Substantial, consistent performance degradation on knowledge-violating examples across all models; some reporting inconsistencies in aggregate numbers across paper sections. Self-Segregation is less effective or yields little improvement on tasks requiring deep multi-step chains (notably FOLIO). Models remain sensitive to surface/factual signals (see model-generated-evidence ablation) and struggle with negations, quantifiers, abstract variables.",
            "insights_or_conclusions": "Parametric knowledge interference (belief-bias-like effect) systematically harms strict logical reasoning in LLMs; explicit metacognitive prompting (Self-Segregation) that makes the model identify knowledge conflicts before reasoning substantially mitigates this effect without harming performance on knowledge-consistent examples. Chain-of-thought helps but does not eliminate belief sensitivity; deeper symbolic/verification methods may be needed for long chains of reasoning.",
            "uuid": "e8588.0",
            "source_info": {
                "paper_title": "If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "CounterLogic (dataset)",
            "name_full": "CounterLogic benchmark dataset",
            "brief_description": "A dataset introduced in this paper designed to evaluate multi-step logical reasoning under controlled knowledge-consistent and knowledge-conflicting (counterfactual) conditions: 1,800 natural-language examples spanning 9 logical schemas and balanced across validity and believability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Evaluated across the 11 LLMs in the study",
            "model_description": "N/A (dataset entry); used with chain-of-thought and Self-Segregation prompting to evaluate various LLMs.",
            "model_size": "N/A",
            "reasoning_task_name": "CounterLogic",
            "reasoning_task_description": "Consists of hierarchical entity triples (a ⊂ b ⊂ c) mapped to 8 sentence templates and 9 inference schemas (e.g., Modus Ponens, Hypothetical Syllogism, Constructive Dilemma). Examples are balanced: 50% valid/invalid and 50% knowledge-aligned/conflicting; natural-language conversion done via GPT-4o to avoid surface template exploitation.",
            "method_or_approach": "Evaluation using direct baseline prompts, chain-of-thought, and Self-Segregation (task-adapted reflection first on conclusion plausibility, then logical validity).",
            "performance": "On CounterLogic specifically, the paper reports baseline average accuracy: knowledge-consistent 88% vs knowledge-violating 85% (gap ~3%) and reports that Self-Segregation raised consistent to 93% and inconsistent to 90% (overall +5% on average for this dataset).",
            "baseline_comparison": "Standard prompting/CoT vs Self-Segregation: Self-Segregation improved both knowledge-consistent and knowledge-violating accuracies on this dataset without degrading performance on consistent examples.",
            "limitations_or_failures": "Dataset focuses on categorical syllogisms and propositional schemas (subset of logical reasoning); cannot capture all forms of complex logical reasoning and real-world conflicts; natural-language reformulation used GPT-4o which could inject artifacts.",
            "insights_or_conclusions": "A balanced benchmark that isolates belief alignment from logical validity, useful for diagnosing belief-bias interference in LLMs; in this dataset Self-Segregation produced modest but consistent improvements.",
            "uuid": "e8588.1",
            "source_info": {
                "paper_title": "If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Self-Segregation",
            "name_full": "Self-Segregation (metacognitive prompting)",
            "brief_description": "A prompting intervention introduced in this paper that elicits explicit metacognitive awareness from the model by asking it to first assess whether premises/conclusions align with its parametric knowledge, then perform logical reasoning based solely on given premises.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applied to all evaluated models (11 LLMs)",
            "model_description": "A two-phase prompt wrapper applied on top of standard Chain-of-Thought prompting: (1) Knowledge Alignment Assessment (is the statement true according to the model's knowledge?) (2) Standard Reasoning Process (evaluate logical validity using given premises).",
            "model_size": "applies to models of various sizes",
            "reasoning_task_name": "Applied across CounterLogic and five other logical reasoning datasets (Hierarchical Syllogisms, KNOT-E/KNOT-I, FOLIO, LogicBench, Deductive Logic)",
            "reasoning_task_description": "Used where tasks include a claim/premises and require binary validity judgment under potential knowledge conflict (counterfactual scenarios).",
            "method_or_approach": "Prompt-level metacognitive intervention; uses explicit belief-alignment question before invoking Chain-of-Thought reasoning; evaluated vs baseline prompts, zero-shot, few-shot, and CoT.",
            "performance": "Reported to reduce average performance gap between knowledge-consistent and knowledge-violating examples from 27% to 11% and to increase overall accuracy (paper reports +7.5% average improvement on some tasks; CounterLogic-specific +5%). Also reported to reduce per-model gaps (example: Qwen-2-72B gap from 47% down to 13%).",
            "baseline_comparison": "Compared to standard prompts and CoT: Self-Segregation consistently improved performance across most models/tasks and did not degrade performance on knowledge-consistent inputs.",
            "limitations_or_failures": "Less effective on tasks requiring deep long-chain reasoning (e.g., FOLIO showed little to no improvement). Performance gains may vary by language/domain and depend on the model's ability to truthfully report its own parametric beliefs; relies on prompt design rather than model architecture changes.",
            "insights_or_conclusions": "Making models explicitly acknowledge knowledge conflicts (epistemic compartmentalization) appears to reduce interference from parametric facts during logical inference; this suggests metacognitive prompting is a practical, model-agnostic mitigation that complements CoT but does not fully solve deep multi-step reasoning failures.",
            "uuid": "e8588.2",
            "source_info": {
                "paper_title": "If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Qwen-2-72B (example model)",
            "name_full": "Qwen-2 (72B) — example evaluated model",
            "brief_description": "A large multilingual model from Alibaba family (mentioned among evaluated systems) that exhibited one of the largest baseline performance disparities between knowledge-consistent and knowledge-violating reasoning examples in this study.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen-2-72B",
            "model_description": "An Alibaba Qwen-series LLM (large-scale, strong multilingual reasoning reported by vendor); included among the 11 models evaluated in the study.",
            "model_size": "72B",
            "reasoning_task_name": "Evaluated across the suite of reasoning tasks (CounterLogic, Hierarchical Syllogisms, KNOT, LogicBench, FOLIO, Deductive Logic)",
            "reasoning_task_description": "Same multi-step logical inference tasks described above requiring reasoning independent of parametric knowledge when prompted with counterfactual premises.",
            "method_or_approach": "Evaluated under baseline prompts (standard/CoT) and Self-Segregation metacognitive prompting.",
            "performance": "Reported baseline accuracy gap of 47% between knowledge-consistent and knowledge-violating examples (largest among models reported). With Self-Segregation the gap reportedly reduced to 13% for this model.",
            "baseline_comparison": "Self-Segregation produced a large relative improvement versus baseline for this model (gap reduction 34 percentage points).",
            "limitations_or_failures": "Strong reliance on parametric knowledge in baseline; large initial susceptibility to knowledge conflict. Improved but not fully solved by prompting—residual gap remained after Self-Segregation.",
            "insights_or_conclusions": "Even high-performing large models can default to parametric knowledge over provided premises; metacognitive prompting can substantially mitigate but not fully remove belief-bias effects.",
            "uuid": "e8588.3",
            "source_info": {
                "paper_title": "If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "gpt-4o-mini (ablation note)",
            "name_full": "gpt-4o-mini (OpenAI model used in ablations)",
            "brief_description": "An OpenAI model used in ablation studies that exhibited degraded performance under few-shot prompting in this study, highlighting sensitivity to in-context demonstrations compared to other models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-4o-mini",
            "model_description": "An OpenAI family model referenced in ablations; used in few-shot/zero-shot/CoT comparisons.",
            "model_size": "not specified in paper",
            "reasoning_task_name": "Ablation studies on syllogistic/ belief-sensitivity experiments (zero-shot, few-shot, CoT) and perturbation with model-generated evidence",
            "reasoning_task_description": "Syllogistic and related logical inference datapoints with consistent/violate/random-gibberish belief conditions used to compare prompting regimes.",
            "method_or_approach": "Evaluated under few-shot, zero-shot, and Chain-of-Thought prompting as part of ablation G.1; also used in perturbation experiments generating supporting/contradicting evidence (G.2).",
            "performance": "Paper reports that for this OpenAI model, few-shot prompting unexpectedly degraded performance across belief types in the ablation, while CoT improved accuracy generally. Exact numeric accuracies for gpt-4o-mini are not tabulated in the main text.",
            "baseline_comparison": "CoT &gt; zero-shot and few-shot in many cases; gpt-4o-mini was uniquely sensitive and showed worse performance in few-shot compared to zero-shot/CoT.",
            "limitations_or_failures": "Sensitivity to in-context examples/formatting can harm performance; models often rely on surface/factual cues (ablation G.2 shows evidence alignment affects inference outcomes).",
            "insights_or_conclusions": "Prompting format and few-shot exemplars can negatively interact with belief-sensitivity; CoT is more robust but not sufficient to eliminate belief bias.",
            "uuid": "e8588.4",
            "source_info": {
                "paper_title": "If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "FOLIO (task-specific result)",
            "name_full": "FOLIO (first-order logic reasoning dataset)",
            "brief_description": "A long-form deductive reasoning benchmark (from Han et al., 2024) used by the authors where Self-Segregation produced little to no improvement, suggesting limitations of the metacognitive prompt for deep chained reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Various evaluated models (11 LLMs)",
            "model_description": "FOLIO problems require multi-step first-order logic deductions over narratives; models were evaluated with baseline prompts, CoT, and Self-Segregation.",
            "model_size": "various",
            "reasoning_task_name": "FOLIO",
            "reasoning_task_description": "Long-form deductive reasoning requiring evaluation of conclusions from multi-step narratives using first-order logic.",
            "method_or_approach": "Standard prompting, CoT, and the task-specific Self-Segregation variant (conclusion plausibility check followed by full narrative analysis).",
            "performance": "Self-Segregation yielded little or no improvement on FOLIO compared to other tasks; authors report that FOLIO's requirement for deep chains of reasoning may limit the efficacy of prompt-level metacognitive interventions.",
            "baseline_comparison": "Other tasks (Hierarchical Syllogisms, KNOT) showed substantial gains with Self-Segregation; FOLIO did not, highlighting task-dependent limits.",
            "limitations_or_failures": "Prompt-based metacognition insufficient for tasks with deep multi-step first-order logic chains; indicates need for more robust verification/ symbolic integration for long proofs.",
            "insights_or_conclusions": "Metacognitive prompts help for many counterfactual/knowledge-conflict scenarios but do not replace methods designed for deep formal reasoning (e.g., symbolic CoT, verification frameworks).",
            "uuid": "e8588.5",
            "source_info": {
                "paper_title": "If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LogicBench",
            "rating": 2,
            "sanitized_title": "logicbench"
        },
        {
            "paper_title": "FOLIO",
            "rating": 2
        },
        {
            "paper_title": "Untangle the KNOT: Interweaving Conflicting Knowledge and Reasoning Skills in Large Language Models",
            "rating": 2,
            "sanitized_title": "untangle_the_knot_interweaving_conflicting_knowledge_and_reasoning_skills_in_large_language_models"
        },
        {
            "paper_title": "Reasoning & Reciting - Deductive Logic (Wu et al., 2024)",
            "rating": 2,
            "sanitized_title": "reasoning_reciting_deductive_logic_wu_et_al_2024"
        },
        {
            "paper_title": "A Systematic Analysis of Large Language Models as Soft Reasoners: The Case of Syllogistic Inferences (Bertolazzi et al.)",
            "rating": 2,
            "sanitized_title": "a_systematic_analysis_of_large_language_models_as_soft_reasoners_the_case_of_syllogistic_inferences_bertolazzi_et_al"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks",
            "rating": 1,
            "sanitized_title": "reasoning_or_reciting_exploring_the_capabilities_and_limitations_of_language_models_through_counterfactual_tasks"
        }
    ],
    "cost": 0.015245749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?
28 May 2025</p>
<p>Ishwar B Balappanawar ishwar.balappanawar@students.iiit.ac.in 
IIIT Hyderabad</p>
<p>Vamshi Krishna Bonagiri vamshi.b@research.iiit.ac.in 
IIIT Hyderabad</p>
<p>Anish R Joishy anish.joishy@research.iiit.ac.in 
IIIT Hyderabad</p>
<p>Manas Gaur 
University of Maryland
Baltimore County</p>
<p>Krishnaprasad Thirunarayan 
Wright State University</p>
<p>Ponnurangam Kumaraguru 
IIIT Hyderabad</p>
<p>If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?
28 May 2025FE3B71A1B2FB1F74E074F53B1BAF1F91arXiv:2505.22318v1[cs.CL]
Large Language Models (LLMs) demonstrate impressive reasoning capabilities in familiar contexts, but struggle when the context conflicts with their parametric knowledge.To investigate this phenomenon, we introduce Coun-terLogic, a dataset containing 1,800 examples across 9 logical schemas, explicitly designed to evaluate logical reasoning through counterfactual (hypothetical knowledge-conflicting) scenarios.Our systematic evaluation of 11 LLMs across 6 different datasets reveals a consistent performance degradation, with accuracies dropping by 27% on average when reasoning through counterfactual information.We propose "Self-Segregate", a prompting method enabling metacognitive awareness (explicitly identifying knowledge conflicts) before reasoning.Our method dramatically narrows the average performance gaps from 27% to just 11%, while significantly increasing the overall accuracy (+7.5%).We discuss the implications of these findings and draw parallels to human cognitive processes, particularly on how humans disambiguate conflicting information during reasoning tasks.Our findings offer practical insights for understanding and enhancing LLMs' reasoning capabilities in real-world applications, especially where models must logically reason independently of their factual knowledge.Our data and code are available here.</p>
<p>Introduction</p>
<p>LLMs have demonstrated remarkable reasoning capabilities across diverse domains, exhibiting proficiency in tasks ranging from elementary problem solving to complex-level multi-step reasoning challenges (Wei et al., 2023;Schick and Schütze, 2021;Kojima et al., 2022;Brown et al., 2020;Zhou et al., 2023a;Patel et al., 2024).Despite these advances, they often exhibit a significant performance degradation when reasoning with information that con- flicts with their parametric knowledge (knowledge acquired during pre-training) (Xu et al., 2024b;Dasgupta et al., 2024;Wu et al., 2024;Lampinen et al., 2024;Wang et al., 2024;Jin et al., 2024;Su et al., 2024).</p>
<p>In Figure 1, the two syllogisms (A logical argument with two premises and a conclusion) are logically equivalent.However, while LLMs excel at reasoning through the first example, they often struggle significantly with the second, despite being explicitly instructed to reason based solely on the given premises (Trichelair et al., 2023;Talmor et al., 2020).This disparity suggests that when faced with premises that contradict their parametric knowledge, LLMs often fail to maintain consistent reasoning performance.</p>
<p>The ability to reason effectively in scenarios with potentially conflicting information is crucial for deploying LLMs in real-world applications where they must process information that may be novel, unexpected, or even contradictory to their training data (Wang et al., 2024).Consider the question: "If the Earth had two suns, how would seasons differ from what we experience now?".Such situations arise frequently in everyday contexts (Pearl and Mackenzie, 2018), and failure to reason in them could lead to unreliable performance (Zhang et al., 2024).Additionally, prior research also suggests that evaluating reasoning in counterfactual situations may serve as a more robust assessment of a model's reasoning capabilities (Wu et al., 2024), as standard reasoning tasks can potentially be hacked through pattern matching.(Lewis and Mitchell, 2024;Wu et al., 2024;Liu et al., 2024;McCoy et al., 2019;Kaushik et al., 2020) While knowledge conflicts are actively studied in language models, prior investigations have focused on relatively simple tasks involving information extraction or single-step reasoning (Example: Who is the current president of the USA?) (Xie et al., 2024b).These studies typically examine how models handle conflicts when retrieving or extracting knowledge directly from their parameters or from provided text.However, there has been limited exploration of how knowledge conflicts affect complex multi-step logical reasoning processes, which is a capability essential for reliable AI systems (Wang et al., 2024;Liu et al., 2024).</p>
<p>To address this gap, we introduce the Coun-terLogic dataset, specifically designed to evaluate complex logical reasoning in counterfactual scenarios.CounterLogic features approximately 1,800 examples spanning 9 logical schemas, carefully balanced across knowledge-consistent (contexts that align with parametric knowledge) and knowledgeconflicting (counterfactual) scenarios.Through a systematic evaluation of 11 state-of-the-art LLMs on 6 different datasets (including CounterLogic), we demonstrate a consistent pattern of performance degradation (-27% on average) when reasoning through counterfactual statements.</p>
<p>We introduce "Self-Segregation", a metacognitive intervention that involves identifying knowledge conflicts before reasoning through a task.Through a series of experiments, we show that this simple strategy, used on top of existing methods such as chain-of-thought (COT) prompting (Wei et al., 2023), significantly boosts LLM reasoning abilities, specifically in counterfactual scenarios.Our results show that with Self-Segregation, the average accuracy gap between knowledge-consistent and knowledge-violating scenarios drops by 16% (from 27% to 11%), while the overall accuracy improves by 7.5%.</p>
<p>Our findings suggest that the initial performance disparity could stem from unresolved or ignored knowledge conflicts rather than inherent limitations in logical reasoning capabilities.Notably, these performance patterns in LLMs mirror human cognitive reasoning processes.By introducing selfsegregation strategies, we can potentially enable LLMs to more effectively compartmentalize conflicting information and apply logical operations, independent of their parametric factual knowledge (Thomas et al., 2013).Our approach draws inspiration from human metacognitive strategies for resolving ambiguities and knowledge conflicts, suggesting a promising direction for enhancing logical reasoning capabilities in language models.</p>
<p>Our contributions can be summarized as follows:</p>
<ol>
<li>
<p>We introduce CounterLogic, a novel dataset for evaluating logical reasoning in counterfactual scenarios, and demonstrate that contemporary models consistently underperform in these contexts despite their strong performance otherwise.</p>
</li>
<li>
<p>We propose a simple yet effective metacognitive awareness intervention, Self-Segregation, that involves prompting models to explicitly identify knowledge conflicts before reasoning.</p>
</li>
</ol>
<p>Our method significantly improves reasoning in knowledge-violating contexts, reducing the performance gap by 16%.</p>
<ol>
<li>Through a series of experiments, we study and discuss how knowledge conflicts impair reasoning in LLMs and how metacognitive interventions can mitigate these effects, drawing parallels to human cognitive processes.</li>
</ol>
<p>2 Related Work</p>
<p>Logical Reasoning in LLMs</p>
<p>Recent advancements in LLMs have demonstrated significant reasoning capabilities through techniques like chain-of-thought prompting (Wei et al., 2023) (guiding models to show intermediate reasoning steps), zero-shot reasoning (Kojima et al., 2022) (reasoning without task-specific examples), and tree-of-thought exploration (Shinn et al., 2023) (exploring multiple reasoning paths).While these methods have improved performance across various benchmarks (Clark et al., 2020;Parmar et al., 2024), studies comparing human and LLM reasoning patterns reveal that models continue to exhibit systematic errors mirroring human reasoning biases (Dasgupta et al., 2024;Eisape et al., 2024).</p>
<p>Research specifically examining logical reasoning limitations shows that models struggle with operations involving negations, quantifiers, and abstract variables (Dasgupta et al., 2024;Bertolazzi et al., 2024).Performance notably degrades when reasoning involves counterfactual information (Chen et al., 2025;Wu et al., 2024), with inconsistent handling of logically equivalent problems presented in different formats (Estermann et al., 2025).Approaches addressing these limitations include symbolic chain-of-thought (Xu et al., 2024a) (integrating symbolic representations into reasoning steps), verification mechanisms (Vacareanu and Ballesteros, 2024) (validating reasoning against formal rules), and theory resolution frameworks (Toroghi et al., 2024) (applying systematic proof methods), all aimed at enhancing logical consistency in complex scenarios.</p>
<p>Knowledge Conflicts and Counterfactual Reasoning</p>
<p>LLMs encode substantial factual knowledge in their parameters (Roberts et al., 2020;Petroni et al., 2019), creating challenges when encountering conflicting information.Recent studies categorize these conflicts into context-memory, intercontext, and intra-memory conflicts (Xu et al., 2024b) based on where the conflicting information originates.Larger models typically default to parametric knowledge over conflicting contextual evidence (Longpre et al., 2021), though this varies based on evidence coherence and source reliability (Wang et al., 2023).</p>
<p>Counterfactual reasoning presents significant challenges, with models often performing poorly on tasks involving hypothetical scenarios that contradict established facts (Chen et al., 2025;Wei et al., 2023).Performance on questions with counterfactual premises drops significantly compared to standard tasks, primarily due to conflicts between parametric knowledge and counterfactual assertions (Lin et al., 2025).Mitigation strategies include counterfactual data augmentation (Neeman et al., 2023) (training on synthetically altered data), specialized prompting techniques (Xie et al., 2024b), and distilled counterfactuals (Chen et al., 2023b) (generating targeted examples that highlight conflicts).</p>
<p>Metacognition, Belief Bias, and Human</p>
<p>Reasoning Parallels</p>
<p>Human reasoning exhibits well-documented cognitive biases, including belief bias, where argument validity judgments are influenced by conclusion believability rather than logical structure (Markovits and Nantel, 1989).This bias intensifies with task difficulty (Trippas et al., 2014) and creates an "illusion of objectivity" (Kunda, 1990), where individuals believe their reasoning is unbiased despite evidence to the contrary.</p>
<p>LLMs mirror these human cognitive patterns, performing better when semantic content supports logical inferences (Dasgupta et al., 2024) and reasoning more effectively about believable situations compared to implausible ones (Macmillan-Scott and Musolesi, 2024).Even advanced models exhibit systematic errors paralleling human reasoning biases (Eisape et al., 2024), suggesting shared underlying mechanisms despite the different architectures.</p>
<p>Metacognitive strategies in humans improve logical reasoning by distinguishing between belief evaluation and logical assessment (Douven et al., 2018) -essentially separating "what I know" from "what follows logically."Similar capabilities are emerging in LLMs, including uncertainty estimation (Zhou et al., 2023b) (expressing confidence in outputs), self-evaluation (Wang et al., 2024) (critiquing own reasoning), and belief identification (Chen et al., 2023a) (recognizing when premises conflict with knowledge).When confirmation bias is modulated by confidence, systems become more receptive to corrective information when confidence is low (Rollwage and Fleming, 2021), suggesting potential mechanisms for improving reasoning with conflicting knowledge in LLMs.</p>
<p>The CounterLogic Dataset</p>
<p>Despite significant advances in evaluating LLMs' logical reasoning capabilities (Wei et al., 2023;Schick and Schütze, 2021;Kojima et al., 2022;Brown et al., 2020;Zhou et al., 2023a;Patel et al., 2024), existing benchmarks fail to systematically disentangle logical validity from belief alignment (whether premises align with parametric knowledge).</p>
<p>As shown in Table 1, current benchmarks either focus on logical structure without controlling for knowledge conflicts (e.g., LogicBench (Parmar et al., 2024), FOLIO (Han et al., 2024)) or em- The dataset features hierarchical entity triples (e.g., siameses ⊂ cats ⊂ felines) mapped to 8 logical sentence templates across 9 inference schemas (see Appendix A).Each example is balanced across validity (50% valid/invalid) and believability (50% aligned/conflicting), with ground truth annotations for both dimensions.The dataset construction combines subset relationships with propositional logic forms (Modus Ponens, Hypothetical Syllogism, etc.) to systematically evaluate knowledge-logic interactions.(B) Our Self-Segregate method: While the standard prompt simply presents LLMs with a counterfactual context followed by related questions, our Self-Segregate approach first engages the model metacognitively by eliciting its responses to knowledge-alignment questions.(This could be as simple as asking whether a given statement is true).phasize knowledge conflicts with simple reasoning tasks (e.g., KNOT (Liu et al., 2024), Reasoning &amp; Reciting (Wu et al., 2024)).To address this gap, we introduce CounterLogic, a benchmark dataset containing 1,800 examples across 9 logical schemas with an equal balance in knowledge-consistent and counterfactual datapoints.The dataset systematically combines hierarchical entity relationships with various levels of formal logical structures to evaluate the interaction between knowledge and reasoning in LLMs.</p>
<p>Dataset Construction</p>
<p>As illustrated in Figure 2, the CounterLogic dataset was constructed through a four-stage process:</p>
<p>(1) Entity Perturbation: We begin with hierarchical entity triples (a, b, c) representing strict subset relationships: a ⊂ b ⊂ c.These include natural taxonomies such as siameses ⊂ cats ⊂ felines.</p>
<p>(2) Sentence Pair Generation: These entities are mapped to four sentence templates forming complementary logical pairs S and ¬S (e.g., "All {A} are {B}" and "Some {A} are not {B}"), yielding diverse sentence pairs that serve as atomic propositions.The complete set of triplets and sen-tence templates is detailed in Appendix A.1.</p>
<p>To ensure systematic coverage, we enforce entity relationship balance: 25% with correct hierarchical relationship (e.g., siameses ⊂ cats), 25% with inverted relationship (e.g., cats ⊂ siameses), and 50% with unrelated entity pairs.All four sentence-pair templates are distributed evenly across examples.</p>
<p>(3) Logical Query Generation: Inspired by LogicBench (Parmar et al., 2024), these sentence pairs are then incorporated into formal logical structures according to the inference schemas such as Modus Ponens (MP), Hypothetical Syllogism (HS), Constructive Dilemma (CD), etc. (See Table 2 in Appendix C).A template-based converter is used to transform these sentences into logical structures.</p>
<p>(4) Natural Language Task Generation: We create binary question-answer tasks with systematic variation across (1) Logical validity (whether conclusions follow from premises) and (2) Knowledge alignment (whether conclusions match parametric knowledge).In order to ensure the LLMs are not simply answering questions via memorized logical rules (Xie et al., 2024a;Wu et al., 2024) , we convert the logical queries to natural language using GPT-4o (see Appendix A.4).We systemat-Table 1: Comparison of logical reasoning benchmarks.CounterLogic uniquely combines multi-step reasoning with knowledgeconflicting scenarios while maintaining balance in labels.This enables rigorous evaluation of how parametric knowledge affects LLMs' logical reasoning capabilities, addressing limitations in existing benchmarks that typically lack proper balance across important evaluation dimensions.While the Syllogistic dataset contains knowledge conflicts data in a balanced manner, it severely lacks natural language queries, diversity, and depth in logical rules (only syllogism).</p>
<p>Dataset</p>
<p>Size # Reasoning Steps</p>
<p>Knowl. Conflict Balanced Labels</p>
<p>LogicBench (Parmar et al., 2024) 2,020 1 ∼ 5 × × FOLIO (Han et al., 2024) 1,435 0 ∼ 7 × × KNOT (Liu et al., 2024) 5,500 1 ∼ 2 ✓ × Reasoning &amp; Reciting -Deductive Logic (Wu et al., 2024) 81 0 ∼ 7 ✓ × Syllogistic (Bertolazzi et al., 2024) 2,120 2 ✓ ✓
CounterLogic (Ours) 1,800 1 ∼ 5 ✓ ✓
ically assign ground truth belief status using the initial sentence beliefs (obtained through hierarchically valid triplets), and logical validity using the rules described in Table 2. Additionally, for each logical form, we construct both Valid (Instances where the conclusion logically follows from the premises) and Invalid Instances (where the logical structure is violated by replacing the statements in the conclusion with statements that cannot be inferred from the premises) examples.This construction allows for a controlled investigation of reasoning performance in the presence or absence of knowledge alignment.</p>
<p>Methodology</p>
<p>Research Questions</p>
<p>Our investigation focuses on three primary research questions:</p>
<ol>
<li>RQ1 How do LLMs perform on logical reasoning tasks in counterfactual scenarios, compared to knowledge-consistent scenarios?2. RQ2: Can prompt-based interventions that modify how models approach reasoning tasks, have any effect?3. RQ3: What mechanisms might explain the observed differences?</li>
</ol>
<p>To address these questions, we conduct a series of experiments across multiple reasoning tasks (Section 4.4), models (Appendix, Section E), and prompting strategies (Appendix, Section B).We first establish baseline performance across 6 reasoning tasks to quantify the impact of knowledge conflicts on reasoning (RQ1), in Section 5.1.We then evaluate our most effective prompt-based intervention (RQ2), "Self-Segregate", in Section 5.2.Finally, we discuss insights from our experiments in Section 6 (RQ3).</p>
<p>Evaluation Methodology</p>
<p>We evaluate 11 state-of-the-art LLMs (listed in Figure 3 and Appendix Section E) spanning different architectures, parameter scales, and training paradigms.To ensure robust performance measurements, we employ self-consistency checks through multiple sampled outputs per datapoint (5 generations per example), and report their respective mean and variance.This approach accounts for generation variability, as LLMs may produce inconsistent results with similar queries (Bonagiri et al., 2024).</p>
<p>Self-Segregation</p>
<p>LLMs tend to process premises directly without explicitly considering whether these premises conflict with their parametric knowledge (This can sometimes occur in extended COT reasoning, but our method proves to be superior).Self-Segregate introduces a metacognitive step that requires models to first identify whether premises align with or contradict their knowledge before performing logical reasoning (illustrated in Figure 2B).The method works in two distinct phases:</p>
<ol>
<li>
<p>Knowledge Alignment Assessment: Models first examine the premises or conclusion and explicitly state whether they align with or contradict their parametric knowledge.This creates an explicit awareness of a "boundary" between the model's factual knowledge and the reasoning task.</p>
</li>
<li>
<p>Standard Reasoning Process: models proceed to evaluate the logical validity of the argument based solely on the given premises.We use COT as our standard prompt due to its superior performance, and compare against it in all of our results.</p>
</li>
</ol>
<p>Our approach is inspired by human metacognitive strategies for handling conflicting information (i.e, when humans consciously recognize that information contradicts their existing knowledge, they can more effectively reason through it by temporarily compartmentalizing that conflict) (Wang and Zhao, 2024;Thomas et al., 2013).</p>
<p>Reasoning Datasets</p>
<p>Along with CounterLogic, we evaluate performance across six other reasoning tasks, each designed to assess specific aspects of logical reasoning under knowledge conflicts (see Table 3 in Appendix D).For each of the following tasks, we implement a tailored version of our Self-Segregation method.The following are the tasks:</p>
<p>Hierarchical Syllogisms: Derived from classical syllogistic reasoning and adapted from Bertolazzi et al. ( 2024)'s work, this task presents logically structured arguments where the conclusion may conflict with world knowledge.Each example contains two premises and a conclusion, with models evaluating logical validity.For Self-Segregation, models first assess the conclusion statement in isolation for its alignment with parametric knowledge, then evaluate the full syllogism's logical validity(see Figure 2B).</p>
<p>KNOT: Adapted from the Knowledge Conflict Resolution benchmark (Liu et al., 2024), this task evaluates reasoning through explicit (KNOT-E) and implicit (KNOT-I) conflict resolution.Each instance contains a passage with counterfactual information, a question, and an answer.The Self-Segregation implementation first presents the answer in isolation for plausibility assessment, then provides the full passage and question-answer pair for contextual reasoning.This separation tests models' ability to distinguish between prior knowledge and contextual truth.</p>
<p>FOLIO: Using long-form deductive reasoning problems from FOLIO (Han et al., 2024), this task requires evaluating whether conclusions logically follow from multi-step narratives.Our Self-Segregation approach first presents the conclusion for isolated plausibility judgment, then provides the complete narrative for logical analysis.</p>
<p>LogicBench: This reasoning dataset (Parmar et al., 2024) combines first-order, non-monotonic, and propositional logic problems.It tests models' ability to follow formal logical rules while overriding potentially conflicting parametric knowledge.The Self-Segregation implementation presents questions and answers without supporting context for initial plausibility assessment, followed by complete logical contexts for formal evaluation.</p>
<p>Reasoning and Reciting, Deductive Logic: Adapted from (Wu et al., 2024) , this task evaluates deductive logic over premise sets.Models must determine whether claims logically follow from premises, regardless of whether those premises contradict physical knowledge.The Self-Segregation implementation presents claims in isolation for plausibility assessment before introducing the complete premise set for logical evaluation.An example presents non-physical premises about objects floating forever, testing the ability to follow logical rules despite contradicting physical knowledge.</p>
<p>CounterLogic: For our novel benchmark, described in Section 3, we apply the same two-stage reflection approach used in the Hierarchical Syllogisms task, first assessing conclusion plausibility in isolation before evaluating logical validity within the full syllogistic context.</p>
<p>Results and Analysis</p>
<p>Our experimental evaluation reveals consistent patterns across all models and tasks, confirming that:</p>
<p>(1) LLMs struggle significantly when reasoning through counterfactual premises and (2) metacognitive awareness interventions via Self-Segregation substantially improve performance in knowledgeconflicting scenarios.We discuss it in detail in this section (Figures 3 and 4 summarize the results, for more detailed results in a table format, please see Table 5 in Appendix F).</p>
<p>Knowledge Conflicts Significantly Impair LLM Logical Reasoning</p>
<p>As shown in Figure 4, when evaluated on the reasoning tasks, all models demonstrate a substantial performance gap between knowledge-consistent and counterfactual scenarios.We find that this holds even across various prompting strategies like Zero-Shot, Few-Shot, and Chain-of-Thought Prompting (more information in Appendix G).Under the baseline condition, models achieve considerably higher accuracy on knowledgeconsistent examples 96% (on average) compared to knowledge-violating examples 69% on average, with performance gaps of about 27% averaged across models.</p>
<p>This pattern holds consistently across all the models, indicating that the phenomenon is not spe- The right bar (sky blue) for each model represents accuracy using standard prompts (ref), while the left bar (salmon) shows accuracy using our Self-Segregate prompts (ref).Self-Segregate consistently improves performance across tasks, including KNOT, LogicBench, FOLIO, Hierarchical Syllogisms, and Deductive Logic.All models were run using the OpenRouter API.cific to particular models or training paradigms.Notably, even the most capable models exhibit this disparity, suggesting that knowledge-conflict interference represents a fundamental challenge in LLM reasoning rather than merely a limitation of smaller or less capable models.Models like Qwen-2-72B show the highest accuracy difference of 47% in the baseline setup, which then greatly improves in the self-segregation setup bringing the gap down to 13%.</p>
<p>This gap appears despite explicit instructions to reason based solely on given premises (see Appendix B), highlighting the pervasive nature of parametric knowledge interference in logical reasoning tasks.Our findings on the CounterLogic dataset further confirm this pattern, with an average performance of 88% on knowledge-consistent examples, 85% on knowledge-violate examples and an average performance gap of 3% on the baseline condition (Figure 4).</p>
<p>Self-Segregation Dramatically Improves both Counterfactual and Overall Performance</p>
<p>Our Self-Segregation method (described in Section 4.3) yields substantial improvements across all evaluated models and datasets.As illustrated in Figure 3, this approach consistently improves the overall accuracy across most models and tasks.Figure 3 presents this improvement across six distinct reasoning tasks.The most dramatic gains are observed on the Hierarchical Syllogisms task, where Self-Segregation improves overall accuracy by an average of 7.5%.</p>
<p>We observe that the self-segregation strategy was more effective for datasets like Hierarchical Syllogisms, KNOT (Implicit and Explicit), and they show the most improvement, while there was little to no improvement on the FOLIO, emphasizing the need for better conflict resolution strategies for tasks that involve deep chains of reasoning (Han et al., 2024) .</p>
<p>The results on our CounterLogic also follow the same trend, with the overall accuracy performance increasing by 5% on average.The performance on knowledge-consistent examples rose to 93% from 88%, and the performance on knowledgeinconsistent examples to 90% from 85%.</p>
<p>Importantly, this intervention improves reasoning on knowledge-violating scenarios without degrading performance on knowledge-consistent ones.In fact, as shown in Figure 4, accuracy on knowledge-consistent examples also improves slightly under the metacognitive condition, suggesting that explicit reflection on knowledge alignment benefits logical reasoning more generally.</p>
<p>Discussion</p>
<p>Our findings reveal a fundamental tension in how LLMs approach logical reasoning when faced with information that contradicts their parametric knowledge.The consistent performance gap observed across models and tasks suggests this challenge is intrinsic to current language model architectures and training paradigms, rather than a limitation of specific models.</p>
<p>This performance disparity echoes welldocumented phenomena in human reasoning.Cognitive psychologists have long observed belief  bias effects, where humans judge argument validity based on conclusion believability rather than logical structure (Markovits and Nantel, 1989;Lampinen et al., 2024).The parallel between human and LLM reasoning biases suggests deeper connections between the cognitive mechanisms underlying both.This alignment in behavior also highlights the potential of leveraging cognitive theories to inform the design of more robust and interpretable language model reasoning frameworks.While humans can override this bias through deliberate metacognitive effort, our experiments demonstrate that LLMs similarly benefit from prompted metacognitive approaches (namely our Self-Segregate method).</p>
<p>The effectiveness of our metacognitive intervention provides insight into how LLMs process conflicting information.By explicitly prompting models to identify knowledge conflicts before reasoning, we create a form of epistemic compartmentalization (Thomas et al., 2013), helping models distinguish between what they "know" from their parameters and what they must accept as given in the current reasoning context.Our approach appears to reduce interference between factual knowledge retrieval and logical operation application, allowing models to maintain logical consistency even when processing counterfactual premises.Our proposed approach is a simple abstraction derived from a set of extensive experiments, with meaningful insights.</p>
<p>Conclusion and Future Work</p>
<p>We demonstrated that LLMs struggle with logical reasoning when premises contradict their parametric knowledge, with performance dropping by 35% in counterfactual scenarios.Our key contribution is the CounterLogic benchmark and the identification of a simple yet effective metacognitive intervention called Self-Segregation, that narrows this performance gap to just 15%.By prompting models to explicitly identify knowledge conflicts before reasoning, we hypothesize that this approach enables more effective compartmentalization of conflicting information without requiring model modifications.</p>
<p>Future work could (1) explore how knowledge conflicts manifest within model representations, (2) investigate applications of metacognitive techniques in other reasoning domains, (3) extend this evaluation to more complex, real-world scenarios, where counterfactual thinking is necessary, (4) Build on methods to either improve, or fundamentally address issues involving parametric memory clashes with reasoning performance, etc.By addressing this specific limitation in counterfactual reasoning, our work contributes to building more robust AI systems capable of reliable logical inference even in contexts that conflict with their training data.</p>
<p>Limitations</p>
<p>While our study provides valuable insights into LLMs' reasoning under knowledge conflicts, sev-eral limitations should be noted.First, our Coun-terLogic dataset, while diverse, cannot capture all forms of complex logical reasoning or knowledge conflicts that might arise in real-world applications.The dataset focuses primarily on categorical syllogisms and propositional logic structures, which represent only a subset but a fundamental part of logical reasoning paradigms.</p>
<p>Second, our experiments were conducted on a specific set of models available at the time of study; newer models may exhibit different patterns of knowledge interference or respond differently to our proposed interventions.The rapid pace of model development means that architectural innovations might soon produce systems with intrinsically different approaches to handling counterfactual information.</p>
<p>Third, the effectiveness of our interventions may vary across different languages, cultures, and knowledge domains, as parametric knowledge itself varies across these dimensions.Our evaluation focused on English-language reasoning with common-knowledge concepts; performance on specialized domains or other languages would require further testing.</p>
<p>Fourth, while we draw parallels to human cognition, the mechanisms of knowledge interference in LLMs may differ fundamentally from human reasoning processes.These parallels provide useful conceptual frameworks but should not be interpreted as evidence of identical or robust cognitive processes.</p>
<p>Finally, while we evaluate LLMs, we also use them to synthetically generate natural language queries, which may pose unnoticed errors, such as inconsistencies or limitations from the LLMs themselves being carried over.</p>
<p>Despite these limitations, our findings demonstrate consistent and substantial improvements in counterfactual reasoning across diverse models and tasks, suggesting that the core insights about metacognitive awareness and knowledge compartmentalization are likely to remain relevant even as specific implementations evolve.</p>
<p>Ethics Statement</p>
<p>This research adheres to the ACL Ethics Policy and addresses several important ethical considerations:</p>
<p>Research Integrity We prioritize transparency and reproducibility throughout our work.All experiments are documented with sufficient detail to enable replication by other researchers.We clearly identify the limitations of our methods and findings in Section 8, acknowledging the boundaries of our conclusions and where further investigation is warranted.</p>
<p>Attribution and Contribution While we utilized large language models as tools to assist with certain aspects of writing and implementation, all research ideas, experimental design, analysis of results, and scientific conclusions presented in this paper are solely attributable to the authors.We have properly credited all relevant prior work and acknowledge the contributions of the research community upon which our work builds.</p>
<p>Data and Resource Considerations</p>
<p>The Coun-terLogic dataset was constructed using synthetic data and template-based generation methods that do not involve the collection of personally identifiable information or data from human subjects.Our evaluation utilized commercially available large language model APIs through standardized interfaces, ensuring fair comparison across systems.</p>
<p>Release of Materials</p>
<p>We commit to releasing all artifacts from this research upon acceptance, if not done already, This includes:</p>
<p>• The complete CounterLogic dataset, including all examples and annotations.(Already has been provided via the anonymous GitHub link).</p>
<p>• Reproducible evaluation code and scripts used in our experiments.(Already provided via the anonymous GitHub link).</p>
<p>• Implementation details of our Self-Segregation method</p>
<p>• Prompt templates and model configurations</p>
<p>• Comprehensive documentation to facilitate use by other researchers</p>
<p>Environmental Considerations We acknowledge the computational resources required for our experiments.To minimize environmental impact, we designed our evaluation to be as efficient as possible, reusing model instances where appropriate and limiting the number of inference runs to the minimum necessary for statistically significant results.</p>
<p>Potential Applications and Impact</p>
<p>The insights and techniques presented in this paper aim to improve the robustness of logical reasoning in AI systems, particularly when handling counterfactual scenarios.These improvements have potential benefits for various applications requiring reliable reasoning capabilities (including education, scientific exploration, and decision support systems) while minimizing the risk of logical errors stemming from knowledge conflicts.</p>
<p>Figure 1 :
1
Figure 1: Example tasks demonstrating LLM reasoning.While LLMs correctly reason through standard, knowledgeconsistent tasks, they often incorrectly assess counterfactual (hypothetical knowledge-conflicting) tasks despite having the same logical structure.</p>
<p>Figure 2 :
2
Figure 2: (A) Dataset Preparation:The dataset features hierarchical entity triples (e.g., siameses ⊂ cats ⊂ felines) mapped to 8 logical sentence templates across 9 inference schemas (see Appendix A).Each example is balanced across validity (50% valid/invalid) and believability (50% aligned/conflicting), with ground truth annotations for both dimensions.The dataset construction combines subset relationships with propositional logic forms (Modus Ponens, Hypothetical Syllogism, etc.) to systematically evaluate knowledge-logic interactions.(B) Our Self-Segregate method: While the standard prompt simply presents LLMs with a counterfactual context followed by related questions, our Self-Segregate approach first engages the model metacognitively by eliciting its responses to knowledge-alignment questions.(This could be as simple as asking whether a given statement is true).</p>
<p>Figure 3 :
3
Figure 3: Accuracy comparision between the baseline setup and our metacognitive self-segregation setup across models.The right bar (sky blue) for each model represents accuracy using standard prompts (ref), while the left bar (salmon) shows accuracy using our Self-Segregate prompts (ref).Self-Segregate consistently improves performance across tasks, including KNOT, LogicBench, FOLIO, Hierarchical Syllogisms, and Deductive Logic.All models were run using the OpenRouter API.</p>
<p>(a) Hierarchical Syllogisms task.(b) CounterLogic task.</p>
<p>Figure 4 :
4
Figure 4: Accuracy comparison between knowledge-consistent and knowledge-violating examples across models.The left panel in each subfigure shows results using ground-truth knowledge-alignment labels (Baseline), and the right panel shows performance when models (Refer legend in Figure-3) use their own knowledge-alignment prediction (self-segregation).Blue bars represent knowledge-consistent examples, while orange bars indicate knowledge-violating ones.The self-segregation setup not only improves accuracy across both subsets but also significantly reduces the performance disparity between them, demonstrating the effectiveness of metacognitive prompting in enhancing belief-robust reasoning.</p>
<p>A CounterLogic Dataset DetailsA.1 Hierarchical Entity TriplesThe CounterLogic dataset uses the following hierarchical entity triples, where each tuple (a, b, c) denotes a strict subset relationship: a ⊂ b ⊂ c.A.3 Dataset StatisticsThe final CounterLogic dataset consists of 1,800 examples, with 200 instances for each of the 9 logical schemas listed in Table2. To ensure a comprehensive and balanced design, four criteria were enforced during dataset construction:• Knowledge alignment Balance: Each logical schema contains 50% of examples where the conclusion is knowledge-consistent under human priors and 50% where it is not.• Validity Balance: Half the examples per schema are logically valid, while the remaining half intentionally violate the logical structure.• Entity Relationship Balance: 25% of examples involve an entity A that is a subset of entity B (e.g., siameses ⊂ cats), 25% feature B as a subset of A (e.g., cats ⊂ siameses), and 50% use unrelated entity pairs (e.g., pines and dogs).• Sentence Template Balance: All eight sentence-pair templates are applied evenly across examples within each logical schema, promoting lexical and syntactic diversity.A.4 Prompt for Natural Language Reformulation of Logical StructuresTo prevent language models from relying on memorized patterns of formal logic structures, we reformulate logical premises and conclusions into natural language contexts and questions.Specifically, we prompt GPT-4o to carry out this transformation.The model is instructed to rewrite the given premise into a natural language context and the conclusion into a straightforward question, without preserving the surface structure of the original logical form.The transformation ensures the resulting question does not include meta-references like "in this context," and the phrasing is natural and intuitive:premise : [ premise ] conclusion :[ conclusion ]premise list : [ premise list ] Make the premise into a context which is like a natural language way of writing the premises .Make conclusion into a question .The context / questions shouldn 't be too complicated but shouldn 't directly be like premise / conclusion either .The question must be asked normally without stating things like " in this context " or " with this information ". premise list is only given for your better understanding .Reply ONLY with a json with two keys ' context ' and ' question 'B Prompting StrategiesWe evaluate three distinct prompting strategies across all tasks:1. Standard Condition: In this baseline condition, models receive direct questions with minimal guidance, instructed to consider only the logical validity of arguments regardless of premise believability:Based on the following premises , determine if the conclusion logically follows .Consider only the logical validity based on the given premises , regardless of whether the premises themselves are factually true .Premises :1Does the conclusion logically follow from the premises ?Answer with " Yes " or " No " and explain your reasoning step by step .Metacognitive Condition:In this condition, we introduce a preliminary reflection step (asking the model what it thinks about a statement) before the reasoning task.Does the conclusion logically follow from the premises ?Answer with " Yes " or " No " and explain your reasoning step by step .The above is the general structure of our prompting method.The prompts are modified according to the dataset we evaluate.C Logical Inference SchemasThe CounterLogic dataset uses various formal propositional logic inference schemas to generate reasoning examples, as detailed in Table2.D Task-Specific Reflection ApproachesOur metacognitive intervention is implemented with task-specific adaptations to ensure appropriate reflection across different reasoning formats.Table 3 details how we adapted the reflective prompting strategy for each task type.Table2: Formal propositional logic inference schemas used in the CounterLogic dataset.Each row presents a canonical logical inference rule and its structure in propositional form.Believability is achieved when both the premises and the conclusion are true independently.Invalid datapoints are created by replacing conclusion statements (e.g., p, q, r) with unrelated ones (e.g., p ′ , q ′ , r ′ ) making the logical rule invalid.Name Propositional Logic FormE ModelsIn our study, we evaluated 11 state-of-the-art large language models from various organizations, spanning different architectures, parameter scales, and training paradigms.Below we provide details about each model, including their version, size, and key characteristics:E.1 Model AccessAll models were accessed through the OpenRouter API to ensure consistent evaluation conditions.This approach allowed us to standardize the inference parameters across different model providers, including temperature settings (0), top-p (0.95), and maximum token length (4096 tokens).E.2 Model Selection CriteriaWe selected these models based on the following criteria:1. State-of-the-art performance: All selected models represent the cutting edge of LLM development at the time of our study.Architectural diversity:We included models with different architectural designs to examine whether the observed patterns generalize across various model architectures.The 3.1 series is an upgrade to the Llama-3 models, enhanced instruction-following and reasoning capabilities.We include both larger (70B) and smaller (8B) parameter variants to examine scaling effects.E.3.4 Alibaba ModelsQwen-2.5-72B and Qwen-2.5-7Bare Alibaba's latest generation language models, known for their strong performance across various benchmarks, particularly in multilingual reasoning tasks.E.3.5 DeepSeek ModelsDeepSeek-V3 is a 671B parameter model developed by DeepSeek AI, designed specifically for dialogue applications with strong reasoning capabilities.DeepSeek-R1-Distill-Llama is reasoning model that is finetuned version of DeepSeek-R1(671b) using the outputs of Llama-3.3-70b-instructmodel.E.4 Model Inference ParametersFor all evaluations, we used consistent inference parameters across models.We use openRouter for all the non OpenAI models and OpenAI API for the 3 openAI models.All the models that support system prompts have standard prompt asking for instruction following.E.5 Cost of the EvaluationsOverall 83.2$ were spend on openRouter for all non-OpenAI models across all tasks.About 2000$ of OpenAI credits were used for running the evaluation, most of which was used by the o1-preview model.F Full ResultsOur detailed results for the Figure3can be found in the Table-5G Ablation StudiesG.1 Comparison of Prompting StrategiesFigure5presents an ablation study evaluating model performance under three prompting strategies: zero-shot, few-shot, and chain-of-thought (CoT) across three belief consistency conditions; consistent, violate, and random gibberish.Here random gibberish datapoints are obtained by replacing entities in consistent and violate scenarios with random strings (such as 'cat' with 'nsjf').Consistent and Violate datapoints are from the Syllogistic Dataset(Bertolazzi et al., 2024).Across all models, We see that Consistent datapoints perform better than gibberish datapoints which perform better than violate datapoints, highlighting the reliance of model on its internal knowledge and inability to reason purely based on logical rules.CoT prompting consistently improves accuracy without altering the general trend observed in belief sensitivity.Interestingly, few-shot prompting  does not universally help: OpenAI models (e.g., gpt-4o-mini) actually show degraded performance in few-shot settings across all belief types, suggesting potential sensitivity to in-context demonstrations or prompt formatting.In contrast, most other models maintain or slightly improve their performance under few-shotG.2 Perturbing with Model-Generated EvidenceTo evaluate whether language models reason purely based on logical structure or are influenced by surface-level content, we perform an experiment involving model-generated evidence.Specifically, we prompt the models to generate evidences for a given conclusions and premises: one that supports the conclusion and one that neagtes it.The model is given complete freedom in how it constructs this evidence, encouraging creativity and variability in content.This is adapted from(Xie et al., 2024b)We then construct a separate task: given a premise, above generated evidence, and a conclusion, we ask whether the conclusion follows from the premise purely logically.The correct answer is determined solely based on the logical relation between the premise and conclusion, independent of the evidence.However, our results reveal a clear pattern: models show an increase in accuracy for logically valid datapoints when the supporting evidence aligns with the conclusion, and a drop in accuracy when the evidence contradicts it.This behavior suggests that models are not performing strict logical reasoning, but are instead heavily influenced by the factuality of premises and conclusions, even when it is explicitly stated to be potentially fabricated.This indicates a reliance on heuristic signals rather than formal logical inference.
A systematic analysis of large language models as soft reasoners: The case of syllogistic inferences. Leonardo Bertolazzi, Albert Gatt, Raffaella Bernardi, arXiv:2406.113412024Preprint</p>
<p>Krishna Vamshi, Sreeram Bonagiri, Priyanshul Vennam, Ponnurangam Govil, Manas Kumaraguru, Gaur, arXiv:2402.13709Sage: Evaluating moral consistency in large language models. 2024arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, 202033</p>
<p>Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge. Jiangjie Chen, Wei Shi, Ziquan Fu, Sijie Cheng, Lei Li, Yanghua Xiao, 10.18653/v1/2023.acl-long.550Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023a1</p>
<p>Yuefei Chen, K Vivek, Jing Singh, Ruixiang Ma, Tang, arXiv:2502.11008Counterbench: A benchmark for counterfactual reasoning in large language models. 2025arXiv preprint</p>
<p>Disco: Distilling counterfactuals with large language models. Zeming Chen, Qiyue Gao, Antoine Bosselut, Ashish Sabharwal, Kyle Richardson, arXiv:2212.105342023bPreprint</p>
<p>Transformers as soft reasoners over language. Elizabeth Clark, Oyvind Tafjord, Kyle Richardson, Ashish Sabharwal, Hannaneh Hajishirzi, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL). the 58th Annual Meeting of the Association for Computational Linguistics (ACL)2020</p>
<p>Language models show human-like content effects on reasoning tasks. Ishita Dasgupta, Andrew K Lampinen, C Y Stephanie, Hannah R Chan, Antonia Sheahan, Dharshan Creswell, James L Kumaran, Felix Mcclelland, Hill, 10.48550/arXiv.2207.07051ArXiv:2207.070512024arXiv preprint</p>
<p>Conditionals and inferential connections: Toward a new semantics. Igor Douven, Shira Elqayam, Henrik Singmann, 10.1016/j.cognition.2018.05.005Cognition. 1782018</p>
<p>A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models. M H Tiwalayo Eisape, Ishita Tessler, Fei Dasgupta, Sjoerd Sha, Tal Van Steenkiste, Linzen, 10.48550/arXiv.2311.00445ArXiv:2311.004452024arXiv preprint</p>
<p>Reasoning effort and problem complexity: A scaling analysis in large language models. Benjamin Estermann, Luca A Lanzendörfer, Roger Wattenhofer, arXiv:2503.151132025arXiv preprint</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Wenfei Zhou, James Coady, David Peng, Yujie Qiao, Luke Benson, Lucy Sun, Alex Wardle-Solano, Hannah Szabo, Ekaterina Zubova, Matthew Burtell, Jonathan Fan, Yixin Liu, Brian Wong, arXiv:2209.00840Malcolm Sailor, and 16 others. 2024. Folio: Natural language reasoning with first-order logic. Preprint</p>
<p>Tug-of-war between knowledge: Exploring and resolving knowledge conflicts in retrieval-augmented language models. Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Xiaojian Jiang, Jiexin Xu, Li Qiuxia, Jun Zhao, Proceedings of LREC-COLING. LREC-COLING2024</p>
<p>Learning the difference that makes a difference with counterfactually-augmented data. Divyansh Kaushik, Eduard Hovy, Zachary C , Proceedings of the International Conference on Learning Representations. the International Conference on Learning Representations2020Lipton</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, arXiv:2205.119162022arXiv preprint</p>
<p>The case for motivated reasoning. Ziva Kunda, 10.1037/0033-2909.108.3.480Psychological Bulletin. 10831990</p>
<p>Language models, like humans, show content effects on reasoning tasks. Ishita Andrew K Lampinen, Dasgupta, C Y Stephanie, Chan, Antonia Hannah R Sheahan, Dharshan Creswell, James L Kumaran, Felix Mcclelland, Hill, 10.1093/pnasnexus/pgae233PNAS Nexus. 37e2332024</p>
<p>Using counterfactual tasks to evaluate the generality of analogical reasoning in large language models. Martha Lewis, Melanie Mitchell, arXiv:2402.089552024arXiv preprint</p>
<p>Haowei Lin, Xiangyu Wang, Ruilin Yan, Baizhou Huang, Haotian Ye, Jianhua Zhu, Zihao Wang, James Zou, Jianzhu Ma, Yitao Liang, Generative reasoning with large language models. 2025</p>
<p>Yantao Liu, Zijun Yao, Xin Lv, Yuchen Fan, Shulin Cao, Jifan Yu, Lei Hou, Juanzi Li, 10.48550/arXiv.2404.03577ArXiv:2404.03577Untangle the KNOT: Interweaving Conflicting Knowledge and Reasoning Skills in Large Language Models. 2024arXiv preprint</p>
<p>Entity-based knowledge conflicts in question answering. Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris Dubois, Sameer Singh, 10.18653/v1/2021.emnlp-main.565Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2021</p>
<ol>
<li>(ir)rationality and cognitive biases in large language models. Olivia Macmillan, - Scott, Mirco Musolesi, 10.1098/rsos.240255Royal Society Open Science. 113240255</li>
</ol>
<p>The beliefbias effect in the production and evaluation of logical conclusions. Henry Markovits, Guilaine Nantel, 10.3758/BF03199552Memory &amp; Cognition. 1711989</p>
<p>Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. R Thomas Mccoy, Ellie Pavlick, Tal Linzen, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, Italy2019</p>
<p>Disentqa: Disentangled question answering. Ella Neeman, Roee Aharoni, Or Honovich, Leshem Choshen, Idan Szpektor, Omri Abend, 10.18653/v1/2023.acl-long.559Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Logicbench: Towards systematic evaluation of logical reasoning ability of large language models. Mihir Parmar, Nisarg Patel, Neeraj Varshney, Mutsumi Nakamura, Man Luo, Santosh Mashetty, Arindam Mitra, Chitta Baral, 2024</p>
<p>Multi-logieval: Towards evaluating multi-step logical reasoning ability of large language models. Nisarg Patel, Mohith Kulkarni, Mihir Parmar, Aashna Budhiraja, Mutsumi Nakamura, Neeraj Varshney, Chitta Baral, Proceedings of EMNLP. EMNLP2024</p>
<p>The Book of Why: The New Science of Cause and Effect. Judea Pearl, Dana Mackenzie, 2018Basic Books</p>
<p>Language Models as Knowledge Bases?. Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, 10.18653/v1/D19-1250Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>How Much Knowledge Can You Pack Into the Parameters of a Language Model?. Adam Roberts, Colin Raffel, Noam Shazeer, 10.48550/arXiv.2002.08910ArXiv:2002.089102020arXiv preprint</p>
<p>Confirmation bias is adaptive when coupled with efficient metacognition. Michael Rollwage, Stephen M Fleming, 10.1098/rstb.2020.0131Philosophical Transactions of the Royal Society B: Biological Sciences. 376202001312021. 1822</p>
<p>It's not just size that matters: Small language models are also fewshot learners. Timo Schick, Hinrich Schütze, 10.18653/v1/2021.naacl-main.185Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Nataniel Shinn, Shinn Yao, Kaixuan Zhao, Dian Yu, Eric Zhao, Dan Zhao, Dragomir Radev, arXiv:2305.106012023Preprint</p>
<p>Zhaochen Su, Jun Zhang, Xiaoye Qu, Tong Zhu, Yanshu Li, Jiashuo Sun, Juntao Li, Min Zhang, Yu Cheng, Conflictbank: A benchmark for evaluating the influence of knowledge conflicts in llm. 2024</p>
<p>Leapof-thought: Teaching pretrained models to systematically reason over implicit knowledge. Alon Talmor, Sewon Min, Kelcey Zhang, Matt Gardner, Hannaneh Hajishirzi, Yejin Choi, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>Compartmentalization: A window on the defensive self 1. Jenna Thomas, Christopher Ditzfeld, 10.1111/spc3.12061Social and Personality Psychology Compass. 102013and Carolin Showers</p>
<p>Right for right reasons: Large language models for verifiable commonsense knowledge graph question answering. Armin Toroghi, Willis Guo, Scott Sanner, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024</p>
<p>Wrong for the right reasons: Diagnosing misconceptions in nli benchmarks. Paul Trichelair, Ellie Pavlick, Tal Linzen, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL). the 61st Annual Meeting of the Association for Computational Linguistics (ACL)2023</p>
<p>Fluency and belief bias in deductive reasoning: new indices for old effects. Dries Trippas, Simon Handley, Michael Verde, 10.3389/fpsyg.2014.00631Frontiers in Psychology. 56312014</p>
<p>General purpose verification for chain of thought prompting. Robert Vacareanu, Miguel Ballesteros, arXiv:2405.002042024arXiv preprint</p>
<p>Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru Tang, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao, Wenyang Gao, Xuming Hu, Zehan Qi, Yidong Wang, Linyi Yang, Jindong Wang, Xing Xie, Zheng Zhang, Yue Zhang, arXiv.orgSurvey on factuality in large language models: Knowledge, retrieval and domainspecificity. 2023</p>
<p>Resolving Knowledge Conflicts in Large Language Models. Yike Wang, Shangbin Feng, Heng Wang, Weijia Shi, Vidhisha Balachandran, Tianxing He, Yulia Tsvetkov, 10.48550/arXiv.2310.00935ArXiv:2310.009352024arXiv preprint</p>
<p>Metacognitive prompting improves understanding in large language models. Yuqing Wang, Yun Zhao, arXiv:2308.053422024Preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.119032023Preprint</p>
<p>Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks. Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, Yoon Kim, 10.48550/arXiv.2307.02477ArXiv:2307.024772024arXiv preprint</p>
<p>Chulin Xie, Yangsibo Huang, Chiyuan Zhang, Da Yu, Xinyun Chen, Bill Yuchen Lin, Bo Li, Badih Ghazi, Ravi Kumar, arXiv:2410.23123memorization of large language models in logical reasoning. 2024aarXiv preprint</p>
<p>Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts. Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, Yu Su, 10.48550/arXiv.2305.13300ArXiv:2305.133002024barXiv preprint</p>
<p>Faithful logical reasoning via symbolic chain-of-thought. Jundong Xu, Hao Fei, Liangming Pan, Qian Liu, M Lee, W Hsu, Annual Meeting of the Association for Computational Linguistics. 2024a</p>
<p>Knowledge Conflicts for LLMs: A Survey. Rongwu Xu, Zehan Qi, Zhijiang Guo, Cunxiang Wang, Hongru Wang, Yue Zhang, Wei Xu, 10.18653/v1/2024.emnlp-main.486Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024b</p>
<p>Yujia Zhang, Wei Wang, Xiaojie Liu, Yiming Chen, Zhenyu Li, arXiv:2502.09101Bridging the gap between llms and human intentions. 2024arXiv preprint</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, International Conference on Learning Representations. Quoc Le, Ed Chi, 2023a</p>
<p>Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence Affect Language Models. Kaitlyn Zhou, Dan Jurafsky, Tatsunori Hashimoto, 10.48550/arXiv.2302.13439ArXiv:2302.134392023barXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1200 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1200</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1200</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-26.html">extraction-schema-26</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <p><strong>Paper ID:</strong> paper-204743857</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1910.07631v1.pdf" target="_blank">Machine learning and big scientific data</a></p>
                <p><strong>Paper Abstract:</strong> This paper reviews some of the challenges posed by the huge growth of experimental data generated by the new generation of large-scale experiments at UK national facilities at the Rutherford Appleton Laboratory (RAL) site at Harwell near Oxford. Such ‘Big Scientific Data’ comes from the Diamond Light Source and Electron Microscopy Facilities, the ISIS Neutron and Muon Facility and the UK's Central Laser Facility. Increasingly, scientists are now required to use advanced machine learning and other AI technologies both to automate parts of the data pipeline and to help find new scientific discoveries in the analysis of their data. For commercially important applications, such as object recognition, natural language processing and automatic translation, deep learning has made dramatic breakthroughs. Google's DeepMind has now used the deep learning technology to develop their AlphaFold tool to make predictions for protein folding. Remarkably, it has been able to achieve some spectacular results for this specific scientific problem. Can deep learning be similarly transformative for other scientific problems? After a brief review of some initial applications of machine learning at the RAL, we focus on challenges and opportunities for AI in advancing materials science. Finally, we discuss the importance of developing some realistic machine learning benchmarks using Big Scientific Data coming from several different scientific domains. We conclude with some initial examples of our ‘scientific machine learning’ benchmark suite and of the research challenges these benchmarks will enable. This article is part of a discussion meeting issue ‘Numerical algorithms for high-performance computational science’.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1200.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1200.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaFold (DeepMind)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep-learning system developed by DeepMind that predicts three-dimensional protein structures from sequence data using co-evolutionary information and distance/angle predictions; achieved top performance in the CASP competition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>De novo structure prediction with deeplearning based scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Deep neural network system combining co-evolutionary analysis of genomic sequence data with learned models that predict inter-residue distances and angles; used downstream with Rosetta for final model building. Architecture details (in this paper) emphasize deep learning on large sequence databases and distance/angle prediction strategies rather than a single disclosed network blueprint.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Structural biology / Protein folding</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>Produced high-accuracy protein structure predictions in the CASP competition that were, on average, better than those of 97 other competitors and produced 'spectacular' results for many target proteins.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td>transformative</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>The paper describes AlphaFold's outcomes as 'remarkably good' and 'spectacular' and frames AlphaFold as evidence that deep learning can have a transformative impact on a specific scientific problem (protein folding). The authors use AlphaFold's CASP performance and qualitative language to motivate the term 'transformative' and to pose whether similar impact can extend to other domains.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Benchmarking in the CASP competition (Critical Assessment of protein Structure Prediction) comparing predicted structures to experimental target structures and scoring performance relative to ~97 other competitors; qualitative assessment by domain experts (e.g., David Baker) and comparison of fold-level accuracy and model refinement quality.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>CASP blind-evaluation framework (comparison to withheld experimental structures); community comparison to competing methods; domain-expert critique (David Baker) and note of post-hoc use of Rosetta for final model building (combination with physics-based refinement).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Novelty is assessed by the system's superior performance in CASP relative to other methods and by the methodological shift to training deep models on very large sequence/co-evolution datasets to predict distances/angles rather than only energy-based or purely physics-driven models.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>CASP ranking (better on average than 97 competitors); described as producing 'spectacular results' (qualitative). No numerical global accuracy metric reported in this paper, but performance is characterized via CASP competition placement and community reaction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>The paper reports AlphaFold was 'better on average than the other 97 competitors' in CASP; David Baker's comments compare AlphaFold's fold-level predictions to those produced by traditional methods (including Rosetta) and highlight strengths and limitations relative to expert, physics-based approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not expressed as a simple single success rate in the paper; success is indicated qualitatively by top CASP performance (better on average than 97 competitors) and by specific examples of high-quality predictions; limitations noted for proteins lacking homologous sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Limitations include weaker performance where few homologous sequences exist and where physical-chemistry refinement is required (AlphaFold relied on Rosetta for final models in some cases); broader challenges include uncertainty quantification, transparency, and robustness for deep learning in science.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning and big scientific data', 'publication_date_yy_mm': '2020-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1200.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1200.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Magnon NN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Convolutional neural network for predicting magnetic coupling constants (J, J') from inelastic neutron scattering images</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A CNN trained on simulated spin-wave spectra to predict magnetic coupling constants (nearest-neighbour J and next-nearest-neighbour J') and then applied to real experimental neutron scattering data to infer physical Hamiltonian parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Magnon CNN (Rb2MnF4 coupling predictor)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A supervised CNN composed of four convolutional layers followed by a dense regression output (two linear nodes for J and J'), using ReLU activations and max-pooling; trained on ~29,957 simulated spin-wave images (SpinW) and validated on held-out simulated data and measured experimental spectra.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Condensed matter physics / Magnetic materials (inelastic neutron scattering)</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>The network inferred the magnetic coupling constants J and J' from an experimental spectrum of Rb2MnF4, predicting J=0.6763 meV and J'=0.0104 meV, in excellent agreement with prior experimental literature values, including detecting a subtle next-nearest-neighbour coupling term.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td>incremental</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>Authors frame the network as augmenting existing expertise and assisting difficult analysis where prior knowledge exists, improving efficiency and enabling parameter extraction from spectra rather than replacing fundamental theory—this positions the contribution as an incremental augmentation of established experimental analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Trained/validation split on simulated dataset (27,000 for training, 2,957 for validation); evaluation via mean absolute error (MAE) metrics on held-out simulated data; final assessment via application to experimental spectrum and comparison to known literature values.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Validation on withheld simulated images (reporting MAE values) and external validation by applying the trained model to real experimental data from ISIS (MARI instrument) and comparing predicted J and J' to previously measured experimental values from the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Novelty arises from using a CNN trained on a large set of simulated spectra to extract Hamiltonian parameters (including subtle J'), demonstrating that ML can learn features that correspond to physical model parameters and can generalize from simulation to real experimental data.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Reported mean average error on withheld data: ±0.0055 meV for J and ±0.0036 meV for J'; experimental prediction values J=0.6763 meV and J'=0.0104 meV compared to literature ranges (e.g., J≈0.648–0.673 meV and J'≈0.006–0.012 meV).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>Direct comparison of predicted coupling constants to previously reported experimental measurements shows excellent agreement; authors emphasize that the NN picked up subtle features (J') consistent with human-derived experimental results.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Quantified by MAE on held-out simulated data (±0.0055 meV for J, ±0.0036 meV for J') and by successful prediction on experimental spectrum agreeing with literature; no separate percent-success rate reported.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Dependence on a representative simulated training set chosen using prior knowledge (limits generality); requirement of informed selection of parameter ranges; potential domain gap between simulation and experimental data; limited discussion of uncertainty quantification for the NN predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning and big scientific data', 'publication_date_yy_mm': '2020-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1200.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1200.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DMS NN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Convolutional neural network for azimuthal angle prediction from diffuse multiple scattering (DMS) patterns</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A CNN trained on >250,000 simulated DMS scattering patterns to predict the sample azimuthal angle, removing a time-consuming expert-driven bottleneck in DMS experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DMS CNN (azimuthal angle predictor)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A CNN with two convolutional layers (32 and 64 3x3 filters with max-pooling and dropout), followed by dense layers and a single linear regression output node for the azimuthal angle; trained on a large simulated dataset (75% train/25% validation).</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Crystallography / Synchrotron scattering (materials science)</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>Automatically predicts the azimuthal angle of a DMS sample from observed scattering images, achieving an accuracy of prediction within 6.5 degrees and delivering results far faster than exhaustive expert-driven image comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td>incremental</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>The method addresses a bottleneck (angle determination) by automating a routine but time-consuming experimental analysis step; authors present it as an efficiency and automation improvement rather than a paradigm shift.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Training/validation on a synthetic dataset (>250,000 simulated patterns) with monitoring of mean absolute error during training; performance visualized via training/validation curves (Figure 6) and quantified by final prediction error statistic.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Validation on withheld simulated validation set (25% of dataset); qualitative statement that the trained NN provides answers 'in a fraction of the time' needed for exhaustive comparison; no external experimental numeric cross-validation reported in the text for angle MAE beyond the quoted 6.5°.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Novel in applying CNNs to automate a DMS-specific, expert-intensive parameter extraction using a large simulated training corpus; novelty is primarily methodological (automating a bottleneck) rather than discovery of new physics.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Prediction accuracy reported as within 6.5 degrees; speedup described qualitatively as providing an answer in a fraction of the time required by exhaustive comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>The paper contrasts the NN's speed and automated prediction with the traditional expert-driven, time-consuming approach, but does not present a numerical head-to-head human vs NN comparison beyond timing and the angle precision statement.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Quantified as prediction within 6.5° of true azimuthal angle on validation data; no percentage success rate provided.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Requires a large simulated training set representative of experimental conditions; potential domain shift between simulation and real detector artefacts; uncertainty quantification for network outputs not addressed in detail.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning and big scientific data', 'publication_date_yy_mm': '2020-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1200.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1200.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Topaz</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Topaz (positive-unlabeled CNN particle picker)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A CNN-based particle picking program for cryo-electron micrographs that uses a positive-vs-unlabeled training scheme and a small set of annotated particles to pick many more particles automatically, improving reconstruction resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Positive-unlabeled convolutional neural networks for particle picking in cryo-electron micrographs.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Topaz</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A positive-unlabeled convolutional neural network that learns to detect particles in cryoEM micrographs from a small positively labeled set and the remainder treated as unlabeled, enabling large-scale automated particle picking.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Structural biology / Electron cryo-microscopy (cryoEM)</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>Automatically identified 1.72× more particles than the published manual picks for a ribosome dataset, which resulted in the highest-resolution 3D reconstruction achieved for that dataset to date, effectively improving structural determination.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td>incremental</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>The improvement is characterized as enhancing the existing single-particle reconstruction workflow (more particles → higher resolution) and is presented as a concrete performance improvement on a dataset rather than a fundamental paradigm shift.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Comparison of number of particles picked (Topaz vs published manual picks) and resulting reconstruction resolution (empirical improvement to highest resolution achieved for that dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Validation by comparing downstream reconstruction quality (resolution) achieved using Topaz-picked particles versus published picks; implicit benchmarking against human-curated picks.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Novelty in using positive-unlabeled learning and CNNs to reduce annotation burden and increase particle yields, enabling higher-resolution reconstructions from existing data.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Numeric improvement: 1.72× more particle picks for a ribosome dataset; resulted in the highest resolution structure for that dataset to date (no absolute resolution number reported in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>Direct comparison to published (human) picks: Topaz found 1.72× more particles and produced a superior reconstruction for that dataset; paper emphasizes reduced human validation time and higher particle yields.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Implicitly high for the reported dataset (1.72× more picks and improved resolution), but no generalized success rate across datasets is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Low contrast and low-dose imaging make particle identification challenging; potential for false positives requires human validation; method effectiveness depends on representative positive examples for training.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning and big scientific data', 'publication_date_yy_mm': '2020-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1200.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1200.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SuRVoS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Super-Region Volume Segmentation (SuRVoS) workbench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A segmentation workbench applying 'shallow' machine learning with limited user annotation to speed up segmentation of 3D cryo-soft X-ray tomography images where large labeled datasets are unavailable.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Super-Region Volume Segmentation workbench.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SuRVoS segmentation workbench</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A semi-automated segmentation tool using shallow machine learning and human-provided annotations on subsets of images to train classifiers that accelerate segmentation of tomographic cellular ultrastructure.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Cellular imaging / Cryo-soft X-ray tomography</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>Facilitates segmentation of 3D cellular tomograms (nucleus, cytoplasm, organelles) with limited labeled data, speeding up a major bottleneck and enabling more rapid analysis of cellular ultrastructure.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td>incremental</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>Presented as a pragmatic automation of tedious segmentation tasks to increase throughput rather than producing novel biological discoveries by itself; it accelerates workflows incrementally.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Demonstrated in operational use with human-in-the-loop annotation plus classifier training; qualitative improvements in segmentation speed and throughput are reported rather than formal numeric benchmarks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Human annotation used both for training and as ground truth for evaluation; engagement of citizen scientists (Zooniverse) to provide additional annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Novelty lies in adapting shallow ML combined with small-scale human annotation and crowd-sourcing to a domain with few labeled examples, enabling practical segmentation workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Qualitative speedup of segmentation workflows; no numerical accuracy or throughput metrics provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>Paper contrasts inability to use deep learning due to few pre-labelled image sets and large cell diversity; uses SuRVoS + human annotation as an effective compromise.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not quantitatively reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Lack of large labeled datasets, high diversity of cell types, need for human annotation for training, and limited generalization across cell types are noted limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning and big scientific data', 'publication_date_yy_mm': '2020-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1200.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1200.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FLImP UNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UNET-based model for automated FLImP region selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A UNET convolutional model applied to segment micrographs to automatically detect regions with suitable fluorescent object density for FLImP single-molecule imaging, enabling automation and potential clinical translation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>FLImP UNET segmentation model</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A UNET-based deep convolutional segmentation model trained to detect micrograph regions with appropriate density and homogeneity for FLImP analysis; designed to support automatic selection of regions of interest and later multi-label classification for clinical samples.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Single-molecule imaging / Biomedical imaging (FLImP)</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>Automates the previously user-intensive step of selecting regions suitable for FLImP, enabling higher throughput and laying groundwork for clinical translation of FLImP assays for cancer diagnostics.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td>incremental</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>Described as enabling automation and scale-up of an established imaging method (FLImP) rather than a fundamental scientific breakthrough; intended to translate an existing assay to clinical use by reducing manual effort.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Demonstrated segmentation of monocell-culture micrographs (Figure 3(C)); planned extension to multi-label classification and collaborative clinical testing, but no numerical segmentation metrics reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Ongoing work with clinical collaborators to extend and validate models on diverse clinical samples; initial validation on monocell culture micrographs shown qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Novelty in applying deep segmentation (UNET) to the specific constraints of FLImP (low signal, diffraction-limited single fluorophores) to automate region selection for single-molecule imaging in fixed cells.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>No numeric metrics reported in this paper; intended impact described as increased throughput and enabling clinical translation of FLImP.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>Paper contrasts current user-intensive manual region selection with the automated UNET approach, but provides no numerical comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not quantitatively reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Challenges include diffraction-limited single fluorophores (difficult segmentation), need to generalize to clinically diverse samples, requirement for multi-label classification, and integration into clinical-grade instrumentation.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning and big scientific data', 'publication_date_yy_mm': '2020-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>De novo structure prediction with deeplearning based scoring. <em>(Rating: 2)</em></li>
                <li>Positive-unlabeled convolutional neural networks for particle picking in cryo-electron micrographs. <em>(Rating: 2)</em></li>
                <li>A mixed-scale dense convolutional neural network for image analysis. <em>(Rating: 1)</em></li>
                <li>Neural Network Approach for Characterizing Structural Transformations by X-Ray Absorption Fine Structure Spectroscopy. <em>(Rating: 1)</em></li>
                <li>Automated generation and ensemble-learned matching of X-ray absorption spectra. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1200",
    "paper_id": "paper-204743857",
    "extraction_schema_id": "extraction-schema-26",
    "extracted_data": [
        {
            "name_short": "AlphaFold",
            "name_full": "AlphaFold (DeepMind)",
            "brief_description": "A deep-learning system developed by DeepMind that predicts three-dimensional protein structures from sequence data using co-evolutionary information and distance/angle predictions; achieved top performance in the CASP competition.",
            "citation_title": "De novo structure prediction with deeplearning based scoring.",
            "mention_or_use": "mention",
            "system_name": "AlphaFold",
            "system_description": "Deep neural network system combining co-evolutionary analysis of genomic sequence data with learned models that predict inter-residue distances and angles; used downstream with Rosetta for final model building. Architecture details (in this paper) emphasize deep learning on large sequence databases and distance/angle prediction strategies rather than a single disclosed network blueprint.",
            "discovery_domain": "Structural biology / Protein folding",
            "discovery_description": "Produced high-accuracy protein structure predictions in the CASP competition that were, on average, better than those of 97 other competitors and produced 'spectacular' results for many target proteins.",
            "discovery_type": "transformative",
            "discovery_type_justification": "The paper describes AlphaFold's outcomes as 'remarkably good' and 'spectacular' and frames AlphaFold as evidence that deep learning can have a transformative impact on a specific scientific problem (protein folding). The authors use AlphaFold's CASP performance and qualitative language to motivate the term 'transformative' and to pose whether similar impact can extend to other domains.",
            "evaluation_methods": "Benchmarking in the CASP competition (Critical Assessment of protein Structure Prediction) comparing predicted structures to experimental target structures and scoring performance relative to ~97 other competitors; qualitative assessment by domain experts (e.g., David Baker) and comparison of fold-level accuracy and model refinement quality.",
            "validation_approaches": "CASP blind-evaluation framework (comparison to withheld experimental structures); community comparison to competing methods; domain-expert critique (David Baker) and note of post-hoc use of Rosetta for final model building (combination with physics-based refinement).",
            "novelty_assessment": "Novelty is assessed by the system's superior performance in CASP relative to other methods and by the methodological shift to training deep models on very large sequence/co-evolution datasets to predict distances/angles rather than only energy-based or purely physics-driven models.",
            "impact_metrics": "CASP ranking (better on average than 97 competitors); described as producing 'spectacular results' (qualitative). No numerical global accuracy metric reported in this paper, but performance is characterized via CASP competition placement and community reaction.",
            "comparison_to_human_discoveries": true,
            "comparison_details": "The paper reports AlphaFold was 'better on average than the other 97 competitors' in CASP; David Baker's comments compare AlphaFold's fold-level predictions to those produced by traditional methods (including Rosetta) and highlight strengths and limitations relative to expert, physics-based approaches.",
            "success_rate": "Not expressed as a simple single success rate in the paper; success is indicated qualitatively by top CASP performance (better on average than 97 competitors) and by specific examples of high-quality predictions; limitations noted for proteins lacking homologous sequences.",
            "challenges_limitations": "Limitations include weaker performance where few homologous sequences exist and where physical-chemistry refinement is required (AlphaFold relied on Rosetta for final models in some cases); broader challenges include uncertainty quantification, transparency, and robustness for deep learning in science.",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1200.0",
            "source_info": {
                "paper_title": "Machine learning and big scientific data",
                "publication_date_yy_mm": "2020-01"
            }
        },
        {
            "name_short": "Magnon NN",
            "name_full": "Convolutional neural network for predicting magnetic coupling constants (J, J') from inelastic neutron scattering images",
            "brief_description": "A CNN trained on simulated spin-wave spectra to predict magnetic coupling constants (nearest-neighbour J and next-nearest-neighbour J') and then applied to real experimental neutron scattering data to infer physical Hamiltonian parameters.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Magnon CNN (Rb2MnF4 coupling predictor)",
            "system_description": "A supervised CNN composed of four convolutional layers followed by a dense regression output (two linear nodes for J and J'), using ReLU activations and max-pooling; trained on ~29,957 simulated spin-wave images (SpinW) and validated on held-out simulated data and measured experimental spectra.",
            "discovery_domain": "Condensed matter physics / Magnetic materials (inelastic neutron scattering)",
            "discovery_description": "The network inferred the magnetic coupling constants J and J' from an experimental spectrum of Rb2MnF4, predicting J=0.6763 meV and J'=0.0104 meV, in excellent agreement with prior experimental literature values, including detecting a subtle next-nearest-neighbour coupling term.",
            "discovery_type": "incremental",
            "discovery_type_justification": "Authors frame the network as augmenting existing expertise and assisting difficult analysis where prior knowledge exists, improving efficiency and enabling parameter extraction from spectra rather than replacing fundamental theory—this positions the contribution as an incremental augmentation of established experimental analysis.",
            "evaluation_methods": "Trained/validation split on simulated dataset (27,000 for training, 2,957 for validation); evaluation via mean absolute error (MAE) metrics on held-out simulated data; final assessment via application to experimental spectrum and comparison to known literature values.",
            "validation_approaches": "Validation on withheld simulated images (reporting MAE values) and external validation by applying the trained model to real experimental data from ISIS (MARI instrument) and comparing predicted J and J' to previously measured experimental values from the literature.",
            "novelty_assessment": "Novelty arises from using a CNN trained on a large set of simulated spectra to extract Hamiltonian parameters (including subtle J'), demonstrating that ML can learn features that correspond to physical model parameters and can generalize from simulation to real experimental data.",
            "impact_metrics": "Reported mean average error on withheld data: ±0.0055 meV for J and ±0.0036 meV for J'; experimental prediction values J=0.6763 meV and J'=0.0104 meV compared to literature ranges (e.g., J≈0.648–0.673 meV and J'≈0.006–0.012 meV).",
            "comparison_to_human_discoveries": true,
            "comparison_details": "Direct comparison of predicted coupling constants to previously reported experimental measurements shows excellent agreement; authors emphasize that the NN picked up subtle features (J') consistent with human-derived experimental results.",
            "success_rate": "Quantified by MAE on held-out simulated data (±0.0055 meV for J, ±0.0036 meV for J') and by successful prediction on experimental spectrum agreeing with literature; no separate percent-success rate reported.",
            "challenges_limitations": "Dependence on a representative simulated training set chosen using prior knowledge (limits generality); requirement of informed selection of parameter ranges; potential domain gap between simulation and experimental data; limited discussion of uncertainty quantification for the NN predictions.",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1200.1",
            "source_info": {
                "paper_title": "Machine learning and big scientific data",
                "publication_date_yy_mm": "2020-01"
            }
        },
        {
            "name_short": "DMS NN",
            "name_full": "Convolutional neural network for azimuthal angle prediction from diffuse multiple scattering (DMS) patterns",
            "brief_description": "A CNN trained on &gt;250,000 simulated DMS scattering patterns to predict the sample azimuthal angle, removing a time-consuming expert-driven bottleneck in DMS experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "DMS CNN (azimuthal angle predictor)",
            "system_description": "A CNN with two convolutional layers (32 and 64 3x3 filters with max-pooling and dropout), followed by dense layers and a single linear regression output node for the azimuthal angle; trained on a large simulated dataset (75% train/25% validation).",
            "discovery_domain": "Crystallography / Synchrotron scattering (materials science)",
            "discovery_description": "Automatically predicts the azimuthal angle of a DMS sample from observed scattering images, achieving an accuracy of prediction within 6.5 degrees and delivering results far faster than exhaustive expert-driven image comparison.",
            "discovery_type": "incremental",
            "discovery_type_justification": "The method addresses a bottleneck (angle determination) by automating a routine but time-consuming experimental analysis step; authors present it as an efficiency and automation improvement rather than a paradigm shift.",
            "evaluation_methods": "Training/validation on a synthetic dataset (&gt;250,000 simulated patterns) with monitoring of mean absolute error during training; performance visualized via training/validation curves (Figure 6) and quantified by final prediction error statistic.",
            "validation_approaches": "Validation on withheld simulated validation set (25% of dataset); qualitative statement that the trained NN provides answers 'in a fraction of the time' needed for exhaustive comparison; no external experimental numeric cross-validation reported in the text for angle MAE beyond the quoted 6.5°.",
            "novelty_assessment": "Novel in applying CNNs to automate a DMS-specific, expert-intensive parameter extraction using a large simulated training corpus; novelty is primarily methodological (automating a bottleneck) rather than discovery of new physics.",
            "impact_metrics": "Prediction accuracy reported as within 6.5 degrees; speedup described qualitatively as providing an answer in a fraction of the time required by exhaustive comparison.",
            "comparison_to_human_discoveries": false,
            "comparison_details": "The paper contrasts the NN's speed and automated prediction with the traditional expert-driven, time-consuming approach, but does not present a numerical head-to-head human vs NN comparison beyond timing and the angle precision statement.",
            "success_rate": "Quantified as prediction within 6.5° of true azimuthal angle on validation data; no percentage success rate provided.",
            "challenges_limitations": "Requires a large simulated training set representative of experimental conditions; potential domain shift between simulation and real detector artefacts; uncertainty quantification for network outputs not addressed in detail.",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1200.2",
            "source_info": {
                "paper_title": "Machine learning and big scientific data",
                "publication_date_yy_mm": "2020-01"
            }
        },
        {
            "name_short": "Topaz",
            "name_full": "Topaz (positive-unlabeled CNN particle picker)",
            "brief_description": "A CNN-based particle picking program for cryo-electron micrographs that uses a positive-vs-unlabeled training scheme and a small set of annotated particles to pick many more particles automatically, improving reconstruction resolution.",
            "citation_title": "Positive-unlabeled convolutional neural networks for particle picking in cryo-electron micrographs.",
            "mention_or_use": "mention",
            "system_name": "Topaz",
            "system_description": "A positive-unlabeled convolutional neural network that learns to detect particles in cryoEM micrographs from a small positively labeled set and the remainder treated as unlabeled, enabling large-scale automated particle picking.",
            "discovery_domain": "Structural biology / Electron cryo-microscopy (cryoEM)",
            "discovery_description": "Automatically identified 1.72× more particles than the published manual picks for a ribosome dataset, which resulted in the highest-resolution 3D reconstruction achieved for that dataset to date, effectively improving structural determination.",
            "discovery_type": "incremental",
            "discovery_type_justification": "The improvement is characterized as enhancing the existing single-particle reconstruction workflow (more particles → higher resolution) and is presented as a concrete performance improvement on a dataset rather than a fundamental paradigm shift.",
            "evaluation_methods": "Comparison of number of particles picked (Topaz vs published manual picks) and resulting reconstruction resolution (empirical improvement to highest resolution achieved for that dataset).",
            "validation_approaches": "Validation by comparing downstream reconstruction quality (resolution) achieved using Topaz-picked particles versus published picks; implicit benchmarking against human-curated picks.",
            "novelty_assessment": "Novelty in using positive-unlabeled learning and CNNs to reduce annotation burden and increase particle yields, enabling higher-resolution reconstructions from existing data.",
            "impact_metrics": "Numeric improvement: 1.72× more particle picks for a ribosome dataset; resulted in the highest resolution structure for that dataset to date (no absolute resolution number reported in this paper).",
            "comparison_to_human_discoveries": true,
            "comparison_details": "Direct comparison to published (human) picks: Topaz found 1.72× more particles and produced a superior reconstruction for that dataset; paper emphasizes reduced human validation time and higher particle yields.",
            "success_rate": "Implicitly high for the reported dataset (1.72× more picks and improved resolution), but no generalized success rate across datasets is provided.",
            "challenges_limitations": "Low contrast and low-dose imaging make particle identification challenging; potential for false positives requires human validation; method effectiveness depends on representative positive examples for training.",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1200.3",
            "source_info": {
                "paper_title": "Machine learning and big scientific data",
                "publication_date_yy_mm": "2020-01"
            }
        },
        {
            "name_short": "SuRVoS",
            "name_full": "Super-Region Volume Segmentation (SuRVoS) workbench",
            "brief_description": "A segmentation workbench applying 'shallow' machine learning with limited user annotation to speed up segmentation of 3D cryo-soft X-ray tomography images where large labeled datasets are unavailable.",
            "citation_title": "Super-Region Volume Segmentation workbench.",
            "mention_or_use": "use",
            "system_name": "SuRVoS segmentation workbench",
            "system_description": "A semi-automated segmentation tool using shallow machine learning and human-provided annotations on subsets of images to train classifiers that accelerate segmentation of tomographic cellular ultrastructure.",
            "discovery_domain": "Cellular imaging / Cryo-soft X-ray tomography",
            "discovery_description": "Facilitates segmentation of 3D cellular tomograms (nucleus, cytoplasm, organelles) with limited labeled data, speeding up a major bottleneck and enabling more rapid analysis of cellular ultrastructure.",
            "discovery_type": "incremental",
            "discovery_type_justification": "Presented as a pragmatic automation of tedious segmentation tasks to increase throughput rather than producing novel biological discoveries by itself; it accelerates workflows incrementally.",
            "evaluation_methods": "Demonstrated in operational use with human-in-the-loop annotation plus classifier training; qualitative improvements in segmentation speed and throughput are reported rather than formal numeric benchmarks in this paper.",
            "validation_approaches": "Human annotation used both for training and as ground truth for evaluation; engagement of citizen scientists (Zooniverse) to provide additional annotations.",
            "novelty_assessment": "Novelty lies in adapting shallow ML combined with small-scale human annotation and crowd-sourcing to a domain with few labeled examples, enabling practical segmentation workflows.",
            "impact_metrics": "Qualitative speedup of segmentation workflows; no numerical accuracy or throughput metrics provided in this paper.",
            "comparison_to_human_discoveries": false,
            "comparison_details": "Paper contrasts inability to use deep learning due to few pre-labelled image sets and large cell diversity; uses SuRVoS + human annotation as an effective compromise.",
            "success_rate": "Not quantitatively reported in this paper.",
            "challenges_limitations": "Lack of large labeled datasets, high diversity of cell types, need for human annotation for training, and limited generalization across cell types are noted limitations.",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1200.4",
            "source_info": {
                "paper_title": "Machine learning and big scientific data",
                "publication_date_yy_mm": "2020-01"
            }
        },
        {
            "name_short": "FLImP UNet",
            "name_full": "UNET-based model for automated FLImP region selection",
            "brief_description": "A UNET convolutional model applied to segment micrographs to automatically detect regions with suitable fluorescent object density for FLImP single-molecule imaging, enabling automation and potential clinical translation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "FLImP UNET segmentation model",
            "system_description": "A UNET-based deep convolutional segmentation model trained to detect micrograph regions with appropriate density and homogeneity for FLImP analysis; designed to support automatic selection of regions of interest and later multi-label classification for clinical samples.",
            "discovery_domain": "Single-molecule imaging / Biomedical imaging (FLImP)",
            "discovery_description": "Automates the previously user-intensive step of selecting regions suitable for FLImP, enabling higher throughput and laying groundwork for clinical translation of FLImP assays for cancer diagnostics.",
            "discovery_type": "incremental",
            "discovery_type_justification": "Described as enabling automation and scale-up of an established imaging method (FLImP) rather than a fundamental scientific breakthrough; intended to translate an existing assay to clinical use by reducing manual effort.",
            "evaluation_methods": "Demonstrated segmentation of monocell-culture micrographs (Figure 3(C)); planned extension to multi-label classification and collaborative clinical testing, but no numerical segmentation metrics reported here.",
            "validation_approaches": "Ongoing work with clinical collaborators to extend and validate models on diverse clinical samples; initial validation on monocell culture micrographs shown qualitatively.",
            "novelty_assessment": "Novelty in applying deep segmentation (UNET) to the specific constraints of FLImP (low signal, diffraction-limited single fluorophores) to automate region selection for single-molecule imaging in fixed cells.",
            "impact_metrics": "No numeric metrics reported in this paper; intended impact described as increased throughput and enabling clinical translation of FLImP.",
            "comparison_to_human_discoveries": false,
            "comparison_details": "Paper contrasts current user-intensive manual region selection with the automated UNET approach, but provides no numerical comparison.",
            "success_rate": "Not quantitatively reported in this paper.",
            "challenges_limitations": "Challenges include diffraction-limited single fluorophores (difficult segmentation), need to generalize to clinically diverse samples, requirement for multi-label classification, and integration into clinical-grade instrumentation.",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1200.5",
            "source_info": {
                "paper_title": "Machine learning and big scientific data",
                "publication_date_yy_mm": "2020-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "De novo structure prediction with deeplearning based scoring.",
            "rating": 2,
            "sanitized_title": "de_novo_structure_prediction_with_deeplearning_based_scoring"
        },
        {
            "paper_title": "Positive-unlabeled convolutional neural networks for particle picking in cryo-electron micrographs.",
            "rating": 2,
            "sanitized_title": "positiveunlabeled_convolutional_neural_networks_for_particle_picking_in_cryoelectron_micrographs"
        },
        {
            "paper_title": "A mixed-scale dense convolutional neural network for image analysis.",
            "rating": 1,
            "sanitized_title": "a_mixedscale_dense_convolutional_neural_network_for_image_analysis"
        },
        {
            "paper_title": "Neural Network Approach for Characterizing Structural Transformations by X-Ray Absorption Fine Structure Spectroscopy.",
            "rating": 1,
            "sanitized_title": "neural_network_approach_for_characterizing_structural_transformations_by_xray_absorption_fine_structure_spectroscopy"
        },
        {
            "paper_title": "Automated generation and ensemble-learned matching of X-ray absorption spectra.",
            "rating": 1,
            "sanitized_title": "automated_generation_and_ensemblelearned_matching_of_xray_absorption_spectra"
        }
    ],
    "cost": 0.0163295,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Machine Learning and Big Scientific Data</p>
<p>Tony Hey 
Scientific Computing Department Rutherford Appleton Laboratory Science and Technology Facilities Council
OX11 0QXDidcotUK</p>
<p>Keith Butler 
Scientific Computing Department Rutherford Appleton Laboratory Science and Technology Facilities Council
OX11 0QXDidcotUK</p>
<p>Sam Jackson 
Scientific Computing Department Rutherford Appleton Laboratory Science and Technology Facilities Council
OX11 0QXDidcotUK</p>
<p>Jeyarajan Thiyagalingam 
Scientific Computing Department Rutherford Appleton Laboratory Science and Technology Facilities Council
OX11 0QXDidcotUK</p>
<p>Machine Learning and Big Scientific Data
A265FF0371E956687E33059B9045423A
This paper reviews some of the challenges posed by the huge growth of experimental data generated by the new generation of large-scale experiments at UK national facilities at the Rutherford Appleton Laboratory site at Harwell near Oxford.Such 'Big Scientific Data' comes from the Diamond Light Source and Electron Microscopy Facilities, the ISIS Neutron and Muon Facility, and the UK's Central Laser Facility.Increasingly, scientists are now needing to use advanced machine learning and other AI technologies both to automate parts of the data pipeline and also to help find new scientific discoveries in the analysis of their data.For commercially important applications, such as object recognition, natural language processing and automatic translation, deep learning has made dramatic breakthroughs.Google's DeepMind has now also used deep learning technology to develop their AlphaFold tool to make predictions for protein folding.Remarkably, they have been able to achieve some spectacular results for this specific scientific problem.Can deep learning be similarly transformative for other scientific problems?After a brief review of some initial applications of machine learning at the Rutherford Appleton Laboratory, we focus on challenges and opportunities for AI in advancing materials science.Finally, we discuss the importance of developing some realistic machine learning benchmarks using Big Scientific Data coming from a number of different scientific domains.We conclude with some initial examples of our 'SciML' benchmark suite and of the research challenges these benchmarks will enable.</p>
<p>The Deep Learning Revolution and 'AI for Science'</p>
<p>It is arguable that the Deep Learning revolution we are now witnessing dates back to the ImageNet database and the AlexNet Deep Learning network [1].ImageNet was a project that was led by Professor Fei-Fei Li from Stanford University and the database contained over 14 million high-resolution images collected from the Web.The images were labeled by crowd-sourcing human labelers recruited using Amazon's Mechanical Turk.Starting in 2010, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge [2] was held using the database.The competition used a subset of the ImageNet collection with roughly 1000 images in each of 1000 categories.In all, there were roughly 1.2 million training images, 50,000 validation images, and 150,000 testing images.The intent was to provide the computer science community with a focus for evaluating the effectiveness and progress of computer vision systems.A landmark breakthrough in image classification was made in 2012 by Geoffrey Hinton and two of his PhD students, Alex Krizhevsky and Ilya Sutskever.AlexNet, as their neural network implementation became to be called, used a so-called 'Deep Neural Network' consisting of five convolutional layers and three fully connected layers and was implemented using two GPUs.</p>
<p>Their paper won the 2012 ImageNet competition and reduced the error rate by an astonishing 10.8% compared to the previous winner [3].The 2015 competition was won by a team from Microsoft Research using a very deep neural network of over 100 layers and achieved an error rate for object recognition comparable to human error rates [4].In the words of Geoffrey Hinton, 'Deep Learning is an algorithm which has no theoretical limitations on what it can learn; the more data you give and the more computational time you provide the better it is' [5].</p>
<p>Can such AI algorithms benefit scientific research?Google's DeepMind subsidiary in the UK has brought together physicists, machine learning experts and structural biologists to create a system called 'AlphaFold' [6].The DeepMind team entered the biennial competition organized by CASP (Critical Assessment of protein Structure Prediction) that assesses the state of the art in threedimensional protein structure modeling [7].David Baker, a CASP organizer and developer of the Rosetta protein folding program at the University of Washington in Seattle [8], commented that 'DeepMind's scientists built on two algorithm strategies pioneered by others.First, by comparing vast troves of genomic data on other proteins, AlphaFold was able to better decipher which pairs of amino acids were most likely to wind up close to one another in folded proteins.Second, related comparisons also helped them gauge the most probable distance between neighboring pairs of amino acids and the angles at which they bound to their neighbors.Both approaches do better with the more data they evaluate, which makes them more apt to benefit from machine learning computer algorithms, such as AlphaFold, that solve problems by crunching large data sets' [9].The predictions of the AlphaFold system were remarkably good and better on average than the other 97 competitors.However, there is still hope for scientists.After the competition David Baker remarked that 'Deep Mind made much better fold level predictions than everybody, including us, using DL on co-evolution data.For problems where there are not many homologous sequences, and for protein structure refinement, I would expect their approach to work less well, as it doesn't have any physical chemistry (they used Rosetta to build their final models from predicted distances)' [10].</p>
<p>In this paper, we make some initial explorations into the application of such Deep Learning approaches applied to scientific data.The Rutherford Appleton Laboratory (RAL), at Harwell near Oxford, hosts several large-scale experimental facilities that now generate large volumes of increasingly complex scientific data.These are the Diamond Synchrotron Light Source and Electron Beam Facility, the ISIS Neutron and Muon Facility and the UK's Central Laser Facility.In addition, the Scientific Computing Department at the Laboratory hosts the UK's Tier 1 Centre for particle physics data from the Large Hadron Collider at CERN and the Natural Environmental Research Council's JASMIN 'Super Data Cluster' that supports their Centre for Environmental Data Analysis.The Scientific Machine Learning (SciML) group at the Lab is now partnering with the Alan Turing Institute, the UK's national institute for data science and artificial intelligence, in their new 'AI for Science' research theme.The SciML group will also be providing the 'PEARL' GPU computing service to Turing researchers on two NVIDIA DGX-2 GPU systems.</p>
<p>After outlining three example applications of machine learning applied to data generated by the RAL Facilities, we discuss the challenges in combining experimental and computational simulation data for progress in materials science.The article concludes with a discussion of progress towards the creation of a scientific machine learning benchmark suite.</p>
<p>Scientific Machine Learning at the Rutherford Appleton Laboratory: Three Examples</p>
<p>Introduction</p>
<p>Machine learning has the potential to be applied for the enhanced operation and functioning of large-scale big science projects.Our work in this area builds on notable successes from the application of machine learning to analyse and interpret data at national facilities, particularly in the USA.At Argonne National Laboratory machine learning is being used to complement reverse Monte Carlo structure determination from scattering experiments, by applying reinforcement learning [11].Researchers are X-ray tomography are also applying machine learning to assist with experiment orientation and facilitating better signal to noise ratios in low-dose experiments [12,13] and also using machine learning approaches to correlate diffraction and microscopy techniques to allow for advanced characterisation of phenomena such as lattice vibrations [14].</p>
<p>The Advanced Light Source at Berkeley is using machine learning to automate the collection and analysis of data from micro tomography experiments [15] and is also working with Argonne and the Materials Virtual Laboratory to automate the collection and curation of X-ray spectroscopic data [16].At Oak Ridge National Laboratory machine learning is being applied to the analysis of electron microscopy data for following materials dynamics, such as perovskite octahedral tilting</p>
<p>[17] and silicon migration in graphene [18], while researchers at the lab's spallation neutron source have used autoencoders to build physical models from inelastic neutron scattering experiments on a spin-ice material [19].Example also exist demonstrating how machine learning can be used for the enhanced operation of large facilities, recently a notable effort showed deep learning with multi-modal data could be used to predict plasma instabilities in large-scale fusion reactors [20].</p>
<p>Diamond Light Source and Cryo-Soft X-ray Tomography (CryoSXT)</p>
<p>The first example is from the Diamond Light Source, an experiment on tomographic biological imaging.Cryo-SXT is a 3D imaging method for the visualisation of cellular ultrastructure and specifically addresses the need for detailed, 3D information on cellular features in thick specimens, such as whole cells, with little or no chemical or mechanical modification [21].A major bottleneck in the analysis of the 3-D images created by such tomograms is in the segmentation of the images to distinguish between the cell nucleus, cytoplasm and the individual organelles.</p>
<p>Because there are few pre-labelled image sets, and the diversity of different cells is very large, it is not possible to straightforwardly use Deep Learning techniques.Instead, Michele Darrow and Mark Basham and their team have used 'shallow' machine learning techniques with some user annotation of a subset of images to speed up the segmentation process.These techniques have been incorporated into their SuRVoS segmentation workbench [22].Figure 1 gives an indication of this process.The team have also enlisted the help of citizen scientists using the Zooinverse platform for their 'Science Scribbler' project [23].showing how user annotation of a few images can be used to train a machine learning classifier to distinguish between the cell nucleus and cytoplasm.(Thanks to Mark Basham, DLS.)</p>
<p>Electron Cryo-Microscopy</p>
<p>Thanks to improvements in instrumentation and software, the use of electron cryo-microscopy (cryoEM) in molecular and cellular biology has increased dramatically in recent years.In the technique of single particle reconstruction, micrographs taken from flash-frozen samples of purified macromolecular complexes allow the reconstruction of high-resolution 3D molecular structures from multiple 2D views [24].Figure 2   Investment in infrastructure, such as the eBIC facility on the Harwell campus [26] and CCP-EM [27], is supporting a rapid expansion in the use of cryoEM in structural studies.Structure determination involves a sequence of steps, each of which could benefit from algorithmic and computational improvements.Given the large amount of data produced by modern microscopes and detectors, much of which is archived by the facilities themselves or by repositories such as EMPIAR [28], this is a promising area for data-driven approaches.</p>
<p>Machine learning approaches are beginning to be developed for cryoEM, in particular exploiting recent advances in image recognition.The particle picking step of single particle analysis, in which individual molecular complexes are located in micrographs, has attracted the most attention.</p>
<p>Due to the intrinsic low contrast of soft matter, and the low dose applied in order to avoid radiation damage, identification of particles is not trivial.Particles represent different views of a molecular complex, depending on their orientation in the sample, and there may be multiple molecular species, as well as contaminants and other artefacts.Furthermore, tens or hundreds of thousands need to be identified from a set of micrographs to yield a high-resolution 3D reconstruction.The promise of machine learning is to reduce the amount of human time required to validate automatically picked particles or to pick missed ones.</p>
<p>As a recent example, Topaz is one of several new programs that use CNNs to learn how to recognise particles in a micrograph [29].It uses a positive vs unlabelled classification scheme to train a model based on a small set of annotated particles.In the case of a ribosome dataset, Topaz picked 1.72x more particles than the published picks, resulting in the highest resolution structure of this dataset to date.</p>
<p>Fluorescence Localisation Imaging with Photobleaching (FLImP)</p>
<p>The OCTOPUS (Optics Clustered To OutPut Unique Solutions) imaging facility in the Central Laser Facility at RAL combines multidisciplinary expertise, techniques and infrastructure to generate and explore data for understanding biological processes from the scale of cells to single molecules [30].Automation is increasingly important to help address this challenge, both to increase throughput and exploitation of the instrumentation, and to reduce the expertise needed by users to use the facility and translate its methods outside the facility environment.A key project is a focus on automation of the Fluorescence Localisation Imaging with Photobleaching (FLImP) method.FLImP, developed in OCTOPUS, is a single molecule method which allows molecular structure determination in fixed cells at ~5nm.It has been used to measure structural fingerprints of cancer-causing proteins in cells with unprecedented detail [31].</p>
<p>A recent collaboration led by the OCTOPUS team, and involving partners from medicine, the pharmaceutical industry and a commercial instrumentation company, seeks to utilise deep learning techniques to automate FLImP to deliver a convenient, high-throughput assay for more efficient use of FLImP in the laboratory and to translate it to the clinic as a new method for precise, personalized cancer diagnosis and treatment.As with all super-resolution imaging techniques, FLImP requires images that meet a number of conditions for implementation of the technique, including: the correct density of fluorescently labelled proteins, the ability to differentiate cells from non-specific background labelling, relative background homogeneity and obtaining sufficient frames to observe the required number of photo-bleaching events and photons.At present, successful FLImP imaging is a user-intensive process, requiring operators to manually select regions of interest for image acquisition.The successful translation of FLImP technology from bench to bedside therefore requires the automation of this image segmentation task to enable the scale-up of this technology.This is a challenging problem, particularly as single fluorescently labelled proteins are diffraction limited in size and therefore difficult to individually segment from images using conventional convolutional neural networks [32].To this end, the OCTOPUS team have utilised a UNET based model capable of automatically and rapidly segmenting regions of appropriate FLImP object density from micrographs derived from monocell cultures (Figure 3 (C)).The team is currently working with clinical collaborators to extend this technique to permit multi-label classification to identify cells of interest from more diverse clinically derived samples that may include cells from a number of populations, only some of which are suitable for FLImP, and integrate these models into instrumentation suitable for clinical translation.</p>
<p>Machine Learning and Materials Science</p>
<p>Overview of Materials Science and Machine Learning</p>
<p>Machine learning has started to change the way that we do materials science; contributing to accelerated characterization, synthesis and modelling.These advances are driven by the availability of easy to use packages for building machine learning models, e.g.Scikit-Learn [33] and Keras [34], as well as a recent proliferation of publicly available datasets, resulting in a materials science "ImageNet moment" where the availability of data fuels a step-change in datadriven approaches.We will briefly survey some of the cutting-edge machine learning work in the areas of materials discovery and characterization as well as outlining some of the work of the SciML team that is using machine learning to analyse data produced at the UK's large national scientific facilities.</p>
<p>Computational materials science dates back to mid-twentieth century, an early example being the quantum chemistry exchange programme, which allowed experimental chemists to perform quantum chemical calculations with relative ease [35].At this early stage, the paradigm of computational materials science was to use computational methods to help interpret experimental results by doing a small number of expensive calculations on materials whose structure was already well known.Density functional theory (DFT) was popularized by Walter Kohn and co-workers in the 1960s; with the advent of powerful super-computers in the late twentieth century performing large numbers of DFT calculations suddenly became feasible [36,37].Structure prediction methods based on global optimization algorithms such as particle swarm optimization and genetic algorithms mean that it is now possible to predict structure and properties for new materials starting from the composition alone [38].The availability of rapid and accurate DFT calculations has also facilitated the development of large, high-quality databases of calculated materials properties, for example the Materials Project, Aflow, Open Quantum Materials Database and Nomad [39].</p>
<p>The sudden availability of these datasets is revolutionizing the way that data-driven approaches are used in materials science.Figure 4 plots the number of publications containing "machine learning materials" in them from the Web of Science.We also indicate on the figure dates that some notable databases became available, suggestive of the important role of these datasets are playing in driving the development of a new paradigm of computational materials science [40].</p>
<p>New machine learning approaches trained on computational databases are capable of making rapid and accurate predictions of materials properties by considering composition alone.The electronic band gap is a good example of a material property that is important in a range of applications from microelectronics to photovoltaics.A number of studies have reported machine learning algorithms that are capable of predicting the band gap of a material from its composition [41 -43].These kinds of algorithms can be incorporated into materials discovery workflows and have recently been applied to the prediction of new photoactive earth-abundant materials for photocatalysis [44].Generative models, using neural networks, are also now being used to postulate new molecular materials [45].For example, the long short-term memory (LSTM) neural network architecture has recently been shown to be able to predict new drug molecules using greatly reduced training data compared to other approaches [46].In the ORGAN project, a combination of generative adversarial network and reinforcement learning combine to bias molecular generation algorithms towards desired final metrics, potentially allowing the automated design of a molecule to meet a specific property [47].Interpretation of complicated experimental spectra has regularly relied on calculations for clarification, but now with databases of calculated properties available it is possible to develop machine learning algorithms to interpret spectra in an automated way, free from human bias and capable of identifying signals which are missed during manual inspection.A powerful recent example is in the field of X-ray absorption spectroscopy (XAS) where a dataset of calculated spectra was recently made available [48].Calculated spectra have been used to train neural networks, which are facilitating unprecedented analysis of materials datasets, for example, in characterising structural transformations in materials, in making on-the-fly predictions about the presence of chemical environments in a sample and in identifying sub-nanometer atomic assemblies [49 -51].Recently an ensemble learning algorithm trained on this dataset that is capable of identifying the oxidation state and coordination environment in a diverse range of chemistries has been made publicly available [52].</p>
<p>Machine Learning and Experimental Materials Data</p>
<p>The rapidly expanding capability of large-scale facilities to analyse material samples means that the demand for robust, automated, on-the-fly analysis is becoming ever more pressing.Examples such as the XAS studies described above show how a fusion of experiment, simulated data and machine learning algorithms can facilitate rapid interpretation of these rich new data sources.In the SciML team we are developing a range of machine learning algorithms for materials data analysis.</p>
<p>Inspired and challenged by the progress in machine learning at other large scale facilities outlined in the start of section 2 we have started to build a machine learning capability at the Rutherford Appleton Laboratory for analysis of materials science data collected on site.Here we present our work on diffuse multiple scattering experiments at the Diamond Light Source and on inelastic neutron scattering experiments at ISIS neutron and muon source.</p>
<p>Diffuse multiple scattering</p>
<p>Diffuse multiple scattering (DMS) is a relatively new crystallographic technique and has been made possible by the immense increase in flux of modern synchrotron sources and modern detector systems [53].DMS can be a powerful technique for allowing measurement of fine details such as lattice strain and for following structural phase transitions in materials.However, the detailed experimental setup requires expert knowledge and several time-consuming steps which limit the routine application of the technique.</p>
<p>One of the parameters that must be known for experimental analysis of DMS data is the azimuthal angle of the sample, which is not known a priori and determines the values at which reciprocal crystal lattice vectors cross the Ewald sphere, as defined in [53].We have trained a neural network consisting of convolutional and densely connected layers to predict the azimuthal angle of sample based on the observed scattering pattern.Typically, determination of the azimuthal angle is time-consuming task, requiring expert knowledge and representing a serious bottleneck for application of DMS.</p>
<p>We have built a database of 250,000+ simulated patterns, Ψ(R)T, using the DMS Python code, which are used to train the neural network [53]</p>
<p>DMS network methods:</p>
<p>The NN used for predicting the azimuthal angle of a DMS sample consists of convolutional and densely connected layers.The first convolutional layer contains 32 3x3 kernel filters, followed by a maxpooling of 2x2; the second convolutional layer contains 64 3x3 filters, followed by maxpooling of 2x2.We include a dropout rate of 0.2 between the convolutional layers to guard against over-fitting.The 2D data is then flattened and fed into a densely connected layer of 32 nodes, connected to a densely connected layer of 16 nodes.The final hidden layer is connected to a single output node with a linear activation function to allow the network to perform regression.All hidden layers are connected with rectified linear unit (ReLU) activation functions.The network is trained on 75% of the dataset and then validated on the remaining 25%.The training and validation curves are shown in Figure 6.</p>
<p>Magnon neutron scattering</p>
<p>Inelastic neutron scattering can provide detailed information about microscopic materials structure.In particular, the magnetic moment of neutrons allows one to probe the magnetic structure and ordering in a material.In this example we have investigated the use of NNs for predicting the magnetic coupling constants (J) in Rb2MnF4.Rb2MnF4 is a near-ideal 2D, spin 5/2 Heisenberg antiferromagnet and has been used extensively to test predictions for the 2D Heisenberg quantum Hamiltonian [54,55].As such, this system provides an ideal test case for exploring the ability of a NN for this task.</p>
<p>Rb2MnF4 consists of planes of MnF2 layers, with magnetic Mn arranged in a square lattice.</p>
<p>Experimentally it has been established that Rb2MnF4 has magnetic coupling between nearest neighbor Mn sites with a coupling constant variously measured as J=0.648±0.003,0.6544±0.014and 0.673±0.028meV depending on the experiment and fitting model [54,55].Careful examination of the spin wave energies along the antiferromagnetic zone boundary reveal that in addition to the nearest neighbor coupling, there is a next-nearest neighbor term in the Hamiltonian J', which has been measured to be 0.006±0.003and 0.012±0.002meV in different experiments [55,56].</p>
<p>In our study we built a training set of 29957 simulated spin wave spectra in the 2D (h, k, 0) plane from 0 ≤ h, k &lt; 1 of the of Rb2MnF4 using the SpinW code [57].This serves as our labelled training set R. We then train our NN to learn the relation between R and (J, J'); (J, J') = f(R), where the function f is the NN.After training (details below), we obtain a NN that has a mean average error of ±0.0055 meV on J and ±0.0036 meV on J', using data that was not included in the training set.</p>
<p>As a true test of the NN we provided experimental data collected on the MARI instrument at the ISIS neutron and muon source.The data was collected on a sample of Rb2MnF4 and the image of the integrated energy over the over the plane is shown in Figure 7.</p>
<p>The NN trained on simulated data predicts a value of J=0.6763 meV and J'=0.0104 meV for the experimental spectrum, in excellent agreement with previous experimental results.This demonstrates the ability of a convolutional NN to learn to predict magnetic coupling constants from simulated data, even picking up subtle, difficult to spot features, such as the value of the next-nearest-neighbour coupling constant J'.We stress here that prior knowledge was used to select a training set representative of a reasonable range of final values together with our intuition about the number of coupling constants present.This fusion of prior knowledge and NN architectures helps to greatly improve the efficiency of training and allows development of highquality models with significantly less data than would otherwise be required.We consider this an example of how NN can be used to augment existing expertise and assist in difficult analysis where some prior knowledge already exists.</p>
<p>Magnon network methods:</p>
<p>The NN used for predicting the magnetic coupling constants consist of four convolutional layers terminated by a densely connected layer.The first convolutional layer contains 32 3x3 kernel filters; the second convolutional layer contains 64 3x3 filters; the third convolutional layer contains 32 3x3 kernel filters; and the final convolutional layer contains 16 3x3 kernel filters, all convolutional layers are followed by maxpooling of 2x2.The 2D data is then flattened and fed into a densely connected layer of 2 nodes with a linear activation function to allow the network to perform regression.All hidden layers are connected with ReLU activation functions.</p>
<p>The network is trained on 27000 images of the dataset and then validated on the remaining 2957 images.The training and validation curves are shown in Figure 8.Before feeding the simulated images into the network they are converted to a 2D histogram of 128 x 128, we also apply a mask to the simulated data to cover the regions of the pattern that are not recorded due to the detector geometry -these appear as areas of purple in the image in Figure 7.</p>
<p>Figure 8 Training and validation scores for the mean absolute error for prediction of the coupling constants from an inelastic neutron scattering pattern.</p>
<p>Further work</p>
<p>In the examples given here, we have used convolutional neural nets (CNNs) to analyse spectra and patterns collected at synchrotron facilities and represented as images.Deep CNNs have revolutionised the field of image processing and recognition in many fields of business and research.As alluded to earlier, the explosion in popularity of NNs, and in particular of deep CNNs for image applications, has been driven largely by the availability of large labelled datasets for training.Deep CNNs typically rely on combinations of many types of operation and connection to achieve their most powerful results on the most difficult problems.This results in networks that not only require vast amounts of labelled data, but also have many tunable hyperparameters.This can hamper application in many materials science problems, where labelled datasets are limited.</p>
<p>Recently a new type of image recognition architecture, the mixed-scale dense MSD-NN neural network was introduced by researchers at Berkeley Laboratory [58].This architecture has several differences from traditional CNNs.The MSD-NN uses dilation filters rather than traditional convolutional kernels, this means that longer range correlations in images can be captured, depending on dilation settings (see Figure 9).In the MSD-NN all of the convolved layers are also fully connected, unlike a CNN where layers connect sequentially.This full connectivity means that the network does not have to remember information from layer to layer for the final outcome.</p>
<p>In initial work it has been shown that the MSD-NN can learn on significantly smaller datasets and with less hyper-parameter tuning than CNNs.In SciML, we are currently exploring the application of MSD-NNs for soft X-ray image segmentation and for a range of materials' science classification problems.</p>
<p>Figure 9 Top an illustration of a typical convolution filter (left) which convolves information from neighbour pixels and a dilation filter (right) which can convolve with pixels further removed.Lower a schematic representation of the fully connected mixed-dense neural network architecture.</p>
<p>Big Scientific Data and Machine Learning Benchmarks</p>
<p>Introduction</p>
<p>Benchmarking, as a means for measuring and assessing the performance of a given computer system, or as a software framework, has been a cornerstone of computing for years.Historically, these efforts began using small kernels or excerpts of loops, such as the Lawrence Livermore Loops, the Dhrystone and Whetstone benchmarks, and the LINPACK linear algebra benchmark.The key driver for these efforts was to compare runtime performance or the number of floating-point operations per second (Mflop/s) for different computer architectures.Over the years, however, the art of performance evaluation has changed to include a suite of benchmarks, such as the SPEC, ASCI, SPLASH, and NPB benchmark suites, and the measured parameters included multiple metrics, such as runtime performance and energy consumption.Although LINPACK is still a major baseline benchmark for estimating peak performance of the TOP500 supercomputing systems in the world, detailed system evaluation relies on multiple benchmark measurements covering multiple hardware platforms, often incorporating both CPUs and GPUs.More recently, new benchmark suites have been developed to cover other important aspects of computing systems such as storage and networking, with appropriate metrics.</p>
<p>Although architectures and compilers still play a critical role in the development of computing systems, the performance of machine learning systems is now becoming equally important with the rise of commercial applications of machine learning.As can be seen from our discussion above, machine learning is now also beginning to play a major role in scientific applications.Suitable scientific machine learning benchmarks are therefore now required not only to assess systems for these applications but also to assess the overall machine learning ecosystem in a scientific context.The complexity and diversity of machine learning frameworks, available hardware systems, evaluation techniques, suitable metrics for quantification, and the limited availability of appropriate scientific datasets make this a challenging endeavour.Early initiatives on this front include MLPerf [59], AI Benchmark Suites from BenchCouncil [60], CORAL-2 [61], and Deep500 [62,63].</p>
<p>• The MLPerf benchmark suite currently relies on several common commercially important machine learning-oriented tasks, such as image and object recognition, speech-to-text, sentiment analysis, translation and recommendation applications along with a set of baseline models [59].</p>
<p>However, it is very likely that the suite will incorporate scientific applications.The key metric of the MLPerf suite is speedup relative to a reference implementation.The MLPerf suite also relies on a number of large-scale datasets, covering different application cases within MLPerf.This collection of datasets is also likely to be extended to include scientific cases.</p>
<p>• The international BenchCouncil [http://www.benchcouncil.org] is organizing an AI System and Algorithm competition in 2019 [60].A number of their benchmark suites, namely, AIBench, HPC AI500, AIoT Bench, Edge AIBench, and BigDataBench, although not primarily focused on scientific applications, could be a useful complement to the SciML benchmarks proposed here.Each of these benchmark suites targets different domain of problem, such as IoTs or Edge computing devices, and includes a number of different types of benchmarks covering micro kernels, components and applications [64][65][66][67][68].</p>
<p>• The CORAL-2 suite includes a ML/DL micro-benchmark suite that captures operations that are fundamental to deep learning and machine learning [61].These include dense/sparse matrix multiplications, convolutions, recurrent-layers, and one-and two-dimensional Fast and Discrete Fourier Transform kernels (FFTs and DFTs).</p>
<p>• Finally, the Deep500 effort is predominantly focused on techniques for reliably reporting performance of deep learning applications using metrics such as scalability, throughput, communication volume and time-to-solution [62,63].This is more focused on methodology (and a corresponding framework) for quantifying and reporting deep learning performance than on any specific application.</p>
<p>One of the key motivations for this work reported in this paper is the lack of a comprehensive machine learning benchmarking initiative for scientific applications, such as particle physics, earth and environmental science, materials, space and life sciences.Such a scientific benchmark suite would facilitate better understanding of machine learning models and their suitability for different operations in a scientific context, rather than being solely oriented on performance.</p>
<p>The SciML Suite -An Overview</p>
<p>Our scientific machine learning benchmark suite, SciML, is intended to span multiple scientific domains, and cover several of the different types of problems arising in each domain.We will therefore provide a number of reasonably large and complex datasets specific to each domain together with one or more baseline models addressing particular domain-specific problems.In addition, the evaluation metrics for the SciML suite go beyond just the simple runtime performance (or speedup).Our goal is to capture the overall performance of a given scientific application by assessing both the training and inference times per sample, as well as the classification accuracy using one or more appropriate metrics.Here, we use classification accuracy, classification loss and F1 score as the relevant metrics.Classification accuracy is the ratio of correctly predicted outcomes to total number of predictions, and thus higher the accuracy, the better the model.The second metric, classification loss, measures the performance of the model by measuring how the predicted outcomes diverge from the actual ones.Finally, the F1 score is the harmonic mean of precision and recall, where the precision is the number of correct positive results divided by the number of all positive results returned by the classifier, and recall is the number of correct positive results divided by the number of all samples that should have been identified as positive.As such, the F1 score is often more useful than the raw classification accuracy when the class distributions are uneven.</p>
<p>The SciML suite will provide the specification of the task plus a reference implementation and can therefore be used to evaluate:</p>
<p>• Different hardware platforms, such as GPUs, TPUs or CPUs • Different ML-specific frameworks like TensorFlow or PyTorch</p>
<p>• Different implementation of models</p>
<p>The benchmark suite is currently in development and is intended to cover a number of different scientific domains with several problems of varying degrees of difficulty that demand different machine learning techniques.We discuss two of our prototype benchmarks in the subsections that follow.</p>
<p>Example 1: Small Angle X-Ray Scattering (SAXS)</p>
<p>The problem Small Angle X-Ray Scattering (SAXS) is one of the benchmarks within the domain of materials science and is particularly relevant to the structure of materials.SAXS helps identify how different materials are structurally organised at the particle level [69,70].Here, the term particle means the collective arrangement of several atoms [70].At this intermediate level of detail, each material can be regarded as being made up of particles of different shapes, such as spheres, rods, ellipsoids and parallelepipeds, and of different sizes, characterised by relevant parameters [71].</p>
<p>When an X-ray beam is sent through a material, particles within the target diffract the incoming X-rays, and the particle sizes, shapes and orientation with respect to the incoming beam determine the resulting diffraction pattern.The distributions of the scattered X-rays are recorded as two-dimensional images.We illustrate an example of different diffraction patterns in Figure 10.This two-dimensional diffraction pattern, at times, may contain more data than necessary.For instance, in some cases the material can be isotropic with particle arrangements symmetric in every direction, yielding diffraction patterns that are two-dimensionally symmetric.Under these circumstances, a one-dimensional profile can be obtained by integrating the two-dimensional profile over the two-dimensional domain.We show an example for a spherical particle which generates an isotropic two-dimensional profile and the matching one-dimensional profile shown in Figure 11.Fig. 11: An example of two-dimensional and one-dimensional scattering profile of a simple spherical particle.The profiles were generated by using the SASView Software [54].</p>
<p>The SAXS benchmark aims to characterise the material given its scattering profile, either one-or two-dimensional.The benchmark uses both simulated and real-world datasets as detailed below.</p>
<p>Here, we present a sub-benchmark of SAXS, namely SAXS-1D.This particular sub-benchmark focusses on the binary classification of a set of simulated one-dimensional profiles.The benchmark includes a dataset and a baseline model as discussed below.</p>
<p>SAXS Dataset</p>
<p>The SAXS-1D benchmark includes a purely simulated dataset with unit dispersity (particle sizes are not mixed).The actual datasets within the SAXS benchmark are in three categories: ideal simulated datasets; noise-added simulated datasets; and beamline datasets.The first two are generated using the relevant mathematical models [73], while the latter dataset is obtained from one of the beamlines at the Diamond Light Source.The key limitation of the last dataset is that there is no established ground truth, whereas the ground truth information is fully known for the simulated data of the other two cases.</p>
<p>The dataset for this benchmark is focused on identifying two different particle shapes: spheres and parallelepipeds.The sphere is characterised by the radius and is two-dimensionally isotropic.</p>
<p>The parallelepiped has three different shape parameters and multiple possible orientations.This sub-benchmark is a simplified case in which the orientation of the parallelepiped and two of the dimensional parameters remain unchanged so that the one-dimensional profile can clearly differentiate the parallelepiped from a sphere.</p>
<p>The simulated dataset contains 10,000 one-dimensional profiles for spheres and 10,000 onedimensional profiles for parallelepipeds.Out of these, we use 16,000 for training and 4,000 for testing with classes of the particle shapes equally distributed across the datasets.Each of these one-dimensional profiles provides the intensity (I) vs magnitude of the momentum vector (q) and has dimension of [1 x 300].</p>
<p>Baseline Model</p>
<p>Although there are several approaches for addressing this challenge, the easiest and perhaps the simplest model is a supervised learning model.Given that the underlying data is obtained through simulation, the ground truth is readily available.</p>
<p>As mentioned in the introductory section, one of the key expectations from the benchmark suite is to obtain a better understanding of different machine learning models and their suitability for different tasks.For this reason, instead of using a more flexible model like a convolutional neural network (CNN) and deep learning, for this sub-benchmark we use a simple, multi-layer neural network for the baseline version.More specifically, this baseline model consists of three densely connected layers, with the first layer capturing the input, which is an array of 300 intensity values, the middle layer with 100 neurons using ReLU as the activation function, and finally the output layer of size one with a sigmoid activation function.</p>
<p>Example 2: Sentinel Cloud Masking</p>
<p>This benchmark is intended to capture one of the challenges arising from the earth and atmospheric sciences, namely, the identification of clouds from satellite imagery.This process is often called 'cloud masking'.The masking or quantification of cloud is often an important precursor to using satellite imagery.Clouds are highly dynamic, and this influences their texture, thickness, opaqueness and transparency.The identification process can be very challenging in the presence of snow, sea ice, aerosols and sun glint.We show a cloud masking example in Figure 12.The Sentinel Cloud Masking benchmark will have several sub-benchmarks covering different image modalities, datasets and challenges.Here, we describe the sub-benchmark Sentinel-SLSTR.</p>
<p>Sentinel-3 is a constellation of two satellites carrying an array of instruments, including the Sea and Land Surface Radiometer (SLSTR) for measuring sea and land surface temperature, colour and topography to high accuracy.The Sentinel-SLSTR benchmark deals with this specific Sentinel modality.Here we describe the simplest case in which the masking is required above a part of the ocean where there is no sun glint.</p>
<p>The Sentinel-SLSTR benchmark deals with the problem of processing the SLSTR-based data.Given an M x N image, the task is to build a machine learning model for marking each pixel as either cloud or non-cloud, using one of the simplest cases.This benchmark uses the SLSTR images only for the purposes of classification.</p>
<p>Sentinel SLSTR Dataset</p>
<p>The overall Sentinel-3 benchmark relies on multiple datasets obtained from different sensors and covers multiple bands in the electromagnetic-spectrum.The Sentinel-SLSTR part of the benchmark uses a collection of 1,000 SLSTR images captured over the South Pacific Ocean region in 2018.The dataset contains significant variation in the number of cloudy pixels with near-ideal illumination of clouds.The data includes 11 channels ranging from very near infra-red, VNIR (0.55 micrometer) to thermal infra-red IR (12.0 micrometer) wavelengths and has two views (nadir and oblique).The spatial resolution is 0.5km in the VNIR and short-wave infra-red (SWIR) channels and 1km in the thermal IR channels.In all experiments the nadir view of channels S1-S9 are used as inputs.To reduce the computational demand, this particular benchmark uses sub-sampled images of 250x250 pixels for each channel.The suite specifies a random selection of 800 images for training with the remaining 200 images for validation.</p>
<p>Baseline Model</p>
<p>Unlike our SAXS-1D benchmark that uses simulated data, the key difficulty in building any supervised machine learning model for this Cloud benchmark is the lack of a reliable ground truth.Collective or crowd-sourced hand labelling of these images for ground truth is infeasible for two reasons: the time required for hand-labelling is prohibitive given the volume of images, and secondly, this is a very subjective process even with among experts.For this reason, we use Bayesian inference to generate our surrogate artificial "ground truth" [74][75][76].More specifically, for each pixel, we apply the method in reference [74] to mark each pixel as cloud or non-cloud with a corresponding confidence value.This is used as a ground truth in training our networks.</p>
<p>The baseline model we implement for masking cloud on SLSTR data is a plain, multi-layer neural network.Although CNNs or DCNNs have not been used for SLSTR or Sentinel-3 data, many authors have attempted to apply deep learning [77][78][79][80][81][82][83][84] and other complex NN models, such as LSTMs [85] and GANs [86] to cloud screening using other remote sensing instruments.In our case, the neural network-based baseline model consists of three densely connected layers with the first layer capturing the nine-channel images as vectorised inputs, the middle layer with 50 neurons using ReLU as the activation function, and finally the output layer using the sigmoid activation function with one neuron.</p>
<p>Results</p>
<p>The SAXS-1D and Sentinel-SLSTR benchmarks were evaluated on two architectures.These were a CPU system with two Xeon E5-2630-v3 processors, 20MB Cache, 64GB RAM, and 16 cores (32 hyper-threaded), and a GPU system with a TITAN-X (Pascal) GPU with 12GB DDR and 3840 GPU cores.</p>
<p>For both cases we report the classification performance (F1, accuracy and loss) and runtimes (training and inference time per sample).Wherever possible, we repeat the same across the different datasets.In Figure 13, we show the classification performance of the SAXS-1D benchmark.The dataset has 20,000 1D profiles (with a 70:30 train:test split).For a simple baseline, it can be observed that different architectures yield different classification performance (both loss and accuracy).As the class divisions are even between the spheres and the parallelepiped, the F1 performance and the classification accuracy are the same here.We then show the overall training time and inference time performance for the same benchmark in Figure 14.The key observation here is that the inference time, as a percentage of overall training time, is different between two different architectures.More specifically, the inference time is 40% of the training time for the CPU architecture while for GPU it is 60%.</p>
<p>We show the classification and runtime performance for the Sentinel-SLSTR in Figure 15, using the dataset described above.Given the baseline is with a single data set, we cannot draw any conclusions on the relationship between accuracy and dataset size.However, we observe that, as expected, the GPU architecture offers better training performance compared to the CPU platform.</p>
<p>Concluding Remarks</p>
<p>Deep learning is transforming many areas of computer science and underpinning the AI revolution that is happening around us.At the UK's Rutherford Appleton Laboratory, the large experimental facilities are now generating large volumes of increasingly complex data which will require new AI technologies to manage and interpret.In this paper we have given some examples of the opportunities for machine learning to play an important role both in the generation and analysis of some of these large datasets.In many areas of science, we are now seeing the emergence of a genuinely new 'Fourth Paradigm' of data-intensive scientific discovery [87,88].</p>
<p>For example, the combination of chemical databases, experimental data and detailed computer simulations is now leading to exciting new opportunities in materials science.</p>
<p>We have also introduced initial results on creating a scientific machine learning benchmark suite (SciML) covering a range of different scientific domains.Such a benchmark suite, based on scientific datasets of a significant scale and complexity, will enable scientists, computer scientists and data scientists to map out the applicability and limitations of deep learning neural networks and other machine learning algorithms applied to a range of real applications.Analysis of the SciML benchmark results will also reveal the strengths and weaknesses of the different computing platforms -from commercial Clouds and HPC systems to GPUs and FPGAs.These benchmarks will also provide scientists with hands-on experience of using machine learning algorithms and environments on realistic-scale scientific datasets.In addition, the SciML benchmark suite will provide an important platform for research.One urgent research issue for scientists is the need to develop a disciplined framework for the uncertainty quantification (UQ) of deep learning algorithms.Another important issue is the need for transparency in understanding how such deep neural networks reach their conclusions.The robustness of deep learning predictions and their vulnerability to adversarial noise attacks also give genuine cause for concern.For applications in areas such as materials science and the life sciences, the challenge of incorporating physical, chemical or biological constraints into deep learning algorithms is an exciting topic for research.Despite these undoubted research challenges, the success of DeepMind's AlphaFold project has shown the effectiveness of deep learning for protein folding prediction.Could deep learning have a similarly transformative impact on other areas of dataintensive science?</p>
<p>Fig. 1 :
1
Fig. 1: Schematic representation of the workflow for a Cryo-Soft X-ray tomography experiment</p>
<p>shows a schematic of the cryoEM data processing pipeline [25].</p>
<p>Fig. 2 A
2
Fig. 2 A schematic of the single particle reconstruction cryoEM pipeline.Image thanks to Creative Biostructure, https://www.creative-biostructure.com</p>
<p>Fig. 3 :
3
Fig. 3: [A] Overview of the techniques employed at OCTOPUS.[B] An illustration of the automating FLImP integrated intensity track selection process.[Bi] Raw FLImP track showing regions deemed suitable (blue) and unsuitable (red) for FLImP analysis.[Bii] Processed FLImP track from [Bi] with distinct levels detected.[C] Automatic detection of regions suitable for FLImP using a deep learning approach.</p>
<p>Figure 4 :
4
Figure 4: The ML explosion in materials science.The number of papers containing the terms machine learning and materials are plotted on a bar chart.We also indicate the dates of materials data repositories becoming available and plot the number of citations for popular machine learning toolkit, Scikit-Learn over the same period.</p>
<p>. The simulated patterns provide a labelled ground truth of azimuthal angles, as a function of the patterns Ψ(R)T.We then train our NN to predict Ψ based on the input image R, updating the filters, weights and biases of the NN to minimize the difference between predicted Ψ(R)NN and Ψ(R)T.The NN that we train is then capable of predicting the azimuthal angle to within 6.5⁰, see Figure 5.The NN, once trained, can provide an answer in a fraction of the time required for exhaustive comparison of images.</p>
<p>Figure 5
5
Figure 5 Schematic representation of the CNN used to predict coupling azimuthal angle from diffuse multiple scattering images.A 2D map of multiple scattering lines is passed through 2 convolutional layers, flattened and passed through two densely connected layers and finally passed to a single output node for Ψ.Note that the numbers of filters and nodes are just for illustration, see methods section on DMS network for details.</p>
<p>Figure 6
6
Figure 6 Training and validation scores for the mean absolute error for prediction of the azimuthal angle of a DMS pattern.</p>
<p>Fig. 7
7
Fig. 7 Schematic representation of the CNN used to predict coupling constants from inelastic neutron scattering images.A 2D map of integrated energy is passed through 4 convolutional layers, flattened and densely connected to two output nodes for J and J'.Note numbers of filters and nodes are just for illustration, see methods section on Magnon network for details.</p>
<p>Fig. 10 :
10
Fig.10: An example of two-dimensional scattering patterns for sub-shapes of sphere, cylinder, ellipsoid and parallelepiped shapes, from left to right, respectively.The profiles were generated by using the SASView Software[72].</p>
<p>Fig. 12 .
12
Fig. 12.This shows an example of cloud masking data.Left to right: actual image, ground truth, our generated probability mask, and our generated map.Here, white regions represent the cloud and yellow regions provide the probability map.The colours in the first image are due to the different reflective behavior of different elements in the image, such as sea, ice, land and cloud.</p>
<p>Fig. 13 :
13
Fig. 13: Performance of the SAXS-baseline model on CPU and GPU systems.The figure shows the classification performance of the binary classification problem on the 1D profiles of mono-disperse shapes on two different datasets, on two different architectures.</p>
<p>Fig. 14 .
14
Fig.14.Training and inference time per sample across two datasets for the SAXS-1D benchmark.</p>
<p>Figure 15 .
15
Figure 15.Classification and runtime performance for the Sentinel-SLSTR benchmark, where the classification task is to mark each pixel as 'Cloud' or 'Not Cloud'.</p>
<p>AcknowledgementsThe authors wish to thank Mark Basham; Tom Burnley, Martyn Winn and Jola Mirecka; Ben Davies and Dan Rolfe for their assistance in describing their work at the Diamond Light Source; at the Electron cryoEM Facility; and the OCTOPUS Laser Facility, respectively.This work was supported by Wave 1 of The UKRI Strategic Priorities Fund under the EPSRC Grant EP/T001569/1, particularly the "AI for Science" theme within that grant &amp; The Alan Turing Institute.We are grateful to James Hetherington, Oonagh McGee, Amit Mulji, Jon Rowe and Adrian Smith at the Turing Institute for their help and support.
The Deep Learning Revolution. T J Sejnowski, 2018 Oct 23MIT Press</p>
<p>ImageNet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L-J Li, Kai Li, Li Fei-Fei, 10.1109/CVPR.2009.5206848</p>
<p>Imagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in Neural Information Processing Systems. 2012</p>
<p>Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>. Geoffrey Hinton, Lukas Masuch, Dec 2015. 26th August, 2019</p>
<p>De novo structure prediction with deeplearning based scoring. R Evans, J Jumper, J Kirkpatrick, L Sifre, T F Green, C Qin, A Zidek, A Nelson, A Bridgland, H Penedones, S Petersen, Annu Rev Biochem. 772018</p>
<p>Critical Assessment of protein Structure Prediction). 4 th September, 2019</p>
<p>. Rosetta , 4 th September, 2019</p>
<p>Google's DeepMind aces protein folding. R Service, 10.1126/science.aaw2747Science. 2018 Dec 6</p>
<p>. David Baker, February 2019private communication</p>
<p>. 10.1002/jcc.24304</p>
<p>. 10.1107/S1600577516020117</p>
<p>. 10.1038/s41598-018-19426-7</p>
<p>. 10.1088/0957-4484/27/37/374002</p>
<p>. 10.1117/12.2274731</p>
<p>. 10.1016/j.commatsci.2019.01.013arXiv:1902.06876</p>
<p>. 10.1038/s41586-019-1116-4</p>
<p>Cryo-soft X-ray tomography: using soft X-rays to explore the ultrastructure of whole cells. Emerging Topics in Life Sciences. M Harkiolaki, M C Darrow, M C Spink, E Kosior, K Dent, E Duke, 10.1042/ETLS201700862018 Mar 292</p>
<p>Super-Region Volume Segmentation workbench. I Luengo, M C Darrow, M C Spink, Y Sun, W Dai, C Y He, 10.1016/j.jsb.2017.02.007Journal of Structural Biology. 19812017 Apr</p>
<p>Zooniverse Science Scribbler: Virus Factory. 27th August, 2019</p>
<p>Cryo-electron microscopy -a primer for the non-microscopist. Jls Milne, M J Borgnia, A Bartesaghi, Eeh Tran, L A Earl, D M Schauder, 10.1111/febs.12078FEBS Journal. 28012012 Dec 17</p>
<p>Electron Bio-Imaging Centre (eBIC): the UK national research facility for biological electron microscopy. D K Clare, C A Siebert, C Hecksel, C Hagen, V Mordhorst, M Grange, 10.1107/S2059798317007756Acta Crystallographica Section D Structural Biology. 7362017 May 31</p>
<p>Collaborative Computational Project for Electron cryo-Microscopy. C Wood, T Burnley, A Patwardhan, S Scheres, M Topf, A Roseman, 10.1107/S1399004714018070Acta Crystallographica Section D Biological Crystallography. 7112015 Jan 1</p>
<p>EMPIAR: a public archive for raw electron microscopy image data. A Iudin, P K Korir, J Salavert-Torres, G J Kleywegt, A Patwardhan, 10.1038/nmeth.3806Nature Methods. 1352016 Mar 21</p>
<p>Positive-unlabeled convolutional neural networks for particle picking in cryo-electron micrographs. T Bepler, A Morin, J Brasch, S Lawrence, A J Noble, B Berger, Res Comput Mol Biol. 2018</p>
<p>. Apr. 10812</p>
<p>Optics clustered to output unique solutions: A multi-laser facility for combined single molecule and ensemble microscopy. D T Clarke, S W Botchway, B C Coles, S R Needham, S K Roberts, D J Rolfe, 10.1063/1.3635536Review of Scientific Instruments. 829937052011 Sep</p>
<p>EGFR oligomerization organizes kinase-active dimers into competent signalling platforms. S R Needham, S K Roberts, A Arkhipov, V P Mysore, C J Tynan, L C Zanetti-Domingues, 10.1038/ncomms13307Nature Communications. 712016 Oct 31</p>
<p>Lightweight Deep Convolutional Network for Tiny Object Recognition. T-D Truong, V-T Nguyen, M-T Tran, 10.5220/0006752006750682Proceedings of the 7th International Conference on Pattern Recognition Applications and Methods. SCITEPRESS -Science and Technology Publications. the 7th International Conference on Pattern Recognition Applications and Methods. SCITEPRESS -Science and Technology Publications2018</p>
<p>Scikit-learn: Machine Learning in Python. F Pedregosa, Journal of Machine Learning Research. 122011 Oct. Oct</p>
<p>. F Chollet, 27th August 2019</p>
<p>Quantum Chemistry Program Exchange, Facilitator of Theoretical and Computational Chemistry in Pre-Internet History. D B Boyd, ACS Symposium Series. Internet</p>
<p>. 10.1021/bk-2013-1122.ch0082013American Chemical Society</p>
<p>Inhomogeneous Electron Gas. Physical Review. P Hohenberg, W Kohn, 10.1103/PhysRev.136.B8641964 Nov 9136</p>
<p>Self-Consistent Equations Including Exchange and Correlation Effects. W Kohn, L J Sham, 10.1103/PhysRev.140.A1133Physical Review. 1404A1965 Nov</p>
<p>Structure prediction drives materials discovery. A R Oganov, C J Pickard, Q Zhu, R J Needs, 10.1038/s41578-019-0101-8Nature Reviews Materials. 452019 Apr 4</p>
<p>The 2019 materials by design roadmap. K Alberi, M B Nardelli, A Zakutayev, L Mitas, S Curtarolo, A Jain, 10.1088/1361-6463/aad926Journal of Physics D: Applied Physics. 521130012018 Oct</p>
<p>Machine learning for molecular and materials science. K T Butler, D W Davies, H Cartwright, O Isayev, A Walsh, 10.1038/s41586-018-0337-2Nature. 55977152018 Jul</p>
<p>Prediction model of band gap for inorganic compounds by combination of density functional theory calculations and machine learning techniques. J Lee, A Seko, K Shitara, K Nakayama, I Tanaka, 10.1103/PhysRevB.93.115104Physical Review B. 93112016 Mar 1</p>
<p>Machine-Learning-Assisted Accurate Band Gap Predictions of Functionalized MXene. A C Rajan, A Mishra, S Satsangi, R Vaish, H Mizuseki, K-R Lee, 10.1021/acs.chemmater.8b00686Chemistry of Materials. 30122018 May 31</p>
<p>Predicting the Band Gaps of Inorganic Solids by Machine Learning. Y Zhuo, Mansouri Tehrani, A Brgoch, J , 10.1021/acs.jpclett.8b00124The Journal of Physical Chemistry Letters. 972018 Mar 13</p>
<p>Computer-aided design of metal chalcohalide semiconductors: from chemical composition to crystal structure. D W Davies, K T Butler, J M Skelton, C Xie, A R Oganov, A Walsh, 10.1039/C7SC03961AChemical Science. 942018</p>
<p>Inverse molecular design using machine learning: Generative models for matter engineering. B Sanchez-Lengeling, A Aspuru-Guzik, 10.1126/science.aat2663Science. 36164002018 Jul 26</p>
<p>Low Data Drug Discovery with One-Shot Learning. H Altae-Tran, B Ramsundar, A S Pappu, V Pande, 10.1021/acscentsci.6b00367ACS Central Science. 342017 Apr 3</p>
<p>Objectivereinforced generative adversarial networks (ORGAN) for sequence generation models. G L Guimaraes, B Sanchez-Lengeling, C Outeiral, P L Farias, A Aspuru-Guzik, arXiv:1705.108432017 May 30arXiv preprint</p>
<p>High-throughput computational X-ray absorption spectroscopy. Scientific Data. K Mathew, C Zheng, D Winston, C Chen, A Dozier, J J Rehr, 10.1038/sdata.2018.1512018 Jul 315</p>
<p>Neural Network Approach for Characterizing Structural Transformations by X-Ray Absorption Fine Structure Spectroscopy. J Timoshenko, A Anspoks, A Cintins, A Kuzmin, J Purans, A I Frenkel, 10.1103/PhysRevLett.120.225502Physical Review Letters. 120222018 May 31</p>
<p>Classification of local chemical environments from xray absorption spectra using supervised machine learning. M R Carbone, S Yoo, M Topsakal, D Lu, 10.1103/PhysRevMaterials.3.033604Physical Review Materials. 332019 Mar 13</p>
<p>Subnanometer Substructures in Nanoassemblies Formed from Clusters under a Reactive Atmosphere Revealed Using Machine Learning. J Timoshenko, A Halder, B Yang, S Seifert, M J Pellin, S Vajda, 10.1021/acs.jpcc.8b07952The Journal of Physical Chemistry C. 122372018 Aug 26</p>
<p>Automated generation and ensemble-learned matching of X-ray absorption spectra. npj Computational Materials. C Zheng, K Mathew, C Chen, Y Chen, H Tang, A Dozier, 10.1038/s41524-018-0067-x2018 Mar 204</p>
<p>Diffuse multiple scattering. Aga Nisbet, G Beutier, F Fabrizi, B Moser, S P Collins, 10.1107/S2053273314026515Acta Crystallographica Section A Foundations and Advances. 7112015 Jan 1</p>
<p>Neutron Scattering Investigation of Phase Transitions and Magnetic Correlations in the Two-Dimensional Antiferromagnets K2NiF4, Rb2MnF4, Rb2FeF4. Physical Review B. R J Birgeneau, H J Guggenheim, G Shirane, 10.1103/PhysRevB.1.22111970 Mar 11</p>
<p>Twomagnon excitations observed by neutron scattering in the two-dimensional spin-5/2 Heisenberg antiferromagnet Rb2MnF4. T Huberman, R Coldea, R A Cowley, D A Tennant, R L Leheny, R J Christianson, 10.1103/PhysRevB.72.014413Physical Review B. 7212005 Jul 6</p>
<p>Spin fluctuations in random magnetic-nonmagnetic two-dimensional antiferromagnets. I. Dynamics. Physical Review B. R A Cowley, G Shirane, R J Birgeneau, H J Guggenheim, 10.1103/PhysRevB.15.42921977 May 115</p>
<p>Linear spin wave theory for single-Q incommensurate magnetic structures. S Toth, B Lake, 10.1088/0953-8984/27/16/166002Journal of Physics: Condensed Matter. 27161660022015 Mar 30</p>
<p>A mixed-scale dense convolutional neural network for image analysis. D M Pelt, J A Sethian, 10.1073/pnas.1715832114Proceedings of the National Academy of Sciences. the National Academy of Sciences2017 Dec 26115</p>
<p>. Mlperf, 13 th May, 2019</p>
<p>BenchCouncil. 4 th September, 2019</p>
<p>Coral-2 Benchmark. 13 th May, 2019</p>
<p>A Modular Benchmarking Infrastructure for High-Performance and Reproducible Deep Learning. T Ben-Nun, M Besta, S Huber, A N Ziogas, D Peter, T Hoefler, IEEE International Parallel &amp; Distributed Processing Symposium. 2019 May33</p>
<p>T Ben-Nun, M Besta, S Huber, A N Ziogas, D Peter, T Hoefler, arXiv:1901.10183A Modular Benchmarking Infrastructure for High-Performance and Reproducible Deep Learning. 2019 Jan 29arXiv preprint</p>
<p>AIBench: An Industry Standard Internet Service AI Benchmark Suite. W Gao, 2019Technical Report</p>
<p>Z Jiang, BenchCouncil International Symposium on Benchmarking. 2018Technical ReportHPC AI500: A Benchmark Suite for HPC AI Systems</p>
<p>AIoT Bench: Towards Comprehensive Benchmarking Mobile and Embedded device Intelligence. C Luo, BenchCouncil International Symposium on Benchmarking. 2018Technical Report</p>
<p>T Hao, Edge AIBench: Towards Comprehensive End-to-end Edge Computing Benchmarking. 2018Technical ReportBenchCouncil International Symposium on Benchmarking</p>
<p>BigDataBench: a Scalable and Unified Big Data and AI Benchmark Suite. W Gao, 2018Technical Report</p>
<p>Small-angle scattering of X-rays. A Guinier, G Fournet, K L Yudowitch, 1955Wiley24</p>
<p>The SAXS guide: getting acquainted with the principles. H Schnablegger, Y Singh, 2011 OctAnton Paar GmbHAustria</p>
<p>SASView for small angle x-ray scattering. 13 th May, 2019</p>
<p>Small-angle X-ray scattering method to characterize molecular interactions: Proof of concept. N Allec, M Choi, N Yesupriya, B Szychowski, M R White, M G Kann, 10.1038/srep12085Scientific Reports. 512015 Jul 10</p>
<p>Atomic form factors, incoherent scattering functions, and photon scattering cross sections. J H Hubbell, W J Veigele, E A Briggs, R T Brown, D T Cromer, R J Howerton, 10.1063/1.555523Journal of Physical and Chemical Reference Data. 431975 Jul</p>
<p>Probabilistic physically based cloud screening of satellite infrared imagery for operational sea surface temperature retrieval. C J Merchant, A R Harris, E Maturi, S Maccallum, 10.1256/qj.05.15Quarterly Journal of the Royal Meteorological Society. 1316112005 Oct 1</p>
<p>Generalized Bayesian cloud detection for satellite imagery. Part 1: Technique and validation for night-time imagery over land and sea. S Mackie, O Embury, C Old, C J Merchant, P Francis, 10.1080/01431160903051703International Journal of Remote Sensing. 31102010 May 20</p>
<p>Generalized Bayesian cloud detection for satellite imagery. Part 2: Technique and validation for daytime imagery. S Mackie, C J Merchant, O Embury, P Francis, 10.1080/01431160903051711International Journal of Remote Sensing. 31102010 May 20</p>
<p>Cloud detection machine learning algorithms for PROBA-V. L Gomez-Chova, Mateo - Garcia, G Munoz-Mari, J , Camps- Valls, G , 10.1109/IGARSS.2017.81274372017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS). 2017</p>
<p>Fast Cloud Segmentation Using Convolutional Neural Networks. Remote Sensing. J Drönner, N Korfhage, S Egli, M Mühling, B Thies, J Bendix, 10.3390/rs101117822018 Nov 10101782</p>
<p>A Cloud Detection Algorithm for Remote Sensing Images Using Fully Convolutional Neural Networks. S Mohajerani, T A Krammer, P Saeedi, 10.1109/MMSP.2018.8547095IEEE 20th International Workshop on Multimedia Signal Processing (MMSP). IEEE2018. 2018</p>
<p>Cloud and cloud shadow detection in Landsat imagery based on deep convolutional neural networks. Remote Sensing of Environment. D Chai, S Newsam, H K Zhang, Y Qiu, J Huang, 10.1016/j.rse.2019.03.0072019 May225</p>
<p>Clouds Classification from Sentinel-2 Imagery with Deep Residual Learning and Semantic Image Segmentation. Remote Sensing. C-C Liu, Y-C Zhang, P-Y Chen, C-C Lai, Y-H Chen, J-H Cheng, 10.3390/rs110201192019 Jan 1011119</p>
<p>Deep learning based cloud detection for medium and high resolution remote sensing images of different sensors. Z Li, H Shen, Q Cheng, Y Liu, S You, Z He, 10.1016/j.isprsjprs.2019.02.017ISPRS Journal of Photogrammetry and Remote Sensing. 1502019 Apr</p>
<p>Distinguishing Cloud and Snow in Satellite Images via Deep Convolutional Network. IEEE Geoscience and Remote Sensing Letters. Y Zhan, J Wang, J Shi, G Cheng, L Yao, W Sun, 10.1109/LGRS.2017.27358012017 Oct14</p>
<p>Utilizing Multilevel Features for Cloud Detection on Satellite Imagery. Remote Sensing. X Wu, Z Shi, 10.3390/rs101118532018 Nov 21101853</p>
<p>M Rußwurm, M Körner, arXiv:1811.02471Convolutional LSTMs for Cloud-Robust Segmentation of Remote Sensing Imagery. 2018 OctarXiv preprint</p>
<p>Cloud Removal for Sentinel-2 Imagery Using a Cyclic Consistent Generative Adversarial Networks. P Singh, N Komodakis, Cloud-Gan, 10.1109/IGARSS.2018.8519033IGARSS 2018 -2018 IEEE International Geoscience and Remote Sensing Symposium. IEEE2018</p>
<p>The fourth paradigm: data-intensive scientific discovery. T Hey, S Tansley, K M Tolle, Hey AJ2009 OctMicrosoft researchRedmond, WA</p>
<p>The Fourth Paradigm Ten Years On. T Hey, A Trefethen, 2019</p>            </div>
        </div>

    </div>
</body>
</html>
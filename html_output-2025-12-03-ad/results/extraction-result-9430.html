<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9430 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9430</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9430</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-162.html">extraction-schema-162</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-34696e5a654f970f9f5cc8cb1e1dade81b5fcae1</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/34696e5a654f970f9f5cc8cb1e1dade81b5fcae1" target="_blank">NLP-ADBench: NLP Anomaly Detection Benchmark</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> NLP-ADBench is introduced, the most comprehensive NLP anomaly detection (NLP-AD) benchmark to date, which includes eight curated datasets and 19 state-of-the-art algorithms that adapt classical, non-AD methods to language embeddings from BERT and OpenAI.</p>
                <p><strong>Paper Abstract:</strong> Anomaly detection (AD) is an important machine learning task with applications in fraud detection, content moderation, and user behavior analysis. However, AD is relatively understudied in a natural language processing (NLP) context, limiting its effectiveness in detecting harmful content, phishing attempts, and spam reviews. We introduce NLP-ADBench, the most comprehensive NLP anomaly detection (NLP-AD) benchmark to date, which includes eight curated datasets and 19 state-of-the-art algorithms. These span 3 end-to-end methods and 16 two-step approaches that adapt classical, non-AD methods to language embeddings from BERT and OpenAI. Our empirical results show that no single model dominates across all datasets, indicating a need for automated model selection. Moreover, two-step methods with transformer-based embeddings consistently outperform specialized end-to-end approaches, with OpenAI embeddings outperforming those of BERT. We release NLP-ADBench at https://github.com/USC-FORTIS/NLP-ADBench, providing a unified framework for NLP-AD and supporting future investigations.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9430.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9430.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI-embed</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI text-embedding-3-large (embedding use)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>High-dimensional transformer-based text embeddings (3072 dimensions) from OpenAI used as input features for classical anomaly detection algorithms in a two-step pipeline for NLP anomaly detection across news, spam, review, and emotion datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>New embedding models and api updates</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-embedding-3-large (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer-based embedding model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3072-dim embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>text (converted from tabular columns, documents, short messages into dense embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>news (AGNews, BBCNews, N24News), email spam, SMS spam, movie reviews, emotion datasets, Yelp reviews</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>semantic/category-level anomalies (downsampled anomalous class/outliers in text corpora)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Generate fixed high-dimensional embeddings for each text instance using OpenAI's text-embedding-3-large, then apply traditional numerical anomaly detection algorithms (two-step approach) such as LUNAR, LOF, ECOD, AE, VAE, IForest, DeepSVDD, SO-GAAL to the embedding vectors to score and rank anomalies.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>End-to-end text AD methods (CVDD, DATE, FATE*), and classical AD methods applied to embeddings: LOF, LUNAR, ECOD, IForest, DeepSVDD, SO-GAAL, AE, VAE</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>AUROC (primary), AUPRC (secondary)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>OpenAI + LUNAR AUROC: AGNews 0.9226, BBCNews 0.9732, EmailSpam 0.9343, Emotion 0.9328, MovieReview 0.6474, N24News 0.8320, SMSSpam 0.7189, YelpReview 0.9452. OpenAI + LOF AUROC: AGNews 0.8905, BBCNews 0.9558, EmailSpam 0.9263, Emotion 0.7304, YelpReview 0.8733. AUPRC examples: OpenAI + LUNAR AUPRC AGNews 0.6918, BBCNews 0.8653, YelpReview 0.4524; OpenAI + AE AUPRC Emotion 0.8355, OpenAI + VAE AUPRC YelpReview 0.8467.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Two-step methods using OpenAI embeddings consistently outperform end-to-end AD methods and BERT embeddings on many multi-class and semantically diverse datasets (examples: OpenAI+LUNAR 0.9226 vs CVDD 0.6046 on AGNews; OpenAI+LOF 0.9558 vs CVDD 0.7221 on BBCNews). Exceptions exist for short-text, lexical-anomaly datasets (e.g., SMS spam) where end-to-end token-level DATE outperforms OpenAI+LUNAR (DATE 0.9398 vs OpenAI+LUNAR 0.7189 AUROC).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High embedding dimensionality increases computational and financial cost; PCA reduction can slightly reduce AUROC and may increase inference runtime; poorer performance relative to token-level methods on short/lexically salient anomaly datasets (high lexical-burstiness); reliance on static precomputed embeddings prevents modelling streaming/dynamic contexts; benchmark labels are predefined which limits unsupervised domain adaptation assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>High-dimensional transformer embeddings (3072-dim) substantially boost two-step AD performance on semantically complex datasets; OpenAI embeddings still outperform BERT when reduced to the same dimensionality (via PCA) in many cases; there is a trade-off between detection performance and computation/latency costs that motivates NLP-AD-specific dimensionality reduction and adaptive embedding strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NLP-ADBench: NLP Anomaly Detection Benchmark', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9430.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9430.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT-embed</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>bert-base-uncased (embedding use)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard transformer encoder (bert-base-uncased) producing 768-dimensional contextual embeddings used as features for classical anomaly detection algorithms in a two-step approach for NLP anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>bert-base-uncased</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer encoder</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>768-dim embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>text (converted from tabular columns, documents, short messages into dense embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>news, spam, emotion, reviews (same NLPAD datasets as above)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>semantic/category-level anomalies (downsampled anomalous classes/outliers)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Compute 768-dimensional BERT embeddings for each text instance and apply traditional numerical anomaly detection algorithms (LOF, LUNAR, ECOD, IForest, AE, VAE, DeepSVDD, SO-GAAL) to the embeddings to detect anomalies.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Same set of AD methods as for OpenAI embeddings (LOF, LUNAR, ECOD, IForest, DeepSVDD, AE, VAE, SO-GAAL) and compared against end-to-end methods (CVDD, DATE, FATE*).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>AUROC (primary), AUPRC (secondary)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Representative AUROC: BERT + LUNAR AGNews 0.7694, BBCNews 0.9260, EmailSpam 0.8417, Emotion 0.5186, MovieReview 0.4687, N24News 0.6284, SMSSpam 0.6953, YelpReview 0.6522. BERT + LOF AGNews 0.7432, BERT + AE YelpReview 0.6441. AUPRC values are generally lower than OpenAI-based two-step methods.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>BERT-based two-step methods are consistently outperformed by OpenAI embeddings combined with the same AD algorithms (example: OpenAI+LUNAR 0.9452 vs BERT+LUNAR 0.6522 on YelpReview, ~44.9% relative improvement). However, BERT two-step methods still outperform many end-to-end baselines on several datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lower embedding dimensionality and less expressive contextual encoding compared to newer embedding models leads to weaker anomaly separation on semantically diverse, multi-topic datasets; still limited by static-embedding two-step pipeline constraints (no streaming adaptation).</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Even when dimensionality is equalized (e.g., PCA on OpenAI to 768), representations from modern high-capacity embedding models retain advantages over BERT, suggesting qualitative representation differences beyond mere dimensionality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NLP-ADBench: NLP Anomaly Detection Benchmark', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9430.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9430.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI+PCA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PCA-reduced OpenAI embeddings (3072->768)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Dimensionality-reduced OpenAI text-embedding-3-large vectors (via PCA to 768 dimensions) evaluated to study performance-efficiency trade-offs when applying classical AD algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-embedding-3-large (OpenAI) + PCA</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer-based embedding model + linear dimensionality reduction</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3072-dim original -> 768-dim PCA projection</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>text (embeddings reduced by PCA)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>news, movie reviews, other NLPAD datasets</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>semantic outliers defined by category downsampling</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Apply PCA to project OpenAI 3072-dim embeddings down to 768 dimensions, then run the same two-step anomaly detection algorithms (e.g., LUNAR, L1R) on the reduced embeddings to measure detection performance and runtime/cost trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>OpenAI (original 3072-dim) + AD algorithms; BERT + AD algorithms</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>AUROC, runtime/cost (inference and embedding cost), AUPRC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Example: PCA-reduced OpenAI + LUNAR AUROC on NLPAD-AGNews dropped slightly (paper example: from 0.907 to 0.890 for one reported comparison). In Table 5 the PCA variants generally retain performance above BERT baselines at the same dimension but show varied effects across datasets; PCA sometimes increases total runtime despite dimensionality reduction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>PCA-reduced OpenAI embeddings still typically outperform BERT embeddings at equal dimensionality, but they can slightly reduce AUROC relative to full OpenAI embeddings; PCA also increased total runtime in the experiments (compression made decision boundaries denser and inference slower in some cases).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>PCA compression can modestly reduce detection accuracy and unexpectedly increase overall runtime/inference cost; simple linear reduction may not remove redundancy optimally for AD tasks and can complicate downstream decision boundaries.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Linear dimensionality reduction (PCA) can preserve much of detection quality while lowering embedding size, but naive PCA may worsen practical efficiency and indicates the need for AD-specific dimensionality reduction techniques that trade off representational fidelity and computation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NLP-ADBench: NLP Anomaly Detection Benchmark', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9430.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9430.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DATE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Detecting Anomalies in Text via Self-Supervision of Transformers (DATE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end transformer-based anomaly detection method for text that uses self-supervised tasks (e.g., replaced-mask detection) to learn normal language patterns and score anomalies based on deviations at the token/sequence level.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Detecting anomalies in text via self-supervision of transformers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DATE (transformer-based self-supervised AD)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer (self-supervised training for anomaly scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>text sequences (short messages, emails, documents)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>SMS spam, email spam, reviews, news, emotion datasets</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>lexical and token-level anomalies (outliers with explicit token irregularities or replaced-mask signal deviations)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Train a transformer using self-supervised tasks (replaced-mask detection) on normal text to learn token/sequence-level patterns; anomalies are scored based on the transformer’s self-supervision loss/deviation on test instances.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Two-step embedding-based methods (OpenAI + AD algorithms, BERT + AD algorithms), end-to-end CVDD, FATE*</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>AUROC, AUPRC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>AUROC examples: AGNews 0.8120, BBCNews 0.9030, EmailSpam 0.9697 (best on EmailSpam), Emotion 0.6291, MovieReview 0.5185, N24News 0.7493, SMSSpam 0.9398 (best on SMS), YelpReview 0.6092. AUPRC EmailSpam 0.8885, SMSSpam 0.6112.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>DATE outperforms many two-step OpenAI/BERT methods on short-text, lexical-anomaly datasets (e.g., SMSSpam and EmailSpam), achieving highest AUROC and AUPRC on those datasets, but is outperformed by two-step transformer-embedding methods (OpenAI+LUNAR/LOF) on multi-class topic-diverse datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Less effective on subtle semantic anomalies that lack token-level irregularities (low lexical-burstiness); relies on self-supervised signal that aligns with surface lexical anomalies, so semantic/contextual anomalies may be missed.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Self-supervised token-level transformer methods are especially effective when anomalies manifest as lexical or formatting irregularities (short texts with high lexical-burstiness), suggesting complementary strengths to embedding-based two-step approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NLP-ADBench: NLP Anomaly Detection Benchmark', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9430.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9430.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CVDD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Context Vector Data Description (CVDD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised end-to-end textual anomaly detection method that uses pre-trained token embeddings and multi-head self-attention to learn context vectors representing normal patterns; anomalies are measured by cosine distance to learned context vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-attentive, multi-context one-class classification for unsupervised anomaly detection on text</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CVDD (context-vector self-attentive one-class model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>self-attentive transformer-style architecture (end-to-end AD)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>text sequences (documents, messages, descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>news, spam, reviews, emotion datasets</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>outliers relative to learned contextual norms (semantic/textual outliers)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Leverage pre-trained token embeddings and multi-head self-attention to learn a set of context vectors that capture normal-instance representations; detect anomalies by measuring cosine distance between instance projections and learned contexts, penalizing overlapping contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Two-step methods using BERT/OpenAI embeddings with LOF, LUNAR, ECOD, IForest, AE, VAE; end-to-end DATE, FATE*</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>AUROC, AUPRC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>AUROC examples: AGNews 0.6046, BBCNews 0.7221, EmailSpam 0.9340, Emotion 0.4867, MovieReview 0.4895, N24News 0.7507, SMSSpam 0.4782, YelpReview 0.5345. AUPRC AGNews 0.1296, EmailSpam 0.5353.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>CVDD is outperformed by two-step transformer-embedding methods on many topic-diverse datasets (e.g., OpenAI+LUNAR 0.9226 vs CVDD 0.6046 on AGNews), although CVDD achieves strong performance on some datasets (e.g., EmailSpam AUROC 0.9340). Overall, CVDD’s relative weakness is attributed to less powerful embeddings and older training regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Relies on older embeddings (historically GloVe in prior work) and may not leverage modern high-capacity transformer embeddings by default, reducing effectiveness on semantically subtle anomalies and diverse-topic corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>End-to-end context-vector approaches can be competitive on some datasets (notably EmailSpam in this benchmark) but generally lag behind two-step methods that leverage up-to-date transformer embeddings; integrating modern embeddings into end-to-end pipelines is a promising direction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NLP-ADBench: NLP Anomaly Detection Benchmark', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9430.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9430.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FATE*</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FATE* (adapted Few-shot Anomaly Detection in Text with Deviation Learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptation of a few-shot, deviation-learning end-to-end anomaly detection method trained here in an unsupervised manner (FATE*) to operate without labeled anomalies, learning reference scores from normal data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Few-shot anomaly detection in text with deviation learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FATE* (adapted deviation-learning transformer model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer-based end-to-end model with deviation learning</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>text (documents, reviews, messages)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>news, spam, emotion, reviews</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>semantic outliers (originally few-shot anomalies; adapted to unsupervised detection)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Adapt the FATE few-shot deviation-learning model by training exclusively on normal data (FATE*), modifying the framework to learn reference scores and deviations without labeled anomalies to produce anomaly scores end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>End-to-end DATE, CVDD and two-step embedding-based methods (OpenAI/BERT + AD algorithms)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>AUROC, AUPRC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>AUROC examples: AGNews 0.7756, BBCNews 0.9310, EmailSpam 0.9061, Emotion 0.5035, MovieReview 0.5289, N24News 0.8073, SMSSpam 0.6262, YelpReview 0.5945. AUPRC examples: BBCNews 0.5805, EmailSpam 0.5529.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>FATE* is competitive on certain datasets (e.g., BBCNews) but generally is outperformed by two-step OpenAI-embedding methods on multi-topic datasets; adaptation to unsupervised training may reduce the original few-shot strengths.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Original few-shot design relies on labeled anomalies; adapting to fully unsupervised training (FATE*) may reduce discriminative power. Like other end-to-end methods, it can struggle with semantically subtle anomalies that benefit from high-capacity external embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>The adaptation demonstrates that few-shot deviation-learning frameworks can be retooled for unsupervised NLP-AD, but performance trade-offs indicate value in hybridizing with modern embeddings or meta-learning model selection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NLP-ADBench: NLP Anomaly Detection Benchmark', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Ad-nlp: A benchmark for anomaly detection in natural language processing <em>(Rating: 2)</em></li>
                <li>Self-attentive, multi-context one-class classification for unsupervised anomaly detection on text <em>(Rating: 2)</em></li>
                <li>Detecting anomalies in text via self-supervision of transformers <em>(Rating: 2)</em></li>
                <li>Few-shot anomaly detection in text with deviation learning <em>(Rating: 2)</em></li>
                <li>Lunar: Unifying local outlier detection methods via graph neural networks <em>(Rating: 2)</em></li>
                <li>New embedding models and api updates <em>(Rating: 2)</em></li>
                <li>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding <em>(Rating: 1)</em></li>
                <li>Ad-llm: Benchmarking large language models for anomaly detection <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9430",
    "paper_id": "paper-34696e5a654f970f9f5cc8cb1e1dade81b5fcae1",
    "extraction_schema_id": "extraction-schema-162",
    "extracted_data": [
        {
            "name_short": "OpenAI-embed",
            "name_full": "OpenAI text-embedding-3-large (embedding use)",
            "brief_description": "High-dimensional transformer-based text embeddings (3072 dimensions) from OpenAI used as input features for classical anomaly detection algorithms in a two-step pipeline for NLP anomaly detection across news, spam, review, and emotion datasets.",
            "citation_title": "New embedding models and api updates",
            "mention_or_use": "use",
            "model_name": "text-embedding-3-large (OpenAI)",
            "model_type": "transformer-based embedding model",
            "model_size": "3072-dim embeddings",
            "data_type": "text (converted from tabular columns, documents, short messages into dense embeddings)",
            "data_domain": "news (AGNews, BBCNews, N24News), email spam, SMS spam, movie reviews, emotion datasets, Yelp reviews",
            "anomaly_type": "semantic/category-level anomalies (downsampled anomalous class/outliers in text corpora)",
            "method_description": "Generate fixed high-dimensional embeddings for each text instance using OpenAI's text-embedding-3-large, then apply traditional numerical anomaly detection algorithms (two-step approach) such as LUNAR, LOF, ECOD, AE, VAE, IForest, DeepSVDD, SO-GAAL to the embedding vectors to score and rank anomalies.",
            "baseline_methods": "End-to-end text AD methods (CVDD, DATE, FATE*), and classical AD methods applied to embeddings: LOF, LUNAR, ECOD, IForest, DeepSVDD, SO-GAAL, AE, VAE",
            "performance_metrics": "AUROC (primary), AUPRC (secondary)",
            "performance_results": "OpenAI + LUNAR AUROC: AGNews 0.9226, BBCNews 0.9732, EmailSpam 0.9343, Emotion 0.9328, MovieReview 0.6474, N24News 0.8320, SMSSpam 0.7189, YelpReview 0.9452. OpenAI + LOF AUROC: AGNews 0.8905, BBCNews 0.9558, EmailSpam 0.9263, Emotion 0.7304, YelpReview 0.8733. AUPRC examples: OpenAI + LUNAR AUPRC AGNews 0.6918, BBCNews 0.8653, YelpReview 0.4524; OpenAI + AE AUPRC Emotion 0.8355, OpenAI + VAE AUPRC YelpReview 0.8467.",
            "comparison_to_baseline": "Two-step methods using OpenAI embeddings consistently outperform end-to-end AD methods and BERT embeddings on many multi-class and semantically diverse datasets (examples: OpenAI+LUNAR 0.9226 vs CVDD 0.6046 on AGNews; OpenAI+LOF 0.9558 vs CVDD 0.7221 on BBCNews). Exceptions exist for short-text, lexical-anomaly datasets (e.g., SMS spam) where end-to-end token-level DATE outperforms OpenAI+LUNAR (DATE 0.9398 vs OpenAI+LUNAR 0.7189 AUROC).",
            "limitations_or_failure_cases": "High embedding dimensionality increases computational and financial cost; PCA reduction can slightly reduce AUROC and may increase inference runtime; poorer performance relative to token-level methods on short/lexically salient anomaly datasets (high lexical-burstiness); reliance on static precomputed embeddings prevents modelling streaming/dynamic contexts; benchmark labels are predefined which limits unsupervised domain adaptation assessment.",
            "unique_insights": "High-dimensional transformer embeddings (3072-dim) substantially boost two-step AD performance on semantically complex datasets; OpenAI embeddings still outperform BERT when reduced to the same dimensionality (via PCA) in many cases; there is a trade-off between detection performance and computation/latency costs that motivates NLP-AD-specific dimensionality reduction and adaptive embedding strategies.",
            "uuid": "e9430.0",
            "source_info": {
                "paper_title": "NLP-ADBench: NLP Anomaly Detection Benchmark",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "BERT-embed",
            "name_full": "bert-base-uncased (embedding use)",
            "brief_description": "Standard transformer encoder (bert-base-uncased) producing 768-dimensional contextual embeddings used as features for classical anomaly detection algorithms in a two-step approach for NLP anomaly detection.",
            "citation_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "mention_or_use": "use",
            "model_name": "bert-base-uncased",
            "model_type": "transformer encoder",
            "model_size": "768-dim embeddings",
            "data_type": "text (converted from tabular columns, documents, short messages into dense embeddings)",
            "data_domain": "news, spam, emotion, reviews (same NLPAD datasets as above)",
            "anomaly_type": "semantic/category-level anomalies (downsampled anomalous classes/outliers)",
            "method_description": "Compute 768-dimensional BERT embeddings for each text instance and apply traditional numerical anomaly detection algorithms (LOF, LUNAR, ECOD, IForest, AE, VAE, DeepSVDD, SO-GAAL) to the embeddings to detect anomalies.",
            "baseline_methods": "Same set of AD methods as for OpenAI embeddings (LOF, LUNAR, ECOD, IForest, DeepSVDD, AE, VAE, SO-GAAL) and compared against end-to-end methods (CVDD, DATE, FATE*).",
            "performance_metrics": "AUROC (primary), AUPRC (secondary)",
            "performance_results": "Representative AUROC: BERT + LUNAR AGNews 0.7694, BBCNews 0.9260, EmailSpam 0.8417, Emotion 0.5186, MovieReview 0.4687, N24News 0.6284, SMSSpam 0.6953, YelpReview 0.6522. BERT + LOF AGNews 0.7432, BERT + AE YelpReview 0.6441. AUPRC values are generally lower than OpenAI-based two-step methods.",
            "comparison_to_baseline": "BERT-based two-step methods are consistently outperformed by OpenAI embeddings combined with the same AD algorithms (example: OpenAI+LUNAR 0.9452 vs BERT+LUNAR 0.6522 on YelpReview, ~44.9% relative improvement). However, BERT two-step methods still outperform many end-to-end baselines on several datasets.",
            "limitations_or_failure_cases": "Lower embedding dimensionality and less expressive contextual encoding compared to newer embedding models leads to weaker anomaly separation on semantically diverse, multi-topic datasets; still limited by static-embedding two-step pipeline constraints (no streaming adaptation).",
            "unique_insights": "Even when dimensionality is equalized (e.g., PCA on OpenAI to 768), representations from modern high-capacity embedding models retain advantages over BERT, suggesting qualitative representation differences beyond mere dimensionality.",
            "uuid": "e9430.1",
            "source_info": {
                "paper_title": "NLP-ADBench: NLP Anomaly Detection Benchmark",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "OpenAI+PCA",
            "name_full": "PCA-reduced OpenAI embeddings (3072-&gt;768)",
            "brief_description": "Dimensionality-reduced OpenAI text-embedding-3-large vectors (via PCA to 768 dimensions) evaluated to study performance-efficiency trade-offs when applying classical AD algorithms.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-embedding-3-large (OpenAI) + PCA",
            "model_type": "transformer-based embedding model + linear dimensionality reduction",
            "model_size": "3072-dim original -&gt; 768-dim PCA projection",
            "data_type": "text (embeddings reduced by PCA)",
            "data_domain": "news, movie reviews, other NLPAD datasets",
            "anomaly_type": "semantic outliers defined by category downsampling",
            "method_description": "Apply PCA to project OpenAI 3072-dim embeddings down to 768 dimensions, then run the same two-step anomaly detection algorithms (e.g., LUNAR, L1R) on the reduced embeddings to measure detection performance and runtime/cost trade-offs.",
            "baseline_methods": "OpenAI (original 3072-dim) + AD algorithms; BERT + AD algorithms",
            "performance_metrics": "AUROC, runtime/cost (inference and embedding cost), AUPRC",
            "performance_results": "Example: PCA-reduced OpenAI + LUNAR AUROC on NLPAD-AGNews dropped slightly (paper example: from 0.907 to 0.890 for one reported comparison). In Table 5 the PCA variants generally retain performance above BERT baselines at the same dimension but show varied effects across datasets; PCA sometimes increases total runtime despite dimensionality reduction.",
            "comparison_to_baseline": "PCA-reduced OpenAI embeddings still typically outperform BERT embeddings at equal dimensionality, but they can slightly reduce AUROC relative to full OpenAI embeddings; PCA also increased total runtime in the experiments (compression made decision boundaries denser and inference slower in some cases).",
            "limitations_or_failure_cases": "PCA compression can modestly reduce detection accuracy and unexpectedly increase overall runtime/inference cost; simple linear reduction may not remove redundancy optimally for AD tasks and can complicate downstream decision boundaries.",
            "unique_insights": "Linear dimensionality reduction (PCA) can preserve much of detection quality while lowering embedding size, but naive PCA may worsen practical efficiency and indicates the need for AD-specific dimensionality reduction techniques that trade off representational fidelity and computation.",
            "uuid": "e9430.2",
            "source_info": {
                "paper_title": "NLP-ADBench: NLP Anomaly Detection Benchmark",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "DATE",
            "name_full": "Detecting Anomalies in Text via Self-Supervision of Transformers (DATE)",
            "brief_description": "An end-to-end transformer-based anomaly detection method for text that uses self-supervised tasks (e.g., replaced-mask detection) to learn normal language patterns and score anomalies based on deviations at the token/sequence level.",
            "citation_title": "Detecting anomalies in text via self-supervision of transformers",
            "mention_or_use": "use",
            "model_name": "DATE (transformer-based self-supervised AD)",
            "model_type": "transformer (self-supervised training for anomaly scoring)",
            "model_size": null,
            "data_type": "text sequences (short messages, emails, documents)",
            "data_domain": "SMS spam, email spam, reviews, news, emotion datasets",
            "anomaly_type": "lexical and token-level anomalies (outliers with explicit token irregularities or replaced-mask signal deviations)",
            "method_description": "Train a transformer using self-supervised tasks (replaced-mask detection) on normal text to learn token/sequence-level patterns; anomalies are scored based on the transformer’s self-supervision loss/deviation on test instances.",
            "baseline_methods": "Two-step embedding-based methods (OpenAI + AD algorithms, BERT + AD algorithms), end-to-end CVDD, FATE*",
            "performance_metrics": "AUROC, AUPRC",
            "performance_results": "AUROC examples: AGNews 0.8120, BBCNews 0.9030, EmailSpam 0.9697 (best on EmailSpam), Emotion 0.6291, MovieReview 0.5185, N24News 0.7493, SMSSpam 0.9398 (best on SMS), YelpReview 0.6092. AUPRC EmailSpam 0.8885, SMSSpam 0.6112.",
            "comparison_to_baseline": "DATE outperforms many two-step OpenAI/BERT methods on short-text, lexical-anomaly datasets (e.g., SMSSpam and EmailSpam), achieving highest AUROC and AUPRC on those datasets, but is outperformed by two-step transformer-embedding methods (OpenAI+LUNAR/LOF) on multi-class topic-diverse datasets.",
            "limitations_or_failure_cases": "Less effective on subtle semantic anomalies that lack token-level irregularities (low lexical-burstiness); relies on self-supervised signal that aligns with surface lexical anomalies, so semantic/contextual anomalies may be missed.",
            "unique_insights": "Self-supervised token-level transformer methods are especially effective when anomalies manifest as lexical or formatting irregularities (short texts with high lexical-burstiness), suggesting complementary strengths to embedding-based two-step approaches.",
            "uuid": "e9430.3",
            "source_info": {
                "paper_title": "NLP-ADBench: NLP Anomaly Detection Benchmark",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "CVDD",
            "name_full": "Context Vector Data Description (CVDD)",
            "brief_description": "An unsupervised end-to-end textual anomaly detection method that uses pre-trained token embeddings and multi-head self-attention to learn context vectors representing normal patterns; anomalies are measured by cosine distance to learned context vectors.",
            "citation_title": "Self-attentive, multi-context one-class classification for unsupervised anomaly detection on text",
            "mention_or_use": "use",
            "model_name": "CVDD (context-vector self-attentive one-class model)",
            "model_type": "self-attentive transformer-style architecture (end-to-end AD)",
            "model_size": null,
            "data_type": "text sequences (documents, messages, descriptions)",
            "data_domain": "news, spam, reviews, emotion datasets",
            "anomaly_type": "outliers relative to learned contextual norms (semantic/textual outliers)",
            "method_description": "Leverage pre-trained token embeddings and multi-head self-attention to learn a set of context vectors that capture normal-instance representations; detect anomalies by measuring cosine distance between instance projections and learned contexts, penalizing overlapping contexts.",
            "baseline_methods": "Two-step methods using BERT/OpenAI embeddings with LOF, LUNAR, ECOD, IForest, AE, VAE; end-to-end DATE, FATE*",
            "performance_metrics": "AUROC, AUPRC",
            "performance_results": "AUROC examples: AGNews 0.6046, BBCNews 0.7221, EmailSpam 0.9340, Emotion 0.4867, MovieReview 0.4895, N24News 0.7507, SMSSpam 0.4782, YelpReview 0.5345. AUPRC AGNews 0.1296, EmailSpam 0.5353.",
            "comparison_to_baseline": "CVDD is outperformed by two-step transformer-embedding methods on many topic-diverse datasets (e.g., OpenAI+LUNAR 0.9226 vs CVDD 0.6046 on AGNews), although CVDD achieves strong performance on some datasets (e.g., EmailSpam AUROC 0.9340). Overall, CVDD’s relative weakness is attributed to less powerful embeddings and older training regimes.",
            "limitations_or_failure_cases": "Relies on older embeddings (historically GloVe in prior work) and may not leverage modern high-capacity transformer embeddings by default, reducing effectiveness on semantically subtle anomalies and diverse-topic corpora.",
            "unique_insights": "End-to-end context-vector approaches can be competitive on some datasets (notably EmailSpam in this benchmark) but generally lag behind two-step methods that leverage up-to-date transformer embeddings; integrating modern embeddings into end-to-end pipelines is a promising direction.",
            "uuid": "e9430.4",
            "source_info": {
                "paper_title": "NLP-ADBench: NLP Anomaly Detection Benchmark",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "FATE*",
            "name_full": "FATE* (adapted Few-shot Anomaly Detection in Text with Deviation Learning)",
            "brief_description": "An adaptation of a few-shot, deviation-learning end-to-end anomaly detection method trained here in an unsupervised manner (FATE*) to operate without labeled anomalies, learning reference scores from normal data.",
            "citation_title": "Few-shot anomaly detection in text with deviation learning",
            "mention_or_use": "use",
            "model_name": "FATE* (adapted deviation-learning transformer model)",
            "model_type": "transformer-based end-to-end model with deviation learning",
            "model_size": null,
            "data_type": "text (documents, reviews, messages)",
            "data_domain": "news, spam, emotion, reviews",
            "anomaly_type": "semantic outliers (originally few-shot anomalies; adapted to unsupervised detection)",
            "method_description": "Adapt the FATE few-shot deviation-learning model by training exclusively on normal data (FATE*), modifying the framework to learn reference scores and deviations without labeled anomalies to produce anomaly scores end-to-end.",
            "baseline_methods": "End-to-end DATE, CVDD and two-step embedding-based methods (OpenAI/BERT + AD algorithms)",
            "performance_metrics": "AUROC, AUPRC",
            "performance_results": "AUROC examples: AGNews 0.7756, BBCNews 0.9310, EmailSpam 0.9061, Emotion 0.5035, MovieReview 0.5289, N24News 0.8073, SMSSpam 0.6262, YelpReview 0.5945. AUPRC examples: BBCNews 0.5805, EmailSpam 0.5529.",
            "comparison_to_baseline": "FATE* is competitive on certain datasets (e.g., BBCNews) but generally is outperformed by two-step OpenAI-embedding methods on multi-topic datasets; adaptation to unsupervised training may reduce the original few-shot strengths.",
            "limitations_or_failure_cases": "Original few-shot design relies on labeled anomalies; adapting to fully unsupervised training (FATE*) may reduce discriminative power. Like other end-to-end methods, it can struggle with semantically subtle anomalies that benefit from high-capacity external embeddings.",
            "unique_insights": "The adaptation demonstrates that few-shot deviation-learning frameworks can be retooled for unsupervised NLP-AD, but performance trade-offs indicate value in hybridizing with modern embeddings or meta-learning model selection.",
            "uuid": "e9430.5",
            "source_info": {
                "paper_title": "NLP-ADBench: NLP Anomaly Detection Benchmark",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Ad-nlp: A benchmark for anomaly detection in natural language processing",
            "rating": 2
        },
        {
            "paper_title": "Self-attentive, multi-context one-class classification for unsupervised anomaly detection on text",
            "rating": 2
        },
        {
            "paper_title": "Detecting anomalies in text via self-supervision of transformers",
            "rating": 2
        },
        {
            "paper_title": "Few-shot anomaly detection in text with deviation learning",
            "rating": 2
        },
        {
            "paper_title": "Lunar: Unifying local outlier detection methods via graph neural networks",
            "rating": 2
        },
        {
            "paper_title": "New embedding models and api updates",
            "rating": 2
        },
        {
            "paper_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "rating": 1
        },
        {
            "paper_title": "Ad-llm: Benchmarking large language models for anomaly detection",
            "rating": 2
        }
    ],
    "cost": 0.018176249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>NLP-ADBench: NLP Anomaly Detection Benchmark</h1>
<p>Yuangang $\mathbf{L i}^{1, <em>}$, Jiaqi $\mathbf{L i}^{1, </em>}$, Zhuo Xiao ${ }^{1, *}$, Tiankai Yang ${ }^{1}$, Yi Nian ${ }^{1}$, Xiyang $\mathbf{H u}^{2^{\dagger}}$, Yue Zhao ${ }^{1^{\dagger}}$<br>${ }^{\dagger}$ University of Southern California ${ }^{2}$ Arizona State University<br>{yuangang, jli77629, zhuoxiao, tiankaiy, yinian, yzhao010}@usc.edu, xiyanghu@asu.edu<br>${ }^{\dagger}$ Corresponding authors</p>
<h4>Abstract</h4>
<p>Anomaly detection (AD) is an important machine learning task with applications in fraud detection, content moderation, and user behavior analysis. However, AD is relatively understudied in a natural language processing (NLP) context, limiting its effectiveness in detecting harmful content, phishing attempts, and spam reviews. We introduce NLP-ADBench, the most comprehensive NLP anomaly detection (NLP-AD) benchmark to date, which includes eight curated datasets and 19 state-of-the-art algorithms. These span 3 end-toend methods and 16 two-step approaches that adapt classical, non-AD methods to language embeddings from BERT and OpenAI. Our empirical results show that no single model dominates across all datasets, indicating a need for automated model selection. Moreover, two-step methods with transformer-based embeddings consistently outperform specialized end-to-end approaches, with OpenAI embeddings outperforming those of BERT. We release NLP-ADBench at https://github. com/USC-FORTIS/NLP-ADBench, providing a unified framework for NLP-AD and supporting future investigations.</p>
<h2>1 Introduction</h2>
<p>Anomaly detection (AD) is a fundamental area in machine learning with diverse applications in web systems, such as fraud detection, content moderation, and user behavior analysis (Chandola et al., 2009; Ahmed et al., 2016). Substantial progress has been achieved in AD for structured data such as tabular, graph, and time series (Chalapathy and Chawla, 2019; Han et al., 2022; Lai et al., 2021; Liu et al., 2022), but its extension to natural language processing (NLP) remains relatively underexplored (Ruff et al., 2021; Yang et al., 2024). This gap limits our ability to identify harmful content, phishing attempts, and spam reviews.</p>
<p>For instance, detecting abusive or threatening language is crucial for ensuring that social media platforms and online forums remain safe environments for users (Fortuna and Nunes, 2018). Likewise, detecting anomalous product reviews or descriptions in e-commerce is important for preserving user trust and platform credibility (Chino et al., 2017). However, many standard AD methods are designed for numeric or categorical data and are not easily adapted to unstructured text (Zhao et al., 2019; Chen et al., 2024). Existing studies on NLP-specific AD are limited in both dataset variety and algorithmic range (Han et al., 2022; Liu et al., 2022; Yang et al., 2024), leaving open questions about which approaches work best under different conditions. These gaps lead to a central research question: How can we systematically evaluate and compare diverse $A D$ methods across real-world text datasets, and what insights can be gained to guide future development in NLP-based AD?
Our Proposal and Key Contributions. We introduce NLP-ADBench, the most comprehensive benchmark for NLP-AD tasks. NLP-ADBench offers four major benefits compared to prior work (Bejan et al., 2023): (i) eight real-world datasets covering a wide range of web use cases; (ii) 19 advanced methods that apply standard AD algorithms to language embeddings or use end-to-end neural architectures; (iii) detailed empirical findings that highlight new directions for NLP-AD; and (iv) fully open-source resources, including datasets, algorithm implementations, and more, aligns with the Resources and Evaluation track.
Key Insights/Takeaways (see details in §3). Our comprehensive experiments reveal: (i) No single model dominates across all datasets, showing the need for model selection; (ii) Transformer-based embeddings substantially boost two-step AD methods (e.g., LUNAR (Goodge et al., 2022) and LOF (Breunig et al., 2000)) relative to end-to-end approaches; (iii) High-dimensional embeddings (e.g., from OpenAI) improve detection performance, but</p>
<p>also raise computational overhead; and (iv) Datasetspecific biases and human-centered anomaly definitions remain challenging for building robust and widely applicable NLP-AD systems.</p>
<h2>2 NLP-ADBench: AD Benchmark for NLP Tasks</h2>
<h3>2.1 Preliminaries and Problem Definition</h3>
<p>Anomaly Detection in Natural Language Processing (NLP-AD) focuses on identifying text instances that deviate significantly from expected or typical patterns. Unlike structured data, text data is inherently unstructured, high-dimensional, and deeply influenced by the nuances of human language, including syntax, semantics, and context (Aggarwal, 2017; Yang et al., 2024). These unique properties introduce significant challenges, making the development of robust and accurate AD methods for NLP a complex and demanding task.</p>
<p>Formally, let $\mathcal{D}=x_{1}, x_{2}, \ldots, x_{N}$ denote a corpus where each $x_{i}$ is a text instance. The goal of NLP-AD is to learn an anomaly scoring function $f: \mathcal{X} \rightarrow \mathbb{R}$ that assigns a real-valued anomaly score to each text instance. Higher scores denote greater deviations from normal patterns, indicating a higher likelihood of an anomalous instance.</p>
<h3>2.2 Curated Benchmark Datasets</h3>
<p>The limited availability of purpose-built datasets constrains the development and evaluation of effective methods in NLP-AD. To address this gap, we curated and transformed 8 existing classification datasets from various NLP domains into specialized datasets tailored for NLP-AD tasks, ensuring that all data are presented in a standard format. These datasets, collectively called the NLPAD datasets, provide a foundational resource for advancing research.</p>
<p>Each transformed dataset is named by adding the prefix "NLPAD-" to the original dataset's name (e.g., NLPAD-AGNews, NLPAD-BBCNews), distinguishing them from the original datasets. The NLPAD datasets are provided in a unified JSON Lines format for compatibility and ease of use. Each line is a JSON object with four fields: text (the text used for anomaly detection), label (the anomaly detection label, where 1 represents an anomaly and 0 represents normal), original_task (the task of the original dataset), and original_label (the category label from the original dataset).</p>
<p>To transform each dataset for NLP-AD, we established a text selection process based on the data</p>
<p>Table 1: Statistical information of the NLPAD dataset.</p>
<table>
<thead>
<tr>
<th>NLPAD Dataset</th>
<th># Samples</th>
<th>#Normal</th>
<th>#Anomaly</th>
<th>\% Anomaly</th>
</tr>
</thead>
<tbody>
<tr>
<td>NLPAD-AGNews</td>
<td>98,207</td>
<td>94,427</td>
<td>3,780</td>
<td>$3.85 \%$</td>
</tr>
<tr>
<td>NLPAD-BBCNews</td>
<td>1,785</td>
<td>1,723</td>
<td>62</td>
<td>$3.47 \%$</td>
</tr>
<tr>
<td>NLPAD-EmailSpam</td>
<td>3,578</td>
<td>3,432</td>
<td>146</td>
<td>$4.08 \%$</td>
</tr>
<tr>
<td>NLPAD-Emotion</td>
<td>361,980</td>
<td>350,166</td>
<td>11,814</td>
<td>$3.26 \%$</td>
</tr>
<tr>
<td>NLPAD-MovieReview</td>
<td>26,369</td>
<td>24,882</td>
<td>1,487</td>
<td>$5.64 \%$</td>
</tr>
<tr>
<td>NLPAD-N24News</td>
<td>59,822</td>
<td>57,994</td>
<td>1,828</td>
<td>$3.06 \%$</td>
</tr>
<tr>
<td>NLPAD-SMSSpam</td>
<td>4,672</td>
<td>4,518</td>
<td>154</td>
<td>$3.30 \%$</td>
</tr>
<tr>
<td>NLPAD-YelpReview</td>
<td>316,924</td>
<td>298,986</td>
<td>17,938</td>
<td>$5.66 \%$</td>
</tr>
</tbody>
</table>
<p>format. For tabular data, we carefully chose appropriate columns as the text source. For documentbased data, we extracted text directly from relevant documents. The anomalous class for each dataset was selected based on semantic distinctions within the dataset categories, ensuring that the identified anomalies represent meaningful deviations from the normal data distribution (Emmott et al., 2015; Han et al., 2022). Once identified, the anomalous class was downsampled to represent less than $10 \%$ of the total instances.</p>
<p>For instance, in the NLPAD-AGNews dataset (tabular data), we selected the "description" column as the text source, with the "World" category serving as the anomalous class due to its semantic divergence from other categories such as "Sports" or "Technology." This anomalous class was then downsampled accordingly. Similarly, in the NLPAD-BBCNews dataset (documentbased data), text from BBC News documents was used, with the "entertainment" category identified as anomalous because its semantic content significantly differs from other categories like "Politics" or "Business." The "entertainment" category was also downsampled to maintain consistency. This semantic-driven approach to defining anomalies was consistently applied across all datasets. Further details of dataset sources and construction processes can be found in Appx. A.1.1. Table. 1 presents the statistical information of the NLPAD datasets, including the total number of samples, the number of normal and anomalous samples, and the anomaly ratio for each dataset.</p>
<h3>2.3 The Most Comprehensive NLP-AD Algorithms with Open Implementations</h3>
<p>Compared to the existing NLP-AD benchmark by Bejan et al. (Bejan et al., 2023), NLP-ADBench provides a broader evaluation by including 19 algorithms, categorized into two groups. The first group comprises 3 end-to-end algorithms that directly process raw text data to produce anomaly detection outcomes. The second group consists of 16 algorithms derived by applying 8 traditional anomaly detection (AD) methods to text embeddings generated</p>
<p>from two models: bert-base-uncased (Devlin et al., 2019) and OpenAI's text-embedding-3-large (OpenAI, 2024). These traditional AD methods do not operate on raw text directly but instead perform anomaly detection on embeddings, offering a complementary approach to the end-to-end methods. This comprehensive algorithm collection enables a robust evaluation of direct and embedding-based NLP anomaly detection techniques. Here, we provide a brief description; see details in Appx. A.2. End-to-end NLP-AD Algorithms. We evaluate 3 end-to-end algorithms tailored for NLP-AD. (1) Context Vector Data Description (CVDD) (Ruff et al., 2019) leverages context vectors and pre-trained embeddings with a multi-head self-attention mechanism to project normal instances close to learned contexts, identifying anomalies based on deviations. (2) Detecting Anomalies in Text via Self-Supervision of Transformers (DATE) (Manolache et al., 2021) trains transformers using self-supervised tasks like replaced mask detection to capture normal text patterns and flag anomalies. (3) Few-shot Anomaly Detection in Text with Deviation Learning (FATE) (Das et al., 2023) uses a few labeled anomalies with deviation learning to distinguish anomalies from normal instances. We adapt it to train solely on normal data, referring to the adapted version as FATE*.
Two-step NLP-AD Algorithms. We evaluate 8 twostep algorithms that rely on embeddings generated by models such as bert-base-uncased (Devlin et al., 2019) and text-embedding-3-large (OpenAI, 2024). These algorithms are designed to work with structured numerical data and cannot directly process raw textual data, requiring text transformation into numerical embeddings. (4) LOF (Breunig et al., 2000) measures local density deviations, while (5) DeepSVDD (Ruff et al., 2018) minimizes the volume of a hypersphere enclosing normal representations. (6) ECOD (Li et al., 2022) uses empirical cumulative distribution functions to estimate densities and assumes anomalies lie in distribution tails. (7) IForest (Liu et al., 2008) recursively isolates anomalies through random splits, and (8) SO_GAAL (Liu et al., 2019) generates adversarial samples to identify anomalies. Reconstructionbased approaches include (9) AE (Aggarwal, 2017), which flags anomalies based on reconstruction errors, and (10) VAE (Kingma and Welling, 2013; Burgess et al., 2018), which identifies anomalies using reconstruction probabilities or latent deviations. Finally, (11) LUNAR (Goodge et al., 2022)
enhances traditional local outlier detection with graph neural networks.</p>
<h2>3 Experiment Results</h2>
<h3>3.1 Experiment Setting</h3>
<p>Datasets, Train/Test Data Split, and Independent Trials. In the NLP-ADBench benchmark, the data is divided by allocating $70 \%$ of the normal data to the training set. The remaining $30 \%$ of normal data, combined with all anomalous data, forms the test set. To ensure the robustness of our findings, we repeat each experiment three times and report the average performance.
Hyperparameter Settings. For all the algorithms in NLP-ADBench, we use their default hyperparameter (HP) settings in the original paper for a fair comparison, same as ADBench (Han et al., 2022). Evaluation Metrics and Statistical Tests. We evaluate different NLP-AD methods by a widely used metric: AUROC (Area Under Receiver Operating Characteristic Curve) and AUPRC (Area Under Precision-Recall Curve) value.</p>
<h2>Embeddings Definitions:</h2>
<ol>
<li>BERT refers specifically to the bert-baseuncased model (Devlin et al., 2019).</li>
<li>OpenAI refers to OpenAI's text-embedding-3large model (OpenAI, 2024).</li>
<li>The term "BERT + AD algorithm" or "OpenAI + AD algorithm" means that we first generate text embeddings using BERT or OpenAI's model, respectively, and then apply the AD algorithm.</li>
</ol>
<h3>3.2 Results, Discussions, and New Directions</h3>
<p>We analyze the AUROC results presented in Table 2 and the average rank summary in Figure 1. For completeness, AUPRC scores and their corresponding average ranks are reported in Appendix A.3.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Average rank on AUROC of 19 NLPAD methods across 8 datasets (the lower the better).
No single model consistently excels across all datasets due to variability in dataset characteristics. AD model performance varies significantly across datasets, complicating the selection of a universally optimal model. For datasets with more categories (e.g., NLPAD-AGNews), two-step methods like OpenAI + LUNAR (0.9226) outperform end-to-end methods such as CVDD (0.6046) by</p>
<p>Table 2: Performance comparison of 19 Algorithms on 8 NLPAD datasets using AUROC, with best results highlighted in bold and shaded</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">NLPAD- <br> AGNews</th>
<th style="text-align: center;">NLPAD- <br> BBCNews</th>
<th style="text-align: center;">NLPAD- <br> EmailSpam</th>
<th style="text-align: center;">NLPAD- <br> Emotion</th>
<th style="text-align: center;">NLPAD- <br> MovieReview</th>
<th style="text-align: center;">NLPAD- <br> N24News</th>
<th style="text-align: center;">NLPAD- <br> SMSSpam</th>
<th style="text-align: center;">NLPAD- <br> YelpReview</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CVDD</td>
<td style="text-align: center;">0.6046</td>
<td style="text-align: center;">0.7221</td>
<td style="text-align: center;">0.9340</td>
<td style="text-align: center;">0.4867</td>
<td style="text-align: center;">0.4895</td>
<td style="text-align: center;">0.7507</td>
<td style="text-align: center;">0.4782</td>
<td style="text-align: center;">0.5345</td>
</tr>
<tr>
<td style="text-align: left;">DATE</td>
<td style="text-align: center;">0.8120</td>
<td style="text-align: center;">0.9030</td>
<td style="text-align: center;">$\mathbf{0 . 9 6 9 7}$</td>
<td style="text-align: center;">0.6291</td>
<td style="text-align: center;">0.5185</td>
<td style="text-align: center;">0.7493</td>
<td style="text-align: center;">$\mathbf{0 . 9 3 9 8}$</td>
<td style="text-align: center;">0.6092</td>
</tr>
<tr>
<td style="text-align: left;">FATE*</td>
<td style="text-align: center;">0.7756</td>
<td style="text-align: center;">0.9310</td>
<td style="text-align: center;">0.9061</td>
<td style="text-align: center;">0.5035</td>
<td style="text-align: center;">0.5289</td>
<td style="text-align: center;">0.8073</td>
<td style="text-align: center;">0.6262</td>
<td style="text-align: center;">0.5945</td>
</tr>
<tr>
<td style="text-align: left;">BERT + LOF</td>
<td style="text-align: center;">0.7432</td>
<td style="text-align: center;">0.9320</td>
<td style="text-align: center;">0.7482</td>
<td style="text-align: center;">0.5435</td>
<td style="text-align: center;">0.4959</td>
<td style="text-align: center;">0.6703</td>
<td style="text-align: center;">0.7190</td>
<td style="text-align: center;">0.6573</td>
</tr>
<tr>
<td style="text-align: left;">BERT + DeepSVDD</td>
<td style="text-align: center;">0.6671</td>
<td style="text-align: center;">0.5683</td>
<td style="text-align: center;">0.6937</td>
<td style="text-align: center;">0.5142</td>
<td style="text-align: center;">0.4287</td>
<td style="text-align: center;">0.4366</td>
<td style="text-align: center;">0.5859</td>
<td style="text-align: center;">0.5871</td>
</tr>
<tr>
<td style="text-align: left;">BERT + ECOD</td>
<td style="text-align: center;">0.6318</td>
<td style="text-align: center;">0.6912</td>
<td style="text-align: center;">0.7052</td>
<td style="text-align: center;">0.5889</td>
<td style="text-align: center;">0.4282</td>
<td style="text-align: center;">0.4969</td>
<td style="text-align: center;">0.5606</td>
<td style="text-align: center;">0.6326</td>
</tr>
<tr>
<td style="text-align: left;">BERT + iForest</td>
<td style="text-align: center;">0.6124</td>
<td style="text-align: center;">0.6847</td>
<td style="text-align: center;">0.6779</td>
<td style="text-align: center;">0.4944</td>
<td style="text-align: center;">0.4420</td>
<td style="text-align: center;">0.4724</td>
<td style="text-align: center;">0.5053</td>
<td style="text-align: center;">0.5971</td>
</tr>
<tr>
<td style="text-align: left;">BERT + SO-GAAL</td>
<td style="text-align: center;">0.4489</td>
<td style="text-align: center;">0.3099</td>
<td style="text-align: center;">0.4440</td>
<td style="text-align: center;">0.5031</td>
<td style="text-align: center;">0.4663</td>
<td style="text-align: center;">0.4135</td>
<td style="text-align: center;">0.3328</td>
<td style="text-align: center;">0.4712</td>
</tr>
<tr>
<td style="text-align: left;">BERT + AE</td>
<td style="text-align: center;">0.7200</td>
<td style="text-align: center;">0.8839</td>
<td style="text-align: center;">0.4739</td>
<td style="text-align: center;">0.5594</td>
<td style="text-align: center;">0.4650</td>
<td style="text-align: center;">0.5749</td>
<td style="text-align: center;">0.6918</td>
<td style="text-align: center;">0.6441</td>
</tr>
<tr>
<td style="text-align: left;">BERT + VAE</td>
<td style="text-align: center;">0.6773</td>
<td style="text-align: center;">0.7409</td>
<td style="text-align: center;">0.4737</td>
<td style="text-align: center;">0.5594</td>
<td style="text-align: center;">0.4398</td>
<td style="text-align: center;">0.4949</td>
<td style="text-align: center;">0.6082</td>
<td style="text-align: center;">0.6441</td>
</tr>
<tr>
<td style="text-align: left;">BERT + LUNAR</td>
<td style="text-align: center;">0.7694</td>
<td style="text-align: center;">0.9260</td>
<td style="text-align: center;">0.8417</td>
<td style="text-align: center;">0.5186</td>
<td style="text-align: center;">0.4687</td>
<td style="text-align: center;">0.6284</td>
<td style="text-align: center;">0.6953</td>
<td style="text-align: center;">0.6522</td>
</tr>
<tr>
<td style="text-align: left;">OpenAI + LOF</td>
<td style="text-align: center;">0.8905</td>
<td style="text-align: center;">0.9558</td>
<td style="text-align: center;">0.9263</td>
<td style="text-align: center;">0.7304</td>
<td style="text-align: center;">0.6156</td>
<td style="text-align: center;">0.7806</td>
<td style="text-align: center;">0.7862</td>
<td style="text-align: center;">0.8733</td>
</tr>
<tr>
<td style="text-align: left;">OpenAI + DeepSVDD</td>
<td style="text-align: center;">0.4680</td>
<td style="text-align: center;">0.5766</td>
<td style="text-align: center;">0.4415</td>
<td style="text-align: center;">0.4816</td>
<td style="text-align: center;">0.6563</td>
<td style="text-align: center;">0.6150</td>
<td style="text-align: center;">0.3491</td>
<td style="text-align: center;">0.5373</td>
</tr>
<tr>
<td style="text-align: left;">OpenAI + ECOD</td>
<td style="text-align: center;">0.7638</td>
<td style="text-align: center;">0.7224</td>
<td style="text-align: center;">0.9263</td>
<td style="text-align: center;">0.6206</td>
<td style="text-align: center;">$\mathbf{0 . 7 3 6 6}$</td>
<td style="text-align: center;">0.7342</td>
<td style="text-align: center;">0.4317</td>
<td style="text-align: center;">0.5984</td>
</tr>
<tr>
<td style="text-align: left;">OpenAI + iForest</td>
<td style="text-align: center;">0.5213</td>
<td style="text-align: center;">0.6064</td>
<td style="text-align: center;">0.6937</td>
<td style="text-align: center;">0.5889</td>
<td style="text-align: center;">0.5064</td>
<td style="text-align: center;">0.4944</td>
<td style="text-align: center;">0.3751</td>
<td style="text-align: center;">0.5871</td>
</tr>
<tr>
<td style="text-align: left;">OpenAI + SO-GAAL</td>
<td style="text-align: center;">0.5945</td>
<td style="text-align: center;">0.2359</td>
<td style="text-align: center;">0.4440</td>
<td style="text-align: center;">0.5031</td>
<td style="text-align: center;">0.6201</td>
<td style="text-align: center;">0.5043</td>
<td style="text-align: center;">0.5671</td>
<td style="text-align: center;">0.5082</td>
</tr>
<tr>
<td style="text-align: left;">OpenAI + AE</td>
<td style="text-align: center;">0.8326</td>
<td style="text-align: center;">0.9520</td>
<td style="text-align: center;">0.7651</td>
<td style="text-align: center;">0.7067</td>
<td style="text-align: center;">0.6088</td>
<td style="text-align: center;">0.7155</td>
<td style="text-align: center;">0.5511</td>
<td style="text-align: center;">0.8524</td>
</tr>
<tr>
<td style="text-align: left;">OpenAI + VAE</td>
<td style="text-align: center;">0.8144</td>
<td style="text-align: center;">0.7250</td>
<td style="text-align: center;">0.5273</td>
<td style="text-align: center;">0.7067</td>
<td style="text-align: center;">0.4515</td>
<td style="text-align: center;">0.7418</td>
<td style="text-align: center;">0.4259</td>
<td style="text-align: center;">0.6163</td>
</tr>
<tr>
<td style="text-align: left;">OpenAI + LUNAR</td>
<td style="text-align: center;">$\mathbf{0 . 9 2 2 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 7 3 2}$</td>
<td style="text-align: center;">0.9343</td>
<td style="text-align: center;">$\mathbf{0 . 9 3 2 8}$</td>
<td style="text-align: center;">0.6474</td>
<td style="text-align: center;">$\mathbf{0 . 8 3 2 0}$</td>
<td style="text-align: center;">0.7189</td>
<td style="text-align: center;">$\mathbf{0 . 9 4 5 2}$</td>
</tr>
</tbody>
</table>
<p>52.6\%. Similarly, on NLPAD-BBCNews, OpenAI + LOF (0.9558) surpasses CVDD (0.7221) by $\mathbf{3 2 . 4 \%}$. Conversely, on the binary-class datasets (e.g., NLPAD-SMSSpam), end-to-end methods perform better, with DATE (0.9398) clearly exceeding OpenAI + LUNAR (0.7189) by $\mathbf{3 0 . 7 \%}$.</p>
<ul>
<li>Future Direction 1: Automated Model Selection. These results emphasize the importance of developing automated approaches to select the most suitable model. One feasible solution will be adapting the meta-learning framework from tabular AD settings (Zhao et al., 2021) to NLP-AD.
Transformer-based embeddings boost the performance of two-step AD methods. Two-step AD algorithms paired with transformer-based embeddings have consistently outperformed end-toend methods in NLP-AD tasks. For instance, OpenAI + LUNAR achieves $\mathbf{0 . 9 2 2 6}$ on NLPADAGNews, surpassing CVDD by $\mathbf{5 2 . 6 \%}$ and FATE<em> by $\mathbf{1 9 . 0 \%}$. Similarly, OpenAI + LOF reaches $\mathbf{0 . 9 5 5 8}$ on NLPAD-BBCNews, exceeding CVDD by $\mathbf{3 2 . 4 \%}$ and FATE</em> by $\mathbf{2 . 7 \%}$. This advantage arises primarily because two-step methods leverage superior contextual embeddings from modern transformer models (e.g., OpenAI), whereas end-toend methods like CVDD rely on older embeddings (e.g., GloVe). This highlights the need for end-toend methods to adopt more advanced embeddings to enhance performance.</li>
<li>Future Direction 2: Transformer Embedding Integration for End-to-End AD. Future end-to-end methods should adopt transformer-based embeddings over static embeddings like GloVe. Re- search should focus on embedding integration optimized for end-to-end AD frameworks.
High-dimensional embeddings enhance detection but require balancing performance and efficiency. Embedding dimensionality significantly impacts both performance and computational efficiency in AD tasks. Compared to BERTbase embeddings ( 768 dimensions), OpenAI's text-embedding-3-large embeddings ( 3072 dimensions, a $\mathbf{3 0 0 \%}$ increase) consistently achieve superior results across multiple datasets in NLP-ADBench. Specifically, OpenAI + LUNAR achieves $\mathbf{0 . 9 4 5 2}$ on NLPAD-YelpReview (outperforming BERT + LUNAR's $\mathbf{0 . 6 5 2 2}$ by $\mathbf{4 4 . 9 \%}$ ), $\mathbf{0 . 9 2 2 6}$ on NLPADAGNews (exceeding BERT + LUNAR's $\mathbf{0 . 7 6 9 4}$ by $\mathbf{1 9 . 9 \%}$ ), and $\mathbf{0 . 8 3 2 0}$ on NLPAD-N24News (surpassing BERT + LUNAR's $\mathbf{0 . 6 2 8 4}$ by $\mathbf{3 2 . 4 \%}$ ). These results clearly demonstrate the advantage of higher-dimensional embeddings for enhancing AD performance. However, higher dimensionality also introduces greater computational costs and potential information redundancy.</li>
<li>Future Direction 3: Optimizing Embedding Dimensionality. Future research should explore NLP-AD-specific dimensionality reduction techniques to reduce redundancy and computational costs without compromising performance. Additionally, adaptive methods that dynamically adjust dimensionality based on dataset characteristics could enhance scalability and efficiency.
3.3 In-depth Analysis of Key Findings
3.3.1 Explaining Method-Dataset Fit</li>
</ul>
<p>To understand why certain models outperform others on specific datasets, we conduct both quanti-</p>
<p>tative and qualitative analyses to identify datasetlevel factors influencing model performance.</p>
<h2>Quantitative Corpus-Level Linguistic Analysis</h2>
<p>We characterize each dataset using three linguistic indicators: (1) Avg-Len, the average number of BERT tokens per sample (text complexity); (2) Lexical-Burstiness, the proportion of top20 anomaly-specific tokens from BERT-tokenized TF-IDF (higher values indicate stronger lexical anomaly signals); (3) Topic-Diversity, the Shannon entropy over the label distribution of normalclass examples ( 0 for binary datasets; higher values indicate broader topic coverage).</p>
<p>Table 3: Dataset characteristics and best method</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Avg-Len</th>
<th>Lexical-Burst.</th>
<th>Topic-Div.</th>
<th>Best Method</th>
</tr>
</thead>
<tbody>
<tr>
<td>NLPAD-AGNews</td>
<td>39.5</td>
<td>0.040</td>
<td>1.583</td>
<td>OpenAI+LUNAR (two-step)</td>
</tr>
<tr>
<td>NLPAD-BRCNews</td>
<td>481.7</td>
<td>0.028</td>
<td>1.981</td>
<td>OpenAI+LUNAR (two-step)</td>
</tr>
<tr>
<td>NLPAD-EmailSpam</td>
<td>238.4</td>
<td>0.024</td>
<td>0.000</td>
<td>DATE (end-to-end)</td>
</tr>
<tr>
<td>NLPAD-Emission</td>
<td>20.3</td>
<td>0.087</td>
<td>1.949</td>
<td>OpenAI+LUNAR (two-step)</td>
</tr>
<tr>
<td>NLPAD-MovieReview</td>
<td>290.3</td>
<td>0.026</td>
<td>0.000</td>
<td>OpenAI+ECOD (two-step)</td>
</tr>
<tr>
<td>NLPAD-N230sews</td>
<td>1034.2</td>
<td>0.016</td>
<td>4.437</td>
<td>OpenAI+LUNAR (two-step)</td>
</tr>
<tr>
<td>NLPAD-SMSSpam</td>
<td>20.5</td>
<td>0.063</td>
<td>0.000</td>
<td>DATE (end-to-end)</td>
</tr>
<tr>
<td>NLPAD-YelpReview</td>
<td>151.6</td>
<td>0.023</td>
<td>0.000</td>
<td>OpenAI+LUNAR (two-step)</td>
</tr>
</tbody>
</table>
<p>From Table. 3, we get two findings: (1) Datasets with short texts, high lexical-burstiness, and explicit lexical markers (e.g., NLPAD-SMSSpam) tend to favor DATE's token-level anomaly detection. (2) Datasets with moderate length, lower lexical-burstiness, and high topical diversity (e.g., NLPAD-AGNews) benefit from richer, contextsensitive embeddings (OpenAI+LUNAR).</p>
<h2>Qualitative Analysis of Representative Cases</h2>
<p>To further investigate the model performance gap, we qualitatively examined anomaly examples from NLPAD-SMSSpam and NLPAD-AGNews (see Table 4). In NLPAD-SMSSpam, anomalies often contain explicit lexical irregularities, such as numeric tokens, unconventional formatting, or urgencyinducing phrases. These surface-level features align closely with DATE's self-supervised scoring mechanism, which is sensitive to token-level deviations. By contrast, anomalies in NLPAD-AGNews exhibit subtle semantic shifts without distinctive lexical markers. Detecting such anomalies requires a deeper understanding of contextual semantics, which exceeds DATE's capacity and favors twostep methods with embedding-based models such as OpenAI and LUNAR.</p>
<h3>3.3.2 Performance-Efficiency Trade-offs in Embedding Dimensionality</h3>
<p>While OpenAI-based two-step methods achieve high anomaly detection performance, their high dimensionality raises concerns about computational and financial cost in deployment scenarios. To explore whether such overhead is justified, we con-</p>
<p>Table 4: Representative anomaly examples from NLPAD-SMSSpam and NLPAD-AGNews</p>
<table>
<thead>
<tr>
<th>NLPAD-SMSSpam</th>
<th>NLPAD-AGNews</th>
</tr>
</thead>
<tbody>
<tr>
<td>PRIVATE! Your 2003 Account Statement for shows 800 un</td>
<td>NEW YORK U.S. stocks are</td>
</tr>
<tr>
<td>redeemed S. I. M. points. Call 08715203694 Identifier Code</td>
<td>expected to open modestly higher</td>
</tr>
<tr>
<td>40533 Express 31/10/04</td>
<td>Tuesday as investors use a slight let</td>
</tr>
<tr>
<td>FREE for 1st week! No1 Nokia tone 4 at each every week just</td>
<td>up to the rise in oil prices to add to</td>
</tr>
<tr>
<td>no NOKIA to 87077 Get tiring and tell at mates, zeal PUBex</td>
<td>portfolios...</td>
</tr>
<tr>
<td>36504 WiFWQ notes150phase 16</td>
<td></td>
</tr>
<tr>
<td>Urgent! Please call 09061213237 from landline. 5000 cash or</td>
<td>AFP Italian shares, Asia's second</td>
</tr>
<tr>
<td>a luxury 4 Canary Islands Holiday await collection. T Cs SAE</td>
<td>top performers last year, are poised</td>
</tr>
<tr>
<td>PO Box 177. M227XY. 150ppm. 16</td>
<td>for long term gains as foreign</td>
</tr>
<tr>
<td>XXXX64tdeMoveClub! To use your credit, click the WAP</td>
<td>investors buy into the market, seeing</td>
</tr>
<tr>
<td>link in the next ter message or click here: xxxxxshile-</td>
<td>the country as an economic "growth</td>
</tr>
<tr>
<td>movieclub.com/tv:Q8KGIGHZIGCBL</td>
<td>story," according to analysts.</td>
</tr>
</tbody>
</table>
<p>duct dimensionality reduction experiments using PCA, projecting OpenAI embeddings to 768 di-mensions-the same as BERT. Table 5: Performance-efficiency trade-offs of dimensionality reduction on datasets</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Embedding Data</th>
<th>Performance</th>
<th></th>
<th></th>
<th>Business (example)</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>0.003C</td>
<td>0.056C</td>
<td>Total</td>
<td>Embedding</td>
<td>PCA</td>
<td>Inference</td>
<td></td>
</tr>
<tr>
<td>NLPAD-AGNews</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>OpenAI + LUNAR (orig.)</td>
<td>3072</td>
<td>0.987</td>
<td>0.842</td>
<td>0.577</td>
<td>0.307</td>
<td>0.000</td>
<td>0.110</td>
<td></td>
</tr>
<tr>
<td>OpenAI + LUNAR (PCA)</td>
<td>768</td>
<td>0.890</td>
<td>0.546</td>
<td>0.400</td>
<td>0.252</td>
<td>0.007</td>
<td>0.156</td>
<td></td>
</tr>
<tr>
<td>BERT + LUNAR</td>
<td>768</td>
<td>0.790</td>
<td>0.325</td>
<td>0.481</td>
<td>0.056</td>
<td>0.000</td>
<td>0.025</td>
<td></td>
</tr>
<tr>
<td>OpenAI + L1R (orig.)</td>
<td>3072</td>
<td>0.896</td>
<td>0.575</td>
<td>0.314</td>
<td>0.239</td>
<td>0.000</td>
<td>0.075</td>
<td></td>
</tr>
<tr>
<td>OpenAI + L1R (PCA)</td>
<td>768</td>
<td>0.798</td>
<td>0.321</td>
<td>0.433</td>
<td>0.245</td>
<td>0.007</td>
<td>0.185</td>
<td></td>
</tr>
<tr>
<td>BERT + L1R</td>
<td>768</td>
<td>0.771</td>
<td>0.304</td>
<td>0.070</td>
<td>0.062</td>
<td>0.000</td>
<td>0.014</td>
<td></td>
</tr>
<tr>
<td>NLPAD-MovieReview</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>OpenAI + LUNAR (orig.)</td>
<td>3072</td>
<td>0.664</td>
<td>0.238</td>
<td>0.322</td>
<td>0.270</td>
<td>0.000</td>
<td>0.052</td>
<td></td>
</tr>
<tr>
<td>OpenAI + LUNAR (PCA)</td>
<td>768</td>
<td>0.681</td>
<td>0.249</td>
<td>0.409</td>
<td>0.270</td>
<td>0.007</td>
<td>0.132</td>
<td></td>
</tr>
<tr>
<td>BERT + LUNAR</td>
<td>768</td>
<td>0.467</td>
<td>0.152</td>
<td>0.071</td>
<td>0.060</td>
<td>0.000</td>
<td>0.010</td>
<td></td>
</tr>
<tr>
<td>OpenAI + L1R (orig.)</td>
<td>3072</td>
<td>0.653</td>
<td>0.242</td>
<td>0.314</td>
<td>0.274</td>
<td>0.000</td>
<td>0.041</td>
<td></td>
</tr>
<tr>
<td>OpenAI + L1R (PCA)</td>
<td>768</td>
<td>0.610</td>
<td>0.226</td>
<td>0.364</td>
<td>0.256</td>
<td>0.007</td>
<td>0.121</td>
<td></td>
</tr>
<tr>
<td>BERT + L1R</td>
<td>768</td>
<td>0.499</td>
<td>0.166</td>
<td>0.060</td>
<td>0.060</td>
<td>0.000</td>
<td>0.009</td>
<td></td>
</tr>
</tbody>
</table>
<p>As shown in Table 5, PCA-reduced OpenAI embeddings slightly change performance (e.g., AUROC drops from 0.907 to 0.890 on NLPADAGNews with LUNAR) and consistently outperform BERT at the same dimension. However, PCA increases total runtime because it compresses embeddings into a denser space, which complicates decision boundaries and slows down inference.</p>
<p>This finding reinforces Future Direction 3, highlighting the need for NLP-AD-specific dimensionality reduction techniques that balance representation quality with computational efficiency.</p>
<h2>4 Conclusion</h2>
<p>We present NLP-ADBench, the most comprehensive benchmark for contextual NLP anomaly detection (NLP-AD), evaluating 19 state-of-the-art algorithms across 8 diverse datasets. Our findings establish the superiority of two-step methods leveraging transformer-based embeddings, such as OpenAI + LUNAR, over end-to-end approaches, demonstrating the power of hybrid strategies for handling complex NLP anomaly detection tasks. By combining advanced text embeddings with traditional anomaly detection methods, NLP-ADBench provides a robust and flexible framework that sets a new standard for evaluating NLP-AD systems. Additionally, we offer actionable insights into model performance, dataset variability, and embedding utilization, paving the way for future research.</p>
<h2>Limitations</h2>
<p>Despite its contributions, NLP-ADBench has certain limitations. First, the datasets included in the benchmark, while diverse, are primarily sourced from existing classification tasks and may not fully reflect emerging challenges such as anomalies in multilingual or multimodal text data. Second, our evaluations focus on static embeddings, leaving dynamic or streaming NLP-AD scenarios unexplored. Third, the reliance on predefined anomaly labels in our benchmark limits the ability to assess unsupervised or domain-adaptive approaches. Future work can expand NLP-ADBench to include more diverse datasets, such as multilingual or multimodal data, and by exploring dynamic anomaly detection in streaming text scenarios. Incorporating benchmarks for unsupervised and adaptive models can also better reflect real-world applications. These advancements will enhance NLP-ADBench's utility as a comprehensive platform for driving progress in NLP anomaly detection.</p>
<h2>Ethics Statement</h2>
<p>This work adheres to ethical standards emphasizing transparency, fairness, and privacy in NLP anomaly detection research. By openly sharing datasets, algorithms, and experimental results, NLP-ADBench provides a standardized foundation for advancing safer and more reliable web-based systems. All datasets are publicly available and contain no personally identifiable information, ensuring privacy compliance. Pre-trained embeddings (such as OpenAI's text-embedding-3-large) are used in accordance with their terms of service. Additionally, we used ChatGPT exclusively to improve minor grammar in the final manuscript text.</p>
<h2>Broader Impacts</h2>
<p>The NLP-ADBench proposed in this paper provides a comprehensive benchmark framework for anomaly detection in NLP. By standardizing datasets and algorithms, this work supports advancements in critical web-based applications, including fraud detection, spam filtering, and content moderation. The benchmark promotes transparency, reproducibility, and facilitates further innovations, ultimately contributing to safer, more reliable online environments.</p>
<h2>Acknowledgments</h2>
<p>This work was partially supported by the National Science Foundation under Award Nos. 2428039,</p>
<p>2346158, and 2449280. We also acknowledge the use of computational resources provided by the Advanced Cyberinfrastructure Coordination Ecosystem (Boerner et al., 2023): Services \&amp; Support (ACCESS) program, supported by NSF grants #2138259, #2138286, #2138307, #2137603, and #2138296. Specifically, this work used NCSA Delta GPU at the National Center for Supercomputing Applications (NCSA) through allocation CIS250073. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. The authors also gratefully acknowledge support from the Amazon Research Awards and Capital One Research Awards.</p>
<h2>References</h2>
<p>Charu C. Aggarwal. 2017. Outlier Analysis, 2nd edition. Springer.</p>
<p>Mohiuddin Ahmed, Abdun Naser Mahmood, and Jiankun Hu. 2016. A survey of network anomaly detection techniques. J. Netw. Comput. Appl., 60:1931.</p>
<p>Tiago A. Almeida, José María G. Hidalgo, and Akebo Yamakami. 2011. Contributions to the study of sms spam filtering: new collection and results. In ACM Symposium on Document Engineering, page 259-262.</p>
<p>Matei Bejan, Andrei Manolache, and Marius Popescu. 2023. Ad-nlp: A benchmark for anomaly detection in natural language processing. In EMNLP, pages 10766-10778.
{Timothy J.} Boerner, Stephen Deems, {Thomas R.} Furlani, {Shelley L.} Knuth, and John Towns. 2023. Access: Advancing innovation: Nsf's advanced cyberinfrastructure coordination ecosystem: Services \&amp; support. In PEARC 2023 - Computing for the common good, PEARC 2023 - Computing for the common good: Practice and Experience in Advanced Research Computing, pages 173-176, United States. Association for Computing Machinery. Publisher Copyright: © 2023 Owner/Author.; 2023 Practice and Experience in Advanced Research Computing, PEARC 2023 ; Conference date: 23-07-2023 Through 27-072023.</p>
<p>Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and Jörg Sander. 2000. Lof: identifying densitybased local outliers. In SIGMOD, pages 93-104.</p>
<p>Christopher P Burgess, Irina Higgins, Loic Matthey Pal, and Alexander Lerchner. 2018. Understanding disentangling in $\beta$-vae. arXiv:1804.03599.</p>
<p>Christine P. Chai. 2022. Comparison of text preprocessing methods. Nat. Lang. Eng., 29:509-553.</p>
<p>Raghavendra Chalapathy and Sanjay Chawla. 2019. Deep learning for anomaly detection: A survey. arXiv:1901.03407.</p>
<p>Varun Chandola, Arindam Banerjee, and Vipin Kumar. 2009. Anomaly detection: A survey. CSUR, 41(3):158 .</p>
<p>Sihan Chen, Zhuangzhuang Qian, Wingchun Siu, Xingcan Hu, Jiaqi Li, Shawn Li, Yuehan Qin, Tiankai Yang, Zhuo Xiao, Wanghao Ye, and others. 2024. PyOD 2: A Python Library for Outlier Detection with LLM-powered Model Selection. In International World Wide Web Conference (TheWebConf Demo Track).</p>
<p>Daniel YT Chino, Alceu F Costa, Agma JM Traina, and Christos Faloutsos. 2017. Voltime: Unsupervised anomaly detection on users' online activity volume. In Proceedings of the 2017 SIAM International Conference on Data Mining, pages 108-116. SIAM.</p>
<p>Anindya Sundar Das, Aravind Ajay, Sriparna Saha, and Monowar Bhuyan. 2023. Few-shot anomaly detection in text with deviation learning. arXiv:2308.11780.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL, pages 4171-4186.</p>
<p>Andrew Emmott, Shubhomoy Das, Thomas Dietterich, and 1 others. 2015. A meta-analysis of the anomaly detection problem. arXiv:1503.01158.</p>
<p>Paula Fortuna and Sérgio Nunes. 2018. A survey on automatic detection of hate speech in text. CSUR, 51(4):1-30.</p>
<p>Adam Goodge, Bryan Hooi, See-Kiong Ng, and Wee Siong Ng. 2022. Lunar: Unifying local outlier detection methods via graph neural networks. In $A A A I$, volume 36, pages 6737-6745.</p>
<p>Derek Greene and Pádraig Cunningham. 2006. Practical solutions to the problem of diagonal dominance in kernel document clustering. In ICML, pages 377384 .</p>
<p>Songqiao Han, Xiyang Hu, and 1 others. 2022. Adbench: Anomaly detection benchmark. NeurIPS, 35:32142-32159.</p>
<p>Diederik P Kingma and Max Welling. 2013. Autoencoding variational bayes. arXiv:1312.6114.</p>
<p>Kwei-Herng Lai, Daochen Zha, Junjie Xu, Yue Zhao, and 1 others. 2021. Revisiting time series outlier detection: Definitions and benchmarks. In NeurIPS.</p>
<p>Zheng Li, Yue Zhao, and 1 others. 2022. Ecod: Unsupervised outlier detection using empirical cumulative distribution functions. TKDE, 35(12):12181-12193.</p>
<p>Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. 2008. Isolation forest. In ICDM, pages 413-422.</p>
<p>Kay Liu, Yingtong Dou, Yue Zhao, and 1 others. 2022. Bond: Benchmarking unsupervised outlier node detection on static attributed graphs. NeurIPS, 35:27021-27035.</p>
<p>Yezheng Liu, Zhe Li, Chong Zhou, Yuanchun Jiang, Jianshan Sun, Meng Wang, and Xiangnan He. 2019. Generative adversarial active learning for unsupervised outlier detection. IEEE Transactions on Knowledge and Data Engineering.</p>
<p>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In $A C L$, pages $142-150$.</p>
<p>Andrei Manolache, Florin Brad, and Elena Burceanu. 2021. Date: Detecting anomalies in text via selfsupervision of transformers. arXiv:2104.05591.</p>
<p>Vangelis Metsis, Ion Androutsopoulos, and Georgios Paliouras. 2006. Spam filtering with naive bayeswhich naive bayes? In CEAS, volume 17, pages 28-69.</p>
<p>OpenAI. 2024. New embedding models and api updates.</p>
<p>Ilham Fadillah Putra. 2023. Yelp review dataset. https://www.kaggle.com/datasets/ ilhamfp31/yelp-review-dataset. Accessed: 2024-11-28.</p>
<p>Aman Anand Rai. 2023. Ag news classification dataset. https://www. kaggle.com/datasets/amananandrai/ ag-news-classification-dataset. Accessed: 2024-11-28.</p>
<p>Lukas Ruff, Jacob R Kauffmann, Robert A Vandermeulen, Grégoire Montavon, Wojciech Samek, Marius Kloft, Klaus-Robert Müller, and Geoff Orr. 2021. A unifying review of deep and shallow anomaly detection. Proc. IEEE, 109(5):756-795.</p>
<p>Lukas Ruff, Robert Vandermeulen, Nico Goernitz, and 1 others. 2018. Deep one-class classification. In ICML, pages 4393-4402. PMLR.</p>
<p>Lukas Ruff, Yury Zemlyanskiy, Robert Vandermeulen, and 1 others. 2019. Self-attentive, multi-context oneclass classification for unsupervised anomaly detection on text. In $A C L$, pages 4061-4071.</p>
<p>Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang, Junlin Wu, and Yi-Shin Chen. 2018. CARER: Contextualized affect representations for emotion recognition. In EMNLP, pages 3687-3697.</p>
<p>Zhen Wang, Xu Shan, Xiangxie Zhang, and Jie Yang. 2022. N24News: A new dataset for multimodal news classification. In LREC, pages 6768-6775.</p>
<p>Tiankai Yang, Yi Nian, Shawn Li, Ruiyao Xu, Yuangang Li, Jiaqi Li, Zhuo Xiao, Xiyang Hu, Ryan Rossi, Kaize Ding, and 1 others. 2024. Ad-llm: Benchmarking large language models for anomaly detection. arXiv preprint arXiv:2412.11142.</p>
<p>Yue Zhao, Zain Nasrullah, and Zheng Li. 2019. PyOD: A Python Toolbox for Scalable Outlier Detection. Journal of Machine Learning Research (JMLR), 20:17.</p>
<p>Yue Zhao, Ryan Rossi, and Leman Akoglu. 2021. Automatic unsupervised outlier model selection. NeurIPS, 34:4489-4502.</p>
<h2>Supplementary Material for NLP-ADBench</h2>
<h2>A Details on NLP-ADBench</h2>
<h2>A. 1 Additional Details on Benchmark Datasets</h2>
<h2>A.1.1 Datasets Sources.</h2>
<ol>
<li>NLPAD-AGNews is constructed from the AG News dataset (Rai, 2023), which was originally intended for news topic classification tasks. The AG News dataset contains 127,600 samples categorized into four classes: World, Sports, Business, and Sci/Tech. We selected the text from the "description" column as NLPAD-AGNews's text data source. The "World" category was designated as the anomaly class and was downsampled accordingly.</li>
<li>NLPAD-BBCNews is constructed from the BBC News dataset (Greene and Cunningham, 2006), which was originally used for document classification across various news topics. The BBC News dataset includes 2,225 articles divided into five categories: Business, Entertainment, Politics, Sport, and Tech. We selected the full text of the news articles as NLPAD-BBC News's text data source. The "Entertainment" category was designated as the anomaly class and was downsampled accordingly.</li>
<li>NLPAD-EmailSpam is constructed from the Spam Emails dataset (Metsis et al., 2006), originally used for email spam detection. The Spam Emails dataset contains 5,171 emails labeled as either spam or ham (not spam). We selected the text from the "text" column containing the email bodies as NLPAD-Emails Spam's text data source. The "spam" category was designated as the anomaly class and was downsampled accordingly.</li>
<li>NLPAD-Emotion: is constructed from the Emotion dataset (Saravia et al., 2018) , which was originally intended for emotion classification tasks in textual data. The Emotion dataset contains 416,809 text samples labeled with six emotions: anger, fear, joy, love, sadness, and surprise. We selected the text from the "text" column as NLPAD-Emotion's text data source. The "fear" category was designated as the anomaly class and was downsampled accordingly.</li>
<li>NLPAD-MovieReview: is constructed from the Movie Review dataset (Maas et al., 2011) , commonly used for sentiment analysis of film critiques. The Movie Review dataset includes 50,000 reviews labeled as positive or negative. We selected the full review texts as NLPADMovieReview's text data source. The "neg" (negative reviews) category was designated as the anomaly class and was downsampled accordingly.</li>
<li>NLPAD-N24News is constructed from the N24News dataset (Wang et al., 2022), originally used for topic classification of news articles. N24News contains 61,235 articles across various categories. We selected the full text of the news articles as NLPAD-N24News's text data source. The "food" category was designated as the anomaly class and was downsampled accordingly.</li>
<li>NLPAD-SMSSpam is constructed from the SMS Spam Collection dataset (Almeida et al., 2011), originally intended for classifying SMS messages as spam or ham (not spam). The SMS Spam Collection dataset comprises 5,574 messages labeled accordingly. We selected the text from the "message text" as NLPAD-SMS Spam's text data source. The "spam" category was designated as the anomaly class and was downsampled accordingly.</li>
<li>NLPAD-YelpReview is constructed from the Yelp Review Polarity dataset (Putra, 2023), originally intended for sentiment classification tasks. The Yelp Review Polarity dataset is created by considering 1 -star and 2 -star ratings as negative, and 3 -star and 4 -star ratings as positive. For each polarity, 280,000 training samples and 19,000 testing samples were randomly selected, resulting in a total of 560,000 training samples and 38,000 testing samples. Negative polarity is labeled as class 1 , and positive polarity as class 2. We selected the text from the text column as NLPAD-YelpReview's text data source. The label 1 (negative reviews) was designated as the anomaly class and was downsampled accordingly.</li>
</ol>
<h2>A.1.2 NLPAD dataset's text pre-processing</h2>
<p>On all 8 datasets, we preprocessed the raw text data to ensure consistency and usability by removing URLs and HTML tags, eliminating unnecessary special characters while retaining essential punctuation, converting line breaks and consecutive spaces into single spaces, and preserving case sensitivity and stop words to maintain linguistic integrity. After processing the text, we found that some texts</p>
<p>became duplicates due to the removal of certain symbols. Consequently, we removed all duplicate data to ensure the uniqueness of each text sample. These preprocessing steps follow established practices to effectively clean text data while retaining its syntactic and semantic features, providing a reliable foundation for natural language processing tasks (Chai, 2022).</p>
<h2>A. 2 Additional Details on Algorithms</h2>
<h2>A.2.1 End-to-End Algorithms</h2>
<ol>
<li>Context Vector Data Description (Ruff et al., 2019)(CVDD) is an unsupervised anomaly detection method for textual data. It utilizes pre-trained word embeddings and a multi-head self-attention mechanism to learn "context vectors" that represent normal patterns in the data. Anomalies are detected by measuring the cosine distance between sequence projections and context vectors, where larger distances indicate higher anomaly likelihoods. CVDD also penalizes overlapping contexts to enhance interpretability.</li>
<li>Detecting Anomalies in Text via Self-Supervision of Transformers (DATE) (Manolache et al., 2021) detects anomalies in text by training self-supervised transformers on tasks like replaced mask detection, enabling the model to learn normal language patterns and identify deviations.</li>
<li>Few-shot Anomaly Detection in Text with Deviation Learning (FATE) (Das et al., 2023) is a deep learning framework that uses a small number of labeled anomalies to learn anomaly scores end-to-end. By employing deviation learning, it ensures normal examples align with reference scores while anomalies deviate significantly. Utilizing multi-head self-attention and multiple instance learning, FATE achieves state-of-the-art performance on benchmark datasets. However, as our approach focuses on unsupervised anomaly detection, we adapt FATE into FATE* by training exclusively on normal data. This adaptation involves modifying the framework to learn reference scores and deviations without access to labeled anomalies, enabling effective detection of anomalous examples in an entirely unsupervised setting.</li>
</ol>
<h2>A.2.2 Traditional Algorithms</h2>
<ol>
<li>Local Outlier Factor (LOF) (Breunig et al., 2000) calculates the local density deviation of a
data point relative to its neighbors. This metric identifies points that have substantially lower density than their neighbors, marking them as outliers.</li>
<li>Deep Support Vector Data Description (DeepSVDD) (Ruff et al., 2018) minimizes the volume of a hypersphere enclosing the data representations learned by a neural network, capturing common patterns while identifying anomalies as points outside the hypersphere.</li>
<li>Empirical-Cumulative-distribution-based Outlier Detection (ECOD) (Li et al., 2022) estimates the empirical cumulative distribution function (ECDF) for each feature independently. It identifies outliers as data points that reside in the tails of these distributions. This approach is hyperparameter-free and offers straightforward interpretability.</li>
<li>Isolation Forest (IForest) (Liu et al., 2008) detects anomalies by isolating observations through random feature selection and splitting, with anomalies requiring fewer splits</li>
<li>Single-Objective Generative Adversarial Active Learning (SO_GAAL) (Liu et al., 2019) optimizes a single objective function to generate adversarial samples and effectively identify anomalies in unsupervised settings.</li>
<li>AutoEncoder (AE) (Aggarwal, 2017) detects anomalies by reconstructing input data, where higher reconstruction errors signify potential anomalies.</li>
<li>Unifying Local Outlier Detection Methods via Graph Neural Networks(LUNAR) (Goodge et al., 2022) uses graph neural networks to integrate and enhance traditional local outlier detection methods, unifying them for better anomaly detection.</li>
<li>Variational AutoEncoder (VAE) (Kingma and Welling, 2013; Burgess et al., 2018) uses probabilistic latent variables to model data distributions, identifying anomalies based on reconstruction probabilities or latent space deviations.</li>
</ol>
<h2>A. 3 More Experiment Results</h2>
<p>We also report AUPRC scores (Table. A1) for all 19 algorithms across the 8 NLPAD datasets, along with their average AUPRC ranks (Fig. A1), to provide a complementary evaluation perspective beyond AUROC.</p>
<p>Table A1: Performance comparison of 19 Algorithms on 8 NLPAD datasets using AUPRC, with best results highlighted in bold and shaded</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">NLPAD- <br> AGNews</th>
<th style="text-align: center;">NLPAD- <br> BBCNews</th>
<th style="text-align: center;">NLPAD- <br> EmailSpam</th>
<th style="text-align: center;">NLPAD- <br> Emotion</th>
<th style="text-align: center;">NLPAD- <br> MovieReview</th>
<th style="text-align: center;">NLPAD- <br> N24News</th>
<th style="text-align: center;">NLPAD- <br> SMSSpam</th>
<th style="text-align: center;">NLPAD- <br> YelpReview</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CVDD</td>
<td style="text-align: center;">0.1296</td>
<td style="text-align: center;">0.2976</td>
<td style="text-align: center;">0.5353</td>
<td style="text-align: center;">0.0955</td>
<td style="text-align: center;">0.1576</td>
<td style="text-align: center;">0.2886</td>
<td style="text-align: center;">0.0712</td>
<td style="text-align: center;">0.1711</td>
</tr>
<tr>
<td style="text-align: left;">DATE</td>
<td style="text-align: center;">0.3996</td>
<td style="text-align: center;">0.5764</td>
<td style="text-align: center;">$\mathbf{0 . 8 8 8 5}$</td>
<td style="text-align: center;">0.1619</td>
<td style="text-align: center;">0.1682</td>
<td style="text-align: center;">0.2794</td>
<td style="text-align: center;">$\mathbf{0 . 6 1 1 2}$</td>
<td style="text-align: center;">0.2149</td>
</tr>
<tr>
<td style="text-align: left;">FATE*</td>
<td style="text-align: center;">0.2787</td>
<td style="text-align: center;">0.5805</td>
<td style="text-align: center;">0.5529</td>
<td style="text-align: center;">0.1026</td>
<td style="text-align: center;">0.1752</td>
<td style="text-align: center;">0.2777</td>
<td style="text-align: center;">0.1257</td>
<td style="text-align: center;">0.2112</td>
</tr>
<tr>
<td style="text-align: left;">BERT + LOF</td>
<td style="text-align: center;">0.2549</td>
<td style="text-align: center;">0.6029</td>
<td style="text-align: center;">0.2370</td>
<td style="text-align: center;">0.1170</td>
<td style="text-align: center;">0.1621</td>
<td style="text-align: center;">0.1678</td>
<td style="text-align: center;">0.1837</td>
<td style="text-align: center;">0.2629</td>
</tr>
<tr>
<td style="text-align: left;">BERT + DeepSVDD</td>
<td style="text-align: center;">0.2160</td>
<td style="text-align: center;">0.1328</td>
<td style="text-align: center;">0.2117</td>
<td style="text-align: center;">0.0986</td>
<td style="text-align: center;">0.1387</td>
<td style="text-align: center;">0.0798</td>
<td style="text-align: center;">0.1178</td>
<td style="text-align: center;">0.2174</td>
</tr>
<tr>
<td style="text-align: left;">BERT + ECOD</td>
<td style="text-align: center;">0.1616</td>
<td style="text-align: center;">0.2037</td>
<td style="text-align: center;">0.2077</td>
<td style="text-align: center;">0.1024</td>
<td style="text-align: center;">0.1374</td>
<td style="text-align: center;">0.0928</td>
<td style="text-align: center;">0.1156</td>
<td style="text-align: center;">0.2197</td>
</tr>
<tr>
<td style="text-align: left;">BERT + iForest</td>
<td style="text-align: center;">0.1559</td>
<td style="text-align: center;">0.2131</td>
<td style="text-align: center;">0.1894</td>
<td style="text-align: center;">0.1007</td>
<td style="text-align: center;">0.1412</td>
<td style="text-align: center;">0.0872</td>
<td style="text-align: center;">0.0994</td>
<td style="text-align: center;">0.2203</td>
</tr>
<tr>
<td style="text-align: left;">BERT + SO-GAAL</td>
<td style="text-align: center;">0.1033</td>
<td style="text-align: center;">0.0849</td>
<td style="text-align: center;">0.1130</td>
<td style="text-align: center;">0.1036</td>
<td style="text-align: center;">0.1486</td>
<td style="text-align: center;">0.0837</td>
<td style="text-align: center;">0.0714</td>
<td style="text-align: center;">0.2440</td>
</tr>
<tr>
<td style="text-align: left;">BERT + AE</td>
<td style="text-align: center;">0.2232</td>
<td style="text-align: center;">0.4274</td>
<td style="text-align: center;">0.2937</td>
<td style="text-align: center;">0.1037</td>
<td style="text-align: center;">0.1479</td>
<td style="text-align: center;">0.1255</td>
<td style="text-align: center;">0.1914</td>
<td style="text-align: center;">0.2525</td>
</tr>
<tr>
<td style="text-align: left;">BERT + VAE</td>
<td style="text-align: center;">0.1878</td>
<td style="text-align: center;">0.2559</td>
<td style="text-align: center;">0.2247</td>
<td style="text-align: center;">0.1019</td>
<td style="text-align: center;">0.1405</td>
<td style="text-align: center;">0.0957</td>
<td style="text-align: center;">0.1360</td>
<td style="text-align: center;">0.2331</td>
</tr>
<tr>
<td style="text-align: left;">BERT + LUNAR</td>
<td style="text-align: center;">0.2717</td>
<td style="text-align: center;">0.5943</td>
<td style="text-align: center;">0.3571</td>
<td style="text-align: center;">0.1053</td>
<td style="text-align: center;">0.1497</td>
<td style="text-align: center;">0.1436</td>
<td style="text-align: center;">0.1817</td>
<td style="text-align: center;">0.2609</td>
</tr>
<tr>
<td style="text-align: left;">OpenAI + LOF</td>
<td style="text-align: center;">0.5443</td>
<td style="text-align: center;">0.7714</td>
<td style="text-align: center;">0.5967</td>
<td style="text-align: center;">0.2290</td>
<td style="text-align: center;">0.2133</td>
<td style="text-align: center;">0.2248</td>
<td style="text-align: center;">0.2450</td>
<td style="text-align: center;">0.5710</td>
</tr>
<tr>
<td style="text-align: left;">OpenAI + DeepSVDD</td>
<td style="text-align: center;">0.1062</td>
<td style="text-align: center;">0.1288</td>
<td style="text-align: center;">0.1195</td>
<td style="text-align: center;">0.1040</td>
<td style="text-align: center;">0.3278</td>
<td style="text-align: center;">0.1297</td>
<td style="text-align: center;">0.0721</td>
<td style="text-align: center;">0.1893</td>
</tr>
<tr>
<td style="text-align: left;">OpenAI + ECOD</td>
<td style="text-align: center;">0.3294</td>
<td style="text-align: center;">0.2424</td>
<td style="text-align: center;">0.5597</td>
<td style="text-align: center;">0.7443</td>
<td style="text-align: center;">$\mathbf{0 . 5 1 6 5}$</td>
<td style="text-align: center;">0.2238</td>
<td style="text-align: center;">0.0821</td>
<td style="text-align: center;">$\mathbf{0 . 8 6 3 9}$</td>
</tr>
<tr>
<td style="text-align: left;">OpenAI + iForest</td>
<td style="text-align: center;">0.1278</td>
<td style="text-align: center;">0.1376</td>
<td style="text-align: center;">0.3283</td>
<td style="text-align: center;">0.1311</td>
<td style="text-align: center;">0.1724</td>
<td style="text-align: center;">0.0913</td>
<td style="text-align: center;">0.0772</td>
<td style="text-align: center;">0.2527</td>
</tr>
<tr>
<td style="text-align: left;">OpenAI + SO-GAAL</td>
<td style="text-align: center;">0.1538</td>
<td style="text-align: center;">0.0665</td>
<td style="text-align: center;">0.1096</td>
<td style="text-align: center;">0.1291</td>
<td style="text-align: center;">0.3005</td>
<td style="text-align: center;">0.0963</td>
<td style="text-align: center;">0.1213</td>
<td style="text-align: center;">0.2735</td>
</tr>
<tr>
<td style="text-align: left;">OpenAI + AE</td>
<td style="text-align: center;">0.4022</td>
<td style="text-align: center;">0.7485</td>
<td style="text-align: center;">0.5580</td>
<td style="text-align: center;">$\mathbf{0 . 8 3 5 5}$</td>
<td style="text-align: center;">0.1969</td>
<td style="text-align: center;">0.1984</td>
<td style="text-align: center;">0.1030</td>
<td style="text-align: center;">0.7063</td>
</tr>
<tr>
<td style="text-align: left;">OpenAI + VAE</td>
<td style="text-align: center;">0.3659</td>
<td style="text-align: center;">0.2424</td>
<td style="text-align: center;">0.5604</td>
<td style="text-align: center;">0.7744</td>
<td style="text-align: center;">0.1486</td>
<td style="text-align: center;">0.2537</td>
<td style="text-align: center;">0.0812</td>
<td style="text-align: center;">0.8467</td>
</tr>
<tr>
<td style="text-align: left;">OpenAI + LUNAR</td>
<td style="text-align: center;">$\mathbf{0 . 6 9 1 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 6 5 3}$</td>
<td style="text-align: center;">0.5810</td>
<td style="text-align: center;">0.3112</td>
<td style="text-align: center;">0.2193</td>
<td style="text-align: center;">$\mathbf{0 . 4 4 2 5}$</td>
<td style="text-align: center;">0.1640</td>
<td style="text-align: center;">0.4524</td>
</tr>
</tbody>
</table>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure A1: Average rank on AUPRC of 19 NLPAD methods across 8 datasets (the lower the better).</p>            </div>
        </div>

    </div>
</body>
</html>
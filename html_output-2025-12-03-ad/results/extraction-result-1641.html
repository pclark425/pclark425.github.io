<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1641 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1641</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1641</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-270357419</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.04920v3.pdf" target="_blank">Sim-to-Real Transfer of Deep Reinforcement Learning Agents for Online Coverage Path Planning</a></p>
                <p><strong>Paper Abstract:</strong> Coverage path planning (CPP) is the problem of finding a path that covers the entire free space of a confined area, with applications ranging from robotic lawn mowing to search-and-rescue. While for known environments, offline methods can find provably complete paths, and in some cases optimal solutions, unknown environments need to be planned online during mapping. We investigate the suitability of continuous-space reinforcement learning (RL) for this challenging problem, and propose a computationally feasible egocentric map representation based on frontiers, as well as a novel reward term based on total variation to promote complete coverage. Compared to existing classical methods, this approach allows for a flexible path space, and enables the agent to adapt to specific environment characteristics. Meanwhile, the deployment of RL models on real robot systems is difficult. Training from scratch may be infeasible due to slow convergence times, while transferring from simulation to reality, i.e. sim-to-real transfer, is a key challenge in itself. We bridge the sim-to-real gap through a semi-virtual environment, including a real robot and real-time aspects, while utilizing a simulated sensor and obstacles to enable environment randomization and automated episode resetting. We investigate what level of fine-tuning is needed for adapting to a realistic setting. Through extensive experiments, we show that our approach surpasses the performance of both previous RL-based approaches and highly specialized methods across multiple CPP variations in simulation. Meanwhile, our method successfully transfers to a real robot. Our code implementation can be found online (Link to code repository: https://github.com/arvijj/rl-cpp).</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1641.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1641.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CPP Sim-to-Real</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim-to-Real Transfer of Deep Reinforcement Learning Agents for Online Coverage Path Planning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sim-to-real transfer study where deep RL agents (continuous-control SAC with multi-scale egocentric maps and SGCNN) are trained in a 2D simulator and transferred to a real robotic platform via a semi-virtual environment (real robot kinematics/dynamics + simulated lidar/obstacles) with online fine-tuning and runtime considerations addressed (action delay, parallel updates).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Husqvarna Research Platform (HRP) and MiR100 evaluation platform</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Primary platform: HRP differential-drive robotic lawnmower (Husqvarna Research Platform) controlled via ROS and an on-board Nvidia Jetson AGX Orin; used for lawn-mowing fine-tuning and semi-virtual experiments. Secondary: MiR100 mobile base used for generalization tests (different dynamics and limited sensor FOV). Both accept continuous linear and angular velocity commands.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics (navigation / coverage path planning)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Custom 2D simulator + semi-virtual projection environment (real robot with simulated lidar/obstacles)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>A 2D physics/kinematics simulator that builds egocentric multi-scale occupancy, frontier and coverage maps from simulated 2D lidar readings and map geometry; for transfer experiments a semi-virtual setup was used where the real robot's pose and kinematics/dynamics are real but lidar and obstacles are simulated and projected onto the floor to allow automated resets and environment randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Mixed: physics/kinematics approximations with measured dynamics injected (moderate-to-high fidelity for kinematics/dynamics aspects), but simplified sensing (simulated 2D lidar) and visually non-photorealistic projection.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Linear and angular accelerations (max acceleration limits), measured action delay (execution latency), egocentric pose updates/localization (with measured noise levels in ablations), 2D lidar sensing (range, FOV, rays), environment geometry/obstacles and coverage state.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Sensor modality remained simulated (no real camera or photorealistic rendering), dynamic obstacles not modeled systematically (mostly static obstacles), wheel slip and some fine-grained friction/detailed contact dynamics not explicitly modeled (only approximated via measured accelerations/delay), and projection-based visualization (no full 3D effects).</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Indoor planar research arena with a high-accuracy positioning system (used for pose), projection of simulated obstacles/coverage onto the floor (semi-virtual); HRP deployed with Jetson AGX Orin for on-board inference and parallelized training; also tested on MiR100 platform with different dynamics and a limited FOV lidar (90°).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Online coverage path planning (CPP) — mapping the free space and producing continuous control signals to visit all accessible points (lawn mowing and exploration variations).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Off-line and on-line reinforcement learning (Soft Actor-Critic), trained in simulation (2–8M iterations) with curriculum learning, procedural/randomized maps; fine-tuned online in semi-virtual/real setting with parallelized data collection and model updates.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Coverage time metrics T90 and T99 (time to reach 90% and 99% coverage), plus collision frequency, path length, accumulated rotations, average speed, and learned entropy coefficient during fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>First-order policy (trained in sim): T90 = 6.0 min, T99 = 9.1 min (Δt = 500 ms). Higher-order policy (trained in sim with inertia/delay and 10 past actions): T90 = 5.8 min, T99 = 8.5 min (Δt = 500 ms).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>First-order policy (deployed without fine-tuning): at 500 ms inference T90 = 7.0 min, T99 = 10.6 min; at high inference rate (~15 ms) T90 = 5.4 min, T99 = 8.3 min. After fine-tuning the first-order policy performance degraded (example reported: after fine-tuning the measured average became worse, e.g. T90 ≈ 6.3 min, T99 ≈ 10.9 min). Higher-order policy (deployed without fine-tuning): T90 = 7.3 min, T99 = 10.3 min; after ~60k real fine-tuning steps performance improved to T90 = 6.7 min, T99 = 10.2 min. (Values taken from Tables 8–9 and extended tables in paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Procedural/randomized map generation (random floorplans, randomized circular obstacles, variable map sizes) used during training to improve generalization; the paper did not primarily use wide-banded randomization of physical parameters (mass, friction) but included measured accelerations and action delay in simulation; map/layout randomization was applied extensively.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Mismatched kinematics/dynamics (wheel slip, different accelerations), inertial and latency effects (non-Markovian dynamics), action delay and computational delays, differences in sensors (simulated lidar vs potential real sensor peculiarities), localization noise, and different inference frequencies between training and deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Use of a semi-virtual environment (real robot kinematics with simulated sensors/obstacles) for controlled fine-tuning, measuring and injecting real-world accelerations and action delay into the simulator, including previous actions in observations for higher-order policies, high inference frequency (enables first-order policies to transfer), online fine-tuning in the realistic setting, and parallelized data collection/model updates to handle real-time constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Including realistic accelerations and measured action delay in simulation is important; high inference frequency can compensate for simpler (first-order) simulated dynamics; higher-order policies trained with inertia/delay and action-history transfer better at lower inference frequencies and benefit from real-world fine-tuning; sensor realism was less emphasized but paper notes that simulated sensors were used and conclusions likely extend to real sensors though additional perception-specific work may be needed.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Online RL fine-tuning in the semi-virtual environment using the same SAC framework with parallelized model updates; reported fine-tuning runs include tens of thousands of real steps (examples: first-order fine-tuning run of 80k steps which degraded performance; higher-order fine-tuning of ~60k steps which improved long-term metrics); total real-world fine-tuning wall time reported roughly 8 hours in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Compared first-order policies (no inertia/delay in sim, no action history) vs higher-order policies (inertia, action delay, include 10 past actions). First-order policies trained in sim transferred acceptably when run at high inference frequency, sometimes outperforming sim when allowed faster inference, but fine-tuning of first-order models in the real environment tended to degrade performance. Higher-order policies transferred better at the target (lower) inference frequency and improved with real fine-tuning (after initial short-term degradation they surpassed original performance for long-term metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) A semi-virtual setup (real robot dynamics + simulated sensors/obstacles) enables controlled, automated sim-to-real transfer and online fine-tuning. 2) Measured kinematic/dynamic parameters (accelerations, action delay) and including past actions in observations reduce the sim-to-real gap; higher-order policies that model inertia and delay transfer better to lower-frequency deployment and improve with fine-tuning. 3) High inference frequency can allow first-order policies (trained without inertia/delay) to transfer well without fine-tuning. 4) Procedural map randomization improves generalization across unseen layouts. 5) Parallelized environment interaction and model updates are critical for real-time online RL and reduce issues from computational delays. Overall, the approach successfully transfers CPP policies from sim to a real robot, but appropriate modeling of dynamics, inference rate selection, and online fine-tuning strategy determine success.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-to-Real Transfer of Deep Reinforcement Learning Agents for Online Coverage Path Planning', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Champion-level drone racing using deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer in deep reinforcement learning for robotics: a survey <em>(Rating: 2)</em></li>
                <li>Data-efficient domain randomization with bayesian optimization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1641",
    "paper_id": "paper-270357419",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "CPP Sim-to-Real",
            "name_full": "Sim-to-Real Transfer of Deep Reinforcement Learning Agents for Online Coverage Path Planning",
            "brief_description": "A sim-to-real transfer study where deep RL agents (continuous-control SAC with multi-scale egocentric maps and SGCNN) are trained in a 2D simulator and transferred to a real robotic platform via a semi-virtual environment (real robot kinematics/dynamics + simulated lidar/obstacles) with online fine-tuning and runtime considerations addressed (action delay, parallel updates).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Husqvarna Research Platform (HRP) and MiR100 evaluation platform",
            "agent_system_description": "Primary platform: HRP differential-drive robotic lawnmower (Husqvarna Research Platform) controlled via ROS and an on-board Nvidia Jetson AGX Orin; used for lawn-mowing fine-tuning and semi-virtual experiments. Secondary: MiR100 mobile base used for generalization tests (different dynamics and limited sensor FOV). Both accept continuous linear and angular velocity commands.",
            "domain": "robotics (navigation / coverage path planning)",
            "virtual_environment_name": "Custom 2D simulator + semi-virtual projection environment (real robot with simulated lidar/obstacles)",
            "virtual_environment_description": "A 2D physics/kinematics simulator that builds egocentric multi-scale occupancy, frontier and coverage maps from simulated 2D lidar readings and map geometry; for transfer experiments a semi-virtual setup was used where the real robot's pose and kinematics/dynamics are real but lidar and obstacles are simulated and projected onto the floor to allow automated resets and environment randomization.",
            "simulation_fidelity_level": "Mixed: physics/kinematics approximations with measured dynamics injected (moderate-to-high fidelity for kinematics/dynamics aspects), but simplified sensing (simulated 2D lidar) and visually non-photorealistic projection.",
            "fidelity_aspects_modeled": "Linear and angular accelerations (max acceleration limits), measured action delay (execution latency), egocentric pose updates/localization (with measured noise levels in ablations), 2D lidar sensing (range, FOV, rays), environment geometry/obstacles and coverage state.",
            "fidelity_aspects_simplified": "Sensor modality remained simulated (no real camera or photorealistic rendering), dynamic obstacles not modeled systematically (mostly static obstacles), wheel slip and some fine-grained friction/detailed contact dynamics not explicitly modeled (only approximated via measured accelerations/delay), and projection-based visualization (no full 3D effects).",
            "real_environment_description": "Indoor planar research arena with a high-accuracy positioning system (used for pose), projection of simulated obstacles/coverage onto the floor (semi-virtual); HRP deployed with Jetson AGX Orin for on-board inference and parallelized training; also tested on MiR100 platform with different dynamics and a limited FOV lidar (90°).",
            "task_or_skill_transferred": "Online coverage path planning (CPP) — mapping the free space and producing continuous control signals to visit all accessible points (lawn mowing and exploration variations).",
            "training_method": "Off-line and on-line reinforcement learning (Soft Actor-Critic), trained in simulation (2–8M iterations) with curriculum learning, procedural/randomized maps; fine-tuned online in semi-virtual/real setting with parallelized data collection and model updates.",
            "transfer_success_metric": "Coverage time metrics T90 and T99 (time to reach 90% and 99% coverage), plus collision frequency, path length, accumulated rotations, average speed, and learned entropy coefficient during fine-tuning.",
            "transfer_performance_sim": "First-order policy (trained in sim): T90 = 6.0 min, T99 = 9.1 min (Δt = 500 ms). Higher-order policy (trained in sim with inertia/delay and 10 past actions): T90 = 5.8 min, T99 = 8.5 min (Δt = 500 ms).",
            "transfer_performance_real": "First-order policy (deployed without fine-tuning): at 500 ms inference T90 = 7.0 min, T99 = 10.6 min; at high inference rate (~15 ms) T90 = 5.4 min, T99 = 8.3 min. After fine-tuning the first-order policy performance degraded (example reported: after fine-tuning the measured average became worse, e.g. T90 ≈ 6.3 min, T99 ≈ 10.9 min). Higher-order policy (deployed without fine-tuning): T90 = 7.3 min, T99 = 10.3 min; after ~60k real fine-tuning steps performance improved to T90 = 6.7 min, T99 = 10.2 min. (Values taken from Tables 8–9 and extended tables in paper.)",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Procedural/randomized map generation (random floorplans, randomized circular obstacles, variable map sizes) used during training to improve generalization; the paper did not primarily use wide-banded randomization of physical parameters (mass, friction) but included measured accelerations and action delay in simulation; map/layout randomization was applied extensively.",
            "sim_to_real_gap_factors": "Mismatched kinematics/dynamics (wheel slip, different accelerations), inertial and latency effects (non-Markovian dynamics), action delay and computational delays, differences in sensors (simulated lidar vs potential real sensor peculiarities), localization noise, and different inference frequencies between training and deployment.",
            "transfer_enabling_conditions": "Use of a semi-virtual environment (real robot kinematics with simulated sensors/obstacles) for controlled fine-tuning, measuring and injecting real-world accelerations and action delay into the simulator, including previous actions in observations for higher-order policies, high inference frequency (enables first-order policies to transfer), online fine-tuning in the realistic setting, and parallelized data collection/model updates to handle real-time constraints.",
            "fidelity_requirements_identified": "Including realistic accelerations and measured action delay in simulation is important; high inference frequency can compensate for simpler (first-order) simulated dynamics; higher-order policies trained with inertia/delay and action-history transfer better at lower inference frequencies and benefit from real-world fine-tuning; sensor realism was less emphasized but paper notes that simulated sensors were used and conclusions likely extend to real sensors though additional perception-specific work may be needed.",
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "Online RL fine-tuning in the semi-virtual environment using the same SAC framework with parallelized model updates; reported fine-tuning runs include tens of thousands of real steps (examples: first-order fine-tuning run of 80k steps which degraded performance; higher-order fine-tuning of ~60k steps which improved long-term metrics); total real-world fine-tuning wall time reported roughly 8 hours in their experiments.",
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Compared first-order policies (no inertia/delay in sim, no action history) vs higher-order policies (inertia, action delay, include 10 past actions). First-order policies trained in sim transferred acceptably when run at high inference frequency, sometimes outperforming sim when allowed faster inference, but fine-tuning of first-order models in the real environment tended to degrade performance. Higher-order policies transferred better at the target (lower) inference frequency and improved with real fine-tuning (after initial short-term degradation they surpassed original performance for long-term metrics).",
            "key_findings": "1) A semi-virtual setup (real robot dynamics + simulated sensors/obstacles) enables controlled, automated sim-to-real transfer and online fine-tuning. 2) Measured kinematic/dynamic parameters (accelerations, action delay) and including past actions in observations reduce the sim-to-real gap; higher-order policies that model inertia and delay transfer better to lower-frequency deployment and improve with fine-tuning. 3) High inference frequency can allow first-order policies (trained without inertia/delay) to transfer well without fine-tuning. 4) Procedural map randomization improves generalization across unseen layouts. 5) Parallelized environment interaction and model updates are critical for real-time online RL and reduce issues from computational delays. Overall, the approach successfully transfers CPP policies from sim to a real robot, but appropriate modeling of dynamics, inference rate selection, and online fine-tuning strategy determine success.",
            "uuid": "e1641.0",
            "source_info": {
                "paper_title": "Sim-to-Real Transfer of Deep Reinforcement Learning Agents for Online Coverage Path Planning",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Champion-level drone racing using deep reinforcement learning",
            "rating": 2,
            "sanitized_title": "championlevel_drone_racing_using_deep_reinforcement_learning"
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Sim-to-real transfer in deep reinforcement learning for robotics: a survey",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_in_deep_reinforcement_learning_for_robotics_a_survey"
        },
        {
            "paper_title": "Data-efficient domain randomization with bayesian optimization",
            "rating": 1,
            "sanitized_title": "dataefficient_domain_randomization_with_bayesian_optimization"
        }
    ],
    "cost": 0.013839999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Sim-to-Real Transfer of Deep Reinforcement Learning Agents for Online Coverage Path Planning
23 Aug 2025</p>
<p>Arvi Jonnarth arvi.jonnarth@ieee.org 
Manta Systems
LinköpingSweden</p>
<p>Member, IEEEOla Johansson ola.johansson@liu.se 
Department of Electrical Engineering
Linköping University
Sweden</p>
<p>Jie Zhao 
Department of Information and Communication Engineering
Dalian University of Technology
China</p>
<p>Michael Felsberg michael.felsberg@liu.se 
Department of Electrical Engineering
Linköping University
Sweden</p>
<p>School of Engineering
University of KwaZulu-Natal
DurbanSouth Africa</p>
<p>Sim-to-Real Transfer of Deep Reinforcement Learning Agents for Online Coverage Path Planning
23 Aug 2025B4FADD2A3A99CA110BDF3C9EC7816A2F10.1109/ACCESS.2025.3581035arXiv:2406.04920v3[cs.RO]
Coverage path planning (CPP) is the problem of finding a path that covers the entire free space of a confined area, with applications ranging from robotic lawn mowing to search-and-rescue.While for known environments, offline methods can find provably complete paths, and in some cases optimal solutions, unknown environments need to be planned online during mapping.We investigate the suitability of continuous-space reinforcement learning (RL) for this challenging problem, and propose a computationally feasible egocentric map representation based on frontiers, as well as a novel reward term based on total variation to promote complete coverage.Compared to existing classical methods, this approach allows for a flexible path space, and enables the agent to adapt to specific environment characteristics.Meanwhile, the deployment of RL models on real robot systems is difficult.Training from scratch may be infeasible due to slow convergence times, while transferring from simulation to reality, i.e. sim-to-real transfer, is a key challenge in itself.We bridge the sim-to-real gap through a semi-virtual environment, including a real robot and real-time aspects, while utilizing a simulated sensor and obstacles to enable environment randomization and automated episode resetting.We investigate what level of fine-tuning is needed for adapting to a realistic setting.Through extensive experiments, we show that our approach surpasses the performance of both previous RL-based approaches and highly specialized methods across multiple CPP variations in simulation.Meanwhile, our method successfully transfers to a real robot.Our code implementation can be found online.a a Link to code repository: https://github.com/arvijj/rl-cppINDEX TERMS Coverage path planning, end-to-end learning, exploration, online, real-time, reinforcement learning, robotics, sim-to-real transfer, total variation.</p>
<p>I. INTRODUCTION</p>
<p>T ASK automation is an ever-growing field, and as au- tomated systems become more intelligent, they are expected to solve increasingly challenging problems.One such problem is coverage path planning (CPP), where the task is to find a path that covers all of the free space of an environment.If the environment is known, an optimal path can be planned offline [1].In contrast, if it is unknown, the path has to be planned online during mapping of the environment, e.g. by a robot, and an optimal path cannot be found in the general case [2].CPP has found its uses in many robotic applications, such as lawn mowing [3], vacuum cleaning [4], search-andrescue [5], and exploration [6].</p>
<p>We aim to develop a task-and robot-agnostic method for online CPP in unknown environments.This is a challenging task, as intricacies in certain applications may be difficult  to model, or even impossible, in some cases.In the real world, unforeseen discrepancies between the modeled and true environment may occur, e.g.due to wheel slip.Incompleteness of the present model must be assumed to account for open world scenarios.Thus, we approach the problem from the perspective of learning through embodiment.Reinforcement learning (RL) lends itself as a natural choice, where an agent can interact with the world and adapt to specific environment conditions, without the need for an explicit model.Concretely, we consider the case where an RL model directly predicts continuous control signals for an agent from sensor data.We present an RL approach for the CPP problem in terms of a multi-scale map feature representation using frontiers, a continuous action space, a network architecture based on grouped convolutions, and a reward term based on total variation to encourage complete coverage.Our approach finds coverage paths in complex environments for different variations of the CPP problem, as can be seen in Fig. 1.</p>
<p>A practical limitation for training machine learning models is the need for large amounts of real-world data, which is tedious and time-consuming to collect.In particular, training reinforcement learning agents for robotics from scratch requires access to the robot for the full duration of the training process.Meanwhile, during the early training phase, the agent is more likely to make mistakes that may damage the hardware or require human supervision for resetting the episodes.Instead, learning in simulation presents an attractive alternative.However, due to differences in the dynamics between the simulation and the real world, transferring a model from simulation to reality is challenging.Prior work reduce the sim-to-real gap by improving the simulation, e.g. by injecting realistic noise [7], [8], applying domain randomization [9], [10], or through meta learning [11], [12].Our goal is to transfer, for the task of coverage path planning, models trained in simulation to real environments, by fine-tuning in a realistic setting.</p>
<p>When training reinforcement learning models in real time on physical robots, additional considerations need to be accounted for compared to training in simulation.(1) There exists a mismatch between the real and simulated robot kinematics, dynamics, and sensing, such as slippage and noisy localization.This leads to different transition probabilities in the real world, where the predicted actions based on simulation may be suboptimal.(2) Due to inertia and latencies in the system, the dynamics are non-Markovian [13].This violates the common assumption that the Markov property holds.(3) Since the robot keeps moving during the various computations in the training process, the state observed by the agent is not perfectly aligned with the true state.This introduces a delay, where the agent predicts actions based on outdated observations of the environment.In this work, we address these problems.</p>
<p>To smoothen the transition into the real world, we use a real robot in a semi-virtual environment, utilizing a highly accurate positioning system in an indoor planar setting, with a simulated sensor and obstacles.This introduces the previously mentioned real-time aspects of reinforcement learning in a controlled manner, while allowing flexibility in utilizing automated episode resetting and varying training environments without having to physically construct them.The conclusions drawn from the fine-tuning experiments in this semivirtual setting are also relevant for the case of completely real environments, although they might need to be complemented with perception-specific findings.</p>
<p>To reduce latency in the training process, we perform model updates in parallel with the action and state selection [14], and perform all computations on on-board hardware.We utilize soft actor-critic learning [15] for its sample efficiency and its efficacy on continuous action spaces.To account for the non-Markovian dynamics in a lightweight manner, we include past actions in the observation space.To reduce the mismatch between the simulated and real kinematics, we measure the real-world linear and angular accelerations as well as action delay, and incorporate them into the simulation.We find that a high inference frequency enables first-order Markovian policies to transfer to a real setting.By introducing delays and action observations, higher-order Markovian models can be fine-tuned to further reduce the sim-to-real gap.Moreover, these models can operate at a lower frequency, thus reducing computational requirements for deployed systems.</p>
<p>This work builds on the conference paper [16], and is part of the PhD thesis [17].Our contributions are as follows:</p>
<p>• We propose an end-to-end deep reinforcement learning approach in continuous space for the online CPP problem.This includes a multi-scale map representation exploiting frontiers, a network architecture using grouped convolutions, and a novel reward term based on total variation.• We propose to divide the sim-to-real problem into two steps with an intermediate case of a real robot in a virtual environment.By performing data collection and model updates in parallel, our approach enables sim-to-real transfer of CPP policies through real-time fine-tuning online without a separate system or stopping the robot for model updates.</p>
<p>• Finally, we conduct extensive experiments, both in simulation and in a realistic setting, including detailed ablations on our proposed map representation, network architecture, and reward function, all of which improve the coverage time.Our approach surpasses the performance of both classical and RL-based methods, while successfully transferring to a real robot.</p>
<p>II. RELATED WORK</p>
<p>This work relates to coverage path planning, transferring models from simulation to the real world, and online RL in real time.We summarize the related work below.</p>
<p>A. COVERAGE PATH PLANNING</p>
<p>Coverage path planning methods can roughly be categorized as planning-based or learning-based.Planning-based methods include decomposition methods, which divide the area into cells based on e.g.boustrophedon cellular decomposition (BCD) [18] or Morse decomposition [19], where each cell is covered with a pre-defined pattern.Grid-based methods, such as Spiral-STC [20] and the backtracking spiral algorithm (BSA) [21], discretize the area into even smaller cells, e.g.squares, with a similar size to the agent.Then, a path is planned based on motion primitives connecting adjacent cells, i.e. to move up, down, left, or right on the grid, such that each cell is visited at least once.Frontier-based methods plan a path to a chosen point on the frontier, i.e. on the boundary between covered and non-covered regions.The choice of frontier point can be based on different aspects, such as the distance to the agent [22], the path of a rapidly exploring random tree (RRT) [23] or the gradient in a potential field [24].</p>
<p>Learning-based methods apply machine learning techniques, typically in combination with planning-based methods, to find coverage paths.Reinforcement learning is the most popular paradigm due to the sequential nature of the task.Chen et al. [25] use RL to find the order in which to cover the cells generated by BCD.Discrete methods [26], [27] use RL to learn which motion primitives to perform.RL has also been combined with frontier-based methods, either for predicting the cost of each frontier point [28], or for learning the control signals to navigate to a chosen point [29].In contrast, we learn continuous control signals end-to-end from a built map and sensor data to fully utilize the flexibility of RL.To the best of our knowledge, we are the first to do so for CPP.</p>
<p>B. MAP REPRESENTATION</p>
<p>To perform coverage path planning, the environment needs to be represented in a suitable manner.Similar to previous work, we discretize the map into a 2D grid with sufficiently high resolution to accurately represent the environment.This mapbased approach presents different choices for the input feature representation.Saha et al. [30] observe such maps in full resolution, where the effort for a d ×d grid is O(d 2 ), which is infeasible for large environments.Niroui et al. [28] resize the maps to keep the input size fixed.However, this means that the information in each grid cell varies between environments, and hinders learning and generalization for differently sized environments.Meanwhile, Shrestha et al. [31] propose to learn the map in unexplored regions.Instead of considering the whole environment at once, other works [32], [33] observe a local neighborhood around the agent.While the computational cost is manageable, the long-term planning potential is limited as the space beyond the local neighborhood cannot be observed.For example, if the local neighborhood is fully covered, the agent must pick a direction at random to explore further.To avoid the aforementioned limitations, we use multiple maps in different scales, similar to Klamt and Behnke [34].</p>
<p>C. SIM-TO-REAL TRANSFER</p>
<p>Transferring from simulation to the real world is challenging due to mismatch in both sensing and actuation [35].Prior work has approached this challenge by different means.Domain randomization has been utilized to randomize physical parameters in simulation, such as mass and joint damping [9], or to randomize textures and lighting conditions in the image domain [10].Other works introduce perturbations in the sensing and actuation [8].The motivation behind these approaches is that a highly randomized simulation would cover the real-world distribution, while avoiding a highly accurate model of it [35].Instead of randomizing various aspects in simulation, we train directly on the real distribution by fine-tuning in the real world.Meta learning methods aim to quickly adapt to new unseen tasks from a wide variety of training task, such as adapting to the real world from simulation [11], [12].Another approach is to learn from expert demonstrations through imitation learning [36], which has previously been applied to coverage path planning [29].In contrast, we avoid human demonstrations in order to reduce the manual effort.When it comes to robot control, Niroui et al. [28] deploy an RL policy for CPP trained in simulation on a differential drive robot without fine-tuning.The policy predicts the next frontier node, where a separate non-learned module navigates to it, thus being less affected by misaligned kinematics between simulation and reality.Kaufmann et al. [7] transfer a lightweight RL control policy using a pretrained perception system for first-person-view drone racing, utilizing empirical noise models to improve the fidelity of the simulator.They collect perception and dynamics residuals in the real world based on a highly accurate positioning system, which they use to augment the simulation.In contrast to these works, we fine-tune our model online in the real world.</p>
<p>D. REAL-TIME REINFORCEMENT LEARNING</p>
<p>Compared to turn-based and simulated environments, where the time for action selection and policy updates are assumed to be zero and the next state can be advanced to instantly, RL in the real world presents new challenges where these assumptions do not hold.For example, the forward and backward passes of the policy network during action selection and model updates take time, which cannot be neglected.</p>
<p>To take this into account, the core idea in the literature is to parallelize aspects of the environment interactions and the model training.Bakker et al. [37] explore quasi-online reinforcement learning, where a model of the environment is built online and in parallel to training the policy based on the built environment model.Ramstedt and Pal [38] propose to perform the action selection in parallel with the state selection.Concretely, given the current state and action to be taken, the next state and new action are computed concurrently.This approach takes into account the time for action selection by delaying the observed state by one time step.However, it does not consider the time for model updates, which is typically much larger.Other works [39], [40] distribute the model training for soft actor-critic (SAC) [15] to a separate system, while performing only the lightweight action and state selections on the on-board target hardware.This allows the data collection to be executed independently from the model updates, where the policy and replay buffer are periodically synchronized.Within this paradigm, Wang et al. [40] propose ReLoD, a system to efficiently distribute the learning algorithm over a resource-limited local computer and a powerful remote computer.Yuan and Mahmood [14] employ a similar approach, but they run everything on a single edge device.They divide the training process into separate processes for the action selection, state selection, batch sampling, and gradient computation.We follow this approach.However, in our experimental setup, the action selection and batch sampling were much faster than the state selection and gradient computations, so we only use two threads in order to reduce complexity and communication overhead.</p>
<p>III. REINFORCEMENT LEARNING OF COVERAGE PATHS</p>
<p>This section presents our approach for learning coverage paths using reinforcement learning.First, we define the online CPP problem in Section III-A, and subsequently formulate it as a partially observable Markow decision process (POMDP) in Section III-B.After that, we present our RL-based approach in terms of observation space in Section III-C, action space in Section III-D, reward function in Section III-E, and agent architecture in Section III-F.</p>
<p>A. PROBLEM DEFINITION AND DELINEATIONS</p>
<p>The goal is to navigate with an agent of radius r to visit all accessible points in a confined area without prior knowledge of its geometry.The free space includes all points inside the area that are not occupied by obstacles.A point is considered visited when the distance between the point and the agent is less than the coverage radius d, and the point is within the field-of-view of the agent.This definition unifies variations where d ≤ r, which we define as the lawn mowing problem, and where d &gt; r, which we refer to as exploration.To interactively gain knowledge about the environment, and for mapping its geometry, the agent needs some form of sensor.Both ranging sensors [29] and depth cameras [41] have been utilized for this purpose.Following Xu et al. [6], we choose a simulated 2D light detection and ranging (lidar) sensor that can detect obstacles in fixed angles relative to the agent, although our proposed framework is not limited to this choice.</p>
<p>Based on the pose of the agent, the detections are transformed to global coordinates, and a map of the environment is continuously built.As the focus of this paper is to learn coverage paths, and not to solve the localization problem, we assume known pose up to a certain noise level.However, our method may be extended to the case with unknown pose with the use of an off-the-shelf SLAM method.Finally, while our method can be further extended to account for moving obstacles and multi-agent coverage, they are beyond the scope of this paper.
N k=t γ k−t r k )
, with discount factor γ. In our particular problem, the policy is a neural network that predicts continuous control signals, a t , for the agent.The state s t includes the full environment geometry with all obstacles, the set of points that have been covered, as well as the pose of the agent.However, the agent only has information about what it has observed, so the observation o t consists of the part of the environment that has been mapped until time step t, along with covered points, agent pose, and sensor data.The reinforcement learning loop is depicted in Fig. 2(a).In the setting with a real robot, the first-order Markovian property is not fulfilled, due to the dynamics including inertia and momentum.The usual approach to augment previous states in   the current one becomes infeasible if the motion is implicitly represented by the agent-centered environment maps.Thus higher-order effects lead to model errors that need correction in subsequent steps.We go into further detail in Section IV-B.</p>
<p>C. OBSERVATION SPACE</p>
<p>To efficiently learn coverage paths, the observation space needs to be mapped into a suitable input feature representation for the neural network policy.To this end, we represent the visited points as a coverage map, and the mapped obstacles and boundary of the area as an obstacle map.The maps are discretized into a 2D grid with sufficiently high resolution to accurately represent the environment.To represent large regions in a scalable manner, we propose to use multi-scale maps, which was necessary for large environments.We make this viable through frontier maps that preserve information about the existence of non-covered space, even in coarse scales.</p>
<p>1) Multi-scale Maps</p>
<p>Inspired by multi-layered maps with varying scale and coarseness levels for search-and-rescue [34], we propose to use multi-scale maps for the coverage and obstacles to solve the scalability issue.We extract multiple local neighborhoods with increasing size and decreasing resolution, keeping the grid size fixed.We start with a local square crop M (1) with side length d 1 for the smallest and finest scale.The multiscale map representation M = {M (i) } m i=1 with m scales is constructed by cropping increasingly larger areas based on a fixed scale factor s. Concretely, the side length of map
M (i) is d i = sd i−1 .
The resolution for the finest scale is chosen sufficiently high such that the desired level of detail is attained in the nearest vicinity of the agent, allowing it to perform precise local navigation.At the same time, the largescale maps allow the agent to perform long-term planning, where a high resolution is less important.This multi-scale map representation can completely contain an area of size d × d in O(log d) number of scales.The total number of grid cells is O(wh log d), where w and h are the fixed width and height of the grids, and do not depend on d.This is a significant improvement over a single fixed-resolution map with O(d 2 ) grid cells.For the observation space, we use a multi-scale map M c for the coverage and M o for the obstacles.These are illustrated in Fig. 2(b).</p>
<p>2) Frontier Maps</p>
<p>When the closest vicinity is covered, the agent needs to make a decision where to explore next.However, the information in the low-resolution large-scale maps may be insufficient.For example, consider an obstacle-cluttered region where the obstacle boundaries have been mapped.A low coverage could either mean that some parts are non-covered free space, or that they are part of the interior of an obstacle.These cases cannot be distinguished if the resolution is too low.As a solution to this problem, we propose to encode a multi-scale frontier map M f , which we define in the following way.In the finest scale, a non-obstacle grid cell that has not been visited is a frontier point if any of its neighbours have been visited.Thus, a frontier point is non-visited free space that is reachable from the covered area.A region where the entire free space has been visited does not induce any frontier points.In the coarser scales, a grid cell is a frontier cell if and only if it contains at least one frontier point.In this way, the existence of frontier points persists through scales.Thus, regions with non-covered free space can be deduced in any scale, based on this multi-scale frontier map representation.</p>
<p>3) Egocentric Maps</p>
<p>As the movement is made relative to the agent, its pose needs to be related to the map of the environment.Following Chen et al. [41], we use egocentric maps which encode the pose by representing the maps in the coordinate frame of the agent.Each multi-scale map is aligned such that the agent is in the center of the map, facing upwards.This allows the agent to easily map observations to movement actions, instead of having to learn an additional mapping from a separate feature representation of its position, such as a 2D one-hot map [42].</p>
<p>4) Sensor Observations</p>
<p>To react on short-term obstacle detections, we include the sensor data in the input feature representation.The depth measurements from the lidar sensor are normalized to [0, 1] based on its maximum range, and concatenated into a vector S. Note, however, that this assumes a fixed set if lidar parameters, i.e. for the range, field-of-view, and number of rays.</p>
<p>D. ACTION SPACE</p>
<p>We let the model directly predict the control signals for the agent.This allows it to adapt to specific environment characteristics, while avoiding a constrained path space, different from a discrete action space.We consider an agent that independently controls the linear velocity v and the angular velocity ω, although the action space may seamlessly be changed to specific vehicle models.To keep a fixed output range, the actions are normalized to [−1, 1] based on the maximum linear and angular velocities, where the sign of the velocities controls the direction of travel and rotation.</p>
<p>In the real-world setting, we use a differential drive wheeled robot, which is controlled by two separately driven wheels.The agent predicts the linear and angular velocities v and ω for the robot, which are converted to angular velocities ω R and ω L for the right and left wheels,
ω R = v r w + ωb 2r w , ω L = v r w − ωb 2r w ,(1)
where r w is the wheel radius and b is the distance between the wheels.</p>
<p>A continuous action space is of course not the only choice, but it is more realistic than a discrete one.There are many reasons why we chose continuous actions.(1) This allows us to model a continuous pose that is not constrained by the grid discretization, and thus not constraining the path space.</p>
<p>(2) The agent can adapt and optimize its path for a specific kinematic model.In our experiments in Section V we find that our approach works well in continuous settings, which hints that it may also work in other continuous control spaces for specific kinematic models.The same conclusion could not be drawn if a discrete action space was used.Our specific choice of linear and angular velocities applies directly to a wide variety of robots, e.g.differential drive systems and the Ackermann kinematic model (by using a constraint on the angular velocity).This makes it more suitable for sim-to-real transfer compared to a discrete action space.(3) Compared to grid-based discrete actions, a continuous action space introduces additional sources for error, both regarding dynamics and localization, which are more realistic.</p>
<p>E. REWARD FUNCTION</p>
<p>As the goal is to cover the free space, a reward based on covering new ground is required.Similar to Chaplot et al. [43]   and Chen et al. [41], we define a reward term R area based on the newly covered area A new in the current time step that was not covered previously.To control the scale of this reward signal, we first normalize it to [0, 1] based on the maximum area that can be covered in each time step, which is related to the maximum speed v max .We subsequently multiply it with the scale factor λ area , which is the maximum possible reward in each step, resulting in the reward
R area = λ area A new 2rv max ∆t , (2)
where r is the agent radius and ∆t is the time step size.See Fig. 3 for an illustration.By only maximizing the coverage reward in (2), the agent is discouraged from overlapping its previous path, as this reduces the reward in the short term.This leads to holes or stripes in the coverage, which we observed in our experiments, see Fig. 4.These leftover parts can be costly to cover later on for reaching complete coverage.Covering the thin stripes afterward only yields a minor reward, resulting in slow convergence towards an optimal path.To reduce the leftover parts, we propose a reward term based on minimizing the total variation (TV) of the coverage map.Minimizing the TV corresponds to reducing the boundary of the coverage map, and thus leads to fewer holes.Given a 2D signal x, the discrete isotropic total variation, which has been used for image denoising [44], is expressed as
V (x) = i,j |x i+1,j − x i,j | 2 + |x i,j+1 − x i,j | 2 .
(3)</p>
<p>We consider two variants of the TV reward term, a global and an incremental.For the global TV reward R G TV , the agent is given a reward based on the global coverage map C t at time step t.To avoid an unbounded TV for large environments, it is scaled by the square root of the covered area A covered , as this results in a constant TV for a given shape of the coverage map, independent of scale.The incremental TV reward R I TV is based on the difference in TV between the current and the previous time step.A positive reward is given if the TV is decreased, and vice versa.The incremental reward is scaled by the maximum possible increase in TV in a time step, which is twice the traveled distance.The global and incremental rewards are respectively given by
R G TV (t) = −λ G TV V (C t ) √ A covered ,(4)R I TV (t) = −λ I TV V (C t ) − V (C t−1 ) 2v max ∆t ,(5)
where λ G TV and λ I TV are reward scaling parameters to make sure that |R TV | &lt; |R area | on average.Otherwise, the optimal behaviour is simply to stand still.</p>
<p>To avoid obstacle collisions, a negative reward R coll is given each time the agent collides with an obstacle.Finally, a small constant negative reward R const is given in each time step to encourage fast execution.Thus, our final reward function reads
R = R area + R G TV + R I TV + R coll + R const .(6)
During training, each episode is terminated when the agent reaches a pre-defined goal coverage, or when it has not covered any new space in τ consecutive time steps.</p>
<p>F. AGENT ARCHITECTURE</p>
<p>Due to the multi-modal nature of the observation space, we use a map feature extractor g m , a sensor feature extractor g s , and a fusing module g f .The map and sensor features are fused, resulting in the control signal prediction
(v, ω) = g f (g m (M c , M o , M f ), g s (S)).(7)
We consider three network architectures, a simple multilayer perceptron (MLP), a standard convolutional neural network (CNN), and our proposed scale-grouped convolutional neural network (SGCNN) that independently processes the different scales in the maps, see Fig. 5.The MLP is mainly used as a benchmark to evaluate the inductive priors in the CNN-based architectures.For the MLP, the feature extractors, g m and g s , are identity functions that simply flatten the inputs, and the fusing module consists of three fully connected (FC) layers.The CNN-based architectures use convolutional layers in the map feature extractor followed by a single FC layer.In SGCNN, we group the maps in M c , M o and M f by scale, as the pixel positions between different scales do not correspond to the same world coordinate.Each scale is convolved separately using grouped convolutions.This ensures that each convolution kernel is applied to grids where the spatial context is consistent across channels.The sensor feature extractor is a single FC layer, and the fusing module consists of three FC layers.More details can be found in Appendix A.</p>
<p>IV. TRANSFERRING CPP AGENTS TO THE REAL WORLD</p>
<p>Training from scratch directly on the real robot is infeasible due to a slow convergence of the training process.In Section V-B, we find that the number of training iterations required is in the order of millions, which would translate to non-stop training for weeks to months in the real world.Meanwhile, it would require some form of manual interaction.Thus, we aim to transfer CPP agents trained in simulation, and fine-tune them on a real robot.In this section, we describe our proposed approach for sim-to-real transfer.</p>
<p>A. LEARNING CPP ON THE REAL SYSTEM</p>
<p>Even if we will not train the agent from scratch in the real setting, we need to be able to fine-tune the transferred system by RL.In common RL-libraries, such as Stable-Baselines3 [45], Tianshou [46], and Spinning Up [47], the data collection and model updates are performed serially.While this is feasible in simulation and turn-based environments where the environment can be paused during model updates and time can be advanced instantly to the next state after action selection, it is not practical for real-time robotic applications where the agent is trained online.</p>
<p>In this case, we need to wait after action selection to observe the next state, while the execution remains idle.After the environment interaction, a batch is sampled from the replay buffer, gradients are computed, and the model is updated.However, this takes time, which is not negligible, especially on low-performance edge devices and embedded systems.During the gradient update step, the robot keeps following its previous action.As a result, the agent ends up in a state which deviates from the recorded one used for action VOLUME 13, 2025 TABLE 1. Measured times of the RL-step on the real system.Timing for (2) is application dependent, and chosen based on the optimal training step size.The overhead effectively becomes part of (1) resulting in an action delay of 50 ms.</p>
<p>Step selection.Following previous work [14], [39], we perform the model updates in parallel with the data collection, utilizing computational resources which would otherwise remain idle while waiting for the next state.</p>
<p>The training process can be divided into four main computational steps, where the respective measured times are given in Table 1:</p>
<p>(1) Action selection: Given the current observation, we sample an action from the policy.This corresponds to a forward pass of the policy network.</p>
<p>(2) State selection: After sampling an action, the control signals are sent to the robot platform, and the new state is observed.In simulation, this would occur instantly, while in the real world we need to let time pass to observe how the action affected the environment.After state selection, the reward is computed, and the action, previous state, new state, and reward are added to the replay buffer.</p>
<p>(3) Batch sampling: A training batch is sampled from the replay buffer.</p>
<p>(4) Model update: Gradients are computed based on the training batch, and the model weights are updated.This is the most computationally intensive part.</p>
<p>Since the action selection and batch sampling were fast compared to the state selection and gradient computations, we only use two threads to reduce overhead and complexity.Our computational graph is shown in Fig. 6, which consists of an environment interaction thread (top) and a model update thread (bottom).Since both threads interact with the replay buffer, we use a mutex to avoid conflicts.Luckily, the environment interaction thread adds an entry at the end of the cycle, while the model update thread samples from it early, so the risk of the threads blocking each other is low.</p>
<p>Since the model update thread modifies the weight tensors during a large portion of the runtime, i.e. both during the backward pass to update the gradients and during the optimizer step to update the parameters, a mutex is not feasible as it would block execution too often.Instead, we keep a copy of the model, which is only accessed by the model update thread.The weights are synchronized when both threads have finished execution.</p>
<p>With this approach, both the environment interaction and the model update can be performed multiple times before model synchronization.This is useful when the computation time for the model update exceeds that of the action and state selection.In this case, the number of environment interaction steps should be chosen to match the computation time for the model update.Meanwhile, if the model update is fast compared to the environment interaction, multiple updates can be performed during one environment interaction step, which was the case in our experiments.</p>
<p>Apart from the four main computational steps, the time for simulating the sensor, creating the observation, and synchronizing the weights results in additional overhead.This effectively becomes part of the action selection as the overhead delays the observed state, and results in an action delay.</p>
<p>B. CLOSING THE SIM-TO-REAL GAP</p>
<p>With the approach in the previous section, we could directly move to a real setting in-the-wild.However, this would make training and fine-tuning cumbersome as manual interaction would be required to supervise the training process.In order to fully automatically (continue to) train the system, we propose to use a semi-virtual setup, see Fig. 7.In this setup, we use the real robot with its kinematics and dynamics, but simulate its lidar sensor and obstacles, and localize the robot using a positioning system.The environment, including obstacles and coverage, e.g. from mowing, is visualized by a projection onto the ground.</p>
<p>The main purpose of using such an environment is to evaluate, in a controlled manner, the ability of the learning algorithm to generalize to a real robot in a real-time setting.We can study how the agent adapts to real-world aspects, such as wheel slip, that occur here.Moreover, as we will see in Section V-B, the proposed RL approach is robust to sensing and localization noise, and can be extended to the case of unknown pose, e.g. by estimating the pose using an off-the-shelf SLAM method.Thus, using our semi-virtual environment, we can draw conclusions about how we expect our approach to generalize to a fully realistic setting.</p>
<p>As mentioned, this semi-virtual setup enables fully automatic RL with the real robot.In particular, if the robot would drive towards one of the walls, both the robot and the environment can be moved back to the middle of the room.Furthermore, since we can automatically change or generate new environments each episode, we can train general CPP policies for completely unseen environments, and not just for a single fixed environment.Finally, the setup can also be used for fully automatic benchmarking of the learned model.</p>
<p>To further reduce the sim-to-real gap, we improve the fidelity of the simulator by taking into consideration the latencies induced by inertia and action delay.We measure the maximum linear acceleration, maximum angular acceleration, and action delay of the real system, and include these aspects in the simulated kinematics and dynamics.</p>
<p>To account for higher-order Markovian dynamics, we include information from previous time steps in the observation space.While the common approach is to stack several previous observations [39], [48], it is less feasible in our setting for multiple reasons.(1) Since the pose is embedded in the egocentric maps, rapid rotations significantly alter the observed state, making it difficult to learn the dynamics.(2) It significantly increases the model size and processing time, which are critical aspects for real-time applications.(3) It severely limits the replay buffer size, as the map observations are fairly large.Instead, we use a history of the previous actions, following previous work [39], [48].This is lightweight and avoids the listed problems, and should be sufficient for learning the dynamics of the robot.Another option would be to use velocity estimates, although they can be highly noisy, in contrast to action observations.Using action observations, the agent can learn to estimate the velocity if necessary.</p>
<p>C. OPTIMAL STRATEGY FOR GOING SIM-TO-REAL</p>
<p>When transferring the CPP model from the simulation to the semi-virtual environment, different levels of fine-tuning can be performed and the training in simulation can happen with or without higher-order dependencies in the Markov process.However, training in simulation with a first-order assumption with subsequent fine-tuning does not make too much sense because fine-tuning will always be subject to higher-order effects in the real system.</p>
<p>Thus, one special case is the transfer without fine-tuning, as this can use a first-order model trained in simulation with arbitrary time-steps under benchmarking.In contrast, the higher-order model implies a certain step-length during benchmarking (and fine-tuning).</p>
<p>Key questions are thus: (1) How does the first-order model trained solely in simulation work on the real robot, dependent on the steplength?(2) How does the higher-order model trained solely in simulation work on the real robot and in comparison to (1)? (3) How does the model with fine-tuning on the real robot perform in comparison to (2) and depend on the number of training steps?The working hypothesis is that (3) outperforms (2), and that (1) approximates (2) with sufficiently small step size.</p>
<p>V. EXPERIMENTS</p>
<p>In this section, we present our experimental results, both in terms of rigorous comparisons and ablations in simulation, as well as sim-to-real transfer to our semi-virtual setting.</p>
<p>A. IMPLEMENTATION DETAILS</p>
<p>Simulation training details.We first evaluate our RL approach in a simulated 2D environment.We utilize soft actorcritic learning [13], and train for 2-8M iterations with learning rate 2 • 10 −5 , batch size 256, replay buffer size 5 • 10 5 , and discount factor γ = 0.99.We evaluate our method on three settings: omnidirectional and non-omnidirectional exploration, as well as on the lawn mowing task.For physical dimensions in the different settings, see Table 2.The training time for one agent varied between 25 to 150 hours on a T4 GPU and a 6226R CPU.</p>
<p>Real-world training details.In our real-world experiments, we evaluate our approach on the lawn mowing task.The fine-tuning on the physical robot is conducted using a Husqvarna Research Platform (HRP) (MIT software licence) [49].This is a robotic lawnmower equipped with special firmware that allows it to be controlled through ROS.We equip the HRP with an Nvidia Jetson AGX Orin development kit.The wheel dimensions in (1) are r w = 12.25 cm and Environment details.Based on initial experiments, we find suitable reward parameters.The episodes are prematurely truncated if τ = 1000 consecutive steps have passed without the agent covering any new space.We set the maximum coverage reward λ area = 1, the incremental TV reward scale λ I TV = 0.2 for exploration and λ I TV = 1 for lawn mowing, the collision reward R coll = −10, and the constant reward R const = −0.1.Note that λ I TV is the only hyperparameter that differs between the two settings.The global TV reward scale was set to λ G TV = 0 as it did not contribute to a performance gain in our ablations in Section V-B4.For the multi-scale maps, we use m = 4 scales with 32 × 32 pixel resolution, a scale factor of s = 4, and 0.0375 meters per pixel for the finest scale.Thus, the maps span a square with side length 76.8 m.For more details regarding the choice of hyperparameters, see Appendix B. We apply curriculum learning, progressively increasing the environment complexity, and the goal coverage rate from 90% to 99% during training.For more details, see Appendix C. To increase the variation of encountered scenarios, we use both a fixed set of maps and procedurally generated maps, by randomizing grid-like floor plans.See Fig. 8 for examples, and Appendix D for more details.During real-world fine-tuning we use both fixed and randomized training maps that fit within the research arena, and set the goal coverage rate to 99% for episode termination.
VOLUME 13, 2025 (a) (b) (c) (d) (e) (f) (g) (h)
First-vs higher-order policies.In our experiments, we train two types of policies.For the first one, inertia and action delay are excluded from the simulation, i.e. the agent reaches the desired linear and angular velocities instantaneously, and the action takes effect immediately.Additionally, the observation space does not include any information about previous states.The action of this model is independent of the previous action sequence and only depends on the current state.Thus, we refer to it as a first-order policy.For the second type of policy, we consider inertia by limiting the maximum linear and angular accelerations, and also include an action delay.This captures higher-order Markovian dynamics, as the next state depends on a finite set of previous states and actions.Policies trained in this setting also observe the 10 previous actions, and we refer to them as higher-order policies.</p>
<p>Evaluation.To evaluate the various RL policies, we measure the times T 90 and T 99 to reach 90% and 99% coverage, respectively.We use these as the main metrics during ablations and comparisons with other methods, as the coverage time is usually important in CPP tasks, and common for benchmarking in the literature [6].Some applications also value path length and accumulated rotations [25], so we also provide these.For further analysis, we also measure the collision frequency, the average speed, and the learned entropy in our fine-tuning experiments.The evaluation is performed on maps that are not seen during training.For details on training and evaluation maps used in simulation and in the real world, see Appendix F. Qualitative results in the form of learned paths can be found in Fig. 15, and videos can be found online. 1</p>
<p>B. SIMULATION EXPERIMENTS</p>
<p>Before going to a realistic setting, we perform comparisons and ablation studies in simulation in this section.Here, we train first-order policies unless stated otherwise.</p>
<p>1) Omnidirectional Exploration</p>
<p>In omnidirectional exploration, the agent observes its surroundings in all directions through a 360 • field-of-view lidar sensor.For this setting, we evaluate our method on Explore-Bench [6], which is a recent benchmark that implements challenging environments, where four methods have been evaluated by the authors.These include three frontier-based methods, namely a distance-based frontier method [22], an RRT-based frontier method [23], and a potential field-based frontier method [24].The fourth method is an RL-based approach, where Xu et al. [6] train an RL model to de- termine a global goal based on active neural SLAM [43].</p>
<p>The benchmark contains six environments; loop, corridor, corner, rooms, combination 1 (rooms with corridors), and combination 2 (complex rooms with tight spaces), which can be found in Appendix F. The results are presented in Table 3.</p>
<p>Our approach surpasses the performance of both the frontierbased methods and Active Neural SLAM.This shows that learning control signals end-to-end with RL is, in fact, a suitable approach to CPP.Furthermore, Fig. 1 (left) shows that the agent has learned an efficient exploration path in a complex and obstacle-cluttered environment.</p>
<p>2) Non-Omnidirectional Exploration</p>
<p>We further evaluate our method on non-omnidirectional exploration, with a 180 • lidar field-of-view.We reimplement and compare with the recent frontier-based RL method of Hu et al. [29], which was trained specifically for this setting.Their method uses RL to navigate to a chosen frontier node.The coverage over time is presented in Fig. 9. Our method outperforms the frontier-based RL approach, demonstrating that end-to-end learning of control signals is superior to a multi-stage approach for adapting to a specific sensor setup.</p>
<p>3) Lawn Mowing</p>
<p>For the lawn mowing task, we compare with the backtracking spiral algorithm (BSA) [21], which is a common benchmark for this CPP variation.Note however that BSA is an offline method and does not solve the mapping problem.As such, we do not expect our approach to outperform it in this comparison.Instead, we use it to see how close we are to a good solution.We further implement and compare with an offline and online version of a baseline that combines A* [51] with a traveling salesman problem (TSP) solver on nodes in a grid-like configuration.This has previously been proposed for the lawn mowing task in the offline setting for small environments [52].To make it feasible for our larger environments, a heuristic was required for distant nodes, and periodic replanning was used for the online case, see Appendix G. Table 4 shows T 90 and T 99 for six maps numbered 1-6, which can be found in Appendix F. Compared to offline BSA, our method takes 35% and 51% more time to reach 90% and 99% coverage respectively.This is an impressive result considering the challenge of simultaneously mapping the environment.Moreover, our approach outperforms the online version of the TSP-based solution, and even surpasses the offline version for 90% coverage.The limiting factors for TSP are likely the grid discretization and suboptimal replanning, which lead to overlap, see Appendix G.</p>
<p>4) Ablation Study</p>
<p>In Tables 5, 6, and 7, as well as in Fig. 10, we explore the impact of different components of our approach via a series of ablations.Since some baselines in Table 6 struggled to reach 90% or 99% coverage, we provide the coverage at fixed times, instead of T 90 and T 99 .Optimal training step size.In Table 5, we investigate the impact of the training step size in simulation, both for firstorder and higher-order policies.These policies were evaluated on the real-world evaluation maps, see Appendix F. In both cases, 500 ms seems to be the best choice, considering 90% and 99% coverage times.Thus, we choose 500 ms for our experiments.If the step size is small, it may take longer to train in simulation since the computation time per step is the same.More steps are needed to complete the same number of episodes.In Table 5, we used the same number of training steps, which meant that the number of episodes were lower for the smaller step sizes.Meanwhile, with a lower step size, future rewards are more strongly discounted, and thus, longterm planning might be limited.It is possible that this could be remedied by tuning hyperparameters, such as the discount factor, γ.We simply used the default value of γ = 0.99 for SAC [15].Finally, a lower step size requires a higher number of good consecutive actions to a achieve a good result, which might inhibit learning.On the other hand, if the step size is too large, the short-term control is limited, as each action is applied over a longer time period.This requires the agent to be more precise in its actions, which makes the problem harder.A step size of 500 ms seems to strike a good balance.Based on this result, the state selection wait time in Table 1 was chosen.Finally, a step size of 500 ms allows us to perform 4 model updates for each environment step in the real setting.</p>
<p>TV rewards.In Table 6, we find that the incremental TV reward has a major impact on the lawn mowing problem, increasing the coverage from 85.1% to 97.8% (③ vs. ⑥).It also affects exploration, but not to the same extent.This is reasonable, as the total variation in the coverage map is lower in this case.When visualizing the learned paths, we found that by increasing λ I TV beyond the optimal value with respect to coverage time, the agent learned more compact paths.For example, it would zig-zag rectangular regions parallel to the short edge instead of the long edge, and in general try to prioritize the nearest empty space.This behavior resulted in longer coverage times due to an increased number of turns.However, these paths could be considered more visually pleasing, and in turn be more appealing in some applications.Finally, as global TV has less temporal variation, and behaves more like a constant reward, it turns out not to be beneficial for CPP (⑤ vs. ⑥).Agent architectures.The inductive prior of the CNN architecture enables a better understanding of the environment, and outperforms the MLP baseline (② vs. ①) in Table 6.Meanwhile, our proposed SGCNN architecture, which groups the different scales and convolves them separately, further improves the performance compared to a naive CNN (⑥ vs. ②), which treats all scales as the same spatial structure.We believe that the reason for the increased performance is that, with SGCNN, the agent is not faced with the additional challenge of discerning how the channels are spatially related.Thus, the representation learning becomes easier, and the agent can learn coverage paths more efficiently.</p>
<p>Multi-scale frontier maps.We further find in Table 6 that our proposed frontier map representation is crucial for achieving a high performance with the multi-scale map approach (④ vs. ⑥).The frontier representation efficiently highlights the locations of non-visited regions, which we believe is especially useful for long-term planning.Without this input feature, too much information is lost in the coarser scales, thus hindering long-term planning.</p>
<p>Number of scales.In Fig. 10, we compare the coverage for different number of scales on the exploration task.It shows that only using one or two scales is not sufficient, where at least three scales are required to represent a large enough environment to enable long-term planning.Meanwhile, increasing the number of scales even further does not hamper the performance, which shows that our approach can be applied to even larger environments than what have been considered in our experiments.The discrepancy was not as large for the lawn mowing problem, as it is more local in nature with a lower coverage radius.Robustness to noise.As the real world is noisy, we evaluate how robust our method is to noise in Table 7.We apply Gaussian noise to the position, heading, and lidar measurements during both training and evaluation.Thus, both the maps and the sensor data perceived by the agent contain noise.We consider three levels of noise and present T 90 and T 99 for the lawn mowing task.The result shows that, even under high levels of noise, our method still functions well.Our approach surpasses the TSP solution under all three noise levels.</p>
<p>5) Collision Statistics</p>
<p>In most real-world applications, it is vital to minimize the collision frequency, as collisions can inflict harm on humans, animals, the environment, and the robot.To gain insights into the collision characteristics of our trained agents, we tracked the collisions during evaluation.For exploration, it varied between once every 100-1000 seconds in simulation, which is roughly once every 50-500 meters.For lawn mowing, it varied between once every 150-250 seconds in simulation, which is roughly once every 30-50 meters.These include all forms of collisions, even low-speed side collisions.The vast majority of the collisions were near-parallel, and we did not observe any head on collisions.The practical implications are very different between these cases.In the real setting, the collision frequency on the lawn mowing task on the six evaluation maps was measured to be once every 6.2 minutes, or once every 65.4 meters, on average.Again, the collisions were mainly from the side of the robot, and not head on.</p>
<p>C. REAL-WORLD EXPERIMENTS</p>
<p>In this section, we fine-tune our CPP agent in our semi-virtual environment, and evaluate how different policies generalize to the real world.</p>
<p>1) Comparison of First-order CPP Policies</p>
<p>We first investigate how well policies that assume a first-order Markov process transfer to our semi-virtual environment.In this experiment, the observation space does not include any previous actions, and the policy is trained for 8M steps in a simulated environment without a limit on the linear and angular accelerations, and does not include any action delay.We evaluate the policy in simulation and in the real environment before and after fine-tuning.Since this policy does not depend on the previous action sequence, we can run it at any frequency.We evaluate it at the training frequency and as fast By comparing coverage times between simulation and reality in Table 8, we see that the real results are slightly worse at the lower frequency.This is expected due to differences in kinematics and other sources of error.When increasing the inference frequency in the real world, we observe faster coverage times, even surpassing the simulation results.This is likely the case due to the fact that the agent can update its actions faster and quickly adapt to errors.Thus, it can navigate more efficiently and produce a smoother pattern.For the performance on the individual evaluation maps, see Appendix E.</p>
<p>However, after fine-tuning, we find that the performance degrades.Our hypothesis is that as the observation includes no higher-order information, the optimal action cannot be deduced.For example, if the robot completely changes the direction of rotation, it takes some time before the new rotation takes effect.The observation may not change much, while the optimal action does.Thus, the performance degrades, which may take a long time to recover from.This is problematic due to the lower training frequency.Running the inference at a high frequency partly circumvents this problem, explaining the high performance of the model without fine-tuning.While we did not evaluate the fine-tuned policy on a lower frequency, we expect it to perform worse as in the non finetuning case.</p>
<p>2) Comparison of Higher-order CPP Policies</p>
<p>Next, we conduct a new experiment to evaluate how higherorder policies compare.We train a new model for 2M steps in simulation, where we include the past 10 actions in the observation, and limit the maximum linear and angular accelerations based on realistic estimates.Furthermore, we include the measured real-world overhead as an action delay in simulation to better align the simulation with reality.Since the 10  actions correspond to specific points back in time, we keep the inference time step the same as during training.</p>
<p>The new model can learn higher-order Markovian dynamics, and in Table 9 we see that it transfers well when directly deployed from simulation, surpassing the first-order model for 99% coverage under this lower inference frequency.Again, the real results are somewhat worse compared to the simulation results, as expected.Comparing Tables 8 and 9, the higher-order policy surpasses the first-order policy in both T 90 and T 99 at 500 ms after fine-tuning.For the performance on the individual evaluation maps, see Appendix E.</p>
<p>In Fig. 11, where we record the performance on a subset of the maps during fine-tuning, we observe that the performance degrades initially, but in contrast to the first-order model, the higher-order model surpasses its original performance.T 99 degrades heavily in the early stage, while T 90 remains more or less constant.This suggests that the agent initially sacrifices long-term planning in favour of low-level control.After it has adapted to the low-level controls, then the long-term planning also improves.Note that T 90 increases slightly after 40k iterations.It could be the case that the agent prioritizes the long-term goal over short-term performance towards the end  of the fine-tuning, e.g. by choosing a less greedy path early on that reduces the final coverage time.Considering the general trend in the second half of Fig. 11, we expect the performance to continue to improve with further fine-tuning steps.</p>
<p>To further analyze the fine-tuned policy, we measure the path length, accumulated rotations, the average speed, and the learned entropy coefficient.Fig. 12 shows the path length and accumulated full rotations at 99% coverage.The number of rotations were computed by summing the absolute differences in heading angle in every time step.Similar to the coverage time, both metrics degrade initially, and then improve over time, surpassing the initial performance.This shows that the agent finds a more efficient path after fine-tuning.It reduces the number of turns, which take time, while it overlaps its previous path less, thus reducing the total path length.</p>
<p>In Fig. 13, we analyse the average speed of the agent during fine-tuning.The initial drop in performance is also evident here.In the initial fine-tuning phase, the agent attains a lower average speed, as a result of e.g. more collisions and many small turns, while it adjusts to the new environment dynamics.Over time, it learns to control the robot in a more efficient manner, allowing it to keep a higher average speed with less turns, which results in lower coverage times.Fig. 14 shows the learned entropy coefficient in the soft actor-critic framework [13].It increases initially, suggesting that the agent prioritizes exploration over exploitation during the distribution shift to the real world.After the initial peak, the entropy coefficient decreases, putting more emphasis on exploitation.However, it does not reach the same level as in simulation, i.e. at 0 fine-tuning steps.This either suggests that the real environment contains more uncertainty, or that the learning process has not entirely converged yet.It is likely a combination of the two.</p>
<p>3) Generalization</p>
<p>To evaluate the generalization capability of our approach, we perform a new exploration experiment, this time on a new robot platform.The new robot consists of an MiR100 platform integrated with a UR5e robotic arm (not used).The MiR platform has two driving wheels in the middle to control rotational and lateral movement, and four passive rotating wheels in each corner for stability.This allows for the linear and angular velocities to be controlled independently, and thus the predicted control signals can be passed directly to the robot platform, after proper renormalization.To further test the generalization capability, we change the sensor setup by severely limiting the visibility of the agent.We use the non-omnidirectional setup, and reduce the lidar field-of-view to 90 • , i.e. ±45 • , and the range to 2 meters.The inference is executed as fast as possible, i.e. with a step size of 15 ms.</p>
<p>We perform the evaluation on a suitable map for this setting, which is shown in Fig. 16 together with a learned path.Table 10 compares T 90 and T 99 between simulation and the MiR platform.The real-world results are slightly worse, which can be expected when considering the discrepancy between the two settings.For example, the maximum linear and angular accelerations were lower on the MiR platform compared to the HRP, which was not accounted for in the simulation.However, the coverage results are within a reasonable margin, which demonstrates that our approach can indeed generalize to different sensor setups and robot platforms.</p>
<p>4) Energy Efficiency</p>
<p>Due to the lightweight nature of our proposed SGCNN architecture, it is well suited for real-time applications.With only 800k parameters, it is significantly smaller and faster than popular computer vision architectures, see Appendix A. Meanwhile, our full computational framework is energy efficient.When measuring the energy consumption on the battery powered Jetson module, it never exceeded 15 W during training, and drawing roughly 10 W during inference.Note that, while training requires more computational power, such as a GPU, the computational requirements at inference time are much lower, and can even be done on a CPU.In total, the finetuning took roughly 8 hours, which is only a small fraction of the weeks it would have taken to train from scratch.</p>
<p>VI. CONCLUSIONS</p>
<p>We present a method for online coverage path planning in unknown environments, based on a continuous end-to-end deep reinforcement learning approach.The agent implicitly solves three tasks simultaneously, namely mapping of the environment, planning a coverage path, and navigating the planned path while avoiding obstacles.For a scalable solution, we propose to use multi-scale maps, which is made viable through frontier maps that preserve the existence of non-covered space in coarse scales.We propose a novel reward term based on total variation, which significantly improves coverage time when applied locally, but not globally.The result is a task-and robot-agnostic CPP method that can be applied to different applications, which has been demonstrated on the exploration and lawn mowing tasks.Furthermore, we transfer the trained CPP policies to a semi-virtual environment in order to reduce the sim-to-real gap.We address the three challenges of sim-to-real transfer presented in the introduction.(1) The mismatch between simulated and real dynamics is reduced by a high inference frequency and fine-tuning the model in a real setting.(2) Non-Markovian dynamics are accounted for by using action observations, and incorporating realistic accelerations and delays into the simulation.(3) Computational delays are addressed by parallelizing the environment interactions and the model updates.In our experiments, we find that training the model in simulation assuming a first-order Markov process transfers well without fine-tuning, as long as the inference frequency is sufficiently high.Meanwhile, a higher-order policy can be further improved through fine-tuning, and can be deployed at a lower inference frequency, which lowers the computational requirements for a deployed system.</p>
<p>VII. LIMITATIONS AND FUTURE WORK</p>
<p>Limitations and future work relate to accurate pose, simulated sensor data, and static environments with no moving obstacles, which we discuss here.</p>
<p>Accurate pose.The real-world experiments are conducted with highly accurate pose estimates from an indoor positioning system, which may not be available, depending on the application.In such cases it needs to be estimated by other means, such as SLAM, which introduces higher levels of noise.However, our simulation experiments show that even under high levels of localization and sensing noise, the performance of our method is not impacted to a significant extent.Thus, we believe that our conclusions likely extend to real environments with higher noise levels.</p>
<p>Sensing.The sim-to-real gap is mainly reduced with respect to robot kinematics and dynamics, while the sensor data remains simulated.This was done to evaluate the sim-to-real transfer capability of the learning algorithm in a controlled manner.Nevertheless, we believe that our conclusions likely extend to real sensors as well, for the same reason as above.</p>
<p>Dynamic obstacles.While dynamic obstacles are not explicitly modeled in the training framework, they can be accounted for by including, for instance, an object detector, to update the map based on detected moving obstacles.When an object is detected, the corresponding location in the obstacle map is replaced with non-free space, and when the object is no longer detected, it is replaced with free space again.This was briefly tested in our experiments, with the following results.The robot did not collide with detected obstacles, but rather it planned around them.When the objects were moved away, and the robot explored the same region again, it planned its path through that region if necessary.In future work, we have to consider such experiments more systematically.</p>
<p>APPENDIX A AGENT ARCHITECTURE DETAILS</p>
<p>As proposed for soft actor-critic (SAC) [13], we use two Qfunctions that are trained independently, together with accompanying target Q-functions whose weights are exponentially moving averages.No separate state-value network is used.All networks, including actor network, Q-functions, and target Q-functions share the same network architecture, which is either a multilayer perceptron (MLP), a naive convolutional neural network (CNN), or our proposed scale-grouped convolutional neural network (SGCNN).These are described in the following paragraphs.The MLP contains a total of 3.2M parameters, where most are part of the initial fully connected layer, which takes the entire observation vector as input.Meanwhile, the CNN-based architectures only contain 0.8M parameters, as the convolution layers process the maps with fewer parameters.</p>
<p>MLP.As mentioned in the main paper, the map and sensor feature extractors for MLP simply consist of flattening layers that restructure the inputs into vectors.The flattened multiscale coverage maps, obstacle maps, frontier maps, and lidar detections are concatenated and fed to the fusion module.For the Q-functions, the predicted action is appended to the input of the fusion module.The fusion module consists of two fully connected layers with ReLU and 256 units each.A final linear prediction head is appended, which predicts the mean and standard deviation for sampling the action in the actor network, or the Q-value in the Q-functions.</p>
<p>Naive CNN.This architecture is identical to SGCNN, see below, except that it convolves all maps simultaneously in one go, without utilizing grouped convolutions.SGCNN.Our proposed SGCNN architecture uses the same fusion module as MLP, but uses additional layers for the feature extractors, including convolutional layers for the imagelike maps.Due to the simple nature of the distance readings for the lidar sensor, the lidar feature extractor only uses a single fully connected layer with ReLU.It has the same number of hidden neurons as there are lidar rays.The map feature extractor consists of a 2 × 2 convolution with stride 2 to reduce the spatial resolution, followed by three 3 × 3 convolutions with stride 1 and without padding.We use 24 total channels in the convolution layers.Note that the maps are grouped by scale, and convolved separately by their own set of filters.The map features are flattened into a vector and fed to a fully connected layer of 256 units, which is the final output of the map feature extractor.ReLU is used as the activation function throughout.</p>
<p>Computational efficiency.As our goal is to perform finetuning and inference on edge hardware in real time, the computational efficiency of our model is highly important.In this regard, our SGCNN architecture is suitable, with only 0.8M parameters.This is lightweight compared to popular ImageNet computer vision models, such as MobileNet [53], ResNet [54], ConvNeXt [55], SwinTransformer [56], [57], and VisionTransformer [58].In Table 11, we compare our model against these architectures in terms of number of parameters and inference time.The inference time for the ImageNet models were averaged over 1000 forward passes on 64 × 64 × 3-dimensional input images, as this corresponds to the same size as our 32 × 32 × 4 × 3-dimensional input maps.However, the VisionTransformer expects a 224 × 224 × 3dimensional input, so this was used instead.As we can see from the table, our SGCNN architecture measures the lowest inference time with the fewest parameters.</p>
<p>APPENDIX B CHOICE OF HYPERPARAMETERS</p>
<p>Multi-scale map parameters.We started by considering the size and resolution in the finest scale, such that sufficiently small details could be represented in the nearest vicinity.We concluded that a resolution of 0.0375 meters per pixel was a good choice as it corresponds to 8 pixels for a robot with a diameter of 30 cm.Next, we chose a size of 32 × 32 pixels, as this results in the finest scale spanning 1.2 × 1.2 meters.Next, we chose the scale factor s = 4, as this allows only a few maps to represent a relatively large region, while maintaining sufficient detail in the second finest scale.Finally, the training needs to be done with a fixed number of scales.The most practical way is to simply train with sufficiently many scales to cover the maximum size for a particular use case.Thus, we chose m = 4 scales, which in total spans a 76.8 × 76.8 m region, and can contain any of the considered training and evaluation environments.If the model is deployed in a small area, the larger scales would not contain any frontier points in the far distance, but the agent can still cover the area by utilizing the smaller scales.For larger use cases, increasing the size of the represented area by adding more scales is fairly cheap, as the computational cost is O(log n).Reward parameters.We started by fixing λ area = 1 and tuned the learning rate accordingly.Due to the normalization of the area and TV rewards, they are given approximately the same weight for λ area = 1 and λ TV = 1.Thus, we used this as a starting point, which seemed to work well for the lawn mowing problem, while a lower weight for the TV rewards was advantageous for exploration.Here, we made a parameter sweep over λ I TV and λ G TV , with values in {0.1, 0.2, 0.5, 1, 2}, and found 0.2 to be the best choice for exploration.For the collision reward, R coll , we found −10 to work best from {0, −1, −10}.For the constant reward, R const , a value of −0.1 was chosen out of {0, −0.1, −1}.</p>
<p>Episode truncation.For large environments, we found it important not to truncate the episodes too early, as this would hinder learning.If the truncation parameter τ was set too low, the agent would not be forced to learn to cover an area completely, as the episode would simply truncate whenever the agent could not progress, without a large penalty.With the chosen value of τ = 1000, the agent would be greatly penalized by the constant reward R const for not reaching the goal coverage, and would be forced to learn to cover the complete area in order to maximize the reward.</p>
<p>APPENDIX C PROGRESSIVE TRAINING</p>
<p>To improve the convergence in the early parts of training, we apply curriculum learning [59], which has shown to be effective in reinforcement learning [60].We use simple maps and increase their difficulty progressively.To this end, we rank the fixed training maps by difficulty, and assign them into tiers, see Fig. 17.The maps in the lower tiers have smaller sizes and simpler obstacles.For the higher tiers, the map size is increased together with the complexity of the obstacles.</p>
<p>Furthermore, we define levels containing certain sets of maps and specific goal coverage percentages, see Table 14.</p>
<p>The agent starts at level 1, and progresses through the levels during training.To progress to the next level, the agent has to complete each map in the current level, by reaching the specified goal coverage.Note that randomly generated maps are also used in the higher levels, in which case the agent has   to additionally complete a map with random floor plans, and a map with randomly placed circular obstacles to progress.Whenever random maps are used in a level, either a fixed map or a random map is chosen with a 50% probability each at the start of every episode.</p>
<p>APPENDIX D RANDOM MAP GENERATION</p>
<p>Inspired by procedural environment generation for reinforcement learning [61], we use randomly generated maps during training to increase the variation in the training data and to improve generalization.We consider random floor plans and randomly placed circular obstacles.First, the side length of a square area is chosen uniformly at random, from the interval [2.4,7.5] meters for mowing and [9.6, 15] meters for exploration.Subsequently, a random floor plan is created with a 70% probability.Finally, with 70% probability, a number of obstacles are placed such that they are far enough apart to guarantee that the agent can visit the entire free space, i.e. that they are not blocking the path between different parts of the free space.An empty map can be generated if neither a floor plan is created nor obstacles placed.In the following paragraphs, we describe the floor plan generation and obstacle placement in more detail.</p>
<p>Random floor plans.The random floor plans contain square rooms of equal size in a grid-like configuration, where neighboring rooms can be accessed through door openings.On occasion, a wall is removed completely or some openings are closed off to increase the variation further.First, floor plan parameters are chosen uniformly at random, such as the side length of the rooms from [1.5, 4.8] meters, wall thickness from [0.075, 0.3] meters, and door opening size from [0.6, 1.2] meters.Subsequently, each vertical and horizontal wall spanning the whole map either top-to-bottom or leftto-right is placed with a 90% probability.After that, door openings are created at random positions between each neighboring room.Finally, one opening is closed off at random for either each top-to-bottom spanning vertical wall or each leftto-right spanning horizontal wall, not both.This is to ensure that each part of the free space can be reached by the agent.</p>
<p>Random circular obstacles.</p>
<p>Circular obstacles with radius 0.25 meters are randomly scattered across the map, where one obstacle is placed every four square meters on average.If the closest distance between a new obstacle and any previous obstacle or wall is less than 0.6 meters, the new obstacle is removed to ensure that large enough gaps are left for the agent to navigate through and that it can reach every part of the free space.</p>
<p>APPENDIX E EXTENDED FINE-TUNING COVERAGE RESULTS</p>
<p>Tables 12 and 13 are extensions of Tables 8 and 9 in the main paper.They contain the 90% and 99% coverage times for each individual evaluation map, as well as the average times.</p>
<p>APPENDIX F MAPS</p>
<p>In Fig. 17</p>
<p>APPENDIX G DETAILS ON THE COMPARED METHODS</p>
<p>TSP-based solution with A*.</p>
<p>For the TSP baseline we subdivide the environment into square grid cells of comparable size to the agent, and apply a TSP solver to find the shortest path to visit the center of each cell, which are the nodes.In order to guarantee that each cell is fully covered by a circular agent, the side length for each grid cell is set to √ 2r, where r is the agent radius.Note that this leads to overlap and thus increases the time to reach full coverage.In the offline case, we compute the weight between each pair of nodes using the shortest path algorithm, A<em> [51].However, due to the size of our environments, the computation time was infeasible.Thus, we implemented a supremum heuristic for distant nodes, where they would be assigned the largest possible path length instead of running A</em>.This improved the runtime considerably, while not affecting the path length to a noticeable extent.For the online case we applied the TSP solver on all visible nodes, executed the path while observing new nodes, and repeated until all nodes were covered.In this case, the TSP execution time on an Epyc 7742 64-core CPU was included in the coverage time, as the path would need to be replanned online in a realistic setting.However, the execution time was relatively small compared to the time to navigate the path.We also tried replanning in shorter intervals, but the gain in time due to a decreased path length was smaller than the increase in computation time.</p>
<p>Code repositories.Table 15 lists the code implementations used for the methods under comparison.Note that for omnidirectional exploration, we evaluate our method under the same settings as in Explore-Bench, for which Xu et al. [6] report the performance of the compared methods.We report these results in Table 3.</p>
<p>Omnidirectional exploration</p>
<p>Official Explore-Bench implementation [6]: ARVI JONNARTH received the B.S. degree in mathematics in 2017, and the M.S. degree in engineering physics in 2018, both from Uppsala University, Sweden.He received his Ph.D. degree in electrical engineering from Linköping University, Sweden, in 2024.</p>
<p>From 2018 to 2019, he was a Software Engineer with Husqvarna Group.Between 2024 and 2025 he was a Vision and Machine Learning Specialist with Husqvarna Group, Sweden.Currently, he is an Engineering Lead with Manta Systems, Sweden.His research interests include semantic segmentation, weakly-supervised learning, and robot vision.His research on visual learning theory includes video object and instance segmentation, classification, segmentation, and registration of point clouds, as well as efficient machine learning techniques for incremental, fewshot, and long-tailed settings.</p>
<dl>
<dt>FIGURE 1 .</dt>
<dt>1</dt>
<dt>FIGURE 1. Learned paths for exploration (left) and lawn mowing (right), including the start (red triangle) and end position (green square).</dt>
<dd>
<p>Multi-scale frontier map</p>
</dd>
</dl>
<p>Frontiers</p>
<p>FIGURE 2 .
2
FIGURE 2. (a) Agent-environment interaction: The observation consists of multi-scale maps from (b) and lidar detections, based on which the model predicts continuous control signals for an agent.(b) Illustration of coverage, obstacle, and frontier maps in multiple scales: This example shows m = 4 scales with a scale factor of s = 2.All scales are centered at the agent, and discretized into the same pixel resolution, resulting in the multi-scale maps Mc , Mo, and M f , of size 8 × 8 × 4 in this example.</p>
<p>FIGURE 3 .
3
FIGURE 3.An illustration of the area reward Rarea, which is based on the maximum possible area that can be covered in each time step.</p>
<p>FIGURE 4 .
4
FIGURE 4. The total variation reward term improves path quality.The figure shows the path (yellow line) without TV reward (left), and with TV reward (right), including the covered region (green), lidar rays (blue lines) and frontier points (magenta).</p>
<p>FIGURE 5 .
5
FIGURE 5. Our proposed SGCNN architecture consists of convolution (CONV) and fully connected (FC) layers.The scales of the multi-scale maps are convolved separately as their spatial positions are not aligned in the grid.x3/x4 refer to the number of layers.</p>
<p>FIGURE 7 .
7
FIGURE 7. A picture of the robot having covered an environment with four square obstacles in our semi-virtual setting.</p>
<p>TABLE 2 .
2
Physical dimensions for the three different CPP variations in simulation.</p>
<p>FIGURE 8 .
8
FIGURE 8. Examples of exploration maps (a-c), lawn mowing maps (d-f), and randomly generated maps (g-h).</p>
<p>FIGURE 9 .
9
FIGURE 9. Simulation coverage over time in non-omnidirectional exploration.</p>
<p>FIGURE 10 .
10
FIGURE 10.Exploration coverage for different number of scales in simulation.</p>
<p>FIGURE 11 .
11
FIGURE 11.Average coverage times during fine-tuning on two evaluation maps.Dashed line: No fine-tuning.</p>
<p>FIGURE 12 .
12
FIGURE 12. Average path length and accumulated full rotations during fine-tuning on two evaluation maps.</p>
<p>FIGURE 13 .
13
FIGURE 13.Average speed during fine-tuning on two evaluation maps.Dashed line: No fine-tuning.</p>
<p>FIGURE 14 .
14
FIGURE 14.The learned entropy coefficient in soft actor-critic[13] during fine-tuning.</p>
<p>FIGURE 15 .
15
FIGURE 15.Learned paths for a fully trained agent, including the starting position (red triangle) and the end position (green square).Top row: Paths in simulation on the lawn mowing task (a-b) and on exploration (c-d).Bottom row: Paths in our semi-virtual environment on the lawn mowing task (a-d).</p>
<p>FIGURE 16 .TABLE 10 .
1610
FIGURE 16.Coverage path on the MiR100 platform, including the start (red triangle) and end position (green square).</p>
<p>FIGURE 17 .
17
FIGURE 17.The fixed training maps used in simulation are grouped into tiers by difficulty, depending on their size and complexity of the obstacles.</p>
<p>FIGURE 18 .
18
FIGURE 18. Training maps used during real-world fine-tuning.</p>
<p>FIGURE 19 .
19
FIGURE 19.Evaluation maps used in simulation for (a) omnidirectional exploration, (b) non-omnidirectional exploration, and (c) lawn mowing.The maps in the last two rows were used additionally for ablations in (d) exploration and (e) lawn mowing.</p>
<p>FIGURE 20 .
20
FIGURE 20.The evaluation maps used for the real-world experiments.</p>
<p>, 18, 19, and 20, we present the maps used in our experiments.Fig.17</p>
<p>and 18  show the training maps used in simulation and the real-world, respectively.Fig.19 and 20show the evaluation maps used in simulation and the realworld, respectively.The maps are ordered from left to right in the same order as they appear in the respective tables in the main paper.</p>
<p>OLA JOHANSSON received the B.S. degree in electrical engineering in 2020, and the M.S degree in systems control and robotics in 2022, both from KTH Royal Institute of Technology, Sweden.He is currently a Research Engineer at the Department of Electrical Engineering, Linköping University, Sweden.His research interests include localisation and motion planning in robotics.JIE ZHAO received the B.E. degree in information management and information systems from Northwest A&amp;F University, China, in 2018, and received the Ph.D. degree in signal and information processing from Dalian University of Technology (DUT), China, in 2023.She is currently a postdoctoral with the school of information and communication engineering, DUT.Her research interests include visual object tracking, and robot vision.MICHAEL FELSBERG received the Ph.D. degree from Kiel University, Germany, in 2002, and the docent degree from Linköping University, in 2005.He is a full professor with Linköping University, Sweden, since 2008.He received the DAGM Olympus award in 2005 and is fellow of the IAPR, ELLIS, and AAIA.</p>
<p>TABLE 3 .
3
Time in seconds for reaching 90% and 99% coverage on Explore-Bench in simulation.Our method surpasses both frontier-based methods and a recent RL-based approach using active neural SLAM.T 90 T 99 T 90 T 99 T 90 T 99 T 90 T 99 T 90 T 99 T 90 T 99 T 90 T 99
Map →LoopCorridorCornerRoomsComb. 1Comb. 2TotalMethod ↓Distance frontier124 145 162 169 210 426 159 210 169 175 230 537 1054 1662RRT frontier145 180 166 170 171 331 176 211 141 192 249 439 1048 1523Potential field frontier 131 152 158 162 133 324 156 191 165 183 224 547967 1559Active Neural SLAM190 214 160 266 324 381 270 315 249 297 588 755 1781 2228Ours89101981288029182998793120 349556 1061</p>
<p>TABLE 4 .
4
Time in minutes for reaching 90% and 99% coverage on the lawn mowing task in simulation.T 90 T 99 T 90 T 99 T 90 T 99 T 90 T 99 T 90 T 99 T 90 T 99 T 90 T 99
Map →Map 1Map 2Map 3Map 4Map 5Map 6TotalTSP Setting Method ↓ Offline BSA45 3049 3545 2951 3543 3148 3655 3461 4127 1730 23110 122 325 361 88 100 229 270OnlineTSP Ours62 4469 6070 4077 5067 4375 4970 4078 6937 2541 32142 158 448 498 118 149 310 409</p>
<p>TABLE 5 .
5
Training step size comparison in ms.T 90 and T 99 : Average time in minutes for reaching 90% and 99% coverage in simulation.
First-orderHigher-order∆tT 90T 99T 90T 991507.112.77.613.72506.49.36.410.15006.09.15.88.510006.19.66.49.1</p>
<p>TABLE 6 .
6
Coverage
(%) at 1500 and 1000 seconds for mowing (Mow) andexploration (Exp) respectively, comparing agent architecture (NN), TVrewards, and frontier map observation (M f ).SettingsCoverageR I TV R G TV M fNNMow Exp① ✓✓MLP81.4 27.3② ✓✓CNN93.2 88.4③✓ SGCNN 85.1 91.5④ ✓SGCNN 72.6 80.6⑤ ✓✓✓ SGCNN 96.6 89.0⑥ ✓✓ SGCNN 97.8 93.0</p>
<p>TABLE 7 .
7
Coverage time in minutes for different levels of noise on the lawn mowing task in simulation.
Noise standard deviationTimePosition HeadingLidarT 90 T 990.01 m0.05 rad 0.05 m 310 4090.02 m0.1 rad0.1 m338 4860.05 m0.2 rad0.2 m317 434</p>
<p>TABLE 8 . First-order policy.
8
Average time in minutes for reaching 90% and 99% coverage on six evaluation maps.Env: Evaluation environment.Steps: Fine-tuning steps.Inference step size is in ms.Note: Training step size is always 500 ms.
EnvSteps ∆tT 90 T 99Sim 0500 6.09.1Real 0500 7.0 10.6Real 0155.48.3Real 80k156.3 10.9</p>
<p>TABLE 9 .
9
Higher-order policy.Average time in minutes for reaching 90% and 99% coverage on six evaluation maps.Env: Evaluation environment.Steps: Fine-tuning steps.Inference step size is in ms.Note: Training step size is always 500 ms.
EnvSteps ∆tT 90 T 99Sim 0500 5.88.5Real 0500 7.3 10.3Real 60k500 6.7 10.2as possible, i.e. without waiting during state selection, whichwas measured to 15 ms per time step.</p>
<p>TABLE 11 .
11
Model comparison in terms of number of parameters and inference time between our proposed SGCNN architecture and existing ImageNet computer vision models.The inference time is measured on two laptops; L1 (i5-520M CPU, no GPU) and L2 (i7-13700H CPU, 4060 GPU).
Model#params Time (L1) Time (L2)MobileNet-V3-Small2.5M17 ms8 msMobileNet-V3-Large5.5M25 ms10 msResNet-1812M31 ms5 msResNet-5026M77 ms8 msConvNeXt-Tiny29M82 ms15 msSwin-V2-T29M175 ms35 msViT-Base-1686M1194 ms116 msSGCNN (ours)0.8M5 ms2 ms</p>
<p>TABLE 12 . First-order policy.
12
Time in minutes for reaching 90% and 99% coverage.Env: Evaluation environment.Steps: Fine-tuning steps.Inference step size is in ms.Note: Training step size is always 500 ms.90 T 99 T 90 T 99 T 90 T 99 T 90 T 99 T 90 T 99 T 90 T 99 T 90 T 99
Map 1Map 2Map 3Map 4Map 5Map 6AverageEnv T Sim 0 Steps ∆t 500 6.18.05.27.85.79.15.99.65.28.18.1 11.8 6.09.1Real 0500 5.79.56.5 10.5 6.59.79.0 10.9 6.09.18.5 13.8 7.0 10.6Real 0155.36.95.17.65.5 10.5 5.06.85.26.86.5 11.0 5.48.3Real 80k156.48.46.1 11.0 5.6 11.6 5.79.06.2 10.4 7.5 15.1 6.3 10.9</p>
<p>TABLE 13 .
13
Higher-order policy.Time in minutes for reaching 90% and 99% coverage.Env: Evaluation environment.Steps: Fine-tuning steps.Inference step size is in ms.Note: Training step size is always 500 ms.99 T 90 T 99 T 90 T 99 T 90 T 99 T 90 T 99 T 90 T 99 T 90 T 99
Map 1Map 2Map 3Map 4Map 5Map 6AverageEnv T 90 T Sim 0 Steps ∆t 500 5.6 8.65.78.75.28.25.37.65.27.37.7 10.3 5.88.5Real 0500 6.8 10.5 7.9 10.7 6.7 10.9 7.09.16.08.39.4 12.5 7.3 10.3Real 60k500 6.79.26.59.56.1 11.0 6.29.16.29.28.4 13.0 6.7 10.2</p>
<p>TABLE 14 .
14
The progressive training levels contain maps of increasingly higher tiers and goal coverage percentages.At the highest levels, generated maps with random floor plans and obstacles are also used.
ExplorationLawn mowingMap RandomGoalMap RandomGoalLevel tiersmapscoverage tiersmapscoverage11,290%090%21,2,490%0,190%31,2,495%0,195%41,2,497%0-295%51,2,499%0-297%61-499%0-299%71-4✓99%0-399%81-5✓99%0-3✓99%</p>
<p>TABLE 15 .
15
Links to code implementations used for the methods under comparison.</p>
<p>VOLUME 13, 2025 <br />
VOLUME 13, 2025 <br />
   VOLUME 13, 2025 <br />
   VOLUME 13, 2025 <br />
   VOLUME 13, 2025 <br />
Videos can be found online at this link: https://drive.google.com/drive/ folders/1J0vpOBuRhCHOxsnAE1Vd1cZ6-qHPlvUh?usp=sharing 10 VOLUME 13,</p>
<p>VOLUME
, 2025
   VOLUME 13, 2025 <br />
   VOLUME 13, 2025 <br />
   VOLUME 13, 2025 <br />
   VOLUME 13, 2025 <br />
During this research, Arvi Jonnarth was an industrial PhD student at Linköping University and Husqvarna Group.This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP), funded by the Knut and Alice Wallenberg (KAW) Foundation.The work was funded in part by the Vinnova project, human-centered autonomous regional airport, Dnr 2022-02678, and by the strategic research environment, ELLIIT, funded by the Swedish government.The computational resources were provided by the National Academic Infrastructure for Supercomputing in Sweden (NAISS), partially funded by the Swedish Research Council, grant agreement no.2022-06725, and by the Berzelius resource, provided by the KAW Foundation at the National Supercomputer Centre (NSC).
Optimal line-sweep-based decompositions for coverage algorithms. W H Huang, Cat. No. 01CH37164. 12001. 2001IEEEProceedings</p>
<p>A survey on coverage path planning for robotics. E Galceran, M Carreras, Robotics and Autonomous Systems. 61122013</p>
<p>Region filling operations with random obstacle avoidance for mobile robots. Z L Cao, Y Huang, E L Hall, Journal of Robotic systems. 521988</p>
<p>Cleaning robot control. F Yasutomi, M Yamada, K Tsukamoto, Proceedings. nullIEEE1988. 1988</p>
<p>Coverage path planning for legged robots in unknown environments. D Jia, M Wermelinger, R Diethelm, P Krüsi, M Hutter, 2016 IEEE international symposium on safety, security, and rescue robotics (SSRR. IEEE2016</p>
<p>Explore-bench: Data sets, metrics and evaluations for frontier-based and deep-reinforcement-learning-based autonomous exploration. Y Xu, J Yu, J Tang, J Qiu, J Wang, Y Shen, Y Wang, H Yang, 2022 International Conference on Robotics and Automation (ICRA). IEEE2022</p>
<p>Champion-level drone racing using deep reinforcement learning. E Kaufmann, L Bauersfeld, A Loquercio, M Müller, V Koltun, D Scaramuzza, Nature. 62079762023</p>
<p>Towards closing the sim-to-real gap in collaborative multi-robot deep reinforcement learning. W Zhao, J P Queralta, L Qingqing, T Westerlund, 5th International conference on robotics and automation engineering (ICRAE). IEEE2020</p>
<p>Data-efficient domain randomization with bayesian optimization. F Muratore, C Eilers, M Gienger, J Peters, IEEE Robotics and Automation Letters. 622021</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE2017</p>
<p>Meta reinforcement learning for sim-to-real domain adaptation. K Arndt, M Hazara, A Ghadirzadeh, V Kyrki, 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE2020</p>
<p>Learning to adapt in dynamic, real-world environments through meta-reinforcement learning. A Nagabandi, I Clavera, S Liu, R S Fearing, P Abbeel, S Levine, C Finn, International Conference on Learning Representations (ICLR). 2019</p>
<p>Soft actor-critic algorithms and applications. T Haarnoja, A Zhou, K Hartikainen, G Tucker, S Ha, J Tan, V Kumar, H Zhu, A Gupta, P Abbeel, arXiv:1812.059052018arXiv preprint</p>
<p>Asynchronous reinforcement learning for real-time control of physical robots. Y Yuan, A R Mahmood, 2022 International Conference on Robotics and Automation (ICRA). IEEE2022</p>
<p>Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. T Haarnoja, A Zhou, P Abbeel, S Levine, International conference on machine learning. PMLR2018</p>
<p>Learning coverage paths in unknown environments with deep reinforcement learning. A Jonnarth, J Zhao, M Felsberg, International Conference on Machine Learning (ICML). PMLR2024235508</p>
<p>Learning robot vision under insufficient data. A Jonnarth, 2024Linköping UniversityPh.D. dissertation</p>
<p>Coverage path planning: The boustrophedon cellular decomposition. H Choset, P Pignon, Field and service robotics. 1998Springer</p>
<p>Morse decompositions for coverage tasks. E U Acar, H Choset, A A Rizzi, P N Atkar, D Hull, The international journal of robotics research. 2142002</p>
<p>Spiral-stc: An on-line coverage algorithm of grid environments by a mobile robot. Y Gabriely, E Rimon, Proceedings 2002 IEEE International Conference on Robotics and Automation (Cat. No. 02CH37292). 2002 IEEE International Conference on Robotics and Automation (Cat. No. 02CH37292)IEEE20021</p>
<p>Bsa: A complete coverage algorithm,'' in proceedings of the. E Gonzalez, O Alvarez, Y Diaz, C Parra, C Bustacara, IEEE. 2005. 2005IEEE</p>
<p>A frontier-based approach for autonomous exploration. B Yamauchi, Proceedings 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA'97.'Towards New Computational Principles for Robotics and Automation. 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA'97.'Towards New Computational Principles for Robotics and AutomationIEEE1997</p>
<p>Autonomous robotic exploration based on multiple rapidly-exploring randomized trees. H Umari, S Mukhopadhyay, 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2017</p>
<p>Smmrexplore: Submap-based multi-robot exploration system with multi-robot multi-target potential field exploration method. J Yu, J Tong, Y Xu, Z Xu, H Dong, T Yang, Y Wang, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE2021</p>
<p>Adaptive deep path: Efficient coverage of a known environment under various configurations. X Chen, T M Tucker, T R Kurfess, R Vuduc, 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2019</p>
<p>Coverage path planning optimization based on q-learning algorithm. L Piardi, J Lima, A I Pereira, P Costa, AIP Conference Proceedings. 21162200022019AIP Publishing LLC</p>
<p>Coverage path planning for decomposition reconfigurable grid-maps using deep reinforcement learning based travelling salesman problem. P T Kyaw, A Paing, T T Thu, R E Mohan, A V Le, P Veerajagadheswar, IEEE Access. 82020</p>
<p>Deep reinforcement learning robot for search and rescue applications: Exploration in unknown cluttered environments. F Niroui, K Zhang, Z Kashino, G Nejat, IEEE Robotics and Automation Letters. 422019</p>
<p>Voronoi-based multirobot autonomous exploration in unknown environments via deep reinforcement learning. J Hu, H Niu, J Carrasco, B Lennox, F Arvin, Transactions on Vehicular Technology. 69124232020</p>
<p>Efficient coverage path planning in initially unknown environments using graph representation. O Saha, V Ganapathy, J Heydari, G Ren, M Shah, 2021 20th International Conference on Advanced Robotics (ICAR). IEEE2021</p>
<p>Learned map prediction for enhanced mobile robot exploration. R Shrestha, F.-P Tian, W Feng, P Tan, R Vaughan, ' , 2019 International Conference on Robotics and Automation (ICRA). IEEE2019</p>
<p>Reinforcement learning-based coverage path planning with implicit cellular decomposition. J Heydari, O Saha, V Ganapathy, ' , arXiv:2110.090182021arXiv preprint</p>
<p>Deep reinforcement learning based online area covering autonomous robot. O Saha, G Ren, J Heydari, V Ganapathy, M Shah, 2021 7th International Conference on Automation, Robotics and Applications (ICARA). IEEE2021</p>
<p>Planning hybrid driving-stepping locomotion on multiple levels of abstraction. T Klamt, S Behnke, 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE2018</p>
<p>Sim-to-real transfer in deep reinforcement learning for robotics: a survey. W Zhao, J P Queralta, T Westerlund, 2020 IEEE symposium series on computational intelligence (SSCI). IEEE2020</p>
<p>Sim-to-real transfer of accurate grasping with eye-in-hand observations and continuous control,'' Workshop on Acting and Interacting in the Real World. M Yan, I Frosio, S Tyree, J Kautz, Advances in Neural Information Processing Systems. 2017</p>
<p>Quasi-online reinforcement learning for robots. B Bakker, V Zhumatiy, G Gruener, J Schmidhuber, Proceedings 2006 IEEE International Conference on Robotics and Automation. 2006 IEEE International Conference on Robotics and AutomationIEEE2006. 2006. 2006</p>
<p>Real-time reinforcement learning. S Ramstedt, C , Advances in neural information processing systems. 201932</p>
<p>Learning to walk via deep reinforcement learning. T Haarnoja, S Ha, A Zhou, J Tan, G Tucker, S Levine, Proceedings of Robotics: Science and Systems. Robotics: Science and SystemsFreiburgimBreisgau, GermanyJune 2019</p>
<p>Real-time reinforcement learning for vision-based robotics utilizing local and remote computers. Y Wang, G Vasan, A R Mahmood, 2023 IEEE International Conference on Robotics and Automation (ICRA). </p>
<p>Learning exploration policies for navigation. T Chen, S Gupta, A Gupta, International Conference on Learning Representations. 2019</p>
<p>Uav coverage path planning under varying power constraints using deep reinforcement learning. M Theile, H Bayerlein, R Nai, D Gesbert, M Caccamo, 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2020</p>
<p>Learning to explore using active neural slam,'' in International Conference on Learning Representations. D S Chaplot, D Gandhi, S Gupta, A Gupta, R Salakhutdinov, 2020</p>
<p>Nonlinear total variation based noise removal algorithms. L I Rudin, S Osher, E Fatemi, Physica D: nonlinear phenomena. 199260</p>
<p>Stable-baselines3: Reliable reinforcement learning implementations. A Raffin, A Hill, A Gleave, A Kanervisto, M Ernestus, N Dormann, Journal of Machine Learning Research. 222682021</p>
<p>Tianshou: A highly modularized deep reinforcement learning library. J Weng, H Chen, D Yan, K You, A Duburcq, M Zhang, Y Su, H Su, J Zhu, Journal of Machine Learning Research. 232672022</p>
<p>Spinning Up in Deep Reinforcement Learning. J Achiam, 2018</p>
<p>Playing atari with deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A Graves, I Antonoglou, D Wierstra, M Riedmiller, arXiv:1312.56022013arXiv preprint</p>
<p>Husqvarna, Husqvarna research platform. 2017</p>
<p>Qualisys motion capture system. Qualisys, 2023</p>
<p>A formal basis for the heuristic determination of minimum cost paths. P E Hart, N J Nilsson, B Raphael, IEEE transactions on Systems Science and Cybernetics. 19684</p>
<p>Indoor coverage path planning: Survey, implementation, analysis. R Bormann, F Jordan, J Hampp, M Hägele, 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE2018</p>
<p>Searching for mobilenetv3. A Howard, M Sandler, G Chu, L.-C Chen, B Chen, M Tan, W Wang, Y Zhu, R Pang, V Vasudevan, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)2019</p>
<p>Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2016</p>
<p>A convnet for the 2020s. Z Liu, H Mao, C.-Y Wu, C Feichtenhofer, T Darrell, S Xie, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2022986</p>
<p>Swin transformer: Hierarchical vision transformer using shifted windows. Z Liu, Y Lin, Y Cao, H Hu, Y Wei, Z Zhang, S Lin, B Guo, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)20211022</p>
<p>Swin transformer v2: Scaling up capacity and resolution. Z Liu, H Hu, Y Lin, Z Yao, Z Xie, Y Wei, J Ning, Y Cao, Z Zhang, L Dong, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)202219</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. A Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, T Unterthiner, M Dehghani, M Minderer, G Heigold, S Gelly, J Uszkoreit, N Houlsby, International Conference on Learning Representations (ICLR). 2021</p>
<p>Curriculum learning. Y Bengio, J Louradour, R Collobert, J Weston, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learning2009</p>
<p>Curriculum learning for reinforcement learning domains: A framework and survey. S Narvekar, B Peng, M Leonetti, J Sinapov, M E Taylor, P Stone, The Journal of Machine Learning Research. 2112020</p>
<p>Illuminating generalization in deep reinforcement learning through procedural level generation. N Justesen, R R Torrado, P Bontrager, A Khalifa, J Togelius, S Risi, NeurIPS Workshop on Deep Reinforcement Learning. 2018</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4825 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4825</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4825</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-103.html">extraction-schema-103</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <p><strong>Paper ID:</strong> paper-261582775</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2309.03736v1.pdf" target="_blank">TradingGPT: Multi-Agent System with Layered Memory and Distinct Characters for Enhanced Financial Trading Performance</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs), prominently highlighted by the recent evolution in the Generative Pre-trained Transformers (GPT) series, have displayed significant prowess across various domains, such as aiding in healthcare diagnostics and curating analytical business reports. The efficacy of GPTs lies in their ability to decode human instructions, achieved through comprehensively processing historical inputs as an entirety within their memory system. Yet, the memory processing of GPTs does not precisely emulate the hierarchical nature of human memory. This can result in LLMs struggling to prioritize immediate and critical tasks efficiently. To bridge this gap, we introduce an innovative LLM multi-agent framework endowed with layered memories. We assert that this framework is well-suited for stock and fund trading, where the extraction of highly relevant insights from hierarchical financial data is imperative to inform trading decisions. Within this framework, one agent organizes memory into three distinct layers, each governed by a custom decay mechanism, aligning more closely with human cognitive processes. Agents can also engage in inter-agent debate. In financial trading contexts, LLMs serve as the decision core for trading agents, leveraging their layered memory system to integrate multi-source historical actions and market insights. This equips them to navigate financial changes, formulate strategies, and debate with peer agents about investment decisions. Another standout feature of our approach is to equip agents with individualized trading traits, enhancing memory diversity and decision robustness. These sophisticated designs boost the system's responsiveness to historical trades and real-time market signals, ensuring superior automated trading accuracy.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4825.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4825.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TradingGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TRADINGGPT: Multi-agent system with layered memory and distinct characters for enhanced financial trading performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-powered multi-agent trading framework that endows each agent with human-inspired layered memories (short-, middle-, long-term), individualized trading characters, inter-agent debate, and vector-based retrieval to make and execute trading decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>TradingGPT multi-agent trading agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A system of LLM-driven agents (prompts implemented with GPT-3.5-turbo in the current stage) where each agent has an LLM core, a distinct risk/investment character, immediate and extended reflection mechanisms, and participates in single-agent and multi-agent (debate) workflows to generate trading recommendations and execute trades.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Layered episodic memory + vector database (retrieval-augmented memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Memories are explicitly partitioned into three layers (short-term, middle-term, long-term). Each memory event is embedded and stored in a FAISS vector index for semantic retrieval. Retrieval ranking per layer uses a computed score combining: recency (exponential decay with layer-specific stability constants Q_short=3, Q_middle=90, Q_long=365), relevancy (cosine similarity between event and prompt embeddings), and importance (layer-specific constant). Scores are min-max normalized and linearly combined with layer-specific weights; thresholds govern retention and layer transitions; an add-counter promotes frequently triggered events to longer-term retention. Immediate and extended reflections and debate feedback are stored in dedicated indices in the Agents' Cognition Schema.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Automated financial trading (stock & fund trading / portfolio decisions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Agents use multi-modal market data (prices, news transcriptions, ARK fund historical trades, etc.) to generate five-level trading recommendations (significantly increase, slightly increase, hold, slightly decrease, significantly decrease), justify decisions via retrieved memories and reflections, and execute simulated or backtested trades. Training uses historical ARK trades and market data (Aug 15, 2020–Feb 15, 2023) and testing runs to Aug 15, 2023 (testing omits ARK ground-truth trades).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ARK fund historical trading records (used for training/backtest) — no standard external benchmark reported</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>The paper argues that layered, decayed memory organized by recency, relevancy, and importance yields more nuanced retrieval and prioritization than treating all memories as a single pool (as in prior generative-agent approaches). No empirical numeric comparisons or ablation results are reported yet; planned ablation studies are described but not executed.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No quantitative performance results or controlled ablations published in this paper; current implementation/experiments are at a prompt-design stage using GPT-3.5-turbo and ablation studies are pending. The approach relies on multiple hyperparameters (decay/stability Qs, layer weights, thresholds, importance constants) that require tuning; testing omits representative fund ground-truth data, which may affect evaluation realism. The paper does not report empirical failure cases or retrieval error rates.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Organizing agent memory into human-inspired layered stores with layer-specific decay and combining recency, relevancy (embedding cosine similarity), and importance produces a prioritized retrieval mechanism tailored to trading; using a vector index (FAISS) enables fast semantic retrieval; adding counters for frequently-triggered events allows important repeated occurrences to persist into longer-term memory; inter-agent debate and character diversity can broaden perspectives and improve robustness of trading decisions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4825.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4825.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generative Agents (Park et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative agents: Interactive simulacra of human behavior</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior multi-agent LLM framework where individual agents maintain separate memory streams and character profiles; agents record experiences, rank memories by recency/significance/relevance, and collaborate via natural language to solve tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative agents: Interactive simulacra of human behavior</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Generative Agents (Park et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Multi-agent LLM system in which each agent holds its own memory archive (seed memories and experiences), maintains a character profile, ranks and retrieves memories by recency, relevance, and significance to respond to prompts and coordinate with other agents.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Per-agent episodic memory streams with weighted retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Memories are stored per agent and retrieved by computing weighted retrieval scores based on recency, significance, and relevance; top-ranked memories are fed into the LLM to produce agent behavior and collaborative planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General interactive agent tasks and collaborative planning / behavior simulation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Agents perform simulated human-like tasks by recalling and prioritizing past experiences; the design is used as prior work to motivate TradingGPT's memory structuring.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Park et al.'s weighted retrieval across an agent's memories inspired TradingGPT; TradingGPT modifies this approach by explicitly splitting memories into three hierarchical layers with separate decay/stability parameters and layer-specific thresholds. No numeric comparison is provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>As discussed in this paper, Park et al.'s approach ranks retrieval from all memories as a single pool, which TradingGPT authors argue is less aligned with human hierarchical memory and may be less effective at prioritizing immediate critical events for trading.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Per-agent memory streams and ranking by recency/significance/relevance enable agents to retrieve contextually important events; this concept underpins TradingGPT's layered-memory refinement.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4825.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4825.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-agent Debate (Du et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Improving Factuality and Reasoning in Language Models through Multiagent Debate</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A debate mechanism where multiple LLM instances propose and contest arguments to improve reasoning and factual accuracy; applied conceptually in TradingGPT to enable agents to debate trading decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving Factuality and Reasoning in Language Models through Multiagent Debate</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Multi-agent Debate (Du et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A framework that runs multiple LLM instances/agents that propose competing arguments and iteratively converge to a more accurate or better-reasoned answer via debate phases.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Debate-mediated memory exchange (agents present stored memories/reflections during debate)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>In TradingGPT, agents share their top-K layered memories and immediate reflections during debates; peer feedback from debates is stored in a debate class of the Agents' Cognition Schema for later use.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Cooperative decision-making and improved factual/reasoned outputs (applied to trading debates)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Agents trading the same stock (but with different sector expertise/characters) exchange top-K memories, recommendations, trade values and returns in a debate to solicit peer feedback and refine trading choices.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>The paper cites Du et al. to motivate integrating a debate phase for inter-agent interaction; TradingGPT implements debates where agents present layered-memory outputs, but no quantitative evaluation of debate vs. no-debate performance is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>The paper does not report empirical limitations specific to debate integration; no quantitative evidence is provided regarding how debate affects trading performance or failure modes related to debate (e.g., reinforcement of incorrect beliefs).</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Incorporating debate phases where agents exchange memory-grounded arguments can enhance cooperative decision-making and reasoning; storing debate feedback as memory allows agents to learn from peer interactions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generative agents: Interactive simulacra of human behavior <em>(Rating: 2)</em></li>
                <li>Improving Factuality and Reasoning in Language Models through Multiagent Debate <em>(Rating: 2)</em></li>
                <li>Billion-scale similarity search with GPUs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4825",
    "paper_id": "paper-261582775",
    "extraction_schema_id": "extraction-schema-103",
    "extracted_data": [
        {
            "name_short": "TradingGPT",
            "name_full": "TRADINGGPT: Multi-agent system with layered memory and distinct characters for enhanced financial trading performance",
            "brief_description": "An LLM-powered multi-agent trading framework that endows each agent with human-inspired layered memories (short-, middle-, long-term), individualized trading characters, inter-agent debate, and vector-based retrieval to make and execute trading decisions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "TradingGPT multi-agent trading agent",
            "agent_description": "A system of LLM-driven agents (prompts implemented with GPT-3.5-turbo in the current stage) where each agent has an LLM core, a distinct risk/investment character, immediate and extended reflection mechanisms, and participates in single-agent and multi-agent (debate) workflows to generate trading recommendations and execute trades.",
            "memory_type": "Layered episodic memory + vector database (retrieval-augmented memory)",
            "memory_description": "Memories are explicitly partitioned into three layers (short-term, middle-term, long-term). Each memory event is embedded and stored in a FAISS vector index for semantic retrieval. Retrieval ranking per layer uses a computed score combining: recency (exponential decay with layer-specific stability constants Q_short=3, Q_middle=90, Q_long=365), relevancy (cosine similarity between event and prompt embeddings), and importance (layer-specific constant). Scores are min-max normalized and linearly combined with layer-specific weights; thresholds govern retention and layer transitions; an add-counter promotes frequently triggered events to longer-term retention. Immediate and extended reflections and debate feedback are stored in dedicated indices in the Agents' Cognition Schema.",
            "task_name": "Automated financial trading (stock & fund trading / portfolio decisions)",
            "task_description": "Agents use multi-modal market data (prices, news transcriptions, ARK fund historical trades, etc.) to generate five-level trading recommendations (significantly increase, slightly increase, hold, slightly decrease, significantly decrease), justify decisions via retrieved memories and reflections, and execute simulated or backtested trades. Training uses historical ARK trades and market data (Aug 15, 2020–Feb 15, 2023) and testing runs to Aug 15, 2023 (testing omits ARK ground-truth trades).",
            "benchmark_name": "ARK fund historical trading records (used for training/backtest) — no standard external benchmark reported",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "The paper argues that layered, decayed memory organized by recency, relevancy, and importance yields more nuanced retrieval and prioritization than treating all memories as a single pool (as in prior generative-agent approaches). No empirical numeric comparisons or ablation results are reported yet; planned ablation studies are described but not executed.",
            "limitations_or_challenges": "No quantitative performance results or controlled ablations published in this paper; current implementation/experiments are at a prompt-design stage using GPT-3.5-turbo and ablation studies are pending. The approach relies on multiple hyperparameters (decay/stability Qs, layer weights, thresholds, importance constants) that require tuning; testing omits representative fund ground-truth data, which may affect evaluation realism. The paper does not report empirical failure cases or retrieval error rates.",
            "key_insights": "Organizing agent memory into human-inspired layered stores with layer-specific decay and combining recency, relevancy (embedding cosine similarity), and importance produces a prioritized retrieval mechanism tailored to trading; using a vector index (FAISS) enables fast semantic retrieval; adding counters for frequently-triggered events allows important repeated occurrences to persist into longer-term memory; inter-agent debate and character diversity can broaden perspectives and improve robustness of trading decisions.",
            "uuid": "e4825.0"
        },
        {
            "name_short": "Generative Agents (Park et al.)",
            "name_full": "Generative agents: Interactive simulacra of human behavior",
            "brief_description": "A prior multi-agent LLM framework where individual agents maintain separate memory streams and character profiles; agents record experiences, rank memories by recency/significance/relevance, and collaborate via natural language to solve tasks.",
            "citation_title": "Generative agents: Interactive simulacra of human behavior",
            "mention_or_use": "mention",
            "agent_name": "Generative Agents (Park et al.)",
            "agent_description": "Multi-agent LLM system in which each agent holds its own memory archive (seed memories and experiences), maintains a character profile, ranks and retrieves memories by recency, relevance, and significance to respond to prompts and coordinate with other agents.",
            "memory_type": "Per-agent episodic memory streams with weighted retrieval",
            "memory_description": "Memories are stored per agent and retrieved by computing weighted retrieval scores based on recency, significance, and relevance; top-ranked memories are fed into the LLM to produce agent behavior and collaborative planning.",
            "task_name": "General interactive agent tasks and collaborative planning / behavior simulation",
            "task_description": "Agents perform simulated human-like tasks by recalling and prioritizing past experiences; the design is used as prior work to motivate TradingGPT's memory structuring.",
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Park et al.'s weighted retrieval across an agent's memories inspired TradingGPT; TradingGPT modifies this approach by explicitly splitting memories into three hierarchical layers with separate decay/stability parameters and layer-specific thresholds. No numeric comparison is provided in this paper.",
            "limitations_or_challenges": "As discussed in this paper, Park et al.'s approach ranks retrieval from all memories as a single pool, which TradingGPT authors argue is less aligned with human hierarchical memory and may be less effective at prioritizing immediate critical events for trading.",
            "key_insights": "Per-agent memory streams and ranking by recency/significance/relevance enable agents to retrieve contextually important events; this concept underpins TradingGPT's layered-memory refinement.",
            "uuid": "e4825.1"
        },
        {
            "name_short": "Multi-agent Debate (Du et al.)",
            "name_full": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
            "brief_description": "A debate mechanism where multiple LLM instances propose and contest arguments to improve reasoning and factual accuracy; applied conceptually in TradingGPT to enable agents to debate trading decisions.",
            "citation_title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
            "mention_or_use": "mention",
            "agent_name": "Multi-agent Debate (Du et al.)",
            "agent_description": "A framework that runs multiple LLM instances/agents that propose competing arguments and iteratively converge to a more accurate or better-reasoned answer via debate phases.",
            "memory_type": "Debate-mediated memory exchange (agents present stored memories/reflections during debate)",
            "memory_description": "In TradingGPT, agents share their top-K layered memories and immediate reflections during debates; peer feedback from debates is stored in a debate class of the Agents' Cognition Schema for later use.",
            "task_name": "Cooperative decision-making and improved factual/reasoned outputs (applied to trading debates)",
            "task_description": "Agents trading the same stock (but with different sector expertise/characters) exchange top-K memories, recommendations, trade values and returns in a debate to solicit peer feedback and refine trading choices.",
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "The paper cites Du et al. to motivate integrating a debate phase for inter-agent interaction; TradingGPT implements debates where agents present layered-memory outputs, but no quantitative evaluation of debate vs. no-debate performance is reported.",
            "limitations_or_challenges": "The paper does not report empirical limitations specific to debate integration; no quantitative evidence is provided regarding how debate affects trading performance or failure modes related to debate (e.g., reinforcement of incorrect beliefs).",
            "key_insights": "Incorporating debate phases where agents exchange memory-grounded arguments can enhance cooperative decision-making and reasoning; storing debate feedback as memory allows agents to learn from peer interactions.",
            "uuid": "e4825.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior",
            "rating": 2
        },
        {
            "paper_title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
            "rating": 2
        },
        {
            "paper_title": "Billion-scale similarity search with GPUs",
            "rating": 1
        }
    ],
    "cost": 0.009651999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>TRADINGGPT: MULTI-AGENT SYSTEM WITH LAYERED MEMORY AND DISTINCT CHARACTERS FOR ENHANCED FINANCIAL TRADING PERFORMANCE</p>
<p>Yang Li 
School of Business
Stevens Institute of Technology Hoboken
NJUnited States</p>
<p>Yangyang Yu 
School of Business
Stevens Institute of Technology Hoboken
NJUnited States</p>
<p>Haohang Li 
School of Business
Stevens Institute of Technology Hoboken
NJUnited States</p>
<p>Zhi Chen 
School of Business
Stevens Institute of Technology Hoboken
NJUnited States</p>
<p>Khaldoun Khashanah kkhashan@stevens.edu 
School of Business
Stevens Institute of Technology Hoboken
NJUnited States</p>
<p>TRADINGGPT: MULTI-AGENT SYSTEM WITH LAYERED MEMORY AND DISTINCT CHARACTERS FOR ENHANCED FINANCIAL TRADING PERFORMANCE
Financial AIMulti-Modal LearningTrading AlgorithmsDeep LearningFinancial Technology
Large Language Models (LLMs), prominently highlighted by the recent evolution in the Generative Pre-trained Transformers (GPT) series, have displayed significant prowess across various domains, such as aiding in healthcare diagnostics and curating analytical business reports. The efficacy of GPTs lies in their ability to decode human instructions, achieved through comprehensively processing historical inputs as an entirety within their memory system. Yet, the memory processing of GPTs does not precisely emulate the hierarchical nature of human memory, which is categorized into long, medium, and short-term layers. This can result in LLMs struggling to prioritize immediate and critical tasks efficiently. To bridge this gap, we introduce an innovative LLM multi-agent framework endowed with layered memories. We assert that this framework is well-suited for stock and fund trading, where the extraction of highly relevant insights from hierarchical financial data is imperative to inform trading decisions. Within this framework, one agent organizes memory into three distinct layers, each governed by a custom decay mechanism, aligning more closely with human cognitive processes. Agents can also engage in inter-agent communication and debate. In financial trading contexts, LLMs serve as the decision core for trading agents, leveraging their layered memory system to integrate multi-source historical actions and market insights. This equips them to navigate financial changes, formulate strategies, and debate with peer agents about investment decisions. Another standout feature of our approach is to enable agents with individualized trading characters, which enrich the diversity of their highlighted essential memories and improve decision-making robustness. By leveraging agents' layered memory processing and consistent information interchange, the entire trading system demonstrates augmented adaptability to historical trades and real-time market cues. This synergistic approach guarantees premier automated trading with heightened execution accuracy.</p>
<p>Introduction</p>
<p>As the influx of diverse data streams continues to rise, there is a growing need for individuals to effectively harness information. This trend is particularly pronounced in the realm of finance, where traders must consider multiple sources to inform their investment decisions. In light of this demand, researchers design intelligent trading robot-agents that can synthesize and interpret data objectively [14,5]. These robot-agents harness diverse machine algorithms, assimilate a broader spectrum of data, autonomously refine trading strategies via methodical planning, and even potentially collaborate [7]. Here, we introduce an advanced LLM-powered multi-agent trading agent framework, supported by layered memories and customized characters. By employing a collaborative multi-agent system and capturing the intricate market dynamics from varied perspectives, this approach significantly enhances automated trading outcomes. This approach substantially elevates the performance of automated trading by fostering collaborative interactions among agents and capturing the intricate dynamics of the market from diverse perspectives.</p>
<p>Previous studies have introduced multi-agent trading algorithms that employ machine learning techniques, such as reinforcement learning and have reported significant performance outcomes [5]. Yet, these methods exhibit limitations in precisely identifying, representing, and emulating crucial components of trading systems. This includes aspects like agents' memory archives and the evolving social interplay among agents.</p>
<p>LLMs, with a particular focus on their recent advancements, such as the Generative Pre-trained Transformer (GPT), have demonstrated remarkable effectiveness in enhancing human decision-making across various domains [9]. Notably, a growing body of research has focused on harnessing this technology to make informed trading decisions for stocks and funds by continuously interacting with financial environment information [17,16]. While current financial LLM applications predominantly operate within single-agent systems based on textual uni-modality, their immense potential to elevate trading performance is becoming increasingly evident. Moreover, these financial agent systems make trading decisions relying solely on pre-trained LLMs or a memory system processing received information streams as an entirety. This can lead to a challenge for LLMs in efficiently prioritizing immediate and critical memory events for optimized trading.</p>
<p>Park et al. [10] recently introduced a generative agent framework aiming to enhance the efficient retrieval of critical events from agents empowered by LLMs. This structure comprises several agents, each distinguished by separate memory streams and unique character profiles configured by LLMs. Each agent, owning its seed memories, not only tracks its actions but also monitors other agents and environmental behaviors. Faced with a task, agents sift through memory segments to input into the language model, ranking them by recency, significance, and relevance. By archiving an agent's experiences, the system integrates individual weighted memories and the nuances of group dynamics. As a result, agents can collaboratively strategize, leveraging their collective knowledge. Moreover, Du et al. [3] presented a debate mechanism for LLM agents, emphasizing enhanced cooperative decision-making through debate phases in inter-agent memory interactions. These advancements align the LLM-driven multi-agent system more with human memory structures, paving the way for a more adept financial automated trading system.</p>
<p>Leveraging the capabilities of LLMs, we propose a novel trading agent framework, "TradingGPT". It offers a realistic scenario simulation through the integration of the trader's layered memory streams and character analysis. This framework is characterized by remarkable self-enhancement ability and performance to conduct automated trading and optimal execution. The primary contributions of our work include:</p>
<p>This represents a pioneering multi-agent trading system that integrates memory streams and debate mechanisms, anchored on LLMs. Building on Park et al.'s weighted memory mechanisms, our system innovatively categorizes the agent's memories into short-term, middle-term, and long-term layers, which are closely aligned with the structure of the human cognitive system. We adapt this layered memory framework to the financial trading system, equipping agents to reflect on past and present events, derive insights from trading performance, and leverage collective wisdom for future decisions. This approach improves the system's robustness.</p>
<p>This marks the debut of the LLM agent trading system that incorporates the character design. The design assigns agents with different varying risk preferences, such as risk-seeking, risk-neutral, and risk-averse, and various investment subscopes across industries. This design enables these collaborative agents to resonate more with human intuition and possess the potential to uncover latent market opportunities.</p>
<p>Our trading system also integrates real-time multi-modal data from diverse information sources, offering a comprehensive view of the financial landscape by encompassing both macro and micro perspectives, as well as historical trading records. With updates available on both daily and minute-by-minute frequencies, our system ensures prompt reactions to daily trades and offers the capability for high-frequency trading.</p>
<p>In this paper, we commence with an in-depth exposition of TradingGPT. We then present multi-modal datasets for the effective training of TradingGPT. We methodically evaluate the pivotal components of the system, illustrating their ability to yield notable results. We prospect that, when deployed on representative fund firms like ARK, TradingGPT will markedly outperform other automated trading strategies.</p>
<p>Related Work</p>
<p>Large language models (LLMs)</p>
<p>The evolution of LLMs has reshaped artificial intelligence and natural language processing. From foundational embeddings like Word2Vec [4] and GloVe [11], the field advanced with the introduction of BERT [2]. Today, the new-generation LLMs, like Generative Pre-trained Transformer series (GPTs) [12,9] and Large Language Model Meta AI (Llamas) [15], demonstrate expressive proficiency across diverse applications.</p>
<p>Generative agent system with memory streams and customized character design</p>
<p>Park et al. [10] introduced generative agents' memory streams and innovatively employed character design concepts from gaming, expanding LLM capabilities for the multi-agent system [13]. In their design, agents display human-like behaviors while retaining individual characters. They dynamically interact with peers and their environment, forging memories and relationships. Moreover, these agents coordinate collaborative tasks through natural language, creating a captivating fusion of artificial intelligence and interactive design.</p>
<p>Multi-agent debate mechanism</p>
<p>Du et al. [3] introduced a debate mechanism leveraging multiple language models in a multi-agent system. Within this framework, various model instances propose debate and collaboratively converge to a unified answer. This approach bolsters mathematical and strategic reasoning while enhancing the factual accuracy of the generated content. </p>
<p>Dataset and Database Structure</p>
<p>For TradingGPT's development, we systematically integrated an extensive array of multi-modal financial data from August 15, 2020, to August 15, 2023. These datasets were sourced from financial databases and APIs, exemplified by the Databento Stock Price Database, Alpaca News API, publicly available daily holdings history records from ARK, etc. This data serves two purposes: (a) to formulate multi-layer memories for agents, and (b) to train, guide, and back-test the agents using ARK funds' historical trading records, refining their trading decisions and actions. In our study, we employed FAISS [6], an open-source vector database, due to its capacity to store data as high-dimensional vectors, enabling semantic searches based on exact matches. Two primary reasons informed our decision: (a) The majority of our data, including audio transcriptions from ARK Invest videos (translated to texts via the Whisper API), benefits from FAISS's unique underlying structure to fast query data. (b) FAISS's compatibility incorporating OpenAI and efficient computation of cosine similarities for specific tickers. the Raw Input schema. This data is then channeled into the Agents' Cognition Schema, guided by both the system's foundational logic and LLM-agent processing. A comprehensive schema structure is in Figure. 1.</p>
<p>Proposed Method</p>
<p>Our methodology integrates LLM across multiple facets of the trading agent workflow. Details and associated notation are provided in the subsequent sections.</p>
<p>Trading Agents Layered Generative Memory Formulation</p>
<p>In our LLM-based trading system, agents autonomously manage their actions and memory trajectories, engaging in communication and deliberation as needed.</p>
<p>Layered-memory structure</p>
<p>Each agent within TradingGPT discerns and categorizes perceived information into three distinct memory layers: long-term, middle-term, and short-term. Compared to the approach of extracting key insights through the computation of ranked retrieval scores from all memories in the generative agent system [10], this layered memory approach introduces a more nuanced ranking mechanism for retrieving crucial events from individual layers. This closely aligns with the human cognition proposed by Atkinson et al. [1]. Our framework initially categorizes memories into separate lists for each layer, guided by predefined rules tailored to specific situations and the nature of events. Subsequently, within each memory layer, we leverage three crucial metrics, inspired by the work of Park et al. -recency, relevancy, and importance -to establish the hierarchical arrangement of events within an agent's memory. However, we have reconstructed their mathematical representations to attain a more logical and advanced formulation.</p>
<p>For a memory event E within the memory layer i ∈ {short, middle, long}, upon the arrival of a prompt P from the LLM, the agent computes the recency score S E Recency as per Equation.</p>
<ol>
<li>This score inversely correlates with the time difference between the prompt's arrival and the event's memory timestamp, aligning with Ebbinghaus's forgetting curve on memory decay [8]. Q i Equation.1 represents the stability term, employed to control the memory decay rates across layers. A higher stability value in the long-term memory layer compared to the short-term layer suggests that memories persist longer in the former. The relevancy score S E relevancy represents the cosine similarity between the embedding vectors for the textual content of the memory event m E and the prompt query m P . The importance score S E Importance is determined using a uniform piecewise function as described in Equation.3, adhering to the relationship c short &lt; c middle &lt; c long . After normalizing their values to the [0,1] range using min-max scaling, these scores, S E Recency , S E Relevancy and S E Importance are linearly combined to produce the final ranking score γ E i for each memory layer in the Equation. 4 (equivalent to retrieval score in the study of Park et al.). In our setup, the ranking score thresholds, γ E i , are 80 for long-term, 60 for middle-term, and 40 for short-term memory. Events scoring below 20 are removed.
S E Recency = e − δ E Q i δ E = t P − t E(1)
, where Q long = 365 for long-term, Q middle = 90 for middle-term, and Q short = 3 for short-term events.
S E Relevancy = m E · m P ∥m E ∥ 2 × ∥m P ∥ 2 (2) S E Importance =    c short if short-term memory c middle if middle-term memory c long if long-term memory(3)
, where c short , c middle and c long are all constants.
γ E i = α E i × S E Recency i + β E i × S E Relevancy i + λ E i × S E Importance i (4)
where each memory event is only associated with one score, as it can only belong to one of the memory layers.</li>
</ol>
<p>To ensure dynamic interactions across memory layers, we define upper and lower thresholds for memory event ranking scores in each layer. We also utilize an add-counter function to boost the scores of events that are triggered by trading executions resulting from significant trading profits and losses. This promotes frequent events to transition from short-term to potentially longer-term memory, enhancing their retention and recall by agents. The hyperparameters α E i , β E i , and λ E i exhibit variations across different layers. The transferable layered memory system allows the agents to capture and prioritize crucial memory events by considering both their types and frequencies when conducting queries.</p>
<p>Memory formulated by individual experience</p>
<p>In the trading paradigm, macro-level market indicators are stored in the long-term memory, quarterly investment strategies are allocated to the mid-term memory, and daily investment messages are channeled into the short-term  memory. These three memory classes constitute the initial structure within the Agents' Cognition Schema of our data warehouse in Figure. 1. In our trading system, agents make informed trading decisions relying on the outcomes of two distinct workflows: the single-agent workflow and the multi-agent workflow, as depicted on the left side of Figure 2.</p>
<p>In the single-agent workflow, when presented with a specific stock ticker, agents' LLM core generates evaluations and reflections, which encompass trading recommendations and the reasons behind them, based on the essential events retrieved from their layered memory. Subsequently, the agent can proceed to execute trading actions in accordance with these generated insights. The key features that empower our system are (a) Immediate reflection: Conducted daily, this mechanism allows agents to consolidate top-ranked events of each memory layer and market facts, such as daily stock prices and ARK fund trading records. Using LLM and specific prompts, agents generate five trading recommendations: "significantly increase position", "slightly increase position", "hold", "slightly decrease position", and "significantly decrease position", with its justification. Each option is associated with a predetermined trade value. which can be adjusted to suit the business scale represented by the agents. Additionally, this reflection captures the agent's trade volumes and returns. (b)Extended reflection: This provides a broader performance overview over a designated period, like a week. It includes stock prices, the agent's trading trends, and self-evaluation. The immediate reflection guides trade execution directly, while the extended reflection acts as a supplementary reference for recalling recent investment transactions. Both types of reflections are stored in the Agents' Cognition Schema's reflection index, as shown in Figure 1, distinguished by a specific flag.</p>
<p>Memory gained by interacting with other agents</p>
<p>For stocks that appear in multiple agents' trading portfolios, TradingGPT enables inter-agent dialogue via a debate mechanism. This mechanism encourages collaboration between agents typically specializing in distinct sectors, with the goal of optimizing trading outcomes. Within these debates, agents present their top-K layered memories as well as immediate reflections, encompassing recommendations, trade values, volumes, and returns, inviting feedback from their peers. All feedback is subsequently stored in the debate class of the Agents' Cognition Schema, tagged with the receiver's index, as shown in Figure. 1.</p>
<p>Design of Training and Testing Workflows</p>
<p>The distinct design of our training and testing workflows is crucial for curating valuable past memory events and strategizing optimal future trading actions.</p>
<p>Training</p>
<p>The training process is twofold: a single-agent workflow followed by a multi-agent phase, as detailed in the left section of Fig. 2. In the single-agent phase, the LLM-driven agent is prompted with key data like stock ticker, date, and trader characters. Using this context, it evaluates top-K-ranked memories across each layer to derive preliminary investment signals, where K is a predefined hyperparameter. The LLM then synchronizes and analyzes these signals with market data, such as daily records from fund firms like ARK and stock closing prices, leading the agent to formulate an immediate reflection and trade accordingly. Subsequently, the agent collaborates in the multi-agent phase, joining debates with agents trading the same stock from varied sectors on that day (refer to 4.1.3).</p>
<p>Test</p>
<p>The testing process, illustrated in the right section of Figure. 2, blends single-agent and multi-agent operations. Both individually processed memories and insights from inter-agent exchanges are concurrently inputted into the LLM to inform trading decisions. Key differences from the training phase include: (a) During testing, agents operate without the guidance of trading records from the representative fund firm, relying solely on daily stock prices as market facts. (b) Time series patterns of prior training reflections and debates, covering a week in our setup, act as auxiliary references in the absence of substantial market ground truths, as noted in (a). Other aspects of the test workflow align with the training phase. </p>
<p>Current Stage And Future work</p>
<p>Our research consists of two phases: prompt design and ablation studies. We've crafted efficient LLM prompts using GPT3.5 turbo as the backbone. Examples of prompts that encapsulate the necessary insights for each phase of the TradingGPT training and testing workflow. The specific design of these prompts is illustrated by examples in Figure. 3.</p>
<p>With our established prompt template, we're poised to undertake ablation studies to assess the trading efficacy of agent systems based on various backbone models. This will involve comparisons within LLMs, such as GPT3.5 turbo versus CodeLlama 34B, and against models like multi-agent reinforcement learning. The training phase will utilize data spanning from August 15, 2020, to February 15, 2023, while the testing phase will extend until August 15, 2023. We'll assess performance using financial metrics like cumulative trade returns, volatility, and the Sharpe Ratio (see 4.1.2).</p>
<p>Harnessing an innovative multi-layer memory system and character design, our main goal is to establish a state-of-the-art LLM-based multi-agent automated trading system adaptable to various LLMs as its core. This system aspires to achieve superior trading performance over other leading trading agent systems by emulating human traders' cognitive behaviors and ensuring responsiveness in the constantly changing market scenario. We also posit that this LLM-based multi-agent design can improve working efficiency and collaborative performance in artificial systems across diverse sectors. Potential applications range from character development in video games to the creation of robo-consultants in business, healthcare, and technology domains.</p>
<p>Figure 1 :
1TradingGPT Data Warehouse.</p>
<p>Figure 2 :
2TradingGPT training and test workflow.</p>
<p>Figure 3 :
3Prompt template for key steps of TradingGPT workflow.
Data entities without specific timestamps are extracted as per the date displayed at the top of the plots.</p>
<p>Human memory: A proposed system and its control processes. C Richard, Richard M Atkinson, Shiffrin, Psychology of learning and motivation. Elsevier2Richard C Atkinson and Richard M Shiffrin. "Human memory: A proposed system and its control processes". In: Psychology of learning and motivation. Vol. 2. Elsevier, 1968, pp. 89-195.</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, arXiv:1810.04805arXiv preprintJacob Devlin et al. "Bert: Pre-training of deep bidirectional transformers for language understanding". In: arXiv preprint arXiv:1810.04805 (2018).</p>
<p>Improving Factuality and Reasoning in Language Models through Multiagent Debate. Yilun Du, arXiv:2305.14325arXiv preprintYilun Du et al. "Improving Factuality and Reasoning in Language Models through Multiagent Debate". In: arXiv preprint arXiv:2305.14325 (2023).</p>
<p>word2vec Explained: deriving Mikolov et al.'s negative-sampling wordembedding method. Yoav Goldberg, Omer Levy, arXiv:1402.3722arXiv preprintYoav Goldberg and Omer Levy. "word2vec Explained: deriving Mikolov et al.'s negative-sampling word- embedding method". In: arXiv preprint arXiv:1402.3722 (2014).</p>
<p>MSPM: A modularized and scalable multi-agent reinforcement learningbased system for financial portfolio management. Zhenhan Huang, Fumihide Tanaka, Plos one. 17263689Zhenhan Huang and Fumihide Tanaka. "MSPM: A modularized and scalable multi-agent reinforcement learning- based system for financial portfolio management". In: Plos one 17.2 (2022), e0263689.</p>
<p>Billion-scale similarity search with GPUs. Jeff Johnson, Matthijs Douze, Hervé Jégou, IEEE Transactions on Big Data. 7Jeff Johnson, Matthijs Douze, and Hervé Jégou. "Billion-scale similarity search with GPUs". In: IEEE Transac- tions on Big Data 7.3 (2019), pp. 535-547.</p>
<p>Adaptive quantitative trading: An imitative deep reinforcement learning approach. Yang Liu, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence34Yang Liu et al. "Adaptive quantitative trading: An imitative deep reinforcement learning approach". In: Proceed- ings of the AAAI conference on artificial intelligence. Vol. 34. 02. 2020, pp. 2128-2135.</p>
<p>Replication and analysis of Ebbinghaus' forgetting curve. M J Jaap, Joeri Murre, Dros, PloS one 10. 7120644Jaap MJ Murre and Joeri Dros. "Replication and analysis of Ebbinghaus' forgetting curve". In: PloS one 10.7 (2015), e0120644.</p>
<p>. Openai, arXiv:2303.08774GPT-4 Technical Report. 2023.cs.CLOpenAI. GPT-4 Technical Report. 2023. arXiv: 2303.08774 [cs.CL].</p>
<p>Generative agents: Interactive simulacra of human behavior. Joon Sung Park, arXiv:2304.03442arXiv preprintJoon Sung Park et al. "Generative agents: Interactive simulacra of human behavior". In: arXiv preprint arXiv:2304.03442 (2023).</p>
<p>Glove: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, Proceedings of the 2014 conference on empirical methods in natural language processing. the 2014 conference on empirical methods in natural language processingJeffrey Pennington, Richard Socher, and Christopher D Manning. "Glove: Global vectors for word representation". In: Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014, pp. 1532-1543.</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Alec Radford et al. "Improving language understanding by generative pre-training". In: (2018).</p>
<p>Interactive narrative: A novel application of artificial intelligence for computer games. Mark Riedl, Vadim Bulitko, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence26Mark Riedl and Vadim Bulitko. "Interactive narrative: A novel application of artificial intelligence for computer games". In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 26. 1. 2012, pp. 2160-2165.</p>
<p>Automatic financial trading agent for low-risk portfolio management using deep reinforcement learning. Wonsup Shin, Seok-Jun, Sung-Bae Bu, Cho, arXiv:1909.03278arXiv preprintWonsup Shin, Seok-Jun Bu, and Sung-Bae Cho. "Automatic financial trading agent for low-risk portfolio management using deep reinforcement learning". In: arXiv preprint arXiv:1909.03278 (2019).</p>
<p>Llama: Open and efficient foundation language models. Hugo Touvron, arXiv:2302.13971arXiv preprintHugo Touvron et al. "Llama: Open and efficient foundation language models". In: arXiv preprint arXiv:2302.13971 (2023).</p>
<p>Bloomberggpt: A large language model for finance. Shijie Wu, arXiv:2303.17564arXiv preprintShijie Wu et al. "Bloomberggpt: A large language model for finance". In: arXiv preprint arXiv:2303.17564 (2023).</p>
<p>FinGPT: Open-Source Financial Large Language Models. Hongyang Yang, Xiao-Yang Liu, Christina Dan Wang, arXiv:2306.06031arXiv preprintHongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. "FinGPT: Open-Source Financial Large Language Models". In: arXiv preprint arXiv:2306.06031 (2023).</p>            </div>
        </div>

    </div>
</body>
</html>
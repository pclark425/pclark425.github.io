<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9645 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9645</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9645</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-167.html">extraction-schema-167</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-271213284</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.10652v2.pdf" target="_blank">Cutting Through the Clutter: The Potential of LLMs for Efficient Filtration in Systematic Literature Reviews</a></p>
                <p><strong>Paper Abstract:</strong> Systematic literature reviews (SLRs) are essential but labor-intensive due to high publication volumes and inefficient keyword-based filtering. To streamline this process, we evaluate Large Language Models (LLMs) for enhancing efficiency and accuracy in corpus filtration while minimizing manual effort. Our open-source tool LLMSurver presents a visual interface to utilize LLMs for literature filtration, evaluate the results, and refine queries in an interactive way. We assess the real-world performance of our approach in filtering over 8.3k articles during a recent survey construction, comparing results with human efforts. The findings show that recent LLM models can reduce filtering time from weeks to minutes. A consensus scheme ensures recall rates>98.8%, surpassing typical human error thresholds and improving selection accuracy. This work advances literature review methodologies and highlights the potential of responsible human-AI collaboration in academic research.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9645.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9645.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMSurver</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMSurver: LLM-based structured literature filtration pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interactive, visual application and pipeline that uses multiple LLM agents to classify title+abstract records for systematic literature reviews (SLRs) with iterative prompt refinement, per-agent justifications, and consensus voting to produce a filtered corpus for downstream review.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMSurver (ensemble of Llama-3 8B/70B, Gemini 1.5 Flash, Claude 3.5 Sonnet, GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A pipeline that runs multiple foundation LLMs in parallel to classify papers based on a structured prompt template (role/context, task, output format, inclusion/exclusion criteria). Outputs per-paper decisions and justifications; results are combined using consensus voting and inspected/edited via a React front-end; supports registering both local/open models and remote/commercial APIs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Keyword-based retrieval across major CS repositories (ACM Digital Library, IEEE Xplore, Eurographics), metadata unified and deduplicated; title+abstract used for zero-shot classification; human-ground-truth created by manual screening.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>8323</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>A survey topic derived from 'Visual Network Analysis in Immersive Environments' — papers about immersive technologies (VR/AR/etc.) applied to graph / network data; broadly: literature filtration for an SLR on that domain.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Zero-shot/fully contextualized prompting per paper (prompt template including role and inclusion/exclusion criteria), iterative human-in-the-loop prompt refinement, multiple independent LLM classifications per paper, and consensus voting: a paper is discarded only if all chosen LLMs vote to discard it and included if at least one includes it; per-decision justifications are generated and presented for human review.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured classification (INCLUDE / EXCLUDE) per paper with textual justification, consensus decision sets, agreement visualizations, and exportable CSV of filtered corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Reduced candidate set from 8,323 papers to ~860 candidate papers (≈90% reduction in manual search space); consensus voting reduced false positives from 774 to 167 and missed only 1 of 88 ground-truth inclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Comparison to human-ground-truth manual screening (PRISMA-style preparation). Reported confusion counts (TP/FP/TN/FN) and standard metrics (accuracy, precision, recall, F1) per model and for consensus schemes; manual inspection of edge-cases.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Individual LLMs achieved >90% accuracy overall; open models (e.g., Llama-3 8B) were conservative with very high recall (Llama-3 8B recall = 97.73%) but low precision, while top commercial models had higher precision but more false negatives. Consensus (Best) (Gemini 1.5 Flash + Claude 3.5 Sonnet + GPT-4o) achieved very high recall (≈98.86%) and substantially reduced false positives; consensus discarded only one ground-truth-included paper.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Very large speedup and cost efficiency (e.g., GPT-4o processed ~4.43M input tokens and ~0.44M output tokens for 8,323 papers in under 10 minutes for $28.81), strong recall, scalable across large corpora, consistent textual rationales per decision, multilingual capability, interactive prompt refinement and human-in-the-loop oversight, and open-source tooling for reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Evaluation on a single corpus and single main prompt limits generalizability; sensitivity to prompt design and corpus characteristics; risk of model hallucinations and biases from training data and RLHF; commercial model costs, rate limits and access constraints; no controlled user study reported; approach limited to classification (search was not LLM-driven) to reduce risk of fabricated references.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>One paper was unanimously misclassified (lost) by all agents; different LLMs made different errors (e.g., Llama-3 8B tended to include more false positives, some commercial models produced more false negatives), necessitating human review; human reviewers also reclassified 34 papers during manual process, indicating ambiguity in borderline cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cutting Through the Clutter: The Potential of LLMs for Efficient Filtration in Systematic Literature Reviews', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9645.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9645.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3 (8B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta Llama-3 8B (instruct Q8_0 variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source LLM used as a conservative classifier in the pipeline; tended to favor inclusion (high recall) at the expense of precision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>meta-llama-3-8b-instruct.Q8_0</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An open-source Llama-3 instruct-tuned model variant run locally/registered in the pipeline; used with the shared prompt template to classify title+abstract pairs and produce justifications.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Same SLR corpus of 8,323 candidate papers (title+abstract), sourced from ACM, IEEE Xplore, and Eurographics; zero-shot classification per record.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>8323</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Filtering for relevance to visual network analysis in immersive environments (topic derived from target survey).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Zero-shot instructed prompting (role, task, output format, inclusion/exclusion criteria) applied per paper; outputs included label and textual justification; part of multi-LLM consensus experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Include/Exclude labels per paper with textual justification and confidence-like behavior (implicit in wording).</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>High inclusion tendency; conservatively included many candidate papers so as not to miss relevant ones (resulting in high recall but many false positives).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compared per-paper against human ground truth; confusion counts and metric computation (accuracy, precision, recall, F1).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>High recall (reported 97.73%) but low precision (many false positives), overall accuracy >90% but low F1 due to precision deficit. Frequently sole contributor to erroneous inclusions in ensemble analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Runs locally (lower cost/confidentiality advantages), excellent recall minimizing false negatives, useful for rapid exploration without cloud costs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Low precision leading to many false positives and larger manual downstream workload; exhibited an inclusion bias that requires further investigation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Responsible for a large share of single-model false-positive inclusions; would inflate the candidate set if used alone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cutting Through the Clutter: The Potential of LLMs for Efficient Filtration in Systematic Literature Reviews', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9645.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9645.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3 (70B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta Llama-3 70B (instruct Q4_K_M variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A larger Llama-3 family model tested in the pipeline; behaved more conservatively than some commercial models but with different error characteristics than the 8B variant.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>meta-llama-3-70b-instruct.Q4_K_M</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The 70B parameter instruct-tuned Llama-3 variant used via the pipeline's model registration; applied the same prompt template as other agents to produce include/exclude classifications and justifications.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Same 8,323 paper title+abstract corpus from ACM/IEEE/Eurographics used for the SLR.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>8323</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Same SLR topic on visual network analysis in immersive environments.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Zero-shot instructed prompting per record with human-in-the-loop prompt edits; outputs included labels and textual rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Per-paper labels (include/exclude) and textual justifications.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>A mix of conservative inclusion behavior with somewhat different FP/FN balance compared to 8B variant (specific counts reported in the paper's Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compared to human-validated ground truth; standard confusion-matrix metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Overall high accuracy (>90%); performance intermediate between the lightweight 8B variant and commercial models, with different trade-offs in FP/FN rates (see paper Table 1 for counts).</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Stronger language understanding than smaller open models and can be run where compute permits; useful to include in ensembles to diversify error modes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Higher compute requirements versus the 8B variant; still exhibited non-negligible false positives/negatives depending on prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Contributed to some incorrect individual decisions that differed from other agents, highlighting ensemble value.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cutting Through the Clutter: The Potential of LLMs for Efficient Filtration in Systematic Literature Reviews', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9645.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9645.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini 1.5 Flash</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini 1.5 Flash</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial foundation model used as one of the classification agents; produced distinct error patterns (notably some false negatives) compared to open models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gemini-1.5-flash-001</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A commercially hosted Gemini model variant; used through API with the shared prompt template for zero-shot per-paper classification and justification generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5B</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Same SLR corpus of 8,323 titles+abstracts from major CS repositories; zero-shot classification per item.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>8323</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Filtering for relevance to immersive visual network analysis (the SLR topic).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Zero-shot instructed prompting per paper; included in consensus experiments and in human-in-the-loop prompt refinement cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Include/Exclude per paper with textual explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Produced several false negatives relative to some open models; often part of the 'Best' consensus subset due to balanced precision/recall.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Direct comparison to human ground truth using confusion counts and derived metrics reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Lower recall relative to top-performing commercial counterparts in some cases (more false negatives), and contributed to different erroneous exclusions than other agents; however, included in Consensus (Best) which achieved high overall recall.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Commercial quality often yields higher precision; different error profile helps ensemble performance in consensus voting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Produced more false exclusions in several cases; access and cost constraints apply as for commercial models.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Often responsible for individual false negatives; errors were largely non-overlapping with other models but required consensus to avoid losing relevant papers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cutting Through the Clutter: The Potential of LLMs for Efficient Filtration in Systematic Literature Reviews', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9645.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9645.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude 3.5 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anthropic Claude 3.5 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial LLM used as a high-performing agent in the ensemble; combined with other top models to form the 'Consensus (Best)'.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>claude-3-5-sonnet@20240620</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Anthropic's Claude 3.5 Sonnet variant called via API using the shared prompt template for per-record classification and textual justification.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Same 8,323 paper title+abstract corpus from ACM/IEEE/Eurographics used for the SLR experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>8323</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Filtering relevance to visual network analysis in immersive environments for the target SLR.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Zero-shot instructed prompting per paper; included in consensus ensembles; justifications surfaced for human review and prompt refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Label per item (include/exclude) plus justification text.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Used as part of the 'Consensus (Best)' ensemble that substantially reduced false positives while maintaining high recall.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compared to human ground truth (TP/FP/TN/FN) and standard metrics (accuracy, precision, recall, F1) in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>High performance as one of the top models; when combined in Consensus (Best) achieved very high recall and improved precision versus single-model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Strong single-model performance, complementary error modes for ensemble consensus, produces useful textual justifications.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Commercial access limitations (cost, rate limits); potential to produce false negatives if used alone.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Occasional individual misclassifications that were non-identical to other agents' errors; needed ensemble consensus to minimize risk of losing relevant papers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cutting Through the Clutter: The Potential of LLMs for Efficient Filtration in Systematic Literature Reviews', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9645.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9645.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4o (2024-05-13)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A top-performing commercial LLM in the study that processed the largest token volume in the experiment and when combined in ensembles helped produce high recall with good precision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4o-2024-05-13</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A commercial OpenAI model called via API for per-paper zero-shot classification using the standardized prompt template; generated labels and textual rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Same SLR corpus: 8,323 paper titles+abstracts from ACM/IEEE/Eurographics; used zero-shot classification across the full corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>8323</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Filtering candidate papers for a survey on visual network analysis in immersive environments.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Zero-shot prompting per record; included in Consensus (Best); measured token usage and cost for the full run.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Per-paper include/exclude labels plus textual justifications; ensemble consensus outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Processed ~4,432,169 input tokens (~532 tokens/paper incl. prompts) and produced ~443,735 output tokens (~53 tokens/paper); full run completed in under 10 minutes for $28.81 (July 2024 pricing).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compared to human ground truth with confusion-matrix-based metrics (TP/FP/TN/FN), accuracy, precision, recall, and F1; inspected edge cases manually.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>High single-model performance and contributed to the high-recall Consensus (Best) ensemble; commercial models generally had higher precision but sometimes more false negatives than conservative open models.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>High throughput, favorable cost/performance in the reported run, strong single-agent performance and complementary ensemble behavior; fast processing time enabling end-to-end SLR-scale runs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Commercial dependency (cost, rate limits, access), possible false negatives when used alone, and the usual risks of hallucination and training-data bias.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Contributed occasional false exclusions; ensemble consensus was required to avoid losing relevant literature.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cutting Through the Clutter: The Potential of LLMs for Efficient Filtration in Systematic Literature Reviews', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9645.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9645.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Consensus (Best)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Consensus (Best) ensemble voting (Gemini 1.5 Flash + Claude 3.5 Sonnet + GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A voting scheme where a paper is included if any of the selected high-performing models includes it and discarded only if all selected models discard it; chosen subset comprises models with F1>50%.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Consensus (Best): Gemini 1.5 Flash + Claude 3.5 Sonnet + GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Ensemble decision rule combining outputs of selected high-performing LLMs; uses 'include-if-any, discard-only-if-all' logic to prioritize recall while improving precision over single conservative models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Full SLR corpus of 8,323 title+abstract records; per-model outputs fed into consensus stage.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>8323</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Same SLR topic (visual network analysis in immersive environments).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Aggregate voting of individual LLM classification outputs (no additional synthesis step); human-in-the-loop prompt refinement and inspection of per-decision justifications informed ensemble selection.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Filtered candidate set (include/exclude) with improved precision and high recall; per-paper justifications available from constituent models.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Consensus (Best) reduced FP count compared to using all models (reduced manual filtering by 695 papers compared to a different ensemble) and achieved high recall while using only three models.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Comparison to human ground truth; confusion counts and aggregate metrics reported; manual inspection of remaining disagreements and edge-cases.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Consensus (Best) matched Consensus (All) on the most critical TP/FN rates, missed only one ground-truth-included paper, significantly reduced FP relative to single-model outputs, and achieved a high F1 (reported >50%).</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Substantially reduces risk of losing relevant papers while decreasing the manual screening burden; requires fewer models (cost/time savings) than larger ensembles; preserves recall near human-level error rates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Still missed one ground-truth inclusion in the experiment; ensemble decisions depend on chosen member models and their complementary error modes; may not generalize across topics without revalidation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>One relevant paper was discarded by the consensus (an edge case that even human reviewers debated); consensus can only mitigate but not entirely eliminate model-specific misjudgments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cutting Through the Clutter: The Potential of LLMs for Efficient Filtration in Systematic Literature Reviews', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Litllm: A toolkit for scientific literature review <em>(Rating: 2)</em></li>
                <li>Llassist: Simple tools for automating literature review using large language models <em>(Rating: 2)</em></li>
                <li>Large language models for literature reviews-an exemplary comparison of llmbased approaches with manual methods <em>(Rating: 2)</em></li>
                <li>Automating research synthesis with domain-specific large language model fine-tuning <em>(Rating: 2)</em></li>
                <li>Chatcite: Llm agent with human workflow guidance for comparative literature summary <em>(Rating: 2)</em></li>
                <li>Using llms to improve reproducibility of literature reviews <em>(Rating: 1)</em></li>
                <li>Streamlining the selection phase of systematic literature reviews (slrs) using ai-enabled gpt-4 assistant api <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9645",
    "paper_id": "paper-271213284",
    "extraction_schema_id": "extraction-schema-167",
    "extracted_data": [
        {
            "name_short": "LLMSurver",
            "name_full": "LLMSurver: LLM-based structured literature filtration pipeline",
            "brief_description": "An interactive, visual application and pipeline that uses multiple LLM agents to classify title+abstract records for systematic literature reviews (SLRs) with iterative prompt refinement, per-agent justifications, and consensus voting to produce a filtered corpus for downstream review.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLMSurver (ensemble of Llama-3 8B/70B, Gemini 1.5 Flash, Claude 3.5 Sonnet, GPT-4o)",
            "model_description": "A pipeline that runs multiple foundation LLMs in parallel to classify papers based on a structured prompt template (role/context, task, output format, inclusion/exclusion criteria). Outputs per-paper decisions and justifications; results are combined using consensus voting and inspected/edited via a React front-end; supports registering both local/open models and remote/commercial APIs.",
            "model_size": null,
            "input_corpus_description": "Keyword-based retrieval across major CS repositories (ACM Digital Library, IEEE Xplore, Eurographics), metadata unified and deduplicated; title+abstract used for zero-shot classification; human-ground-truth created by manual screening.",
            "input_corpus_size": 8323,
            "topic_query_description": "A survey topic derived from 'Visual Network Analysis in Immersive Environments' — papers about immersive technologies (VR/AR/etc.) applied to graph / network data; broadly: literature filtration for an SLR on that domain.",
            "distillation_method": "Zero-shot/fully contextualized prompting per paper (prompt template including role and inclusion/exclusion criteria), iterative human-in-the-loop prompt refinement, multiple independent LLM classifications per paper, and consensus voting: a paper is discarded only if all chosen LLMs vote to discard it and included if at least one includes it; per-decision justifications are generated and presented for human review.",
            "output_type": "Structured classification (INCLUDE / EXCLUDE) per paper with textual justification, consensus decision sets, agreement visualizations, and exportable CSV of filtered corpus.",
            "output_example": "Reduced candidate set from 8,323 papers to ~860 candidate papers (≈90% reduction in manual search space); consensus voting reduced false positives from 774 to 167 and missed only 1 of 88 ground-truth inclusions.",
            "evaluation_method": "Comparison to human-ground-truth manual screening (PRISMA-style preparation). Reported confusion counts (TP/FP/TN/FN) and standard metrics (accuracy, precision, recall, F1) per model and for consensus schemes; manual inspection of edge-cases.",
            "evaluation_results": "Individual LLMs achieved &gt;90% accuracy overall; open models (e.g., Llama-3 8B) were conservative with very high recall (Llama-3 8B recall = 97.73%) but low precision, while top commercial models had higher precision but more false negatives. Consensus (Best) (Gemini 1.5 Flash + Claude 3.5 Sonnet + GPT-4o) achieved very high recall (≈98.86%) and substantially reduced false positives; consensus discarded only one ground-truth-included paper.",
            "strengths": "Very large speedup and cost efficiency (e.g., GPT-4o processed ~4.43M input tokens and ~0.44M output tokens for 8,323 papers in under 10 minutes for $28.81), strong recall, scalable across large corpora, consistent textual rationales per decision, multilingual capability, interactive prompt refinement and human-in-the-loop oversight, and open-source tooling for reproducibility.",
            "limitations": "Evaluation on a single corpus and single main prompt limits generalizability; sensitivity to prompt design and corpus characteristics; risk of model hallucinations and biases from training data and RLHF; commercial model costs, rate limits and access constraints; no controlled user study reported; approach limited to classification (search was not LLM-driven) to reduce risk of fabricated references.",
            "failure_cases": "One paper was unanimously misclassified (lost) by all agents; different LLMs made different errors (e.g., Llama-3 8B tended to include more false positives, some commercial models produced more false negatives), necessitating human review; human reviewers also reclassified 34 papers during manual process, indicating ambiguity in borderline cases.",
            "uuid": "e9645.0",
            "source_info": {
                "paper_title": "Cutting Through the Clutter: The Potential of LLMs for Efficient Filtration in Systematic Literature Reviews",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Llama-3 (8B)",
            "name_full": "Meta Llama-3 8B (instruct Q8_0 variant)",
            "brief_description": "An open-source LLM used as a conservative classifier in the pipeline; tended to favor inclusion (high recall) at the expense of precision.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "meta-llama-3-8b-instruct.Q8_0",
            "model_description": "An open-source Llama-3 instruct-tuned model variant run locally/registered in the pipeline; used with the shared prompt template to classify title+abstract pairs and produce justifications.",
            "model_size": "8B",
            "input_corpus_description": "Same SLR corpus of 8,323 candidate papers (title+abstract), sourced from ACM, IEEE Xplore, and Eurographics; zero-shot classification per record.",
            "input_corpus_size": 8323,
            "topic_query_description": "Filtering for relevance to visual network analysis in immersive environments (topic derived from target survey).",
            "distillation_method": "Zero-shot instructed prompting (role, task, output format, inclusion/exclusion criteria) applied per paper; outputs included label and textual justification; part of multi-LLM consensus experiments.",
            "output_type": "Include/Exclude labels per paper with textual justification and confidence-like behavior (implicit in wording).",
            "output_example": "High inclusion tendency; conservatively included many candidate papers so as not to miss relevant ones (resulting in high recall but many false positives).",
            "evaluation_method": "Compared per-paper against human ground truth; confusion counts and metric computation (accuracy, precision, recall, F1).",
            "evaluation_results": "High recall (reported 97.73%) but low precision (many false positives), overall accuracy &gt;90% but low F1 due to precision deficit. Frequently sole contributor to erroneous inclusions in ensemble analysis.",
            "strengths": "Runs locally (lower cost/confidentiality advantages), excellent recall minimizing false negatives, useful for rapid exploration without cloud costs.",
            "limitations": "Low precision leading to many false positives and larger manual downstream workload; exhibited an inclusion bias that requires further investigation.",
            "failure_cases": "Responsible for a large share of single-model false-positive inclusions; would inflate the candidate set if used alone.",
            "uuid": "e9645.1",
            "source_info": {
                "paper_title": "Cutting Through the Clutter: The Potential of LLMs for Efficient Filtration in Systematic Literature Reviews",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Llama-3 (70B)",
            "name_full": "Meta Llama-3 70B (instruct Q4_K_M variant)",
            "brief_description": "A larger Llama-3 family model tested in the pipeline; behaved more conservatively than some commercial models but with different error characteristics than the 8B variant.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "meta-llama-3-70b-instruct.Q4_K_M",
            "model_description": "The 70B parameter instruct-tuned Llama-3 variant used via the pipeline's model registration; applied the same prompt template as other agents to produce include/exclude classifications and justifications.",
            "model_size": "70B",
            "input_corpus_description": "Same 8,323 paper title+abstract corpus from ACM/IEEE/Eurographics used for the SLR.",
            "input_corpus_size": 8323,
            "topic_query_description": "Same SLR topic on visual network analysis in immersive environments.",
            "distillation_method": "Zero-shot instructed prompting per record with human-in-the-loop prompt edits; outputs included labels and textual rationales.",
            "output_type": "Per-paper labels (include/exclude) and textual justifications.",
            "output_example": "A mix of conservative inclusion behavior with somewhat different FP/FN balance compared to 8B variant (specific counts reported in the paper's Table 1).",
            "evaluation_method": "Compared to human-validated ground truth; standard confusion-matrix metrics reported.",
            "evaluation_results": "Overall high accuracy (&gt;90%); performance intermediate between the lightweight 8B variant and commercial models, with different trade-offs in FP/FN rates (see paper Table 1 for counts).",
            "strengths": "Stronger language understanding than smaller open models and can be run where compute permits; useful to include in ensembles to diversify error modes.",
            "limitations": "Higher compute requirements versus the 8B variant; still exhibited non-negligible false positives/negatives depending on prompt.",
            "failure_cases": "Contributed to some incorrect individual decisions that differed from other agents, highlighting ensemble value.",
            "uuid": "e9645.2",
            "source_info": {
                "paper_title": "Cutting Through the Clutter: The Potential of LLMs for Efficient Filtration in Systematic Literature Reviews",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Gemini 1.5 Flash",
            "name_full": "Gemini 1.5 Flash",
            "brief_description": "A commercial foundation model used as one of the classification agents; produced distinct error patterns (notably some false negatives) compared to open models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gemini-1.5-flash-001",
            "model_description": "A commercially hosted Gemini model variant; used through API with the shared prompt template for zero-shot per-paper classification and justification generation.",
            "model_size": "1.5B",
            "input_corpus_description": "Same SLR corpus of 8,323 titles+abstracts from major CS repositories; zero-shot classification per item.",
            "input_corpus_size": 8323,
            "topic_query_description": "Filtering for relevance to immersive visual network analysis (the SLR topic).",
            "distillation_method": "Zero-shot instructed prompting per paper; included in consensus experiments and in human-in-the-loop prompt refinement cycles.",
            "output_type": "Include/Exclude per paper with textual explanation.",
            "output_example": "Produced several false negatives relative to some open models; often part of the 'Best' consensus subset due to balanced precision/recall.",
            "evaluation_method": "Direct comparison to human ground truth using confusion counts and derived metrics reported in Table 1.",
            "evaluation_results": "Lower recall relative to top-performing commercial counterparts in some cases (more false negatives), and contributed to different erroneous exclusions than other agents; however, included in Consensus (Best) which achieved high overall recall.",
            "strengths": "Commercial quality often yields higher precision; different error profile helps ensemble performance in consensus voting.",
            "limitations": "Produced more false exclusions in several cases; access and cost constraints apply as for commercial models.",
            "failure_cases": "Often responsible for individual false negatives; errors were largely non-overlapping with other models but required consensus to avoid losing relevant papers.",
            "uuid": "e9645.3",
            "source_info": {
                "paper_title": "Cutting Through the Clutter: The Potential of LLMs for Efficient Filtration in Systematic Literature Reviews",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Claude 3.5 Sonnet",
            "name_full": "Anthropic Claude 3.5 Sonnet",
            "brief_description": "A commercial LLM used as a high-performing agent in the ensemble; combined with other top models to form the 'Consensus (Best)'.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "claude-3-5-sonnet@20240620",
            "model_description": "Anthropic's Claude 3.5 Sonnet variant called via API using the shared prompt template for per-record classification and textual justification.",
            "model_size": null,
            "input_corpus_description": "Same 8,323 paper title+abstract corpus from ACM/IEEE/Eurographics used for the SLR experiments.",
            "input_corpus_size": 8323,
            "topic_query_description": "Filtering relevance to visual network analysis in immersive environments for the target SLR.",
            "distillation_method": "Zero-shot instructed prompting per paper; included in consensus ensembles; justifications surfaced for human review and prompt refinement.",
            "output_type": "Label per item (include/exclude) plus justification text.",
            "output_example": "Used as part of the 'Consensus (Best)' ensemble that substantially reduced false positives while maintaining high recall.",
            "evaluation_method": "Compared to human ground truth (TP/FP/TN/FN) and standard metrics (accuracy, precision, recall, F1) in Table 1.",
            "evaluation_results": "High performance as one of the top models; when combined in Consensus (Best) achieved very high recall and improved precision versus single-model outputs.",
            "strengths": "Strong single-model performance, complementary error modes for ensemble consensus, produces useful textual justifications.",
            "limitations": "Commercial access limitations (cost, rate limits); potential to produce false negatives if used alone.",
            "failure_cases": "Occasional individual misclassifications that were non-identical to other agents' errors; needed ensemble consensus to minimize risk of losing relevant papers.",
            "uuid": "e9645.4",
            "source_info": {
                "paper_title": "Cutting Through the Clutter: The Potential of LLMs for Efficient Filtration in Systematic Literature Reviews",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "OpenAI GPT-4o (2024-05-13)",
            "brief_description": "A top-performing commercial LLM in the study that processed the largest token volume in the experiment and when combined in ensembles helped produce high recall with good precision.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-4o-2024-05-13",
            "model_description": "A commercial OpenAI model called via API for per-paper zero-shot classification using the standardized prompt template; generated labels and textual rationales.",
            "model_size": null,
            "input_corpus_description": "Same SLR corpus: 8,323 paper titles+abstracts from ACM/IEEE/Eurographics; used zero-shot classification across the full corpus.",
            "input_corpus_size": 8323,
            "topic_query_description": "Filtering candidate papers for a survey on visual network analysis in immersive environments.",
            "distillation_method": "Zero-shot prompting per record; included in Consensus (Best); measured token usage and cost for the full run.",
            "output_type": "Per-paper include/exclude labels plus textual justifications; ensemble consensus outputs.",
            "output_example": "Processed ~4,432,169 input tokens (~532 tokens/paper incl. prompts) and produced ~443,735 output tokens (~53 tokens/paper); full run completed in under 10 minutes for $28.81 (July 2024 pricing).",
            "evaluation_method": "Compared to human ground truth with confusion-matrix-based metrics (TP/FP/TN/FN), accuracy, precision, recall, and F1; inspected edge cases manually.",
            "evaluation_results": "High single-model performance and contributed to the high-recall Consensus (Best) ensemble; commercial models generally had higher precision but sometimes more false negatives than conservative open models.",
            "strengths": "High throughput, favorable cost/performance in the reported run, strong single-agent performance and complementary ensemble behavior; fast processing time enabling end-to-end SLR-scale runs.",
            "limitations": "Commercial dependency (cost, rate limits, access), possible false negatives when used alone, and the usual risks of hallucination and training-data bias.",
            "failure_cases": "Contributed occasional false exclusions; ensemble consensus was required to avoid losing relevant literature.",
            "uuid": "e9645.5",
            "source_info": {
                "paper_title": "Cutting Through the Clutter: The Potential of LLMs for Efficient Filtration in Systematic Literature Reviews",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Consensus (Best)",
            "name_full": "Consensus (Best) ensemble voting (Gemini 1.5 Flash + Claude 3.5 Sonnet + GPT-4o)",
            "brief_description": "A voting scheme where a paper is included if any of the selected high-performing models includes it and discarded only if all selected models discard it; chosen subset comprises models with F1&gt;50%.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Consensus (Best): Gemini 1.5 Flash + Claude 3.5 Sonnet + GPT-4o",
            "model_description": "Ensemble decision rule combining outputs of selected high-performing LLMs; uses 'include-if-any, discard-only-if-all' logic to prioritize recall while improving precision over single conservative models.",
            "model_size": null,
            "input_corpus_description": "Full SLR corpus of 8,323 title+abstract records; per-model outputs fed into consensus stage.",
            "input_corpus_size": 8323,
            "topic_query_description": "Same SLR topic (visual network analysis in immersive environments).",
            "distillation_method": "Aggregate voting of individual LLM classification outputs (no additional synthesis step); human-in-the-loop prompt refinement and inspection of per-decision justifications informed ensemble selection.",
            "output_type": "Filtered candidate set (include/exclude) with improved precision and high recall; per-paper justifications available from constituent models.",
            "output_example": "Consensus (Best) reduced FP count compared to using all models (reduced manual filtering by 695 papers compared to a different ensemble) and achieved high recall while using only three models.",
            "evaluation_method": "Comparison to human ground truth; confusion counts and aggregate metrics reported; manual inspection of remaining disagreements and edge-cases.",
            "evaluation_results": "Consensus (Best) matched Consensus (All) on the most critical TP/FN rates, missed only one ground-truth-included paper, significantly reduced FP relative to single-model outputs, and achieved a high F1 (reported &gt;50%).",
            "strengths": "Substantially reduces risk of losing relevant papers while decreasing the manual screening burden; requires fewer models (cost/time savings) than larger ensembles; preserves recall near human-level error rates.",
            "limitations": "Still missed one ground-truth inclusion in the experiment; ensemble decisions depend on chosen member models and their complementary error modes; may not generalize across topics without revalidation.",
            "failure_cases": "One relevant paper was discarded by the consensus (an edge case that even human reviewers debated); consensus can only mitigate but not entirely eliminate model-specific misjudgments.",
            "uuid": "e9645.6",
            "source_info": {
                "paper_title": "Cutting Through the Clutter: The Potential of LLMs for Efficient Filtration in Systematic Literature Reviews",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Litllm: A toolkit for scientific literature review",
            "rating": 2,
            "sanitized_title": "litllm_a_toolkit_for_scientific_literature_review"
        },
        {
            "paper_title": "Llassist: Simple tools for automating literature review using large language models",
            "rating": 2,
            "sanitized_title": "llassist_simple_tools_for_automating_literature_review_using_large_language_models"
        },
        {
            "paper_title": "Large language models for literature reviews-an exemplary comparison of llmbased approaches with manual methods",
            "rating": 2,
            "sanitized_title": "large_language_models_for_literature_reviewsan_exemplary_comparison_of_llmbased_approaches_with_manual_methods"
        },
        {
            "paper_title": "Automating research synthesis with domain-specific large language model fine-tuning",
            "rating": 2,
            "sanitized_title": "automating_research_synthesis_with_domainspecific_large_language_model_finetuning"
        },
        {
            "paper_title": "Chatcite: Llm agent with human workflow guidance for comparative literature summary",
            "rating": 2,
            "sanitized_title": "chatcite_llm_agent_with_human_workflow_guidance_for_comparative_literature_summary"
        },
        {
            "paper_title": "Using llms to improve reproducibility of literature reviews",
            "rating": 1,
            "sanitized_title": "using_llms_to_improve_reproducibility_of_literature_reviews"
        },
        {
            "paper_title": "Streamlining the selection phase of systematic literature reviews (slrs) using ai-enabled gpt-4 assistant api",
            "rating": 1,
            "sanitized_title": "streamlining_the_selection_phase_of_systematic_literature_reviews_slrs_using_aienabled_gpt4_assistant_api"
        }
    ],
    "cost": 0.016601499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>28 Apr 2025
28 Apr 20251BD048CBBDC9559D6B297E1552A4177CarXiv:2407.10652v2[cs.LG]
Figure1: Schematic overview of leveraging LLM-based agents for structured literature filtration in a systematic review (SLR).Keywordbased search in online libraries generates a large set of candidate papers that are classified by multiple LLMs based on title and abstract using a customized prompt.A consensus voting scheme determines inclusion or rejection, providing justifications that users can review and refine.</p>
<p>Introduction</p>
<p>Literature reviews, and in particular systematic literature reviews (SLRs), have been described as the gold standard for conducting literature research in academia [ESA01,DMBM14].They provide a transparent and reproducible approach to systematically synthesize and categorize research findings, providing a comprehensive overview of a research topic [Nig09].Such reviews date back to the 18th century [vTC21] and help identify research gaps, future directions, and ensuring consistency and reliability in academic research [Lam19].The creation of SLRs, however, is typically a highly manual and labor-intensive process [SJD21].Egger et al. describe the standard process through a set of eight stages (1.research question, 2. criteria definition, 3. locating, 4. selection, 5. assessment, 6. data extraction, 7. presentation, 8. interpretation) [ESA01].</p>
<p>With PRISMA [LAT * 09], a well-established, standardized method exists, describing the process of retrieving a paper corpus (steps 3-6) by keyword-based search, duplicate removal, manual screening based on title and abstract, and the full-text manuscript review.One of the most time-consuming tasks in this pipeline is the man-ual title and abstract screening, particularly in domains or research fields where classical keyword-based filtering may lead to ambiguous results.According to Wallace et al. [WTL * 10], an experienced peer-reviewer can manually screen about two papers per minute based on title and abstract.At this rate, a corpus of about 8,000 potentially relevant publications for a large SLR requires approximately 66 person-hours (about one and a half full work weeks) of uninterrupted work time.Effects like fatigue, loss of accuracy, inefficiencies, dual verification, and other work commitments typically increase the required time frame significantly, resulting in survey latency times closer to a few months for the initial screening alone.Given the repetitive but still demanding nature of the tasks and the ever-faster progress in academia, it stands to reason if-and how-this process can be improved upon.Using automation for such a (relatively) well-defined classification task is not a new idea, with the first use of automation being reported in the mid-2000s [vTC21].However, the recent advancements of Large Language Models (LLMs) prove promising for the tasks of initial literature filtration during the creation of SLRs primarily due to two reasons: (1) their capability to understand nuanced semantic ambiguities, potentially reaching a feasible accuracy (recall, precision) threshold, and (2) their unparalleled speed and cost-efficiency w.r.t. to human labor.While existing LLM chatbots have the ability to search external databases through function calls, limited research has been conducted on a schematic pipeline of the whole process from repository acquisition to final paper selection and its evaluation, ensuring completeness, reliability, and accountability, which is the focus of this research.In this work, we present a visualinteractive approach leveraging LLMs for literature filtration during SLR creation, allowing users to iteratively refine prompts, evaluate the results, and interactively create a consensus scheme leading to the desired classification result.Thereby, we make the following contributions:</p>
<p>• A conceptual schema for the structured literature filtration process leveraging LLM-based agents with consensus voting • A visual-interactive open-source application, LLMSurver, implementing our framework and making it accessible to others • A comprehensive evaluation for a large SLR (8.3k papers) with an extensive discussion on the pitfalls, potentials, and future prospects of leveraging AI agents for literature filtration</p>
<p>Related Work</p>
<p>With the recent successes of machine learning and in particular LLMs, an increasing number of publications show how language models can leverage some parts of the tasks Therefore, it has been investigated how agent-based systems can help with the formulation, filtering, and search of a research domain [WH23, SR * 24, HT23], using LLMs for specific keyword generation and retrieval through RAG [ALCP24], or more generally, how machine learning [WTL * 10, vTC21, SJD21], but also LLMs [ACR23, Sus23, RMBK23, BSOM24, HT24, PBH * 24] can support the overall process.Further, the summarization step may be supported using LLMs [LCL * 24].One particular aspect that has received less attention is the accurate filtering and classification of a (relatively) large body of potentially relevant research concerning a particular research question to speed up the paper pre-selection process.This is particularly relevant for topics or domains where keyword-based filtering is difficult to use, for example, due to semantic ambiguities or duplicated word use.Haryanto [Har24] explores the usability of LLMs for performing this specific task, focusing on the vote of individual LLMs.Also, fairly recently, automatic tooling approaches for SLR-generation using LLMs have been proposed [SHJ * 24, GLAACG24, Jaf24, SHR * 25].Gehrmann et al. [GQB24] introduced the only LLM-based automated preselection approach, showing that negative prompting can boost accuracy.Building on this, we propose a similar pipeline for classifying large paper corpora, designed as a visual-interactive process that incorporates and evaluates voting schemes from multiple LLM agents and lets users iteratively refine prompts and LLMs.We also compare results with a manual SLR selection on the same dataset, offering insights into reliability and accuracy.</p>
<p>Methodology</p>
<p>To evaluate the applicability of LLMs for pre-filtering the paper corpus for an SLR, we followed a structured methodology, starting with a topic definition, using an early, preliminary version of our recent literature survey "Visual Network Analysis in Immersive Environments: A Survey" [JFR * 25].This topic leads to a sufficiently large corpus of potential papers since all papers dealing with some immersive technology, such as Virtual Reality, Augmented Reality, and others, focusing on the widespread data type of graphs are of relevance.For the initial paper selection, we followed the PRISMA pipeline [LAT * 09], starting with a structural keyword-based search in paper titles and abstracts of potential paper candidates in major computer science repositories.In our case, we included papers from the ACM Digital Library, IEEE Xplore, and Eurographics.After unifying the format of the paper metadata, removing duplicates, and excluding all non-paper publications, we retrieved an initial corpus of 8,323 papers in the preliminary version (the published version used a later iteration with more results).Papers were manually screened in multiple iterations.This process was highly time-You are a professor in computer science conducting a literature review.Please decide and classify if the following paper belongs to a specific research direction or not.For this, you are provided with the title and the abstract, which should give you sufficient information for an informed and accurate decision.
1 1 1 1 1 9 9 1 1 1 3 6 1 1 1 2 5 1 Total Llama3 8B Llama3 70B Gemini 1.5 Flash Claude 3.5 Sonnet GPT-4o
Figure 3: Number of papers (gray background) that were incorrectly (inc.)voted to be included (left) or excluded (right) by the agents, grouped by the number of incorrect agents involved in a decision.The individual bars show how many times a particular agent was involved in the wrong decision.It can be seen on the far right that only a single paper is unanimously misjudged by all agents (and therefore lost forever), demonstrating that N-Consensus voting is beneficial when prioritizing recall.consuming, involving multiple researchers for multiple weeks, but led to the ground truth categorization: for the initial corpus, we identified 88 papers that needed to be included and 8235 to discard.</p>
<p>Based on the ground-truth categorization, we investigate the potential for LLMs to facilitate the laborious process of filtering a paper corpus, considering five open and commercial state-of-the-art foundation models (see Table 1 for details).</p>
<p>We initially tested the LLMs with different prompt styles, asking the models to classify each paper individually, and quickly found a basic prompt schema that works well.In this schema, we tell the LLM its context and role, the overall task, before concluding with an output format and the paper title and abstract.For the final prompt (see Figure 2), we added further exclusion and inclusion criteria, leading to the following results.</p>
<p>Evaluation</p>
<p>The results of the individual LLM classifications are summarized in Table 1.In general, the LLMs performed well with an accuracy above 90 % across all models.However, there are still notable differences: While the open-source models-especially Llama3 8Bwere more conservative, including more papers in general (high FP rate), trying not to exclude any relevant papers (low FN rate), the commercial models discarded more papers (higher TN rate), but with the downside of having more papers erroneously excluded (higher FN rate).Interestingly, the falsely classified papers were mostly different across the LLMs: Regarding the erroneous inclusions (FP), for most papers, only one LLM-often Llama3 8B-was responsible for the wrong classification (see Figure 3 left).The number of papers to exclude that were falsely included by multiple LLMs is drastically lower.This is also the case for relevant, incorrectly discarded papers (FN), where mostly individual LLMs (mostly Gemini 1.5 Flash) generated errors, but false exclusions by multiple LLMs were way lower (see Figure 3 right).Therefore, we also analyzed the performance of a consensus voting of all LLMs-Consensus (All)-and a selection of the best-performing LLMs with an F 1 score above 50 %-Consensus (Best)-consisting of Gemini 1.5 Flash, Claude 3.5 Sonnet, and GPT-4o.For consensus voting, a paper is only discarded if all of the involved LLMs agree to discard it-and included if at least one LLM includes it.The results of both consensus approaches (see Table 1, right column) are highly encouraging, showing great results, especially for the TP and the FN rates.By consensus voting, only one paper would be discarded that should be part of the survey (based on the human ground-truth data).A manual inspection revealed that this paper was also an edge case for the involved researchers, who might have excluded the paper based on the abstract and title but ultimately included it after investigating its content.While both consensus approaches lead to the same TP and FN rates, which are of most relevance for our use case, the Consensus (Best) approach comes with a lower FP rate (see Figure 4), reducing the manual filtering by 695 papers, and only requires three instead of five LLMs, reducing time and cost.</p>
<p>Interactive Human-AI Collaboration</p>
<p>The results of our experiment show that LLMs can support the initial filtering process.However, relying solely on (individual) LLMs without human intervention is of high risk.Therefore, we propose a new pipeline (see Figure 1) to form the paper corpus of survey papers, incorporating a tight collaboration between the researcher (human) and LLMs (AI).The first steps of our suggested pipeline remain the same as for classical paper retrieval (see Section 3): Online repositories are searched for papers of relevance based on keywords, leading to a pre-processed initial paper corpus.Then, multiple LLMs classify each paper independently.The survey authors are an essential part of the process, as they iteratively create and adapt prompts (as in Figure 2) and investigate sampled LLM output through a visual-interactive interface until the results are of sufficient quality.Investigating the LLMs' justification of their de- cisions is often highly useful for evaluating the prompt and for refining it.When the preliminary results are sufficient, all papers are classified by each LLM (or the most promising ones), and the results are combined through a consensus voting.Again, the consensus results can be iteratively adapted and evaluated through a visual-interactive process, highlighting similarities and differences across the LLMs (similar to Figure 3).As our evaluation demonstrated, consensus voting is highly effective in reducing the number of papers, while the rate of erroneously removed papers remains very low.Human classification can similarly result in false exclusions, although these papers can typically be recovered in a subsequent snowballing step [Woh14].Therefore, a small number of removals during the LLM step may be considered acceptable.</p>
<p>Application</p>
<p>We developed the interactive open-source application LLMSurver (https://github.com/dbvis-ukon/LLMSurver)featuring a user interface (UI) implementing our proposed pipeline.This tool demonstrates the practical application and provides support for researchers conducting their own literature surveys.The application is fully containerized and follows a frontend (single-page React), backend Python-based FastAPI) database (SQLite) architecture.The UI is structured as a dashboard, with visually distinct components reflecting the pipeline steps (see Figure 5).The main table To aid decision-making, component
B C A E D B C A E D
presents two charts: one shows the classification distribution across LLMs, while the other visualizes agreement levels, highlighting outliers (e.g., LLama3 8B in our evaluation) that may reduce consensus quality.The opensource tool is adaptable for other use cases, supporting custom consensus methods or additional decision-making visualizations.</p>
<p>Discussion</p>
<p>We have demonstrated that incorporating AI techniques, particularly LLM-based agents, into a structured analysis pipeline can effectively support the initial stages of a systematic literature review with surprisingly high quality.A key advantage is the speed and cost-efficiency of filtration.In our case study with 8,323 papers, GPT-4o processed 4,432,169 input tokens-approximately 532 tokens per paper (including prompts)-and 443,735 output tokens, or around 53 per paper.This entire process was completed in under 10 minutes for just $28.81 (as of July 2024), demonstrating the scalability of LLMs.Their ability to operate continuously or scale up through additional GPU resources makes them ideal for large-scale literature reviews.Compared to manual filtering, which requires a minimum of 69 hours of concentrated human effort, this represents a significant reduction in both time and cost, making systematic reviews more accessible and efficient.When cost or confidentiality is a concern, smaller, open-source models-capable of running locally on a standard laptop-still achieve impressive recall rates of 97.73%, though with lower precision.Even so, these models allow researchers to explore research fields quickly, reducing the manual search space by nearly 90% (from 8,323 papers to 860) in just a few hours.Notably, the recall difference between top-performing models and Llama3 8B was minimal, with only one additional paper lost as a false positive.The model's bias towards inclusion requires further investigation.To enhance precision, using a consensus approach reduced false positives from 774 to 167-a 98% re- duction in the validation space, missing only one out of 88.These results are comparable to human error thresholds, which vary depending on factors like task difficulty, familiarity, stress, and repetition [SS11].Established frameworks such as HEART [Hum88], TESEO [BC80], and THERP [Kir88] suggest error rates ranging from 0.5% to 9%, placing our filtration results within or even below these ranges.Additionally, during our manual review process, 34 papers were reclassified after initial human filtration, further highlighting the strengths of our LLM-based approach.</p>
<p>A significant advantage of using LLMs is the ability to generate consistent and descriptive classification explanations through prompting, a task that would require considerable additional effort from human reviewers.Automating the initial filtration phase also leads to better resource allocation, allowing researchers to focus on higher-level analysis and interpretation, thereby improving overall productivity while reducing fatigue from repetitive tasks.This efficiency enables researchers to explore a more diverse range of research fields by lowering the entry cost of initial surveys.Additionally, it can help identify gaps in existing literature by semi-automatically gathering relevant publications for broader overviews of specific topics.The multilingual capabilities of LLMs further enhance the accessibility of non-English academic literature, facilitating the inclusion of relevant publications from specialized venues or fields with older literature not available in English.Finally, automation inherently improves data management.Using a pipeline architecture helps structure large datasets, making the literature easier to navigate compared to manual processes.</p>
<p>Limitations and Future Work</p>
<p>The use of automation and generative models presents several challenges and limitations.Our study is based on a single large corpus and prompt, which may not generalize to other research areas.Also, our tool has not yet been evaluated in a controlled user study.A validity risk, in particular when avoiding snowballing [Woh14], is a careful selection of the initial set of bibliographical entries and source databases.Other factors, such as prompt design, corpus characteristics, or writing style, could also influence performance.Nevertheless, given the strong text comprehension abilities of LLMs, their potential for literature filtration remains promising, warranting further investigation into their capabilities and limitations.LLMs face well-known challenges, including hallucinations, biases from training data, and accuracy concerns.To ensure completeness, we limited the role of generative AI to classification within a structured schema, avoiding direct involvement in the search process and minimizing the risk of generating false references [HQS Future research should explore the development of interactive literature review platforms where LLMs assist researchers in a collaborative environment, integrating user feedback mechanisms into the review process.While our approach facilitates keywordbased paper search, the potential of new LLMs with access to search engines for retrieving the paper corpus (prompt-based) should be investigated.Extending the use of LLMs to support semi-automatic paper coding-especially when full-text papers are available-could help evaluate the interpretative capabilities of language models more effectively.Additionally, applying this approach to conduct SLRs across multiple disciplines could enable broader, cross-disciplinary analyses, facilitating research efforts that were previously infeasible due to scale or complexity.</p>
<p>Conclusion</p>
<p>This work evaluates the potential of LLMs to enhance filtration in academic literature reviews.We propose a semi-automated filtration schema for systematic reviews, leveraging recent foundation models-Llama3 (8B and 70B), Gemini 1.5 Flash, Claude 3.5 Sonnet, and GPT-4o-as classification agents to filter large corpora of publications relevant to specific research questions.Our method addresses the limitations of traditional keyword-based filtering, which often struggles with semantic ambiguities and inconsistent terminology, requiring time-consuming manual checks.With our opensource tool LLMSurver, users can iteratively test different prompts and LLMs while interactively evaluating the results.We assess LLM performance during the construction of a recent literature survey [JFR * 25], comparing results against human filtering on a dataset of 8,323 articles.The findings show that LLMs can drastically accelerate the review process, shrinking search space by an order of magnitude and reducing weeks of effort to minutes, while maintaining recall (&gt; 98%), even below typical human error rates.This efficiency not only enhances SLR but also holds promise for broader academic applications.Overall, this study highlights the effective use of LLMs to streamline academic research.</p>
<p>The research direction is the topic of "TITLE".Therefore include papers that deal with ASPECT_1, ASPECT_2, ... Examples of AS-PECT_1 are: term 1, term 2, . . .You MUST discard papers that EXCLUSION_EXCEPTION_1, . . .You MUST include papers that INCLUSION_EXCEPTION_1, . . .Below is the title and abstract.You must only answer with INCLUDE or DISCARD and a 2-sentence reason of why.</p>
<p>Figure 2 :
2
Figure 2: Prompt template for the individual agents.</p>
<p>Figure 4 :
4
Figure 4: Pairwise comparison of the incorrect decisions by the agents.A paper's validated ground truth classification is shown in the top row (left part: included, right part: discarded), followed by the individual agents and then the two consensus methods.Incorrect exclusions (FN) can be seen as red discarded lines (left part), while incorrect inclusion (FP) can be seen as green included lines (larger right part).</p>
<p>.D</p>
<p>from the corpus, populated by uploading Bibtex files or providing DOI numbers.A prompt editor component craft and refine classification prompts for selected LLMs in component Users can register new LLMs (local or remote) by entering necessary details such as API keys or hostnames.Classifications can be applied to subsets for testing or the entire corpus, with intermediate results saved.Classification results are visualized in the main table and can be exported as a CSV file.Users can view individual LLM outputs, particularly useful for ambiguous results (indicated by an orange error icon).The consensus component enables run selection, statistics visualization, and LLM selection for consensus-building, with results reflected in the main table.</p>
<p>Figure 5 :
5
Figure 5: The user interface of our application implementing the proposed pipeline, consisting of a paper table</p>
<p>Table 1 :
1
Evaluation results of the LLM agents and two consensus schemes (all models and best models only) for our reference survey, with the validated human classification as ground truth.
MetricLlama-3 (8B) 1Llama-3 (70B) 2Gemini 1.5 Flash 3Claude 3.5 Sonnet 4 GPT-4o 5 Consensus (All) 6Consensus (Best) 7CountsTP (↑) FP (↓) TN (↑)86 774 746185 194 804167 91 814476 95 814080 50 818587 862 737387 167 8068FN (↓)232112811EvaluationAcc. (↑) Prec. (↑) Rec. (↑) F 1 (↑)90.68 10.00 97.73 18.1497.63 30.47 96.59 46.3298.65 42.41 76.14 54.4798.71 44.44 86.36 58.6999.30 61.54 90.91 73.3989.63 9.17 98.86 16.7897.98 34.25 98.86 50.88models with F
1 &gt; 50% (i.e.without Llama3 variants).</p>
<p>23].Despite high accuracy rates, these models can still produce misleading outputs, with performance influenced by model quality, prompt formulation, and contextual understanding.Our study did not focus on prompt engineering [WFH * 23], which could potentially improve outcomes.While our approach primarily employed zero-shot learning with contextual examples, exploring few-shot learning could further enhance accuracy, albeit with increased token usage.Inherent biases, originating from training data or the Reinforcement Learning from Human Feedback (RLHF) process [BJN * 22], can also lead to skewed or incomplete results.Addressing these biases through interactive feedback loops and visual analytics [FHJ * 22] is essential for ensuring research ac-curacy.Although state-of-the-art commercial models demonstrate the highest performance, they present access limitations due to cost, availability, and rate restrictions, potentially disadvantaging smaller research groups or independent researchers [BHA * 22].Developing reliable evaluation metrics for LLM-generated literature surveys is an important area for future research.A potential risk is the over-reliance on automation, which could undermine researchers' critical thinking and analytical skills [BHA * 22].Balancing automation with human oversight [FHJ * 22] remains essential.</p>
<p>*</p>
<p>meta-llama-3-8b-instruct.Q8_0
meta-llama-3-70b-instruct.Q4_K_M
gemini-1.5-flash-001
claude-3-5-sonnet@20240620
gpt-4o-2024-05-13
Consensus between all (five) models.
Consensus between
© 2025 The Authors. Proceedings published by Eurographics -The European Association for Computer Graphics.
AcknowledgementsThe authors gratefully acknowledge financial support by the Federal Ministry for Economic Affairs and Climate Action (BMWK, grant No. 03EI1048D) and the Deutsche Forschungsgemeinschaft (DFG) -Project-ID 251654672 -TRR 161.
Using llm to improve efficiency in literature review for undergraduate research. S A Antu, H Chen, C K Richards, </p>
<p>Litllm: A toolkit for scientific literature review. S Agarwal, I H Laradji, L Charlin, C Pal, 10.48550/ARXIV.2402.017882024</p>
<p>The human factors in risk analyses of process plants: The control room operator model 'teseo'. G Bello, V Colombari, Reliability engineering. 11980</p>
<p>On the opportunities and risks of foundation models. R Bommasani, D A Hudson, E Adeli, Altman R, S Arora, Von Arx S, M S Bernstein, J Bohg, A Bosselut, E Brunskill, A L Et, 10.48550/arXiv.2108.07258BHA * 22. 2022</p>
<p>Training a helpful and harmless assistant with reinforcement learning from human feedback. ] Bjn * 22, Y Bai, A Jones, K Ndousse, A Askell, A L Et, 10.48550/arXiv.2204.058622022</p>
<p>Artificial intelligence for literature reviews: Opportunities and challenges. F Bolanos, A Salatino, F Osborne, E Motta, 10.1007/s10462-024-10902-3Artificial Intelligence Review. 572592024</p>
<p>Viewing systematic reviews and meta-analysis in social research through different lenses. J Davis, K Mengersen, S Bennett, L Mazerolle, 10.1186/2193-1801-3-511SpringerPlus. 322014</p>
<p>Systematic reviews in health care: meta-analysis in context. M Egger, G D Smith, D Altman, 10.1002/978047069392620011</p>
<p>Promoting ethical awareness in communication analysis: Investigating potentials and limits of visual analytics for intelligence applications. M T Fischer, S D Hirsbrunner, W Jentner, M Miller, D A Keim, P Helm, 10.1145/3531146.3533151Proc. FAcct '22. FAcct '22ACM2022</p>
<p>How will artificial intelligence affect scientific writing, reviewing and editing? the future is here. R Gilat, B J Cole, 10.1016/j.arthro.2023.01.014Arthroscopy: The Journal of Arthroscopic &amp; Related Surgery. 392023</p>
<p>Leveraging llms for efficient topic reviews. B Gana, A Leiva-Araos, H Allende-Cid, J Gar-Cía, 10.3390/app14177675Applied Sciences. 1476752024</p>
<p>Large language models for literature reviews-an exemplary comparison of llmbased approaches with manual methods. J Gehrmann, L Quakulinski, Beyan O, 10.1109/FLLM63129.2024.10852447Proc. FLLM. FLLMIEEE2024</p>
<p>Llassist: Simple tools for automating literature review using large language models. C Y Haryanto, 10.48550/arXiv.2407.139932024</p>
<p>A survey on large language models: Applications, challenges, limitations, and practical usage. M U Hadi, Qureshi R, A Shah, M Irfan, A L Et, HQS * 23. 20235</p>
<p>The role of chatgpt in scientific communication: writing better scientific review articles. J Huang, Tan M, AJCR. 13242023</p>
<p>Efficient systematic reviews: Literature filtering with transformers &amp; transfer learning. J Hawkins, D Tivey, 10.48550/arXiv.2405.203542024</p>
<p>Human reliability assessors guide: an overview. Human factors and decision making: their influence on safety and reliability. P Humphreys, 19885</p>
<p>Streamlining the selection phase of systematic literature reviews (slrs) using ai-enabled gpt-4 assistant api. S M A Jafari, 10.48550/arXiv.2402.185822024</p>
<p>Visual network analysis in immersive environments: A survey. L Joos, M T Fischer, J Rauscher, D A Keim, T Dwyer, F Schreiber, K Klein, 10.48550/arXiv.2501.0850020255</p>
<p>Systematic literature reviews: An introduction. B Kirwan, 10.1017/dsi.2019.169Human Factors and Decision Making: Their influence on safety and reliability. 1988. 20191Proceedings of the design society</p>
<p>The prisma statement for reporting systematic reviews and meta-analyses of studies that evaluate healthcare interventions. A Liberati, LAT * 09D G Altman, LAT * 09J Tetzlaff, LAT * 09C Mulrow, LAT * 09P C Gøtzsche, LAT * 09A L Et, LAT * 0910.1136/bmj.b2700BMJ. 33922009</p>
<p>Chatcite: Llm agent with human workflow guidance for comparative literature summary. ] Lcl * 24, Y Li, L Chen, A Liu, K Yu, L Wen, 10.48550/arXiv.2403.025742024</p>
<p>Chatgpt and a new academic reality: Artificial intelligence-written research papers and the ethics of the large language models in scholarly publishing. B D Lund, T Wang, N R Mannuru, B Nie, S Shim-Ray, Z Wang, 10.1002/asi.24750LWM * 23. 202374</p>
<p>A guide to systematic literature reviews. A Nightingale, 10.1016/j.mpsur.2009.07.005Surgery (Oxford). 2722009Determining surgical efficacy</p>
<p>Using llms to improve reproducibility of literature reviews. R Peinl, J Baernthaler, A Haberl, S R Chouguley, S Thalmann, Proc. of 2024 Pre-ICIS SIGDSA Symposium. of 2024 Pre-ICIS SIGDSA Symposium2024PBH * 24</p>
<p>P21 a comparative analysis of large language models (llm) utilised in systematic literature review. H Rathi, A Malik, D Behera, G Kamboj, 10.1016/j.jval.2023.09.030Value in Health. 26S62023</p>
<p>The emergence of large language models (llm) as a tool in literature reviews: an llm automated systematic review. D Scherbakov, N Hubig, V Jansari, A Bakumenko, L A Lenert, 10.48550/arXiv.2409.046002024SHJ * 24</p>
<p>Automating research synthesis with domain-specific large language model fine-tuning. T Susnjak, P Hwang, N H Reyes, A L C Barczak, T R Mcintosh, S Ranathunga, 10.1145/3715964ACM Trans. Knowl. Discov. Data. 2025</p>
<p>A roadmap toward the automatic composition of systematic literature reviews. E M D Silva, M L Dutra, 10.47909/ijsmc.52IJSMC. 122021</p>
<p>System for systematic literature review using multiple ai agents: Concept and an empirical evaluation. A M Sami, Z Rasheed, A L Et, 10.48550/ARXIV.2403.083992024SR * 24</p>
<p>Chapter 5 -reliability modeling techniques. D J Smith, K G Simpson, 10.1016/B978-0-08-096781-3.10005-7Safety Critical Systems Handbook. 2011</p>
<p>Prisma-dfllm: An extension of prisma for systematic literature reviews using domain-specific finetuned large language models. T Susnjak, 10.48550/arXiv.2306.149052023</p>
<p>Ai-driven review systems: Evaluating llms in scalable and bias-aware academic reviews. K Tsl * 24] Tyser, B Segev, G Longhitano, A L Et, 10.48550/arXiv.2408.103652024</p>
<p>Automation of systematic literature reviews: A systematic literature review. R Van Dinter, B Tekinerdogan, C Catal, 10.1016/j.infsof.2021.106589IST. 13622021</p>
<p>A prompt pattern catalog to enhance prompt engineering with chatgpt. ] Wfh * 23, J White, Q Fu, S Hays, M Sandborn, C Olea, A L Et, 10.48550/arXiv.2302.113822023</p>
<p>Elicit: Ai literature review research assistant. S Whitfield, M A Hofmann, 10.1080/15228959.2023.2224125Public Services Quarterly. 192023</p>
<p>Guidelines for snowballing in systematic literature studies and a replication in software engineering. C Wohlin, 10.1145/2601248.2601268Proc. EASE. EASEACM201445</p>
<p>Semi-automated screening of biomedical citations for systematic reviews. B C Wallace, T A Trikalinos, J Lau, C Brodley, C H Schmid, 10.1186/1471-2105-11-55Proceedings published by Eurographics -The European Association for Computer Graphics. published by Eurographics -The European Association for Computer Graphics201011© 2025 The Authors</p>            </div>
        </div>

    </div>
</body>
</html>
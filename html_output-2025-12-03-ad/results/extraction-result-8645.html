<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8645 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8645</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8645</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-b85c299e556c1b0b3d66787b31a25d54e7528283</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b85c299e556c1b0b3d66787b31a25d54e7528283" target="_blank">Efficient Evolutionary Search Over Chemical Space with Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work redesigns crossover and mutation operations in EAs using LLMs trained on large corpora of chemical information, demonstrating that the joint usage of LLMs with EAs yields superior performance over all baseline models across single- and multi-objective settings.</p>
                <p><strong>Paper Abstract:</strong> Molecular discovery, when formulated as an optimization problem, presents significant computational challenges because optimization objectives can be non-differentiable. Evolutionary Algorithms (EAs), often used to optimize black-box objectives in molecular discovery, traverse chemical space by performing random mutations and crossovers, leading to a large number of expensive objective evaluations. In this work, we ameliorate this shortcoming by incorporating chemistry-aware Large Language Models (LLMs) into EAs. Namely, we redesign crossover and mutation operations in EAs using LLMs trained on large corpora of chemical information. We perform extensive empirical studies on both commercial and open-source models on multiple tasks involving property optimization, molecular rediscovery, and structure-based drug design, demonstrating that the joint usage of LLMs with EAs yields superior performance over all baseline models across single- and multi-objective settings. We demonstrate that our algorithm improves both the quality of the final solution and convergence speed, thereby reducing the number of required objective evaluations. Our code is available at http://github.com/zoom-wang112358/MOLLEO</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8645.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8645.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary, web-scale pretrained transformer LLM used here as a chemistry-aware genetic operator: prompted to perform crossover and to produce edited SMILES that improve task-specific objectives within an evolutionary algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-turbo checkpoint)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer, autoregressive GPT-family LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Described as web-scale text corpus (proprietary); model checkpoint referenced is gpt-4-turbo (hosted on Azure).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery / molecular optimization (single- and multi-objective), molecular rediscovery, structure-based design (protein–ligand docking).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based editing/generation used as a CROSSOVER operator in an evolutionary algorithm (MolLEO): prompts include two parent SMILES and their objective scores and request a new molecule with improved target objective; fallback to default Graph-GA crossover if output invalid. GPT-4 was used primarily as crossover; Graph-GA default mutation was often used to reduce LLM queries.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Authors report that for open-source LLMs they checked and found no overlap between generated JNK3 molecules and the public datasets used to train open-source models (ZINC20, PubChem); for GPT-4 specifically, the paper does not report explicit dataset overlap checks but reports per-edit validity and improvement rates (Table A1): valid fraction per edit — perindopril_mpo 86.2%, JNK3 83.5%; fraction of edits with higher fitness — perindopril_mpo 24.0%, JNK3 26.3%; mean fitness increase for improved edits: +0.032 (perindopril_mpo) and +0.0262 (JNK3). Generated offspring were pruned/selected using Tanimoto structural similarity to the top molecule to control search locality.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Objective-specific prompts that include the target objective(s) and input molecule scores; evaluation in-loop with task-specific oracles (pretrained activity classifiers, QED/SA metrics, AutoDock Vina docking scores). Selection pressure (mutate top Y molecules, prune by Tanimoto distance) enforces structural locality and prevents unrelated edits. Multi-objective tasks encoded via text prompt that describes multiple objectives (sum-aggregation or Pareto selection used downstream).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Top-10 AUC (area under curve of top-k average property vs oracle calls), hypervolume for Pareto fronts (multi-objective), docking score (AutoDock Vina), QED, SA score, percent valid molecules, percent of edits that improve fitness, mean fitness increase, Tanimoto distance selection.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>When integrated into MolLEO, GPT-4 produced the best overall performance across the evaluated benchmarks: ranked 1st overall (total Top-10 AUC sum 17.862 across PMO tasks), outperformed Graph-GA and other baselines on many single-objective tasks (GPT-4 achieved top performance on 15/23 PMO tasks). GPT-4 MolLEO showed faster convergence and better final solutions on most tasks and multi-objective hypervolume metrics; example JNK3 Top-10 AUC in Table 1: GPT-4 = 0.7992 (Graph-GA = 0.5532). In initialization-from-best-ZINC experiment, MolLEO (GPT-4) achieved JNK3 Top-10 AUC 0.844 ± 0.052 (better than Graph-GA 0.787 ± 0.035). There are task-dependent failures: GPT-4 performed worse than Graph-GA on the Adenosine A2A receptor docking task, and direct single-step LLM edits are insufficient without EA integration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to Graph-GA, REINVENT, Augmented Memory, GP BO, DST and MolLEO variants with open-source LLMs, MolLEO (GPT-4) had superior aggregate performance and faster convergence for most tasks. In docking tasks GPT-4 usually produced better (lower) docking scores than Graph-GA except for the Adenosine A2A receptor case. Using GPT-4 as the crossover operator outperformed random crossover in many ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>GPT-4 sometimes produces invalid SMILES (non-negligible invalid output rate), has limited ability to guarantee strict numerical constraints in prompts, and can fail on certain structure-based docking tasks; it is proprietary (cost and access constraints). Multi-objective, information-dense prompts can be challenging for LLMs; selection pressure and post-filtering are required to ensure useful, local edits. Benchmarks are proxies and do not guarantee real-world activity; computational and API costs are nontrivial.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficient Evolutionary Search Over Chemical Space with Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8645.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8645.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioT5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioT5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source, chemistry-aware T5-style encoder-decoder model trained on molecule–text paired data, SELFIES, protein sequences, and scientific text; used here as a mutation operator via SELFIES-conditioned generation to edit molecules toward target objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BioT5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BioT5</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer encoder-decoder (T5-style), cross-modal (text ↔ molecule) model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Two-phase training: initial pretraining on molecule–text pairs (339k samples), SELFIES molecular strings, protein sequences, and general scientific text; then fine-tuned on downstream tasks including text-conditioned molecular generation.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery / molecular optimization, molecular rediscovery, property optimization, and structure-based design tasks within MolLEO.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based mutation using SELFIES: BioT5 is prompted with a SELFIES input and a textual description of the target objective; it outputs SELFIES that are decoded to SMILES. Used as the MUTATION operator (editing top Y molecules) within the evolutionary loop; since SELFIES always decode, the model yields valid molecules by construction.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>BioT5 outputs are always decodable to valid molecules (SELFIES guarantee). From direct-edit experiments (Table A1), percent valid molecules = 100% for both perindopril_mpo and JNK3; percent of edits that improved fitness: perindopril_mpo 56.8%, JNK3 51.3%; mean fitness increase on improved edits: +0.208 (perindopril_mpo) and +0.0320 (JNK3). The authors checked generated JNK3 molecules against ZINC20 and PubChem (used in open-source model training) and found no overlap for the evaluated final molecules (no direct training-set memorization detected for those outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>BioT5 conditions generation on explicit textual objective descriptions; integrity of outputs ensured by encoding/decoding via SELFIES. Integration in MolLEO uses selection pressure (mutate top Y, prune by Tanimoto similarity) and oracle feedback (property classifiers, docking) to guide edits toward application-specific aims (binding, QED, SA).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same MolLEO metrics: Top-10 AUC, hypervolume (multi-objective), docking scores, QED, SA score, percent valid molecules (100% due to SELFIES), percent edits improving fitness, mean fitness increase, Tanimoto distances.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MolLEO (BioT5) produced strong, often second-best performance overall (ranked 2nd in aggregate PMO total 15.424), improved single-step fitness distributions more consistently than other models, and performed best on several docking tasks (in some docking tests BioT5 achieved best docking convergence). BioT5 consistently produced valid molecules and had the largest fraction of edits that increased fitness in direct-edit ablations. It performed worse than GPT-4 in many multi-objective tasks where context size/complexity mattered.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared with baseline Graph-GA and other generative baselines, MolLEO (BioT5) substantially improved optimization efficiency and final scores in many tasks, and outperformed MolSTM in percent-valid and percent-improving edits. Compared to GPT-4, BioT5 is free/open-source and more reliable in validity but sometimes underperforms for complex multi-objective prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Performance degrades on prompts containing SMARTS patterns or negative pattern-matching because these patterns likely not present in BioT5 training; may struggle with very information-dense multi-objective prompts. While SELFIES guarantees decoding, physical grounding and precise numerical constraint satisfaction remain challenges. Open-source training corpora coverage can bias what edits are possible.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficient Evolutionary Search Over Chemical Space with Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8645.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8645.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MoleculeSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-modal Molecule Structure-Text Model (MoleculeSTM / MolSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal model that aligns molecule and text embeddings via contrastive training on molecule–text pairs and uses an adaptor to enable text-conditioned molecular editing by optimizing molecule latent embeddings toward text prompts and decoding with a pretrained generative decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multi-modal molecule structure-text model for text-based retrieval and editing</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MoleculeSTM (MolSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Multi-modal model with molecule encoder, text encoder (contrastive), and a generative molecule decoder (encoder–decoder); editing performed via latent embedding gradient optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Trained on molecule–text pairs from PubChem (contrastive objective to align text and molecule embeddings); an adaptor module aligns MolSTM molecule encoder to the pretrained generative model encoder/decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery / molecular optimization, property-conditioned molecular editing, and rediscovery tasks within MolLEO.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Text-conditioned embedding editing: take input SMILES, encode to molecule embedding (E_Mg), iteratively update embedding x_t by gradient descent to minimize a loss that increases cosine similarity to text-embedding of the target prompt (E_Tc) while regularizing distance from the original embedding; decode final embedding with a pretrained generative decoder to obtain SMILES. Used as MUTATION operator applied to top Y molecules in the EA.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Direct-edit statistics (Table A1): percent valid molecules — perindopril_mpo 93.8%, JNK3 92.8%; percent of edits with higher fitness — perindopril_mpo 45.6%, JNK3 20.6%; mean fitness increase on improved edits: +0.033 (perindopril_mpo) and +0.022 (JNK3). Authors report no overlap between MolSTM-generated final JNK3 molecules and PubChem subset used to train MolSTM (i.e., generated molecules for that task were not direct memorized entries).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Specificity enforced by the text prompt describing target property; editing objective explicitly maximizes embedding similarity to text prompt while penalizing large deviations from the original molecule embedding (lambda regularizer), controlling edit magnitude and preserving chemical locality.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Top-10 AUC, hypervolume, docking scores, QED, SA score, percent valid molecules, percent edits improving fitness, mean fitness increase, and use of Tanimoto-based selection/pruning.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MolLEO (MolSTM) improves over Graph-GA baseline on many tasks and provides robust, valid edits in most cases; however, it generally underperforms GPT-4 and BioT5 in aggregate benchmarks (aggregate total 14.557 in Table 1) and shows smaller fractions of improved edits in some tasks. MolSTM-based MolLEO yields substantial improvements when integrated into EA compared to single-step direct queries.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>MolSTM-integration outperformed the Graph-GA baseline in multiple tasks but was generally outperformed by MolLEO (BioT5) and MolLEO (GPT-4) in overall aggregate metrics and in multi-objective hypervolume in many cases. MolSTM differs methodologically from prompt-only LLMs by performing embedding-space optimization (latent-space editing) rather than direct textual generation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Requires careful hyperparameter tuning (learning rate, number of gradient-descent steps, regularization lambda) to avoid unstable edits; can produce invalid SMILES occasionally (validity <100%); is slower due to embedding optimization and decoding steps; degrades in performance for certain multi-objective prompts and can be sensitive to the number of top candidates selected for editing. As with other LLM-based methods, benchmark proxies may not reflect real-world experimental performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficient Evolutionary Search Over Chemical Space with Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>BioT5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations <em>(Rating: 2)</em></li>
                <li>Multi-modal molecule structure-text model for text-based retrieval and editing <em>(Rating: 2)</em></li>
                <li>GPT-4 technical report <em>(Rating: 2)</em></li>
                <li>Chemcrow: Augmenting large-language models with chemistry tools <em>(Rating: 2)</em></li>
                <li>Large language models for chemistry robotics <em>(Rating: 2)</em></li>
                <li>Language models can generate molecules, materials, and protein binding sites directly in three dimensions as xyz, cif, and pdb files <em>(Rating: 2)</em></li>
                <li>What can large language models do in chemistry? a comprehensive benchmark on eight tasks <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8645",
    "paper_id": "paper-b85c299e556c1b0b3d66787b31a25d54e7528283",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A proprietary, web-scale pretrained transformer LLM used here as a chemistry-aware genetic operator: prompted to perform crossover and to produce edited SMILES that improve task-specific objectives within an evolutionary algorithm.",
            "citation_title": "GPT-4 technical report",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-turbo checkpoint)",
            "model_type": "Transformer, autoregressive GPT-family LLM",
            "model_size": null,
            "training_data": "Described as web-scale text corpus (proprietary); model checkpoint referenced is gpt-4-turbo (hosted on Azure).",
            "application_domain": "Drug discovery / molecular optimization (single- and multi-objective), molecular rediscovery, structure-based design (protein–ligand docking).",
            "generation_method": "Prompt-based editing/generation used as a CROSSOVER operator in an evolutionary algorithm (MolLEO): prompts include two parent SMILES and their objective scores and request a new molecule with improved target objective; fallback to default Graph-GA crossover if output invalid. GPT-4 was used primarily as crossover; Graph-GA default mutation was often used to reduce LLM queries.",
            "novelty_of_chemicals": "Authors report that for open-source LLMs they checked and found no overlap between generated JNK3 molecules and the public datasets used to train open-source models (ZINC20, PubChem); for GPT-4 specifically, the paper does not report explicit dataset overlap checks but reports per-edit validity and improvement rates (Table A1): valid fraction per edit — perindopril_mpo 86.2%, JNK3 83.5%; fraction of edits with higher fitness — perindopril_mpo 24.0%, JNK3 26.3%; mean fitness increase for improved edits: +0.032 (perindopril_mpo) and +0.0262 (JNK3). Generated offspring were pruned/selected using Tanimoto structural similarity to the top molecule to control search locality.",
            "application_specificity": "Objective-specific prompts that include the target objective(s) and input molecule scores; evaluation in-loop with task-specific oracles (pretrained activity classifiers, QED/SA metrics, AutoDock Vina docking scores). Selection pressure (mutate top Y molecules, prune by Tanimoto distance) enforces structural locality and prevents unrelated edits. Multi-objective tasks encoded via text prompt that describes multiple objectives (sum-aggregation or Pareto selection used downstream).",
            "evaluation_metrics": "Top-10 AUC (area under curve of top-k average property vs oracle calls), hypervolume for Pareto fronts (multi-objective), docking score (AutoDock Vina), QED, SA score, percent valid molecules, percent of edits that improve fitness, mean fitness increase, Tanimoto distance selection.",
            "results_summary": "When integrated into MolLEO, GPT-4 produced the best overall performance across the evaluated benchmarks: ranked 1st overall (total Top-10 AUC sum 17.862 across PMO tasks), outperformed Graph-GA and other baselines on many single-objective tasks (GPT-4 achieved top performance on 15/23 PMO tasks). GPT-4 MolLEO showed faster convergence and better final solutions on most tasks and multi-objective hypervolume metrics; example JNK3 Top-10 AUC in Table 1: GPT-4 = 0.7992 (Graph-GA = 0.5532). In initialization-from-best-ZINC experiment, MolLEO (GPT-4) achieved JNK3 Top-10 AUC 0.844 ± 0.052 (better than Graph-GA 0.787 ± 0.035). There are task-dependent failures: GPT-4 performed worse than Graph-GA on the Adenosine A2A receptor docking task, and direct single-step LLM edits are insufficient without EA integration.",
            "comparison_to_other_methods": "Compared to Graph-GA, REINVENT, Augmented Memory, GP BO, DST and MolLEO variants with open-source LLMs, MolLEO (GPT-4) had superior aggregate performance and faster convergence for most tasks. In docking tasks GPT-4 usually produced better (lower) docking scores than Graph-GA except for the Adenosine A2A receptor case. Using GPT-4 as the crossover operator outperformed random crossover in many ablations.",
            "limitations_and_challenges": "GPT-4 sometimes produces invalid SMILES (non-negligible invalid output rate), has limited ability to guarantee strict numerical constraints in prompts, and can fail on certain structure-based docking tasks; it is proprietary (cost and access constraints). Multi-objective, information-dense prompts can be challenging for LLMs; selection pressure and post-filtering are required to ensure useful, local edits. Benchmarks are proxies and do not guarantee real-world activity; computational and API costs are nontrivial.",
            "uuid": "e8645.0",
            "source_info": {
                "paper_title": "Efficient Evolutionary Search Over Chemical Space with Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "BioT5",
            "name_full": "BioT5",
            "brief_description": "An open-source, chemistry-aware T5-style encoder-decoder model trained on molecule–text paired data, SELFIES, protein sequences, and scientific text; used here as a mutation operator via SELFIES-conditioned generation to edit molecules toward target objectives.",
            "citation_title": "BioT5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations",
            "mention_or_use": "use",
            "model_name": "BioT5",
            "model_type": "Transformer encoder-decoder (T5-style), cross-modal (text ↔ molecule) model",
            "model_size": null,
            "training_data": "Two-phase training: initial pretraining on molecule–text pairs (339k samples), SELFIES molecular strings, protein sequences, and general scientific text; then fine-tuned on downstream tasks including text-conditioned molecular generation.",
            "application_domain": "Drug discovery / molecular optimization, molecular rediscovery, property optimization, and structure-based design tasks within MolLEO.",
            "generation_method": "Prompt-based mutation using SELFIES: BioT5 is prompted with a SELFIES input and a textual description of the target objective; it outputs SELFIES that are decoded to SMILES. Used as the MUTATION operator (editing top Y molecules) within the evolutionary loop; since SELFIES always decode, the model yields valid molecules by construction.",
            "novelty_of_chemicals": "BioT5 outputs are always decodable to valid molecules (SELFIES guarantee). From direct-edit experiments (Table A1), percent valid molecules = 100% for both perindopril_mpo and JNK3; percent of edits that improved fitness: perindopril_mpo 56.8%, JNK3 51.3%; mean fitness increase on improved edits: +0.208 (perindopril_mpo) and +0.0320 (JNK3). The authors checked generated JNK3 molecules against ZINC20 and PubChem (used in open-source model training) and found no overlap for the evaluated final molecules (no direct training-set memorization detected for those outputs).",
            "application_specificity": "BioT5 conditions generation on explicit textual objective descriptions; integrity of outputs ensured by encoding/decoding via SELFIES. Integration in MolLEO uses selection pressure (mutate top Y, prune by Tanimoto similarity) and oracle feedback (property classifiers, docking) to guide edits toward application-specific aims (binding, QED, SA).",
            "evaluation_metrics": "Same MolLEO metrics: Top-10 AUC, hypervolume (multi-objective), docking scores, QED, SA score, percent valid molecules (100% due to SELFIES), percent edits improving fitness, mean fitness increase, Tanimoto distances.",
            "results_summary": "MolLEO (BioT5) produced strong, often second-best performance overall (ranked 2nd in aggregate PMO total 15.424), improved single-step fitness distributions more consistently than other models, and performed best on several docking tasks (in some docking tests BioT5 achieved best docking convergence). BioT5 consistently produced valid molecules and had the largest fraction of edits that increased fitness in direct-edit ablations. It performed worse than GPT-4 in many multi-objective tasks where context size/complexity mattered.",
            "comparison_to_other_methods": "Compared with baseline Graph-GA and other generative baselines, MolLEO (BioT5) substantially improved optimization efficiency and final scores in many tasks, and outperformed MolSTM in percent-valid and percent-improving edits. Compared to GPT-4, BioT5 is free/open-source and more reliable in validity but sometimes underperforms for complex multi-objective prompts.",
            "limitations_and_challenges": "Performance degrades on prompts containing SMARTS patterns or negative pattern-matching because these patterns likely not present in BioT5 training; may struggle with very information-dense multi-objective prompts. While SELFIES guarantees decoding, physical grounding and precise numerical constraint satisfaction remain challenges. Open-source training corpora coverage can bias what edits are possible.",
            "uuid": "e8645.1",
            "source_info": {
                "paper_title": "Efficient Evolutionary Search Over Chemical Space with Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "MoleculeSTM",
            "name_full": "Multi-modal Molecule Structure-Text Model (MoleculeSTM / MolSTM)",
            "brief_description": "A multimodal model that aligns molecule and text embeddings via contrastive training on molecule–text pairs and uses an adaptor to enable text-conditioned molecular editing by optimizing molecule latent embeddings toward text prompts and decoding with a pretrained generative decoder.",
            "citation_title": "Multi-modal molecule structure-text model for text-based retrieval and editing",
            "mention_or_use": "use",
            "model_name": "MoleculeSTM (MolSTM)",
            "model_type": "Multi-modal model with molecule encoder, text encoder (contrastive), and a generative molecule decoder (encoder–decoder); editing performed via latent embedding gradient optimization.",
            "model_size": null,
            "training_data": "Trained on molecule–text pairs from PubChem (contrastive objective to align text and molecule embeddings); an adaptor module aligns MolSTM molecule encoder to the pretrained generative model encoder/decoder.",
            "application_domain": "Drug discovery / molecular optimization, property-conditioned molecular editing, and rediscovery tasks within MolLEO.",
            "generation_method": "Text-conditioned embedding editing: take input SMILES, encode to molecule embedding (E_Mg), iteratively update embedding x_t by gradient descent to minimize a loss that increases cosine similarity to text-embedding of the target prompt (E_Tc) while regularizing distance from the original embedding; decode final embedding with a pretrained generative decoder to obtain SMILES. Used as MUTATION operator applied to top Y molecules in the EA.",
            "novelty_of_chemicals": "Direct-edit statistics (Table A1): percent valid molecules — perindopril_mpo 93.8%, JNK3 92.8%; percent of edits with higher fitness — perindopril_mpo 45.6%, JNK3 20.6%; mean fitness increase on improved edits: +0.033 (perindopril_mpo) and +0.022 (JNK3). Authors report no overlap between MolSTM-generated final JNK3 molecules and PubChem subset used to train MolSTM (i.e., generated molecules for that task were not direct memorized entries).",
            "application_specificity": "Specificity enforced by the text prompt describing target property; editing objective explicitly maximizes embedding similarity to text prompt while penalizing large deviations from the original molecule embedding (lambda regularizer), controlling edit magnitude and preserving chemical locality.",
            "evaluation_metrics": "Top-10 AUC, hypervolume, docking scores, QED, SA score, percent valid molecules, percent edits improving fitness, mean fitness increase, and use of Tanimoto-based selection/pruning.",
            "results_summary": "MolLEO (MolSTM) improves over Graph-GA baseline on many tasks and provides robust, valid edits in most cases; however, it generally underperforms GPT-4 and BioT5 in aggregate benchmarks (aggregate total 14.557 in Table 1) and shows smaller fractions of improved edits in some tasks. MolSTM-based MolLEO yields substantial improvements when integrated into EA compared to single-step direct queries.",
            "comparison_to_other_methods": "MolSTM-integration outperformed the Graph-GA baseline in multiple tasks but was generally outperformed by MolLEO (BioT5) and MolLEO (GPT-4) in overall aggregate metrics and in multi-objective hypervolume in many cases. MolSTM differs methodologically from prompt-only LLMs by performing embedding-space optimization (latent-space editing) rather than direct textual generation.",
            "limitations_and_challenges": "Requires careful hyperparameter tuning (learning rate, number of gradient-descent steps, regularization lambda) to avoid unstable edits; can produce invalid SMILES occasionally (validity &lt;100%); is slower due to embedding optimization and decoding steps; degrades in performance for certain multi-objective prompts and can be sensitive to the number of top candidates selected for editing. As with other LLM-based methods, benchmark proxies may not reflect real-world experimental performance.",
            "uuid": "e8645.2",
            "source_info": {
                "paper_title": "Efficient Evolutionary Search Over Chemical Space with Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "BioT5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations",
            "rating": 2,
            "sanitized_title": "biot5_enriching_crossmodal_integration_in_biology_with_chemical_knowledge_and_natural_language_associations"
        },
        {
            "paper_title": "Multi-modal molecule structure-text model for text-based retrieval and editing",
            "rating": 2,
            "sanitized_title": "multimodal_molecule_structuretext_model_for_textbased_retrieval_and_editing"
        },
        {
            "paper_title": "GPT-4 technical report",
            "rating": 2,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "Chemcrow: Augmenting large-language models with chemistry tools",
            "rating": 2,
            "sanitized_title": "chemcrow_augmenting_largelanguage_models_with_chemistry_tools"
        },
        {
            "paper_title": "Large language models for chemistry robotics",
            "rating": 2,
            "sanitized_title": "large_language_models_for_chemistry_robotics"
        },
        {
            "paper_title": "Language models can generate molecules, materials, and protein binding sites directly in three dimensions as xyz, cif, and pdb files",
            "rating": 2,
            "sanitized_title": "language_models_can_generate_molecules_materials_and_protein_binding_sites_directly_in_three_dimensions_as_xyz_cif_and_pdb_files"
        },
        {
            "paper_title": "What can large language models do in chemistry? a comprehensive benchmark on eight tasks",
            "rating": 2,
            "sanitized_title": "what_can_large_language_models_do_in_chemistry_a_comprehensive_benchmark_on_eight_tasks"
        }
    ],
    "cost": 0.016083,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>EFFICIENT EvolutionARY SEARCH OVER Chemical Space with Large Language Models</h1>
<p>Haorui Wang ${ }^{\wedge, 1}$, Marta Skreta ${ }^{&lt;, 2,3}$, Cher-Tian Ser ${ }^{2}$, Wenhao Gao ${ }^{4}$, Lingkai Kong ${ }^{1}$, Felix Strieth-Kalthoff ${ }^{5}$, Chenru Duan ${ }^{6}$, Yuchen Zhuang ${ }^{1}$, Yue Yu ${ }^{1}$, Yanqiao Zhu ${ }^{7}$, Yuanqi Du ${ }^{7,8}$, Alán Aspuru-Guzik ${ }^{1,2,3}$, Kirill Neklyudov ${ }^{7,9,10}$, Chao Zhang ${ }^{7,1}$<br>${ }^{1}$ Georgia Institute of Technology, ${ }^{2}$ University of Toronto, ${ }^{3}$ Vector Institute, ${ }^{4}$ Massachusetts Institute of Technology, ${ }^{5}$ University of Wuppertal, ${ }^{6}$ Deep Principle Inc., ${ }^{7}$ University of California, Los Angeles ${ }^{8}$ Cornell University, ${ }^{9}$ Université de Montréal, ${ }^{10}$ Mila - Quebec AI Institute hwang984@gatech.edu, martaskreta@cs.toronto.edu</p>
<h4>Abstract</h4>
<p>Molecular discovery, when formulated as an optimization problem, presents significant computational challenges because optimization objectives can be nondifferentiable. Evolutionary Algorithms (EAs), often used to optimize black-box objectives in molecular discovery, traverse chemical space by performing random mutations and crossovers, leading to a large number of expensive objective evaluations. In this work, we ameliorate this shortcoming by incorporating chemistryaware Large Language Models (LLMs) into EAs. Namely, we redesign crossover and mutation operations in EAs using LLMs trained on large corpora of chemical information. We perform extensive empirical studies on both commercial and open-source models on multiple tasks involving property optimization, molecular rediscovery, and structure-based drug design, demonstrating that the joint usage of LLMs with EAs yields superior performance over all baseline models across single- and multi-objective settings. We demonstrate that our algorithm improves both the quality of the final solution and convergence speed, thereby reducing the number of required objective evaluations.</p>
<h2>1 INTRODUCTION</h2>
<p>Molecular discovery is a complex and iterative process involving the design, synthesis, evaluation, and refinement of molecule candidates. This process is often slow and laborious, making it difficult to meet the increasing demand for new molecules in domains such as pharmaceuticals, optoelectronics, and energy storage (Tom et al., 2024). One significant challenge is that evaluating molecular properties often requires expensive evaluations (oracles), such as wet-lab experiments, bioassays, and computational simulations (Gensch et al., 2022; Stokes et al., 2020). Even approximate computational evaluations require substantial resources (Gensch et al., 2022). Consequently, the development of efficient algorithms for molecular search, prediction, and generation has gained traction in chemistry to accelerate the discovery process. These advancements in computational techniques, particularly machine learning-driven methods, have facilitated the rapid identification and proposal of promising molecular candidates for real-world experiments (Kristiadi et al., 2024; Atz et al., 2021; Du et al., 2024; Nigam et al., 2023).</p>
<p>Several current approaches used to generate molecular candidates are based on Evolutionary Algorithms (EAs) (Holland, 1992), which do not require the evaluation of gradients and are thus well-suited for black-box objectives in molecular discovery. However, a major downside is that they generate proposals randomly without leveraging task-specific information. Consequently, producing reasonable candidates requires numerous evaluations of the objective function, limiting the practical application of these algorithms. Thus, proposals generated by operators that incorporate task-specific information can help reduce the number of evaluations required to optimize the objective function.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of MolLEO. Given an initial pool of molecules, mates are selected using default Graph-GA (Jensen, 2019) heuristics and converted to SMILES or SELFIES strings. LLMs then function as mutation or crossover operators, editing the molecule string representations based on text prompts that describe the target objective(s). The offspring molecules are then evaluated using an oracle, and the best-scoring ones are passed to the next generation. This process is repeated until the maximum number of allowed molecule evaluations is performed.</p>
<p>Natural language processing (NLP) has increasingly been utilized to represent molecular structures (Chithrananda et al.; Schwaller et al., 2019; Öztürk et al., 2020) and extract chemical knowledge from literature (Tshitoyan et al. 2019). The connection between NLP and molecular systems is facilitated by molecular representations such as the Simplified Molecular Input Line Entry System (SMILES) and Self-Referencing Embedded Strings (SELFIES) (Weininger, 1988; Daylight Chemical Information Systems, 2007; Krenn et al., 2020). These methods convert 2D molecular graphs into text, allowing molecular structures to be represented in the same modality as their textual descriptions.</p>
<p>Recently, the performance of Large Language Models (LLMs) has been investigated in several chemistry-related tasks, such as predicting molecular properties (Guo et al., 2023b; Jablonka et al., 2024), retrieving optimal molecules (Kristiadi et al., 2024; Ramos et al., 2023; Ye et al., 2023), automating chemistry experiments (Bran et al. 2023; Boiko et al. 2023; Yoshikawa et al. 2023; Darvish et al. 2024), and generating molecules with target properties (Flam-Shepherd &amp; Aspuru-Guzik, 2023; Liu et al., 2024; Ye et al., 2023). Because LLMs have been trained on large corpora of text that include a wide range of tasks, they demonstrate general-purpose language comprehension as well as knowledge of basic chemistry, making them interesting tools for chemical discovery tasks (White, 2023). However, many LLM-based approaches depend on in-context learning and prompt engineering (Guo et al., 2023b). This can pose issues when designing molecules with strict numerical objectives, as LLMs may struggle to satisfy precise numerical constraints or optimize for specific numerical targets (AI4Science &amp; Quantum, 2023). Furthermore, methods that solely depend on LLM prompting may produce molecules with lower fitness due to a lack of physical grounding, or they may produce invalid SMILES that cannot be decoded into chemical structures (Skinnider, 2024).</p>
<p>In this work, we propose <strong>Molecular Language-Enhanced Evolutionary Optimization (MolLEO)</strong>, which incorporates LLMs into EAs to enhance the quality of generated proposals and accelerate the optimization process (see Figure 1). MolLEO leverages LLMs as genetic operators to produce new proposals through crossover or mutation. To our knowledge, this is the first demonstration of how LLMs can be incorporated into EA frameworks for molecular generation. In this work, we consider three LLMs: GPT-4 (Achiam et al., 2023), BioT5 (Pei et al., 2023), and MoleculeSTM (MolSTM) (Liu et al., 2023b). We integrate each LLM into separate crossover and mutation procedures, justifying our design choices through ablation studies. We empirically demonstrate the superior performance of MolLEO across multiple black-box optimization tasks, including single-objective and multi-objective optimization. For all tasks, including more challenging ones like protein-ligand docking, MolLEO outperforms the baseline EA and other optimization algorithms based on reinforcement learning (RL) and Bayesian Optimization (BO). To further illustrate how our model can be used in novel molecular discovery settings, we show that MolLEO can improve on the best existing JNK3 inhibitor molecules in ZINC 250K (Sterling &amp; Irwin, 2015).</p>
<h1>2 Related Work</h1>
<h3>2.1 Molecular Optimization</h3>
<p>The molecular design field, encompassing multiple fundamental problems in chemistry, has developed numerous methods. In general, all the existing approaches define the space of possible molecular structures and run a combinatorial search to find the molecule with the target properties. Namely, conventional methods include Monte Carlo Tree Search (MCTS) (Yang et al., 2017), Reinforcement Learning (RL) (Olivecrona et al., 2017a; Guo \&amp; Schwaller, 2023), and Genetic Algorithms (GA) (Jensen, 2019; Fu et al., 2021; Nigam et al., 2022; Fu et al., 2022).</p>
<p>Due to existing challenges such as searching through exponentially large chemical space and evaluating expensive objectives (Bohacek et al., 1996; Stumpfe \&amp; Bajorath, 2012), conventional algorithms have recently recoursed to machine learning techniques, especially generative modeling (Du et al., 2024). Generative models learn a probability distribution of the observed data which can be later used to propose new molecular structures, thereby concentrating the search space around valid molecular structures. Depending on the type of the data and necessary properties for the search algorithms, different generative models have been considered: autoregressive models (ARs) (Popova et al., 2019; Gao et al., 2021), variational autoencoders (VAEs) (Gómez-Bombarelli et al., 2018; Jin et al., 2018), flow-based models Madhawa et al. (2019); Shi et al. (2020), diffusion models Hoogeboom et al. (2022); Schneuing et al. (2022).</p>
<p>Despite concentrating the search space around valid molecules by the usage of generative modeling, the optimization of necessary properties can remain infeasible. To narrow down the search space further, one can consider the conditional generative modeling, where the molecular structures are sampled from the conditional distribution having some predefined properties (Gómez-Bombarelli et al., 2018; Griffiths \&amp; Hernández-Lobato, 2020; Zang \&amp; Wang, 2020; Du et al., 2022; Wei et al., 2024). In this paper, we demonstrate the use of chemistry-aware LLMs as conditional generative models that improve the efficiency of combinatorial search in the molecular space.</p>
<h3>2.2 LANGUAGE MODELS IN CHEMISTRY</h3>
<p>LLMs have been widely investigated for their applicability in scientific domains (Achiam et al., 2023; AI4Science \&amp; Quantum, 2023), as well as their ability to leverage chemistry tools for chemical discovery and characterization (Bran et al., 2023; Boiko et al., 2023). Several works have benchmarked LLMs such as GPT-4 on chemistry tasks and found that while LLMs can outperform human chemists in some zero-shot question-answering settings, they still struggle with chemical reasoning (Mirza et al., 2024; Guo et al., 2023b). Several smaller, open-source models have been trained or fine-tuned specifically on chemistry text (Taylor et al., 2022; Christofidellis et al., 2023; Pei et al., 2023).
Recently, language models have also been used to guide a given input molecular structure towards specific objective properties; a widely-used term used for this is molecular editing (Liu et al., 2023b; Ye et al., 2023). Modifying structures towards specified properties is important so that they can satisfy potentially many required criteria, a requirement in pharmaceutical development where molecules need to be non-toxic and effective against their target (among other things), or in battery design, where molecules need to have a large energy capacity and a long lifespan. In this paper, we focus on molecular optimization to find molecules with desired properties, rather than editing. For interested readers, we provide additional related works about how LLMs have been combined with EAs for code and text generation, as well as benchmarking LLMs in chemical tasks in Appendix A.1.</p>
<h2>3 The MolLEO Framework</h2>
<h3>3.1 Problem Statement</h3>
<p>Black-box optimization. Molecule discovery with a given property can be formulated as an optimization problem</p>
<p>$$
m^{*}=\arg \max _{m \in M} F(m)
$$</p>
<p>where $m$ is a molecular structure and $M$ denotes the set of valid molecules constituting the entire chemical space. The objective $F(m): M \rightarrow \mathbb{R}$ is a black-box scalar-valued function that measures a certain molecule property $m$.</p>
<p>The measurement of chemical properties can involve complicated simulations or in vivo experiments, making it impossible to evaluate the gradients of the objective function $F$. Additionally, we assume that the main computational expense of the optimization procedure comes from the objective evaluation (oracle call). Therefore, we design algorithms to minimize the number of oracle calls and compare all the algorithms with the same call budget.</p>
<p>Multi-objective black-box optimization. Oftentimes, molecules need to meet multiple, potentially competing objectives simultaneously. Multi-objective optimization aims to find the Pareto-optimal solution, where none of the objectives can be improved without deteriorating any of them (Lin et al., 2022). The naive approach to optimize given objectives $\left{F_{i}(\cdot)\right}_{i=1}^{n}$ jointly is to consider an aggregate objective, such as the sum of all individual objectives, i.e.</p>
<p>$$
m^{*}=\arg \max <em i="i">{m \in M} \sum</em>(m)
$$} w_{i} F_{i</p>
<p>where $w_{i}$ is the weight of $i$-th objective, which can be considered a hyperparameter. However, determining the weight of each objective function might be nontrivial (Kusanda et al., 2022).</p>
<p>The rigorous approach to multi-objective optimization is the introduction of partial order and considering the solutions from the Pareto frontier (Geoffrion, 1968; Ekins et al., 2010). In this context, the partial order is defined by comparing all the objectives $\left{F_{i}(\cdot)\right}_{i=1}^{n}$ for the given molecules, i.e., $m^{\prime}$ surpasses $m$ if every objective evaluated on $m^{\prime}$ is greater than the same objective evaluated on $m$ (assuming the maximization of objectives). Formally,</p>
<p>$$
m^{\prime} \succeq m \Longleftrightarrow \forall i \quad F_{i}\left(m^{\prime}\right) \geq F_{i}(m)
$$</p>
<p>For the given set of molecules $S=\left{m_{j}\right}_{j=1}^{m}$, the Pareto frontier $P(S)$ is defined as the set of non-dominated solutions. Namely, for every molecule $m \in P(s)$ there is no other molecule in $S$ surpassing $m$, i.e.</p>
<p>$$
P(S)=\left{m \in S:\left{m^{\prime} \in S: m^{\prime} \succeq m, m^{\prime} \neq m\right}=\varnothing\right}
$$</p>
<p>When jointly optimizing several objectives, we use the Pareto frontier to select candidates during the evolutionary search and compare algorithms. Namely, assuming that the objectives are bounded (e.g., $F(\cdot) \in[0,1])$, one can compare two Pareto frontiers by evaluating their hypervolume</p>
<p>$$
\text { Volume }(P(S))=\text { Volume }\left(\cup_{m \in P(s)} H(m)\right), \quad H(m)=\left{x \in[0,1]^{n}: x_{i} \leq F_{i}(m), \forall i\right}
$$</p>
<p>where $H(m)$ is the hyperrectangle associated with the objectives evaluated on molecule $m$, and Volume $(\cdot)$ evaluates the Euclidean volume of the input set.</p>
<h1>3.2 Evolutionary Algorithms</h1>
<p>We build our MolLEO framework upon the Graph-GA algorithm (Jensen, 2019) — an evolutionary algorithm that operates as follows. An initial pool of molecules is randomly selected, and their fitnesses are calculated using a black-box oracle, $F(\cdot)$. Two parents are then sampled with a probability proportional to their fitnesses and combined using a CROSSOVER operator to generate an offspring, followed by a random MUTATION with probability $p_{m}$. This process is repeated num_crossover times, and the children are added to the pool of offspring. Finally, the fitnesses of the offspring are measured using $F(\cdot)$ and the offspring are added to the population. For single-objective optimization, the $n_{c}$ fittest members from the population at a given step are selected to pass on to the next generation. For multi-objective optimization, two strategies are investigated: (1) Objective summation, where the summation of individual objectives is used as a single objective, and the $n_{c}$ fittest members are retained; and (2) Pareto set selection, where only the Pareto frontier of the current population is kept. This process is repeated until the maximum allowed oracle calls (oracle budget) have been made. This process is outlined in Algorithm 1.</p>
<p>We incorporate chemistry-aware LLMs into the structure of Graph-GA by using them as proposal generators at CROSSOVER and MUTATION steps. That is, for the CROSSOVER step, instead of randomly</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="err">:</span><span class="w"> </span><span class="n">MolLEO</span><span class="w"> </span><span class="n">Algorithm</span>
<span class="k">Data</span><span class="err">:</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">initial</span><span class="w"> </span><span class="n">pool</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathbb</span><span class="err">{</span><span class="n">M</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="mi">0</span><span class="err">}\</span><span class="p">);</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">objective</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">F</span><span class="err">\</span><span class="p">);</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">population</span><span class="w"> </span><span class="k">size</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">n_</span><span class="err">{</span><span class="n">c</span><span class="err">}\</span><span class="p">);</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">offspring</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">n_</span><span class="err">{</span><span class="n">o</span><span class="err">}\</span><span class="p">).</span>
<span class="k">Result</span><span class="err">:</span><span class="w"> </span><span class="n">Optimized</span><span class="w"> </span><span class="n">molecule</span><span class="w"> </span><span class="n">population</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathbb</span><span class="err">{</span><span class="n">M</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="o">*</span><span class="err">}\</span><span class="p">)</span>
<span class="k">begin</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">m</span><span class="w"> </span><span class="err">\</span><span class="ow">in</span><span class="w"> </span><span class="err">\</span><span class="n">mathbb</span><span class="err">{</span><span class="n">M</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="mi">0</span><span class="err">}\</span><span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span><span class="k">Compute</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">F</span><span class="p">(</span><span class="n">m</span><span class="p">)</span><span class="err">\</span><span class="p">);</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">t</span><span class="w"> </span><span class="err">\</span><span class="ow">in</span><span class="o">[</span><span class="n">1\), oracle_budget \(</span><span class="o">]</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span><span class="n">offspring</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[]</span><span class="p">;</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">num_crossovers</span><span class="w"> </span><span class="n">do</span>
<span class="w">            </span><span class="n">sample</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">m_</span><span class="err">{</span><span class="mi">0</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">m_</span><span class="err">{</span><span class="mi">1</span><span class="err">}\</span><span class="p">)</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathbb</span><span class="err">{</span><span class="n">M</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">t</span><span class="err">}\</span><span class="p">)</span><span class="w"> </span><span class="n">proportionally</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">objective</span><span class="w"> </span><span class="k">value</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">F</span><span class="p">(</span><span class="n">m</span><span class="p">)</span><span class="err">\</span><span class="p">);</span>
<span class="w">            </span><span class="n">offspring</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">CROSSOVER</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="n">m_</span><span class="err">{</span><span class="mi">0</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">m_</span><span class="err">{</span><span class="mi">1</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="p">);</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathbb</span><span class="err">{</span><span class="n">M</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">sorted</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="err">\</span><span class="n">mathbb</span><span class="err">{</span><span class="n">M</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">t</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">);</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="err">\</span><span class="ow">in</span><span class="o">[</span><span class="n">1\), num_mutations</span><span class="o">]</span><span class="w"> </span><span class="n">do</span>
<span class="w">            </span><span class="n">offspring</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">MUTATION</span><span class="p">(</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nf">left</span><span class="p">.</span><span class="err">\</span><span class="n">mathbb</span><span class="err">{</span><span class="n">M</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="err">\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="p">);</span>
<span class="w">        </span><span class="n">offspring</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">search</span><span class="p">(</span><span class="n">offspring</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nf">left</span><span class="o">[</span><span class="n">: n_{o}\right</span><span class="o">]</span><span class="w"> </span><span class="err">\</span><span class="n">quad</span><span class="w"> </span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">smallest</span><span class="w"> </span><span class="n">Tanimoto</span><span class="w"> </span><span class="n">distance</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathbb</span><span class="err">{</span><span class="n">M</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="o">[</span><span class="n">0</span><span class="o">]</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathbb</span><span class="err">{</span><span class="n">M</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">offspring</span><span class="p">;</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">m</span><span class="w"> </span><span class="err">\</span><span class="ow">in</span><span class="w"> </span><span class="err">\</span><span class="n">mathbb</span><span class="err">{</span><span class="n">M</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">t</span><span class="err">}\</span><span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">            </span><span class="k">Compute</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">F</span><span class="p">(</span><span class="n">m</span><span class="p">)</span><span class="err">\</span><span class="p">);</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">Task_type</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">single_objective</span><span class="w"> </span><span class="k">then</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathbb</span><span class="err">{</span><span class="n">M</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">sorted</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="err">\</span><span class="n">mathbb</span><span class="err">{</span><span class="n">M</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">t</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="nf">left</span><span class="o">[</span><span class="n">: n_{c}\right</span><span class="o">]</span><span class="err">\</span><span class="p">);</span>
<span class="w">            </span><span class="k">else</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathbb</span><span class="err">{</span><span class="n">M</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">Pareto_Frontier</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="err">\</span><span class="n">mathbb</span><span class="err">{</span><span class="n">M</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">t</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">);</span>
<span class="w">    </span><span class="k">Return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathbb</span><span class="err">{</span><span class="n">M</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">t</span><span class="err">}\</span><span class="p">);</span>
</code></pre></div>

<p>combining two parent molecules, we generate molecules that maximize the objective fitness function guided by the objective description. For the MUTATION step, the operator mutates the fittest members of the current population based on the target description. However, we noticed that LLMs do not always generate candidates with higher fitness than the input molecule (demonstrated in Appendix C.1), and so we constructed a selection pressure to filter edited molecules based on structural similarity to the top molecule (Nigam et al., 2022). That is, we sort the existing population by fitness, apply a mutation to the top population members, and then add them to the pool of offspring. Then, we prune the pool by selecting the $n_{o}$ most similar offspring to the fittest molecule in the entire pool based on Tanimoto distance. We ablate the impact of this filter in Appendix C.2.
For each LLM, we describe below the details of how we implement the CROSSOVER and MUTATION operators. We empirically studied different combinations of models and hyperparameters (Appendix C.2), and in what follows, we describe the operators that resulted in the best performance.</p>
<p>Graph-GA The baseline algorithm that we build upon and compare against in our experiments.
$\triangleright$ CROSSOVER: (default Graph-GA crossover): Two parent molecules are sampled with a probability proportional to their fitness. Crossover takes place at a ring or non-ring position with equal likelihood. Parents are cut at random positions into fragments, and then fragments from both parents are combined. Invalid molecules are filtered out, and a randomly spliced molecule is returned Jensen (2019).
$\triangleright$ MUTATION: (default Graph-GA mutation): Random operations such as bond insertion or deletion, atom insertion or deletion, bond order swapping, or atom identity changes are done with predetermined likelihoods Jensen (2019).</p>
<p>MolLEO (GPT-4) GPT-4 is a proprietary LLM trained on a web-scale text corpus.
$\triangleright$ CROSSOVER: Two parent molecules are sampled the same way as in Graph-GA. GPT-4 is then prompted to generate an offspring with the template $t_{i n}=$ "I have two molecules and their [target_objective] scores: $\left(s_{i n, 0}, f_{0}\right),\left(s_{i n, 1}, f_{1}\right)$. Propose a new molecule with a higher [target_objective] by making crossover and mutations based on the given molecules." , where $s_{i n, x}$ is an input SMILES and $f_{x}$ is its fitness score. This prompt template is similar to those found in AI4Science \&amp; Quantum (2023); all prompts can be found in Appendix E. We then obtain an edited SMILES molecule as an output: $s_{\text {out }}=\operatorname{GPT}-4\left(t_{i n}\right)$. If $s_{\text {out }}$ cannot be</p>
<p>decoded to a valid molecule structure, we generate an offspring using the default crossover operation from Graph-GA. We demonstrate the frequency of invalid LLM edits in Appendix C.1.
$\triangleright$ MUTATION: While GPT-4 performs well as a MUTATION operator when paired with GPT-4 CROSSOVER (Appendix C.2), we found that the default Graph-GA mutation achieves comparable performance with fewer LLM queries. Therefore, we opt to use the default Graph-GA mutation here.</p>
<p>MolLEO (BioT5) BioT5 was developed with a two-phase training process using a baseline T5 model (Raffel et al., 2020). Initially, the model was trained on molecule-text data (339K samples), SELFIES structures, protein sequences, and general scientific text from multiple sources (Pei et al., 2023) using language masking as a training objective. Following this, the model was fine-tuned on specific downstream tasks, including text-based molecular generation, where molecules are generated given an input description (Edwards et al., 2022).
$\triangleright$ CROSSOVER: We use the default Graph-GA crossover.
$\triangleright$ MUTATION: For the top $Y$ molecules in the entire pool, we mutate them by prompting BioT5 with the template $t_{\text {in }}=$ "Definition: You are given a molecule SELFIES. Your job is to generate a SELFIES molecule that [target_objective]. Now complete the following example Input: <bom> $\left[l_{\text {in }}\right]&lt;$ eom&gt; Output", where $l_{\text {in }}$ is the SELFIES representation of a molecule. These prompts have the same format as those proposed in Pei et al. (2023); exact prompts for all tasks are in Appendix E. We then obtain an edited SELFIES molecule as an output: $l_{\text {out }}=\operatorname{BioT5}\left(t_{\text {in }}\right)$. We transform $l_{\text {out }}$ back to the SMILES representation and add it to the pool of offspring. Since SELFIES can always be decoded into a molecular structure, there are no issues with BioT5 generating invalid molecules. With $X$ offspring produced from crossover and $Y$ offspring from the editing procedure, we select the top $n_{c}$ offspring overall. This selection is based on structural similarity determined using Tanimoto distance to the fittest molecule in the entire pool Nigam et al. (2022).</p>
<p>MolLEO (MolSTM) MoleculeSTM was developed by jointly training molecule and text encoders on molecule-text pairs from PubChem using a contrastive loss, which maximizes the embedding similarity of each pair (Liu et al., 2023b). To enable molecular editing, they implemented a simple adaptor module to align their molecule encoder with the encoder of a pre-trained generative model. This alignment allowed them to utilize the generative model's decoder for structure generation.
$\triangleright$ CROSSOVER: We use the default Graph-GA crossover.
$\triangleright$ MUTATION: For the top $Y$ molecules in the entire pool, we edited them by following a single textconditioned editing step from (Liu et al., 2023b). Given the MoleculeSTM molecule and text encoders ( $E_{M c}$ and $E_{T c}$, respectively), a pre-trained generative model consisting of an encoder $E_{M g}$ and decoder $D_{M g}$ (Irwin et al., 2022), and an adaptor module ( $A_{g c}$ ) to align embeddings from $E_{M c}$ and $E_{M g}$, an input molecule SMILES $\left(s_{i n}\right)$ is edited towards a text prompt describing the objective by updating the embedding from $E_{M g}$. First, the molecule embedding $x_{0}$ is obtained from $E_{M g}\left(s_{i n}\right)$. Then, $x_{0}$ is updated using gradient descent for $T$ iterations:</p>
<p>$$
x_{t+1}=x_{t}-\alpha \nabla_{x_{t}} \mathcal{L}\left(x_{t}\right)
$$</p>
<p>where $\alpha$ is the learning rate and $\mathcal{L}\left(x_{t}\right)$ is defined as:</p>
<p>$$
\mathcal{L}\left(x_{t}\right)=-\text { cosine_sim }\left(E_{M c}\left(A_{g c}\left(x_{t}\right)\right), E_{T c}(\text { text_prompt })\right)+\lambda\left|x_{t}-x_{0}\right|_{2}
$$</p>
<p>$\lambda$ controls how much the embedding at iteration $t$ can deviate from the input embedding. Finally, $x_{T}$ is passed to the decoder $D_{M g}$ to generate a molecule SMILES $s_{\text {out }}$. The text prompts follow the format "This molecule {has/is/other verb property}", which follows Liu et al. (2023b); exact prompts for all tasks are in Appendix E. We ablate MolSTM hyperparameter selection in Appendix C.4. If $s_{\text {out }}$ cannot be decoded into a valid molecule (see Appendix C.1), we edit the next best molecule (so that we have $Y$ offspring after the editing has finished). Similarly to MolLEO (BIOT5), we combine the $X$ crossover and $Y$ mutated offspring and select the $n_{c}$ most similar molecules to the top molecule overall to keep.</p>
<h1>4 EXPERIMENTS</h1>
<h3>4.1 EXPERIMENTAL SETUP</h3>
<p>We evaluate MolLEO on 26 tasks from two molecular generation benchmarks, Practical Molecular Optimization (PMO) (Gao et al., 2022) and Therapeutics Data Commons (TDC) (Huang et al., 2021). Exact task definitions can be found in TDC ${ }^{1}$. We organize the tasks into the following categories:</p>
<ol>
<li>Structure-based optimization, which optimizes for molecules based on target structures. It includes isomer generation based on a target molecular formula (isomers_c7h8n2o2, isomers_c9h10n2o2pf2cl) and two tasks based on matching or avoiding scaffolds and substructure motifs (deco_hop, scaffold_hop, valsartan_smarts).</li>
<li>Name-based optimization. These tasks involve finding compounds similar to known drugs (Mestranol, Albuterol, Thiothixene, Celecoxib, troglitazone) and seven multi-property optimization tasks (MPO) that aim to rediscover drugs (Perindopril, Ranolazine, Sitagliptin, Amlodipine, Fexofenadine, Osimertinib, Zaleplon) while optimizing for other properties such as hydrophobicity (LogP) and permeability (TPSA). Two tasks, median1 and median2, aim to generate molecules with properties similar to several known drugs simultaneously. Successfully completing these tasks means that LLMs can make perturbations toward desired molecules when given a chemical optimization goal.</li>
<li>Property optimization. We first consider the trivial property optimization task QED (Bickerton et al., 2012), which measures the drug-likeness of a molecule based on a set of simple heuristics. We then focus on the three tasks that measure a molecule's activity against the following proteins: DRD2 (Dopamine receptor D2), GSK3 $\beta$ (Glycogen synthase kinase-3 beta), and JNK3 (c-Jun N-terminal kinase-3). For these tasks, molecular inhibition is determined by pre-trained classifiers that take in a SMILES string and output a value $p \in[0,1]$, where $p \geq 0.5$ predicts that a molecule inhibits protein activity. Finally, we include three protein-ligand docking tasks from TDC (Graff et al., 2021) (also referred to as structure-based drug design (Kuntz, 1992)), which are more difficult tasks closer to real-world drug design compared to simple physicochemical properties (Cieplinski et al., 2020). The proteins we consider are DRD3 (dopamine receptor D3, PDB ID: 3PBL), EGFR (epidermal growth factor receptor, PDB ID: 2RGP), and Adenosine A2A receptor (PDB ID: 3EML). Molecules are docked against the protein using AutoDock Vina (Eberhardt et al., 2021), with the output being the docking score of the binding process.</li>
</ol>
<p>To evaluate our method, we follow (Gao et al., 2022) and report the area under the curve of top- $k$ average property values versus the number of oracle calls (AUC top- $k$ ), which takes into account both the objective values and the computational budget spent. For this study, we set $k=10$ in order to identify a small, distinct set of top molecular candidates. For the multi-objective optimization, we consider two metrics: top-10 AUC for summing all optimized objectives and the hypervolume of the Pareto frontier (see Equation (5)).
For baselines, we use the highest three ranking models from the PMO benchmark (Gao et al., 2022), including REINVENT (Olivecrona et al., 2017b), an RNN that utilizes a reinforcement learning-based policy to guide generation; Graph-GA; and Gaussian process Bayesian optimization (GP BO) (Tripp et al., 2021), where a GP acquisition function is optimized with methods from Graph-GA. We also include Augmented Memory (Guo \&amp; Schwaller, 2024), which combines data augmentation with experience replay to enhance the reinforcement learning-based policy for guiding generation, as well as Differentiable Scaffolding Tree for Molecule Optimization (DST, (Fu et al., 2021)), which optimizes a molecule structure using gradient ascent in the latent space of a graph neural network trained to predict a target property.
For the initial population of molecules, we randomly sample 120 molecules from ZINC 250K (Sterling \&amp; Irwin, 2015). In all runs, we restrict the budget of oracle calls to 10,000 but terminate the algorithm early if the average fitness of the top- 100 molecules does not increase by $10^{-3}$ within 5 epochs, as was done in (Gao et al., 2022). For the docking experiments, we restrict the budget to 1000 calls due to higher evaluation costs. Additional experimental details and the choice of hyperparameters are provided in Appendix B.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Population fitness over increasing number of iterations for JNK3 inhibition. In the lightest blue, we plot the fitness distribution of the initial molecule pool. We then pass the molecules through a single round of LLM edits (pink curve), or a single round of random crossover/mutation operations (yellow curve). We then show the fitnesses of the top-10 molecules after 1000-4000 oracle calls.</p>
<p>Table 1: Top-10 AUC of single-objective tasks. The best model for each task is bolded and the top three are underlined. We also report the sum of all tasks (total) and the rank of each model overall.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task type</th>
<th style="text-align: center;">Method objective ( $\uparrow$ )</th>
<th style="text-align: center;">REINVENT</th>
<th style="text-align: center;">Augmented Memory</th>
<th style="text-align: center;">Graph GA</th>
<th style="text-align: center;">GP BO</th>
<th style="text-align: center;">MolLEO (MolSTM)</th>
<th style="text-align: center;">MolLEO (BioT5)</th>
<th style="text-align: center;">MolLEO (GPT-4)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Property optimization</td>
<td style="text-align: center;">QED</td>
<td style="text-align: center;">0.9412</td>
<td style="text-align: center;">0.9412</td>
<td style="text-align: center;">0.9402</td>
<td style="text-align: center;">0.9372</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.9372</td>
<td style="text-align: center;">0.002</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">JNK3</td>
<td style="text-align: center;">0.7832</td>
<td style="text-align: center;">0.7732</td>
<td style="text-align: center;">0.5532</td>
<td style="text-align: center;">0.5642</td>
<td style="text-align: center;">0.6432</td>
<td style="text-align: center;">0.7282</td>
<td style="text-align: center;">0.7992</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DRD2</td>
<td style="text-align: center;">0.9452</td>
<td style="text-align: center;">0.9622</td>
<td style="text-align: center;">0.9642</td>
<td style="text-align: center;">0.9232</td>
<td style="text-align: center;">0.9752</td>
<td style="text-align: center;">0.9912</td>
<td style="text-align: center;">0.9682</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GSK33</td>
<td style="text-align: center;">0.8652</td>
<td style="text-align: center;">0.8892</td>
<td style="text-align: center;">0.7882</td>
<td style="text-align: center;">0.8512</td>
<td style="text-align: center;">0.8982</td>
<td style="text-align: center;">0.841</td>
<td style="text-align: center;">0.8892</td>
</tr>
<tr>
<td style="text-align: center;">Name-based optimization</td>
<td style="text-align: center;">mestranol_similarity</td>
<td style="text-align: center;">0.6182</td>
<td style="text-align: center;">0.7642</td>
<td style="text-align: center;">0.5792</td>
<td style="text-align: center;">0.6272</td>
<td style="text-align: center;">0.689</td>
<td style="text-align: center;">0.7962</td>
<td style="text-align: center;">0.7172</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">albuteol_similarity</td>
<td style="text-align: center;">0.8962</td>
<td style="text-align: center;">0.9182</td>
<td style="text-align: center;">0.8742</td>
<td style="text-align: center;">0.9022</td>
<td style="text-align: center;">0.919</td>
<td style="text-align: center;">0.9682</td>
<td style="text-align: center;">0.9852</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">thiothizone_rediscovery</td>
<td style="text-align: center;">0.5342</td>
<td style="text-align: center;">0.5622</td>
<td style="text-align: center;">0.4792</td>
<td style="text-align: center;">0.5592</td>
<td style="text-align: center;">0.625</td>
<td style="text-align: center;">0.6962</td>
<td style="text-align: center;">0.7272</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">celecoxib_rediscovery</td>
<td style="text-align: center;">0.7162</td>
<td style="text-align: center;">0.7842</td>
<td style="text-align: center;">0.5822</td>
<td style="text-align: center;">0.7282</td>
<td style="text-align: center;">0.7942</td>
<td style="text-align: center;">0.5942</td>
<td style="text-align: center;">0.8632</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">troglitazone_rediscovery</td>
<td style="text-align: center;">0.4522</td>
<td style="text-align: center;">0.5562</td>
<td style="text-align: center;">0.3772</td>
<td style="text-align: center;">0.4052</td>
<td style="text-align: center;">0.3812</td>
<td style="text-align: center;">0.3902</td>
<td style="text-align: center;">0.6622</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">perindopril_mpo</td>
<td style="text-align: center;">0.5372</td>
<td style="text-align: center;">0.5982</td>
<td style="text-align: center;">0.5382</td>
<td style="text-align: center;">0.4932</td>
<td style="text-align: center;">0.5542</td>
<td style="text-align: center;">0.5782</td>
<td style="text-align: center;">0.6002</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ranolazine_mpo</td>
<td style="text-align: center;">0.7602</td>
<td style="text-align: center;">0.8922</td>
<td style="text-align: center;">0.7282</td>
<td style="text-align: center;">0.7352</td>
<td style="text-align: center;">0.7252</td>
<td style="text-align: center;">0.7492</td>
<td style="text-align: center;">0.7692</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">sitagliptin_mpo</td>
<td style="text-align: center;">0.0212</td>
<td style="text-align: center;">0.093</td>
<td style="text-align: center;">0.4792</td>
<td style="text-align: center;">0.4332</td>
<td style="text-align: center;">0.1862</td>
<td style="text-align: center;">0.5482</td>
<td style="text-align: center;">0.5082</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">amlodipine_mpo</td>
<td style="text-align: center;">0.6422</td>
<td style="text-align: center;">0.6662</td>
<td style="text-align: center;">0.6252</td>
<td style="text-align: center;">0.5522</td>
<td style="text-align: center;">0.6742</td>
<td style="text-align: center;">0.7762</td>
<td style="text-align: center;">0.7732</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">fexofenadine_mpo</td>
<td style="text-align: center;">0.7692</td>
<td style="text-align: center;">0.6862</td>
<td style="text-align: center;">0.610</td>
<td style="text-align: center;">0.7792</td>
<td style="text-align: center;">0.7452</td>
<td style="text-align: center;">0.7992</td>
<td style="text-align: center;">0.7732</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">osimertinib_mpo</td>
<td style="text-align: center;">0.8342</td>
<td style="text-align: center;">0.8562</td>
<td style="text-align: center;">0.813</td>
<td style="text-align: center;">0.7622</td>
<td style="text-align: center;">0.8232</td>
<td style="text-align: center;">0.7712</td>
<td style="text-align: center;">0.8472</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">zaleplon_mpo</td>
<td style="text-align: center;">0.3472</td>
<td style="text-align: center;">0.4382</td>
<td style="text-align: center;">0.4562</td>
<td style="text-align: center;">0.2722</td>
<td style="text-align: center;">0.4752</td>
<td style="text-align: center;">0.4652</td>
<td style="text-align: center;">0.5192</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">median1</td>
<td style="text-align: center;">0.3722</td>
<td style="text-align: center;">0.3352</td>
<td style="text-align: center;">0.2872</td>
<td style="text-align: center;">0.3252</td>
<td style="text-align: center;">0.2982</td>
<td style="text-align: center;">0.3182</td>
<td style="text-align: center;">0.3322</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">median2</td>
<td style="text-align: center;">0.2942</td>
<td style="text-align: center;">0.2902</td>
<td style="text-align: center;">0.2292</td>
<td style="text-align: center;">0.3082</td>
<td style="text-align: center;">0.2512</td>
<td style="text-align: center;">0.2592</td>
<td style="text-align: center;">0.2752</td>
</tr>
<tr>
<td style="text-align: center;">Structurebased optimization</td>
<td style="text-align: center;">isomers_c7b8n2o2</td>
<td style="text-align: center;">0.8422</td>
<td style="text-align: center;">0.029</td>
<td style="text-align: center;">0.9492</td>
<td style="text-align: center;">0.6622</td>
<td style="text-align: center;">0.971</td>
<td style="text-align: center;">0.9482</td>
<td style="text-align: center;">0.9282</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">isomers_c9b10n2o2pf2o1</td>
<td style="text-align: center;">0.6422</td>
<td style="text-align: center;">0.654</td>
<td style="text-align: center;">0.8302</td>
<td style="text-align: center;">0.7192</td>
<td style="text-align: center;">0.4692</td>
<td style="text-align: center;">0.8712</td>
<td style="text-align: center;">0.8742</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">deco_hop</td>
<td style="text-align: center;">0.6662</td>
<td style="text-align: center;">0.644</td>
<td style="text-align: center;">0.6882</td>
<td style="text-align: center;">0.6192</td>
<td style="text-align: center;">0.6292</td>
<td style="text-align: center;">0.6132</td>
<td style="text-align: center;">0.8272</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">scaffold_hop</td>
<td style="text-align: center;">0.5602</td>
<td style="text-align: center;">0.5652</td>
<td style="text-align: center;">0.5172</td>
<td style="text-align: center;">0.5482</td>
<td style="text-align: center;">0.5272</td>
<td style="text-align: center;">0.5592</td>
<td style="text-align: center;">0.5712</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">valsartan_smarts</td>
<td style="text-align: center;">0.0002</td>
<td style="text-align: center;">0.0002</td>
<td style="text-align: center;">0.0002</td>
<td style="text-align: center;">0.0002</td>
<td style="text-align: center;">0.0002</td>
<td style="text-align: center;">0.0002</td>
<td style="text-align: center;">0.0002</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Total ( $\uparrow$ )</td>
<td style="text-align: center;">14.036</td>
<td style="text-align: center;">15.356</td>
<td style="text-align: center;">13.823</td>
<td style="text-align: center;">13.182</td>
<td style="text-align: center;">14.557</td>
<td style="text-align: center;">15.424</td>
<td style="text-align: center;">17.862</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rank ( $\downarrow$ )</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<h1>4.2 EMPIRICAL STUDY</h1>
<p>First, we motivate the idea of why incorporating chemistry-aware LLMs in GA pipelines is effective. In Figure 2, we show the fitness distribution of an initial pool of random molecules inhibiting JNK3. We then perform a single round of edits to all molecules in the pool using each LLM and plot the resulting fitness distribution of the edited molecules. We find that the distribution for each LLM shifts to slightly higher fitness values, indicating that LLMs do provide useful modifications. However, the overall objective scores are still low, so single-step editing is not sufficient. We then show the fitness distributions of the populations as the genetic optimization progresses and find that the fitness increases to higher values on average, given the same number of oracle calls. We show the performance of direct LLM querying versus the optimization procedure for additional tasks in Appendix C.1.</p>
<p>The results of single-objective optimization across 23 tasks in PMO are shown in Table 1, reporting the AUC top-10 for each task and the overall rank of each model. We show the performance of additional baselines in Appendix D.1. The results indicate that employing any of the three LLMs we tested as genetic operators improves performance over the default Graph-GA. Notably, MolLEO (GPT-4) outperforms all models in 15 out of 23 tasks and ranks first overall, demonstrating its utility in molecular generation tasks. MolLEO (BioT5) achieves the second-best results out of all the</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Average docking score of top-10 molecules when docked against DRD3, EGFR, or Adenosine A2A receptor proteins. Lower docking scores are better. For each model, we show the convergence point (the moment of stabilization of the population scores) with a star, if the model converges before 1000 oracle calls have been made. Here, the model is considered to have converged if the mean score of the top 100 molecules does not increase by at least 1e-3 within 5 epochs.
models tested, obtaining a total score close to that of MolLEO (GPT-4), and has the benefit of being free to use. We observe that MolLEO (BioT5) generally performs better than MolLEO (MolSTM), producing a higher percentage of molecules with improved fitness after editing, as shown in Appendix C.1. For the tasks deco_hop and scaffold_hop, there is only a small gain for the open-source MolLEO models. We speculate that this is because these models have not been trained on molecular descriptions containing SMARTS patterns. Also, it is unclear how well these models perform with negative matching (e.g., This molecule does not contain the scaffold [#7]-c1n[c;h1]nc2 [c;h1]c(-[#8])[c;h0][c;h1]c12). We were also interested in knowing whether the open-source models were generating molecules that could have been seen during training. We took ZINC20 (Irwin et al., 2020), a database of 1.4 billion compounds that were used to generate the training set for BioT5, and PubChem (Kim et al., 2023)( 250K molecules), which was used to generate the training set for MoleculeSTM, and checked if the final molecules for the JNK3 task from each model appeared in the respective datasets. We found that this was not the case; there was no overlap between the generated molecules and the datasets.</p>
<p>We demonstrate empirically that MolLEO algorithms consistently converge faster than all the considered baselines, i.e., for any given budget of oracle calls, MolLEO achieves better objective values (see Appendix C.3). This is important when considering how these models can translate to real-world experiments to reduce the number of experiments needed to find ideal candidates. We also study the computational cost of MolLEO in Appendix D.4.</p>
<p>In Figure 3, we present results for more challenging protein-ligand docking tasks, which better approximate real-world molecular generation scenarios compared to those in Table 1. We plot the average docking scores of the top-10 best molecules for MolLEO and Graph-GA against the number of oracle calls. We observe that nearly all LLMs in MolLEO generate molecules with lower (better) docking scores than the baseline model for all three proteins, and they converge faster to the optimal set. Among the three LLMs, MolLEO (BioT5) achieves the best performance. Surprisingly, MolLEO (GPT-4) performs worse than Graph-GA in the Adenosine A2A receptor docking task. In practice, better docking scores and faster convergence rates could result in requiring fewer bioassays to screen molecules, making the process both more cost- and time-effective. We visualize the top-10 molecules found by MolLEO in EGFR docking and deco_hop tasks in Appendix D.8.</p>
<p>In Table 2, we show the results of our multi-objective optimization for three tasks. Tasks 1 and 2 are inspired by goals in drug discovery and aim for simultaneous optimization of three objectives: maximizing a molecule's QED, minimizing its synthetic accessibility (SA) score (meaning that it is easier to synthesize), and maximizing its binding score to either JNK3 (Task 1) or GSK3 $\beta$ (Task 2). Task 3 is more challenging as it targets five objectives simultaneously: maximizing QED and JNK3 binding, as well as minimizing GSK3 $\beta$ binding, DRD2 binding, and SAScore. We find that MolLEO (GPT-4) consistently outperforms the baseline Graph-GA in all three tasks in terms of hypervolume and summation. In Table 2, we see that the performance of open-source LLMs degrades when introducing multiple objectives into the prompt. We speculate that this performance drop may come from their inability to capture large, information-dense contexts. We further assess both the</p>
<p>Table 2: Summation and hypervolume scores of multi-objective tasks. We report the results for two aggregation methods: Summation (Sum) and Pareto optimality (PO). Sum(AUC) refers to the summation of top-10 AUC for all optimized objectives. The best results for each task are bolded.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Task 1: QED ( $\uparrow$ ), JNK3 ( $\uparrow$ ), SAscore ( $\downarrow$ )</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Task 2: QED ( $\uparrow$ ), GSK3// ( $\uparrow$ ), SAscore ( $\downarrow$ )</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Task 3: QED ( $\uparrow$ ), JNK3 ( $\uparrow$ ), SAscore ( $\downarrow$ ),GSK3// ( $\downarrow$ ), DRD2 ( $\downarrow$ )</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Aggregate objective</td>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">Sum(AUC)</td>
<td style="text-align: center;">Hypervolume</td>
<td style="text-align: center;">Sum(AUC)</td>
<td style="text-align: center;">Hypervolume</td>
<td style="text-align: center;">Sum(AUC)</td>
<td style="text-align: center;">Hypervolume</td>
</tr>
<tr>
<td style="text-align: center;">Sum</td>
<td style="text-align: center;">Graph-GA</td>
<td style="text-align: center;">1.967 $\pm 0.088$</td>
<td style="text-align: center;">0.713 $\pm 0.083$</td>
<td style="text-align: center;">2.186 $\pm 0.069$</td>
<td style="text-align: center;">0.719 $\pm 0.055$</td>
<td style="text-align: center;">3.856 $\pm 0.075$</td>
<td style="text-align: center;">0.162 $\pm 0.048$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MolLEO (MolSTM)</td>
<td style="text-align: center;">2.177 $\pm 0.178$</td>
<td style="text-align: center;">0.625 $\pm 0.162$</td>
<td style="text-align: center;">2.349 $\pm 0.132$</td>
<td style="text-align: center;">0.303 $\pm 0.024$</td>
<td style="text-align: center;">4.040 $\pm 0.097$</td>
<td style="text-align: center;">0.474 $\pm 0.193$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MolLEO (BioT5)</td>
<td style="text-align: center;">1.946 $\pm 0.222$</td>
<td style="text-align: center;">0.592 $\pm 0.199$</td>
<td style="text-align: center;">2.306 $\pm 0.120$</td>
<td style="text-align: center;">0.693 $\pm 0.093$</td>
<td style="text-align: center;">3.904 $\pm 0.092$</td>
<td style="text-align: center;">0.266 $\pm 0.201$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MolLEO (GPT-4)</td>
<td style="text-align: center;">2.367 $\pm 0.044$</td>
<td style="text-align: center;">0.752 $\pm 0.085$</td>
<td style="text-align: center;">2.543 $\pm 0.014$</td>
<td style="text-align: center;">0.832 $\pm 0.024$</td>
<td style="text-align: center;">4.017 $\pm 0.048$</td>
<td style="text-align: center;">0.606 $\pm 0.086$</td>
</tr>
<tr>
<td style="text-align: center;">PO</td>
<td style="text-align: center;">Graph-GA</td>
<td style="text-align: center;">2.120 $\pm 0.159$</td>
<td style="text-align: center;">0.603 $\pm 0.082$</td>
<td style="text-align: center;">2.339 $\pm 0.139$</td>
<td style="text-align: center;">0.640 $\pm 0.034$</td>
<td style="text-align: center;">4.051 $\pm 0.155$</td>
<td style="text-align: center;">0.606 $\pm 0.052$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MolLEO (MolSTM)</td>
<td style="text-align: center;">2.234 $\pm 0.246$</td>
<td style="text-align: center;">0.472 $\pm 0.248$</td>
<td style="text-align: center;">2.340 $\pm 0.254$</td>
<td style="text-align: center;">0.202 $\pm 0.054$</td>
<td style="text-align: center;">3.989 $\pm 0.145$</td>
<td style="text-align: center;">0.381 $\pm 0.204$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MolLEO (BioT5)</td>
<td style="text-align: center;">2.325 $\pm 0.164$</td>
<td style="text-align: center;">0.630 $\pm 0.120$</td>
<td style="text-align: center;">2.299 $\pm 0.203$</td>
<td style="text-align: center;">0.645 $\pm 0.127$</td>
<td style="text-align: center;">3.946 $\pm 0.115$</td>
<td style="text-align: center;">0.367 $\pm 0.177$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MolLEO (GPT-4)</td>
<td style="text-align: center;">2.482 $\pm 0.057$</td>
<td style="text-align: center;">0.727 $\pm 0.038$</td>
<td style="text-align: center;">2.631 $\pm 0.023$</td>
<td style="text-align: center;">0.820 $\pm 0.024$</td>
<td style="text-align: center;">4.212 $\pm 0.034$</td>
<td style="text-align: center;">0.696 $\pm 0.029$</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">JNK3 Top-10 AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Initial fitness</td>
<td style="text-align: center;">0.373 $\pm 0.079$</td>
</tr>
<tr>
<td style="text-align: center;">Graph-GA</td>
<td style="text-align: center;">0.787 $\pm 0.035$</td>
</tr>
<tr>
<td style="text-align: center;">MOLLEO (MOLSTM)</td>
<td style="text-align: center;">0.815 $\pm 0.048$</td>
</tr>
<tr>
<td style="text-align: center;">MOLLEO (BIOT5)</td>
<td style="text-align: center;">0.799 $\pm 0.036$</td>
</tr>
<tr>
<td style="text-align: center;">MOLLEO (GPT-4)</td>
<td style="text-align: center;">0.844 $\pm 0.052$</td>
</tr>
</tbody>
</table>
<p>Table 3: Initializing MolLEO with the best molecules from ZINC 250K (Sterling \&amp; Irwin, 2015). The results of three different LLMs in MolLEO and Graph-GA are compared. For all molecules in ZINC 250K, we run the JNK3 oracle and select the top 120 molecule pool. We run MolLEO initializing from this pool of molecules and optimizing JNK3. We report the top-10 AUC on the output of MolLEO.
structural and objective diversity of the Pareto optimal sets across all tasks, and we visualize these sets in objective space for MolLEO and Graph-GA on Tasks 1 and 2 (see Appendix D.7).</p>
<p>Given that the goal of EAs is to improve upon the properties of an initial pool of molecules and discover new molecules, we showcase these abilities by generating a set of molecules with higher objective values than the best-known molecules from ZINC 250K (Sterling \&amp; Irwin, 2015). That is, we initialize the molecular pool with the best molecules from ZINC 250K and run the optimization with MolLEO and Graph-GA. We report the top-10 AUC on the JNK3 task in Table 3 and find that MolLEO algorithms are consistently able to outperform the baseline model and improve upon the best values found in the existing dataset. We briefly investigate the use of retrieval augmented search in Appendix C. 5 and find that incorporating information from existing databases is helpful. To further validate the effectiveness of the LLM-based genetic operators, we compare the molecules before and after LLMs’ editing in Appendix D. 5 and check whether the optimization objectives are in the open-source LLMs training data in Appendix D.6. We also incorporate MolLEO into other GAs and generative models to validate its generalization capability in Appendix D. 2 and Appendix D.3.</p>
<h1>5 CONCLUSION</h1>
<p>Herein, we propose MolLEO: the first demonstration of incorporating LLMs into evolutionary algorithms for molecular discovery. We show that chemistry-aware LLMs can serve as informed proposal generators, resulting in superior optimization performance across multiple molecular optimization benchmarks, including protein-ligand docking. Furthermore, we show that both open-source and commercial versions of MolLEO can be used in scenarios that involve numerous objective evaluations and can generate higher-ranked candidates with fewer evaluation calls compared to baseline models. Because the structural perturbations of MolLEO are more effective than random perturbations in a genetic algorithm, it will become more feasible to deploy oracles that are computationally more expensive but more accurate in representing the target property, generating candidates that show greater promise for real-life applications. This is an important consideration due to the high experimental costs of testing candidates. As LLMs continue to advance, we anticipate that the performance of the MolLEO framework will also continue to improve, making MolLEO a promising tool for applications in generative chemistry. We introduce the future work in Appendix A.2.</p>
<h1>6 REPRODUCIbility STATEMENT</h1>
<p>Our code is available at https://github.com/zoom-wang112358/MOLLEO. We provide the experimental details and the choice of hyperparameters in Section 4.1 and Appendix B. The pseudocode of MolLEO algorithm is in Algorithm 1.</p>
<h2>7 ACKNOWLEDGEMENTS</h2>
<p>M.S. thanks Ella Rajaonson for feedback on protein-ligand docking and Austin Cheng for discussions on molecule generation. Y.D. thanks Delia Qu for helpful discussions on evolutionary algorithms. A. A.-G. thanks Dr. Anders G. Frøseth for his generous support. A. A.-G. also acknowledges the generous support of Natural Resources Canada and the Canada 150 Research Chairs program.
This work was supported in part by NSF IIS-2008334, IIS-2106961, CAREER IIS-2144338, ONR MURI N00014-17-1-2656, and computing resources from Microsoft Azure. Resources used in preparing this research were also provided, in part, by the Vector Institute and the Acceleration Consortium.</p>
<h2>REFERENCES</h2>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.</p>
<p>Microsoft Research AI4Science and Microsoft Azure Quantum. The impact of large language models on scientific discovery: a preliminary study using gpt-4. arXiv preprint arXiv:2311.07361, 2023.</p>
<p>Kenneth Atz, Francesca Grisoni, and Gisbert Schneider. Geometric deep learning on molecular representations. Nature Machine Intelligence, 3(12):1023-1032, 2021.</p>
<p>Nikhil Behari, Edwin Zhang, Yunfan Zhao, Aparna Taneja, Dheeraj Nagaraj, and Milind Tambe. A decision-language model (dlm) for dynamic restless multi-armed bandit tasks in public health. arXiv preprint arXiv:2402.14807, 2024.</p>
<p>G Richard Bickerton, Gaia V Paolini, Jérémy Besnard, Sorel Muresan, and Andrew L Hopkins. Quantifying the chemical beauty of drugs. Nature chemistry, 4(2):90-98, 2012.</p>
<p>Regine S Bohacek, Colin McMartin, and Wayne C Guida. The art and practice of structure-based drug design: a molecular modeling perspective. Med. Res. Rev., 16(1):3-50, 1996.</p>
<p>Daniil A Boiko, Robert MacKnight, Ben Kline, and Gabe Gomes. Autonomous chemical research with large language models. Nature, 624(7992):570-578, 2023.</p>
<p>Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, and Philippe Schwaller. Chemcrow: Augmenting large-language models with chemistry tools. arXiv preprint arXiv:2304.05376, 2023.</p>
<p>Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. Chemberta: Large-scale selfsupervised pretraining for molecular property prediction.</p>
<p>Dimitrios Christofidellis, Giorgio Giannone, Jannis Born, Ole Winther, Teodoro Laino, and Matteo Manica. Unifying molecular and textual representations via multi-task language modelling. In International Conference on Machine Learning, pp. 6140-6157. PMLR, 2023.</p>
<p>Tobiasz Cieplinski, Tomasz Danel, Sabina Podlewska, and Stanislaw Jastrzebski. We should at least be able to design molecules that dock well. arXiv preprint arXiv:2006.16955, 2020.</p>
<p>Kourosh Darvish, Marta Skreta, Yuchi Zhao, Naruki Yoshikawa, Sagnik Som, Miroslav Bogdanovic, Yang Cao, Han Hao, Haoping Xu, Alán Aspuru-Guzik, et al. Organa: A robotic assistant for automated chemistry experimentation and characterization. arXiv preprint arXiv:2401.06949, 2024.</p>
<p>Inc. Daylight Chemical Information Systems. Smarts-a language for describing molecular patterns, 2007.</p>
<p>Kalyanmoy Deb and Himanshu Jain. An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting approach, part i: solving problems with box constraints. IEEE transactions on evolutionary computation, 18(4):577-601, 2013.</p>
<p>Yuanqi Du, Xian Liu, Nilay Mahesh Shah, Shengchao Liu, Jieyu Zhang, and Bolei Zhou. Chemspace: Interpretable and interactive chemical space exploration. Transactions on Machine Learning Research, 2022.</p>
<p>Yuanqi Du, Arian R. Jamasb, Jeff Guo, Tianfan Fu, Charles Harris, Yingheng Wang, Chenru Duan, Pietro Liò, Philippe Schwaller, and Tom L. Blundell. Machine learning-aided generative molecular design. Nature Machine Intelligence, June 2024. ISSN 2522-5839. doi: 10.1038/ s42256-024-00843-5. URL https://doi.org/10.1038/s42256-024-00843-5.</p>
<p>Jerome Eberhardt, Diogo Santos-Martins, Andreas F Tillack, and Stefano Forli. Autodock vina 1.2. 0: New docking methods, expanded force field, and python bindings. Journal of chemical information and modeling, 61(8):3891-3898, 2021.</p>
<p>Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, and Heng Ji. Translation between molecules and natural language. arXiv preprint arXiv:2204.11817, 2022.</p>
<p>Sean Ekins, J Dana Honeycutt, and James T Metz. Evolving molecules using multi-objective optimization: applying to adme/tox. Drug discovery today, 15(11-12):451-460, 2010.</p>
<p>Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rocktäschel. Promptbreeder: Self-referential self-improvement via prompt evolution. arXiv preprint arXiv:2309.16797, 2023.</p>
<p>Daniel Flam-Shepherd and Alán Aspuru-Guzik. Language models can generate molecules, materials, and protein binding sites directly in three dimensions as xyz, cif, and pdb files. arXiv preprint arXiv:2305.05708, 2023.</p>
<p>Tianfan Fu, Wenhao Gao, Cao Xiao, Jacob Yasonik, Connor W Coley, and Jimeng Sun. Differentiable scaffolding tree for molecular optimization. arXiv preprint arXiv:2109.10469, 2021.</p>
<p>Tianfan Fu, Wenhao Gao, Connor Coley, and Jimeng Sun. Reinforced genetic algorithm for structurebased drug design. Advances in Neural Information Processing Systems, 35:12325-12338, 2022.</p>
<p>Wenhao Gao, Rocío Mercado, and Connor W Coley. Amortized tree generation for bottom-up synthesis planning and synthesizable molecular design. arXiv preprint arXiv:2110.06389, 2021.</p>
<p>Wenhao Gao, Tianfan Fu, Jimeng Sun, and Connor Coley. Sample efficiency matters: a benchmark for practical molecular optimization. Advances in neural information processing systems, 35: $21342-21357,2022$.</p>
<p>Tobias Gensch, Gabriel dos Passos Gomes, Pascal Friederich, Ellyn Peters, Théophile Gaudin, Robert Pollice, Kjell Jorner, AkshatKumar Nigam, Michael Lindner-D’Addario, Matthew S Sigman, et al. A comprehensive discovery platform for organophosphorus ligands for catalysis. Journal of the American Chemical Society, 144(3):1205-1217, 2022.</p>
<p>AM Geoffrion. Proper efficiencyand the theory of vector optimization. J. Math. Anal. Appl, 22, 1968.
Rafael Gómez-Bombarelli, Jennifer N Wei, David Duvenaud, José Miguel Hernández-Lobato, Benjamín Sánchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Alán Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS central science, 4(2):268-276, 2018.</p>
<p>DE Graff, EI Shakhnovich, and CW Coley. Accelerating high-throughput virtual screening through molecular pool-based active learning, chem. Sci, 12:7866-7881, 2021.</p>
<p>Ryan-Rhys Griffiths and José Miguel Hernández-Lobato. Constrained bayesian optimization for automatic chemical design using variational autoencoders. Chemical science, 11(2):577-586, 2020 .</p>
<p>Jeff Guo and Philippe Schwaller. Augmented memory: Capitalizing on experience replay to accelerate de novo molecular design. arXiv preprint arXiv:2305.16160, 2023.</p>
<p>Jeff Guo and Philippe Schwaller. Augmented memory: Sample-efficient generative molecular design with reinforcement learning. Jacs $A u, 2024$.</p>
<p>Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu Yang. Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. In The Twelfth International Conference on Learning Representations, 2023a.</p>
<p>Taicheng Guo, Bozhao Nan, Zhenwen Liang, Zhichun Guo, Nitesh Chawla, Olaf Wiest, Xiangliang Zhang, et al. What can large language models do in chemistry? a comprehensive benchmark on eight tasks. Advances in Neural Information Processing Systems, 36:59662-59688, 2023b.</p>
<p>John H Holland. Genetic algorithms. Scientific american, 267(1):66-73, 1992.
Emiel Hoogeboom, Víctor Garcia Satorras, Clément Vignac, and Max Welling. Equivariant diffusion for molecule generation in 3d. In International conference on machine learning, pp. 8867-8887. PMLR, 2022.</p>
<p>Kexin Huang, Tianfan Fu, Wenhao Gao, Yue Zhao, Yusuf H Roohani, Jure Leskovec, Connor W Coley, Cao Xiao, Jimeng Sun, and Marinka Zitnik. Therapeutics data commons: Machine learning datasets and tasks for drug discovery and development. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2021.</p>
<p>John J Irwin, Khanh G Tang, Jennifer Young, Chinzorig Dandarchuluun, Benjamin R Wong, Munkhzul Khurelbaatar, Yurii S Moroz, John Mayfield, and Roger A Sayle. Zinc20—a free ultralarge-scale chemical database for ligand discovery. Journal of chemical information and modeling, 60(12):6065-6073, 2020.</p>
<p>Ross Irwin, Spyridon Dimitriadis, Jiazhen He, and Esben Jannik Bjerrum. Chemformer: a pre-trained transformer for computational chemistry. Machine Learning: Science and Technology, 3(1): 015022, 2022.</p>
<p>Kevin Maik Jablonka, Philippe Schwaller, Andres Ortega-Guerrero, and Berend Smit. Leveraging large language models for predictive chemistry. Nature Machine Intelligence, pp. 1-9, 2024.</p>
<p>Jan H Jensen. A graph-based genetic algorithm and generative model/monte carlo tree search for the exploration of chemical space. Chemical science, 10(12):3567-3572, 2019.</p>
<p>Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for molecular graph generation. In International conference on machine learning, pp. 2323-2332. PMLR, 2018.</p>
<p>Hyeonah Kim, Minsu Kim, Sanghyeok Choi, and Jinkyoo Park. Genetic-guided gflownets for sample efficient molecular optimization. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024.</p>
<p>Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, et al. Pubchem 2023 update. Nucleic acids research, 51(D1): D1373-D1380, 2023.</p>
<p>Mario Krenn, Florian Häse, AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik. Selfreferencing embedded strings (selfies): A 100\% robust molecular string representation. Machine Learning: Science and Technology, 1(4):045024, 2020.</p>
<p>Agustinus Kristiadi, Felix Strieth-Kalthoff, Marta Skreta, Pascal Poupart, Alán Aspuru-Guzik, and Geoff Pleiss. A sober look at llms for material discovery: Are they actually good for bayesian optimization over molecules? arXiv preprint arXiv:2402.05015, 2024.</p>
<p>Irwin D Kuntz. Structure-based strategies for drug design and discovery. Science, 257(5073): 1078-1082, 1992.</p>
<p>Nathanael Kusanda, Gary Tom, Riley Hickman, AkshatKumar Nigam, Kjell Jorner, and Alan AspuruGuzik. Assessing multi-objective optimization of molecules with genetic algorithms against relevant baselines. In AI for Accelerated Materials Design NeurIPS 2022 Workshop, 2022.</p>
<p>Joel Lehman, Jonathan Gordon, Shawn Jain, Kamal Ndousse, Cathy Yeh, and Kenneth O Stanley. Evolution through large models. In Handbook of Evolutionary Machine Learning, pp. 331-366. Springer, 2023.</p>
<p>Xi Lin, Zhiyuan Yang, and Qingfu Zhang. Pareto set learning for neural multi-objective combinatorial optimization. In International Conference on Learning Representations, 2022. URL https: //openreview.net/forum?id=QuObT9BTWo.</p>
<p>Shengcai Liu, Caishun Chen, Xinghua Qu, Ke Tang, and Yew-Soon Ong. Large language models as evolutionary optimizers. arXiv preprint arXiv:2310.19046, 2023a.</p>
<p>Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu, Jian Tang, Chaowei Xiao, and Animashree Anandkumar. Multi-modal molecule structure-text model for text-based retrieval and editing. Nature Machine Intelligence, 5(12):1447-1457, 2023b.</p>
<p>Shengchao Liu, Jiongxiao Wang, Yijin Yang, Chengpeng Wang, Ling Liu, Hongyu Guo, and Chaowei Xiao. Conversational drug editing using retrieval and domain feedback. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id= yRrPfKyJQ2.</p>
<p>Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward design via coding large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=IEduRUO55F.</p>
<p>Kaushalya Madhawa, Katushiko Ishiguro, Kosuke Nakago, and Motoki Abe. Graphnvp: An invertible flow model for generating molecular graphs. arXiv preprint arXiv:1905.11600, 2019.</p>
<p>Adrian Mirza, Nawaf Alampara, Sreekanth Kunchapu, Benedict Emoekabu, Aswanth Krishnan, Mara Wilhelmi, Macjonathan Okereke, Juliane Eberhardt, Amir Mohammad Elahi, Maximilian Greiner, et al. Are large language models superhuman chemists? arXiv preprint arXiv:2404.01475, 2024.</p>
<p>AkshatKumar Nigam, Robert Pollice, and Alan Aspuru-Guzik. Janus: parallel tempered genetic algorithm guided by deep neural networks for inverse molecular design. arXiv preprint arXiv:2106.04011, 2021.</p>
<p>AkshatKumar Nigam, Robert Pollice, and Alán Aspuru-Guzik. Parallel tempered genetic algorithm guided by deep neural networks for inverse molecular design. Digital Discovery, 1(4):390-404, 2022.</p>
<p>AkshatKumar Nigam, Robert Pollice, Gary Tom, Kjell Jorner, John Willes, Luca Thiede, Anshul Kundaje, and Alán Aspuru-Guzik. Tartarus: A benchmarking platform for realistic and practical inverse molecular design. Advances in Neural Information Processing Systems, 36:3263-3306, 2023.</p>
<p>Marcus Olivecrona, Thomas Blaschke, Ola Engkvist, and Hongming Chen. Molecular de-novo design through deep reinforcement learning. Journal of cheminformatics, 9:1-14, 2017a.</p>
<p>Marcus Olivecrona, Thomas Blaschke, Ola Engkvist, and Hongming Chen. Molecular de novo design through deep reinforcement learning. CoRR, abs/1704.07555, 2017b. URL http://arxiv.org/ abs/1704.07555.</p>
<p>Hakime Öztürk, Arzucan Özgür, Philippe Schwaller, Teodoro Laino, and Elif Ozkirimli. Exploring chemical space using natural language processing methodologies for drug discovery. Drug Discovery Today, 25(4):689-705, 2020.</p>
<p>Qizhi Pei, Wei Zhang, Jinhua Zhu, Kehan Wu, Kaiyuan Gao, Lijun Wu, Yingce Xia, and Rui Yan. BioT5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1102-1123, 2023.</p>
<p>Mariya Popova, Mykhailo Shvets, Junier Oliva, and Olexandr Isayev. Molecularrnn: Generating realistic molecular graphs with optimized properties. arXiv preprint arXiv:1905.13372, 2019.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1-67, 2020.</p>
<p>Mayk Caldas Ramos, Shane S Michtavy, Marc D Porosoff, and Andrew D White. Bayesian optimization of catalysts with in-context learning. arXiv preprint arXiv:2304.05341, 2023.</p>
<p>Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, et al. Mathematical discoveries from program search with large language models. Nature, 625(7995):468-475, 2024.</p>
<p>Arne Schneuing, Yuanqi Du, Charles Harris, Arian Jamasb, Ilia Igashov, Weitao Du, Tom Blundell, Pietro Lió, Carla Gomes, Max Welling, et al. Structure-based drug design with equivariant diffusion models. arXiv preprint arXiv:2210.13695, 2022.</p>
<p>Philippe Schwaller, Teodoro Laino, Théophile Gaudin, Peter Bolgar, Christopher A Hunter, Costas Bekas, and Alpha A Lee. Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction. ACS central science, 5(9):1572-1583, 2019.</p>
<p>Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang. Graphaf: a flow-based autoregressive model for molecular graph generation. 2020.</p>
<p>Dong-Hee Shin, Young-Han Son, Ji-Wung Han, Tae-Eui Kam, et al. Dymol: Dynamic many-objective molecular optimization with objective decomposition and progressive optimization. In ICLR 2024 Workshop on Generative and Experimental Perspectives for Biomolecular Design.</p>
<p>Michael A Skinnider. Invalid smiles are beneficial rather than detrimental to chemical language models. Nature Machine Intelligence, pp. 1-12, 2024.</p>
<p>Teague Sterling and John J Irwin. Zinc 15-ligand discovery for everyone. Journal of chemical information and modeling, 55(11):2324-2337, 2015.</p>
<p>Jonathan M Stokes, Kevin Yang, Kyle Swanson, Wengong Jin, Andres Cubillos-Ruiz, Nina M Donghia, Craig R MacNair, Shawn French, Lindsey A Carfrae, Zohar Bloom-Ackermann, et al. A deep learning approach to antibiotic discovery. Cell, 180(4):688-702, 2020.</p>
<p>Dagmar Stumpfe and Jürgen Bajorath. Exploring activity cliffs in medicinal chemistry: miniperspective. Journal of medicinal chemistry, 55(7):2932-2942, 2012.</p>
<p>Mengying Sun, Jing Xing, Han Meng, Huijun Wang, Bin Chen, and Jiayu Zhou. Molsearch: searchbased multi-objective molecular generation and property optimization. In Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining, pp. 4724-4732, 2022.</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022.</p>
<p>Gary Tom, Stefan P. Schmid, Sterling G. Baird, Yang Cao, Kourosh Darvish, Han Hao, Stanley Lo, Sergio Pablo-García, Ella M. Rajaonson, Marta Skreta, and et al. Self-driving laboratories for chemistry and materials science. ChemRxiv, 2024. doi: 10.26434/chemrxiv-2024-rj946.</p>
<p>Austin Tripp, Gregor N. C. Simm, and José Miguel Hernández-Lobato. A fresh look at de novo molecular design benchmarks. In NeurIPS 2021 AI for Science Workshop, 2021. URL https: //openreview.net/forum?id=gS3XMun4cl_.</p>
<p>Vahe Tshitoyan, John Dagdelen, Leigh Weston, Alexander Dunn, Ziqin Rong, Olga Kononova, Kristin A Persson, Gerbrand Ceder, and Anubhav Jain. Unsupervised word embeddings capture latent knowledge from materials science literature. Nature, 571(7763):95-98, 2019.</p>
<p>Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. Scibench: Evaluating college-level scientific problem-solving abilities of large language models. arXiv preprint arXiv:2307.10635, 2023.</p>
<p>Guanghao Wei, Yining Huang, Chenru Duan, Yue Song, and Yuanqi Du. Navigating chemical space with latent flows. arXiv preprint arXiv:2405.03987, 2024.</p>
<p>David Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. Journal of chemical information and computer sciences, 28(1):31-36, 1988.</p>
<p>Andrew D White. The future of chemistry is language. Nature Reviews Chemistry, 7(7):457-458, 2023.</p>
<p>Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=Bb4VGOWELI.</p>
<p>Xiufeng Yang, Jinzhe Zhang, Kazuki Yoshizoe, Kei Terayama, and Koji Tsuda. Chemts: an efficient python library for de novo molecular generation. Science and technology of advanced materials, 18(1):972-976, 2017.</p>
<p>Geyan Ye, Xibao Cai, Houtim Lai, Xing Wang, Junhong Huang, Longyue Wang, Wei Liu, and Xiangxiang Zeng. Drugassist: A large language model for molecule optimization. arXiv preprint arXiv:2401.10334, 2023.</p>
<p>Naruki Yoshikawa, Kei Terayama, Masato Sumita, Teruki Homma, Kenta Oono, and Koji Tsuda. Population-based de novo molecule generation, using grammatical evolution. Chemistry Letters, 47(11):1431-1434, 2018.</p>
<p>Naruki Yoshikawa, Marta Skreta, Kourosh Darvish, Sebastian Arellano-Rubach, Zhi Ji, Lasse Bjørn Kristensen, Andrew Zou Li, Yuchi Zhao, Haoping Xu, Artur Kuramshin, et al. Large language models for chemistry robotics. Autonomous Robots, 47(8):1057-1086, 2023.</p>
<p>Chengxi Zang and Fei Wang. Moflow: an invertible flow model for generating molecular graphs. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery \&amp; data mining, pp. 617-626, 2020.</p>
<p>Qingfu Zhang and Hui Li. Moea/d: A multiobjective evolutionary algorithm based on decomposition. IEEE Transactions on evolutionary computation, 11(6):712-731, 2007.</p>
<p>Yiheng Zhu, Jialu Wu, Chaowen Hu, Jiahuan Yan, Tingjun Hou, Jian Wu, et al. Sample-efficient multi-objective molecular optimization with gflownets. Advances in Neural Information Processing Systems, 36, 2024.</p>
<h1>Appendix</h1>
<h2>A EXTENDED DESCRIPTIONS</h2>
<h2>A. 1 EXTENDED RELATED WORK</h2>
<p>Benchmarking LLMs on Chemistry Tasks ChemLLMBench benchmarked several widely-used LLMs on a set of eight chemistry tasks, such as property prediction, reaction prediction, and molecule captioning (Guo et al., 2023b). The results showed that while LLMs can perform well in selection tasks, they struggle with tasks requiring more in-depth chemical reasoning, such as propertyconditioned generation. This motivates the need to improve how LLMs are used in generative tasks. Similarly, SciBench evaluated LLMs on free-response college-level exam questions across various science disciplines, including chemistry, which required complex, multi-step solutions (Wang et al., 2023). Their results indicated that LLMs were unable to generate correct solutions for the majority of questions (Wang et al., 2023). However, progress of LLMs has been noted in general questionanswering capabilities: a recent work introduced ChemBench, a dataset of over 7,000 question-answer pairs aimed at providing a systematic understanding of LLM capabilities across different subdomains in chemistry (Mirza et al., 2024). It was concluded that state-of-the-art LLMs such as GPT-4 and Claude 3 were able to beat human chemists on these questions on average, although they still struggle with physical and commonsense chemical reasoning.</p>
<p>LLMs and Evolutionary Algorithms Previous research has demonstrated that language models can be incorporated as operators in evolutionary algorithms in applications such as code and prompt generation (Lehman et al., 2023). For example, OPRO and LMEA use LLMs to optimize solutions for different mathematical optimization problems (Yang et al., 2024; Liu et al., 2023a). Other works have shown that LLMs can be used as crossover and mutation operators to directly optimize prompts using a training set, outperforming human-engineered prompts (Fernando et al., 2023; Guo et al., 2023a). Other applications of LLMs in evolutionary frameworks have been code synthesis (FunSearch (Romera-Paredes et al., 2024)), generation of reward functions in RL for robot control (Eureka (Ma et al., 2024), and resource allocation in public health settings (Behari et al., 2024).</p>
<p>Multi-objective optimization frameworks In our work, we study the effectiveness of LLM-based mutations in a multi-objective molecular optimization setting. To ensure simplicity and clarity in our evaluation, we adopt straightforward approaches, such as the sum of objectives and Pareto set selection as selection criteria. Classic methods like MOEA/D (Zhang \&amp; Li, 2007) and NSGA-III (Deb \&amp; Jain, 2013) are designed to handle scenarios where the Pareto set exceeds the population capacity. MOEA/D uses decomposition to assign each solution to a specific subproblem defined by a weight vector. If the size of the Pareto set exceeds the population size, MOEA/D will select solutions based on their contribution to specific subproblems, so that it can ensure a balance between diversity and convergence. NSGA-III uses reference points in the objective space to maintain diversity. When the Pareto front size exceeds the population, a clustering mechanism based on the reference points is applied to select solutions that best represent different regions of the Pareto front. Additionally, there are also some recent works focusing on this topic. For example, Sun et al. (2022) developed a Monte Carlo tree search algorithm that evaluates rewards by comparing new molecular structures against a maintained global Pareto set. Shin et al. introduced a method to decompose the optimization process into a progressive sequence based on the order of objectives. Zhu et al. (2024) integrated GFlowNets with a preference-conditioned sum of objective functions, further advancing the optimization landscape.</p>
<h2>A. 2 Future work</h2>
<p>Molecular discovery and design is a rich field with numerous practical applications, many of which extend beyond the current study's scope but remain relevant to the proposed framework. Integrating LLMs into evolutionary algorithms offers versatility through plain text specifications, suggesting that the MolLEO framework can be applied to scenarios such as drug discovery, expensive in silico simulations, and the design of materials or large biomolecules. Future work will aim to further improve the quality of proposed candidates, both in terms of their objective values and the speed with which they are found.</p>
<h1>A. 3 COMPUTATIONAL RESOURCES</h1>
<p>Our experiments were computed on NVIDIA A100-SXM4-80GB and T4V2 GPUs. Some of our experiments utilized the GPT-4 model; this refers to the gpt-4-turbo checkpoint from 2023-07-01 2. All GPT-4 checkpoints were hosted on Microsoft Azure ${ }^{3}$.</p>
<h2>A. 4 LIMITATIONS</h2>
<p>All benchmarks and tasks evaluated in this study are proxies for real chemical properties and may not correctly capture the true chemical performance of molecules in the real world. Thus, the effectiveness of our model in real-world applications remains to be thoroughly validated.</p>
<h2>A. 5 BROADER IMPACT</h2>
<p>The methods proposed in this paper aim to find compounds with desired properties more efficiently, which can benefit many areas, including drug discovery and materials design. While we do not foresee negative societal impacts from our methods, we acknowledge the potential of their dual use for nefarious purposes. We encourage discussions around these issues and strongly support the development and deployment of safeguards to prevent them.</p>
<h2>B HYPERPARAMETERS AND ADDITIONAL EXPERIMENTAL DETAILS</h2>
<p>For the choice of hyperparameters, we use the best practices from Graph-GA (Jensen, 2019), the baseline genetic algorithm that we build our method upon. We kept the best hyperparameters that were determined in (Gao et al., 2022). In each iteration, Graph-GA samples two molecules with a probability proportional to their fitnesses for crossover and mutation and then randomly mutates the offspring with probability $p_{m}=0.067$. This process is repeated to generate 70 offspring. The fitnesses of the offspring are measured, and the top 120 most fit molecules in the entire pool are kept for the next generation. For docking experiments, we reduce the number of generated offspring to 7 and the population size to 12 due to long experiment runtimes. We set the maximum number of oracle calls to 10,000 for all experiments except docking, where we set it to 1,000 . We kept the default early-stopping criterion the same as in PMO (Gao et al., 2022), which is that we terminate the algorithm if the mean score of the top 100 molecules does not increase by at least 1e-3 within five epochs.
In the multi-objective optimization tasks, we applied a simple transformation by using $1-$ score for the objectives involving minimization. Also, we ensure all objectives remain within the range of 0 to 1 by using normalization. This approach allows for consistent scalarization and comparability across objectives.</p>
<p>MolLEO (MolSTM) involves additional hyperparameters when doing gradient descent; we investigate their selection in Appendix C.4. Additionally, we investigate design choices for MolLEO (GPT-4) in Appendix C.5. We use three tasks for model development: JNK3, perindopril_mpo, and isomers_c9h10n2o2pf2cl; the rest are only evaluated during test-time. For each model, we show the prompts we used in Appendix E. We created prompts similar to those demonstrated in the original source code of each model, replacing each template with a task description. We briefly investigate the impact of prompt selection in Appendix C.6.</p>
<p>All experiments are conducted with five random seeds. The computational resources we utilized are described in Appendix A.3.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table A1: Viability of LLM edits. We prompt different LLMs with descriptions of JNK3 and perindopril_mpo target objectives on an initial random pool of molecules drawn from 5 random seeds. We report the percentage of valid molecules (number of valid molecules/number of total molecules), the percentage of molecules with higher fitness after editing, and the mean fitness increase of those molecules.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">MoleculeSTM</th>
<th style="text-align: center;">BioT5</th>
<th style="text-align: center;">GPT-4</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Percent valid molecules</td>
<td style="text-align: center;">peridopril_mpo:</td>
<td style="text-align: center;">peridopril_mpo:</td>
<td style="text-align: center;">peridopril_mpo:</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.938</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.862</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">JNK3:</td>
<td style="text-align: center;">JNK3:</td>
<td style="text-align: center;">JNK3:</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.928</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.835</td>
</tr>
<tr>
<td style="text-align: center;">Percent molecules with higher fitness after editting</td>
<td style="text-align: center;">peridopril_mpo:</td>
<td style="text-align: center;">peridopril_mpo:</td>
<td style="text-align: center;">peridopril_mpo:</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.456</td>
<td style="text-align: center;">0.568</td>
<td style="text-align: center;">0.240</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">JNK3:</td>
<td style="text-align: center;">JNK3:</td>
<td style="text-align: center;">JNK3:</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.206</td>
<td style="text-align: center;">0.513</td>
<td style="text-align: center;">0.263</td>
</tr>
<tr>
<td style="text-align: center;">Mean fitness increase</td>
<td style="text-align: center;">peridopril_mpo:</td>
<td style="text-align: center;">peridopril_mpo:</td>
<td style="text-align: center;">peridopril_mpo:</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+0.033$</td>
<td style="text-align: center;">$+0.208$</td>
<td style="text-align: center;">$+0.032$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">JNK3:</td>
<td style="text-align: center;">JNK3:</td>
<td style="text-align: center;">JNK3:</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+0.022$</td>
<td style="text-align: center;">$+0.0320$</td>
<td style="text-align: center;">$+0.0262$</td>
</tr>
</tbody>
</table>
<h1>C Ablation STUDIES</h1>
<h2>C. 1 PERFORMANCE OF SINGLE-STEP MOLECULE EDITING</h2>
<p>To motivate the incorporation of LLMs into a GA framework, we directly query the LLMs we consider to edit a molecule towards a certain property and calculate: (1) the percentage of valid molecules that are output (given that not all SMILES are valid molecules) and (2) which of the output molecules have higher fitness. We show these results on the JNK3 inhibition task in Table A1 and find that MolSTM and GPT-4 are not always able to produce valid molecules, whereas BioT5 always is due to its use of SELFIES. We also found that BioT5 produced more molecules with higher fitness values compared to the other LLMs.</p>
<p>In Table A3, we show the performance of directly querying LLMs with an initial pool of molecules on additional tasks. We find that while LLMs are able to edit the molecule pool to improve fitness marginally, using them in an optimization framework results in much better fitness values.</p>
<h2>C. 2 INCORPORATING LLM-BASED GENETIC OPERATORS INTO GRAPH-GA</h2>
<p>There are many ways to incorporate LLMs as genetic operators in a GA framework. We investigate several options. First, we investigate using LLMs as a crossover operator. For GPT-4 and BioT5, we gave each model two parent molecules as input and a description of the objective, and asked the model to produce a molecule as an output. Because MolSTM aligns molecule embeddings with text embeddings, our crossover operation was to either take a linear or spherical interpolation of the parent molecule embeddings and maximize the similarity of the resulting embedding to the text objective. For the mutation operator, we prompted each LLM with a molecule and a description of the objective. Finally, we investigated the impact of applying a selection pressure in the form of a filter, where we only mutated the top $Y$ molecules and pruned the resulting offspring by distance to the best molecule overall. We show the results for all operator settings we tried in Table A2 and show which operators we ended up using for each LLM in the final framework.</p>
<h2>C. 3 OPTIMIZATION TRENDS OVER SINGLE-OBJECTIVE TASKS.</h2>
<p>In Figure A1, we show the optimization curves for three tasks: JNK3, perindopril_mpo, and isomers_c9h10n2o2pf2cl.</p>
<h2>C. 4 MOLECULESTM HYPERPARAMETER SELECTION</h2>
<p>MolSTM has several hyperparameters; in this section, we motivate our choices for the final model. The first is the number of population members that are selected to undergo LLM-based mutations</p>
<p>Table A2: Top-10 AUC on 5 random seeds for the JNK3 and perindopril_mpo tasks using different combinations of genetic operators. The operators used for each model to compute the final results in the main paper are indicated with a symbol.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Operators</th>
<th style="text-align: center;">Graph-GA <br> (Baseline)</th>
<th style="text-align: center;">MolLEO (MolSTM)</th>
<th style="text-align: center;">MolLEO (BioT5)</th>
<th style="text-align: center;">MolLEO (GPT-4)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">(Default Graph-GA settings) <br> CROSSOVER: <br> Random <br> MUTATION: <br> Random, $p_{\mathrm{m}}=0.067$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { peridopril_mpo: } \ &amp; 0.538 \pm 0.009 \ &amp; \text { JNK3: } \ &amp; 0.553 \pm 0.136 \end{aligned}$</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;">CROSSOVER: <br> LLM <br> MUTATION: <br> Random, $p_{\mathrm{m}}=0.067$</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { peridopril_mpo: } \ &amp; 0.499 \pm 0.012 \text { [linear] } \ &amp; 0.505 \pm 0.018 \text { [spherical] } \ &amp; \text { JNK3: } \ &amp; 0.722 \pm 0.046 \text { [linear] } \ &amp; 0.744 \pm 0.055 \text { [spherical] } \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { peridopril_mpo: } \ &amp; 0.727 \pm 0.013 \ &amp; \text { JNK3: } \ &amp; 0.436 \pm 0.052 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { peridopril_mpo: } \ &amp; 0.600 \pm 0.031 \ &amp; \text { JNK3: } \ &amp; 0.790 \pm 0.027 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">CROSSOVER: <br> Random <br> MUTATION: <br> LLM, $p_{\mathrm{m}}=0.067$</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { peridopril_mpo: } \ &amp; 0.532 \pm 0.034 \ &amp; \text { JNK3: } \ &amp; 0.631 \pm 0.327 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { peridopril_mpo: } \ &amp; 0.676 \pm 0.034 \ &amp; \text { JNK3: } \ &amp; 0.650 \pm 0.096 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { peridopril_mpo: } \ &amp; 0.552 \pm 0.024 \ &amp; \text { JNK3: } \ &amp; 0.673 \pm 0.047 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">CROSSOVER: <br> Random <br> MUTATION: <br> LLM, $p_{\mathrm{m}}=1$</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { peridopril_mpo: } \ &amp; 0.513 \pm 0.040 \ &amp; \text { JNK3: } \ &amp; 0.553 \pm 0.193 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { peridopril_mpo: } \ &amp; 0.686 \pm 0.343 \ &amp; \text { JNK3: } \ &amp; 0.708 \pm 0.030 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { peridopril_mpo: } \ &amp; 0.615 \pm 0.058 \ &amp; \text { JNK3: } \ &amp; 0.762 \pm 0.044 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">CROSSOVER: <br> Random <br> MUTATION: <br> Selected top $Y$ molecules, randomly mutated, pruned offspring by distance to top-1 molecule</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { peridopril_mpo: } \ &amp; 0.579 \pm 0.044 \ &amp; \text { JNK3: } \ &amp; 0.571 \pm 0.109 \end{aligned}$</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;">CROSSOVER: <br> Random <br> MUTATION: <br> Selected top $Y$ molecules, mutated with LLM, pruned offspring by distance to top-1 molecule</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { peridopril_mpo: } \ &amp; 0.554 \pm 0.034 \ &amp; \text { JNK3: } \ &amp; 0.730 \pm 0.188 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { peridopril_mpo: } \ &amp; 0.740 \pm 0.032 \ &amp; \text { JNK3: } \ &amp; 0.728 \pm 0.079 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { peridopril_mpo: } \ &amp; 0.575 \pm 0.074 \ &amp; \text { JNK3: } \ &amp; 0.758 \pm 0.031 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">CROSSOVER: <br> LLM <br> MUTATION: <br> Selected top $Y$ molecules, mutated with LLM, pruned offspring by distance to top-1 molecule</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { peridopril_mpo: } \ &amp; 0.490 \pm 0.016 \text { [linear] } \ &amp; 0.517 \pm 0.006 \text { [spherical] } \ &amp; \text { JNK3: } \ &amp; 0.692 \pm 0.110 \text { [linear] } \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { peridopril_mpo: } \ &amp; 0.736 \pm 0.014 \ &amp; \text { JNK3: } \ &amp; 0.429 \pm 0.110 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { peridopril_mpo: } \ &amp; 0.592 \pm 0.035 \ &amp; \text { JNK3: } \ &amp; 0.794 \pm 0.026 \end{aligned}$</td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure A1: Average of top-10 molecules generated by MolLEO and Graph-GA models for three tasks over an increasing number of oracle calls. For each model, we show the convergence point with a star. The model is considered to have converged if the mean score of the top 100 molecules does not increase by at least $1 \mathrm{e}-3$ within five epochs.</p>
<p>Table A3: Ablation studies of LLM editing based on direct user queries. Top-10 average objective scores are reported.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">JNK3</th>
<th style="text-align: center;">isomers_c9h10n2o2pf2c1</th>
<th style="text-align: center;">perindopril_mpo</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Initial population</td>
<td style="text-align: center;">$0.085 \pm 0.010$</td>
<td style="text-align: center;">$0.101 \pm 0.025$</td>
<td style="text-align: center;">$0.281 \pm 0.026$</td>
</tr>
<tr>
<td style="text-align: center;">MolSTM - direct query</td>
<td style="text-align: center;">$0.084 \pm 0.008$</td>
<td style="text-align: center;">$0.201 \pm 0.040$</td>
<td style="text-align: center;">$0.390 \pm 0.008$</td>
</tr>
<tr>
<td style="text-align: center;">MolLEO (MolSTM)</td>
<td style="text-align: center;">$\mathbf{0 . 7 1 6} \pm \mathbf{0 . 2 4 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 0 5} \pm \mathbf{0 . 0 3 7 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 7 2} \pm \mathbf{0 . 0 4 1}$</td>
</tr>
<tr>
<td style="text-align: center;">BioT5 - direct query</td>
<td style="text-align: center;">$0.109 \pm 0.012$</td>
<td style="text-align: center;">$0.260 \pm 0.076$</td>
<td style="text-align: center;">$0.648 \pm 0.019$</td>
</tr>
<tr>
<td style="text-align: center;">MolLEO (BioT5)</td>
<td style="text-align: center;">$\mathbf{0 . 8 8 3} \pm \mathbf{0 . 0 4 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 0 9} \pm \mathbf{0 . 0 1 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 5 9} \pm \mathbf{0 . 0 1 9}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 - direct query</td>
<td style="text-align: center;">$0.164 \pm 0.076$</td>
<td style="text-align: center;">$0.686 \pm 0.127$</td>
<td style="text-align: center;">$0.388 \pm 0.075$</td>
</tr>
<tr>
<td style="text-align: center;">MolLEO (GPT-4)</td>
<td style="text-align: center;">$\mathbf{0 . 9 2 6} \pm \mathbf{0 . 0 5 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 3 5} \pm \mathbf{0 . 0 4 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 4 3} \pm \mathbf{0 . 0 9 4}$</td>
</tr>
</tbody>
</table>
<p>(Algorithm 1). In Table A4, we show the Top-10 AUC after choosing different numbers of top-scoring candidates for editing by MoleculeSTM. We found that 30 candidates resulted in the best performance. Note that we used a different prompt for this experiment than the one used to obtain results in Table 1 (see Appendix C.6). We use 30 candidates anytime the filter is employed for all models, although this hyperparameter can be ablated independently for each model.
MoleculeSTM has several hyperparameters related to molecule generation since it involves gradient descent to optimize an input molecule embedding based on a text prompt. We look at two hyperparameters, the number of gradient descent steps (epochs) and learning rate, and plot the results in Figure A2. We find that if the learning rate is too large ( $\mathrm{lr}=1$ ), the mean fitness changes unpredictably, but if it is too small ( $\mathrm{lr}=1 \mathrm{e}-2$ ), there are minimal changes to the mean fitness. Setting the learning rate to $1 \mathrm{e}-1$ results in more consistent improvements in mean fitness. We also set the number of epochs to 30 since more epochs are too time-consuming and fewer do not result in noticeable fitness changes.</p>
<h1>C. 5 GPT-4 ablations</h1>
<p>We conduct experiments to understand the performance of MolLEO (GPT-4) in the following settings: different numbers of offspring in each generation, different underlying GPT models, incorporating retrieval augmentation methods, and different rules from Graph-GA and SMILES-GA in Table A5 and Table A6, and describe the results in following sections.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Number of top-scoring <br> candidates selected for mutation</th>
<th style="text-align: center;">Top-10 AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">$0.680 \pm 0.213$</td>
</tr>
<tr>
<td style="text-align: center;">30</td>
<td style="text-align: center;">$\mathbf{0 . 7 3 0} \pm \mathbf{0 . 1 8 8}$</td>
</tr>
<tr>
<td style="text-align: center;">50</td>
<td style="text-align: center;">$0.627 \pm 0.250$</td>
</tr>
</tbody>
</table>
<p>Table A4: Top-10 AUC on JNK3 binding task with varying numbers of topscoring candidates selected to undergo LLM-based mutations.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$.https://platform.openai.com/docs/models
${ }^{3}$ *.openai.azure.com&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1293 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1293</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1293</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-2470fcf0f89082de874ac9133ccb3a8667dd89a8</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2470fcf0f89082de874ac9133ccb3a8667dd89a8" target="_blank">Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper considers the challenging Atari games domain, and proposes a new exploration method based on assigning exploration bonuses from a concurrently learned model of the system dynamics that provides the most consistent improvement across a range of games that pose a major challenge for prior methods.</p>
                <p><strong>Paper Abstract:</strong> Achieving efficient and scalable exploration in complex domains poses a major challenge in reinforcement learning. While Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees, they are often impractical in higher dimensions due to their reliance on enumerating the state-action space. Hence, exploration in complex domains is often performed with simple epsilon-greedy methods. In this paper, we consider the challenging Atari games domain, which requires processing raw pixel inputs and delayed rewards. We evaluate several more sophisticated exploration strategies, including Thompson sampling and Boltzman exploration, and propose a new exploration method based on assigning exploration bonuses from a concurrently learned model of the system dynamics. By parameterizing our learned model with a neural network, we are able to develop a scalable and efficient approach to exploration bonuses that can be applied to tasks with complex, high-dimensional state spaces. In the Atari domain, our method provides the most consistent improvement across a range of games that pose a major challenge for prior methods. In addition to raw game-scores, we also develop an AUC-100 metric for the Atari Learning domain to evaluate the impact of exploration on this benchmark.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1293.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1293.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model-prediction exploration bonuses</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-prediction exploration bonuses via learned dynamics and representation (curiosity-driven)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An online RL exploration method that learns a compact state encoding with an autoencoder and a predictive dynamics model; novelty is measured as the model's prediction error on encoded next-states and used as a time-decaying exploration bonus added to the reward.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DQN + Model-prediction exploration bonuses (Static AE / Dynamic AE)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Policy learner: Deep Q-Network (DQN). Representation encoder σ: 8-layer autoencoder (use of sixth layer output as encoding). Dynamics model M_φ: simple two-layer neural network predicting σ(s_{t+1}) from (σ(s_t), a_t). Novelty computed as squared Euclidean prediction error e(s,a) normalized by max error and decayed by factor (t*C); augmented reward R_Bonus = R + β*(normalized_error/(t*C)). Dynamics model updated once per epoch (epoch = 50,000 steps); autoencoder optionally retrained every 5 epochs in Dynamic AE, or pre-trained offline for Static AE.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>curiosity-driven exploration via novelty bonuses from model prediction error (novelty-based active exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each transition (s_t,a_t,s_{t+1}), compute encoded states σ(s_t), σ(s_{t+1}). Predict σ(s_{t+1}) with M_φ(σ(s_t),a_t); compute error e = ||σ(s_{t+1}) - M_φ(...)||^2. Normalize by running max e and divide by (t*C) to produce a decaying novelty score; multiply by scale β and add to environment reward. The policy then updates using these augmented rewards, causing the agent to preferentially try actions/states with high prediction error (i.e., where model uncertainty/ignorance is high). The model and (optionally) encoder are retrained periodically using accumulated experience.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Arcade Learning Environment (Atari 14-game subset used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown environment dynamics, high-dimensional raw pixel observations (screen images), finite discrete action set (ALE action set), partially observable from single frames (requires learned representation/history), varying dynamics across gameplay (some games add non-stationarity across levels), sparse/delayed rewards in some games (e.g., Montezuma's Revenge).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>High-dimensional visual observation space (raw frames collected from 14 Atari games); experiments run for 100 epochs where 1 epoch = 50,000 time steps (total 5M steps). Action space: finite discrete ALE actions (moderate number of discrete actions typical for Atari). Tasks include long-horizon gameplay with delayed rewards and varying dynamics across levels.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Significant empirical improvements in learning speed and final score on many games. Examples from Table 1: Bowling max score: Static AE 130, Dynamic AE 133 vs DQN 68.1; Frostbite: Static AE 649 vs DQN 369; Seaquest: Dynamic AE 4472 vs DQN 2106. In terms of AUC-100 (learning speed metric), exploration bonuses outperform baseline DQN on 7/14 games and show AUC-100 improvements (e.g., Seaquest AUC-100: DQN 0.160 -> Dynamic AE 0.265; Frostbite: 0.573 -> Static AE 0.971). Experiments averaged over 3 trials reported after 100 epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baseline DQN with epsilon-greedy: lower sample efficiency and lower scores on many games (examples: Bowling 68.1, Frostbite 369, Seaquest 2106; corresponding AUC-100 in Table 2). Boltzmann and Thompson sampling baselines sometimes improved over epsilon-greedy but were generally less consistent than model-based bonuses.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Improved sample efficiency shown via AUC-100 over 100 epochs (1 epoch = 50k steps). Exploration-bonus agents attained higher area under learning curve with same interaction budget (example: Static AE Seaquest AUC-100 0.172 vs DQN 0.160; Dynamic AE Seaquest 0.265). Models trained dynamics and encoder periodically (model updated every epoch using 50k-step memory; autoencoder retrained every ~5 epochs in Dynamic AE).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Balancing induced by augmenting reward with a novelty bonus that decays over time (normalized error/(t*C)) and is scaled by β. As model prediction error decreases with experience, the novelty bonus reduces, leading to more exploitation of learned high-reward behaviors. The decay by time t*C prevents perpetual high bonus for the same states.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against: DQN with epsilon-greedy (baseline), Boltzmann exploration, Thompson sampling (via dropout-based Bayesian approximation), and prior published DQN results.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Assigning exploration bonuses from a concurrently learned dynamics model over learned representations yields the most consistent improvements across a diverse set of Atari games, producing faster learning (higher AUC-100) and higher final scores on multiple games (notably Bowling and Frostbite). Static AE (pretrained encoder) and Dynamic AE (online-retrained encoder) variants both improved performance, with static sometimes performing best. The normalized, time-decayed prediction error is an effective novelty signal that integrates into standard RL pipelines (DQN) and scales to high-dimensional observations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Method assumes misprediction arises from model uncertainty/ignorance; in stochastic or strongly non-stationary environments this assumption breaks down. Reported failures: Q*bert - background color change after level one caused model to assign high error almost everywhere, harming learning (exploration bonuses reduced final score). Montezuma's Revenge - extreme sparse rewards: all methods (including exploration bonuses) achieved 0.0 score. Authors note need for more nuanced treatment in stochastic systems and sensitivity to dramatic environment shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models', 'publication_date_yy_mm': '2015-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1293.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1293.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Boltzmann exploration</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Boltzmann (softmax) exploration over Q-values</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A softmax action selection strategy that assigns a probability to each action proportional to exp(Q(s,a)/τ), enabling graded exploration controlled by temperature τ.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploration Strategies for Model-based Learning in Multi-agent Systems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DQN + Boltzmann exploration</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Deep Q-Network policy with action selection via Boltzmann (softmax) distribution over estimated Q-values using a temperature parameter to control exploration vs exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>stochastic exploration using Boltzmann (softmax) action selection</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Actions sampled from a softmax over Q-values; temperature parameter (τ) determines randomness: higher τ -> more exploration. No explicit model of environment is learned for exploration bonuses; adaptation is via stochastic sampling shaped by current Q-value estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Arcade Learning Environment (Atari 14-game subset used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Same as main experiments: unknown dynamics, high-dimensional raw observations, discrete action set, varying reward sparsity and possible non-stationarity in some games.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same Atari game complexity as used for DQN experiments (high-dimensional image inputs, moderate discrete action set, long episodes).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Often improves over epsilon-greedy baseline on some games; e.g., Breakout max score 219 (Boltzmann) > DQN 146, Enduro 284 (Boltzmann) slightly > DQN 281. In some games Boltzmann tied or outperformed other baseline methods; however it was less consistently beneficial than model-based exploration bonuses across the 14 games.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Compared to epsilon-greedy DQN baseline: sometimes better (as above) but inconsistent across games; exact baseline numbers reported in Table 1 and AUC-100 Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Improves sample efficiency on certain games as shown by higher AUC-100 on those titles (see Table 2: e.g., Breakout AUC-100 Boltzmann 0.294 vs DQN 0.191), but not consistently across all games.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Controlled by temperature τ in the Boltzmann policy; no explicit time-decay or model-driven novelty signal was used in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against DQN epsilon-greedy, Thompson sampling, and model-prediction exploration bonuses.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Boltzmann exploration yields improvements over naïve epsilon-greedy in several games and is a simple, scalable alternative, but was generally less consistently effective than the model-prediction exploration bonus approach.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>No mechanism to incorporate state novelty beyond Q-values; performance depends on tuning temperature and on quality of Q-value estimates; inconsistent improvements across games.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models', 'publication_date_yy_mm': '2015-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1293.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1293.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Thompson sampling (dropout)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Thompson sampling via dropout-based Bayesian approximation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sampling-based exploration method approximating posterior sampling over Q-networks by using dropout at test time to sample different network instantiations for action selection (approximate Thompson sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dropout as a Bayesian approximation: Insights and applications</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DQN + Thompson sampling (dropout-based)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>DQN where action selection approximates posterior sampling by using dropout as a Bayesian approximation to produce multiple Q-network samples; actions selected according to sampled Q estimates, encouraging exploration in regions with posterior uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Thompson sampling / posterior sampling via dropout approximation</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Use dropout at (action-selection) time to draw a sampled network (an approximate posterior sample), select action greedily under that sample; repeated sampling yields exploration driven by epistemic uncertainty in Q estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Arcade Learning Environment (Atari 14-game subset used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Same high-dimensional, unknown Atari environments with discrete actions, delayed/sparse rewards, partial observability from raw pixel inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same Atari complexity; experiments over 100 epochs (50k steps/epoch) with average scores reported.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Mixed results: Thompson sampling improved over epsilon-greedy on some games (e.g., Pong AUC-100 0.612 vs DQN 0.520) but was not consistently superior to model-based exploration bonuses; Table 1 shows Thompson achieved best raw score on Breakout (222) among tested baselines, but overall fewer wins than model-based bonuses.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baseline DQN typically had lower scores on games where Thompson sampling helped; full baseline numbers available in Table 1 and AUC-100 in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Moderate improvements in sample efficiency on some games as measured by AUC-100; inconsistent across titles.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Exploration emerges from randomized posterior samples of Q-values (via dropout), leading to temporally-extended exploration behavior where actions reflect sampled beliefs rather than fixed ε-randomness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against DQN epsilon-greedy, Boltzmann exploration, and the model-prediction exploration bonus method.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Thompson-sampling-style exploration via dropout can improve performance in some Atari games and is a principled alternative to ε-greedy, but in these experiments it was less consistently effective than the model-based exploration bonuses.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performance depends on quality of posterior approximation via dropout; does not directly leverage model prediction error or representation learning; inconsistent across games.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models', 'publication_date_yy_mm': '2015-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1293.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1293.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Epsilon-greedy DQN baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Q-Network with epsilon-greedy exploration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard DQN agent that selects a random action with probability ε and otherwise chooses the action with highest Q-value; used as the principal baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Human-level Control Through Deep Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DQN (epsilon-greedy)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Deep Q-Network policy learning from raw Atari frames using experience replay and target networks (as per DQN); exploration via standard epsilon-greedy scheduling.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>epsilon-greedy (non-adaptive/randomized exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Random action with probability ε; otherwise exploit highest Q. ε usually annealed over time (standard DQN practice). Does not use state novelty or model uncertainty for exploration decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Arcade Learning Environment (Atari 14-game subset used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>High-dimensional pixel inputs, unknown dynamics, discrete action set, delayed/sparse rewards in some games, potentially non-stationary dynamics in some titles.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same as above: 14 Atari games, evaluated over 100 epochs (50k steps/epoch).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Reported baseline raw scores in Table 1 (examples: Alien 1018, Bowling 68.1, Frostbite 369, Seaquest 2106) and AUC-100 values in Table 2 (e.g., Seaquest 0.160, Frostbite 0.573). These are lower or slower-learning on many games compared to model-based exploration bonuses.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Lower sample efficiency than model-based exploration bonuses on many games; required full 100-epoch runs to achieve reported scores.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Controlled by ε schedule; pure random exploration with no directed novelty-seeking.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Used as the primary baseline against Boltzmann, Thompson sampling, and model-prediction exploration bonuses.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Epsilon-greedy DQN often converges more slowly and to lower scores on exploration-challenging games compared to model-based bonuses and in some cases to Boltzmann/Thompson variants.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Inefficient exploration in large/high-dimensional observation spaces; fails to find rewarding behaviors in very sparse-reward games (e.g., Montezuma's Revenge).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models', 'publication_date_yy_mm': '2015-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Near-Bayesian Exploration in Polynomial Time <em>(Rating: 2)</em></li>
                <li>Human-level Control Through Deep Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Dropout as a Bayesian approximation: Insights and applications <em>(Rating: 2)</em></li>
                <li>Developmental Robotics, Optimal Artificial Curiosity, Creativity, Music, and the Fine Arts <em>(Rating: 2)</em></li>
                <li>Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1293",
    "paper_id": "paper-2470fcf0f89082de874ac9133ccb3a8667dd89a8",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "Model-prediction exploration bonuses",
            "name_full": "Model-prediction exploration bonuses via learned dynamics and representation (curiosity-driven)",
            "brief_description": "An online RL exploration method that learns a compact state encoding with an autoencoder and a predictive dynamics model; novelty is measured as the model's prediction error on encoded next-states and used as a time-decaying exploration bonus added to the reward.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "DQN + Model-prediction exploration bonuses (Static AE / Dynamic AE)",
            "agent_description": "Policy learner: Deep Q-Network (DQN). Representation encoder σ: 8-layer autoencoder (use of sixth layer output as encoding). Dynamics model M_φ: simple two-layer neural network predicting σ(s_{t+1}) from (σ(s_t), a_t). Novelty computed as squared Euclidean prediction error e(s,a) normalized by max error and decayed by factor (t*C); augmented reward R_Bonus = R + β*(normalized_error/(t*C)). Dynamics model updated once per epoch (epoch = 50,000 steps); autoencoder optionally retrained every 5 epochs in Dynamic AE, or pre-trained offline for Static AE.",
            "adaptive_design_method": "curiosity-driven exploration via novelty bonuses from model prediction error (novelty-based active exploration)",
            "adaptation_strategy_description": "At each transition (s_t,a_t,s_{t+1}), compute encoded states σ(s_t), σ(s_{t+1}). Predict σ(s_{t+1}) with M_φ(σ(s_t),a_t); compute error e = ||σ(s_{t+1}) - M_φ(...)||^2. Normalize by running max e and divide by (t*C) to produce a decaying novelty score; multiply by scale β and add to environment reward. The policy then updates using these augmented rewards, causing the agent to preferentially try actions/states with high prediction error (i.e., where model uncertainty/ignorance is high). The model and (optionally) encoder are retrained periodically using accumulated experience.",
            "environment_name": "Arcade Learning Environment (Atari 14-game subset used in experiments)",
            "environment_characteristics": "Unknown environment dynamics, high-dimensional raw pixel observations (screen images), finite discrete action set (ALE action set), partially observable from single frames (requires learned representation/history), varying dynamics across gameplay (some games add non-stationarity across levels), sparse/delayed rewards in some games (e.g., Montezuma's Revenge).",
            "environment_complexity": "High-dimensional visual observation space (raw frames collected from 14 Atari games); experiments run for 100 epochs where 1 epoch = 50,000 time steps (total 5M steps). Action space: finite discrete ALE actions (moderate number of discrete actions typical for Atari). Tasks include long-horizon gameplay with delayed rewards and varying dynamics across levels.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Significant empirical improvements in learning speed and final score on many games. Examples from Table 1: Bowling max score: Static AE 130, Dynamic AE 133 vs DQN 68.1; Frostbite: Static AE 649 vs DQN 369; Seaquest: Dynamic AE 4472 vs DQN 2106. In terms of AUC-100 (learning speed metric), exploration bonuses outperform baseline DQN on 7/14 games and show AUC-100 improvements (e.g., Seaquest AUC-100: DQN 0.160 -&gt; Dynamic AE 0.265; Frostbite: 0.573 -&gt; Static AE 0.971). Experiments averaged over 3 trials reported after 100 epochs.",
            "performance_without_adaptation": "Baseline DQN with epsilon-greedy: lower sample efficiency and lower scores on many games (examples: Bowling 68.1, Frostbite 369, Seaquest 2106; corresponding AUC-100 in Table 2). Boltzmann and Thompson sampling baselines sometimes improved over epsilon-greedy but were generally less consistent than model-based bonuses.",
            "sample_efficiency": "Improved sample efficiency shown via AUC-100 over 100 epochs (1 epoch = 50k steps). Exploration-bonus agents attained higher area under learning curve with same interaction budget (example: Static AE Seaquest AUC-100 0.172 vs DQN 0.160; Dynamic AE Seaquest 0.265). Models trained dynamics and encoder periodically (model updated every epoch using 50k-step memory; autoencoder retrained every ~5 epochs in Dynamic AE).",
            "exploration_exploitation_tradeoff": "Balancing induced by augmenting reward with a novelty bonus that decays over time (normalized error/(t*C)) and is scaled by β. As model prediction error decreases with experience, the novelty bonus reduces, leading to more exploitation of learned high-reward behaviors. The decay by time t*C prevents perpetual high bonus for the same states.",
            "comparison_methods": "Compared against: DQN with epsilon-greedy (baseline), Boltzmann exploration, Thompson sampling (via dropout-based Bayesian approximation), and prior published DQN results.",
            "key_results": "Assigning exploration bonuses from a concurrently learned dynamics model over learned representations yields the most consistent improvements across a diverse set of Atari games, producing faster learning (higher AUC-100) and higher final scores on multiple games (notably Bowling and Frostbite). Static AE (pretrained encoder) and Dynamic AE (online-retrained encoder) variants both improved performance, with static sometimes performing best. The normalized, time-decayed prediction error is an effective novelty signal that integrates into standard RL pipelines (DQN) and scales to high-dimensional observations.",
            "limitations_or_failures": "Method assumes misprediction arises from model uncertainty/ignorance; in stochastic or strongly non-stationary environments this assumption breaks down. Reported failures: Q*bert - background color change after level one caused model to assign high error almost everywhere, harming learning (exploration bonuses reduced final score). Montezuma's Revenge - extreme sparse rewards: all methods (including exploration bonuses) achieved 0.0 score. Authors note need for more nuanced treatment in stochastic systems and sensitivity to dramatic environment shifts.",
            "uuid": "e1293.0",
            "source_info": {
                "paper_title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models",
                "publication_date_yy_mm": "2015-07"
            }
        },
        {
            "name_short": "Boltzmann exploration",
            "name_full": "Boltzmann (softmax) exploration over Q-values",
            "brief_description": "A softmax action selection strategy that assigns a probability to each action proportional to exp(Q(s,a)/τ), enabling graded exploration controlled by temperature τ.",
            "citation_title": "Exploration Strategies for Model-based Learning in Multi-agent Systems",
            "mention_or_use": "use",
            "agent_name": "DQN + Boltzmann exploration",
            "agent_description": "Deep Q-Network policy with action selection via Boltzmann (softmax) distribution over estimated Q-values using a temperature parameter to control exploration vs exploitation.",
            "adaptive_design_method": "stochastic exploration using Boltzmann (softmax) action selection",
            "adaptation_strategy_description": "Actions sampled from a softmax over Q-values; temperature parameter (τ) determines randomness: higher τ -&gt; more exploration. No explicit model of environment is learned for exploration bonuses; adaptation is via stochastic sampling shaped by current Q-value estimates.",
            "environment_name": "Arcade Learning Environment (Atari 14-game subset used in experiments)",
            "environment_characteristics": "Same as main experiments: unknown dynamics, high-dimensional raw observations, discrete action set, varying reward sparsity and possible non-stationarity in some games.",
            "environment_complexity": "Same Atari game complexity as used for DQN experiments (high-dimensional image inputs, moderate discrete action set, long episodes).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Often improves over epsilon-greedy baseline on some games; e.g., Breakout max score 219 (Boltzmann) &gt; DQN 146, Enduro 284 (Boltzmann) slightly &gt; DQN 281. In some games Boltzmann tied or outperformed other baseline methods; however it was less consistently beneficial than model-based exploration bonuses across the 14 games.",
            "performance_without_adaptation": "Compared to epsilon-greedy DQN baseline: sometimes better (as above) but inconsistent across games; exact baseline numbers reported in Table 1 and AUC-100 Table 2.",
            "sample_efficiency": "Improves sample efficiency on certain games as shown by higher AUC-100 on those titles (see Table 2: e.g., Breakout AUC-100 Boltzmann 0.294 vs DQN 0.191), but not consistently across all games.",
            "exploration_exploitation_tradeoff": "Controlled by temperature τ in the Boltzmann policy; no explicit time-decay or model-driven novelty signal was used in these experiments.",
            "comparison_methods": "Compared against DQN epsilon-greedy, Thompson sampling, and model-prediction exploration bonuses.",
            "key_results": "Boltzmann exploration yields improvements over naïve epsilon-greedy in several games and is a simple, scalable alternative, but was generally less consistently effective than the model-prediction exploration bonus approach.",
            "limitations_or_failures": "No mechanism to incorporate state novelty beyond Q-values; performance depends on tuning temperature and on quality of Q-value estimates; inconsistent improvements across games.",
            "uuid": "e1293.1",
            "source_info": {
                "paper_title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models",
                "publication_date_yy_mm": "2015-07"
            }
        },
        {
            "name_short": "Thompson sampling (dropout)",
            "name_full": "Thompson sampling via dropout-based Bayesian approximation",
            "brief_description": "A sampling-based exploration method approximating posterior sampling over Q-networks by using dropout at test time to sample different network instantiations for action selection (approximate Thompson sampling).",
            "citation_title": "Dropout as a Bayesian approximation: Insights and applications",
            "mention_or_use": "use",
            "agent_name": "DQN + Thompson sampling (dropout-based)",
            "agent_description": "DQN where action selection approximates posterior sampling by using dropout as a Bayesian approximation to produce multiple Q-network samples; actions selected according to sampled Q estimates, encouraging exploration in regions with posterior uncertainty.",
            "adaptive_design_method": "Thompson sampling / posterior sampling via dropout approximation",
            "adaptation_strategy_description": "Use dropout at (action-selection) time to draw a sampled network (an approximate posterior sample), select action greedily under that sample; repeated sampling yields exploration driven by epistemic uncertainty in Q estimates.",
            "environment_name": "Arcade Learning Environment (Atari 14-game subset used in experiments)",
            "environment_characteristics": "Same high-dimensional, unknown Atari environments with discrete actions, delayed/sparse rewards, partial observability from raw pixel inputs.",
            "environment_complexity": "Same Atari complexity; experiments over 100 epochs (50k steps/epoch) with average scores reported.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Mixed results: Thompson sampling improved over epsilon-greedy on some games (e.g., Pong AUC-100 0.612 vs DQN 0.520) but was not consistently superior to model-based exploration bonuses; Table 1 shows Thompson achieved best raw score on Breakout (222) among tested baselines, but overall fewer wins than model-based bonuses.",
            "performance_without_adaptation": "Baseline DQN typically had lower scores on games where Thompson sampling helped; full baseline numbers available in Table 1 and AUC-100 in Table 2.",
            "sample_efficiency": "Moderate improvements in sample efficiency on some games as measured by AUC-100; inconsistent across titles.",
            "exploration_exploitation_tradeoff": "Exploration emerges from randomized posterior samples of Q-values (via dropout), leading to temporally-extended exploration behavior where actions reflect sampled beliefs rather than fixed ε-randomness.",
            "comparison_methods": "Compared against DQN epsilon-greedy, Boltzmann exploration, and the model-prediction exploration bonus method.",
            "key_results": "Thompson-sampling-style exploration via dropout can improve performance in some Atari games and is a principled alternative to ε-greedy, but in these experiments it was less consistently effective than the model-based exploration bonuses.",
            "limitations_or_failures": "Performance depends on quality of posterior approximation via dropout; does not directly leverage model prediction error or representation learning; inconsistent across games.",
            "uuid": "e1293.2",
            "source_info": {
                "paper_title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models",
                "publication_date_yy_mm": "2015-07"
            }
        },
        {
            "name_short": "Epsilon-greedy DQN baseline",
            "name_full": "Deep Q-Network with epsilon-greedy exploration",
            "brief_description": "Standard DQN agent that selects a random action with probability ε and otherwise chooses the action with highest Q-value; used as the principal baseline in experiments.",
            "citation_title": "Human-level Control Through Deep Reinforcement Learning",
            "mention_or_use": "use",
            "agent_name": "DQN (epsilon-greedy)",
            "agent_description": "Deep Q-Network policy learning from raw Atari frames using experience replay and target networks (as per DQN); exploration via standard epsilon-greedy scheduling.",
            "adaptive_design_method": "epsilon-greedy (non-adaptive/randomized exploration)",
            "adaptation_strategy_description": "Random action with probability ε; otherwise exploit highest Q. ε usually annealed over time (standard DQN practice). Does not use state novelty or model uncertainty for exploration decisions.",
            "environment_name": "Arcade Learning Environment (Atari 14-game subset used in experiments)",
            "environment_characteristics": "High-dimensional pixel inputs, unknown dynamics, discrete action set, delayed/sparse rewards in some games, potentially non-stationary dynamics in some titles.",
            "environment_complexity": "Same as above: 14 Atari games, evaluated over 100 epochs (50k steps/epoch).",
            "uses_adaptive_design": false,
            "performance_with_adaptation": null,
            "performance_without_adaptation": "Reported baseline raw scores in Table 1 (examples: Alien 1018, Bowling 68.1, Frostbite 369, Seaquest 2106) and AUC-100 values in Table 2 (e.g., Seaquest 0.160, Frostbite 0.573). These are lower or slower-learning on many games compared to model-based exploration bonuses.",
            "sample_efficiency": "Lower sample efficiency than model-based exploration bonuses on many games; required full 100-epoch runs to achieve reported scores.",
            "exploration_exploitation_tradeoff": "Controlled by ε schedule; pure random exploration with no directed novelty-seeking.",
            "comparison_methods": "Used as the primary baseline against Boltzmann, Thompson sampling, and model-prediction exploration bonuses.",
            "key_results": "Epsilon-greedy DQN often converges more slowly and to lower scores on exploration-challenging games compared to model-based bonuses and in some cases to Boltzmann/Thompson variants.",
            "limitations_or_failures": "Inefficient exploration in large/high-dimensional observation spaces; fails to find rewarding behaviors in very sparse-reward games (e.g., Montezuma's Revenge).",
            "uuid": "e1293.3",
            "source_info": {
                "paper_title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models",
                "publication_date_yy_mm": "2015-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Near-Bayesian Exploration in Polynomial Time",
            "rating": 2
        },
        {
            "paper_title": "Human-level Control Through Deep Reinforcement Learning",
            "rating": 2
        },
        {
            "paper_title": "Dropout as a Bayesian approximation: Insights and applications",
            "rating": 2
        },
        {
            "paper_title": "Developmental Robotics, Optimal Artificial Curiosity, Creativity, Music, and the Fine Arts",
            "rating": 2
        },
        {
            "paper_title": "Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning",
            "rating": 1
        }
    ],
    "cost": 0.01362525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models</h1>
<p>Bradly C. Stadie<br>Department of Statistics<br>University of California, Berkeley<br>Berkeley, CA 94720<br>bstadie@berkeley.edu</p>
<p>Sergey Levine Pieter Abbeel<br>EECS Department<br>University of California, Berkeley<br>Berkeley, CA 94720<br>{svlevine, pabbeel}@cs.berkeley.edu</p>
<h4>Abstract</h4>
<p>Achieving efficient and scalable exploration in complex domains poses a major challenge in reinforcement learning. While Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees, they are often impractical in higher dimensions due to their reliance on enumerating the state-action space. Hence, exploration in complex domains is often performed with simple epsilon-greedy methods. In this paper, we consider the challenging Atari games domain, which requires processing raw pixel inputs and delayed rewards. We evaluate several more sophisticated exploration strategies, including Thompson sampling and Boltzman exploration, and propose a new exploration method based on assigning exploration bonuses from a concurrently learned model of the system dynamics. By parameterizing our learned model with a neural network, we are able to develop a scalable and efficient approach to exploration bonuses that can be applied to tasks with complex, high-dimensional state spaces. In the Atari domain, our method provides the most consistent improvement across a range of games that pose a major challenge for prior methods. In addition to raw gamescores, we also develop an AUC-100 metric for the Atari Learning domain to evaluate the impact of exploration on this benchmark.</p>
<h2>1 INTRODUCTION</h2>
<p>In reinforcement learning (RL), agents acting in unknown environments face the exploration versus exploitation tradeoff. Without adequate exploration, the agent might fail to discover effective control strategies, particularly in complex domains. Both PAC-MDP algorithms, such as MBIE-EB [1], and Bayesian algorithms such as Bayesian Exploration Bonuses (BEB) [2] have managed this tradeoff by assigning exploration bonuses to novel states. In these methods, the novelty of a state-action pair is derived from the number of times an agent has visited that pair. While these approaches offer strong formal guarantees, their requirement of an enumerable representation of the agent's environment renders them impractical for large-scale tasks. As such, exploration in large RL tasks is still most often performed using simple heuristics, such as the epsilon-greedy strategy [3], which can be inadequate in more complex settings.</p>
<p>In this paper, we evaluate several exploration strategies that can be scaled up to complex tasks with high-dimensional inputs. Our results show that Boltzman exploration and Thompson sampling significantly improve on the naïve epsilon-greedy strategy. However, we show that the biggest and most consistent improvement can be achieved by assigning exploration bonuses based on a learned model of the system dynamics with learned representations. To that end, we describe a method that learns a state representation from observations, trains a dynamics model using this representation concurrently with the policy, and uses the misprediction error in this model to asses the novelty of each state. Novel states are expected to disagree more strongly with the model than those states that have been visited frequently in the past, and assigning exploration bonuses based on this disagreement can produce rapid and effective exploration.</p>
<p>Using learned model dynamics to assess a state's novelty presents several challenges. Capturing an adequate representation of the agent's environment for use in dynamics predictions can be accom-</p>
<p>plished by training a model to predict the next state from the previous ground-truth state-action pair. However, one would not expect pixel intensity values to adequately capture the salient features of a given state-space. To provide a more suitable representation of the system's state space, we propose a method for encoding the state space into lower dimensional domains. To achieve sufficient generality and scalability, we modeled the system's dynamics with a deep neural network. This allows for on-the-fly learning of a model representation that can easily be trained in parallel to learning a policy.</p>
<p>Our main contribution is a scalable and efficient method for assigning exploration bonuses in large RL problems with complex observations, as well as an extensive empirical evaluation of this approach and other simple alternative strategies, such as Boltzman exploration and Thompson sampling. Our approach assigns model-based exploration bonuses from learned representations and dynamics, using only the observations and actions. It can scale to large problems where Bayesian approaches to exploration become impractical, and we show that it achieves significant improvement in learning speed on the task of learning to play Atari games from raw images [24]. Our approach achieves state-of-the-art results on a number of games, and achieves particularly large improvements for games on which human players strongly outperform prior methods. Aside from achieving a high final score, our method also achieves substantially faster learning. To evaluate the speed of the learning process, we propose the AUC-100 benchmark to evaluate learning progress on the Atari domain.</p>
<h1>2 Preliminaries</h1>
<p>We consider an infinite-horizon discounted Markov decision process (MDP), defined by the tuple $(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \rho_{0}, \gamma)$, where $\mathcal{S}$ is a finite set of states, $\mathcal{A}$ a finite set of actions, $\mathcal{P}: \mathcal{S} \times A \times \mathcal{S} \rightarrow \mathbb{R}$ the transition probability distribution, $\mathcal{R}: \mathcal{S} \rightarrow \mathbb{R}$ the reward function, $\rho_{0}$ an initial state distribution, and $\gamma \in(0,1)$ the discount factor. We are interested in finding a policy $\pi: \mathcal{S} \times \mathcal{A} \rightarrow[0,1]$ that maximizes the expected reward over all time. This maximization can be accomplished using a variety of reinforcement learning algorithms.</p>
<p>In this work, we are concerned with online reinforcement learning wherein the algorithm receives a tuple $\left(s_{t}, a_{t}, s_{t+1}, r_{t}\right)$ at each step. Here, $s_{t} \in \mathcal{S}$ is the previous state, $a_{t} \in \mathcal{A}$ is the previous action, $s_{t+1} \in \mathcal{S}$ is the new state, and $r_{t}$ is the reward collected as a result of this transition. The reinforcement learning algorithm must use this tuple to update its policy and maximize longterm reward and then choose the new action $a_{t+1}$. It is often insufficient to simply choose the best action based on previous experience, since this strategy can quickly fall into a local optimum. Instead, the learning algorithm must perform exploration. Prior work has suggested methods that address the exploration problem by acting with "optimism under uncertainty." If one assumes that the reinforcement learning algorithm will tend to choose the best action, it can be encouraged to visit state-action pairs that it has not frequently seen by augmenting the reward function to deliver a bonus for visiting novel states. This is accomplished with the augmented reward function</p>
<p>$$
\mathcal{R}_{\text {Bonus }}(s, a)=\mathcal{R}(s, a)+\beta \mathcal{N}(s, a)
$$</p>
<p>where $\mathcal{N}(s, a): \mathcal{S} \times \mathcal{A} \rightarrow[0,1]$ is a novelty function designed to capture the novelty of a given state-action pair. Prior work has suggested a variety of different novelty functions e.g., [1, 2] based on state visitation frequency.</p>
<p>While such methods offer a number of appealing guarantees, such as near-Bayesian exploration in polynomial time [2], they require a concise, often discrete representation of the agent's stateaction space to measure state visitation frequencies. In our approach, we will employ function approximation and representation learning to devise an alternative to these requirements.</p>
<h2>3 Model Learning For Exploration Bonuses</h2>
<p>We would like to encourage agent exploration by giving the agent exploration bonuses for visiting novel states. Identifying states as novel requires we supply some representation of the agent's state space, as well as a mechanism to use this representation to assess novelty. Unsupervised learning methods offer one promising avenue for acquiring a concise representation of the state with a good</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Reinforcement learning with model prediction exploration bonuses
    Initialize \(\max <span class="ge">_{e}=1\), EpochLength, \(\beta, C\)</span>
<span class="ge">    for iteration \(t\) in \(T\) do</span>
<span class="ge">        Observe \(\left(s_</span>{t}, a_{t}, s_{t+1}, \mathcal{R}\left(s_{t}, a_{t}\right)\right)\)
        Encode the observations to obtain \(\sigma\left(s_{t}\right)\) and \(\sigma\left(s_{t+1}\right)\)
        Compute \(e\left(s_{t}, a_{t}\right)=\left\|\sigma\left(s_{t+1}\right)-\mathcal{M}_{\phi}\left(\sigma\left(s_{t}\right), a_{t}\right)\right\|_{2}^{2}\) and \(\bar{e}\left(s_{t}, a_{t}\right)=\frac{e\left(s_{t}, a_{t}\right)}{\max <span class="ge">_{e}}\).</span>
<span class="ge">        Compute \(\mathcal{R}_</span>{\text {Bonus }}\left(s_{t}, a_{t}\right)=\mathcal{R}(s, a)+\beta\left(\frac{\bar{e}_{t}\left(s_{t}, a_{t}\right)}{t * C}\right)\)
        if \(e\left(s_{t}, a_{t}\right)&gt;\max <span class="ge">_{e}\) then</span>
<span class="ge">            \(\max _</span>{e}=e\left(s_{t}, a_{t}\right)\)
        end if
        Store \(\left(s_{t}, a_{t}, \mathcal{R}_{\text {bonus }}\right)\) in a memory bank \(\Omega\).
        Pass \(\Omega\) to the reinforcement learning algorithm to update \(\pi\).
        if \(t \bmod\) EpochLength \(==0\) then
            Use \(\Omega\) to update \(\mathcal{M}\).
            Optionally, update \(\sigma\).
        end if
    end for
    return optimized policy \(\pi\)
</code></pre></div>

<p>similarity metric. This can be accomplished using dimensionality reduction, clustering, or graphbased techniques [4, 5]. In our work, we draw on recent developments in representation learning with neural networks, as discussed in the following section. However, even with a good learned state representation, maintaining a table of visitation frequencies becomes impractical for complex tasks. Instead, we learn a model of the task dynamics that can be used to assess the novelty of a new state.</p>
<p>Formally, let $\sigma(s)$ denote the encoding of the state $s$, and let $\mathcal{M}<em _phi="\phi">{\phi}: \sigma(\mathcal{S}) \times \mathcal{A} \rightarrow \sigma(\mathcal{S})$ be a dynamics predictor parameterized by $\phi . \mathcal{M}</em>$ is discussed further in the next section.}$ takes an encoded version of a state $s$ at time $t$ and the agent's action at time $t$ and attempts to predict an encoded version of the agent's state at time $t+1$. The parameterization of $\mathcal{M</p>
<p>For each state transition $\left(s_{t}, a_{t}, s_{t+1}\right)$, we can attempt to predict $\sigma\left(s_{t+1}\right)$ from $\left(\sigma\left(s_{t}\right), a_{t}\right)$ using our predictive model $\mathcal{M}_{\phi}$. This prediction will have some error</p>
<p>$$
e\left(s_{t}, a_{t}\right)=\left|\sigma\left(s_{t+1}\right)-\mathcal{M}<em t="t">{\phi}\left(\sigma\left(s</em>
$$}\right), a_{t}\right)\right|_{2}^{2</p>
<p>Let $\overline{e_{T}}$, the normalized prediction error at time $T$, be given by $\overline{e_{T}}:=\frac{e_{T}}{\max <em t="t">{t \leq T}\left{e</em>\right)$ via}\right}}$. We can assign a novelty function to $\left(s_{t}, a_{t</p>
<p>$$
\mathcal{N}\left(s_{t}, a_{t}\right)=\frac{\bar{e}<em t="t">{t}\left(s</em>
$$}, a_{t}\right)}{t * C</p>
<p>where $C&gt;0$ is a decay constant. We can now realize our augmented reward function as</p>
<p>$$
\mathcal{R}<em t="t">{\text {Bonus }}(s, a)=\mathcal{R}(s, a)+\beta\left(\frac{\bar{e}</em>\right)
$$}\left(s_{t}, a_{t}\right)}{t * C</p>
<p>This approach is motivated by the idea that, as our ability to model the dynamics of a particular state-action pair improves, we have come to understand the state better and hence its novelty is lower. When we don't understand the state-action pair well enough to make accurate predictions, we assume that more knowledge about that particular area of the model dynamics is needed and hence a higher novelty measure is assigned.
Using learned model dynamics to assign novelty functions allows us to address the exploration versus exploitation problem in a non-greedy way. With an appropriate representation $\sigma\left(s_{t}\right)$, even when we encounter a new state-action pair $\left(s_{t}, a_{t}\right)$, we expect $\mathcal{M}<em t="t">{\phi}\left(\sigma\left(s</em>\right)$ to be accurate so long as enough similar state-action pairs have been encountered.
Our model-based exploration bonuses can be incorporated into any online reinforcement learning algorithm that updates the policy based on state, action, reward tuples of the form $\left(s_{t}, a_{t}, s_{t+1}, r_{t}\right)$,}\right), a_{t</p>
<p>such as Q-learning or actor-critic algorithms. Our method is summarized in Algorithm 1. At each step, we receive a tuple $\left(s_{t}, a_{t}, s_{t+1}, \mathcal{R}\left(s_{t}, a_{t}\right)\right)$ and compute the Euclidean distance between the encoded state $\sigma\left(s_{t+1}\right)$ to the prediction made by our model $\mathcal{M}<em t="t">{\phi}\left(\sigma\left(s</em>}\right), a_{t}\right)$. This is used to compute the exploration-augmented reward $\mathcal{R<em t="t">{\text {Bonus }}$ using Equation (4). The tuples $\left(s</em>}, a_{t}, s_{t+1}, \mathcal{R<em _phi="\phi">{\text {Bonus }}\right)$ are stored in a memory bank $\Omega$ at the end of every step. Every step, the policy is updated. ${ }^{1}$ Once per epoch, corresponding to 50000 observations in our implementation, the dynamics model $\mathcal{M}</em>$ is updated to improve its accuracy. If desired, the representation encoder $\sigma$ can also be updated at this time. We found that retraining $\sigma$ once every 5 epochs to be sufficient.</p>
<p>This approach is modular and compatible with any representation of $\sigma$ and $\mathcal{M}$, as well as any reinforcement learning method that updates its policy based on a continuous stream of observation, action, reward tuples. Incorporating exploration bonuses does make the reinforcement learning task nonstationary, though we did not find this to be a major issue in practice, as shown in our experimental evaluation. In the following section, we discuss the particular choice for $\sigma$ and $\mathcal{M}$ that we use for learning policies for playing Atari games from raw images.</p>
<h1>4 DEEP LEARNING ARCHITECTURES</h1>
<p>Though the dynamics model $\mathcal{M}_{\phi}$ and the encoder $\sigma$ from the previous section can be parametrized by any appropriate method, we found that using deep neural networks for both achieved good empirical results on the Atari games benchmark. In this section, we discuss the particular networks used in our implementation.</p>
<h3>4.1 AUTOENCODERS</h3>
<p>The most direct way of learning a dynamics model is to directly predict the state at the next time step, which in the Atari games benchmark corresponds to the next frame's pixel intensity values. However, directly predicting these pixel intensity values is unsatisfactory, since we do not expect pixel intensity to capture the salient features of the environment in a robust way. In our experiments, a dynamics model trained to predict raw frames exhibited extremely poor behavior, assigning exploration bonuses in near equality at most time steps, as discussed in our experimental results section.</p>
<p>To overcome these difficulties, we seek a function $\sigma$ which encodes a lower dimensional representation of the state $s$. For the task of representing Atari frames, we found that an autoencoder could be used to successfully obtain an encoding function $\sigma$ and achieve dimensionality reduction and feature extraction [6]. Our autoencoder has 8 hidden layers, followed by a Euclidean loss layer,
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Left: Autoencoder used on input space. The circle denotes the hidden layer that was extracted and utilized as input for dynamics learning. Right: Model learning architecture.
which computes the distance between the output features and the original input image. The hidden layers are reduced in dimension until maximal compression occurs with 128 units. After this, the activations are decoded by passing through hidden layers with increasingly large size. We train the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>network on a set of 250,000 images and test on a further set of 25,000 images. We compared two separate methodologies for capturing these images.</p>
<ol>
<li>Static AE: A random agent plays for enough time to collect the required images. The auto-encoder $\sigma$ is trained offline before the policy learning algorithm begins.</li>
<li>Dynamic AE: Initialize with an epsilon-greedy strategy and collect images and actions while the agent acts under the policy learning algorithm. After 5 epochs, train the auto encoder from this data. Continue to collect data and periodically retrain the auto encoder in parallel with the policy training algorithm.</li>
</ol>
<p>We found that the reconstructed input achieves a small but non-trivial residual on the test set regardless of which auto encoder training technique is utilized, suggesting that in both cases it learns underlying features of the state space while avoiding overfitting.</p>
<p>To obtain a lower dimensional representation of the agent's state space, a snapshot of the network's first six layers is saved. The sixth layer's output (circled in figure one) is then utilized as an encoding for the original state space. That is, we construct an encoding $\sigma\left(s_{t}\right)$ by running $s_{t}$ through the first six hidden layers of our autoencoder and then taking the sixth layers output to be $\sigma\left(s_{t}\right)$. In practice, we found that using the sixth layer's output (rather than the bottleneck at the fifth layer) obtained the best model learning results. See the appendix for further discussion on this result.</p>
<h1>4.2 Model Learning Architecture</h1>
<p>Equipped with an encoding $\sigma$, we can now consider the task of predicting model dynamics. For this task, a much simpler two layer neural network $\mathcal{M}<em _phi="\phi">{\phi}$ suffices. $\mathcal{M}</em>\right)$. We find that this model initially learns a representation close to the identity function and consequently the loss residual is similar for most state-action pairs. However, after approximately 5 epochs, it begins to learn more complex dynamics and consequently better identify novel states. We evaluate the quality of the learned model in the appendix.}$ takes as input the encoded version of a state $s_{t}$ at time $t$ along with the agent's action $a_{t}$ and seeks to predict the encoded next frame $\sigma\left(s_{t+1}\right)$. Loss is computed via a Euclidean loss layer regressing on the ground truth $\sigma\left(s_{t+1</p>
<h2>5 Related Work</h2>
<p>Exploration is an intensely studied area of reinforcement learning. Many of the pioneering algorithms in this area, such as $R-M a x$ [7] and $E^{3}$ [8], achieve efficient exploration that scales polynomially with the number of parameters in the agent's state space (see also [9, 10]). However, as the size of state spaces increases, these methods quickly become intractable. A number of prior methods also examine various techniques for using models and prediction to incentivize exploration [11, 12, 13, 14]. However, such methods typically operate directly on the transition matrix of a discrete MDP, and do not provide for a straightforward extension to very large or continuous spaces, where function approximation is required. A number of prior methods have also been proposed to incorporate domain-specific factors to improve exploration. Doshi-Velez et al. [15] proposed incorporating priors into policy optimization, while Lang et al. [16] developed a method specific to relational domains. Finally, Schmidhuber et al. have developed a curiosity driven approach to exploration which uses model predictors to aid in control [17].
Several exploration techniques have been proposed that can extend more readily to large state spaces. Among these, methods such as C-PACE [18] and metric- $E^{3}$ [19] require a good metric on the state space that satisfies the assumptions of the algorithm. The corresponding representation learning issue has some parallels to the representation problem that we address by using an autoecoder, but it is unclear how the appropriate metric for the prior methods can be acquired automatically on tasks with raw sensory input, such as the Atari games in our experimental evaluation. Methods based on Monte-Carlo tree search can also scale gracefully to complex domains [20], and indeed previous work has applied such techniques to the task of playing Atari games from screen images [21]. However, this approach is computationally very intensive, and requires access to a generative model of the system in order to perform the tree search, which is not always available in online reinforcement learning. On the other hand, our method readily integrates into any online reinforcement learning algorithm.</p>
<p>Finally, several recent papers have focused on driving the Q value higher. In [22], the authors use network dropout to perform Thompson sampling. In Boltzman exploration, a positive probability is assigned to any possible action according to its expected utility and according to a temperature parameter [23]. Both of these methods focus on controlling Q values rather than model-based exploration. A comparison to both is provided in the next section.</p>
<h1>6 EXPERIMENTAL RESULTS</h1>
<p>We evaluate our approach on 14 games from the Arcade Learning Environment [24]. The task consists of choosing actions in an Atari emulator based on raw images of the screen. Previous work has tackled this task using Q-learning with epsilon-greedy exploration [3], as well as Monte Carlo tree search [21] and policy gradient methods [25]. We use Deep Q Networks (DQN) [3] as the reinforcement learning algorithm within our method, and compare its performance to the same DQN method using only epsilon-greedy exploration, Boltzman exploration, and a Thompson sampling approach.
The results for 14 games in the Arcade Learning Environment are presented in Table 1. We chose those games that were particularly challenging for prior methods and ones where human experts outperform prior learning methods. We evaluated two versions of our approach; using either an autoencoder trained in advance by running epsilon-greedy Q-learning to collect data (denoted as "Static AE"), or using an autoencoder trained concurrently with the model and policy on the same image data (denoted as "Dynamic AE"). Table 1 also shows results from the DQN implementation reported in previous work, along with human expert performance on each game [3]. Note that our DQN implementation did not attain the same score on all of the games as prior work due to a shorter running time. Since we are primarily concerned with the rate of learning and not the final results, we do not consider this a deficiency. To directly evaluate the benefit of including exploration bonuses, we compare the performance of our approach primarily to our own DQN implementation, with the prior scores provided for reference.
In addition to raw-game scores, and learning curves, we also analyze our results on a new benchmark we have named Area Under Curve 100 (AUC-100). For each game, this benchmark computes the area under the game-score learning curve (using the trapezoid rule to approximate the integral). This area is then normalized by 100 times the score maximum game score achieved in [3], which represents 100 epochs of play at the best-known levels. This metric more effectively captures improvements to the game's learning rate and does not require running the games for 1000 epochs as in [3]. For this reason, we suggest it as an alternative metric to raw game-score.</p>
<p>Bowling The policy without exploration tended to fixate on a set pattern of nocking down six pins per frame. When bonuses were added, the dynamics learner quickly became adept at predicting this outcome and was thus encouraged to explore other release points.</p>
<p>Frostbite This game's dynamics changed substantially via the addition of extra platforms as the player progressed. As the dynamics of these more complex systems was not well understood, the system was encouraged to visit them often (which required making further progress in the game).</p>
<p>Seaquest A submarine must surface for air between bouts of fighting sharks. However, if the player resurfaces too soon they will suffer a penalty with effects on the game's dynamics. Since these effects are poorly understood by the model learning algorithm, resurfacing receives a high exploration bonus and hence the agent eventually learns to successfully resurface at the correct time.
$\mathbf{Q}^{<em>}$ bert Exploration bonuses resulted in a lower score. In Q</em>bert, the background changes color after level one. The dynamics predictor is unable to quickly adapt to such a dramatic change in the environment and consequently, exploration bonuses are assigned in near equality to almost every state that is visited. This negatively impacts the final policy.
Learning curves for each of the games are shown in Figure (3). Note that both of the exploration bonus algorithms learn significantly faster than epsilon-greedy Q-learning, and often continue learning even after the epsilon-greedy strategy converges. All games had the inputs normalized according</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Full learning curves and AUC-100 scores for all Atari games. We present the raw AUC100 scores in the appendix.</p>
<p>to [3] and were run for 100 epochs (where one epoch is 50,000 time steps). Between each epoch, the policy was updated and then the new policy underwent 10,000 time steps of testing. The results represent the average testing score across three trials after 100 epoch each.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Game</th>
<th style="text-align: left;">DQN <br> $\mathbf{1 0 0}$ <br> epochs</th>
<th style="text-align: left;">Exploration <br> Static AE <br> $\mathbf{1 0 0}$ epochs</th>
<th style="text-align: left;">Exploration <br> Dynamic <br> AE <br> $\mathbf{1 0 0}$ epochs</th>
<th style="text-align: left;">Boltzman <br> Exploration <br> $\mathbf{1 0 0}$ epochs</th>
<th style="text-align: left;">Thompson <br> Sampling <br> $\mathbf{1 0 0}$ epochs</th>
<th style="text-align: left;">DQN [3] <br> $\mathbf{1 0 0 0}$ epochs</th>
<th style="text-align: left;">Human <br> Expert <br> [3]</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Alien</td>
<td style="text-align: left;">1018</td>
<td style="text-align: left;">$\mathbf{1 4 3 6}$</td>
<td style="text-align: left;">1190</td>
<td style="text-align: left;">1301</td>
<td style="text-align: left;">1322</td>
<td style="text-align: left;">3069</td>
<td style="text-align: left;">6875</td>
</tr>
<tr>
<td style="text-align: left;">Asteroids</td>
<td style="text-align: left;">1043</td>
<td style="text-align: left;">$\mathbf{1 4 8 6}$</td>
<td style="text-align: left;">939</td>
<td style="text-align: left;">1287</td>
<td style="text-align: left;">812</td>
<td style="text-align: left;">1629</td>
<td style="text-align: left;">13157</td>
</tr>
<tr>
<td style="text-align: left;">Bank Heist</td>
<td style="text-align: left;">102</td>
<td style="text-align: left;">$\mathbf{1 3 1}$</td>
<td style="text-align: left;">95</td>
<td style="text-align: left;">101</td>
<td style="text-align: left;">129</td>
<td style="text-align: left;">429.7</td>
<td style="text-align: left;">734.4</td>
</tr>
<tr>
<td style="text-align: left;">Beam Rider</td>
<td style="text-align: left;">1604</td>
<td style="text-align: left;">1520</td>
<td style="text-align: left;">$\mathbf{1 6 4 0}$</td>
<td style="text-align: left;">1228</td>
<td style="text-align: left;">1361</td>
<td style="text-align: left;">6846</td>
<td style="text-align: left;">5775</td>
</tr>
<tr>
<td style="text-align: left;">Bowling</td>
<td style="text-align: left;">68.1</td>
<td style="text-align: left;">130</td>
<td style="text-align: left;">$\mathbf{1 3 3}$</td>
<td style="text-align: left;">113</td>
<td style="text-align: left;">85.2</td>
<td style="text-align: left;">42.4</td>
<td style="text-align: left;">154.8</td>
</tr>
<tr>
<td style="text-align: left;">Breakout</td>
<td style="text-align: left;">146</td>
<td style="text-align: left;">162</td>
<td style="text-align: left;">178</td>
<td style="text-align: left;">219</td>
<td style="text-align: left;">$\mathbf{2 2 2}$</td>
<td style="text-align: left;">401.2</td>
<td style="text-align: left;">31.8</td>
</tr>
<tr>
<td style="text-align: left;">Enduro</td>
<td style="text-align: left;">281</td>
<td style="text-align: left;">264</td>
<td style="text-align: left;">277</td>
<td style="text-align: left;">$\mathbf{2 8 4}$</td>
<td style="text-align: left;">236</td>
<td style="text-align: left;">301.8</td>
<td style="text-align: left;">309.6</td>
</tr>
<tr>
<td style="text-align: left;">Freeway</td>
<td style="text-align: left;">10.5</td>
<td style="text-align: left;">10.5</td>
<td style="text-align: left;">12.5</td>
<td style="text-align: left;">$\mathbf{1 3 . 9}$</td>
<td style="text-align: left;">12.0</td>
<td style="text-align: left;">30.3</td>
<td style="text-align: left;">29.6</td>
</tr>
<tr>
<td style="text-align: left;">Frostbite</td>
<td style="text-align: left;">369</td>
<td style="text-align: left;">$\mathbf{6 4 9}$</td>
<td style="text-align: left;">380</td>
<td style="text-align: left;">605</td>
<td style="text-align: left;">494</td>
<td style="text-align: left;">328.3</td>
<td style="text-align: left;">4335</td>
</tr>
<tr>
<td style="text-align: left;">Montezuma</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">4367</td>
</tr>
<tr>
<td style="text-align: left;">Pong</td>
<td style="text-align: left;">17.6</td>
<td style="text-align: left;">$\mathbf{1 8 . 5}$</td>
<td style="text-align: left;">18.2</td>
<td style="text-align: left;">18.2</td>
<td style="text-align: left;">18.2</td>
<td style="text-align: left;">18.9</td>
<td style="text-align: left;">9.3</td>
</tr>
<tr>
<td style="text-align: left;">Q'bert</td>
<td style="text-align: left;">$\mathbf{4 6 4 9}$</td>
<td style="text-align: left;">3291</td>
<td style="text-align: left;">3263</td>
<td style="text-align: left;">4014</td>
<td style="text-align: left;">3251</td>
<td style="text-align: left;">10596</td>
<td style="text-align: left;">13455</td>
</tr>
<tr>
<td style="text-align: left;">Seaquest</td>
<td style="text-align: left;">2106</td>
<td style="text-align: left;">2636</td>
<td style="text-align: left;">$\mathbf{4 4 7 2}$</td>
<td style="text-align: left;">3808</td>
<td style="text-align: left;">1337</td>
<td style="text-align: left;">5286</td>
<td style="text-align: left;">20182</td>
</tr>
<tr>
<td style="text-align: left;">Space Invaders</td>
<td style="text-align: left;">634</td>
<td style="text-align: left;">649</td>
<td style="text-align: left;">$\mathbf{7 1 6}$</td>
<td style="text-align: left;">697</td>
<td style="text-align: left;">459</td>
<td style="text-align: left;">1976</td>
<td style="text-align: left;">1652</td>
</tr>
</tbody>
</table>
<p>Table 1: A comparison of maximum scores achieved by different methods. Static AE trains the state-space auto encoder on 250000 raw game frames prior to policy optimization (raw frames are taken from random agent play). Dynamic AE retrains the auto encoder after each epoch, using the last 250000 images as a training set. Note that exploration bonuses help us to achieve state of the art results on Bowling and Frostbite. Each of these games provides a significant exploration challenge. Bolded numbers indicate the best-performing score among our experiments. Note that this score is sometimes lower than the score reported for DQN in prior work as our implementation only one-tenth as long as in [3].</p>
<p>The results show that more nuanced exploration strategies generally improve on the naive epsilon greedy approach, with the Boltzman and Thompson sampling methods achieving the best results on three of the games. However, exploration bonuses achieve the fastest learning and the best results most consistently, outperforming the other three methods on 7 of the 14 games in terms of AUC-100.</p>
<h1>7 CONCLUSION</h1>
<p>In this paper, we evaluated several scalable and efficient exploration algorithms for reinforcement learning in tasks with complex, high-dimensional observations. Our results show that a new method based on assigning exploration bonuses most consistently achieves the largest improvement on a range of challenging Atari games, particularly those on which human players outperform prior learning methods. Our exploration method learns a model of the dynamics concurrently with the policy. This model predicts a learned representation of the state, and a function of this prediction error is added to the reward as an exploration bonus to encourage the policy to visit states with high novelty.
One of the limitations of our approach is that the misprediction error metric assumes that any misprediction in the state is caused by inaccuracies in the model. While this is true in deterministic environments, stochastic dynamics violate this assumption. An extension of our approach to stochastic systems requires a more nuanced treatment of the distinction between stochastic dynamics and uncertain dynamics, which we hope to explore in future work. Another intriguing direction for future work is to examine how the learned dynamics model can be incorporated into the policy learning process, beyond just providing exploration bonuses. This could in principle enable substantially faster learning than purely model-free approaches.</p>
<h1>REFERENCES</h1>
<p>[1] A. L. Strehl and M. L. Littman, An Analysis of Model-Based Interval Estimation for Markov Decision Processes. Journal of Computer and System Sciences, 74, 12091331.
[2] J. Z. Kolter and A. Y. Ng, Near-Bayesian Exploration in Polynomial Time Proceedings of the 26th Annual International Conference on Machine Learning, pp. 18, 2009.
[3] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, et al. Human-level Control Through Deep Reinforcement Learning. Nature, 518(7540):529533, 2015.
[4] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning. Springer series in statistics. Springer, New York, 2001.
[5] M. Belkin, P. Niyogi and V. Sindhwani, Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples. JMLR, vol. 7, pp. 2399-2434, Nov. 2006.
[6] G. E. Hinton and R. Salakhutdinov, Reducing the dimensionality of data with neural networks. Science, $313,504-507,2006$.
[7] R. I. Brafman and M. Tennenholtz, R-max, a general polynomial time algorithm for near-optimal reinforcement learning. Journal of Machine Learning Research, 2002.
[8] M. Kearns and D. Koller, Efficient reinforcement learning in factored MDPs. Proc. IJCAI, 1999.
[9] M. Kearns and S. Singh, Near-optimal reinforcement learning in polynomial time. Machine Learning Journal, 2002.
[10] W. D. Smart and L. P. Kaelbling, Practical reinforcement learning in continuous spaces. Proc. ICML, 2000.
[11] J. Sorg, S. Singh, R. L. Lewis, Variance-Based Rewards for Approximate Bayesian Reinforcement Learning. Proc. UAI, 2010.
[12] M. Lopes, T. Lang, M. Toussaint and P.-Y. Oudeyer, Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress. NIPS, 2012.
[13] M. Geist and O. Pietquin, Managing Uncertainty within Value Function Approximation in Reinforcement Learning. W. on Active Learning and Experimental Design, 2010.
[14] M. Araya, O. Buffet, and V. Thomas. Near-optimal BRL Using Optimistic Local Transitions. (ICML-12), ser. ICML 12, J. Langford and J. Pineau, Eds. New York, NY, USA: Omnipress, Jul. 2012, pp. 97-104.
[15] F. Doshi-Velez, D. Wingate, N. Roy, and J. Tenenbaum, Nonparametric Bayesian Policy Priors for Reinforcement Learning. NIPS, 2014.
[16] T. Lang, M. Toussaint, K. Keristing, Exploration in relational domains for model-based reinforcement learning Proc. AAMAS, 2014.
[17] Juergen Schmidhuber Developmental Robotics, Optimal Artificial Curiosity, Creativity, Music, and the Fine Arts. . Connection Science, vol. 18 (2), p 173-187. 2006.
[18] J. Pazis and R. Parr, PAC Optimal Exploration in Continuous Space Markov Decision Processes. Proc. AAAI, 2013.
[19] Kakade, S., Kearns, M., and Langford, J. (2003). Exploration in metric state spaces. Proc. ICML.
[20] A. Guez, D. Silver, P. Dayan, Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search. NIPS, 2014.
[21] X. Guo, S. Singh, H. Lee, R. Lewis, X. Wang, Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning. NIPS, 2014.
[22] Y Gal, Z Ghahramani. Dropout as a Bayesian approximation: Insights and applications Deep Learning Workshop, ICML
[23] David Carmel and Shaul Markovitch. Exploration Strategies for Model-based Learning in Multi-agent Systems Autonomous Agents and Multi-Agent Systems Volume 2, Issue 2 , pp 141-172
[24] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling, The Arcade Learning Environment: An Evaluation Platform for General Agents. JAIR. Volume 47, p.235-279. June 2013.
[25] John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel. Trust Region Policy Optimization Arxiv preprint 1502.05477.</p>
<h1>8 APPENDIX</h1>
<h3>8.1 ON AUTO ENCODER LAYER SELECTION</h3>
<p>Recall that we trained an auto-encoder to encode the game's state space. We then trained a predictive model on the next auto-encoded frame rather than directly training on the pixel intensity values of the next frame. To obtain the encoded space, we ran each state through an eight layer auto-encoder for training and then utilized the auto-encoder's sixth layer as an encoded state space. We chose to use the sixth layer rather than the bottleneck fourth layer because we found that, over 20 iterations of Seaquest at 100 epochs per iteration, using this layer for encoding delivered measurably better performance than using the bottleneck layer. The results of that experiment are presented below.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Game score averaged over 20 Seaquest iterations with various choices for the state-space encoding layer. Notice that choosing the sixth layer to encode the state space significantly outperformed the bottleneck layer.</p>
<h3>8.2 ON THE QUALITY OF THE LEARNED MODEL DYNAMICS</h3>
<p>Evaluating the quality of the learned dynamics model is somewhat difficult because the system is rewarded achieving higher error rates. A dynamics model that converges quickly is not useful for exploration bonuses. Nevertheless, when we plot the mean of the normalized residuals across all games and all trials used in our experiments, we see that the errors of the learned dynamics models continually decrease over time. The mean normalized residual after 100 epochs is approximately half of the maximal mean achieved. This suggests that each dynamics model was able to correctly learn properties of underlying dynamics for its given game.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Normalized dynamics model prediction residual across all trials of all games. Note that the dynamics model is retrained from scratch for each trial.</p>
<h1>8.3 RAW AUC-100 SCORES</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Game</th>
<th style="text-align: left;">DQN</th>
<th style="text-align: left;">Exploration <br> Static AE</th>
<th style="text-align: left;">Exploration <br> Dynamic <br> AE</th>
<th style="text-align: left;">Boltzman <br> Exploration</th>
<th style="text-align: left;">Thompson <br> Sampling</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Alien</td>
<td style="text-align: left;">0.153</td>
<td style="text-align: left;">0.198</td>
<td style="text-align: left;">0.171</td>
<td style="text-align: left;">0.187</td>
<td style="text-align: left;">0.204</td>
</tr>
<tr>
<td style="text-align: left;">Asteroids</td>
<td style="text-align: left;">0.259</td>
<td style="text-align: left;">0.415</td>
<td style="text-align: left;">0.254</td>
<td style="text-align: left;">0.456</td>
<td style="text-align: left;">0.223</td>
</tr>
<tr>
<td style="text-align: left;">Bank Heist</td>
<td style="text-align: left;">0.0715</td>
<td style="text-align: left;">0.1459</td>
<td style="text-align: left;">0.089</td>
<td style="text-align: left;">0.089</td>
<td style="text-align: left;">0.1303</td>
</tr>
<tr>
<td style="text-align: left;">Beam Rider</td>
<td style="text-align: left;">0.1122</td>
<td style="text-align: left;">0.0919</td>
<td style="text-align: left;">0.1112</td>
<td style="text-align: left;">0.0817</td>
<td style="text-align: left;">0.0897</td>
</tr>
<tr>
<td style="text-align: left;">Bowling</td>
<td style="text-align: left;">0.964</td>
<td style="text-align: left;">1.493</td>
<td style="text-align: left;">1.836</td>
<td style="text-align: left;">1.338</td>
<td style="text-align: left;">1.122</td>
</tr>
<tr>
<td style="text-align: left;">Breakout</td>
<td style="text-align: left;">0.191</td>
<td style="text-align: left;">0.202</td>
<td style="text-align: left;">0.192</td>
<td style="text-align: left;">0.294</td>
<td style="text-align: left;">0.254</td>
</tr>
<tr>
<td style="text-align: left;">Enduro</td>
<td style="text-align: left;">0.518</td>
<td style="text-align: left;">0.495</td>
<td style="text-align: left;">0.589</td>
<td style="text-align: left;">0.538</td>
<td style="text-align: left;">0.466</td>
</tr>
<tr>
<td style="text-align: left;">Freeway</td>
<td style="text-align: left;">0.206</td>
<td style="text-align: left;">0.213</td>
<td style="text-align: left;">0.295</td>
<td style="text-align: left;">0.313</td>
<td style="text-align: left;">0.228</td>
</tr>
<tr>
<td style="text-align: left;">Frostbite</td>
<td style="text-align: left;">0.573</td>
<td style="text-align: left;">0.971</td>
<td style="text-align: left;">0.622</td>
<td style="text-align: left;">0.928</td>
<td style="text-align: left;">0.746</td>
</tr>
<tr>
<td style="text-align: left;">Montezuma</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
</tr>
<tr>
<td style="text-align: left;">Pong</td>
<td style="text-align: left;">0.52</td>
<td style="text-align: left;">0.56</td>
<td style="text-align: left;">0.424</td>
<td style="text-align: left;">0.612</td>
<td style="text-align: left;">0.612</td>
</tr>
<tr>
<td style="text-align: left;">Q"bert</td>
<td style="text-align: left;">0.155</td>
<td style="text-align: left;">0.104</td>
<td style="text-align: left;">0.121</td>
<td style="text-align: left;">0.13</td>
<td style="text-align: left;">0.127</td>
</tr>
<tr>
<td style="text-align: left;">Seaquest</td>
<td style="text-align: left;">0.16</td>
<td style="text-align: left;">0.172</td>
<td style="text-align: left;">0.265</td>
<td style="text-align: left;">0.194</td>
<td style="text-align: left;">0.174</td>
</tr>
<tr>
<td style="text-align: left;">Space Invaders</td>
<td style="text-align: left;">0.205</td>
<td style="text-align: left;">0.183</td>
<td style="text-align: left;">0.219</td>
<td style="text-align: left;">0.183</td>
<td style="text-align: left;">0.146</td>
</tr>
</tbody>
</table>
<p>Table 2: AUC-100 is computed by comparing the area under the game-score learning curve for 100 epochs of play to the area under of the rectangle with dimensions 100 by the maximum DQN score the game achieved in [3]. The integral is approximated with the trapezoid rule. This more holistically captures the games learning rate and does not require running the games for 1000 epochs as in [3]. For this reason, we suggest it as an alternative metric to raw game-score.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ In our implementation, the memory bank $\Omega$ is used to retrain the RL algorithm via experience replay once per epoch (50000 steps). Hence, 49999 of these policy updates will simply do nothing.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
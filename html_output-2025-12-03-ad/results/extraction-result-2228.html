<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2228 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2228</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2228</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-60.html">extraction-schema-60</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <p><strong>Paper ID:</strong> paper-278740334</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.11737v3.pdf" target="_blank">Token-Level Uncertainty Estimation for Large Language Model Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> While Large Language Models (LLMs) have demonstrated impressive capabilities, their output quality remains inconsistent across various application scenarios, making it difficult to identify trustworthy responses, especially in complex tasks requiring multi-step reasoning. In this paper, we propose a token-level uncertainty estimation framework to enable LLMs to self-assess and self-improve their generation quality in mathematical reasoning. Specifically, we introduce low-rank random weight perturbation to LLM decoding, generating predictive distributions that we use to estimate token-level uncertainties. We then aggregate these uncertainties to reflect semantic uncertainty of the generated sequences. Experiments on mathematical reasoning datasets of varying difficulty demonstrate that our token-level uncertainty metrics strongly correlate with answer correctness and model robustness. Additionally, we explore using uncertainty to directly enhance the model’s reasoning performance through multiple generations and the particle filtering algorithm. Our approach consistently outperforms existing uncertainty estimation methods, establishing effective uncertainty estimation as a valuable tool for both evaluating and improving reasoning generation in LLMs.</p>
                <p><strong>Cost:</strong> 0.028</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2228.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2228.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TokUR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Token-level Uncertainty estimation for Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training-free framework that estimates token-level epistemic and aleatoric uncertainty for autoregressive LLM decoding via low-rank random weight perturbations and aggregates these to obtain response-level uncertainty for reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>TokUR (Token-level Uncertainty estimation for Reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Introduces calibrated low-rank Gaussian perturbations to attention weight matrices during decoding to form an approximate weight posterior; samples token-level predictive distributions stepwise, computes Total / Aleatoric / Epistemic uncertainties per token, and sums them to a response score used for detection, ranking, and test-time guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Natural language reasoning (mathematical problem solving with LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Token-level Epistemic/Aleatoric/Total Uncertainty (TokUR EU/AU/TU)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Per-token epistemic uncertainty is mutual information I(y_t; θ | y_<t, x) estimated by low-rank weight perturbation (LoRA-style noise) and BMA over M perturbation samples then aggregated (summed) across tokens to form a response-level uncertainty score. Perturbation rank r' = 8, perturbation strength σ_q = 0.1, M=2 typically used.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>data-driven ML uncertainty estimation (Bayesian approximation via low-rank stochastic perturbation)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Answer correctness (binary True/False); dataset ground-truth labels</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Correctness is determined by canonicalized exact numeric or symbolic equality between model output and dataset answer (numerical equality and symbolic equivalence checks described in Appendix D.5).</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>AUROC / AUPRC / Top-50% ACC relative to ground-truth labels. Example (Llama-3.2-1B-Instruct, MATH500): TokUR (TU) AUROC = 80.64% ±0.29, AUPRC = 56.79% ±0.74, ACC* = 44.67% ±0.46. (Compare baseline LL AUROC = 55.41% ±0.54, AUPRC = 25.88% ±0.87, ACC* = 29.87% ±0.82.)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>See quantitative_gap_measure. On MATH500 (Llama-3.2-1B): AUROC 80.64%, AUPRC 56.79%, ACC* 44.67%. On GSM8K: AUROC 75.07%, AUPRC 70.29%, ACC* 62.31%.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>Model baseline pass@1 accuracy (ground-truth answer accuracy) reported for reference: GSM8K pass@1 = 44.43% (Llama-3.2-1B-Instruct), MATH500 pass@1 = 25.60% (Llama-3.2-1B-Instruct). TokUR is evaluated against these ground-truth labels to compute AUROC/AUPRC/ACC*.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>In-distribution / near-training-data math reasoning benchmarks (GSM8K, MATH500, DeepScaleR subset) — problems of varying difficulty, but generally within benchmark distributions rather than wet-lab or physical discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>The paper reports that discriminative power (AUROC) of uncertainty measures decreases as question difficulty increases (difficulty proxy = number of failed attempts by another model), with AUROC falling in mid-to-high difficulty levels (levels 7–9) though sometimes rising at extreme difficulty due to dataset imbalance; this indicates the proxy→ground-truth alignment worsens for harder / more novel problems.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Low-rank Bayesian perturbation for token-level uncertainty; aggregation into response score; length normalization (optional), selection/reranking (Maj@N, Weighted Best-of-N), and using uncertainty as an implicit reward for particle-filtering based online test-time scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>Substantial: e.g., on MATH500 (Llama-3.2-1B) TokUR (TU) increases AUROC from LL's 55.41% to 80.64% (+25.23 percentage points) and AUPRC from 25.88% to 56.79% (+30.91 pp); in test-time scaling (Maj@N, N=16) TokUR (EU) increases GSM8K accuracy from LL 47.10% to 50.38% (+3.28 pp) and MATH500 from LL 26.42% to 28.28% (+1.86 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Paper discusses computational cost: repeated weight-perturbation sampling at inference time incurs extra compute and latency (efficiency challenge) compared with single-pass decoding; no physical experimental (wet-lab) cost comparison is relevant in this NLP setting.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging ML methodology — extends established Bayesian LoRA ideas to long-form autoregressive generation; computational uncertainty estimation is developing but not yet standard for long-form reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td>Partial: TokUR's token-level EU correlates with correctness (AUROC substantially > 0.5). The paper reports AUROC/AUPRC as empirical calibration/performance metrics but does not provide standard calibration plots (e.g., reliability diagrams) linking predicted uncertainty magnitudes to absolute error rates; quantitative calibration measures beyond AUROC/AUPRC are not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Computational-only cascade: base LLM generation → TokUR perturbation-based uncertainty scoring → candidate ranking and selection (Maj@N / WBoN) → evaluation against dataset ground-truth labels. Errors propagate from poor uncertainty estimates to candidate selection; no intermediate experimental stages.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>High variance in TokUR estimates (sampling noise), inference-time compute overhead from repeated perturbations, token-level aggregation may miss higher-level semantic/logical inconsistencies, and decreased discriminative power on very hard / imbalanced problems.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Sequence length bias (longer sequences accumulate higher summed uncertainty), decoding temperature affects AU (aleatoric) more than EU, problem difficulty (harder problems produce higher uncertainty and lower AUROC), and dataset class imbalance can inflate metrics (e.g., AUROC at extreme difficulty levels).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2228.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2228.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Log-Likelihood (mean token log-probability)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A common proxy score computed as the mean (or cumulative) token log-probabilities of a generated sequence; used as a surrogate for generation quality/confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Log-Likelihood (LL) scoring</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Compute mean (or cumulative) of token-wise log-probabilities assigned by the LLM to the generated response; used to rank candidate generations and as a confidence proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Natural language generation (LLM reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Mean token Log-Likelihood (LL)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Average (or sum) of per-token log-probabilities under the model's next-token distribution for the generated sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>data-driven ML prediction (internal model score / likelihood surrogate)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Answer correctness (binary, dataset labels)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Ground-truth correctness via canonicalized equality checking against dataset solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Example (Llama-3.2-1B-Instruct, MATH500): LL AUROC = 55.41% ±0.54, AUPRC = 25.88% ±0.87, ACC* = 29.87% ±0.82, compared to TokUR TU AUROC = 80.64% ±0.29 on same dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>AUROC ~55.4%, AUPRC ~25.9% on MATH500 (Llama-3.2-1B); for GSM8K LL AUROC = 69.01% ±0.03, AUPRC = 58.51% ±0.09 (Llama-3.2-1B) per Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>Model Pass@1 accuracies used as ground-truth baselines: GSM8K 44.43%, MATH500 25.60% (Llama-3.2-1B-Instruct).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>In-distribution for benchmark problems; known to be a weak proxy in multi-step reasoning because high likelihood can correspond to plausible but wrong reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>LL's discriminative power degrades on harder questions and multi-step reasoning tasks; TokUR significantly outperforms LL especially on difficult benchmarks (quantified above).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Not applicable within LL itself; paper compares LL ranking versus uncertainty-based reranking (TokUR).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>Replacing LL with TokUR for candidate ranking increases accuracy (e.g., GSM8K Maj@N N=16: LL 47.10% → TokUR (EU) 50.38%, improvement ~+3.28 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>LL is cheap (single pass), TokUR requires repeated perturbations and is more costly at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established baseline and widely used proxy in NLP.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td>LL scores are not explicitly calibrated to ground-truth correctness in this paper; evaluated by AUROC/AUPRC only.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Direct ranking of candidates by LL then evaluation against dataset labels.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>LL can be overconfident for wrong but fluent outputs, does not decompose epistemic vs aleatoric uncertainty, poorer discriminative power on long-form multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Length normalization can affect LL ranking; beam search / length bias factors influence LL scores.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2228.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2228.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Predictive Entropy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Entropy of the predicted token distribution (mean token entropy aggregated) used as an uncertainty proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Predictive Entropy (PE)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Compute entropy H[p(y_t | y_<t, x)] for tokens and average across tokens (or sum) to yield a predictive uncertainty score (Total Uncertainty analog).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>LLM reasoning / uncertainty estimation</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Predictive Entropy (mean token entropy)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Average per-token Shannon entropy of the model predictive distributions; measures total uncertainty (mix of aleatoric+epistemic under single model).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>data-driven ML prediction (logit-derived entropy)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Answer correctness (dataset labels)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Canonicalized equality check between generated answer and ground-truth.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Example (Llama-3.2-1B-Instruct, MATH500): PE AUROC = 57.08% ±0.89, AUPRC = 26.88% ±1.05, ACC* = 31.33% ±0.82 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>AUROC ~57.1% on MATH500 (Llama-3.2-1B).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>Pass@1 baselines as above (MATH500 25.60%).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>In-distribution; entropy may fail to separate model uncertainty from intrinsic output multiplicity.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>PE discrimination degrades on harder problems; AUROC above chance but below TokUR.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Not in PE itself; TokUR decomposes entropy into AU/EU and uses perturbations to better capture epistemic part.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>TokUR EU/TU significantly outperform PE (e.g., TokUR TU 80.64% AUROC vs PE 57.08% on MATH500).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>PE is inexpensive (single forward pass entropies); TokUR is more costly due to perturbation sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established method for predictive uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td>PE reports AUROC/AUPRC but not calibration curves versus absolute correctness rates.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Entropy-based ranking evaluated vs dataset labels.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Conflates aleatoric and epistemic uncertainty; token-/sequence-level marginalizations are intractable without approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Decoding temperature increases AU markedly (aleatoric), EU less affected per ablation studies.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2228.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2228.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Certainty</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Certainty (KL-from-uniform confidence)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A ranking/confidence proxy that measures the KL divergence between the predicted token distribution and a uniform distribution to quantify confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Self-Certainty</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>At each decoding step compute KL(p(.|.) || Uniform) (or equivalent measure) to produce a confidence score; aggregate across tokens to rank candidate responses.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>LLM candidate selection / de-hallucination</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>KL-divergence from uniform (Self-Certainty)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Distance between model token distribution and uniform distribution per token; higher KL means more confident (peaked) distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>data-driven ML prediction (logit-based confidence metric)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Answer correctness (dataset labels)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Canonical answer equality checks used to label true vs false.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Example (Llama-3.2-1B-Instruct, MATH500): Self-Certainty AUROC = 71.17% ±0.30, AUPRC = 48.37% ±0.50, ACC* = 38.13% ±0.61 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>AUROC ~71.2% on MATH500 (Llama-3.2-1B); on GSM8K Self-Certainty AUROC = 73.41% ±0.00 per Table 1 for that model.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>See model pass@1 accuracies used as ground truth baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>In-distribution; designed as internal-signal-only method to prioritize high-confidence outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>Self-Certainty benefits from length normalization in some datasets (Table E.4.3) — its AUROC improves markedly with normalization on MATH500 (e.g., from ~23.76 to 71.17 in ablation), meaning performance is sensitive to aggregation choices and difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Aggregation and normalization of per-token KL; not a Bayesian uncertainty method but a heuristic confidence measure.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>Per Table 1, Self-Certainty is competitive and sometimes second-best, but TokUR still outperforms it on many metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Efficient (uses internal logits), cheaper than TokUR which requires sampling perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Recently proposed heuristic (2025), gaining use as a strong internal-signal baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td>Empirical AUROC/AUPRC reported; no explicit calibration to absolute error rates beyond those metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Used for candidate selection and evaluated against dataset labels.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Sensitive to sequence length and normalization; may overestimate confidence on long but incorrect chains unless normalized.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Aggregation/normalization choices and decoding temperature affect performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2228.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2228.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepConf</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Think With Confidence (DeepConf)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An uncertainty-driven scoring method that aggregates top-K log-probabilities at each decoding step to derive a confidence score for sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepConf</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Aggregates the sum (or mean) of the top-K token log-probabilities across decoding steps to produce a confidence ranking for candidate generations.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>LLM confidence estimation / candidate ranking</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Aggregated top-K log-probability score (DeepConf)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>At each token step sum top-K token log-probabilities; aggregate across sequence to get a confidence score that emphasizes peakedness of top mass.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>data-driven ML prediction (logit-derived surrogate)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Answer correctness (dataset labels)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Canonicalized answer comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Example (Llama-3.2-1B-Instruct, MATH500): DeepConf AUROC = 71.77% ±0.12, AUPRC = 46.00% ±0.42, ACC* = 39.87% ±0.46 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>AUROC ~71.8% on MATH500 (Llama-3.2-1B).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>Model pass@1 ground-truth accuracies as above.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>In-distribution evaluation; heuristic tailored to logit shape.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>Performance declines on more difficult reasoning problems; benefits from length normalization in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Not internal to DeepConf; paper compares it to TokUR which reduces gap further using epistemic uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>TokUR improves over DeepConf by several percentage points in AUROC/AUPRC across datasets (see Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Single-pass logits aggregation — cheaper than TokUR sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Recent proposal (2025) for uncertainty-driven test-time scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td>Evaluated with AUROC/AUPRC; no direct calibration vs absolute error rates reported.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Used to rank candidates for Maj@N or WBoN aggregation and evaluated vs ground-truth labels.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Relies on top-K logits which may not capture epistemic uncertainty; sensitive to K and normalization choices.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Logit sharpness and decoding temperature influence scores.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2228.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2228.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantic Entropy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An external-signal-based proxy that prompts the LLM (or uses an external NLI/embedding model) to verify the same response multiple times and computes semantic entropy across verifications to detect hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Semantic Entropy (SE)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Repeatedly ask a verifier model (LLM or external NLI/embedding) to judge or rephrase a response; compute the semantic-entropy of these repeated outputs as a proxy for confidence/hallucination detection.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Hallucination detection / LLM output verification</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Semantic Entropy (over verifier outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Entropy measured over multiple semantic verifications/paraphrases of the same response (requires an external verifier embedding or NLI model to compare semantic content).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>hybrid (LLM-derived + external semantic similarity model)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Answer correctness (dataset labels)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Canonicalized equality checks vs dataset answers.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Example (Llama-3.2-1B-Instruct, MATH500): SE AUROC = 47.29% ±3.81, AUPRC = 25.71% ±2.33, ACC* = 24.13% ±4.42 (Table 1) — lower performance in this reproduction setting relative to internal-only methods, but SE requires external models.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>AUROC ~47.3% on MATH500 in the paper's adapted setup (note SE uses external verifier and may have implementation variability).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>Models' pass@1 accuracies used as ground-truth baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>In-distribution verification, but employs external resources that may shift distributional assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>Performance depends on quality and calibration of external verifier; can be unstable and dataset-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Using stronger or task-tuned external verifiers could reduce the gap; not explored extensively in this paper beyond adaptation for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>Not quantified beyond the reported comparative AUROC/AUPRC numbers; SE did not outperform TokUR in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Higher cost because it requires multiple verification calls and/or external models; more expensive than internal-score proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging; semantic-verification approaches are an active area for hallucination detection.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td>Empirically measured via AUROC/AUPRC but not calibrated to absolute error rates; relies on external verifier calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>External verifier loop → semantic entropy → compare with ground-truth labels.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires external models (fairness/implementation differences), sensitive to verifier quality, and in this paper's reproduction yielded weaker empirical discrimination.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Quality of external NLI/embedding models and prompt templates for verification strongly affect performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2228.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2228.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>P(True)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>P(True) self-query</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proxy that directly queries the model to predict the probability its own output is 'True' (or 'False') and uses that probability as a confidence measure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>P(True) scoring</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>After generating an answer, ask the model to output the probability (or token probabilities) of the token 'True' vs 'False' for the correctness of that output, normalize and use as a score.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>LLM self-evaluation / calibration</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>P(True) (model-predicted probability of 'True')</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Probability mass assigned by the model to an explicit 'True' token (or affirmative) in a verification prompt; normalized between True/False tokens to produce a confidence score.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>verbalized ML self-evaluation (LLM-predicted probability)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Answer correctness (dataset labels)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Canonicalized equality checks vs dataset answers used as ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Example (Llama-3.2-1B-Instruct, MATH500): P(True) AUROC = 54.38% ±1.20, AUPRC = 26.39% ±1.26, ACC* = 27.60% ±1.18 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>AUROC ~54.4% on MATH500 (Llama-3.2-1B-Instruct).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>Pass@1 accuracies as ground-truth baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>In-distribution self-assessment; model tends to be overconfident or miscalibrated on multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>P(True) shows limited discriminative power on hard reasoning tasks; performance depends on the model's ability to self-verify chain-of-thought.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Not examined in-depth here; TokUR provides alternative uncertainty signals that outperform P(True) in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>TokUR (EU/TU) substantially outperforms P(True) in AUROC/AUPRC on MATH500 and GSM8K.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Requires extra verification prompt calls (cheaper than external-verifier SE but more expensive than pure-logit proxies).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Explored previously for self-evaluation; efficacy on long-form reasoning remains limited.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td>Empirical AUROC/AUPRC provided but explicit calibration vs error rates not given.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Generate answer → self-query P(True) → compare with label.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Relies on model's meta-cognitive ability to report correctness; prone to overconfidence and failure on multi-step errors.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Quality of verification prompt and model's training on meta-evaluation behaviors.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2228.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2228.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-Check</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-Check (reproduction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that evaluates model generations by asking the model (or a verifier LLM) to check answers multiple times; used as an internal-signal baseline in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-Check</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Reproduced official LLM-Check implementation for repeated verification of outputs to produce confidence signals; uses internal LLM checks rather than external NLI.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>LLM verification / hallucination detection</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Repeated LLM verification score (LLM-Check)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Score derived from multiple self-checks/verifications made by the LLM about the generated response; aggregated into confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>verbalized/self-verification hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Answer correctness</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Dataset label comparison (canonicalization) to determine correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Example (Llama-3.2-1B-Instruct, MATH500): LLM-Check AUROC = 56.41% ±0.96, AUPRC = 27.01% ±1.22, ACC* = 31.33% ±1.29 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>AUROC ~56.4% on MATH500 in the reproduced implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>Model pass@1 accuracies used as labels.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>In-distribution; internal verification can fail for complex multi-step errors.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>Performance varies by dataset and model size; tends to underperform TokUR.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Not implemented; compared as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>Less effective than TokUR in the experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Multiple verification calls increase compute vs single-pass logit proxies but are cheaper than using separate heavyweight external verifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Recent evaluation approach.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td>AUROC/AUPRC reported; no further calibration analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Repeated self-checking → scoring → compare to ground-truth.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Dependence on LLM's self-check quality; reproduction variability across implementations and prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Prompt design for verification and the model's exposure to 'self-check' style fine-tuning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2228.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2228.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>INSIDE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>INSIDE (Internal States for hallucInation DEtection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Method that queries the LLM to verify the same response multiple times and computes semantic-like entropy using the model's internal states or verification outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>INSIDE</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Tailored to estimate query-level uncertainty by repeated verification attempts using the LLM and computing an entropy-like measure across those attempts to detect hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Hallucination detection / uncertainty estimation</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>INSIDE entropy over repeated verifications</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Entropy computed across multiple verifier outputs produced by internal LLM prompts; used as a proxy for response reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>verbalized/self-verification hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Answer correctness</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Canonical equality checks versus dataset ground-truth.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Example (Llama-3.2-1B-Instruct, MATH500): INSIDE AUROC = 55.71% ±4.69, AUPRC = 28.82% ±4.05, ACC* = 29.20% ±4.33 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>AUROC ~55.7% on MATH500 (reproduced setup).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>Dataset correctness labels used for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>In-distribution verification; leverages internal LLM signals.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>Performance sensitive to dataset and prompt design; generally outperformed by TokUR.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Not applied; used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>Lower than TokUR in reported AUROC/AUPRC numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Requires multiple verification prompts — moderate compute overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Recently proposed; demonstrated for hallucination detection.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td>Empirical AUROC/AUPRC reported; no formal calibration analysis provided.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Repeated verification by LLM → entropy calculation → evaluation vs labels.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>High variance and sensitivity to prompt and sampling; requires many repeats to reduce noise.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Model-specific self-evaluation capability and prompt quality.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2228.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2228.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Shifting Attention to Relevance (SAR) (adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An external-signal method that computes sentence-level relevance/uncertainty using an external semantic similarity model; adapted in this paper for response-level uncertainty comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SAR (adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Compute sentence-level relevance/uncertainty scores using an external semantic similarity model across multiple verification attempts and aggregate into a response-level proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>LLM hallucination detection / uncertainty</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Sentence-level SAR score (semantic similarity-based relevance)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Uses an external semantic similarity model to assess relevance/consistency across multiple verification attempts; aggregated into an uncertainty/reliability score.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>hybrid (external semantic similarity model + LLM outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Answer correctness</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Canonicalized equality checks.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Example (Llama-3.2-1B-Instruct, MATH500): SAR AUROC = 44.57% ±2.04, AUPRC = 24.03% ±2.53, ACC* = 21.07% ±1.62 (Table 1) in the adapted setup.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>AUROC ~44.6% on MATH500 in this paper's adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>Dataset labels used as ground-truth.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>In-distribution; relies on external semantic model quality.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>External verifier dependency causes variable performance across datasets; SAR underperformed compared to internal proxies and TokUR in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Improving external similarity models or ensembling verifiers could reduce gap (not explored here).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>Not shown to outperform TokUR in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Higher due to external semantic model calls and multiple verifications.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Experimental/adapted approach.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td>AUROC/AUPRC reported but no formal calibration analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>External semantic checks → SAR scoring → evaluation vs labels.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires external models; fairness/implementation differences; observed weaker empirical performance in this reproduction/adaptation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning <em>(Rating: 2)</em></li>
                <li>Bayesian low-rank adaptation for large language models <em>(Rating: 2)</em></li>
                <li>Blob: Bayesian low-rank adaptation by backpropagation for large language models <em>(Rating: 2)</em></li>
                <li>Detecting hallucinations in large language models using semantic entropy <em>(Rating: 2)</em></li>
                <li>Scalable best-of-n selection for large language models via self-certainty <em>(Rating: 2)</em></li>
                <li>Deep think with confidence <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2228",
    "paper_id": "paper-278740334",
    "extraction_schema_id": "extraction-schema-60",
    "extracted_data": [
        {
            "name_short": "TokUR",
            "name_full": "Token-level Uncertainty estimation for Reasoning",
            "brief_description": "A training-free framework that estimates token-level epistemic and aleatoric uncertainty for autoregressive LLM decoding via low-rank random weight perturbations and aggregates these to obtain response-level uncertainty for reasoning tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "TokUR (Token-level Uncertainty estimation for Reasoning)",
            "system_description": "Introduces calibrated low-rank Gaussian perturbations to attention weight matrices during decoding to form an approximate weight posterior; samples token-level predictive distributions stepwise, computes Total / Aleatoric / Epistemic uncertainties per token, and sums them to a response score used for detection, ranking, and test-time guidance.",
            "domain": "Natural language reasoning (mathematical problem solving with LLMs)",
            "proxy_metric_name": "Token-level Epistemic/Aleatoric/Total Uncertainty (TokUR EU/AU/TU)",
            "proxy_metric_description": "Per-token epistemic uncertainty is mutual information I(y_t; θ | y_&lt;t, x) estimated by low-rank weight perturbation (LoRA-style noise) and BMA over M perturbation samples then aggregated (summed) across tokens to form a response-level uncertainty score. Perturbation rank r' = 8, perturbation strength σ_q = 0.1, M=2 typically used.",
            "proxy_metric_type": "data-driven ML uncertainty estimation (Bayesian approximation via low-rank stochastic perturbation)",
            "ground_truth_metric": "Answer correctness (binary True/False); dataset ground-truth labels",
            "ground_truth_description": "Correctness is determined by canonicalized exact numeric or symbolic equality between model output and dataset answer (numerical equality and symbolic equivalence checks described in Appendix D.5).",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "AUROC / AUPRC / Top-50% ACC relative to ground-truth labels. Example (Llama-3.2-1B-Instruct, MATH500): TokUR (TU) AUROC = 80.64% ±0.29, AUPRC = 56.79% ±0.74, ACC* = 44.67% ±0.46. (Compare baseline LL AUROC = 55.41% ±0.54, AUPRC = 25.88% ±0.87, ACC* = 29.87% ±0.82.)",
            "proxy_performance": "See quantitative_gap_measure. On MATH500 (Llama-3.2-1B): AUROC 80.64%, AUPRC 56.79%, ACC* 44.67%. On GSM8K: AUROC 75.07%, AUPRC 70.29%, ACC* 62.31%.",
            "ground_truth_performance": "Model baseline pass@1 accuracy (ground-truth answer accuracy) reported for reference: GSM8K pass@1 = 44.43% (Llama-3.2-1B-Instruct), MATH500 pass@1 = 25.60% (Llama-3.2-1B-Instruct). TokUR is evaluated against these ground-truth labels to compute AUROC/AUPRC/ACC*.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_characterization": "In-distribution / near-training-data math reasoning benchmarks (GSM8K, MATH500, DeepScaleR subset) — problems of varying difficulty, but generally within benchmark distributions rather than wet-lab or physical discovery.",
            "gap_varies_with_novelty": true,
            "gap_variation_details": "The paper reports that discriminative power (AUROC) of uncertainty measures decreases as question difficulty increases (difficulty proxy = number of failed attempts by another model), with AUROC falling in mid-to-high difficulty levels (levels 7–9) though sometimes rising at extreme difficulty due to dataset imbalance; this indicates the proxy→ground-truth alignment worsens for harder / more novel problems.",
            "gap_reduction_method": "Low-rank Bayesian perturbation for token-level uncertainty; aggregation into response score; length normalization (optional), selection/reranking (Maj@N, Weighted Best-of-N), and using uncertainty as an implicit reward for particle-filtering based online test-time scaling.",
            "gap_reduction_effectiveness": "Substantial: e.g., on MATH500 (Llama-3.2-1B) TokUR (TU) increases AUROC from LL's 55.41% to 80.64% (+25.23 percentage points) and AUPRC from 25.88% to 56.79% (+30.91 pp); in test-time scaling (Maj@N, N=16) TokUR (EU) increases GSM8K accuracy from LL 47.10% to 50.38% (+3.28 pp) and MATH500 from LL 26.42% to 28.28% (+1.86 pp).",
            "validation_cost_comparison": "Paper discusses computational cost: repeated weight-perturbation sampling at inference time incurs extra compute and latency (efficiency challenge) compared with single-pass decoding; no physical experimental (wet-lab) cost comparison is relevant in this NLP setting.",
            "temporal_validation": null,
            "domain_maturity": "Emerging ML methodology — extends established Bayesian LoRA ideas to long-form autoregressive generation; computational uncertainty estimation is developing but not yet standard for long-form reasoning.",
            "uncertainty_quantification": true,
            "uncertainty_calibration": "Partial: TokUR's token-level EU correlates with correctness (AUROC substantially &gt; 0.5). The paper reports AUROC/AUPRC as empirical calibration/performance metrics but does not provide standard calibration plots (e.g., reliability diagrams) linking predicted uncertainty magnitudes to absolute error rates; quantitative calibration measures beyond AUROC/AUPRC are not reported.",
            "multiple_proxies": false,
            "proxy_correlation": null,
            "validation_cascade": "Computational-only cascade: base LLM generation → TokUR perturbation-based uncertainty scoring → candidate ranking and selection (Maj@N / WBoN) → evaluation against dataset ground-truth labels. Errors propagate from poor uncertainty estimates to candidate selection; no intermediate experimental stages.",
            "publication_bias_discussion": false,
            "limitations_challenges": "High variance in TokUR estimates (sampling noise), inference-time compute overhead from repeated perturbations, token-level aggregation may miss higher-level semantic/logical inconsistencies, and decreased discriminative power on very hard / imbalanced problems.",
            "domain_specific_factors": "Sequence length bias (longer sequences accumulate higher summed uncertainty), decoding temperature affects AU (aleatoric) more than EU, problem difficulty (harder problems produce higher uncertainty and lower AUROC), and dataset class imbalance can inflate metrics (e.g., AUROC at extreme difficulty levels).",
            "uuid": "e2228.0"
        },
        {
            "name_short": "LL",
            "name_full": "Log-Likelihood (mean token log-probability)",
            "brief_description": "A common proxy score computed as the mean (or cumulative) token log-probabilities of a generated sequence; used as a surrogate for generation quality/confidence.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Log-Likelihood (LL) scoring",
            "system_description": "Compute mean (or cumulative) of token-wise log-probabilities assigned by the LLM to the generated response; used to rank candidate generations and as a confidence proxy.",
            "domain": "Natural language generation (LLM reasoning)",
            "proxy_metric_name": "Mean token Log-Likelihood (LL)",
            "proxy_metric_description": "Average (or sum) of per-token log-probabilities under the model's next-token distribution for the generated sequence.",
            "proxy_metric_type": "data-driven ML prediction (internal model score / likelihood surrogate)",
            "ground_truth_metric": "Answer correctness (binary, dataset labels)",
            "ground_truth_description": "Ground-truth correctness via canonicalized equality checking against dataset solutions.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "Example (Llama-3.2-1B-Instruct, MATH500): LL AUROC = 55.41% ±0.54, AUPRC = 25.88% ±0.87, ACC* = 29.87% ±0.82, compared to TokUR TU AUROC = 80.64% ±0.29 on same dataset.",
            "proxy_performance": "AUROC ~55.4%, AUPRC ~25.9% on MATH500 (Llama-3.2-1B); for GSM8K LL AUROC = 69.01% ±0.03, AUPRC = 58.51% ±0.09 (Llama-3.2-1B) per Table 1.",
            "ground_truth_performance": "Model Pass@1 accuracies used as ground-truth baselines: GSM8K 44.43%, MATH500 25.60% (Llama-3.2-1B-Instruct).",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_characterization": "In-distribution for benchmark problems; known to be a weak proxy in multi-step reasoning because high likelihood can correspond to plausible but wrong reasoning.",
            "gap_varies_with_novelty": true,
            "gap_variation_details": "LL's discriminative power degrades on harder questions and multi-step reasoning tasks; TokUR significantly outperforms LL especially on difficult benchmarks (quantified above).",
            "gap_reduction_method": "Not applicable within LL itself; paper compares LL ranking versus uncertainty-based reranking (TokUR).",
            "gap_reduction_effectiveness": "Replacing LL with TokUR for candidate ranking increases accuracy (e.g., GSM8K Maj@N N=16: LL 47.10% → TokUR (EU) 50.38%, improvement ~+3.28 pp).",
            "validation_cost_comparison": "LL is cheap (single pass), TokUR requires repeated perturbations and is more costly at inference time.",
            "temporal_validation": null,
            "domain_maturity": "Established baseline and widely used proxy in NLP.",
            "uncertainty_quantification": false,
            "uncertainty_calibration": "LL scores are not explicitly calibrated to ground-truth correctness in this paper; evaluated by AUROC/AUPRC only.",
            "multiple_proxies": false,
            "proxy_correlation": null,
            "validation_cascade": "Direct ranking of candidates by LL then evaluation against dataset labels.",
            "publication_bias_discussion": false,
            "limitations_challenges": "LL can be overconfident for wrong but fluent outputs, does not decompose epistemic vs aleatoric uncertainty, poorer discriminative power on long-form multi-step reasoning.",
            "domain_specific_factors": "Length normalization can affect LL ranking; beam search / length bias factors influence LL scores.",
            "uuid": "e2228.1"
        },
        {
            "name_short": "PE",
            "name_full": "Predictive Entropy",
            "brief_description": "Entropy of the predicted token distribution (mean token entropy aggregated) used as an uncertainty proxy.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Predictive Entropy (PE)",
            "system_description": "Compute entropy H[p(y_t | y_&lt;t, x)] for tokens and average across tokens (or sum) to yield a predictive uncertainty score (Total Uncertainty analog).",
            "domain": "LLM reasoning / uncertainty estimation",
            "proxy_metric_name": "Predictive Entropy (mean token entropy)",
            "proxy_metric_description": "Average per-token Shannon entropy of the model predictive distributions; measures total uncertainty (mix of aleatoric+epistemic under single model).",
            "proxy_metric_type": "data-driven ML prediction (logit-derived entropy)",
            "ground_truth_metric": "Answer correctness (dataset labels)",
            "ground_truth_description": "Canonicalized equality check between generated answer and ground-truth.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "Example (Llama-3.2-1B-Instruct, MATH500): PE AUROC = 57.08% ±0.89, AUPRC = 26.88% ±1.05, ACC* = 31.33% ±0.82 (Table 1).",
            "proxy_performance": "AUROC ~57.1% on MATH500 (Llama-3.2-1B).",
            "ground_truth_performance": "Pass@1 baselines as above (MATH500 25.60%).",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_characterization": "In-distribution; entropy may fail to separate model uncertainty from intrinsic output multiplicity.",
            "gap_varies_with_novelty": true,
            "gap_variation_details": "PE discrimination degrades on harder problems; AUROC above chance but below TokUR.",
            "gap_reduction_method": "Not in PE itself; TokUR decomposes entropy into AU/EU and uses perturbations to better capture epistemic part.",
            "gap_reduction_effectiveness": "TokUR EU/TU significantly outperform PE (e.g., TokUR TU 80.64% AUROC vs PE 57.08% on MATH500).",
            "validation_cost_comparison": "PE is inexpensive (single forward pass entropies); TokUR is more costly due to perturbation sampling.",
            "temporal_validation": null,
            "domain_maturity": "Established method for predictive uncertainty.",
            "uncertainty_quantification": true,
            "uncertainty_calibration": "PE reports AUROC/AUPRC but not calibration curves versus absolute correctness rates.",
            "multiple_proxies": false,
            "proxy_correlation": null,
            "validation_cascade": "Entropy-based ranking evaluated vs dataset labels.",
            "publication_bias_discussion": false,
            "limitations_challenges": "Conflates aleatoric and epistemic uncertainty; token-/sequence-level marginalizations are intractable without approximations.",
            "domain_specific_factors": "Decoding temperature increases AU markedly (aleatoric), EU less affected per ablation studies.",
            "uuid": "e2228.2"
        },
        {
            "name_short": "Self-Certainty",
            "name_full": "Self-Certainty (KL-from-uniform confidence)",
            "brief_description": "A ranking/confidence proxy that measures the KL divergence between the predicted token distribution and a uniform distribution to quantify confidence.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Self-Certainty",
            "system_description": "At each decoding step compute KL(p(.|.) || Uniform) (or equivalent measure) to produce a confidence score; aggregate across tokens to rank candidate responses.",
            "domain": "LLM candidate selection / de-hallucination",
            "proxy_metric_name": "KL-divergence from uniform (Self-Certainty)",
            "proxy_metric_description": "Distance between model token distribution and uniform distribution per token; higher KL means more confident (peaked) distribution.",
            "proxy_metric_type": "data-driven ML prediction (logit-based confidence metric)",
            "ground_truth_metric": "Answer correctness (dataset labels)",
            "ground_truth_description": "Canonical answer equality checks used to label true vs false.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "Example (Llama-3.2-1B-Instruct, MATH500): Self-Certainty AUROC = 71.17% ±0.30, AUPRC = 48.37% ±0.50, ACC* = 38.13% ±0.61 (Table 1).",
            "proxy_performance": "AUROC ~71.2% on MATH500 (Llama-3.2-1B); on GSM8K Self-Certainty AUROC = 73.41% ±0.00 per Table 1 for that model.",
            "ground_truth_performance": "See model pass@1 accuracies used as ground truth baselines.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_characterization": "In-distribution; designed as internal-signal-only method to prioritize high-confidence outputs.",
            "gap_varies_with_novelty": true,
            "gap_variation_details": "Self-Certainty benefits from length normalization in some datasets (Table E.4.3) — its AUROC improves markedly with normalization on MATH500 (e.g., from ~23.76 to 71.17 in ablation), meaning performance is sensitive to aggregation choices and difficulty.",
            "gap_reduction_method": "Aggregation and normalization of per-token KL; not a Bayesian uncertainty method but a heuristic confidence measure.",
            "gap_reduction_effectiveness": "Per Table 1, Self-Certainty is competitive and sometimes second-best, but TokUR still outperforms it on many metrics.",
            "validation_cost_comparison": "Efficient (uses internal logits), cheaper than TokUR which requires sampling perturbations.",
            "temporal_validation": null,
            "domain_maturity": "Recently proposed heuristic (2025), gaining use as a strong internal-signal baseline.",
            "uncertainty_quantification": true,
            "uncertainty_calibration": "Empirical AUROC/AUPRC reported; no explicit calibration to absolute error rates beyond those metrics.",
            "multiple_proxies": false,
            "proxy_correlation": null,
            "validation_cascade": "Used for candidate selection and evaluated against dataset labels.",
            "publication_bias_discussion": false,
            "limitations_challenges": "Sensitive to sequence length and normalization; may overestimate confidence on long but incorrect chains unless normalized.",
            "domain_specific_factors": "Aggregation/normalization choices and decoding temperature affect performance.",
            "uuid": "e2228.3"
        },
        {
            "name_short": "DeepConf",
            "name_full": "Deep Think With Confidence (DeepConf)",
            "brief_description": "An uncertainty-driven scoring method that aggregates top-K log-probabilities at each decoding step to derive a confidence score for sequences.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "DeepConf",
            "system_description": "Aggregates the sum (or mean) of the top-K token log-probabilities across decoding steps to produce a confidence ranking for candidate generations.",
            "domain": "LLM confidence estimation / candidate ranking",
            "proxy_metric_name": "Aggregated top-K log-probability score (DeepConf)",
            "proxy_metric_description": "At each token step sum top-K token log-probabilities; aggregate across sequence to get a confidence score that emphasizes peakedness of top mass.",
            "proxy_metric_type": "data-driven ML prediction (logit-derived surrogate)",
            "ground_truth_metric": "Answer correctness (dataset labels)",
            "ground_truth_description": "Canonicalized answer comparisons.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "Example (Llama-3.2-1B-Instruct, MATH500): DeepConf AUROC = 71.77% ±0.12, AUPRC = 46.00% ±0.42, ACC* = 39.87% ±0.46 (Table 1).",
            "proxy_performance": "AUROC ~71.8% on MATH500 (Llama-3.2-1B).",
            "ground_truth_performance": "Model pass@1 ground-truth accuracies as above.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_characterization": "In-distribution evaluation; heuristic tailored to logit shape.",
            "gap_varies_with_novelty": true,
            "gap_variation_details": "Performance declines on more difficult reasoning problems; benefits from length normalization in some settings.",
            "gap_reduction_method": "Not internal to DeepConf; paper compares it to TokUR which reduces gap further using epistemic uncertainty.",
            "gap_reduction_effectiveness": "TokUR improves over DeepConf by several percentage points in AUROC/AUPRC across datasets (see Table 1).",
            "validation_cost_comparison": "Single-pass logits aggregation — cheaper than TokUR sampling.",
            "temporal_validation": null,
            "domain_maturity": "Recent proposal (2025) for uncertainty-driven test-time scaling.",
            "uncertainty_quantification": true,
            "uncertainty_calibration": "Evaluated with AUROC/AUPRC; no direct calibration vs absolute error rates reported.",
            "multiple_proxies": false,
            "proxy_correlation": null,
            "validation_cascade": "Used to rank candidates for Maj@N or WBoN aggregation and evaluated vs ground-truth labels.",
            "publication_bias_discussion": false,
            "limitations_challenges": "Relies on top-K logits which may not capture epistemic uncertainty; sensitive to K and normalization choices.",
            "domain_specific_factors": "Logit sharpness and decoding temperature influence scores.",
            "uuid": "e2228.4"
        },
        {
            "name_short": "SE",
            "name_full": "Semantic Entropy",
            "brief_description": "An external-signal-based proxy that prompts the LLM (or uses an external NLI/embedding model) to verify the same response multiple times and computes semantic entropy across verifications to detect hallucinations.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Semantic Entropy (SE)",
            "system_description": "Repeatedly ask a verifier model (LLM or external NLI/embedding) to judge or rephrase a response; compute the semantic-entropy of these repeated outputs as a proxy for confidence/hallucination detection.",
            "domain": "Hallucination detection / LLM output verification",
            "proxy_metric_name": "Semantic Entropy (over verifier outputs)",
            "proxy_metric_description": "Entropy measured over multiple semantic verifications/paraphrases of the same response (requires an external verifier embedding or NLI model to compare semantic content).",
            "proxy_metric_type": "hybrid (LLM-derived + external semantic similarity model)",
            "ground_truth_metric": "Answer correctness (dataset labels)",
            "ground_truth_description": "Canonicalized equality checks vs dataset answers.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "Example (Llama-3.2-1B-Instruct, MATH500): SE AUROC = 47.29% ±3.81, AUPRC = 25.71% ±2.33, ACC* = 24.13% ±4.42 (Table 1) — lower performance in this reproduction setting relative to internal-only methods, but SE requires external models.",
            "proxy_performance": "AUROC ~47.3% on MATH500 in the paper's adapted setup (note SE uses external verifier and may have implementation variability).",
            "ground_truth_performance": "Models' pass@1 accuracies used as ground-truth baselines.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_characterization": "In-distribution verification, but employs external resources that may shift distributional assumptions.",
            "gap_varies_with_novelty": true,
            "gap_variation_details": "Performance depends on quality and calibration of external verifier; can be unstable and dataset-dependent.",
            "gap_reduction_method": "Using stronger or task-tuned external verifiers could reduce the gap; not explored extensively in this paper beyond adaptation for comparison.",
            "gap_reduction_effectiveness": "Not quantified beyond the reported comparative AUROC/AUPRC numbers; SE did not outperform TokUR in reported experiments.",
            "validation_cost_comparison": "Higher cost because it requires multiple verification calls and/or external models; more expensive than internal-score proxies.",
            "temporal_validation": null,
            "domain_maturity": "Emerging; semantic-verification approaches are an active area for hallucination detection.",
            "uncertainty_quantification": true,
            "uncertainty_calibration": "Empirically measured via AUROC/AUPRC but not calibrated to absolute error rates; relies on external verifier calibration.",
            "multiple_proxies": false,
            "proxy_correlation": null,
            "validation_cascade": "External verifier loop → semantic entropy → compare with ground-truth labels.",
            "publication_bias_discussion": false,
            "limitations_challenges": "Requires external models (fairness/implementation differences), sensitive to verifier quality, and in this paper's reproduction yielded weaker empirical discrimination.",
            "domain_specific_factors": "Quality of external NLI/embedding models and prompt templates for verification strongly affect performance.",
            "uuid": "e2228.5"
        },
        {
            "name_short": "P(True)",
            "name_full": "P(True) self-query",
            "brief_description": "A proxy that directly queries the model to predict the probability its own output is 'True' (or 'False') and uses that probability as a confidence measure.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "P(True) scoring",
            "system_description": "After generating an answer, ask the model to output the probability (or token probabilities) of the token 'True' vs 'False' for the correctness of that output, normalize and use as a score.",
            "domain": "LLM self-evaluation / calibration",
            "proxy_metric_name": "P(True) (model-predicted probability of 'True')",
            "proxy_metric_description": "Probability mass assigned by the model to an explicit 'True' token (or affirmative) in a verification prompt; normalized between True/False tokens to produce a confidence score.",
            "proxy_metric_type": "verbalized ML self-evaluation (LLM-predicted probability)",
            "ground_truth_metric": "Answer correctness (dataset labels)",
            "ground_truth_description": "Canonicalized equality checks vs dataset answers used as ground truth.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "Example (Llama-3.2-1B-Instruct, MATH500): P(True) AUROC = 54.38% ±1.20, AUPRC = 26.39% ±1.26, ACC* = 27.60% ±1.18 (Table 1).",
            "proxy_performance": "AUROC ~54.4% on MATH500 (Llama-3.2-1B-Instruct).",
            "ground_truth_performance": "Pass@1 accuracies as ground-truth baselines.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_characterization": "In-distribution self-assessment; model tends to be overconfident or miscalibrated on multi-step reasoning.",
            "gap_varies_with_novelty": true,
            "gap_variation_details": "P(True) shows limited discriminative power on hard reasoning tasks; performance depends on the model's ability to self-verify chain-of-thought.",
            "gap_reduction_method": "Not examined in-depth here; TokUR provides alternative uncertainty signals that outperform P(True) in reported experiments.",
            "gap_reduction_effectiveness": "TokUR (EU/TU) substantially outperforms P(True) in AUROC/AUPRC on MATH500 and GSM8K.",
            "validation_cost_comparison": "Requires extra verification prompt calls (cheaper than external-verifier SE but more expensive than pure-logit proxies).",
            "temporal_validation": null,
            "domain_maturity": "Explored previously for self-evaluation; efficacy on long-form reasoning remains limited.",
            "uncertainty_quantification": true,
            "uncertainty_calibration": "Empirical AUROC/AUPRC provided but explicit calibration vs error rates not given.",
            "multiple_proxies": false,
            "proxy_correlation": null,
            "validation_cascade": "Generate answer → self-query P(True) → compare with label.",
            "publication_bias_discussion": false,
            "limitations_challenges": "Relies on model's meta-cognitive ability to report correctness; prone to overconfidence and failure on multi-step errors.",
            "domain_specific_factors": "Quality of verification prompt and model's training on meta-evaluation behaviors.",
            "uuid": "e2228.6"
        },
        {
            "name_short": "LLM-Check",
            "name_full": "LLM-Check (reproduction)",
            "brief_description": "A method that evaluates model generations by asking the model (or a verifier LLM) to check answers multiple times; used as an internal-signal baseline in the paper's experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "LLM-Check",
            "system_description": "Reproduced official LLM-Check implementation for repeated verification of outputs to produce confidence signals; uses internal LLM checks rather than external NLI.",
            "domain": "LLM verification / hallucination detection",
            "proxy_metric_name": "Repeated LLM verification score (LLM-Check)",
            "proxy_metric_description": "Score derived from multiple self-checks/verifications made by the LLM about the generated response; aggregated into confidence.",
            "proxy_metric_type": "verbalized/self-verification hybrid",
            "ground_truth_metric": "Answer correctness",
            "ground_truth_description": "Dataset label comparison (canonicalization) to determine correctness.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "Example (Llama-3.2-1B-Instruct, MATH500): LLM-Check AUROC = 56.41% ±0.96, AUPRC = 27.01% ±1.22, ACC* = 31.33% ±1.29 (Table 1).",
            "proxy_performance": "AUROC ~56.4% on MATH500 in the reproduced implementation.",
            "ground_truth_performance": "Model pass@1 accuracies used as labels.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_characterization": "In-distribution; internal verification can fail for complex multi-step errors.",
            "gap_varies_with_novelty": true,
            "gap_variation_details": "Performance varies by dataset and model size; tends to underperform TokUR.",
            "gap_reduction_method": "Not implemented; compared as baseline.",
            "gap_reduction_effectiveness": "Less effective than TokUR in the experiments reported.",
            "validation_cost_comparison": "Multiple verification calls increase compute vs single-pass logit proxies but are cheaper than using separate heavyweight external verifiers.",
            "temporal_validation": null,
            "domain_maturity": "Recent evaluation approach.",
            "uncertainty_quantification": true,
            "uncertainty_calibration": "AUROC/AUPRC reported; no further calibration analysis.",
            "multiple_proxies": false,
            "proxy_correlation": null,
            "validation_cascade": "Repeated self-checking → scoring → compare to ground-truth.",
            "publication_bias_discussion": false,
            "limitations_challenges": "Dependence on LLM's self-check quality; reproduction variability across implementations and prompts.",
            "domain_specific_factors": "Prompt design for verification and the model's exposure to 'self-check' style fine-tuning.",
            "uuid": "e2228.7"
        },
        {
            "name_short": "INSIDE",
            "name_full": "INSIDE (Internal States for hallucInation DEtection)",
            "brief_description": "Method that queries the LLM to verify the same response multiple times and computes semantic-like entropy using the model's internal states or verification outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "INSIDE",
            "system_description": "Tailored to estimate query-level uncertainty by repeated verification attempts using the LLM and computing an entropy-like measure across those attempts to detect hallucination.",
            "domain": "Hallucination detection / uncertainty estimation",
            "proxy_metric_name": "INSIDE entropy over repeated verifications",
            "proxy_metric_description": "Entropy computed across multiple verifier outputs produced by internal LLM prompts; used as a proxy for response reliability.",
            "proxy_metric_type": "verbalized/self-verification hybrid",
            "ground_truth_metric": "Answer correctness",
            "ground_truth_description": "Canonical equality checks versus dataset ground-truth.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "Example (Llama-3.2-1B-Instruct, MATH500): INSIDE AUROC = 55.71% ±4.69, AUPRC = 28.82% ±4.05, ACC* = 29.20% ±4.33 (Table 1).",
            "proxy_performance": "AUROC ~55.7% on MATH500 (reproduced setup).",
            "ground_truth_performance": "Dataset correctness labels used for evaluation.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_characterization": "In-distribution verification; leverages internal LLM signals.",
            "gap_varies_with_novelty": true,
            "gap_variation_details": "Performance sensitive to dataset and prompt design; generally outperformed by TokUR.",
            "gap_reduction_method": "Not applied; used for comparison.",
            "gap_reduction_effectiveness": "Lower than TokUR in reported AUROC/AUPRC numbers.",
            "validation_cost_comparison": "Requires multiple verification prompts — moderate compute overhead.",
            "temporal_validation": null,
            "domain_maturity": "Recently proposed; demonstrated for hallucination detection.",
            "uncertainty_quantification": true,
            "uncertainty_calibration": "Empirical AUROC/AUPRC reported; no formal calibration analysis provided.",
            "multiple_proxies": false,
            "proxy_correlation": null,
            "validation_cascade": "Repeated verification by LLM → entropy calculation → evaluation vs labels.",
            "publication_bias_discussion": false,
            "limitations_challenges": "High variance and sensitivity to prompt and sampling; requires many repeats to reduce noise.",
            "domain_specific_factors": "Model-specific self-evaluation capability and prompt quality.",
            "uuid": "e2228.8"
        },
        {
            "name_short": "SAR",
            "name_full": "Shifting Attention to Relevance (SAR) (adapted)",
            "brief_description": "An external-signal method that computes sentence-level relevance/uncertainty using an external semantic similarity model; adapted in this paper for response-level uncertainty comparisons.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "SAR (adapted)",
            "system_description": "Compute sentence-level relevance/uncertainty scores using an external semantic similarity model across multiple verification attempts and aggregate into a response-level proxy.",
            "domain": "LLM hallucination detection / uncertainty",
            "proxy_metric_name": "Sentence-level SAR score (semantic similarity-based relevance)",
            "proxy_metric_description": "Uses an external semantic similarity model to assess relevance/consistency across multiple verification attempts; aggregated into an uncertainty/reliability score.",
            "proxy_metric_type": "hybrid (external semantic similarity model + LLM outputs)",
            "ground_truth_metric": "Answer correctness",
            "ground_truth_description": "Canonicalized equality checks.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "Example (Llama-3.2-1B-Instruct, MATH500): SAR AUROC = 44.57% ±2.04, AUPRC = 24.03% ±2.53, ACC* = 21.07% ±1.62 (Table 1) in the adapted setup.",
            "proxy_performance": "AUROC ~44.6% on MATH500 in this paper's adaptation.",
            "ground_truth_performance": "Dataset labels used as ground-truth.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_characterization": "In-distribution; relies on external semantic model quality.",
            "gap_varies_with_novelty": true,
            "gap_variation_details": "External verifier dependency causes variable performance across datasets; SAR underperformed compared to internal proxies and TokUR in reported experiments.",
            "gap_reduction_method": "Improving external similarity models or ensembling verifiers could reduce gap (not explored here).",
            "gap_reduction_effectiveness": "Not shown to outperform TokUR in the experiments.",
            "validation_cost_comparison": "Higher due to external semantic model calls and multiple verifications.",
            "temporal_validation": null,
            "domain_maturity": "Experimental/adapted approach.",
            "uncertainty_quantification": true,
            "uncertainty_calibration": "AUROC/AUPRC reported but no formal calibration analysis.",
            "multiple_proxies": false,
            "proxy_correlation": null,
            "validation_cascade": "External semantic checks → SAR scoring → evaluation vs labels.",
            "publication_bias_discussion": false,
            "limitations_challenges": "Requires external models; fairness/implementation differences; observed weaker empirical performance in this reproduction/adaptation.",
            "uuid": "e2228.9"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning",
            "rating": 2
        },
        {
            "paper_title": "Bayesian low-rank adaptation for large language models",
            "rating": 2
        },
        {
            "paper_title": "Blob: Bayesian low-rank adaptation by backpropagation for large language models",
            "rating": 2
        },
        {
            "paper_title": "Detecting hallucinations in large language models using semantic entropy",
            "rating": 2
        },
        {
            "paper_title": "Scalable best-of-n selection for large language models via self-certainty",
            "rating": 2
        },
        {
            "paper_title": "Deep think with confidence",
            "rating": 2
        }
    ],
    "cost": 0.02753875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>TO KUR: TOKEN-LEVEL UNCERTAINTY ESTIMATION FOR LARGE LANGUAGE MODEL REASONING
25 Sep 2025</p>
<p>Tunyu Zhang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#116;&#121;&#46;&#122;&#104;&#97;&#110;&#103;&#64;&#114;&#117;&#116;&#103;&#101;&#114;&#115;&#46;&#101;&#100;&#117;">&#116;&#121;&#46;&#122;&#104;&#97;&#110;&#103;&#64;&#114;&#117;&#116;&#103;&#101;&#114;&#115;&#46;&#101;&#100;&#117;</a> 
Haizhou Shi 
Yibin Wang 
Hengyi Wang 
Xiaoxiao He 
Zhuowei Li 
Haoxian Chen 
Ligong Han 
Kai Xu 
Huan Zhang 
Dimitris Metaxas 
Hao Wang 
Rutgers University </p>
<p>Red Hat AI Innovation</p>
<p>Hao Wang</p>
<p>TO KUR: TOKEN-LEVEL UNCERTAINTY ESTIMATION FOR LARGE LANGUAGE MODEL REASONING
25 Sep 20252D6A2428924A557B420E15EE58CE57FCarXiv:2505.11737v3[cs.LG]
While Large Language Models (LLMs) have demonstrated impressive capabilities, their output quality remains inconsistent across various application scenarios, making it difficult to identify trustworthy responses, especially in complex tasks requiring multi-step reasoning.In this paper, we propose a Token-level Uncertainty estimation framework for Reasoning (TokUR) that enables LLMs to self-assess and self-improve their responses in mathematical reasoning.Specifically, we introduce low-rank random weight perturbation during LLM decoding to generate predictive distributions for token-level uncertainty estimation, and we aggregate these uncertainty quantities to capture the semantic uncertainty of generated responses.Experiments on mathematical reasoning datasets of varying difficulty demonstrate that TokUR exhibits a strong correlation with answer correctness and model robustness, and the uncertainty signals produced by TokUR can be leveraged to enhance the model's reasoning performance at test time.These results highlight the effectiveness of TokUR as a principled and scalable approach for improving the reliability and interpretability of LLMs in challenging reasoning tasks.</p>
<p>INTRODUCTION</p>
<p>Large Language Models (LLMs) have demonstrated remarkable capabilities in various reasoning tasks (Wei et al., 2022a;Wang et al., 2022;Chung et al., 2024;Guo et al., 2025), yet they often struggle to reliably assess the quality of their own responses (Xiong et al., 2023;Tian et al., 2023;Kapoor et al., 2024;Liu et al., 2024;Zhang &amp; Zhang, 2025;Da et al., 2025;Liu et al., 2025).This limitation becomes particularly evident in complex reasoning scenarios where models may generate seemingly convincing but incorrect solutions without indicating uncertainty.</p>
<p>Beyond the dominant body of uncertainty estimation methods that largely focus on short-form question answering (Zhang et al., 2023;Yadkori et al., 2024) and classification tasks (Yang et al., 2023;Wang et al., 2024;Shi et al., 2024), two main approaches have been explored for the more challenging setting of sequence uncertainty estimation: (i) Query-level methods (Gal et al., 2016;Osband et al., 2023;Hou et al., 2023), despite their solid theoretical foundation, estimate uncertainties U(y|x) with respect to input prompts x alone, without evaluating the quality of specific generated responses y conditioned on those inputs (see Sec. 2.1).Besides, these methods require marginalization over the entire output space y; this becomes intractable as sequence length grows.(ii) Response-level methods (Murray &amp; Chiang, 2018;Malinin &amp; Gales, 2021;Kadavath et al., 2022), typically variants of log-probabilities, have shown empirical success but lack strong theoretical grounding (Kuhn et al., 2023).As a result, the limitations of the aforementioned methods in capturing response-specific uncertainty hinder the deployment of LLMs in high-stakes reasoning tasks that demand reliable self-assessment.</p>
<p>To address this challenge, we propose a principled framework, dubbed Token-level Uncertainty estimation for Reasoning (TokUR), for estimating the uncertainty of generated sequences by aggregating token-level uncertainties based on random low-rank weight perturbation.TokUR introduces carefully calibrated perturbations to the weights of attention layers, creating an ensemble of model variants that enables principled uncertainty estimation without requiring costly retraining or extensive parameter updates.Building on this, we decompose the total uncertainty of each generated token into aleatoric uncertainty (inherent randomness in the data) and epistemic uncertainty (model uncertainty about its parameters), providing a theoretically grounded assessment of confidence across the generation process.We then aggregate these token-level uncertainties to evaluate entire reasoning responses, demonstrating both theoretical consistency with established uncertainty principles and practical utility in downstream applications.</p>
<p>Empirically, TokUR enhances LLM reasoning in three key aspects: (i) token-level epistemic uncertainty effectively identifies incorrect reasoning paths, outperforming baselines across three mathematical reasoning benchmarks, (ii) TokUR excels at selecting high-quality solutions from multiple candidates, and (iii) it functions as an implicit reward to guide reasoning, improving accuracy when combined with off-the-shelf test-time-scaling algorithms (Puri et al., 2025).In summary, our contributions are:</p>
<p>• We introduce TokUR, a training-free token-level uncertainty estimation approach for LLM reasoning through low-rank weight perturbation, providing a principled decomposition of uncertainties with proven theoretical properties.• We demonstrate that epistemic uncertainty can serve as a good metric to measure the quality of generated reasoning paths, consistently outperforming conventional confidence metrics across diverse mathematical reasoning tasks.• We demonstrate practical applications1 of our uncertainty estimation framework: it improves reasoning performance through incorrect path detection, high-quality solution selection, and uncertainty-guided generation.</p>
<p>PRELIMINARIES</p>
<p>In this section, we first introduce the notation used in the remaining sections, and then review the key concepts of uncertainties (Sec.2.1) and existing Bayesian LLMs for downstream adaptation (Sec.2.2).</p>
<p>Notation.In this paper, scalars are denoted by lowercase letters (x), vectors by lowercase bold-math letters (x), random vectors by lowercase boldface letters (x), and matrices by uppercase boldface letters (X).We use [m] = {1, 2, • • • , m} to denote the set of consecutive integer numbers from 1 to m.Following convention, we use p for probability, E for expectation, H for entropy, and I for mutual information.Specifically, H [y|x] denotes the conditional entropy between random variables y and x.We use H[p(y|x = x)] to denote the predictive entropy of the output variable conditioned on input x, with H[p(y|x)] as a shorthand notation when context is clear.</p>
<p>UNCERTAINTY ESTIMATION OF LONG-FORM GENERATION</p>
<p>Prediction with Bayesian Neural Networks.Bayesian Neural Networks (BNNs) (Neal, 2012;Hernández-Lobato &amp; Adams, 2015;Gal &amp; Ghahramani, 2016;Blundell et al., 2015;Wang &amp; Yeung, 2016;Wang et al., 2016;Lakshminarayanan et al., 2017;Wang &amp; Yeung, 2020) predict responses and estimate their uncertainties using the variational distribution q(θ|D) that approximates the true weight posterior p(θ|D).Given an input sequence x = (x 1 , • • • , x L ) ∈ X , the probability of the output sequence y = (y 1 , • • • , y T ) ∈ Y is defined as marginalization over the parameters and estimated by Bayesian Model Averaging (BMA) of size M : p(y|x) = p(y|x; θ) q(θ|D) dθ ≈ 1 M M m=1 p(y|x; θ (m) ), θ (m) ∼ q(θ|D).</p>
<p>(1)</p>
<p>Query-Level Uncertainty Estimation.Established techniques of uncertainty estimation (Gal et al., 2016) mainly quantify the uncertainty of input x (query-level uncertainty) by
H[p(y|x)] = E y∼p(y|x) [− log p(y|x)].(2)
In the context of BNNs (Eqn.1), the predictive distribution of y is the marginalized predictive distribution over the model parameters, and hence Eqn. 2 is defined as "total uncertainty" (Gal et al., 2016;Depeweg et al., 2017).</p>
<p>A model's uncertainty about a specific input cannot be solely attributed to the randomness of the approximate posterior q(θ|D), which is input-agnostic.For instance, when faced with a query "Name a city in the UK?" (Yadkori et al., 2024), even if an infinite amount of data is observed (eliminating the randomness of the model parameters), the uncertainty of this question remains high, as there are many correct candidate answers.Hence to distinguish different sources of uncertainty, total uncertainty is decomposed into epistemic uncertainty and aleatoric uncertainty (Gal et al., 2016):
H[p(y|x)] Total Uncertainty = E q(θ|D) [H[p(y|x; θ)]] Aleatoric Uncertainty + I(y; θ|x) Epistemic Uncertainty .(3)
Here, aleatoric uncertainty captures the intrinsic randomness in data and cannot be reduced even with more data observed.In contrast, epistemic uncertainty, defined as the mutual information I(y; θ|x) between y and θ, reflects the model's uncertainty about its own parameters, which can in principle be reduced by collecting more evidence.We use U(y|x) defined in Definition 2.1 to denote any of the three uncertainties.Definition 2.1 (Query-Level Uncertainty).Query-level uncertainty U(y|x) is the uncertainty of the predictive distribution p(y|x) given an input query x.Total Uncertainty (TU), Aleatoric Uncertainty (AU), and Epistemic Uncertainty (EU) in Eqn. 3 are all instances of query-level uncertainty.</p>
<p>Limitations of Query-Level Uncertainty.Using the chain rule for conditional entropy (Cover, 1999), the query-level uncertainty estimation can be decomposed token-by-token as
U(y|x) = T t=1 U(y t |y &lt;t , x).(4)
However, the uncertainty term U(y t |y &lt;t , x) in Eqn. 4 requires marginalization over the random variable y &lt;t , which is (i) computationally intractable, and (ii) only reflecting the quality of the input query.Hence, these query-level uncertainties are not proper indicators for evaluating a concrete output response y.</p>
<p>BAYESIAN LARGE LANGUAGE MODELS</p>
<p>Bayesian Low-Rank Adaptation.For a pre-trained network layer with weight matrix W 0 , Low-Rank Adaptation (LoRA) (Hu et al., 2022) optimizes the parameters within a constrained low-rank subspace.Specifically, the weight update matrix is modeled by ∆W = BA, where ∆W ∈ R m×n , B ∈ R m×r , A ∈ R r×n , and r ≪ min(m, n).The output z ∈ R m×1 of forwarding the input vector h ∈ R n×1 is then
z = W 0 h + ∆W h = W 0 h + BAh.(5)
Leveraging LoRA's parameter efficiency, Bayesian LoRAs (Yang et al., 2023;Wang et al., 2024;Shi et al., 2024) aim to further integrate BNN's uncertainty estimation capabilities into LLMs without significant increasing memory complexity.The key idea is to model A and/or B as approximate distributions of the true weight posterior.The asymmetric Bayesianization, exemplified by BLoB (Wang et al., 2024) and TFB (Shi et al., 2024), models the elements of A with independent Gaussian distributions while keeping B deterministic.Specifically, we have
q(A|{M , Ω}) = ij q(A ij |M ij , Ω ij ) = ij N (A ij |M ij , Ω 2 ij ),(6)
where M and Ω share the same shape as A and denote the mean and standard deviation of the random variable A, respectively.To estimate this distribution, BLoB jointly trains the mean and covariance through the re-parameterization trick (Wang et al., 2024), while TFB uses a simple training-free maximal variance searching technique by fixing the approximate distribution to the family of low-rank isotropic Gaussian distributions (Shi et al., 2024).</p>
<p>Limited Scope of Existing Bayesian LLMs.Existing Bayesian LLMs have been primarily validated in downstream classification tasks of simple single-or multiple-choice problems, where uncertainty estimation is quantitatively assessed via the alignment of prediction confidence and accuracy (Yang et al., 2023;Balabanov &amp; Linander, 2024;Wang et al., 2023;2024;Shi et al., 2024).However, these methods have not yet demonstrated effective generalization to long-form generation tasks, i.e., LLM reasoning.Therefore, our TokUR, which estimates token-level uncertainties via weight perturbations, represents an initial step toward extending Bayesian LLMs to long-form generation, an area where uncertainty estimation remains largely unexplored and technically challenging.</p>
<p>3 TO KUR: TOKEN-LEVEL UNCERTAINTY ESTIMATION VIA LOW-RANK WEIGHT PERTURBATION Sec.3.1 introduces the key techniques of token-level uncertainty estimation.Sec.3.2 then details how token-level uncertainties can be aggregated for response-level uncertainty estimation, and describes the underlying theoretical foundation.Finally, Sec.3.3 presents our low-rank weight perturbation as posterior approximation.All proofs of propositions can be found in Appendix C.</p>
<p>TOKEN-LEVEL UNCERTAINTIES IN GENERAL</p>
<p>Given an approximate posterior q(θ|D), a fixed input query x ∈ X and a specific output response y = (y 1 , y 2 , . . ., y T ) ∈ Y sampled from the base policy p(y|x), we denote the predictive distribution of the next token y t produced by marginalization over weights as
p(y t |y &lt;t , x) ≜ E θ∼q(•|D) [p(y t |y &lt;t , x; θ)].(7)
Assumption 3.1 (Stepwise Posterior Sampling).We assume that the weights θ sampled from the approximate posterior q(•|D) are not shared across decoding steps.Formally, the probability of a sequence is factorized as
p(y|x) ≜ T t=1 p(y t |x, y &lt;t ) = T t=1 E θt∼q(•|D) [p(y t |x, y &lt;t , θ t )] ,(8)
instead of adopting the joint formulation
p(y|x) ≜ E θ∼q(•|D) [p(y|x, θ)].(9)
While both are valid probabilistic models, the joint formulation is incompatible with the autoregressive decoding mechanism of LLMs.Hence, we adopt the stepwise formulation in Assumption 3.1.To validate this assumption, we further conduct an ablation study comparing the stepwise formulation in Assumption 3.1 with the joint formulation, and report the results in Appendix E.4.4.</p>
<p>Given an input x and a partial output y &lt;t , for the time step t, we have the following three uncertainties:</p>
<p>• Epistemic Uncertainty (EU) is the difference between TU and AU:
EU(y t |y &lt;t , x) ≜ TU(y t |y &lt;t , x) − AU(y t |y &lt;t , x) = I(y t ; θ|y &lt;t , x), (12)
where V is the vocabulary and all the expectations are estimated with BMA.</p>
<p>TOKEN-LEVEL UNCERTAINTY FOR RESPONSE-LEVEL UNCERTAINTY ESTIMATION</p>
<p>Definition 3.1 (Response-Level Uncertainty).Given the token-level uncertainties U(y t |y &lt;t , x) defined in Eqn.10-12, we define response-level uncertainty as their cumulative sum across all tokens in the output sequence:
U(y|x) ≜ T t=1 U(y t |y &lt;t , x),(13)
where U can denote any of the considered uncertainty measures (TU, AU, or EU in Eqn.10-12).Proposition 3.1 (Response-Level Uncertainty as an Unbiased Estimator of Query-Level Uncertainty).Given an input query x, let y ∼ p(y|x) be a generated sample of length T .Then the response-level uncertainty U (Definition 3.1) is an unbiased estimator of the query-level uncertainty U (Definition 2.1), i.e.,
E y∼p(y|x) [ U(y|x)] = U(y|x). (14)
Proposition 3.2 (Token-Level and Response-Level Uncertainty).Given an input query x, let y ∼ p(y|x) be a generated sample of length T .Let U(y t |y &lt;t , x) denote the token-level uncertainty as defined in Eqn.10-12, with U(y|x) as the corresponding response-level uncertainty (Definition 3.1).Our token-level uncertainty is equivalent to the response-level uncertainty when T = 1:
U(y 1 |x) = U(y 1 |x).(15)
The two propositions above provide key connections between Eqn. 13 and existing uncertainty estimation theory (Malinin &amp; Gales, 2021;Ling et al., 2024).Proposition 3.1 shows that U(y|x) is an unbiased estimator of the true query-level uncertainty U(y|x), ensuring its statistical consistency with the ideal formulation.Proposition 3.2 confirms that when the sequence length T = 1, e.g., single-token prediction tasks such as multiple-choice QA (Yang et al., 2023;Wang et al., 2024), the estimator exactly recovers the token-level uncertainty, demonstrating structural consistency.These results support the validity and reliability of our approximation.</p>
<p>Advantages of Token-Level Uncertainty.Compared to Query-Level Uncertainty (Definition 2.1), token-and response-level uncertainties (i) avoid expensive marginalization over sequences (note the difference between y &lt;t in Eqn. 4 and y &lt;t in Eqn.14) while still (ii) capturing the expected uncertainty conditioned on the generated output response.Moreover, since U(y t |y &lt;t , x) depends on the quality of the prefix y &lt;t , (iii) the estimate retains rich semantic information, making it well-suited for entropy-based sequential decision-making (Kuhn et al., 2023;Ye et al., 2025) or hallucination detection (Farquhar et al., 2024;Kossen et al., 2024;Ye et al., 2025) in downstream tasks.</p>
<p>LOW-RANK WEIGHT PERTURBATION AS APPROXIMATION OF WEIGHT POSTERIOR</p>
<p>Suppose that we have an LLM policy p(y|x).To estimate the uncertainty of its output, we cast this model into a Bayesian framework by introducing weight perturbations.Due to the established advantages of efficiency, performance preservation of pre-perturbation model, and effectiveness of uncertainty estimation (Shi et al., 2024), we adopt a low-rank structure for the noise added to the model weights.Given a rank-r weight matrix W 0 ∈ R m×n of a neural network layer, we first perform compact Singular Value Decomposition (SVD) (Klema &amp; Laub, 1980):
W 0 = U diag(d)V ⊤ ,(16)
where d ≻ 0 ∈ R r×1 is the vector of singular values, and U ∈ R m×r and V ∈ R n×r both contain orthonormal columns, i.e., U ⊤ U = V ⊤ V = I r .To ensure computational efficiency, we introduce a low-rank noise matrix ϵ ∈ R n×r ′ whose rank r ′ ≪ r is significantly smaller than the rank of weight matrix, and whose entries are sampled i.i.d.from a Gaussian distribution of standard deviation of σ q , which we refer to as perturbation strength, i.e., ϵ ij ∼ N (0,
σ 2 q ), ∀i ∈ [n], j ∈ [r ′ ].
The perturbed weight matrix is then constructed as
W = W 0 + U ′ ϵ ⊤ ,(17)
where the matrix U ′ contains the top-r ′ columns of U .This perturbation transforms the deterministic W 0 to a variational low-rank isotropic Gaussian distribution W (Wang et al., 2024;Shi et al., 2024):
q(vec(W )|σ q ) = N (vec(W )|µ q , Σ q ),
where µ q = vec(W 0 ),
Σ q = σ 2 q I n ⊗ I r ′ 0 m−r ′ . (18)
Let θ denote the collection of all perturbed weight matrices across the model.By assuming the statistical independence among layers, the overall approximate posterior becomes
q(θ|σ q ) = i q(vec(W i )|σ q ). (19)
Utilizing the Approximate Weight Posterior q(θ|σ q ).Notably, while we leverage the variational posterior formulation of Eqn.19 to quantify uncertainty (detailed in Sec.3.1), we use only the mean weights W 0 for decoding of each step rather than BMA as in Eqn. 1.This approach allows for a controlled study of the effects of uncertainty estimation itself, separate from the effects of BNNs.For the complete algorithmic description and overview, please refer to Appendix B.</p>
<p>EXPERIMENTS</p>
<p>This section presents practical applications of our TokUR for LLM reasoning.For additional experimental results, please refer to Appendix E.</p>
<p>Datasets.We run our experiments on three mathematical reasoning benchmarks of varying difficulty levels: GSM8K (Cobbe et al., 2021) (grade-school arithmetic problems), MATH500 (Lightman et al., 2023) (challenging high school/college mathematics competition problems), and 5,000-example subset of DeepScaleR (Luo et al., 2025) (high-difficulty problems from diverse sources).For these complex math problems, LLMs often need to perform multi-step reasoning (Wei et al., 2022b;Yao et al., 2023;Zhou et al., 2023) to reach the final answer.These tasks inherently involve long-form generation, therefore well-suited for evaluation of uncertainty estimation methods.</p>
<p>Models.</p>
<p>We use two open-source LLMs in our experiments: Llama-3.2-1B-Instruct and Llama-3.1-8B-Instruct(Grattafiori et al., 2024).These models represent recent advances in downstream tasks and offer a good balance between performance and efficiency.Besides, their different model scales enable comparisons of uncertainty estimation across varying model sizes.</p>
<p>Implementation of our TokUR.We estimate token-level uncertainties by applying random perturbations as in Eqn. 17 to the query and key weight matrices (W Q , W K ) (Vaswani et al., 2017) in all the attention layers of LLMs (Hu et al., 2022;Yang et al., 2023;Wang et al., 2024;Shi et al., 2024).</p>
<p>For more details, please refer to Appendix D.1.</p>
<p>DO TO KUR'S UNCERTAINTIES ACCURATELY REFLECT RESPONSE QUALITY?</p>
<p>This section assesses if our TokUR's uncertainties reflect response quality in math reasoning tasks.</p>
<p>TO KUR'S UNCERTAINTIES AND QUESTION DIFFICULTY</p>
<p>Experimental Setting.To better understand the relationships among uncertainty estimates, question difficulty, and their ability to distinguish correct from incorrect responses, we sample a subset of math questions from math-orz (Hu et al., 2025).A question's difficulty level is determined by the number of failed attempts out of 10 when using the Qwen2.5-3B-Instructmodel.A difficulty level of 0 means the model solved the question every time, while a level of 10 indicates it failed on every attempt.We sample 500 questions per difficulty level, yielding a 5,500-question dataset.We then prompt Llama-3.2-1B-Instruct to solve each question with greedy decoding and apply TokUR to compute uncertainties for both correct and incorrect responses across difficulty levels.Fig. 1 summarizes the results.</p>
<p>Results.TokUR's uncertainty estimates remain positively correlated with question difficulty: for all three types of uncertainty (AU, TU, and EU), incorrect responses consistently exhibit higher uncertainty than correct ones across difficulty levels.In terms of discriminative power, AUROC values are consistently above random (0.5), confirming that TokUR provides useful signals for distinguishing correct from incorrect reasoning.Yet, AUROC tends to decrease as difficulty increases, especially in the mid-to-high range (levels 7-9), showing that uncertainty estimates become less reliable at separating outcomes on challenging tasks.Interestingly, the AUROC score shows a slight increase at the highest difficulty level (10).This is likely a result of the imbalanced data distribution (mostly incorrect), where the model consistently produces high uncertainty, which causes a misleading high metric value.</p>
<p>TO KUR FOR INCORRECT REASONING PATH DETECTION</p>
<p>Experimental Setting.The preliminary study demonstrates that our TokUR's uncertainty estimation can reflect the quality of generated responses, with lower uncertainty generally associated with better outputs.One important application of uncertainty estimation is hallucination detection in LLMs (Farquhar et al., 2024;Kossen et al., 2024;Ye et al., 2025).In this context, we treat uncertainty as a scoring function to identify hallucinated (incorrect) responses for long-form reasoning tasks.We adopt three metrics: Area Under the Receiver Operating Characteristic Curve (AUROC), Area Under the Precision-Recall Curve (AUPRC), and Top-50% ACC (ACC*) (Farquhar et al., 2024;Ye et al., 2025;Hanley &amp; McNeil, 1982;Boyd et al., 2013).AUROC and AUPRC measure the overall power of uncertainty scores in distinguishing correct from incorrect responses.In addition, we report Top-50% ACC, defined as the accuracy of the top 50% samples ranked by the corresponding score.This metric reflects the model's ability to prioritize higher-quality generations under a fixed budget.We repeat the experiments with three different random seeds to obtain the mean and standard deviation across runs.</p>
<p>Baselines.We systematically categorize our baselines into two distinct types: (i) those relying solely on the LLM's internal signals, including the most recent Self-Certainty (Kang et al., 2025), Deep Think With Confidence (DeepConf) (Fu et al., 2025) Results.As shown in Table 1, our proposed TokUR consistently outperforms baselines across AU-ROC, AUPRC, and ACC*.For example, on Llama-3.2-1B-Instruct,TokUR (TU) achieves an AUROC of 80.64% and an AUPRC of 56.79% on MATH500, clearly surpassing all baselines.</p>
<p>On the larger Llama-3.1-8B-Instruct, the improvements are also substantial: TokUR (EU) attains 82.86% AUROC and 81.35% AUPRC on MATH500, establishing new state-of-the-art performance.These results highlight an important insight: TokUR provides a reliable and scalable uncertainty estimation framework, achieving strong performance without relying on external signals.</p>
<p>CAN TO KUR'S UNCERTAINTIES IMPROVE GENERATION QUALITY?</p>
<p>In this section, we explore the direct application of TokUR to reasoning tasks to enhance generation quality.Following previous works (Fu et al., 2025), we apply TokUR to measure the confidence of reasoning traces generated from a question and aggregate them via voting to obtain a final solution.</p>
<p>In addition, we investigate the possibility of utilizing TokUR in an online manner to dynamically guide the generation process itself.Further details of online method are provided in E.3.</p>
<p>Baselines.We adopt Log-Likelihood (LL) as a baseline, given its widespread use as a proxy for generation quality (Manakul et al., 2023;Rafailov et al., 2023;Chen et al., 2024b).In addition, we compare against Self-Certainty ( Kang et al., 2025) and DeepConf (Fu et al., 2025), two recent uncertainty-driven approaches for test-time scaling.As our study emphasizes model self-awareness of the boundaries of its knowledge, we do not include baselines that rely on external reward models (Guan et al., 2025;Puri et al., 2025;Beeching et al.;Uesato et al., 2022;Lightman et al., 2023).</p>
<p>Response Aggregation with Uncertainties.We first rank all N candidate responses using one of the scoring methods (LL, Self-Certainty, DeepConf, or TokUR) and retain the top-P % candidates.</p>
<p>We then employ two common aggregation strategies: Weighted Best-of-N (WBoN) and Majority Voting (Maj@N) (Brown et al., 2024).WBoN performs weighted voting by assigning weights to the retained candidates according to their scores, whereas Maj@N simply selects the most frequent response among them, regardless of scoring.</p>
<p>Experimental Setting.We randomly sample 512 responses for each question in MATH500 and GSM8K with a decoding temperature of τ = 0.8.For each N , we first retain the top-10% of samples ranked by their scores.From this subset, the final prediction is determined using either Maj@N or WBoN.Each experiment is repeated 10 times (sample w/o replacement using offline records).</p>
<p>Results.As shown in Table 2, accuracy consistently improves with larger N across both GSM8K and MATH500.Our TokUR-based selection methods achieve clear gains over all baselines, particularly in the low-sample regime (N =16), where they deliver up to 3-4 points of improvement.Notably, TokUR (EU) attains the best overall performance on both datasets, with strong advantages in the challenging MATH500 benchmark.In addition, results for Maj@N and WBoN are similar, indicating that both aggregation strategies are similarly effective once the top candidates are identified.</p>
<p>RELATED WORK</p>
<p>Uncertainty Estimation of LLMs.Uncertainty estimation in LLMs is gaining traction for improving model calibration in data-scarce adaptation tasks and for reducing hallucinations in text generation (Liu et al., 2025;Vashurin et al., 2025).One prominent approach is Bayesian Adaptation, which combines Bayesian inference with low-rank adaptation (LoRA) (Hu et al., 2022) to approximate weight posterior distributions efficiently, avoiding the high computational cost of full Bayesian modeling (Yang et al., 2023;Wang et al., 2024;Shi et al., 2024).To estimate uncertainty in generation, two main lines of work have emerged.The first focuses on verbalized uncertainty, where models are prompted to express confidence in natural language (Lin et al., 2022;Kadavath et al., 2022;Tian et al., 2023;Kapoor et al., 2024).The second line includes logits-based methods, which estimate uncertainty directly from the model's output distributions (Van Der Poel et al., 2022;Ren et al., 2023;Duan et al., 2024;Darrin et al., 2023).In parallel, other approaches aim to refine these estimation strategies.For instance, (Malinin &amp; Gales, 2021) investigates techniques for estimating epistemic uncertainty in structured prediction tasks, while semantic entropy (Kuhn et al., 2023) captures uncertainty by leveraging invariance in meaning across paraphrases.More recently, (Zhang &amp; Zhang, 2025) introduces a method that leverages the reasoning capabilities of LLMs to enhance uncertainty quantification, using chain-of-thought prompting to better reflect model confidence in multi-step tasks.These works complement verbalized and logits-based methods by offering orthogonal perspectives on how uncertainty can be interpreted and measured.</p>
<p>Uncertainty for Improving LLM Generation.Uncertainty estimation for improving LLM generation, while not entirely novel, has been predominantly limited to approaches based on log-probability or its variants, Self-Certainty (Kang et al., 2025) estimates confidence via KL divergence from a uniform distribution, DeepConf (Fu et al., 2025) aggregates top-K log-probabilities as scores.Beam search (Lowerre, 1976;Sutskever et al., 2014;Freitag &amp; Al-Onaizan, 2017;Xie et al., 2023) selects higher-confidence sequences by retaining candidates with the largest cumulative log-probability.UAG (Yin et al., 2024) leverages abrupt log-probability changes to select appropriate demonstrations for in-context learning (Brown et al., 2020).UnCert-CoT (Zhu et al., 2025) alternates between greedy and Chain-of-Thought decoding based on log-probability scores.Our work differs fundamentally by estimating token-level uncertainties with rigorous theoretical foundations, representing a significant step toward extending Bayesian LLMs to long-form generation scenarios.</p>
<p>CONCLUSION</p>
<p>In this paper, we introduce a novel framework TokUR to quantify uncertainty in LLM reasoning generations.By incorporating low-rank random weight perturbation during the LLM decoding procedure, TokUR provides a new perspective for uncertainty estimation in auto-regressive longform generation with sound theoretical grounding.Through comprehensive empirical evaluation, we demonstrate that TokUR's uncertainty estimations effectively reflect the quality of generated reasoning paths and can thereby improve reasoning performance in LLMs.These contributions extend Bayesian uncertainty estimation to long-form reasoning, providing both theoretical foundations and practical tools for more reliable, self-aware LLMs.</p>
<p>Limitations.Our work is subject to several limitations.First, although compatible with efficient deployment frameworks such as vLLM (Kwon et al., 2023), repeated weight perturbation sampling during inference still poses efficiency challenges for real-time use.Second, our token-level uncertainty aggregation may miss higher-level semantic or logical inconsistencies across multiple tokens or reasoning steps, limiting its utility in complex generation tasks.Finally, the problem of high-variance estimation in our TokUR remains unresolved, constraining reliability in real-world scenarios.</p>
<p>APPENDIX</p>
<p>In Appendix A, we describe the role of large language models (LLMs) in our work.In Appendix B, we present the full algorithmic description of our method with low-rank weight perturbation.In Appendix C, we provide detailed proofs for all propositions presented in the main paper.In Appendix D, we provide our implementation details of the experiments, including:</p>
<p>• implementation of our TokUR (Appendix D.1),</p>
<p>• dataset details (Appendix D.2),</p>
<p>• prompt templates used in LLM reasoning (Appendix D.3),</p>
<p>• baseline details (Appendix D.4),</p>
<p>• and evaluation metrics (Appendix D.5).</p>
<p>Finally, in Appendix E, we present additional empirical results, including:</p>
<p>• preliminary study on the uncertainty distributions produced by TokUR (Appendix E.1),</p>
<p>• detailed numerical results of the test-time scaling (Appendix E.2),</p>
<p>• online test-time scaling of TokUR (Appendix E.3),</p>
<p>• an ablation study on different components of our token-level uncertainties (Appendix E.4),</p>
<p>• and a case study of our token-level uncertainties (Appendix E.5).</p>
<p>A LLM USAGE DISCLOSURE</p>
<p>We used large language models (LLMs) solely to assist with polishing the writing of this paper, including improving grammar, clarity, and readability.The LLMs did not contribute to research ideation, experimental design, analysis, or the generation of scientific content.All technical contributions, claims, and conclusions are the authors' own.</p>
<p>B ALGORITHM DETAILS</p>
<p>Algorithm 1 Low-Rank Weight Perturbation as Approximation of Weight Posterior.</p>
<p>1: Input</p>
<p>2:</p>
<p>The base model policy p(y|x);</p>
<p>3:</p>
<p>The set of weight matrices to be Bayesianized {W k 0 } N k=1 ;</p>
<p>4:</p>
<p>rank of noise matrix r ′ ; 5:</p>
<p>The perturbation strength σ q .6: for i = 1 to N do 7:
U , diag(d), V ⊤ ← SVD(W k 0 ). ▷ Eqn. 16 8:
U ′ ← the first r ′ columns of matrix U .9:</p>
<p>Sample noise matrix ϵ ∈ R n×r ′ : ϵ ij ∼ N (0, σ q ).10:</p>
<p>Perturb the weight matrix:
W k ← W k 0 + U ′ ϵ ⊤ .
▷ Eqn. 17 Get the weight posterior: q(vec(W k )|σ q ).▷ Eqn.18 12: end for 13: Output: The overall approximate posterior: q(θ|σ q ) ← k q(vec(W k )|σ q ) C PROOF OF PROPOSITIONS Lemma C.1 (Definition of Conditional Entropy (Cover, 1999)).Give (y, x) ∼ p(y, x), the conditional entropy H(y|x) is defined as
H(y|x) = x∈X p(x)H(y|x) = E x∼p(x) [H(y|x)]. (20)
Algorithm 2 Particle Filtering for Inference-Time Scaling (Puri et al., 2025) 1: Input 2:</p>
<p>The number of particles N ;
(i) t } N i=1 ∼ P t (j = i) = θ i . 11:
Update the set of particles as {x (j
(I) t ) 1:t } N i=1 .
12:
Transition {x i t+1 ∼ p M (•|c, x (i) 1:t )} N i=1 .
13:</p>
<p>t ← t + 1. 14: end while 15: Output: The set of particles in the end.</p>
<p>Lemma C.2 (Chain rule of Conditional Entropy (Cover, 1999)).Let X and Y be two random variables, then the conditional entropy of the joint distribution H(X, Y ) can be decomposed as: Cover, 1999) reveals the relationship between conditional entropy H(y|x) and the entropy derived from conditional probability distributions.Lemma C.2 lays the foundation for estimating the uncertainties of sequences.The two lemmas together give us the following proposition.Proposition C.1 (Decomposition of Query-Level Uncertainty, Eqn. 4).Suppose that we have an input sequence x and a model policy p(y|x).The sequence-level uncertainty U(y|x) can be decomposed token-by-token as:
H(X, Y ) = H(X) + H(Y |X) (21) Lemma C.1 (U(y|x) = T t=1 U(y t |y &lt;t , x), (22)
where U(y t |y &lt;t , x) is token-level uncertainty metric as defined in Eqn. 10 ~Eqn.12.</p>
<p>Proof.For Aleatoric Uncertainty (AU) and Total Uncertainty (TU) defined in Eqn. 10 and Eqn.11, both are expressed in terms of entropy.Therefore, the decomposition of sequence-level uncertainty can be directly derived using the chain rule stated in the Lemma C.2.</p>
<p>For Epistemic Uncertainty (EU), also called mutual information defined in Eqn. 12, we proceed with the following derivation:
H(p(y|x)) =H E p(θ|D) [p(y 1 |x; θ)] • ••• • E p(θ|D) <a href="23">p(y T |y &lt;T , x; θ)</a>
= U(y t |y &lt;t , x).</p>
<p>(30)</p>
<p>Therefore, the uncertainty of the sequence defined in Eqn.13:
E y∼p(y|x) [ U(y|x)] = E p(y|x) [ T t=1 U(y t |y &lt;t , x)] (31) = T t=1 E p(y|x) [U(y t |y &lt;t , x)] (32) = T t=1 E y&lt;t∼p(•|x) [U(y t |y &lt;t , x)] (33) = T t=1 U(y t |y &lt;t , x) (34) = U(y|x),(35)
where the final step follows from the chain rule of entropy (Proposition C.1).</p>
<p>Proposition 3.2 (Token-Level and Response-Level Uncertainty).Given an input query x, let y ∼ p(y|x) be a generated sample of length T .Let U(y t |y &lt;t , x) denote the token-level uncertainty as defined in Eqn.10-12, with U(y|x) as the corresponding response-level uncertainty (Definition 3.1).</p>
<p>Our token-level uncertainty is equivalent to the response-level uncertainty when T = 1:
U(y|x) = U(y 1 |x). (36)
Proof.When the sequence length T = 1, based on the definition of uncertainty of sequence in Eqn. 13, we have
U(y|x) = T t=1 U(y t |y &lt;t , x) = U (y 1 |x).
This proposition implies that the sequence uncertainty collapses to token-level uncertainty when the output sequence length is 1, reflecting the structural consistency of the estimator.</p>
<p>Proposition C.2 (Approximate Distribution of the Weight W Perturbed by Low-Rank Noise, Eqn.18).Given the weight matrix W 0 ∈ R m×n , the low-rank noise matrix ϵ ∈ R n×r ′ whose rank r ′ ≪ r is significantly smaller than the rank r of W 0 , and whose entries are sampled i.i.d.from a Gaussian distribution of standard deviation of σ q :
ϵ ij ∼ N (0, σ 2 q ), ∀i ∈ [n], j ∈ [r ′ ],
we have the perturbed weighted matrix W as defined in Eqn. 17 .The variational distribution q(vec(W )|σ q ) defined on the weight matrix W is q(vec(W )|σ q ) = N (vec(W )|µ q , Σ q ), where µ q = vec(W 0 ),
Σ q = σ 2 q I n ⊗ I r ′ 0 m−r ′ . (37)
Proof.We begin with compact SVD decomposition of the weight matrix W 0 as described in Eqn.16:
W 0 = U diag(d)V ⊤ ,(38)
where d ≻ 0 ∈ R r×1 is the vector of singular values, and U ∈ R m×r , V ∈ R n×r are orthogonal matrices.We denote the first r ′ columns of U as U ′ ∈ R m×r ′ to analyze the updated matrix U ′ ϵ ⊤ in Eqn. 17.</p>
<p>Since each entry in ϵ has zero mean, it is evident that the updated matrix also has zero mean.Consequently, we have µ q = vec(W 0 ) + 0 = vec(W 0 ).</p>
<p>Next, we focus on the proof of the variance Σ q .Gien U
′ = (u 1 , u 2 , • • • , u r ′ ) ∈ R m×r ′ , and ϵ = (ϵ 1 , ϵ 2 , • • • , ϵ r ′ ) ∈ R n×r ′
as defined above, we have the following properties:
U ′ U ′⊤ = r ′ i=1 u i u ⊤ i = I r ′ 0 m−r ′ , (39) vec(U ′ ϵ ⊤ ) = vec( r ′ i=1 u i ϵ ⊤ i ) = r ′ i=1 (ϵ i ⊗ u i ). (40)
We can now derive the covariance matrix as:
Σ q = Var[vec(W )] = Var[vec(W 0 + U ′ ϵ ⊤ )] = Var[vec(U ′ ϵ ⊤ )] (41) = Var[ r ′ i=1 ϵ i ⊗ u i ] = r ′ i=1 Var[ϵ i ⊗ u i ] (42) = r ′ i=1 E ϵi [(ϵ i ⊗ u i )(ϵ i ⊗ u i ) ⊤ ] − E ϵi [(ϵ i ⊗ u i )]E ϵi [(ϵ i ⊗ u i ) ⊤ ] (43) = r ′ i=1 E ϵi [ϵ i ϵ ⊤ i ] ⊗ (u i u ⊤ i ) − (E ϵi [ϵ i ]E ϵi [ϵ i ] ⊤ ) ⊗ (u i u ⊤ i ) (44) = r ′ i=1 σ 2 q I n ⊗ (u i u ⊤ i ) = σ 2 q I n ⊗ r ′ i=1 u i u ⊤ i = σ 2 q I n ⊗ I r ′ 0 m−r ′ . (45)
D IMPLEMENTATION DETAILS D.1 IMPLEMENTATION OF TO KUR'S TOKEN-LEVEL UNCERTAINTIES Unless otherwise specified, we set the rank of low-rank noise to r ′ = 8, the perturbation strength σ q = 0.1, and the number of samples per uncertainty estimation to M = 2.For the test-time scaling experiments in Sec.4.2, we apply length normalization to TokUR to mitigate the bias introduced by varying sequence lengths.In contrast, the effect of length normalization may differ in hallucination detection tasks.To investigate this, we conduct additional ablation studies in Appendix E.4.3, examining the impact of length normalization in that setting.To ensure practical applicability in real-world scenarios, we implement our method as a seamless integration with vLLM (Kwon et al., 2023).</p>
<p>D.2 DATASETS</p>
<p>Table 3 shows the statistics of datasets in our experiments.These datasets collectively span a wide range of difficulty levels, from moderate to highly challenging, covering both elementary-level numerical reasoning and advanced symbolic mathematical tasks.In addition, the problem domains are diverse, including: algebra, geometry, and number theory.Such a design ensures that our experiments are comprehensive and representative, facilitating a thorough assessment of the model's capability across varied reasoning scenarios.</p>
<p>Prompt Example</p>
<p>Solve the following math problem efficiently and clearly:</p>
<p>-For simple problems (2 steps or fewer): Provide a concise solution with minimal explanation.</p>
<p>-For complex problems (3 steps or more): Use this step-by-step format: Regardless of the approach, always conclude with: Therefore, the final answer is: answer .I hope it is correct.Where [answer] is just the final number or expression that solves the problem.</p>
<p>D.4 BASELINES</p>
<p>We compare our uncertainty estimation approach against several baseline methods:</p>
<p>• Log-Likelihood (LL) (Murray &amp; Chiang, 2018): Mean of token-wise log-probabilities of the output sequence, representing the model's overall confidence in its generation.• Predictive Entropy (PE) (Malinin &amp; Gales, 2021): Mean entropy of the predicted distribution of each token.• P(True) (Kadavath et al., 2022): Directly queries the model about the correctness of its own output and uses the predicted probability of the token "True", normalized by the sum of probabilities of token "True" and "False", as a confidence score.• Self-Certainty (Kang et al., 2025): Quantifies confidence using the KL divergence between the predicted token distribution and a uniform distribution over the vocabulary at each decoding step.• DeepConf (Fu et al., 2025): Computes confidence scores by aggregating the log-probabilities of the top-K candidate tokens at each decoding step.• The Degree Matrix (Lin et al., 2023): Utilizes the degree matrix of the graph Laplacian of the similarities matrix of responses.• LLM-Check (Sriramanan et al., 2024): We faithfully reproduced the official implementation for comparison.• INSIDE (Chen et al., 2024a): INSIDE is a method to estimate query-level uncertainty.We tailored INSIDE to our setting by asking the LLM to verify the same response multiple times and then calculating the semantic entropy across these verification attempts.</p>
<p>• Semantic Entropy (SE) (Kuhn et al., 2023): We adapted SE to our setting by prompting the LLM to verify the same response multiple times and computing the semantic entropy of these verification attempts.While this provides a signal of response quality, we note that SE requires an external NLI or embedding model, giving it an inherent advantage compared to our method.• SAR (Duan et al., 2024): We adapted SAR to our setting by computing sentence-level SAR scores over multiple verification attempts, following a similar procedure to SE.While this method provides a meaningful proxy for uncertainty, it requires an external semantic similarity model, which raises fairness concerns compared to our approach, which operates solely with the base LLM.</p>
<p>D.5 EVALUATION PARSING AND METRICS</p>
<p>Parsing.To automate the evaluation of outputs generated by large language models, we design specific prompts (see Appendix D.3) that constrain the model to follow a fixed structure and require it to place the final answer within a \box{}.Considering that in mathematical reasoning tasks, the same answer can be expressed in various forms, we standardize all answers into a canonical form before comparison (Beeching et al.).During the evaluation, we assess the correctness from two perspectives: numerical equality and symbolic equality, to label each generation as "True" or "False".</p>
<p>Metrics.To comprehensively assess model performance in binary classification tasks, we adopt the following metrics: Area Under the Receiver Operating Characteristic Curve (AUROC), Area Under the Precision-Recall Curve (AUPRC), and Top 50% Accuracy (Farquhar et al., 2024;Ye et al., 2025;Hanley &amp; McNeil, 1982;Boyd et al., 2013).</p>
<p>• AUROC measures the trade-off between true positive rate (TPR) and false positive rate (FPR) at various threshold settings.Formally, for a set of predictions with associated confidence scores, AUROC is computed as:
AUROC = 1 0 TPR(FPR −1 (x)) dx,(46)
where TPR and FPR are defined as: TPR = TP TP+FN , FPR = FP FP+TN .</p>
<p>• AUPRC evaluates the trade-off between precision and recall, which is particularly useful in imbalanced datasets.It is calculated as:
AUPRC = 1 0 Precision(Recall −1 (x)) dx,(47)
where precision and recall are defined as: Precision = TP TP+FP , Recall = TP TP+FN .</p>
<p>• Top 50% Accuracy evaluates the correctness of the top half predictions ranked by confidence.Let N be the total number of predictions and S be the set of indices corresponding to the top ⌈N/2⌉ predictions with highest confidence.The metric is defined as:
Top 50% Accuracy = 1 |S| i∈S δ( y i = y i ),(48)
where y i is the predicted label and y i is the ground-truth label.</p>
<p>E ADDITIONAL EXPERIMENTAL RESULTS</p>
<p>E.1 PRELIMINARY STUDY: DISTRIBUTION OF UNCERTAINTIES</p>
<p>We conduct a preliminary study to examine the relationship between responses' token-level uncertainties and their correctness.We generate responses on the GSM8K dataset using a greedy decoding strategy with Llama-3.2-1B-Instruct, and label each response as correct or incorrect based on an exact match with the ground-truth answer.Considering the class imbalance in the model responses, we construct a balanced subset for visualization.Specifically, we retain all incorrect responses and randomly sample an equal number of correct responses.We compute the TokUR (EU) and TokUR (AU) with our proposed token-level uncertainties in Eqn. 13, and plot the results in the Normalized EU-AU space (Fig. 3).We observe that both TokUR (EU) and TokUR (AU) show a better-than-chance separation between correct and incorrect outputs.Although some overlap exists, their distribution peaks differ significantly, indicating that our uncertainty estimates meaningfully correlate with generation quality.∈ 16, 32, 64, 128, 256, 512).</p>
<p>All experiments use Llama-3.2-1B-Instruct as the base model.For reference, the Pass@1 baseline accuracy (GSM8K: 44.43%; MATH500: 25.60%) is also shown as red dashed lines, highlighting the gains achieved through test-time scaling.Moreover, we provide an extended version of the results in Table 4, which builds on Table 2 to include additional test-time sample configurations.</p>
<p>E.3 TO KUR FOR TEST-TIME SCALING (ONLINE)</p>
<p>One popular approach to improving model performance uses a Process Reward Model (PRM) to score each intermediate step during multi-step generation (Guan et al., 2025;Puri et al., 2025;Beeching et al.;Uesato et al., 2022;Lightman et al., 2023), thereby guiding the model's reasoning path.In this section, we explore an alternative: guiding the generation process using uncertainty as an intrinsic reward, without relying on an explicit reward model.</p>
<p>Experimental Setting.Particle Filtering (PF) (Puri et al., 2025) is an inference-time scaling method for LLM reasoning (details in Appendix B).Building upon this algorithm, we use uncertainty as the score for each particle at each step to guide the model's generation process.We set the number of particles to N = 16 and the decoding temperature to τ = 0.8.We repeat the experiments with three different random seeds to obtain the mean and standard deviation across runs.</p>
<p>Results.Table 5 shows the results.Compared to LL, our TokUR, especially TokUR (EU), yields a slight performance gain.Given that guiding generation through stepwise scoring is inherently challenging, we consider the lack of a significant performance gain from uncertainty estimation to be acceptable.Nevertheless, we believe this experiment offers valuable insights that may inform the future design of process reward models.</p>
<p>E.4 ABLATION STUDY</p>
<p>This section presents an ablation study on our token-level uncertainty estimation method using low-rank perturbations.Appendix E.4.1 examines the effect of varying perturbation strength σ q , while Appendix E.4.2 analyzes the impact of different decoding temperatures.In Appendix E.4.3, we investigate the effect of length normalization in the context of hallucination detection tasks.Finally, in Appendix E.4.4,we assess the validity of Assumption 3.1 as it pertains to TokUR.</p>
<p>E.4.1 THE EFFECT OF PERTURBATION STRENGTH σ q ON UNCERTAINTY ESTIMATION</p>
<p>To investigate the impact of perturbation strength on uncertainty estimation, we conducted a series of experiments under varying σ q settings, as shown in Fig. 4. First, we computed the average uncertainty estimates (TU, AU, and EU) on samples generated from the GSM8K test dataset using Llama-3.2-1B-Instruct.As illustrated in Fig. 4 Left, the model's uncertainty increases steadily with higher perturbation strength.However, once σ q exceeds a critical threshold (e.g., 0.2), a sharp rise in uncertainty is observed.This rise illustrates that the current approximate distribution of the weights has deviated too far from the pre-trained point estimation of the parameters, leading to unreliable uncertainty estimates.</p>
<p>We further evaluate the effect of perturbation strength on downstream task performance.Specifically, we assess how effectively the uncertainty estimates, obtained under different σ q values, can be used as scoring signals to distinguish between correct and incorrect samples, as described in Sec.4.1.2.As shown in Fig. 4 Right, for TokUR (EU), too small σ q does not lead to meaningful During text generation with large language models, the decoding temperature introduces uncertainty into the model's output.In general, higher temperatures lead to more diverse responses.In this section, we investigate the relationship between decoding temperature τ and uncertainties estimated by our token-level approach.Specifically, we use Llama-3.2-1B-Instruct to answer questions from the MATH500 dataset under different decoding temperature settings and estimate the average uncertainty of the model's responses.</p>
<p>As shown in Fig. 5 Left, increasing the decoding temperature τ results in a notable rise in tokenlevel Aleatoric Uncertainty (AU) of the model, whereas the Epistemic Uncertainty (EU) remains relatively unaffected.Additionally, we report the AUROC scores of various uncertainty estimation approaches across different temperature settings in Fig. 5 Right.the temperature τ does not harm the performance of TokUR, highlighting its robustness to changes in decoding temperature.</p>
<p>E.4.3 ABLATION STUDY OF LENGTH NORMALIZATION</p>
<p>Length normalization is a standard technique for aggregating token-level uncertainty into sequencelevel uncertainty (Fu et al., 2025;Kang et al., 2025), as it mitigates the bias introduced by sequence length when evaluating generation confidence.However, as described in Eqn. 13, we do not apply normalization when computing TokUR.To assess the impact of sequence length on uncertainty estimation, we therefore conduct an ablation study on length normalization.</p>
<p>Experimental Setup.We investigate the effect of length normalization on incorrect reasoning path detection across three datasets (MATH500, GSM8K, and DEEPSCALER), following the same settings as in Table 1.We compare TokUR with and without Length Normalization (LN), along with representative baselines.In addition, we introduce a naive baseline, Negative Length, which uses sequence length alone as a confidence signal.</p>
<p>Results.As shown in Table 6, the impact of length normalization varies significantly across methods.</p>
<p>For both LL and TokUR, normalization consistently reduces AUROC and AUPRC, indicating that raw sequence length introduces a favorable bias that benefits uncertainty aggregation in de-hallucination tasks.This observation is further reinforced by the strong performance of the Negative Length baseline, which alone achieves competitive results across all datasets.In contrast, Self-Certainty and DeepConf show clear gains with normalization (e.g., Self-Certainty improves from 23.76 to 71.17 AUROC on MATH500), suggesting that normalization is essential for stabilizing their performance.Overall, these findings reveal that the role of length normalization is highly method-dependent.</p>
<p>TokUR (TU) -Scaling Efficiency</p>
<p>Stepwise Modeling (Maj@N)</p>
<p>Stepwise Modeling (WBoN) Joint Modeling (Maj@N) Joint Modeling (WBoN)</p>
<p>E.4.4 ABLATION STUDY OF STEPWISE POSTERIOR SAMPLING</p>
<p>To examine the validity of Assumption 3.1, we perform an ablation study comparing stepwise posterior sampling against the joint posterior formulation.Concretely, we evaluate test-time scaling on MATH500 dataset, using TokUR with both stepwise and joint modeling on Llama-3.2-1B-Instruct, while keeping all other settings consistent with Table 2.</p>
<p>As shown in Fig. 6, stepwise modeling consistently outperforms joint modeling across all uncertainty measures (AU, EU, and TU).Specifically, stepwise sampling achieves higher mean accuracy and larger improvements over the baseline, while also demonstrating superior scaling efficiency with increasing numbers of samples.These results provide strong empirical support for our assumption that posterior samples should not be shared across decoding steps, validating the design choice in Assumption 3.1.</p>
<p>E.5 CASE STUDY</p>
<p>In this section, we present several representative examples from the MATH500 and GSM8K datasets, along with their corresponding solutions generated by Llama-3.2-1B-Instruct.We estimate token-level uncertainty for each output using the definitions provided in Eqn.10~Eqn.12.The visualizations are shown in Fig. 7~Fig.10, where Aleatoric Uncertainty (AU, in RED) and Epistemic Uncertainty (EU, in GREEN) are visualized as text-heatmap.The background shading of each token corresponds to the magnitude of its uncertainty: the darker the shade, the higher the uncertainty, indicating a lower model confidence for that token.</p>
<p>We observe that incorrect solutions consistently exhibit elevated uncertainty at or near the token where the wrong final answer is generated.For instance, as shown in Fig. 7, sharp spikes in uncertainties happens with the arithmetic error of reversing "9600 -7200" into "7200 -9600".In contrast, correct solutions tend to show lower uncertainty overall and maintain low uncertainty on key answer tokens.</p>
<p>Furthermore, incorrect outputs tend to contain a higher density of high-uncertainty tokens throughout the solution, whereas correct outputs are generally more consistent and confident.These observations suggest that our token-level uncertainty estimation method can serve as a useful signal for identifying potential reasoning failures or unreliable outputs, offering a valuable diagnostic tool for both model interpretability and downstream error detection.</p>
<p>Figure 1 :
1
Figure 1: Distribution of TokUR's Uncertainty Scores and AUROC across Different Difficulty Levels, applied to Llama-3.2-1B-Instruct.Left: TokUR (AU, Ours); Middle: TokUR (TU, Ours); Right: TokUR (EU, Ours).</p>
<p>p M and a prompt c. 5: Initialize N particles {x i 1 ∼ p M (•|c)} N i=1 .6: t ← 1. 7: while not all particles stop do</p>
<h1></h1>
<p>Figure 3 :
3
Figure3: Distribution of responses from GSM8K(Cobbe et al., 2021) plotted in the Length Normalized EU-AU uncertainty space, as quantified by our token-level uncertainty metrics (Eqn.13).</p>
<p>Figure 4 :Figure 5 :
45
Figure 4: Left: Uncertainty estimation with different perturbation strength σ q .Right: Influence of perturbation strength on uncertainty-based AUROC scores.</p>
<p>Figure 6 :
6
Figure 6: Ablation of stepwise posterior sampling.Comparison of stepwise vs. joint modeling on Llama-3.2-1B-Instructacross accuracy, improvement, and efficiency.Stepwise modeling consistently achieves better scaling performance, validating Assumption 3.1.</p>
<p>•</p>
<p>Total Uncertainty (TU) is the entropy of random variable y t conditioned on x and y &lt;t :
TU(y t |y &lt;t , x) ≜ H[p(y t |y &lt;t , x)] = −yt∈Vp(y t |y &lt;t , x) log p(y t |y &lt;t , x), (10)• Aleatoric Uncertainty (AU) is the expectation of entropy of random variable y t over theweights θ sampled from the approximate posterior q(•|D) as in Eqn. 3:AU(y t |y &lt;t , x) ≜ E θ∼q(•|D) H[p(y t |y &lt;t , x; θ)] ,</p>
<p>Table 1 :
1
Performance of Uncertainty Estimation Methods for Incorrect Reasoning Path Detection.AUROC, AUPRC, and ACC * are all reported as percentage (%)."ISO?" indicates whether the method utilizes Internal Signal Only for uncertainty estimation.We include the accuracy of CoT (i.e., greedy decoding with Chain-of-Thought prompting) in the first row for reference.Boldface and underlining denote the best and the second-best performance, respectively.
MethodISO?MATH500GSM8KDeepScaleRAUROCAUPRCACC  <em>AUROCAUPRCACC  </em>AUROCAUPRCACC  *Llama-3.2-1B-InstructCoT (Lower-Bound)---25.60±0.00--44.43±0.00--14.25±0.00SE✗47.29±3.81 25.71±2.33 24.13±4.42 50.64±4.44 45.09±0.72 42.62±0.16 46.30±0.21 12.94±0.23 12.58±0.49SAR✗44.57±2.04 24.03±2.53 21.07±1.62 50.28±0.97 43.24±0.89 43.95±0.77 43.14±1.42 12.34±0.35 11.14±0.47UEcc✗48.75±1.05 25.79±1.83 25.20±0.33 49.05±0.46 60.02±0.44 59.62±0.22 48.68±0.24 13.77±0.29 14.23±0.45UDeg✗60.57±2.31 36.32±2.59 30.93±0.94 66.60±0.36 75.72±0.36 71.99±0.39 56.88±0.54 18.04±0.63 16.50±0.39P(True)✓54.38±1.20 26.39±1.26 27.60±1.18 56.64±0.04 48.22±0.03 48.92±0.00 59.58±0.43 17.48±0.25 17.52±0.50LLM-Check✓56.41±0.96 27.01±1.22 31.33±1.29 71.01±0.02 61.29±0.08 59.54±0.00 55.76±0.48 14.55±0.26 17.30±0.51INSIDE✓55.71±4.69 28.82±4.05 29.20±4.33 53.66±0.92 46.03±0.23 45.79±1.25 54.73±0.82 15.50±0.48 16.30±0.35PE✓57.08±0.89 26.88±1.05 31.33±0.82 71.21±0.03 61.61±0.08 59.85±0.00 56.09±0.46 14.74±0.23 17.33±0.92LL✓55.41±0.54 25.88±0.87 29.87±0.82 69.01±0.03 58.51±0.09 57.38±0.00 53.84±0.47 13.93±0.23 16.83±0.48Self-Certainty✓71.17±0.30 48.37±0.50 38.13±0.61 73.41±0.00 68.38±0.00 61.38±0.00 71.93±0.04 33.81±0.08 21.76±0.04DeepConf✓71.77±0.12 46.00±0.42 39.87±0.46 75.70±0.00 69.72±0.00 62.77±0.00 71.65±0.04 29.99±0.05 22.00±0.04TokUR (TU, Ours)✓80.64±0.29 56.79±0.74 44.67±0.46 75.07±0.05 70.29±0.07 62.31±0.00 83.55±0.02 47.56±0.04 25.71±0.02TokUR (AU, Ours)✓80.61±0.27 56.73±0.75 44.67±0.46 75.03±0.06 70.22±0.05 62.21±0.18 83.52±0.02 47.48±0.05 25.71±0.02TokUR (EU, Ours)✓79.74±0.21 56.64±0.41 44.13±0.83 71.79±0.80 66.40±1.02 59.74±1.00 82.87±0.32 46.76±0.38 25.52±0.11Llama-3.1-8B-InstructCoT (Lower-Bound)---48.60±0.00--85.69±0.00--24.86±0.00SE✗62.93±0.90 55.21±1.04 55.73±0.83 55.61±3.36 87.16±1.14 86.77±1.01 67.68±0.94 35.18±1.00 35.55±0.37SAR✗69.42±2.19 63.74±3.03 59.20±1.06 60.16±2.22 89.24±0.74 87.99±0.81 73.01±0.28 42.89±0.65 37.51±0.12UEcc✗50.23±2.23 49.48±2.44 49.60±2.04 47.47±2.15 84.69±0.89 84.87±1.17 50.16±0.66 25.08±0.18 25.48±0.53UDeg✗58.62±0.36 57.69±0.90 53.47±1.64 67.22±1.06 92.24±0.53 92.62±0.88 59.14±0.37 32.64±0.43 29.75±0.36P(True)✓33.41±0.25 36.05±0.55 35.33±0.19 41.94±0.01 82.19±0.00 82.77±0.00 33.64±0.20 18.06±0.06 16.23±0.02LLM-Check✓57.41±0.44 49.69±1.07 52.80±1.38 73.98±0.01 93.37±0.01 93.23±0.00 55.42±0.27 26.46±0.19 28.37±0.40INSIDE✓62.94±1.72 55.06±3.19 57.33±1.01 58.86±2.11 87.44±0.94 88.21±0.90 67.05±0.49 33.83±0.42 34.13±0.10PE✓57.98±0.49 49.72±0.84 53.07±0.94 74.03±0.01 93.37±0.00 93.23±0.00 55.90±0.23 26.80±0.16 28.65±0.22LL✓55.36±0.49 47.24±0.90 51.07±0.94 72.21±0.02 92.64±0.00 92.46±0.00 52.82±0.32 24.48±0.13 26.85±0.19Self-Certainty✓76.41±0.61 76.22±0.87 69.07±0.83 80.60±0.11 95.65±0.03 96.26±0.09 76.72±0.09 56.15±0.30 39.03±0.23DeepConf✓71.86±0.70 69.57±0.94 66.27±1.15 83.30±0.07 96.23±0.02 96.56±0.09 73.05±0.08 48.76±0.10 37.48±0.14TokUR (TU, Ours)✓82.47±0.47 79.62±0.33 74.00±0.69 81.01±0.04 95.53±0.05 95.54±0.00 85.33±0.07 65.25±0.01 43.91±0.09TokUR (AU, Ours)✓82.43±0.48 79.56±0.35 74.00±0.69 80.97±0.02 95.52±0.03 95.49±0.09 85.31±0.07 65.20±0.02 43.89±0.08TokUR (EU, Ours)✓82.86±0.42 81.35±0.66 72.40±1.20 78.31±1.58 94.91±0.59 94.67±0.77 84.92±0.28 65.57±0.43 43.89±0.27</p>
<p>Table 2 :
2
Performance of Uncertainty Estimation Methods for Test-Time Scaling.Boldface and underlining denote the best and the second-best performance, respectively.
DatasetScoreMethodLlama-3.2-1B-InstructLlama-3.1-8B-InstructN=16N=64N=256N=16N=64N=256LLMaj@N 47.10±0.85 54.11±0.52 58.89±0.36 86.74±0.62 90.48±0.48 91.01±0.28 WBoN 47.10±0.85 54.15±0.55 58.92±0.37 86.74±0.62 90.48±0.49 91.00±0.29Self-CertaintyMaj@N 45.02±0.92 52.61±0.72 57.18±0.53 80.02±0.70 87.25±0.49 90.05±0.40 WBoN 45.02±0.92 52.65±0.70 57.22±0.54 80.02±0.70 87.25±0.50 90.05±0.41GSM8K (Pass@1: 44.43 / 85.69)DeepConfMaj@N 46.72±0.89 53.50±0.66 58.05±0.44 86.24±0.66 90.34±0.46 90.92±0.28 WBoN 46.72±0.89 53.47±0.65 58.10±0.45 86.24±0.66 90.32±0.46 91.02±0.00TokUR (TU, Ours)Maj@N 50.29±1.03 57.18±0.45 60.68±0.49 87.68±0.57 90.67±0.45 90.96±0.36 WBoN 50.29±1.03 57.22±0.45 60.71±0.49 87.68±0.57 90.65±0.46 90.98±0.37TokUR (AU, Ours)Maj@N 50.20±0.98 57.21±0.46 60.70±0.41 87.42±0.66 90.60±0.44 90.99±0.32 WBoN 50.20±0.98 57.19±0.44 60.78±0.42 87.42±0.66 90.57±0.43 91.02±0.00TokUR (EU, Ours)Maj@N 50.38±0.92 56.92±0.60 59.88±0.52 88.06±0.57 90.69±0.47 91.07±0.33 WBoN 50.38±0.92 56.89±0.54 59.91±0.58 88.06±0.57 90.67±0.48 91.09±0.36LLMaj@N 26.42±0.84 33.28±0.97 38.56±0.75 50.92±1.77 59.36±0.74 64.10±0.61 WBoN 26.42±0.84 33.30±1.10 38.58±0.73 50.92±1.77 59.46±0.78 64.02±0.71Self-CertaintyMaj@N 20.14±1.14 29.12±1.11 36.68±0.83 44.00±1.82 55.56±1.08 62.66±0.75 WBoN 20.14±1.14 29.16±0.99 36.80±0.80 44.00±1.82 55.58±1.06 62.52±0.53MATH500 (Pass@1: 25.60 / 48.60)DeepConfMaj@N 25.68±1.38 33.30±1.10 38.52±0.43 49.88±1.29 59.74±1.17 64.30±0.63 WBoN 25.68±1.38 32.44±1.20 37.08±0.78 49.88±1.29 58.22±1.00 63.14±0.55TokUR (TU, Ours)Maj@N 27.06±0.94 33.76±0.84 39.18±0.70 51.26±1.36 59.44±1.31 63.86±0.44 WBoN 27.06±0.94 33.60±0.82 39.20±0.65 51.26±1.36 59.44±1.30 63.84±0.51TokUR (AU, Ours)Maj@N 27.06±0.91 33.64±0.76 39.12±0.72 51.16±1.45 59.42±1.16 64.00±0.44 WBoN 27.06±0.91 33.48±0.73 39.10±0.69 51.16±1.45 59.44±1.19 63.92±0.47TokUR (EU, Ours)Maj@N 28.28±1.32 35.44±0.79 39.44±0.88 52.40±1.39 60.90±0.93 65.32±0.80 WBoN 28.28±1.32 35.44±0.78 39.38±0.87 52.40±1.39 61.04±0.88 65.48±0.75
(Kang et al., 2025;Fu et al., 2025;Kadavath et al., 2022;Malinin &amp; Gales, 2021;l., 2023), and INternal States for hallucInation DEtection (INSIDE)(Chen et al., 2024a), as well as classic P(True)(Kadavath et al., 2022), Predictive Entropy (PE)(Malinin &amp; Gales, 2021), Log-Likelihood (LL)(Murray &amp; Chiang, 2018), and (ii) those leveraging external signals, such as an auxiliary Natural Language Inference model(He et al., 2020): Semantic Entropy (SE)(Kuhn et al., 2023), and Shifting Attention to Relevance (SAR)(Duan et al., 2024).Note that, apart from the five baselines with underlines(Kang et al., 2025;Fu et al., 2025;Kadavath et al., 2022;Malinin &amp; Gales, 2021; Murray &amp; Chiang, 2018), the others were originally designed for query-level de-hallucination in short-form QA tasks and are therefore not directly comparable to TokUR; we include them for completeness (see Appendix D.4 for details).</p>
<p>Proposition 3.1 (Response-Level Uncertainty as an Unbiased Estimator of Query-Level Uncertainty).Given an input query x, let y ∼ p(y|x) be a generated sample of length T .Then the response-level uncertainty U (Definition 3.1) is an unbiased estimator of the query-level uncertainty U (Definition 2.1), i.e., Proof.Based on Lemma C.1, for the token-level uncertainty U(y t |y &lt;t , x) defined in Eqn.10~Eqn.12,we haveE y&lt;t∼p(•|x) [U(y t |y &lt;t , x)] =
E y∼p(y|x) [ U(y|x)] = U(y|x).(28)T=tH(E p(θ|D) [p(y t |y &lt;t , x; θ)])(24)TT=tI(y t ; θ|y &lt;t , x) +tE p(θ|D) <a href="25">H(p(y t |y &lt;t , x; θ))</a>T=tI(y t ; θ|y &lt;t , x) + E p(θ|D) H(p(y|x; θ))(26)Finally, based on the definition of mutual information, we obtain:
I(y; θ|x) = H(p(y|x)) − E p(θ|D) H(p(y|x; θ)) = T t I(y t ; θ|y &lt;t , x) (27) y&lt;t∈Y p(y &lt;t |x)U(y t |y &lt;t , x)</p>
<p>Table 3 :
3
Statistics of the datasets used in our experiments.
DatasetSamples UsedSplitTask TypeLanguageLevelGSM8K1,300Training splitMathematical ReasoningEnglishModerateMATH500500Full setMathematical ReasoningEnglishDifficultDeepScaleR5,000First 5,000 samples Mathematical ReasoningEnglishHighly ChallengingD.3 PROMPT TEMPLATESIn this work, we use the following prompts published by Meta 2 .</p>
<p>Our TokUR (AU, EU, and TU) consistently outperforms the LL baseline, particularly when N is small.Please refer to Table4for detailed numerical results.E.2 TEST-TIME SCALING VIA UNCERTAINTY ESTIMATIONWe provide an additional visualization of the test-time scaling results in Fig.2.While the complete numerical results are reported in Table2, this new figure offers an intuitive view of how accuracy improves with increasing numbers of test-time samples (N
GSM8KMATH50045 50 55 65 60 43.4% Accuracy (%)N=16N=32N=64 Number of Samples N=128 LL AU (Ours) N=256N=512 EU (Ours)40 25 30 35 25.6% Accuracy (%) TU (Ours)N=16 Pass@1N=32 Maj@NN=64 Number of Samples N=128 WBoNN=256N=512Figure 2: Performance on GSM8K (Left) and MATH500 (Right) when scaling up sample sizeN at test time of Llama-3.2-1B-Instruct.</p>
<p>Table 5 :
5
TokUR as Implicit Reward for Test-Time Scaling (Online), on MATH500.
Intrinsic RewardBoNWBoNLL26.27±0.2526.27±0.41TokUR (TU, Ours)27.93±0.2528.13±0.38TokUR (AU, Ours)25.20±0.9925.13±0.74TokUR (EU, Ours)28.93±0.0829.20±0.98</p>
<p>Table 4 :
4
Test-Time Scaling for GSM8K and MATH500.Performance comparison of different methods with varying numbers of test-time samples (N = 16 to 512) using Llama-3.2-1B-Instruct as the base model.Methods evaluated include log-likelihood (LL) and three variants of TokUR (TU, AU and EU) with both Maj@N and WBoN strategies.Boldface and underlining denote the best and the second-best performance, respectively.
DatasetScoreMethodN=16N=32Number of Samples N N=64 N=128N=256N=512Llama-3.2-1B-InstructLLMaj@N WBoN47.10±0.85 47.10±0.8550.45±0.64 50.45±0.6454.11±0.52 54.15±0.5556.77±0.40 56.72±0.4258.89±0.36 58.92±0.3759.72±0.00 59.81±0.00Self-CertaintyMaj@N WBoN45.02±0.92 45.02±0.9248.97±0.81 48.97±0.8152.61±0.72 52.65±0.7055.22±0.67 55.30±0.6457.18±0.53 57.22±0.5458.03±0.00 58.10±0.00GSM8KDeepConfMaj@N WBoN46.72±0.89 46.72±0.8950.12±0.71 50.12±0.7153.50±0.66 53.47±0.6556.10±0.52 56.08±0.4958.05±0.44 58.10±0.4558.97±0.00 59.05±0.00(Pass@1: 44.43)LLMaj@N WBoN86.74±0.62 86.74±0.6289.16±0.53 89.16±0.5390.48±0.48 90.48±0.4990.99±0.35 90.99±0.3691.01±0.28 91.00±0.2991.00±0.00 91.00±0.00Self-CertaintyMaj@N WBoN80.02±0.70 80.02±0.7084.13±0.66 84.13±0.6687.25±0.49 87.25±0.5089.22±0.40 89.21±0.3990.05±0.40 90.05±0.4190.77±0.00 90.77±0.00DeepConfMaj@N WBoN86.24±0.66 86.24±0.6688.74±0.64 88.74±0.6490.34±0.46 90.32±0.4690.88±0.46 90.90±0.4590.92±0.28 90.94±0.2891.01±0.00 91.02±0.00TokUR (TU, Ours)Maj@N WBoN87.68±0.57 87.68±0.5789.72±0.55 89.72±0.5590.67±0.45 90.65±0.4691.06±0.38 91.06±0.3790.96±0.36 90.98±0.3791.02±0.00 91.02±0.00TokUR (AU, Ours)Maj@N WBoN87.42±0.66 87.42±0.6689.59±0.55 89.59±0.5590.60±0.44 90.57±0.4391.01±0.31 91.04±0.3590.99±0.32 90.98±0.3090.93±0.00 91.02±0.00TokUR (EU, Ours)Maj@N WBoN88.06±0.57 88.06±0.5789.88±0.39 89.88±0.3990.69±0.47 90.67±0.4891.19±0.40 91.19±0.3991.07±0.33 91.09±0.3691.02±0.00 91.05±0.00</p>
<p>Table 6 :
6
Uncertainties for Incorrect Reasoning Path Detection.AUROC, AUPRC, and ACC * are all reported as percentage (%), where ACC * (%) denotes the accuracy of the Top 50% generations identified by different uncertainty measures.Rows with shading indicate methods without Length Normalization (LN) for uncertainty estimation.
MethodMATH500GSM8KDeepScaleRAUROC AUPRCACC  <em>AUROC AUPRCACC  </em>AUROC AUPRCACC  *</p>
<p>These results indicate that varying
250EU TU AU0.72 0.74 0.760 50 100 150 2000.010.05 Perturbation Strength q 0.10.20.5AUROC Score0.60 0.62 0.64 0.66 0.68 0.700.010.05 Perturbation Strength q 0.10.20.5 AU TU EU LL
We provide an implementation of our framework that is compatible with vLLM(Kwon et al.,<br />
) for efficient deployment.
https://huggingface.co/datasets/meta-llama/Llama-3.2-1B-Instruct-evals
The is from GSM8K, whose correct answer is 2400.In the incorrect solution, the model demonstrated significant uncertainty when mistakenly reversing "9600 − 7200" as "7200 − 9600", and also exhibited high uncertainty at the negative sign "−" in the final answer.The sample is from MATH500.In this example, the model demonstrated notably high uncertainty at the incorrect answer token "60".In the correct solution on the left, the model had low uncertainty for the correct answer "36".
Uncertainty quantification in fine-tuned llms using lora ensembles. Oleksandr Balabanov, Hampus Linander, arXiv:2402.122642024arXiv preprint</p>
<p>Scaling test-time compute with open models. Edward Beeching, Lewis Tunstall, Sasha Rush, </p>
<p>Weight uncertainty in neural network. Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra, International conference on machine learning. PMLR2015</p>
<p>Area under the precision-recall curve: point estimates and confidence intervals. Kendrick Boyd, Kevin H Eng, C David, Page , Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2013. Prague, Czech RepublicSpringerSeptember 23-27, 2013. 2013Proceedings, Part III 13</p>
<p>Large language monkeys: Scaling inference compute with repeated sampling. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Christopher Quoc V Le, Azalia Ré, Mirhoseini, arXiv:2407.217872024arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Inside: Llms' internal states retain the power of hallucination detection. Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, Jieping Ye, arXiv:2402.03744arXiv:2405.14953and Wenpin Tang. Mallowspo: Fine-tune your llm with preference dispersions. Hanyang Zhao, Henry Lam, David YaoHaoxian Chen2024a. 2024barXiv preprint</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Journal of Machine Learning Research. 25702024</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Elements of information theory. M Thomas, Cover, 1999John Wiley &amp; Sons</p>
<p>Understanding the uncertainty of llm explanations: A perspective based on reasoning topology. Longchao Da, Xiaoou Liu, Jiaxin Dai, Lu Cheng, Yaqing Wang, Hua Wei, arXiv:2502.170262025arXiv preprint</p>
<p>Rainproof: An umbrella to shield text generator from out-of-distribution data. Maxime Darrin, Pablo Piantanida, Pierre Colombo, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Decomposition of uncertainty for active learning and reliable reinforcement learning in stochastic systems. Stefan Depeweg, José Miguel Hernández-Lobato, Finale Doshi-Velez, Steffen Udluft, stat. 10501112017</p>
<p>Shifting attention to relevance: Towards the predictive uncertainty quantification of free-form large language models. Jinhao Duan, Hao Cheng, Shiqi Wang, Alex Zavalny, Chenan Wang, Renjing Xu, Bhavya Kailkhura, Kaidi Xu, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Detecting hallucinations in large language models using semantic entropy. Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, Yarin Gal, Nature. 63080172024</p>
<p>Beam search strategies for neural machine translation. Markus Freitag, Yaser Al-Onaizan, arXiv:1702.018062017arXiv preprint</p>
<p>Yichao Fu, Xuewei Wang, Yuandong Tian, Jiawei Zhao, arXiv:2508.15260Deep think with confidence. 2025arXiv preprint</p>
<p>Dropout as a bayesian approximation: Representing model uncertainty in deep learning. Yarin Gal, Zoubin Ghahramani, international conference on machine learning. PMLR2016</p>
<p>Uncertainty in deep learning. Yarin Gal, 2016</p>
<p>The llama 3 herd of models. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, arXiv:2407.217832024arXiv preprint</p>
<p>rstar-math: Small llms can master math reasoning with self-evolved deep thinking. Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, Mao Yang, arXiv:2501.045192025arXiv preprint</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>The meaning and use of the area under a receiver operating characteristic (roc) curve. A James, Barbara J Hanley, Mcneil, Radiology. 14311982</p>
<p>Deberta: Decoding-enhanced bert with disentangled attention. Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, arXiv:2006.036542020arXiv preprint</p>
<p>Probabilistic backpropagation for scalable learning of bayesian neural networks. José Miguel, Hernández-Lobato , Ryan Adams, International conference on machine learning. PMLR2015</p>
<p>Decomposing uncertainty for large language models through input clarification ensembling. Bairu Hou, Yujian Liu, Kaizhi Qian, Jacob Andreas, Shiyu Chang, Yang Zhang, arXiv:2311.087182023arXiv preprint</p>
<p>LoRA: Low-rank adaptation of large language models. J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations. 2022</p>
<p>Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, Heung-Yeung Shum, arXiv:2503.242902025arXiv preprint</p>
<p>. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova Dassarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei2022Chris Olah, and Jared KaplanTom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlishLanguage models (mostly) know what they know</p>
<p>Scalable best-of-n selection for large language models via self-certainty. Zhewei Kang, Xuandong Zhao, Dawn Song, arXiv:2502.185812025arXiv preprint</p>
<p>Large language models must be taught to know what they don't know. Sanyam Kapoor, Nate Gruver, Manley Roberts, Katherine Collins, Arka Pal, Umang Bhatt, Adrian Weller, Samuel Dooley, Micah Goldblum, Andrew Gordon, Wilson , arXiv:2406.083912024arXiv preprint</p>
<p>The singular value decomposition: Its computation and some applications. Virginia Klema, Alan Laub, IEEE Transactions on automatic control. 2521980</p>
<p>Semantic entropy probes: Robust and cheap hallucination detection in llms. Jannik Kossen, Jiatong Han, Muhammed Razzak, Lisa Schut, Shreshth Malik, Yarin Gal, arXiv:2406.159272024arXiv preprint</p>
<p>Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. Lorenz Kuhn, Yarin Gal, Sebastian Farquhar, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, Ion Stoica, Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. the ACM SIGOPS 29th Symposium on Operating Systems Principles2023</p>
<p>Simple and scalable predictive uncertainty estimation using deep ensembles. Alexander Balaji Lakshminarayanan, Charles Pritzel, Blundell, 201730Advances in neural information processing systems</p>
<p>Let's verify step by step. Vineet Hunter Lightman, Yuri Kosaraju, Harrison Burda, John Edwards ; Leike, Ilya Schulman, Karl Sutskever, Cobbe, The Twelfth International Conference on Learning Representations. Bowen Baker, Teddy LeeJan. 2023</p>
<p>Teaching models to express their uncertainty in words. Stephanie Lin, Jacob Hilton, Owain Evans, Transactions on Machine Learning Research. 2022</p>
<p>Generating with confidence: Uncertainty quantification for black-box large language models. Zhen Lin, Shubhendu Trivedi, Jimeng Sun, arXiv:2305.191872023arXiv preprint</p>
<p>Uncertainty quantification for in-context learning of large language models. Chen Ling, Xujiang Zhao, Xuchao Zhang, Wei Cheng, Yanchi Liu, Yiyou Sun, Mika Oishi, Takao Osaki, Katsushi Matsuda, Jie Ji, arXiv:2402.101892024arXiv preprint</p>
<p>Dellma: Decision making under uncertainty with large language models. Ollie Liu, Deqing Fu, Dani Yogatama, Willie Neiswanger, arXiv:2402.023922024arXiv preprint</p>
<p>Uncertainty quantification and confidence calibration in large language models: A survey. Xiaoou Liu, Tiejin Chen, Longchao Da, Chacha Chen, Zhen Lin, Hua Wei, arXiv:2503.158502025arXiv preprint</p>
<p>The harpy speech recognition system. Bruce T Lowerre, 1976Carnegie Mellon University</p>
<p>Deepscaler: Surpassing o1-preview with a 1.5 b model by scaling rl. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, Notion Blog. 2025</p>
<p>Uncertainty estimation in autoregressive structured prediction. Andrey Malinin, Mark Gales, International Conference on Learning Representations. 2021</p>
<p>Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. Potsawee Manakul, Adian Liusie, Mark, Gales, arXiv:2303.08896arXiv:1808.10006Kenton Murray and David Chiang. Correcting length bias in neural machine translation. 2023. 2018arXiv preprint</p>
<p>Bayesian learning for neural networks. Neal Radford, 2012Springer Science &amp; Business Media118</p>
<p>Epistemic neural networks. Ian Osband, Zheng Wen, Seyed Mohammad Asghari, Vikranth Dwaracherla, Morteza Ibrahimi, Xiuyuan Lu, Benjamin Van Roy, Advances in Neural Information Processing Systems. 202336</p>
<p>A probabilistic inference approach to inference-time scaling of llms using particle-based monte carlo methods. Isha Puri, Shivchander Sudalairaj, Guangxuan Xu, Kai Xu, Akash Srivastava, arXiv:2502.016182025arXiv preprint</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances in Neural Information Processing Systems. 362023</p>
<p>Out-of-distribution detection and selective generation for conditional language models. Jie Ren, Jiaming Luo, Yao Zhao, Kundan Krishna, Mohammad Saleh, Balaji Lakshminarayanan, Peter J Liu, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Training-free bayesianization for low-rank adapters of large language models. Haizhou Shi, Yibin Wang, Ligong Han, Huan Zhang, Hao Wang, arXiv:2412.057232024arXiv preprint</p>
<p>Llm-check: Investigating detection of hallucinations in large language models. Gaurang Sriramanan, Siddhant Bharti, Shoumik Vinu Sankar Sadasivan, Priyatham Saha, Soheil Kattakinda, Feizi, Advances in Neural Information Processing Systems. 202437</p>
<p>Sequence to sequence learning with neural networks. Ilya Sutskever, Oriol Vinyals, Quoc V Le, Advances in neural information processing systems. 201427</p>
<p>Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, Christopher Manning, 10.18653/v1/2023.emnlp-main.330Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Solving math word problems with process-and outcome-based feedback. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, Irina Higgins, arXiv:2211.142752022arXiv preprint</p>
<p>Mutual information alleviates hallucinations in abstractive summarization. Liam Van Der, Ryan Poel, Clara Cotterell, Meister, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Benchmarking uncertainty quantification methods for large language models with lm-polygraph. Roman Vashurin, Ekaterina Fadeeva, Artem Vazhentsev, Lyudmila Rvanova, Daniil Vasilev, Akim Tsvigun, Sergey Petrakov, Rui Xing, Abdelrahman Sadallah, Kirill Grishchenkov, 2025Transactions of the Association for Computational Linguistics13</p>
<p>Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730Attention is all you need</p>
<p>Towards bayesian deep learning: A framework and some existing methods. Hao Wang, Dit-Yan Yeung, IEEE Transactions on Knowledge and Data Engineering. 28122016</p>
<p>A survey on bayesian deep learning. Hao Wang, Dit-Yan Yeung, ACM computing surveys (csur). 202053</p>
<p>Natural-parameter networks: A class of probabilistic neural networks. Hao Wang, Dit-Yan Shi Xingjian, Yeung, NIPS. 2016</p>
<p>Xi Wang, Laurence Aitchison, Maja Rudolph, Lora ensembles for large language model finetuning. 2023</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Blob: Bayesian low-rank adaptation by backpropagation for large language models. Yibin Wang, Haizhou Shi, Ligong Han, Dimitris Metaxas, Hao Wang, arXiv:2406.116752024arXiv preprint</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022aarXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 2022b35</p>
<p>Self-evaluation guided beam search for reasoning. Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, Michael Xie, Advances in Neural Information Processing Systems. 202336</p>
<p>Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, Bryan Hooi, arXiv:2306.130632023arXiv preprint</p>
<p>Yasin Abbasi Yadkori, Ilja Kuzborskij, András György, Csaba Szepesvári, arXiv:2406.02543To believe or not to believe your llm. 2024arXiv preprint</p>
<p>Bayesian low-rank adaptation for large language models. Maxime Adam X Yang, Xi Robeyns, Laurence Wang, Aitchison, arXiv:2308.131112023arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in neural information processing systems. 202336</p>
<p>Uncertainty-aware step-wise verification with generative reward models. Zihuiwen Ye, Luckeciano Carvalho Melo, Younesse Kaddar, Phil Blunsom, Sam Staton, Yarin Gal, arXiv:2502.112502025arXiv preprint</p>
<p>Reasoning in flux: Enhancing large language models reasoning through uncertainty-aware adaptive guidance. Zhangyue Yin, Qiushi Sun, Qipeng Guo, Zhiyuan Zeng, Xiaonan Li, Junqi Dai, Qinyuan Cheng, Xuan-Jing Huang, Xipeng Qiu, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Cot-uq: Improving response-wise uncertainty quantification in llms with chain-of-thought. Boxuan Zhang, Ruqi Zhang, arXiv:2502.172142025arXiv preprint</p>
<p>Enhancing uncertainty-based hallucination detection with stronger focus. Tianhang Zhang, Lin Qiu, Qipeng Guo, Cheng Deng, Yue Zhang, Zheng Zhang, Chenghu Zhou, Xinbing Wang, Luoyi Fu, arXiv:2311.132302023arXiv preprint</p>
<p>Language agent tree search unifies reasoning acting and planning in language models. Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, Yu-Xiong Wang, arXiv:2310.044062023arXiv preprint</p>
<p>Uncertainty-guided chain-of-thought for code generation with llms. Yuqi Zhu, Ge Li, Xue Jiang, Jia Li, Hong Mei, Zhi Jin, Yihong Dong, arXiv:2503.153412025arXiv preprintTokUR (TU, Ours</p>
<p>. ( Tokur, Au, ) Ours, @ Maj, 50.20±0.98 53.77±0.90 57.21±0.46 58.99±0.61 60.70±0.41 61.38±0.00</p>
<p>Ours) Maj@N 50. ( Tokur, Eu, 38±0.92 52.98±0.67 56.92±0.60 58.77±0.38 59.88±0.52 60.69±0.00</p>
<p>Self-Certainty Maj@N 20. 14±1.14 23.24±1.44 29.12±1.11 33.80±0.89 36.68±0.83 38.60±0.00</p>
<p>Ours) Maj@N 27. ( Tokur, Tu, 06±0.94 29.18±1.06 33.76±0.84 37.62±0.70 39.18±0.70 39.00±0.00</p>
<p>Ours) Maj@N 27. ( Tokur, Au, 06±0.91 29.08±1.14 33.64±0.76 37.52±0.82 39.12±0.72 39.20±0.00</p>
<p>Ours) Maj@N 28. ( Tokur, Eu, 28±1.32 31.36±1.05 35.44±0.79 38.00±0.77 39.44±0.88 39.60±0.00</p>
<p>Ours) Maj@N 51. ( Tokur, Tu, 26±1.36 55.54±0.70 59.44±1.31 62.28±0.95 63.86±0.44 65.20±0.00</p>
<p>Ours) Maj@N 51. ( Tokur, Au, 16±1.45 55.52±0.66 59.42±1.16 62.32±1.07 64.00±0.44 65.60±0.00</p>
<p>Ours) Maj@N 52. ( Tokur, Eu, 40±1.39 57.02±0.61 60.90±0.93 64.24±0.83 65.32±0.80 67.00±0.00</p>
<p>) by distorting the original semantic content. Based on these findings, we set σ q = 0.1 for the experiments reported in Sec. 29±3.81 25.71±2.33 24.13±4.42 50.64±4.44 45.09±0.72 42.62±0.16 46.30±0.21 12.94±0.23 12.58±0.49 SAR 44.57±2.04 24.03±2.53 21.07±1.62 50.28±0.97 43.24±0.89 43.95±0.77 43.14±1.42 12.34±0.35 11.14±0.47log-likelihood, whereas an excessively large σ q harms performance of all three TokUR. 4TU and EULlama-3.2-1B-Instruct SE 47</p>
<p>. P , True) 54.38±1.20 26.39±1.26 27.60±1.18 56.64±0.04 48.22±0.03 48.92±0.00 59.58±0.43 17.48±0.25 17.52±0.50</p>
<p>. ( Tokur, Tu, Ours) 57.14±0.81 26.92±0.98 31.87±1.00 70.92±0.04 61.32±0.13 58.92±0.15 56.20±0.49 14.79±0.20 17.52±0.53 -LN 80.64±0.29 56.79±0.74 44.67±0.46 75.07±0.05 70.29±0.07 62.31±0.00 83.55±0.02 47.56±0.04 25.71±0.02</p>
<p>. ( Tokur, Au, 56.95±0.82 26.81±0.99 31.60±0.98 70.90±0.05 61.26±0.13 58.87±0.32 56.02±0.49 14.73±0.19 17.47±0.47 -LN 80.61±0.27 56.73±0.75 44.67±0.46 75.03±0.06 70.22±0.05 62.21±0.18 83.52±0.02 47.48±0.05 25.71±0.02</p>
<p>. ( Tokur, Eu, Ours) 61.64±0.97 31.07±1.31 33.20±1.42 65.98±0.75 60.02±0.82 56.05±0.73 62.10±0.09 17.73±0.35 19.10±0.29 -LN 79.74±0.21 56.64±0.41 44.13±0.83 71.79±0.80 66.40±1.02 59.74±1.00 82.87±0.32 46.76±0.38 25.52±0.11</p>
<p>. P , True) 33.41±0.25 36.05±0.55 35.33±0.19 41.94±0.01 82.19±0.00 82.77±0.00 33.64±0.20 18.06±0.06 16.23±0.02</p>
<p>. ( Tokur, Tu, 56.49±0.46 48.24±0.85 52.13±0.75 73.98±0.05 93.27±0.04 93.13±0.09 54.86±0.17 25.97±0.12 27.97±0.17 -LN 82.47±0.47 79.62±0.33 74.00±0.69 81.01±0.04 95.53±0.05 95.54±0.00 85.33±0.07 65.25±0.01 43.91±0.09</p>
<p>. ( Tokur, Au, 56.31±0.47 48.11±0.84 51.87±0.68 73.97±0.02 93.26±0.03 93.13±0.09 54.77±0.18 25.90±0.13 27.93±0.11 -LN 82.43±0.48 79.56±0.35 74.00±0.69 80.97±0.02 95.52±0.03 95.49±0.09 85.31±0.07 65.20±0.02 43.89±0.08</p>
<p>. ( Tokur, Eu, Ours) 60.92±0.46 52.64±0.71 56.13±1.36 67.92±0.72 92.41±0.24 92.15±0.41 57.42±0.23 28.32±0.16 29.65±0.10 -LN 82.86±0.42 81.35±0.66 72.40±1.20 78.31±1.58 94.91±0.59 94.67±0.77 84.92±0.28 65.57±0.43 43.89±0.27</p>            </div>
        </div>

    </div>
</body>
</html>
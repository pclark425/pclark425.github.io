<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9218 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9218</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9218</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-261696561</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2309.05557v3.pdf" target="_blank">An Empirical Study of NetOps Capability of Pre-Trained Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Nowadays, the versatile capabilities of Pre-trained Large Language Models (LLMs) have attracted much attention from the industry. However, some vertical domains are more interested in the in-domain capabilities of LLMs. For the Networks domain, we present NetEval, an evaluation set for measuring the comprehensive capabilities of LLMs in Network Operations (NetOps). NetEval is designed for evaluating the commonsense knowledge and inference ability in NetOps in a multi-lingual context. NetEval consists of 5,732 questions about NetOps, covering five different sub-domains of NetOps. With NetEval, we systematically evaluate the NetOps capability of 26 publicly available LLMs. The results show that only GPT-4 can achieve a performance competitive to humans. However, some open models like LLaMA 2 demonstrate significant potential.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9218.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9218.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot vs Few-shot (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot versus Few-shot Prompt Formats (aggregate evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregate comparison of zero-shot and few-shot prompt presentation formats on the NetEval multi-choice benchmark across many evaluated LLMs, showing that the effect of few-shot varies strongly by model (small to moderate gains for some, large gains for a few, and negative impact for at least one).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various evaluated LLMs (e.g., GPT-4, GPT-3.5, LLaMA, LLaMA-2, Falcon, GLM, ChatGLM, Baichuan, Moss, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NetEval multi-choice (NetOps certification-style questions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>5,269 multi-choice NetOps questions (English 73%, Chinese 27%) drawn from certification exams and other NetOps sources; accuracy is used as the metric.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot prompts (task description + question) and Few-shot prompts (task description + 5 in-context examples sampled from development set; 5-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Varies by model; example: GPT-4 zero-shot accuracy 77%, GPT-4 few-shot accuracy 81%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Examples: GPT-4: 77% (zero-shot) vs 81% (few-shot). For many models the few-shot improvement was mild (<3%); Falcon-40B and ChatGLM2-6B showed >6% improvement; GLM-130B showed negative impact when using few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Range observed: negative impact for GLM-130B (no numeric given); mild improvement <3% for LLaMA, LLaMA-2, GLM-130B (contradictory behavior noted per-model), ChatGLM-6B, GPT-3.5; >6% for Falcon-40B and ChatGLM2-6B; GPT-4 +4% (zero→few).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Paper hypothesizes differences arise from model pretraining and prompt-understanding ability; in some cases the few-shot examples may mislead models (e.g., GLM-130B) possibly due to domain/language mismatch or limited prompt understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Multi-choice set size 5,269; development sets include five examples per subject used for 5-shot few-shot prompts. Prompts end with 'Answer:'; post-processing extracts answer labels via first-line truncation and regex. Accuracy used as metric. Only models with zero-shot >=40% were displayed for some comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study of NetOps Capability of Pre-Trained Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9218.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9218.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 few-shot vs zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 performance under few-shot and zero-shot prompts on NetEval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Measured GPT-4 accuracy on the NetEval multi-choice NetOps benchmark showing GPT-4 is the only model to pass the 80% certification threshold under few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NetEval multi-choice (NetOps certification-style questions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>5,269 multi-choice NetOps questions; accuracy reported.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot prompt (task description + question) and Few-shot prompt (5-shot examples from development set included).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Zero-shot vs Few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>few-shot accuracy: 81%; zero-shot accuracy: 77% (accuracy on the multi-choice NetEval test set).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Few-shot 81% vs Zero-shot 77% (effect +4 percentage points). GPT-4 (few-shot) is the only model that meets the 80% passing threshold for the certification-style exam.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+4% accuracy (few-shot vs zero-shot) for GPT-4 on NetEval multi-choice.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Few-shot in-context examples improve performance by providing in-context task demonstrations; the paper uses 5-shot examples sampled from development set.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Prompts include task description and 5-shot examples; evaluation uses deterministic matching of extracted answer labels to ground truth; post-processing truncates to first line and applies regex to extract labels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study of NetOps Capability of Pre-Trained Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9218.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9218.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot Chain-of-Thought Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of few-shot Chain-of-Thought prompts on selected models shows CoT benefits primarily smaller models (LLaMA-7B, LLaMA-2-7B) with modest absolute gains, while for many larger models CoT produced no positive results; zero-shot CoT was observed to perform extremely poorly and was not used.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Selected models (LLaMA-7B, LLaMA-2-7B, others tested selectively)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B for the explicit positive examples; others unspecified</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NetEval multi-choice (subset evaluated with CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-choice NetOps questions; CoT increases prompt and output length; accuracy used as metric.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot Chain-of-Thought (few-shot CoT) — prompts include a few examples with step-by-step reasoning demonstrations; zero-shot CoT was observed but not used due to very poor performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to standard few-shot (non-CoT) and zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>LLaMA-7B and LLaMA-2-7B: few-shot accuracy increased by ~4–5% with CoT. Other evaluated models showed no positive result from CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CoT gave +4–5% lift for LLaMA-7B and LLaMA-2-7B; no measurable positive effect for many other models; zero-shot CoT extremely poor (not used).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+4–5% accuracy for LLaMA-7B and LLaMA-2-7B; otherwise null or negative effect for other models (no positive results reported).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Paper suggests few-shot CoT may be more effective on smaller models, possibly because such models benefit from explicit reasoning demonstrations, whereas larger models or models with different training regimes may not be helped by CoT in this domain or by the specific CoT examples used.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>CoT was only evaluated on selected models due to increased computation cost. The authors avoided zero-shot CoT after observing extremely bad performance across models. Few-shot CoT examples were sampled from development sets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study of NetOps Capability of Pre-Trained Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9218.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9218.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Retrieval-Augmented Generation Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of retrieval-augmented prompts (providing retrieved external context from a vector index) in zero-shot mode consistently improved model performance on NetEval multi-choice compared with plain zero-shot and sometimes outperformed few-shot without RAG.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Subset of evaluated models (4 models evaluated with RAG; models not exhaustively listed)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NetEval multi-choice (NetOps)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-choice NetOps questions; RAG supplies semantically retrieved context passages within prompt; accuracy used as metric.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot RAG prompt: zero-shot prompt augmented with retrieved relevant text snippets from a sentence-BERT/faiss Wikipedia index (pre-built) provided as context.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to zero-shot and few-shot (without RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>RAG consistently improved zero-shot performance across all 4 evaluated models; in some cases RAG zero-shot outperformed few-shot without RAG (no absolute numeric values provided in the paper for each model).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>RAG (zero-shot) > zero-shot (no retrieval); RAG (zero-shot) sometimes > few-shot (no retrieval). No per-model numeric accuracies were reported in the text for all four models.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Not quantified numerically in the paper for all models; described qualitatively as consistent improvements and sometimes outperforming few-shot without retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>RAG enriches context with external, relevant factual knowledge enabling better inference without model retraining; effective for domain-specific knowledge recall.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>RAG evaluation limited to zero-shot due to cost; vector engine: faiss with sentence-BERT encoder and a pre-built Wikipedia index; custom sharding used. Prompts include retrieved passages and end with 'Answer:'.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study of NetOps Capability of Pre-Trained Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9218.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9218.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruction Tuning (IT) impact</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of Instruction Tuning / Supervised Fine-Tuning on model accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of base models and their instruction-tuned / chat/instruct variants (e.g., LLaMA-2-Chat, Falcon-instruct) showed that instruction tuning with general-domain IT data sometimes decreased NetOps multi-choice accuracy, suggesting domain mismatch issues.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2 (base vs chat), Falcon (base vs instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NetEval multi-choice (NetOps)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-choice NetOps questions; accuracy measured.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Comparison between base (pretrained) model inference and instruction-tuned / chat/instruct variants (prompting consistent across variants).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Base model vs instruction-tuned/chat/instructed variant</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Instruction-tuned versions of LLaMA-2 and Falcon performed worse than their original/base counterparts on NetEval multi-choice (no exact numeric differences reported in text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Base > Instruction-tuned (for LLaMA-2 and Falcon) on NetEval multi-choice according to Figure 7; numeric deltas not specified in text.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Not numerically specified; qualitative negative effect reported.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Likely due to IT (SFT) corpora being general-domain and substantially different from NetOps domain knowledge; instruction tuning may shift model behavior away from domain-specific recall required for NetOps questions.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Comparison illustrated in Figure 7; authors recommend domain-specific IT data to improve NetOps effectiveness. Evaluation used the same prompting and post-processing pipeline as other experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study of NetOps Capability of Pre-Trained Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9218.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9218.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot CoT negative</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Extremely poor performance of Zero-shot Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper reports that zero-shot Chain-of-Thought prompting produced extremely bad performance across evaluated models, and therefore they did not use zero-shot CoT in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various evaluated LLMs (attempted zero-shot CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NetEval multi-choice (NetOps)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Attempted zero-shot CoT prompting (asking model to provide chain-of-thought without few-shot examples) resulted in very poor outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot Chain-of-Thought (no examples; prompt requests step-by-step reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared implicitly to few-shot CoT and standard zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Described as 'extremely bad performance' (no numeric metrics provided); authors did not use zero-shot CoT in their experiments based on this observation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Zero-shot CoT frequently produces poor reasoning outputs unless supported by few-shot demonstrations; authors observed universally poor results for zero-shot CoT in this domain.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Zero-shot CoT was observed prior to main experiments and then excluded from systematic evaluation because of its poor behavior; no quantitative results provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study of NetOps Capability of Pre-Trained Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9218.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9218.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Non-multi-choice (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot performance on non-multi-choice (filling-blanks and QA) NetEval questions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Zero-shot evaluation on a complementary set of 463 non-multi-choice NetOps questions (fill-in-the-blank and open QA) produced low BLEU/ROUGE scores (<0.3) for all evaluated models, indicating the task is challenging for current models in closed-book generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>12 selected models (including LLaMA family, LLaMA-2, Falcon, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NetEval non-multi-choice (filling-blanks and open question-answering)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>463 non-multi-choice NetOps questions extracted from multi-choice and external corpora; BLEU and ROUGE used as evaluation metrics for generated answers.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot prompt (task description + question) used for non-multi-choice evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>All evaluated models achieved BLEU/ROUGE scores below 0.3 (indicating poor performance). LLaMA (base) variants performed better than many others including LLaMA-2 in this closed-book QA setting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLaMA models generally outperformed other models on non-multi-choice zero-shot; LLaMA-2 performed worse than LLaMA for closed-book QA. For LLaMA-2-7B and Falcon (base vs chat/instruct), the chat/instruct variants had lower precision but higher recall.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Non-multi-choice tasks require longer generation and domain-specific factual recall; poor scores suggest current pretraining/fine-tuning lacks sufficient NetOps-specific training data for closed-book generative QA.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Non-multi-choice set size 463; evaluation only on selected models due to larger generation lengths and higher cost; BLEU and ROUGE metrics computed; authors did not release the non-multi-choice test set publicly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study of NetOps Capability of Pre-Trained Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Measuring massive multitask language understanding. <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Gpt-4 technical report. <em>(Rating: 1)</em></li>
                <li>Llama: Open and efficient foundation language models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9218",
    "paper_id": "paper-261696561",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "Zero-shot vs Few-shot (aggregate)",
            "name_full": "Zero-shot versus Few-shot Prompt Formats (aggregate evaluation)",
            "brief_description": "Aggregate comparison of zero-shot and few-shot prompt presentation formats on the NetEval multi-choice benchmark across many evaluated LLMs, showing that the effect of few-shot varies strongly by model (small to moderate gains for some, large gains for a few, and negative impact for at least one).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various evaluated LLMs (e.g., GPT-4, GPT-3.5, LLaMA, LLaMA-2, Falcon, GLM, ChatGLM, Baichuan, Moss, etc.)",
            "model_size": null,
            "task_name": "NetEval multi-choice (NetOps certification-style questions)",
            "task_description": "5,269 multi-choice NetOps questions (English 73%, Chinese 27%) drawn from certification exams and other NetOps sources; accuracy is used as the metric.",
            "presentation_format": "Zero-shot prompts (task description + question) and Few-shot prompts (task description + 5 in-context examples sampled from development set; 5-shot).",
            "comparison_format": null,
            "performance": "Varies by model; example: GPT-4 zero-shot accuracy 77%, GPT-4 few-shot accuracy 81%.",
            "performance_comparison": "Examples: GPT-4: 77% (zero-shot) vs 81% (few-shot). For many models the few-shot improvement was mild (&lt;3%); Falcon-40B and ChatGLM2-6B showed &gt;6% improvement; GLM-130B showed negative impact when using few-shot.",
            "format_effect_size": "Range observed: negative impact for GLM-130B (no numeric given); mild improvement &lt;3% for LLaMA, LLaMA-2, GLM-130B (contradictory behavior noted per-model), ChatGLM-6B, GPT-3.5; &gt;6% for Falcon-40B and ChatGLM2-6B; GPT-4 +4% (zero→few).",
            "explanation_or_hypothesis": "Paper hypothesizes differences arise from model pretraining and prompt-understanding ability; in some cases the few-shot examples may mislead models (e.g., GLM-130B) possibly due to domain/language mismatch or limited prompt understanding.",
            "null_or_negative_result": true,
            "experimental_details": "Multi-choice set size 5,269; development sets include five examples per subject used for 5-shot few-shot prompts. Prompts end with 'Answer:'; post-processing extracts answer labels via first-line truncation and regex. Accuracy used as metric. Only models with zero-shot &gt;=40% were displayed for some comparisons.",
            "uuid": "e9218.0",
            "source_info": {
                "paper_title": "An Empirical Study of NetOps Capability of Pre-Trained Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "GPT-4 few-shot vs zero-shot",
            "name_full": "GPT-4 performance under few-shot and zero-shot prompts on NetEval",
            "brief_description": "Measured GPT-4 accuracy on the NetEval multi-choice NetOps benchmark showing GPT-4 is the only model to pass the 80% certification threshold under few-shot prompting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "task_name": "NetEval multi-choice (NetOps certification-style questions)",
            "task_description": "5,269 multi-choice NetOps questions; accuracy reported.",
            "presentation_format": "Zero-shot prompt (task description + question) and Few-shot prompt (5-shot examples from development set included).",
            "comparison_format": "Zero-shot vs Few-shot",
            "performance": "few-shot accuracy: 81%; zero-shot accuracy: 77% (accuracy on the multi-choice NetEval test set).",
            "performance_comparison": "Few-shot 81% vs Zero-shot 77% (effect +4 percentage points). GPT-4 (few-shot) is the only model that meets the 80% passing threshold for the certification-style exam.",
            "format_effect_size": "+4% accuracy (few-shot vs zero-shot) for GPT-4 on NetEval multi-choice.",
            "explanation_or_hypothesis": "Few-shot in-context examples improve performance by providing in-context task demonstrations; the paper uses 5-shot examples sampled from development set.",
            "null_or_negative_result": false,
            "experimental_details": "Prompts include task description and 5-shot examples; evaluation uses deterministic matching of extracted answer labels to ground truth; post-processing truncates to first line and applies regex to extract labels.",
            "uuid": "e9218.1",
            "source_info": {
                "paper_title": "An Empirical Study of NetOps Capability of Pre-Trained Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT)",
            "name_full": "Few-shot Chain-of-Thought Prompting",
            "brief_description": "Evaluation of few-shot Chain-of-Thought prompts on selected models shows CoT benefits primarily smaller models (LLaMA-7B, LLaMA-2-7B) with modest absolute gains, while for many larger models CoT produced no positive results; zero-shot CoT was observed to perform extremely poorly and was not used.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Selected models (LLaMA-7B, LLaMA-2-7B, others tested selectively)",
            "model_size": "7B for the explicit positive examples; others unspecified",
            "task_name": "NetEval multi-choice (subset evaluated with CoT)",
            "task_description": "Multi-choice NetOps questions; CoT increases prompt and output length; accuracy used as metric.",
            "presentation_format": "Few-shot Chain-of-Thought (few-shot CoT) — prompts include a few examples with step-by-step reasoning demonstrations; zero-shot CoT was observed but not used due to very poor performance.",
            "comparison_format": "Compared to standard few-shot (non-CoT) and zero-shot",
            "performance": "LLaMA-7B and LLaMA-2-7B: few-shot accuracy increased by ~4–5% with CoT. Other evaluated models showed no positive result from CoT.",
            "performance_comparison": "CoT gave +4–5% lift for LLaMA-7B and LLaMA-2-7B; no measurable positive effect for many other models; zero-shot CoT extremely poor (not used).",
            "format_effect_size": "+4–5% accuracy for LLaMA-7B and LLaMA-2-7B; otherwise null or negative effect for other models (no positive results reported).",
            "explanation_or_hypothesis": "Paper suggests few-shot CoT may be more effective on smaller models, possibly because such models benefit from explicit reasoning demonstrations, whereas larger models or models with different training regimes may not be helped by CoT in this domain or by the specific CoT examples used.",
            "null_or_negative_result": true,
            "experimental_details": "CoT was only evaluated on selected models due to increased computation cost. The authors avoided zero-shot CoT after observing extremely bad performance across models. Few-shot CoT examples were sampled from development sets.",
            "uuid": "e9218.2",
            "source_info": {
                "paper_title": "An Empirical Study of NetOps Capability of Pre-Trained Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Retrieval-Augmented Generation (RAG)",
            "name_full": "Zero-shot Retrieval-Augmented Generation Prompting",
            "brief_description": "Use of retrieval-augmented prompts (providing retrieved external context from a vector index) in zero-shot mode consistently improved model performance on NetEval multi-choice compared with plain zero-shot and sometimes outperformed few-shot without RAG.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Subset of evaluated models (4 models evaluated with RAG; models not exhaustively listed)",
            "model_size": null,
            "task_name": "NetEval multi-choice (NetOps)",
            "task_description": "Multi-choice NetOps questions; RAG supplies semantically retrieved context passages within prompt; accuracy used as metric.",
            "presentation_format": "Zero-shot RAG prompt: zero-shot prompt augmented with retrieved relevant text snippets from a sentence-BERT/faiss Wikipedia index (pre-built) provided as context.",
            "comparison_format": "Compared to zero-shot and few-shot (without RAG)",
            "performance": "RAG consistently improved zero-shot performance across all 4 evaluated models; in some cases RAG zero-shot outperformed few-shot without RAG (no absolute numeric values provided in the paper for each model).",
            "performance_comparison": "RAG (zero-shot) &gt; zero-shot (no retrieval); RAG (zero-shot) sometimes &gt; few-shot (no retrieval). No per-model numeric accuracies were reported in the text for all four models.",
            "format_effect_size": "Not quantified numerically in the paper for all models; described qualitatively as consistent improvements and sometimes outperforming few-shot without retrieval.",
            "explanation_or_hypothesis": "RAG enriches context with external, relevant factual knowledge enabling better inference without model retraining; effective for domain-specific knowledge recall.",
            "null_or_negative_result": false,
            "experimental_details": "RAG evaluation limited to zero-shot due to cost; vector engine: faiss with sentence-BERT encoder and a pre-built Wikipedia index; custom sharding used. Prompts include retrieved passages and end with 'Answer:'.",
            "uuid": "e9218.3",
            "source_info": {
                "paper_title": "An Empirical Study of NetOps Capability of Pre-Trained Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Instruction Tuning (IT) impact",
            "name_full": "Effect of Instruction Tuning / Supervised Fine-Tuning on model accuracy",
            "brief_description": "Comparison of base models and their instruction-tuned / chat/instruct variants (e.g., LLaMA-2-Chat, Falcon-instruct) showed that instruction tuning with general-domain IT data sometimes decreased NetOps multi-choice accuracy, suggesting domain mismatch issues.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2 (base vs chat), Falcon (base vs instruct)",
            "model_size": null,
            "task_name": "NetEval multi-choice (NetOps)",
            "task_description": "Multi-choice NetOps questions; accuracy measured.",
            "presentation_format": "Comparison between base (pretrained) model inference and instruction-tuned / chat/instruct variants (prompting consistent across variants).",
            "comparison_format": "Base model vs instruction-tuned/chat/instructed variant",
            "performance": "Instruction-tuned versions of LLaMA-2 and Falcon performed worse than their original/base counterparts on NetEval multi-choice (no exact numeric differences reported in text).",
            "performance_comparison": "Base &gt; Instruction-tuned (for LLaMA-2 and Falcon) on NetEval multi-choice according to Figure 7; numeric deltas not specified in text.",
            "format_effect_size": "Not numerically specified; qualitative negative effect reported.",
            "explanation_or_hypothesis": "Likely due to IT (SFT) corpora being general-domain and substantially different from NetOps domain knowledge; instruction tuning may shift model behavior away from domain-specific recall required for NetOps questions.",
            "null_or_negative_result": true,
            "experimental_details": "Comparison illustrated in Figure 7; authors recommend domain-specific IT data to improve NetOps effectiveness. Evaluation used the same prompting and post-processing pipeline as other experiments.",
            "uuid": "e9218.4",
            "source_info": {
                "paper_title": "An Empirical Study of NetOps Capability of Pre-Trained Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Zero-shot CoT negative",
            "name_full": "Extremely poor performance of Zero-shot Chain-of-Thought prompting",
            "brief_description": "The paper reports that zero-shot Chain-of-Thought prompting produced extremely bad performance across evaluated models, and therefore they did not use zero-shot CoT in experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various evaluated LLMs (attempted zero-shot CoT)",
            "model_size": null,
            "task_name": "NetEval multi-choice (NetOps)",
            "task_description": "Attempted zero-shot CoT prompting (asking model to provide chain-of-thought without few-shot examples) resulted in very poor outputs.",
            "presentation_format": "Zero-shot Chain-of-Thought (no examples; prompt requests step-by-step reasoning)",
            "comparison_format": "Compared implicitly to few-shot CoT and standard zero-shot",
            "performance": "Described as 'extremely bad performance' (no numeric metrics provided); authors did not use zero-shot CoT in their experiments based on this observation.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Zero-shot CoT frequently produces poor reasoning outputs unless supported by few-shot demonstrations; authors observed universally poor results for zero-shot CoT in this domain.",
            "null_or_negative_result": true,
            "experimental_details": "Zero-shot CoT was observed prior to main experiments and then excluded from systematic evaluation because of its poor behavior; no quantitative results provided.",
            "uuid": "e9218.5",
            "source_info": {
                "paper_title": "An Empirical Study of NetOps Capability of Pre-Trained Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Non-multi-choice (zero-shot)",
            "name_full": "Zero-shot performance on non-multi-choice (filling-blanks and QA) NetEval questions",
            "brief_description": "Zero-shot evaluation on a complementary set of 463 non-multi-choice NetOps questions (fill-in-the-blank and open QA) produced low BLEU/ROUGE scores (&lt;0.3) for all evaluated models, indicating the task is challenging for current models in closed-book generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "12 selected models (including LLaMA family, LLaMA-2, Falcon, etc.)",
            "model_size": null,
            "task_name": "NetEval non-multi-choice (filling-blanks and open question-answering)",
            "task_description": "463 non-multi-choice NetOps questions extracted from multi-choice and external corpora; BLEU and ROUGE used as evaluation metrics for generated answers.",
            "presentation_format": "Zero-shot prompt (task description + question) used for non-multi-choice evaluation.",
            "comparison_format": null,
            "performance": "All evaluated models achieved BLEU/ROUGE scores below 0.3 (indicating poor performance). LLaMA (base) variants performed better than many others including LLaMA-2 in this closed-book QA setting.",
            "performance_comparison": "LLaMA models generally outperformed other models on non-multi-choice zero-shot; LLaMA-2 performed worse than LLaMA for closed-book QA. For LLaMA-2-7B and Falcon (base vs chat/instruct), the chat/instruct variants had lower precision but higher recall.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Non-multi-choice tasks require longer generation and domain-specific factual recall; poor scores suggest current pretraining/fine-tuning lacks sufficient NetOps-specific training data for closed-book generative QA.",
            "null_or_negative_result": true,
            "experimental_details": "Non-multi-choice set size 463; evaluation only on selected models due to larger generation lengths and higher cost; BLEU and ROUGE metrics computed; authors did not release the non-multi-choice test set publicly.",
            "uuid": "e9218.6",
            "source_info": {
                "paper_title": "An Empirical Study of NetOps Capability of Pre-Trained Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Measuring massive multitask language understanding.",
            "rating": 2,
            "sanitized_title": "measuring_massive_multitask_language_understanding"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Gpt-4 technical report.",
            "rating": 1,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "Llama: Open and efficient foundation language models.",
            "rating": 1,
            "sanitized_title": "llama_open_and_efficient_foundation_language_models"
        }
    ],
    "cost": 0.01300525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AN EMPIRICAL STUDY OF NETOPS CAPABILITY OF PRE-TRAINED LARGE LANGUAGE MODELS</p>
<p>Yukai Miao 
Zhongguancun Laboratory</p>
<p>Yu Bai 
Zhongguancun Laboratory</p>
<p>Li Chen 
Zhongguancun Laboratory</p>
<p>Dan Li 
Zhongguancun Laboratory</p>
<p>Tsinghua University</p>
<p>Haifeng Sun 
Beijing University of Posts and Telecommunications</p>
<p>Xizheng Wang 
Tsinghua University</p>
<p>Ziqiu Luo 
Tsinghua University</p>
<p>Yanyu Ren 
Tsinghua University</p>
<p>Dapeng Sun 
Zhongguancun Laboratory</p>
<p>Xiuting Xu 
Zhongguancun Laboratory</p>
<p>Qi Zhang 
China Telecom Corporation Limited Research Institute</p>
<p>Chao Xiang 
China Telecom Corporation Limited Research Institute</p>
<p>Xinchi Li 
China Telecom Corporation Limited Research Institute</p>
<p>AN EMPIRICAL STUDY OF NETOPS CAPABILITY OF PRE-TRAINED LARGE LANGUAGE MODELS
Network Operation · Pre-trained Language Model
Nowadays, the versatile capabilities of Pre-trained Large Language Models (LLMs) have attracted much attention from the industry. However, some vertical domains are more interested in the indomain capabilities of LLMs. For the Networks domain, we present NetEval 1 , an evaluation set for measuring the comprehensive capabilities of LLMs in Network Operations (NetOps). NetEval is designed for evaluating the commonsense knowledge and inference ability in NetOps in a multilingual context. NetEval consists of 5,732 questions about NetOps, covering five different subdomains of NetOps. With NetEval, we systematically evaluate the NetOps capability of 26 publicly available LLMs. The results show that only GPT-4 can achieve a performance competitive to humans.However, some open models like LLaMA 2 demonstrate significant potential. 2</p>
<p>Introduction</p>
<p>Empowering NetOps with LLMs</p>
<p>Pre-trained Large Language Models (LLMs), are deep neural network models trained with massively large unlabeled textual data via self-supervised learning, obtaining comprehensive knowledge and generalization capabilities. With methods like fine-tuning and in-context learning, LLMs may serve as the foundation model for downstream tasks and achieve better performance and efficiency than traditional methods. In recent years, pre-trained LLMs have achieved remarkable progress in various tasks in general-domain Natural Language Processing tasks. Meanwhile, pre-trained LLMs are also applied in other domains like Software Engineering, Business and Finance, Biology and Medicine, etc. For NetOps, LLMs have enormous potential as well. The following are some of the potential application scenarios of LLMs in NetOps.</p>
<p>• Network Monitoring: Network monitoring software is deployed to monitor the performance data in networks in real-time, conduct analysis, generate reports, and quickly find and solve network faults to ensure the stability of the network.</p>
<p>• Network Topology Planning: According to the needs of enterprises or organizations, network topology design and planning are carried out to ensure the rationality and scalability of the network structure.</p>
<p>• Network Device Management: Configure, install, monitor, maintain, upgrade, and test network equipment (such as switches, routers, firewalls, etc.) to ensure the normal operation of network equipment and avoid network failures and downtime.</p>
<p>• Network Troubleshooting: Locate, analyze, and repair network faults to ensure rapid recovery and high availability.</p>
<p>• Network Performance Optimization: Monitor and optimize performance indicators such as network bandwidth, latency, and packet loss rate to ensure stable and efficient operation of the network, and improve users' network experience and work efficiency.</p>
<p>• Network Security: Carry out security assessment, vulnerability scanning, intrusion detection and defense on the network, protect the network from malicious attacks and data leakage, and ensure the security of the network.</p>
<p>• Network Backup and Recovery: Back up network data regularly and make recovery plans to address unexpected data loss or catastrophic events, and safeguard essential data.</p>
<p>In summary, NetOps scenarios as above are crucial to the normal operation of enterprises and organizations. They not only ensure network stability and security but also enhance network performance and user experience, which makes greater value for enterprises and organizations. Leveraging the powerful representation and transfer capabilities, LLMs can effectively analyze and process NetOps data, thereby improve the efficiency and quality of NetOps and drive the high-quality development of intelligent NetOps.</p>
<p>Evaluation of NetOps Capabilities of LLMs</p>
<p>Evaluating pre-trained LLMs for NetOps holds significant importance. On one hand, assessing LLMs' performance in NetOps provides a strong guidance for designing and optimizing LLMs for NetOps. On the other hand, such evaluation promote more applications of AI technology in NetOps, driving the further advancement of intelligent NetOps.</p>
<p>The evaluation of LLMs is by nature challenging. It is crucial to ensure the objectivity and fairness of the evaluation but often hard to achieve. The diverse nature of the tasks and data in NetOps presents additional challenges for evaluating pre-trained LLMs. To address these challenges, this work adopts the following methods to evaluate the capabilities of pre-trained LLMs in NetOps:</p>
<ol>
<li>
<p>Collect Data from Multiple Sources: As shown in Figure 1 , to address the comprehensiveness issue, multiple data sources that are related to NetOps are incorporated into the evaluation dataset. As the major component of the dataset, certification exam questions offer the advantage of not only thoroughly examining LLMs capabilities but also directly comparing LLMs with human performance, endorsing future application in real-world NetOps scenarios.</p>
</li>
<li>
<p>Use Multi-format Questions as Evaluation Set: To address the objectiveness issue, we extract multi-choice questions from the data sources as the major part of evaluation set. Since multi-choice questions have deterministic answers, evaluating the correctness of the generated choice labels from LLMs can be highly automated. Besides, we also include some cloze questions and Closed-QA questions as a complement to the dataset.</p>
</li>
<li>
<p>Diverse Prompt Engineering Methods to Enhance Model Output: Providing test questions directly as prompts to LLMs led to varied output quality, hindering most LLMs from performing at their full potential. To address this, we apply various prompt engineering techniques and rule-based post-processing to improve the evaluation results.</p>
</li>
</ol>
<p>This article is organized as follows: Section 2 introduces the dataset we use for evaluation, Section 3 elaborates the LLMs we evaluate, Section 4 discusses the evaluation method, Section 5 provides detailed evaluation results and analysis, and at last Section 6 concludes the paper.</p>
<p>Evaluation Dataset</p>
<p>We present a multi-lingual LLM evaluation dataset for NetOps, named "NetEval". It consists of more than six thousand questions about NetOps, most of which are multi-choice questions. The construction process of NetEval is illustrated in Figure 1. NetEval has been released to the public and will be continuously updated.</p>
<p>Multi-choice Evaluation Set</p>
<p>Multiple-choice questions are the most widely used questions in both real-world certification exams and existing LLM benchmarks. Many existing LLM evaluation datasets, e.g. the Massive Multitask Language Understanding (MMLU)  Inspired by MMLU, we construct a comprehensive multi-choice problem set for the NetOps domain by collecting certification exam questions from both the public websites and network device vendors we collaborate with. We argue that although answering multi-choice questions is far from solving real NetOps problems, achieving a human-level accuracy in NetOps certification exam questions is still one of the fundamental capabilities we expect LLMs to have. The collected evaluation set includes 5,269 multiple-choice questions. As show in in Figure 2, we sample some of the questions and annotate the category each question is within from 5 distinct subjects. Table 1 shows the example question in each subject. Due to the varied data sources, the evaluation set contains both English questions (73%) and Chinese questions (27%), which facilitates the evaluation of multi-lingual NetOps capability.</p>
<p>Regarding the organization of the dataset, the questions in each subject are divided into three parts: the development set, the validation set, and the test set. The development set includes five examples for 5-shot evaluation, as will be discussed in Section 4. The validation set is used for hyperparameter tuning, while the test set is used for the final evaluation.</p>
<p>To avoid the development set and the validation set having similar questions to that in test set, we select questions that were least similar to other questions as the development set and validation set. To measure the similarity between questions, we use sentence-BERT [2] to encode each question sample into a vector and calculate the cosine similarity between vectors as the measurement of semantic similarity between samples.</p>
<p>We release all the development sets and validation sets of the multi-choice questions, while the test sets are only partially available to the public. The users of this dataset need to submit the request of evaluation to obtain the results on the full test set. </p>
<p>Non-multi-choice Evaluation Set</p>
<p>The evaluation solely on the multi-choice questions has some obvious limitations. First, the LLMs may guess the answer labels, regardless how much NetOps knowledge does it have. Second, the lack of interpretability of multi-choice question-answering hinders finer-grained evaluation and in-depth analysis. Thus, as a complement to our evaluation set, we also include a small collection of non-multi-choice questions, consisting of 463 filling-blanks and question-answering questions.</p>
<p>We adopt two ways to generate these questions. First, we use rule-based filtering method to extract a few questions from the multi-choice question set, and eliminate the choices to obtain a filling-blanks or question-answering questions. The filtering rules are manually crafted to identify the questions which are still valid given only the question content and the correct choice. Second, we also adopt GPT-4 to automatically extract question-answering pairs from the raw corpus about NetOps, including the technical standards (e.g. RFC), networking textbooks and Wikipedia articles that are related to NetOps. We parse the raw corpus and split them into small chunks (at most 4k characters in each chunk), combine each chunk with a piece of instruction about the requirement of the extraction output, and finally send the instructions to GPT-4 via the API. We also adopt similar methods in Alpaca to post-process the response from GPT-4 to extract the expected output. To shortlist the Wikipedia articles that are relevant to NetOps, we manually create a list of seed Wikipedia pages of NetOps terms, e.g. IPv6, and then expand the article list by traversing the hyperlinks in the articles in the current list. In this way, we collected around two thousand articles.</p>
<p>After the collection of non-multi-choice questions, we further filter the questions with manual inspection and eventually select 463 questions to do the evaluation. However, since the inference on such questions requires a larger generation length, resulting in a much larger cost, we only evaluate some selected models in Table 2 on this evaluation set.</p>
<p>We organize the non-multi-choice questions in a similar way as that for multi-choice questions, i.e. split the questions into the development set, validation set and the test test. Due to the potentially fast change in he non-multi-choice questions. We choose not to release them for now, but we are happy to share them upon request.</p>
<p>Evaluated Models</p>
<p>With the rapid advances of LLM technology and the vast amount of investment in LLM from various institutions and companies, new LLMs keep emerging in the market. For example, at the time of writing this article, there have been more than 100 LLMs developed in China. We are not able to cover all the models in our evaluation due to the limited computational resources. Instead, we select the most representative ones in our experiments.  As shown in Table 2, we evaluate general-domain models from 6 institutions or companies. OpenAI developed many huge LLMs with hundreds or thousands of billions of parameters. Among these models, text-davinci-003 is the 175B base model in GPT-3.5 series, GPT-3.5-turbo [3] is a chat-oriented model obtained by supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF) on top of text-davinci-003, and the successor GPT-4 [4] is recognized as the most advanced LLM that ever exists. OpenAI does not provide the model weights to the public but exposes some APIs to the developers, thus we evaluate these models via the APIs. As a good open alternative to GPT-3/4 series, LLaMA [5] was introduced by Meta, and attracted much attention from both the academia and the industry. There are multiple sizes available in LLaMA series, ranging from 7B to 65B. Its successor LLaMA 2 [6] provides both the base models and chat-oriented models of 3 different sizes. Falcon [7] was developed by UAE TII, consisting of both the base models and the instruction-tuned models of two sizes, once ranked the first in the Open LLM Leaderboard [8]. GLM [9] is a series of base models from Tsinghua University, trained with a distinct architecture and bilingual (English+Chinese) corpus. The chat-variants of GLM, including ChatGLM and ChatGLM 2, are also derived from the base model with SFT and RLHF. Besides, the Moss [10] and Baichuan [11] are also competitive LLMs that pre-trained with bilingual corpus.</p>
<p>Evaluation Method</p>
<p>Evaluation Pipeline</p>
<p>The evaluation process involves three steps, i.e. prompt generation, model inference, and post-processing. The main difference is in the prompt generation.</p>
<p>Prompt Generation: In-context Learning (ICL) allows LLMs to perform tasks by observing a few prompt examples, without requiring any updates to the model parameters. Prompt generation involves transforming the questions from the evaluation set into textual input for LLM inference. The generated prompts mainly contain task descriptions, the content of the evaluation, and the final question. The task description presents the context of task, for example, "The following are single-choice questions related to NetOps...". As illustrated in Figure 3, we adopt four types of prompts, namely zero-shot prompts, few-shot prompts, few-shot Chain-of-Thought (CoT) prompt and Retrieval-Augmented Generation (RAG) prompt.</p>
<p>Zeor-shot and Few-shot prompts are two commonly used prompts in various LLM tasks. Zero-shot prompts contain only the content of the task description and test question, while few-shot prompts also include some examples with questions and answers. We use the development set in the dataset for sampling few-shot examples.</p>
<p>Most studies indicate that Chain of Thought (CoT) [12] has a significant impact on the output results of generative models. Therefore, we also consider the few-Shot Chain-of-Thought (Few-shot-CoT) prompts in our evaluation. Few-shot-CoT provides a small number of examples of chain of thought demonstrations to guide the reasoning process of the language model and consequently affect the inference results. We do not use zero-shot CoT as we observe extremely bad performance on all the evaluated models in our experiments. </p>
<p>The following are multiple choice questions (with answers) about network.</p>
<p>0-Shot Few-Shot Few-Shot CoT</p>
<p>Use the following pieces of context to answer the question at the end. Moreover, we also include Retrieval-Augmented Generation (RAG) prompt in our evaluation. In the field of prompt engineering, leveraging information retrieval techniques such as vector retrieval has become a prominent approach to enhancing the memory of LLMs. Semantic indexing allows for the swift and accurate retrieval of a set of text pieces from an extensive pool of candidate data that are semantically relevant to a specific task. In scenarios involving the addition of new samples, model retraining is unnecessary. Instead, by providing retrieval-based factual information to the model through prompts, its generation capability can be augmented. In our experiments, considering the computational cost, we only evaluate zero-shot RAG prompt.</p>
<p>At the end of the prompts, we used text like "Answer:" to indicate that the model should output the answer afterward.</p>
<p>Model Inference: After generating prompts, we invoked the interfaces of various language models to obtain model outputs.</p>
<p>For non-open source language models, such as ChatGPT, we achieved model inference by calling the API. Each evaluation question was processed individually as an individual session. The API input conversation only comprising a default system message and a user message containing the prompt content. For open source models, we load and test the model in the local environment. Depending on the implementation of the models, we call appropriate functions for text generation to conduct the inference of the model. When we call such functions, we refer to the example parameter configurations in the official sites of the models and restrict the maximal number of output tokens.</p>
<p>Post-Processing: Since all the evaluated LLMs are generative models, it is hard to unify the output formats. Postprocessing is required to extract final answers from the model outputs. Initially, we truncate the output content using the newline character and retain only the first line. For multi-choice questions, it is often insufficient to extract the first line of the output. For example, the first line of the output might be 'the correct choice is A'. In cases as such, a series of predefined regular expressions are employed to match and extract the answer label from the generated text.</p>
<p>Upon completing the above steps, we compute the performance metrics on the model output. For multi-choice questions, we compute the accuracy of answers by directly comparing the extracted answer label with the ground truth. For non-multi-choice questions, we adopt BLEU [13] and ROUGE [14] scores to evaluate the quality of the output.</p>
<p>Implementation Details</p>
<p>Our evaluation method is fundamentally similar to existing mainstream methods such as MMLU and C-Eval. For open source models, we loaded and tested the models in a local test environment. Depending on the implementation of the evaluated models, we call the low-level text generation functions or high-level pipeline functions to execute the inference given the prompt. When calling local models, we referred to parameter combinations in example inference code for each model in their official websites and constrained the number of output tokens. Our evaluation used two types of hosts as the testing environment as depicted in Table 3. For larger models such as LLaMA-65B and Falcon 40B, we used the Type B hosts for testing, while other models were dynamically allocated to all the hosts according to resource availability. The model inference was accelerated with model parallelism, which distribute the computation into multiple GPUs.  The Retrieval-Augmented Generation relies on an external vector database and an efficient vector engine. In our evaluation, we adopt faiss [15] as the basis of the vector engine, combined with customized sharding strategy. We use a pre-built Wikipedia index, built with the sentence-bert [2] encoder, as the vector database.</p>
<p>Results and Analysis</p>
<p>Since most of the questions in our evaluation set are multi-choice questions, we put most of our efforts on evaluating the LLMs on multi-choice questions only. For non-multi-choice questions, we examine selected models with zero-shot prompt only. We leave further experiments and full-scale evaluations on these questions as future work.</p>
<p>The accuracy of various models under zero-shot and few-shot prompts is illustrated in Figure 4. Overall, GPT-4 achieves the best performance, i.e. 81% accuracy under few-shot prompts and 77% under zero-shot prompts. NetOps related certification exams require an 80% passing threshold. In our test, GPT-4 (few-shot) is the only model that can pass the certification. The other models exhibited lower accuracy levels, preventing them from passing the human-certified exam. We consider the main reason is that other models lack pre-training and fine-tuning on specialized NetOps corpora. However, open models like LLaMA 2 showcased considerable potential. Consequently, we deem it essential to further train LLMs specialized in the NetOps domain based on general open-source LLMs.</p>
<p>Relationship between Model Parameters and Accuracy</p>
<p>As depicted in Figure 5, a comparison of accuracy across different parameter scales of the same model reveals that larger size LLMs perform better. When comparing different models, we observe that LLaMA 2 70B is the best-performed open model that is competitive with GPT-4. Furthermore, accuracy of LLaMA 65B is greater than GLM 130B, which has more than twice the parameter count. This could be attributed to the scarcity of NetOps-related English language data in the training corpus of domestic models.</p>
<p>Impact of Zero-shot/Few-shot Prompts on Accuracy</p>
<p>With LLMs getting ever larger, prompt engineering is a powerful tuning method for a specific task. We compared the two common prompts settings, zero-shot and few-shot prompt. The result is shown in Figure 6, we only display the accuracy of models which obtain at least 40% accuracy in zero-shot setting. It is found that the effect of few-shot prompts on model accuracy varies depending on the model type. For LLaMA, LLaMA-2, GLM-130B, ChatGLM-6B, and GPT-3.5, few-shot prompts had a mild impact (&lt;3%). Falcon-40B and ChatGLM2-6B exhibited substantial improvement, with an improvement of more than 6%. Falcon-40B also achieved comparable performance to that of LLaMA-65B. However, for GLM-130B, few-shot prompts had a negative impact. It possibly dues to its limitation prompt understanding in English. The examples in the prompt seem to mislead the model instead.</p>
<p>Impact of Instruction Tuning on Accuracy</p>
<p>Instruction Tuning (IT) or Supervised Fine-Tuning(SFT) is a common technique used to further align models to tasks after pretraining. We aimed to assess whether IT is beneficial for models in the NetOps domain. As shown in Figure  7, we compared the performance of original versions and the IT versions for both LLaMA-2 and Falcon models. Interestingly, for both LLaMA-2 and Falcon, the models after IT did not perform as well as the original models. This suggests that IT might not necessarily improve model performance. This outcome could be attributed to the fact that the IT data used for LLaMA-2-Chat and Falcon-instruct may have substantial differences from the domain knowledge of NetOps. Therefore, it is necessary to employ domain-specific data for IT to enhance model effectiveness in NetOps field.  </p>
<p>Effect of Chain-of-Thought</p>
<p>As CoT substantially increase the length of both the prompt and the output, which consumes much more computation resources than non-CoT settings, we only evaluate the CoT prompt in selected models. As shown in Figure 8, LLaMA-7B and LLaMA-2-7B obtain 4-5% increase in few-shot accuracy when CoT is applied, while other models do not have positive results. This suggests that few-shot CoT might be more effective on smaller models.</p>
<p>Effect of Retrieval-Augmented Generation</p>
<p>Like CoT prompt, RAG prompt incurs high cost as well. Thus, as shown in Figure 9, we only evaluate 4 models on RAG prompts. We find that for all the evaluated models, RAG consistently improves the zero-shot performance and even achieves better results than few-shot prompts without RAG. This result is consistent with our expectation that RAG enriches the context information with external relevant knowledge and facilitates the model inference.</p>
<p>Impact of Question Language on Accuracy</p>
<p>The distribution of languages in the pre-training data may affect the performance of the pre-trained LLM on multi-lingual tasks. As shown in Figure 10, we compute the model accuracy on English questions and Chinese questions separately to measure the multi-lingual NetOps capability. Except GLM-130B, models pre-trained with Chinese/English bilingual corpus like Baichuan, Moss, and ChatGLM 2 achieve higher accuracy on Chinese questions than on English questions.  percents higher accuracy on Chinese questions than the accuracy on English questions. For other models, the accuracy on Chinese questions is lower than the accuracy on English questions. For GPT-4, the zero-shot accuracy on Chinese questions is 18 percents lower than that on English questions, which suggests that there is still substantial space for improvement on the Chinese NetOps capabilities of LLMs.</p>
<p>Evaluation on Non-multi-choice Questions</p>
<p>As shown in Figure 11, we evaluate 12 models on non-multi-choice questions. We adopt zero-shot prompt in this evaluation. Overall, all the evaluated models do not achieve good performance as their BLEU/ROUGE scores are all below 0.3, which shows that this evaluation set is hard. We find that LLaMA models have better performance than the other evaluated models, including LLaMA 2. It is likely that LLaMA 2 is not as good as LLaMA in closed-book question-answering. For LLaMA-2-7B and Falcon, comparing their base model and the chat/instruct model, we see that the chat/instruct model achieves lower precision but higher recall.  In this report, we present a comprehensive multi-lingual LLM evaluation set for NetOps, named NetEval, and systematically evaluate the NetOps capabilities of 26 widely used LLMs . The results show that only GPT-4 is capable of reaching human-level performance, which reveals both the huge potential and the space for improvement of LLMs in NetOps domain.</p>
<p>In the future, we will keep improving the quantity, quality and diversity of the questions in NetEval, involve more LLMs in the evaluation, and share the results and insights with the community. Furthermore, as NetOps is a subject emphasizing on practice, we are considering adding to NetEval some practical NetOps tasks, which are executed and evaluated in simulated networking environments. </p>
<p>Figure 2 :
2Categories and the Question Distribution of Multi-choice Evaluation Set</p>
<p>Figure 3 :
3Different ways of Construction of Prompts</p>
<p>shot accuracy ( 175B) zero-shot accuracy (gpt-4) trend of zero-shot accuracy few-shot accuracy ( 175B) few-shot accuracy (gpt-4) trend of few-shot accuracy</p>
<p>Figure 4 :
4Model Accuracy (Models with accuracy below 30% are not displayed in thisfigure)</p>
<p>Figure 5 :
5For instance, Moss-moon-sft under zero-shot prompt and Baichuan-13B-base under few-shot prompt achieve 14 to 17 Zero-shot Accuracy of Models with Different Scales within the Same Category</p>
<p>Figure 6 :
6Comparison of Zero-shot and Few-shot Accuracy</p>
<p>Figure 7 :
7Impact of Instruction Tuning with general domain corpus</p>
<p>Figure 8 :
8Results on Chain-of-Thought prompts 6 Conclusion and Future Work</p>
<p>Figure 9 :Figure 10 :Figure 11 :
91011Results Accuracy Results on Non-multi-choice Questions</p>
<p>Answer: All traffic exceeding the CIR is marked discard eligible.Figure 1: Overview of Evaluation Data Collection and the Evaluation MethodsCertification Exam 
Question Bank </p>
<p>Textbooks </p>
<p>Multi-choice </p>
<p>Question-Answering </p>
<p>Filling-blanks </p>
<p>Data Sources 
Question Types 
Evaluation Methods </p>
<p>Zero-Shot Prompt </p>
<p>Few-Shot Prompt </p>
<p>Few-Shot CoT 
Prompt </p>
<p>Retrieval Augmented 
Prompt 
GPT-4 </p>
<p>Filtering </p>
<p>Wikipedia Articles </p>
<p>Technical Standards </p>
<p>Question: Which subnet address is for the 
IP address 172.19.20.23/28? </p>
<p>A. 172.19.20.20 </p>
<p>B. 172.19.20.0 </p>
<p>C. 172.19.20.32 </p>
<p>D. 172.19.20.15 </p>
<p>Answer: A </p>
<p>Question: What occurs on a Frame Relay 
network when the CIR is exceeded? </p>
<p>Question: RIP路由协议每隔<strong><em>_ 
秒进行一次路由更新 
(Translation: The RIP routing 
protocol makes routing updates 
every </em></strong>_ seconds) </p>
<p>Answer: 30 </p>
<p>Vector 
Database </p>
<p>Subject 
Question 
Choices </p>
<p>Network Access 
Which MTU size can cause a baby giant error? 
A. 1500 B. 9216 C. 1600 D. 1518 </p>
<p>IP Connectivity </p>
<p>A. IP routing must be enabled to allow the two hosts to communicate. 
Two hosts are attached to a switch with the default configuration. 
B. The two hosts are in the same broadcast domain. 
Which statement about the configuration is true? 
C. The switch must be configured with a VLAN to allow the two hosts to communicate. 
D. Port security prevents the hosts from connecting to the switch. </p>
<p>IP Services 
Which NAT term is defined as a group of addresses available for NAT use? 
A. NAT pool B. dynamic NAT C. static NAT D. one-way NAT </p>
<p>Security </p>
<p>以下哪个攻击可以提供拦截和修改HTTP数据包功能？ 
A. Metasploit B. Hackbar C. Sqlmap D. Burpsuite 
(Translation: Which of the following attacks is able 
to intercept and modify HTTP packets?) </p>
<p>An administrator is in the process of changing the configuration of a router. 
A. Router# show startup-config 
Automation and 
What command will allow the administrator to check the changes that 
B. Router# show current-config 
Programmability 
have been made prior to saving the new configuration? 
C. Router# show running-config 
D. Router# show memory </p>
<p>Table 1 :
1Example question in each subject </p>
<p>Table 2 :
2Evaluated Pre-trained LLMs</p>
<p>Table 3 :
3Environment for Model Evaluation
https://huggingface.co/datasets/NASP/neteval-exam 2 This is an on-going work, the contents are subject to change in the future.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, arXiv:2009.03300Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprintDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.</p>
<p>Nils Reimers, Iryna Gurevych, arXiv:1908.10084Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprintNils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019.</p>
<p>. Openai, Chatgpt, mar 14 version. large language modelOpenAI. Chatgpt (mar 14 version) [large language model].</p>
<p>Gpt-4 technical report. Openai, Technical reportOpenAI. Gpt-4 technical report. Technical report, 2023.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Naman Baptiste Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Open and efficient foundation language models. arXiv preprintHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288arXiv preprintHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.</p>
<p>The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, Julien Launay, arXiv:2306.01116arXiv preprintGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023.</p>
<p>Clémentine Fourrier. Open llm leaderboard. Nathan Habib Sheon Han Nathan Lambert Nazneen Rajani Omar Sanseviero Lewis Tunstall Thomas Wolf Edward BeechingNathan Habib Sheon Han Nathan Lambert Nazneen Rajani Omar Sanseviero Lewis Tunstall Thomas Wolf Edward Beeching, Clémentine Fourrier. Open llm leaderboard. https://huggingface.co/spaces/ HuggingFaceH4/open_llm_leaderboard, 2023.</p>
<p>Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang, Glm, arXiv:2103.10360General language model pretraining with autoregressive blank infilling. arXiv preprintZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. arXiv preprint arXiv:2103.10360, 2021.</p>
<p>Moss: Training conversational language models from synthetic data. Fudan UniversityFudan University. Moss: Training conversational language models from synthetic data. https://github.com/ OpenLMLab/MOSS, 2023.</p>
<p>. Baichuan Intelligent Technology. Baichuan. Baichuan Intelligent Technology. Baichuan. https://huggingface.co/baichuan-inc, 2023.</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, V Quoc, Denny Le, Zhou, Advances in Neural Information Processing Systems. 35Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022.</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Annual Meeting of the Association for Computational Linguistics. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Annual Meeting of the Association for Computational Linguistics, 2002.</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Annual Meeting of the Association for Computational Linguistics. Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Annual Meeting of the Association for Computational Linguistics, 2004.</p>
<p>Billion-scale similarity search with GPUs. Jeff Johnson, Matthijs Douze, Hervé Jégou, IEEE Transactions on Big Data. 73Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3):535-547, 2019.</p>            </div>
        </div>

    </div>
</body>
</html>
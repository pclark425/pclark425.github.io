<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3511 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3511</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3511</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-5e35895fc4731858f0b286cb5a1613a819cc2367</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5e35895fc4731858f0b286cb5a1613a819cc2367" target="_blank">CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> A diagnostic benchmark suite, named CLUTRR, is introduced to clarify some key issues related to the robustness and systematicity of NLU systems, and highlights a substantial performance gap between state-of-the-art NLU models.</p>
                <p><strong>Paper Abstract:</strong> The recent success of natural language understanding (NLU) systems has been troubled by results highlighting the failure of these models to generalize in a systematic and robust way. In this work, we introduce a diagnostic benchmark suite, named CLUTRR, to clarify some key issues related to the robustness and systematicity of NLU systems. Motivated by the classic work on inductive logic programming, CLUTRR requires that an NLU system infer kinship relations between characters in short stories. Successful performance on this task requires both extracting relationships between entities, as well as inferring the logical rules governing these relationships. CLUTRR allows us to precisely measure a model’s ability for systematic generalization by evaluating on held-out combinations of logical rules, and allows us to evaluate a model’s robustness by adding curated noise facts. Our empirical results highlight a substantial performance gap between state-of-the-art NLU models (e.g., BERT and MAC) and a graph neural network model that works directly with symbolic inputs—with the graph-based model exhibiting both stronger generalization and greater robustness.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3511.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3511.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BiLSTM-Attn</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bidirectional LSTM with Attention</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bi-directional LSTM encoder augmented with an attention mechanism that produces story embeddings from text input; used as an unstructured text-based baseline for CLUTRR.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BiLSTM (with attention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Bidirectional LSTM recurrent neural network encoder that produces token-level representations which are aggregated via an attention mechanism; trained end-to-end on CLUTRR textual stories (no pretrained embeddings).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CLUTRR</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Semi-synthetic kinship reasoning benchmark requiring induction and composition of logical rules from short natural-language stories to predict an unseen kinship relation (multi-step relational/inductive logical reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Trained on CLUTRR with hold-out paraphrases (20%) and hold-out logical clauses (10%); trained on clauses of lengths k={2,3} or k={2,3,4}; Cloze-style anonymization of entities; evaluated with added noise facts (supporting, irrelevant, disconnected).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Clean test (train on clean): 0.58 ± 0.05 accuracy; Average across noise/test variants: 0.61 ± 0.06 accuracy. Performance degrades monotonically as clause length (k) increases (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>GAT (structured graph input): Clean test 1.00 ± 0.00; Average 0.77 ± 0.06.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>No improvement; BiLSTM-Attn underperforms the structured GAT baseline by ~0.39 absolute accuracy on clean tests and ~0.16 on average (GAT substantially better).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Struggles to systematically generalize to unseen combinations of logical rules and to longer multi-step reasoning (accuracy drops as k increases); main failure appears to be mapping diverse unseen natural-language paraphrases into underlying logical facts.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>When train and test use identical paraphrases (removing linguistic generalization), text-based models become competitive with GAT, indicating the principal bottleneck is parsing/linguistic generalization rather than pure symbolic reasoning capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3511.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3511.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BiLSTM-Mean</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bidirectional LSTM with Mean Pooling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bi-directional LSTM encoder whose token representations are aggregated by mean pooling to form a story embedding; used as an unstructured baseline on CLUTRR.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BiLSTM (mean pooling)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Bidirectional LSTM encoder with mean pooling over token outputs to obtain fixed-dimensional story embeddings; trained end-to-end on CLUTRR textual stories without pretrained embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CLUTRR</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Inductive kinship relation inference from natural-language stories (requires composing logical rules and multi-step relational reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Same CLUTRR training regimes as other text models (holdout paraphrases and clauses, variable clause lengths, Cloze anonymization); tested with added noise paths (supporting, irrelevant, disconnected).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Clean test (train on clean): 0.53 ± 0.05 accuracy; Average across noise/test variants: 0.59 ± 0.06 accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>GAT (structured graph input): Clean 1.00 ± 0.00; Average 0.77 ± 0.06.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>No improvement; underperforms GAT by large margin (≈0.47 absolute on clean tests).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Same limitations as other unstructured text models: poor systematic generalization to unseen rule combinations and longer reasoning chains; sensitive to linguistic variation in paraphrases.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Performance improves substantially (becomes more competitive with GAT) when the same paraphrases are used in train and test splits, indicating parsing/linguistic variation is the dominant issue.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3511.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3511.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relation Network (RN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural module designed to bias networks toward relational reasoning by explicitly computing pairwise relations between object representations; used as a text-based reasoning baseline on CLUTRR.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Relation Network (RN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A model that forms pairwise combinations of representation vectors (here from tokens/entities in the story) and processes them with an MLP to capture relational patterns; adapted to operate on CLUTRR text inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CLUTRR</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multi-step relational/inductive reasoning about kinship relations expressed in natural language; requires composing binary predicates (logical rules) to infer unseen relations.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Trained on CLUTRR with held-out paraphrases/clauses; evaluated on varying k and with added noise facts. No additional symbolic interface; works from raw text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Clean test (train on clean): 0.49 ± 0.06 accuracy; Average across noise/test variants: 0.54 ± 0.07 accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>GAT (structured): Clean 1.00 ± 0.00; Average 0.77 ± 0.06.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>No improvement; RN performs substantially worse than the structured GAT baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Fails to reliably induce and compose unseen logical rule combinations from text; performance limited by natural language variation and generalization to longer reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Like other text-based models, RN becomes more competitive when trained and tested on identical paraphrases, highlighting parsing/linguistic-generalization as the key bottleneck rather than relational inductive machinery per se.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3511.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3511.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Compositional Attention Network (MAC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural architecture with iterative MAC cells that perform multi-step attention-based reasoning; evaluated as a text-based reasoning model on CLUTRR.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MAC (Compositional Attention Network)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A compositional, multi-step reasoning architecture (MAC cells) that iteratively attends over the input to perform reasoning steps; adapted to encode CLUTRR stories for relation prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CLUTRR</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Inductive kinship inference from narrative text, requiring multiple reasoning steps and composition of logical rules.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Trained under CLUTRR protocol (hold-out paraphrases/clauses, varying clause lengths), tested with/without added noise facts. No symbolic graph input used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Clean test (train on clean): 0.63 ± 0.06 accuracy; Average across noise/test variants: 0.61 ± 0.06 accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>GAT (structured): Clean 1.00 ± 0.00; Average 0.77 ± 0.06.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>No improvement; MAC is the strongest among some text models in clean setting but still notably below GAT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Although MAC has inductive biases for multi-step reasoning, it still struggles with linguistic generalization and unseen combinations of logical clauses; accuracy falls with increasing clause length.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Training on larger clause-set (k up to 4) improves performance for all models (including MAC), but GAT benefits most. Textual-paraphrase overlap experiments show parsing remains a main obstacle.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3511.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3511.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT (base, pretrained)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large pretrained bidirectional Transformer language model used as a text encoder baseline on CLUTRR, without task-specific structural input.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bert: Pre-training of deep bidirectional transformers for language understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (pretrained transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained deep bidirectional Transformer (Devlin et al.) used to embed tokens from CLUTRR stories; in experiments BERT is used directly (and also in a modified BERT-LSTM variant); no model size is specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CLUTRR</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Text-based inductive kinship reasoning requiring induction and composition of logical rules from short narratives.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Used pretrained BERT embeddings; also evaluated a BERT-LSTM variant (BERT embeddings followed by trainable LSTM encoder). Training regimes include holdout paraphrases/clauses and noise additions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Clean test (train on clean): 0.37 ± 0.06 accuracy; Average across noise/test variants: 0.30 ± 0.07 accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>GAT (structured): Clean 1.00 ± 0.00; Average 0.77 ± 0.06.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>No improvement; BERT underperforms other text-based models (notably BERT-LSTM and MAC) and is far below structured GAT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>BERT, despite strong pretrained language capabilities, performed poorly on CLUTRR, suggesting off-the-shelf pretrained representations do not suffice for mapping diverse narratives into the required logical facts. BERT-based models also did not benefit from added supporting/irrelevant facts (contrasting with some unstructured baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Adding a trainable LSTM on top of BERT embeddings (BERT-LSTM) improved performance, indicating that additional task-specific sequence modeling helps; however, the dominant failure mode remains linguistic-to-logical mapping rather than lack of pretrained knowledge alone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3511.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3511.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT-LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT with a trainable LSTM encoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid model that uses pretrained BERT embeddings followed by a trainable LSTM encoder to better adapt to CLUTRR's reasoning-from-text demands; top-performing text model in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Model that takes pretrained BERT token embeddings and feeds them into a trainable LSTM encoder (trained on CLUTRR) before predicting relations; designed to combine pretrained contextual embeddings with task-specific sequence modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CLUTRR</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Inductive reasoning over kinship facts expressed in natural language, requiring detection of relations and composing logical rules across sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Pretrained BERT embeddings + trainable LSTM; trained with CLUTRR protocols (holdout paraphrases/clauses, varying k, noise additions); Cloze anonymization used for entity placeholders.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Clean test (train on clean): 0.67 ± 0.05 accuracy (best among text-based models in clean setting); Average across noise/test variants: 0.56 ± 0.05 accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>GAT (structured): Clean 1.00 ± 0.00; Average 0.77 ± 0.06.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Improves over vanilla BERT and many other unstructured models in clean settings but remains substantially worse than GAT (≈0.33 gap on clean tests).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Still fails to reach the robustness and systematic generalization of GAT; performance declines on longer reasoning chains and unseen logical clause combinations; benefits from pretrained embeddings but not sufficient for full logical generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>The addition of an LSTM on top of BERT embeddings yielded the best text-model performance, showing that task-specific sequence modeling aids performance; however, experiments where train/test paraphrases matched show that linguistic generalization is the main barrier.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3511.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3511.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GAT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Attention Network (GAT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured graph neural network (with attention) that receives symbolic graph input of facts (rather than raw text) and performs relational reasoning; achieves the best performance on CLUTRR.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph Attention Networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Graph Attention Network (GAT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Graph neural network employing attention mechanisms over graph neighbors to aggregate relational information; in CLUTRR experiments the GAT is provided direct access to the symbolic kinship graph (grounded facts) rather than natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CLUTRR</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Inductive multi-step logical reasoning about kinship relations, performed over structured symbolic graphs representing grounded predicates; requires composing logical rules/path reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Provided explicit structured graph input (constructed from backbone graph and backward-chaining) instead of textual stories; trained on same CLUTRR splits including holdout clauses and noisy examples (supporting, irrelevant, disconnected).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Clean test (train on clean): 1.00 ± 0.00 accuracy (near-perfect on held-out logical clauses of length k=3); Average across noise/test variants: 0.77 ± 0.06 accuracy. Performance degrades less rapidly with increasing clause length compared to text models; benefits most from training on larger clause sets (k up to 4).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Text-based models (examples): BERT-LSTM clean 0.67 ± 0.05, MAC clean 0.63 ± 0.06, BiLSTM-Attn clean 0.58 ± 0.05, RN clean 0.49 ± 0.06.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Substantially better than all unstructured text baselines (e.g., +0.33 over best text model BERT-LSTM on clean tests). Also only model that consistently improved when trained on noisy examples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Sensitive to supporting facts that introduce alternative paths/cycles in the graph (performance drops when supporting facts added without training on them), indicating vulnerability to cyclic/ambiguous graph structures; however robust to disconnected noise facts. Requires symbolic/structured input, which sidesteps the linguistic parsing challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Analysis shows GAT's superior performance stems from direct access to structured facts (removing language-parsing bottleneck); when text-models are given identical paraphrases in train/test they become competitive, confirming that parsing is the main limiting factor for text models. Training on expanded clause-lengths yields largest gains for GAT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Bert: Pre-training of deep bidirectional transformers for language understanding <em>(Rating: 2)</em></li>
                <li>Graph Attention Networks <em>(Rating: 2)</em></li>
                <li>Compositional attention networks for machine reasoning <em>(Rating: 2)</em></li>
                <li>A simple neural network module for relational reasoning <em>(Rating: 2)</em></li>
                <li>End-to-end differentiable proving <em>(Rating: 1)</em></li>
                <li>Inductive logic programming <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3511",
    "paper_id": "paper-5e35895fc4731858f0b286cb5a1613a819cc2367",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "BiLSTM-Attn",
            "name_full": "Bidirectional LSTM with Attention",
            "brief_description": "A bi-directional LSTM encoder augmented with an attention mechanism that produces story embeddings from text input; used as an unstructured text-based baseline for CLUTRR.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "BiLSTM (with attention)",
            "model_description": "Bidirectional LSTM recurrent neural network encoder that produces token-level representations which are aggregated via an attention mechanism; trained end-to-end on CLUTRR textual stories (no pretrained embeddings).",
            "model_size": null,
            "reasoning_task_name": "CLUTRR",
            "reasoning_task_description": "Semi-synthetic kinship reasoning benchmark requiring induction and composition of logical rules from short natural-language stories to predict an unseen kinship relation (multi-step relational/inductive logical reasoning).",
            "method_or_intervention": "Trained on CLUTRR with hold-out paraphrases (20%) and hold-out logical clauses (10%); trained on clauses of lengths k={2,3} or k={2,3,4}; Cloze-style anonymization of entities; evaluated with added noise facts (supporting, irrelevant, disconnected).",
            "performance": "Clean test (train on clean): 0.58 ± 0.05 accuracy; Average across noise/test variants: 0.61 ± 0.06 accuracy. Performance degrades monotonically as clause length (k) increases (qualitative).",
            "baseline_performance": "GAT (structured graph input): Clean test 1.00 ± 0.00; Average 0.77 ± 0.06.",
            "improvement_over_baseline": "No improvement; BiLSTM-Attn underperforms the structured GAT baseline by ~0.39 absolute accuracy on clean tests and ~0.16 on average (GAT substantially better).",
            "limitations_or_failures": "Struggles to systematically generalize to unseen combinations of logical rules and to longer multi-step reasoning (accuracy drops as k increases); main failure appears to be mapping diverse unseen natural-language paraphrases into underlying logical facts.",
            "ablation_or_analysis": "When train and test use identical paraphrases (removing linguistic generalization), text-based models become competitive with GAT, indicating the principal bottleneck is parsing/linguistic generalization rather than pure symbolic reasoning capacity.",
            "uuid": "e3511.0",
            "source_info": {
                "paper_title": "CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "BiLSTM-Mean",
            "name_full": "Bidirectional LSTM with Mean Pooling",
            "brief_description": "A bi-directional LSTM encoder whose token representations are aggregated by mean pooling to form a story embedding; used as an unstructured baseline on CLUTRR.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "BiLSTM (mean pooling)",
            "model_description": "Bidirectional LSTM encoder with mean pooling over token outputs to obtain fixed-dimensional story embeddings; trained end-to-end on CLUTRR textual stories without pretrained embeddings.",
            "model_size": null,
            "reasoning_task_name": "CLUTRR",
            "reasoning_task_description": "Inductive kinship relation inference from natural-language stories (requires composing logical rules and multi-step relational reasoning).",
            "method_or_intervention": "Same CLUTRR training regimes as other text models (holdout paraphrases and clauses, variable clause lengths, Cloze anonymization); tested with added noise paths (supporting, irrelevant, disconnected).",
            "performance": "Clean test (train on clean): 0.53 ± 0.05 accuracy; Average across noise/test variants: 0.59 ± 0.06 accuracy.",
            "baseline_performance": "GAT (structured graph input): Clean 1.00 ± 0.00; Average 0.77 ± 0.06.",
            "improvement_over_baseline": "No improvement; underperforms GAT by large margin (≈0.47 absolute on clean tests).",
            "limitations_or_failures": "Same limitations as other unstructured text models: poor systematic generalization to unseen rule combinations and longer reasoning chains; sensitive to linguistic variation in paraphrases.",
            "ablation_or_analysis": "Performance improves substantially (becomes more competitive with GAT) when the same paraphrases are used in train and test splits, indicating parsing/linguistic variation is the dominant issue.",
            "uuid": "e3511.1",
            "source_info": {
                "paper_title": "CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "RN",
            "name_full": "Relation Network (RN)",
            "brief_description": "A neural module designed to bias networks toward relational reasoning by explicitly computing pairwise relations between object representations; used as a text-based reasoning baseline on CLUTRR.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Relation Network (RN)",
            "model_description": "A model that forms pairwise combinations of representation vectors (here from tokens/entities in the story) and processes them with an MLP to capture relational patterns; adapted to operate on CLUTRR text inputs.",
            "model_size": null,
            "reasoning_task_name": "CLUTRR",
            "reasoning_task_description": "Multi-step relational/inductive reasoning about kinship relations expressed in natural language; requires composing binary predicates (logical rules) to infer unseen relations.",
            "method_or_intervention": "Trained on CLUTRR with held-out paraphrases/clauses; evaluated on varying k and with added noise facts. No additional symbolic interface; works from raw text.",
            "performance": "Clean test (train on clean): 0.49 ± 0.06 accuracy; Average across noise/test variants: 0.54 ± 0.07 accuracy.",
            "baseline_performance": "GAT (structured): Clean 1.00 ± 0.00; Average 0.77 ± 0.06.",
            "improvement_over_baseline": "No improvement; RN performs substantially worse than the structured GAT baseline.",
            "limitations_or_failures": "Fails to reliably induce and compose unseen logical rule combinations from text; performance limited by natural language variation and generalization to longer reasoning chains.",
            "ablation_or_analysis": "Like other text-based models, RN becomes more competitive when trained and tested on identical paraphrases, highlighting parsing/linguistic-generalization as the key bottleneck rather than relational inductive machinery per se.",
            "uuid": "e3511.2",
            "source_info": {
                "paper_title": "CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "MAC",
            "name_full": "Compositional Attention Network (MAC)",
            "brief_description": "A neural architecture with iterative MAC cells that perform multi-step attention-based reasoning; evaluated as a text-based reasoning model on CLUTRR.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MAC (Compositional Attention Network)",
            "model_description": "A compositional, multi-step reasoning architecture (MAC cells) that iteratively attends over the input to perform reasoning steps; adapted to encode CLUTRR stories for relation prediction.",
            "model_size": null,
            "reasoning_task_name": "CLUTRR",
            "reasoning_task_description": "Inductive kinship inference from narrative text, requiring multiple reasoning steps and composition of logical rules.",
            "method_or_intervention": "Trained under CLUTRR protocol (hold-out paraphrases/clauses, varying clause lengths), tested with/without added noise facts. No symbolic graph input used.",
            "performance": "Clean test (train on clean): 0.63 ± 0.06 accuracy; Average across noise/test variants: 0.61 ± 0.06 accuracy.",
            "baseline_performance": "GAT (structured): Clean 1.00 ± 0.00; Average 0.77 ± 0.06.",
            "improvement_over_baseline": "No improvement; MAC is the strongest among some text models in clean setting but still notably below GAT.",
            "limitations_or_failures": "Although MAC has inductive biases for multi-step reasoning, it still struggles with linguistic generalization and unseen combinations of logical clauses; accuracy falls with increasing clause length.",
            "ablation_or_analysis": "Training on larger clause-set (k up to 4) improves performance for all models (including MAC), but GAT benefits most. Textual-paraphrase overlap experiments show parsing remains a main obstacle.",
            "uuid": "e3511.3",
            "source_info": {
                "paper_title": "CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "BERT",
            "name_full": "BERT (base, pretrained)",
            "brief_description": "Large pretrained bidirectional Transformer language model used as a text encoder baseline on CLUTRR, without task-specific structural input.",
            "citation_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "mention_or_use": "use",
            "model_name": "BERT (pretrained transformer)",
            "model_description": "Pretrained deep bidirectional Transformer (Devlin et al.) used to embed tokens from CLUTRR stories; in experiments BERT is used directly (and also in a modified BERT-LSTM variant); no model size is specified in this paper.",
            "model_size": null,
            "reasoning_task_name": "CLUTRR",
            "reasoning_task_description": "Text-based inductive kinship reasoning requiring induction and composition of logical rules from short narratives.",
            "method_or_intervention": "Used pretrained BERT embeddings; also evaluated a BERT-LSTM variant (BERT embeddings followed by trainable LSTM encoder). Training regimes include holdout paraphrases/clauses and noise additions.",
            "performance": "Clean test (train on clean): 0.37 ± 0.06 accuracy; Average across noise/test variants: 0.30 ± 0.07 accuracy.",
            "baseline_performance": "GAT (structured): Clean 1.00 ± 0.00; Average 0.77 ± 0.06.",
            "improvement_over_baseline": "No improvement; BERT underperforms other text-based models (notably BERT-LSTM and MAC) and is far below structured GAT.",
            "limitations_or_failures": "BERT, despite strong pretrained language capabilities, performed poorly on CLUTRR, suggesting off-the-shelf pretrained representations do not suffice for mapping diverse narratives into the required logical facts. BERT-based models also did not benefit from added supporting/irrelevant facts (contrasting with some unstructured baselines).",
            "ablation_or_analysis": "Adding a trainable LSTM on top of BERT embeddings (BERT-LSTM) improved performance, indicating that additional task-specific sequence modeling helps; however, the dominant failure mode remains linguistic-to-logical mapping rather than lack of pretrained knowledge alone.",
            "uuid": "e3511.4",
            "source_info": {
                "paper_title": "CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "BERT-LSTM",
            "name_full": "BERT with a trainable LSTM encoder",
            "brief_description": "A hybrid model that uses pretrained BERT embeddings followed by a trainable LSTM encoder to better adapt to CLUTRR's reasoning-from-text demands; top-performing text model in some settings.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "BERT-LSTM",
            "model_description": "Model that takes pretrained BERT token embeddings and feeds them into a trainable LSTM encoder (trained on CLUTRR) before predicting relations; designed to combine pretrained contextual embeddings with task-specific sequence modeling.",
            "model_size": null,
            "reasoning_task_name": "CLUTRR",
            "reasoning_task_description": "Inductive reasoning over kinship facts expressed in natural language, requiring detection of relations and composing logical rules across sentences.",
            "method_or_intervention": "Pretrained BERT embeddings + trainable LSTM; trained with CLUTRR protocols (holdout paraphrases/clauses, varying k, noise additions); Cloze anonymization used for entity placeholders.",
            "performance": "Clean test (train on clean): 0.67 ± 0.05 accuracy (best among text-based models in clean setting); Average across noise/test variants: 0.56 ± 0.05 accuracy.",
            "baseline_performance": "GAT (structured): Clean 1.00 ± 0.00; Average 0.77 ± 0.06.",
            "improvement_over_baseline": "Improves over vanilla BERT and many other unstructured models in clean settings but remains substantially worse than GAT (≈0.33 gap on clean tests).",
            "limitations_or_failures": "Still fails to reach the robustness and systematic generalization of GAT; performance declines on longer reasoning chains and unseen logical clause combinations; benefits from pretrained embeddings but not sufficient for full logical generalization.",
            "ablation_or_analysis": "The addition of an LSTM on top of BERT embeddings yielded the best text-model performance, showing that task-specific sequence modeling aids performance; however, experiments where train/test paraphrases matched show that linguistic generalization is the main barrier.",
            "uuid": "e3511.5",
            "source_info": {
                "paper_title": "CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "GAT",
            "name_full": "Graph Attention Network (GAT)",
            "brief_description": "A structured graph neural network (with attention) that receives symbolic graph input of facts (rather than raw text) and performs relational reasoning; achieves the best performance on CLUTRR.",
            "citation_title": "Graph Attention Networks",
            "mention_or_use": "use",
            "model_name": "Graph Attention Network (GAT)",
            "model_description": "Graph neural network employing attention mechanisms over graph neighbors to aggregate relational information; in CLUTRR experiments the GAT is provided direct access to the symbolic kinship graph (grounded facts) rather than natural language.",
            "model_size": null,
            "reasoning_task_name": "CLUTRR",
            "reasoning_task_description": "Inductive multi-step logical reasoning about kinship relations, performed over structured symbolic graphs representing grounded predicates; requires composing logical rules/path reasoning.",
            "method_or_intervention": "Provided explicit structured graph input (constructed from backbone graph and backward-chaining) instead of textual stories; trained on same CLUTRR splits including holdout clauses and noisy examples (supporting, irrelevant, disconnected).",
            "performance": "Clean test (train on clean): 1.00 ± 0.00 accuracy (near-perfect on held-out logical clauses of length k=3); Average across noise/test variants: 0.77 ± 0.06 accuracy. Performance degrades less rapidly with increasing clause length compared to text models; benefits most from training on larger clause sets (k up to 4).",
            "baseline_performance": "Text-based models (examples): BERT-LSTM clean 0.67 ± 0.05, MAC clean 0.63 ± 0.06, BiLSTM-Attn clean 0.58 ± 0.05, RN clean 0.49 ± 0.06.",
            "improvement_over_baseline": "Substantially better than all unstructured text baselines (e.g., +0.33 over best text model BERT-LSTM on clean tests). Also only model that consistently improved when trained on noisy examples.",
            "limitations_or_failures": "Sensitive to supporting facts that introduce alternative paths/cycles in the graph (performance drops when supporting facts added without training on them), indicating vulnerability to cyclic/ambiguous graph structures; however robust to disconnected noise facts. Requires symbolic/structured input, which sidesteps the linguistic parsing challenge.",
            "ablation_or_analysis": "Analysis shows GAT's superior performance stems from direct access to structured facts (removing language-parsing bottleneck); when text-models are given identical paraphrases in train/test they become competitive, confirming that parsing is the main limiting factor for text models. Training on expanded clause-lengths yields largest gains for GAT.",
            "uuid": "e3511.6",
            "source_info": {
                "paper_title": "CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text",
                "publication_date_yy_mm": "2019-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "rating": 2
        },
        {
            "paper_title": "Graph Attention Networks",
            "rating": 2
        },
        {
            "paper_title": "Compositional attention networks for machine reasoning",
            "rating": 2
        },
        {
            "paper_title": "A simple neural network module for relational reasoning",
            "rating": 2
        },
        {
            "paper_title": "End-to-end differentiable proving",
            "rating": 1
        },
        {
            "paper_title": "Inductive logic programming",
            "rating": 1
        }
    ],
    "cost": 0.01459225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text</h1>
<p>Koustuv Sinha ${ }^{1,3,4}$, Shagun Sodhani ${ }^{2,3}$, Jin Dong ${ }^{1,3}$, Joelle Pineau ${ }^{1,3,4}$ and William L. Hamilton ${ }^{1,3,4}$<br>${ }^{1}$ School of Computer Science, McGill University, Canada<br>${ }^{2}$ Université de Montréal, Canada<br>${ }^{3}$ Montreal Institute of Learning Algorithms (Mila), Canada<br>${ }^{4}$ Facebook AI Research (FAIR), Montreal, Canada<br>{koustuv.sinha, sshagunsodhani, jin.dong, jpineau, wlh} @{mail.mcgill.ca, gmail.com, mail.mcgill.ca, cs.mcgill.ca, cs.mcgill.ca}</p>
<h2>Abstract</h2>
<p>The recent success of natural language understanding (NLU) systems has been troubled by results highlighting the failure of these models to generalize in a systematic and robust way. In this work, we introduce a diagnostic benchmark suite, named CLUTRR, to clarify some key issues related to the robustness and systematicity of NLU systems. Motivated by classic work on inductive logic programming, CLUTRR requires that an NLU system infer kinship relations between characters in short stories. Successful performance on this task requires both extracting relationships between entities, as well as inferring the logical rules governing these relationships. CLUTRR allows us to precisely measure a model's ability for systematic generalization by evaluating on held-out combinations of logical rules, and it allows us to evaluate a model's robustness by adding curated noise facts. Our empirical results highlight a substantial performance gap between state-of-the-art NLU models (e.g., BERT and MAC) and a graph neural network model that works directly with symbolic inputs-with the graph-based model exhibiting both stronger generalization and greater robustness.</p>
<h2>1 Introduction</h2>
<p>Natural language understanding (NLU) systems have been extremely successful at reading comprehension tasks, such as question answering (QA) and natural language inference (NLI). An array of existing datasets are available for these tasks. This includes datasets that test a system's ability to extract factual answers from text (Rajpurkar et al., 2016; Nguyen et al., 2016; Trischler et al., 2016; Mostafazadeh et al., 2016; Su et al., 2016), as well as datasets that emphasize commonsense inference, such as entailment between sentences (Bowman et al., 2015; Williams et al., 2018).
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: CLUTRR inductive reasoning task.
However, there are growing concerns regarding the ability of NLU systems-and neural networks more generally-to generalize in a systematic and robust way (Bahdanau et al., 2019; Lake and Baroni, 2018; Johnson et al., 2017). For instance, recent work has highlighted the brittleness of NLU systems to adversarial examples (Jia and Liang, 2017), as well as the fact that NLU models tend to exploit statistical artifacts in datasets, rather than exhibiting true reasoning and generalization capabilities (Gururangan et al., 2018; Kaushik and Lipton, 2018). These findings have also dovetailed with the recent dominance of large pre-trained language models, such as BERT, on NLU benchmarks (Devlin et al., 2018; Peters et al., 2018), which suggest that the primary difficulty in these datasets is incorporating the statistics of the natural language, rather than reasoning.</p>
<p>An important challenge is thus to develop NLU benchmarks that can precisely test a model's capability for robust and systematic generalization. Ideally, we want language understanding systems that can not only answer questions and draw inferences from text, but that can also do so in a systematic, logical, and robust way. While such reasoning capabilities are certainly required for many existing NLU tasks, most datasets combine several challenges of language understanding into one, such as co-reference/entity resolution, incorporating world knowledge, and semantic parsing-making it difficult to isolate and diagnose a model's capabilities for systematic generalization and robustness.</p>
<p>Our work. Inspired by the classic AI challenge of inductive logic programming (Quinlan, 1990)as well as the recently developed CLEVR dataset for visual reasoning (Johnson et al., 2017)—we propose a semi-synthetic benchmark designed to explicitly test an NLU model's ability for systematic and robust logical generalization.</p>
<p>Our benchmark suite-termed CLUTRR (Compositional Language Understanding and Text-based Relational Reasoning)-contains a large set of semi-synthetic stories involving hypothetical families. Given a story, the goal is to infer the relationship between two family members, whose relationship is not explicitly mentioned (Figure 1). To solve this task, a learning agent must extract the relationships mentioned in the text, induce the logical rules governing the kinship relationships (e.g., the transitivity of the sibling relation), and use these rules to infer the relationship between a given pair of entities. Crucially, the CLUTRR benchmark allows us to test a learning agent's ability for systematic generalization by testing on stories that contain unseen combinations of logical rules. CLUTRR also allows us to precisely test for the various forms of model robustness by adding different kinds of superfluous noise facts to the stories.</p>
<p>We compare the performance of several state-of-the-art NLU systems on this task-including Relation Networks (Santoro et al., 2017), Compositional Attention Networks (Hudson and Manning, 2018) and BERT (Devlin et al., 2018). We find that the generalization ability of these NLU systems is substantially below that of a Graph Attention Network (Veličković et al., 2018), which is given direct access to symbolic representations of the stories. Moreover, we find that the robustness of the NLU systems generally does not improve by training on noisy data, whereas the GAT model is able to effectively learn robust reasoning strategies by training on noisy examples. Both of these results highlight important open challenges for closing the gap between machine reasoning models that work with unstructured text and models that are given access to more structured input.</p>
<h2>2 Related Work</h2>
<p>We draw inspiration from the classic work on inductive logic programming (ILP), a long line of reading comprehension benchmarks in NLP, as well as work combining language and knowledge graphs.</p>
<p>Reading comprehension benchmarks. Many datasets have been proposed to test the reading comprehension ability of NLP systems. This includes the SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2016), and MCTest (Richardson et al., 2013) benchmarks that focus on factual questions; the SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018) benchmarks for sentence understanding; and the bABI tasks (Weston et al., 2015), to name a few. Our primary contribution to this line of work is the development of a carefully designed diagnostic benchmark to evaluate model robustness and systematic generalization in the context of NLU.
Question-answering with knowledge graphs. Our work is also related to the domain of question answering and reasoning in knowledge graphs (Das et al., 2018; Xiong et al., 2018; Hamilton et al., 2018; Wang et al., 2018; Xiong et al., 2017; Welbl et al., 2018; Kartsaklis et al., 2018), where either the model is provided with a knowledge graph to perform inference over or where the model must infer a knowledge graph from the text itself. However, unlike previous benchmarks in this domain-which are generally transductive and focus on leveraging and extracting knowledge graphs as a source of background knowledge about a fixed set of entities-CLUTRR requires inductive logical reasoning, where every example requires reasoning over a new set of previously unseen entities.</p>
<h2>3 Benchmark Design</h2>
<p>In order to design an NLU benchmark that explicitly tests inductive reasoning and systematic generalization, we build upon the classic ILP task of inferring family (i.e., kinship) relations (Hinton et al., 1986; Muggleton, 1991; Lavrac and Dzeroski, 1994; Kok and Domingos, 2007; Rocktäschel and Riedel, 2017). For example, given the facts that "Alice is Bob's mother" and "Jim is Alice's father", one can infer with reasonable certainty that "Jim is Bob's grandfather." While this example may appear trivial, it is a challenging task to design models that can learn from data to induce the logical rules necessary to make such inferences, and it is even more challenging to design models that can systematically generalize by composing these induced rules.</p>
<p>Inspired by this classic task of logical induction and reasoning, the CLUTRR benchmark requires an NLU system to infer and reason about kinship</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Data generation pipeline. Step 1: generate a kinship graph. Step 2: sample a target fact. Step 3: Use backward chaining to sample a set of facts. Step 4: Convert sampled facts to a natural language story.
relations by reading short stories. Requiring that the models learn directly from natural language makes this task much more challenging than the purely symbolic ILP setting. However, we leverage insights from traditional ILP to generate these stories in a semi-synthetic manner, providing precise control over the complexity of the reasoning required to solve the task.</p>
<p>In its entirety, the CLUTRR benchmark suite allows researchers to generate diverse semi-synthetic short stories to test different aspects of inductive reasoning capabilities. We publicly release the entire benchmark suite, including code to generate the semi-synthetic examples, the specific datasets that we introduce here, and the different baselines that we compare with. ${ }^{1}$</p>
<h3>3.1 Overview of data generation process</h3>
<p>The core idea behind the CLUTRR benchmark suite is the following: Given a natural language story describing a set of kinship relations, the goal is to infer the relationship between two entities, whose relationship is not explicitly stated in the story. To generate these stories, we first design a knowledge base (KB) with rules specifying how kinship relations resolve, and we use the following steps to create semi-synthetic stories based on this knowledge base:
Step 1. Generate a random kinship graph that satisfies the rules in our KB.</p>
<p>Step 2. Sample a target fact (i.e., relation) to predict from the kinship graph.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Step 3. Apply backward chaining to sample a set of facts that can prove the target relation (and optionally sample a set of "distracting" or "irrelevant" noise facts).</p>
<p>Step 4. Convert the sampled facts into a natural language story through pre-specified text templates and crowd-sourced paraphrasing.
Figure 2 provides a high-level overview of this idea, and the following subsections describe the data generation process in detail, as well as the diagnostic flexibility afforded by CLUTRR.</p>
<h3>3.2 Story generation</h3>
<p>The short stories in CLUTRR are essentially narrativized renderings of a set of logical facts. In this section, we describe how we sample the logical facts that make up a story by generating random kinship graphs and using backward chaining to produce logical reasoning chains. The conversion from logical facts to natural language narratives is then described in Section 3.3.
Terminology and background. Following standard practice in formal semantics, we use the term atom to refer to a predicate symbol and a list of terms, such as [grandfatherOf, $X, Y$ ], where the predicate grandfatherOf denotes the relation between the two variables, $X$ and $Y$. We restrict the predicates to have an arity of 2, i.e., binary predicates. A logical rule in this setting is of the form $\mathcal{H} \vdash \mathcal{B}$, where $\mathcal{B}$ is the body of the rule, i.e., a conjunction of two atoms $\left(\left[\alpha_{1}, \alpha_{2}\right]\right)$ and $\mathcal{H}$ is the head, i.e., a single atom ( $\alpha$ ) that can be viewed as the goal or query. For instance, given a knowledge base (KB) $R$ that contains the single rule [grandfatherOf, $X, Y$ ] $\vdash$ [[fatherOf, $X, Z]$, [fatherOf, $Z, Y]$ ], the query [grandfatherOf, $X, Y$ ] evaluates to true if and only if the body $\mathcal{B}=[[$ fatherOf, $X, Z]$, [fatherOf, $Z, Y]]$ is also true in a given world. A rule is called a grounded rule if all atoms in the rule are themselves grounded, i.e., all variables are replaced with constants or entities in a world. A fact is a grounded binary predicate. A clause is a conjunction of two or more atoms $\left(\mathcal{C}=\left(\mathcal{H}<em _mathcal_C="\mathcal{C">{\mathcal{C}} \vdash \mathcal{B}</em>\right]\right)\right)\right)$ which can be built using a set of rules.}}=\left(\left[\alpha_{1}, \ldots, \alpha_{n</p>
<p>In the context of our data generation process, we distinguish between the knowledge base, $R$, which contains a finite number of predicates and rules specifying how kinship relations in a family resolve, and a particular kinship graph $G$, which</p>
<p>contains a grounded set of atoms specifying the particular kinship relations that underlie a single story. In other words, $R$ contains the logical rules that govern all the generated stories in CLUTRR, while $G$ contains the grounded facts that underlie a specific story.
Graph generation. To generate the kinship graph $G$ underlying a particular story, we first sample a set of gendered ${ }^{2}$ entities and kinship relations using a stochastic generation process. This generation process contains a number of tunable parameterssuch as the maximum number of children at each node, the probability of an entity being married to another entity, etc.-and is designed to produce a valid, but possibly incomplete "backbone graph". For instance, this backbone graph generation process will specify "parent"/"child" relations between entities but does not add "grandparent" relations. After this initial generation process, we recursively apply the logical rules in $R$ to the backbone graph to produce a final graph $G$ that contains the full set of kinship relations between all the entities.
Backward chaining. The resulting graph $G$ provides the background knowledge for a specific story, as each edge in this graph can be treated as a grounded predicate (i.e., fact) between two entities. From this graph $G$, we sample the facts that make up the story, as well as the target fact that we seek to predict: First, we (uniformly) sample a target relation $\mathcal{H}<em _mathcal_C="\mathcal{C">{\mathcal{C}}$, which is the fact that we want to predict from the story. Then, from this target relation $\mathcal{H}</em>}}$, we run a simple variation of the backward chaining (Gallaire and Minker, 1978) algorithm for $k$ iterations starting from $\mathcal{H<em _mathcal_C="\mathcal{C">{\mathcal{C}}$, where at each iteration we uniformly sample a subgoal to resolve and then uniformly sample a KB rule that resolves this subgoal. Crucially, unlike traditional backward chaining, we do not stop the algorithm when a proof is obtained; instead, we run for a fixed number of iterations $k$ in order to sample a set of $k$ facts $\mathcal{B}</em>$.}}$ that imply the target relation $\mathcal{H}_{\mathcal{C}</p>
<h3>3.3 Adding natural language</h3>
<p>So far, we have described the process of generating a conjunctive logical clause $\mathcal{C}=\left(\mathcal{H}<em _mathcal_C="\mathcal{C">{\mathcal{C}} \vdash \mathcal{B}</em>}}\right)$, where $\mathcal{H<em _mathcal_C="\mathcal{C">{\mathcal{C}}=\left[\alpha^{*}\right]$ is the target fact (i.e., relation) we seek to predict and $\mathcal{B}</em>\right]$ is the set of supporting facts that imply the target relation. We now describe how we convert this logical represen-}}=\left[\alpha_{1}, \ldots, \alpha_{k</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Illustration of how a set of facts can split and combined in various ways across sentences.
tation to natural language through crowd-sourcing. Paraphrasing using Amazon Mechanical Turk. The basic idea behind our approach is that we show Amazon Mechanical Turk (AMT) crowd-workers the set of facts $\mathcal{B}<em _mathcal_C="\mathcal{C">{\mathcal{C}}$ corresponding to a story and ask the workers to paraphrase these facts into a narrative. Since workers are given a set of facts $\mathcal{B}</em>\right|$ grows. This combinatorial explosion for large $k$-combined with the difficulty of maintaining the quality of the crowd-sourced paraphrasing for long stories-makes it infeasible to obtain a large number of paraphrased examples for $k&gt;3$. To circumvent this issue and increase the flexibility of our benchmark, we reuse and compose AMT paraphrases to generate longer stories. In particular, we collected paraphrases for stories containing $k=1,2,3$ supporting facts and then replaced the entities from these collected stories with placeholders in order to re-use them to generate longer semi-synthetic stories. An example of a story generated by stitching together two shorter paraphrases is provided below:
[Frank] went to the park with his father, [Brett].
[Frank] called his brother [Boyd] on the phone.
He wanted to go out for some beers. [Boyd] went
to the baseball game with his son [Jim].
Q: What is [Brett] and [Jim]'s relationship?
Thus, instead of simply collecting paraphrases for a fixed number of stories, we instead obtain a diverse}}$ to work from, they are able to combine and split multiple facts across separate sentences and construct diverse narratives (Figure 3). Appendix 1.6 contains further details on our AMT interface (based on the ParlAI framework (Miller et al., 2017)), data collection, and the quality controls we employed. Reusability and composition. One challenge for data collection via AMT is that the number of possible stories generated by CLUTRR grows combinatorially as the number of supporting facts increases, i.e., as $k=\left|\mathcal{B}_{\mathcal{C}</p>
<p>Table 1: Statistics of the AMT paraphrases. Jaccard word overlap is calculated within the templates of each individual clause of length $k$.</p>
<table>
<thead>
<tr>
<th>Number of Paraphrases</th>
<th></th>
<th></th>
<th># clauses</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>$k=1$</td>
<td>1,868</td>
<td>20</td>
</tr>
<tr>
<td></td>
<td>$k=2$</td>
<td>1,890</td>
<td>58</td>
</tr>
<tr>
<td></td>
<td>$k=3$</td>
<td>2,258</td>
<td>236</td>
</tr>
<tr>
<td></td>
<td>Total</td>
<td>6,016</td>
<td></td>
</tr>
<tr>
<td>Unique Word Count</td>
<td></td>
<td>3,797</td>
<td></td>
</tr>
<tr>
<td>Jaccard Word Overlap</td>
<td>Unigrams</td>
<td>0.201</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Bigrams</td>
<td>0.0385</td>
<td></td>
</tr>
</tbody>
</table>
<p>collection of natural language templates that can be programmatically recombined to generate stories with various properties.
Dataset statistics. At the time of submission, we have collected 6,016 unique paraphrases with an average of 19 paraphrases for every possible logical clause of length $k=1,2,3$. Table 1 contains summary statistics of the collected paraphrases. Overall, we found high linguistic diversity in the collected paraphrases. For instance, the average Jaccard overlap in unigrams between pairs paraphrases corresponding to the same logical clause was only 0.201 and only 0.0385 for bigrams. The Appendix contains further examples of the paraphrases.
Human performance. To get a sense of the data quality and difficulty involved in CLUTRR, we asked human annotators to solve the task for random examples of length $k=2,3, \ldots, 6$. We found that time-constrained AMT annotators performed well (i.e., $&gt;70 \%$ ) accuracy for $k \leq 3$ but struggled with examples involving longer stories, achieving $40-50 \%$ accuracy for $k&gt;3$. However, trained annotators with unlimited time were able to solve $100 \%$ of the examples (Appendix 1.7), highlighting the fact that this task requires attention and involved reasoning, even for humans.</p>
<h3>3.4 Query representation and inference</h3>
<p>Representing the question. The AMT paraphrasing approach described above allows us to convert the set of supporting facts $\mathcal{B}<em _mathcal_C="\mathcal{C">{\mathcal{C}}$ to a natural language story, which can be used to predict the target relation/query $\mathcal{H}</em>\right]$, to a natural language question, we instead opt to represent the target query as a $K$-way classification task, where the two entities in the target relation are provided as input and the goal is to classify the relation that holds between
these two entities. This representation avoids the pitfall of revealing information about the answer in the question (Kaushik and Lipton, 2018).
Representing entities. When generating stories, entity names are randomly drawn from a set of 300 common gendered English names. Thus, depending on each run, the entities are never the same. This ensures that the entity names are simply placeholders and uncorrelated from the task.}}$. However, instead of converting the target query, $\mathcal{H}_{\mathcal{C}}=\left[\alpha^{*</p>
<h3>3.5 Variants of CLUTRR</h3>
<p>The modular nature of CLUTRR provides rich diagnostic capabilities for evaluating the robustness and generalization abilities of neural language understanding systems. We highlight some key diagnostic capabilities available via different variations of CLUTRR below. These diagnostic variations correspond to the concrete datasets that we generated in this work, and we describe the results on these Datasets in Section 4.
Systematic generalization. Most prominently, CLUTRR allows us to explicitly evaluate a model's ability for systematic generalization. In particular, we rely on the following hold-out procedures to test systematic generalization:</p>
<ul>
<li>During training, we hold out a subset of the collected paraphrases, and we only use this held-out subset of paraphrases when generating the test set. Thus, to succeed on CLUTRR, an NLU system must exhibit linguistic generalization and be robust to linguistic variation at test time.</li>
<li>We also hold out a subset of the logical clauses during training (for clauses of length $k&gt;2$ ). ${ }^{3}$ In other words, during training, the model sees all logical rules but does not see all combinations of these logical rules. Thus, in addition to linguistic generalization, success on this task also requires logical generalization.</li>
<li>Lastly, as a more extreme form of both logical and linguistic generalization, we consider the setting where the models are trained on stories generated from clauses of length $\leq k$ and evaluated on stories generated from larger clauses of length $&gt;k$. Thus, we explicitly test the ability for models to generalize on examples that require more steps of reasoning that any example they encountered during training.</li>
</ul>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Noise generation procedures of CLUTRR.
Robust Reasoning. In addition to evaluating systematic generalization, the modular setup of CLUTRR also allows us to diagnose model robustness by adding noise facts to the generated narratives. Due to the controlled semi-synthetic nature of CLUTRR, we are able to provide a precise taxonomy of the kinds of noise facts that can be added (Figure 4). In order to structure this taxonomy, it is important to recall that any set of supporting facts $\mathcal{B}<em _mathcal_C="\mathcal{C">{\mathcal{C}}$ generated by CLUTRR can be interpreted as a path, $p</em>$, from the kinship graph $G$ :}}$, in the corresponding kinship graph $G$ (Figure 2). Based on this interpretation, we view adding noise facts from the perspective of sampling three different types of noise paths, $p_{n</p>
<ul>
<li>Irrelevant facts: We add a path $p_{n}$, which has exactly one shared end-point with $p_{c}$. In this way, this is a distractor path, which contains facts that are connected to one of the entities in the target relation, $\mathcal{H}_{\mathcal{C}}$, but do not provide any information that could be used to help answer the query.</li>
<li>Supporting facts: We add a path $p_{n}$, whose two end-points are on the path $p_{\mathcal{C}}$. The facts on this path $p_{n}$ are noise because they are not needed to answer the query, but they are supporting facts because they can, in principle, be used to construct alternative (longer) reasoning paths that connect the two target entities.</li>
<li>Disconnected facts: We add paths which neither originate nor end in any entity on $p_{c}$. These disconnected facts involve entities and relations that are completely unrelated to the target query.</li>
</ul>
<h2>4 Experiments</h2>
<p>We evaluate several neural language understanding systems on the proposed CLUTRR benchmark to surface the relative strengths and shortcomings of these models in the context of inductive reasoning and combinatorial generalization. ${ }^{4}$ We aim to answer the following key questions:</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>(Q1) How do state-of-the-art NLU models compare in terms of systematic generalization? Can these models generalize to stories with unseen combinations of logical rules?
(Q2) How does the performance of neural language understanding models compare to a graph neural network that has full access to graph structure underlying the stories?
(Q3) How robust are these models to the addition of noise facts to a given story?</p>
<h3>4.1 Baselines</h3>
<p>Our primary baselines are neural language understanding models that take unstructured text as input. We consider bidirectional LSTMs (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) (with and without attention), as well as recently proposed models that aim to incorporate inductive biases towards relational reasoning: Relation Networks (RN) (Santoro et al., 2017) and Compositional Memory Attention Network (MAC) (Hudson and Manning, 2018). We also use the large pretrained language model, BERT (Devlin et al., 2018), as well as a modified version of BERT having a trainable LSTM encoder on top of the pretrained BERT embeddings. All of these models (except BERT) were re-implemented in PyTorch 1.0 (Paszke et al., 2017) and adapted to work with the CLUTRR benchmark.</p>
<p>Since the underlying relations in the stories generated by CLUTRR inherently form a graph, we also experiment with a Graph Attention Network (GAT) (Veličković et al., 2018). Rather than taking the textual stories as input, the GAT baseline receives a structured graph representation of the facts that underlie the story.
Entity and query representations. We use the various baseline models to encode the natural language story (or graph) into a fixed-dimensional embedding. With the exception of the BERT models, we do not use pre-trained word embeddings and learn the word embeddings from scratch using end-to-end backpropagation. An important note, however, is that we perform Cloze-style anonymization (Hermann et al., 2015) of the entities (i.e., names) in the stories, where each entity name is replaced by a @ entity-k placeholder, which is randomly sampled from a small, fixed pool of placeholder tokens. The embeddings for these placeholders are randomly initialized and fixed during training. ${ }^{5}$</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Systematic generalization performance of different models when trained on clauses of length $k=2,3$ (Left) and $k=2,3,4$ (Right).</p>
<p>To make a prediction about a target query given a story, we concatenate the embedding of the story (generated by the baseline model) with the embeddings of the two target entities and we feed this concatenated embedding to a 2-layer feed-forward neural network with a softmax prediction layer.</p>
<h3>4.2 Experimental Setup</h3>
<p><strong>Hyperparameters.</strong> We selected hyperparameters for all models using an initial grid search on the systematic generalization task (described below). All models were trained for 100 epochs with Adam optimizer and a learning rate of 0.001. The Appendix provides details on the selected hyperparameters.</p>
<p><strong>Generated datasets.</strong> For all experiments, we generated datasets with 10-15k training examples. In many experiments, we report training and testing results on stories with different clause lengths $k$. (For brevity, we use the phrase "clause length" throughout this section to refer to the value $k = |\mathcal{B_C}|$, i.e., the number of steps of reasoning that are required to predict the target query.) In all cases, the training set contains 5000 train stories per $k$ value, and, during testing, all experiments use 100 test stories per $k$ value. All experiments were run 10 times with different randomly generated stories, and means and standard errors over these 10 runs are reported. As discussed in Section 3.5, during training we hold out 20% of the paraphrases, as well as 10% of the possible logical clauses.</p>
<h3>4.3 Results and Discussion</h3>
<p>With our experimental setup in place, we now address the three key questions (<strong>Q1-Q3</strong>) outlined at the beginning of Section 4.</p>
<h3>Q1: Systematic Generalization</h3>
<p>We begin by using CLUTRR to evaluate the ability of the baseline models to perform systematic generalization (<strong>Q1</strong>). In this setting, we consider two training regimes: in the first regime, we train all models with clauses of length $k = 2, 3$, and in the second regime, we train with clauses of length $k = 2, 3, 4$. We then test the generalization of these models on test clauses of length $k = 2, \ldots, 10$.</p>
<p>Figure 5 illustrates the performance of different models on this generalization task. We observe that the GAT model is able to perform near-perfectly on the held-out logical clauses of length $k = 3$, with the BERT-LSTM being the top-performer among the text-based models but still significantly below the GAT. Not surprisingly, the performance of all models degrades monotonically as we increase the length of the test clauses, which highlights the challenge of "zero-shot" systematic generalization (Lake and Baroni, 2018; Sodhani et al., 2018). However, as expected, all models improve on their generalization performance when trained on $k = 2, 3, 4$ rather than just $k = 2, 3$ (Figure 5, right). The GAT, in particular, achieves the biggest gain by this expanded training.</p>
<h3>Q2: The Benefit of Structure</h3>
<p>The empirical results on systematic generalization also provide insight into how the text-based NLU systems compare against the graph-based GAT model that has full access to the logical graph structure underlying the stories (<strong>Q2</strong>). Indeed, the relatively strong performance of the GAT model (Figure 5) suggests that the language-based models fail to learn a robust mapping from the natural language narratives to the underlying logical facts.</p>
<p>To further confirm this trend, we ran experiments</p>
<p>Table 2: Testing the robustness of the various models when training and testing on stories containing various types of noise facts. The types of noise facts (supporting, irrelevant, and disconnected) are defined in Section 3.5.</p>
<table>
<thead>
<tr>
<th>Models</th>
<th></th>
<th>Unstructured models (no graph)</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>Structured model (with graph)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Training</td>
<td>Testing</td>
<td>BiLSTM - Attention</td>
<td>BiLSTM - Mean</td>
<td>RN</td>
<td>MAC</td>
<td>BERT</td>
<td>BERT-LSTM</td>
<td>GAT</td>
<td></td>
</tr>
<tr>
<td>Clean</td>
<td>Clean</td>
<td>0.58 2,0.05</td>
<td>0.53 2,0.05</td>
<td>0.49 2,0.06</td>
<td>0.63 2,0.06</td>
<td>0.37 2,0.06</td>
<td>0.67 2,0.05</td>
<td>1.0 2,0.0</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Supporting</td>
<td>0.76 2,0.02</td>
<td>0.64 2,0.22</td>
<td>0.58 2,0.06</td>
<td>0.71 2,0.07</td>
<td>0.28 2,0.1</td>
<td>0.66 2,0.06</td>
<td>0.24 2,0.2</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Irrelevant</td>
<td>0.7 2,0.15</td>
<td>0.76 2,0.02</td>
<td>0.59 2,0.06</td>
<td>0.69 2,0.05</td>
<td>0.24 2,0.09</td>
<td>0.55 2,0.05</td>
<td>0.51 2,0.15</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Disconnected</td>
<td>0.49 2,0.05</td>
<td>0.45 2,0.05</td>
<td>0.5 2,0.06</td>
<td>0.59 2,0.05</td>
<td>0.24 2,0.09</td>
<td>0.5 2,0.06</td>
<td>0.8 2,0.07</td>
<td></td>
</tr>
<tr>
<td>Supporting</td>
<td>Supporting</td>
<td>0.67 2,0.06</td>
<td>0.66 2,0.07</td>
<td>0.68 2,0.05</td>
<td>0.65 2,0.04</td>
<td>0.32 2,0.09</td>
<td>0.57 2,0.04</td>
<td>0.98 2,0.05</td>
<td></td>
</tr>
<tr>
<td>Irrelevant</td>
<td>Irrelevant</td>
<td>0.51 2,0.06</td>
<td>0.52 2,0.06</td>
<td>0.5 2,0.04</td>
<td>0.56 2,0.04</td>
<td>0.25 2,0.06</td>
<td>0.53 2,0.06</td>
<td>0.93 2,0.05</td>
<td></td>
</tr>
<tr>
<td>Disconnected</td>
<td>Disconnected</td>
<td>0.57 2,0.07</td>
<td>0.57 2,0.06</td>
<td>0.45 2,0.11</td>
<td>0.4 2,0.1</td>
<td>0.17 2,0.05</td>
<td>0.47 2,0.06</td>
<td>0.96 2,0.05</td>
<td></td>
</tr>
<tr>
<td>Average</td>
<td></td>
<td>0.61 2,0.06</td>
<td>0.59 2,0.06</td>
<td>0.54 2,0.07</td>
<td>0.61 2,0.06</td>
<td>0.30 2,0.07</td>
<td>0.56 2,0.05</td>
<td>0.77 2,0.06</td>
<td></td>
</tr>
</tbody>
</table>
<p>with modified train and test splits for the text-based models, where the same set of natural language paraphrases were used to construct the narratives in both the train and test splits (see Appendix 1.3 for details). In this simplified setting, the text-based models must still learn to reason about held-out logical patterns, but the difficulty of parsing the natural language is essentially removed, as the same natural language paraphrases are used during testing and training. We found that the text-based models were competitive with the GAT model in this simplified setting (Appendix Figure 1), confirming that the poor performance of the text-based models on the main task is driven by the difficulty of parsing the unseen natural language narratives.</p>
<h2>Q3: Robust Reasoning</h2>
<p>Finally, we use CLUTRR to systematically evaluate how various baseline neural language understanding systems cope with noise (Q3). In all the experiments we provide a combination of $k=2$ and $k=3$ length clauses in training and testing, with noise facts being added to the train and/or test set depending on the setting (Table 2). We use the different types of noise facts defined in Section 3.5.</p>
<p>Overall, we find that the GAT baseline outperforms the unstructured text-based models across most testing scenarios (Table 2), which showcases the benefit of a structured feature space for robust reasoning. When training on clean data and testing on noisy data, we observe two interesting trends that highlight the benefits and shortcomings of the various model classes:</p>
<ol>
<li>All the text-based models excluding BERT actually perform better when testing on examples that have supporting or irrelevant facts added. This suggests that these models actually benefit from having more content related to the entities in the story. Even though this content is not strictly useful or needed for the reasoning
task, it may provide some linguistic cues (e.g., about entity genders) that the models exploit. In contrast, the BERT-based models do not benefit from the inclusion of this extra content, which is perhaps due to the fact that they are already built upon a strong language model (e.g., that already adequately captures entity genders.)</li>
<li>The GAT model performs poorly when supporting facts are added but has no performance drop when disconnected facts are added. This suggests that the GAT model is sensitive to changes that introduce cycles in the underlying graph structure but is robust to the addition of noise that is disconnected from the target entities.</li>
</ol>
<p>Moreover, when we trained on noisy examples, we found that only the GAT model was able to consistently improve its performance (Table 2). Again, this highlights the performance gap between the unstructured text-based models and the GAT.</p>
<h2>5 Conclusion</h2>
<p>In this paper we introduced the CLUTRR benchmark suite to test the systematic generalization and inductive reasoning capababilities of NLU systems. We demonstrated the diagnostic capabilities of CLUTRR and found that existing NLU systems exhibit relatively poor robustness and systematic generalization capabilities-especially when compared to a graph neural network that works directly with symbolic input. These results highlight the gap that remains between machine reasoning models that work with unstructured text and models that are given access to more structured input. We hope that by using this benchmark suite, progress can be made in building more compositional, modular, and robust NLU systems.</p>
<h2>6 Acknowledgements</h2>
<p>The authors would like to thank Jack Urbanek, Stephen Roller, Adina Williams, Dzmitry Bahdanau, Prasanna Parthasarathy, Harsh Satija for useful discussions and technical help. The authors would also like to thank Abhishek Das, Carlos Eduardo Lassance, Gunshi Gupta, Milan Aggarwal, Rim Assouel, Weiping Song, and Yue Dong for feedback on the draft. The authors also like to thank the many anonymous Mechanical Turk participants for providing paraphrases, and thank Sumana Basu, Etienne Denis, Jonathan Lebensold, and Komal Teru for providing human performance measures. The authors would also like to thank Sanghyun Yoo, Jehun Jeon and Dr Young Sang Choi of Samsung Advanced Institute of Technology (SAIT) for supporting the previous workshop version of this work. The authors are grateful to Facebook AI Research (FAIR) for providing extensive compute and GPU resources and support. This research was supported by the Canada CIFAR Chairs in AI program.</p>
<h2>References</h2>
<p>Dzmitry Bahdanau, Shikhar Murty, Michael Noukhovitch, Thien Huu Nguyen, Harm de Vries, and Aaron Courville. 2019. Systematic generalization: What is required and can it be learned? In International Conference on Learning Representations.</p>
<p>Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages $632-642$.</p>
<p>Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 17241734 .</p>
<p>Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Luke Vilnis, Ishan Durugkar, Akshay Krishnamurthy, Alex Smola, and Andrew McCallum. 2018. Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning. In International Conference on Learning Representations.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p>
<p>Herve Gallaire and Jack Minker. 1978. Logic and Data Bases. Perseus Publishing.</p>
<p>Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A Smith. 2018. Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107-112.</p>
<p>Will Hamilton, Payal Bajaj, Marinka Zitnik, Dan Jurafsky, and Jure Leskovec. 2018. Embedding logical queries on knowledge graphs. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 2026-2037. Curran Associates, Inc.</p>
<p>Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems, pages 1693-1701.</p>
<p>Geoffrey E Hinton et al. 1986. Learning distributed representations of concepts. In Proceedings of the eighth annual conference of the cognitive science society, volume 1, page 12. Amherst, MA.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735-1780.</p>
<p>Drew Arad Hudson and Christopher D. Manning. 2018. Compositional attention networks for machine reasoning. In International Conference on Learning Representations.</p>
<p>Robin Jia and Percy Liang. 2017. Adversarial examples for evaluating reading comprehension systems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2021-2031.</p>
<p>Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. 2017. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on, pages 19881997. IEEE.</p>
<p>Dimitri Kartsaklis, Mohammad Taher Pilehvar, and Nigel Collier. 2018. Mapping text to knowledge graph entities using multi-sense lstms. arXiv preprint arXiv:1808.07724.</p>
<p>Divyansh Kaushik and Zachary C Lipton. 2018. How much reading does reading comprehension require? a critical investigation of popular benchmarks. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 5010-5015.</p>
<p>Stanley Kok and Pedro Domingos. 2007. Statistical predicate invention. In Proceedings of the 24th International Conference on Machine Learning, ICML '07, pages 433-440, New York, NY, USA. ACM.</p>
<p>Brenden Lake and Marco Baroni. 2018. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In International Conference on Machine Learning, pages 2879-2888.</p>
<p>Nada Lavrac and Saso Dzeroski. 1994. Inductive logic programming. In WLP, pages 146-160. Springer.</p>
<p>Alexander Miller, Will Feng, Dhruv Batra, Antoine Bordes, Adam Fisch, Jiasen Lu, Devi Parikh, and Jason Weston. 2017. Parlai: A dialog research software platform. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 79-84.</p>
<p>Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. A corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of NAACLHLT, pages 839-849.</p>
<p>Stephen Muggleton. 1991. Inductive logic programming. New Generation Computing, 8(4):295-318.</p>
<p>Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. Ms marco: A human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268.</p>
<p>Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Automatic differentiation in pytorch.</p>
<p>Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In NAACL.</p>
<p>J R Quinlan. 1990. Learning logical definitions from relations. Mach. Learn., 5(3):239-266.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392.</p>
<p>Matthew Richardson, Christopher JC Burges, and Erin Renshaw. 2013. Mctest: A challenge dataset for the open-domain machine comprehension of text. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 193-203.</p>
<p>Tim Rocktäschel and Sebastian Riedel. 2017. End-toend differentiable proving. In Advances in Neural Information Processing Systems, pages 3788-3800.</p>
<p>Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Timothy Lillicrap. 2017. A simple neural network module for relational reasoning. In Advances in neural information processing systems, pages 4967-4976.</p>
<p>Shagun Sodhani, Sarath Chandar, and Yoshua. Bengio. 2018. On Training Recurrent Neural Networks for Lifelong Learning. arXiv e-prints.</p>
<p>Yu Su, Huan Sun, Brian Sadler, Mudhakar Srivatsa, Izzeddin Gur, Zenghui Yan, and Xifeng Yan. 2016. On generating characteristic-rich question sets for qa evaluation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 562-572.</p>
<p>Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. 2016. NewsQA: A machine comprehension dataset. arXiv preprint, pages 1-12.</p>
<p>Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2018. Graph attention networks. In International Conference on Learning Representations.
Z. Wang, L. Li, D. D. Zeng, and Y. Chen. 2018. Attention-based multi-hop reasoning for knowledge graph. In 2018 IEEE International Conference on Intelligence and Security Informatics (ISI), pages 211-213.</p>
<p>Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. 2018. Constructing datasets for multi-hop reading comprehension across documents. Transactions of the Association of Computational Linguistics, 6:287-302.</p>
<p>Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart van Merriënboer, Armand Joulin, and Tomas Mikolov. 2015. Towards AI-Complete question answering: A set of prerequisite toy tasks.</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), volume 1, pages 1112-1122.</p>
<p>Wenhan Xiong, Thien Hoang, and William Yang Wang. 2017. Deeppath: A reinforcement learning method for knowledge graph reasoning. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 564-573.</p>
<p>Wenhan Xiong, Mo Yu, Shiyu Chang, Xiaoxiao Guo, and William Yang Wang. 2018. One-shot relational learning for knowledge graphs. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1980-1990.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ Code to reproduce all the results in this section will be released at https://github.com/facebookresearch/clutrr/.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{5}$ See Appendix 1.5 for a comparison of placeholder em-&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8926 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8926</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8926</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-994a08fd88f8e90affe78e7e518a4fe74f024ca6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/994a08fd88f8e90affe78e7e518a4fe74f024ca6" target="_blank">Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Critique Fine-Tuning is proposed, a method more effective than SFT for reasoning tasks that trains models to critique noisy responses, inspired by human learning processes that emphasize critical thinking, deeper analysis, and nuanced understanding - traits often overlooked by standard SFT.</p>
                <p><strong>Paper Abstract:</strong> Supervised Fine-Tuning (SFT) is commonly used to train language models to imitate annotated responses for given instructions. In this paper, we propose Critique Fine-Tuning (CFT), a method more effective than SFT for reasoning tasks. Instead of simply imitating correct responses, CFT trains models to critique noisy responses, inspired by human learning processes that emphasize critical thinking, deeper analysis, and nuanced understanding - traits often overlooked by standard SFT. To validate the effectiveness of CFT, we construct multiple critique datasets (e.g., WebInstruct, MetaMath, NuminaMath), where GPT-4o serves as the teacher to generate critiques in the form of ([query; noisy response], critique). Experiments on these datasets demonstrate that CFT consistently outperforms SFT by 4-10% across six mathematical reasoning benchmarks, and is effective across different base models including Qwen2.5, Qwen2.5-Math, and DeepSeek-Math. Notably, our model Qwen2.5-Math-CFT only requires 1 hour of training on 8 x H100 over the 50K examples, yet matches or outperforms strong competitors like Qwen2.5-Math-Instruct on most benchmarks, which use over 2M samples. Moreover, it matches the performance of SimpleRL, which is a DeepSeek-r1 replication trained with 140 x more compute. Experiments on IF_Eval and MT-Bench further demonstrate that CFT can significantly enhance the model's general generation and instruction-following capabilities, outperforming the Qwen2.5-Math-Instruct by a large margin. Ablation studies show that CFT is robust to noisy response sources and teacher critique models. These findings highlight that CFT offers a more effective alternative to advance the reasoning of language models.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8926.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8926.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Single-pass self-critique</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single-pass self-critique (generate-and-critique in one pass)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An inference-time procedure where the model generates a solution and immediately critiques that same solution within one inference pass and attempts to correct it if errors are detected.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-Math-7B-CFT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B-parameter Qwen2.5-Math model fine-tuned with Critique Fine-Tuning (CFT) on a 50K WebInstruct critique dataset; standard inference configuration unless otherwise specified.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Single-pass self-critique</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The model is prompted to: (1) generate a step-by-step solution, (2) produce a critique of that solution, and (3) if errors are found, provide a corrected solution -- all within one inference call (one generate+critique pass). Prompts combine solution and critique in a single template (see Appendix A.4.1).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MATH; Minerva-Math</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>MATH: a benchmark of school/competition-level math problems; Minerva-Math: quantitative reasoning benchmark used for evaluating LLM mathematical problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Example reported results (temperature-dependent): MATH = 77.2% (temp 0.1), 76.1% (temp 0.3), 73.5% (temp 0.6); Minerva-Math = 36.7% (temp 0.1), 35.2% (temp 0.3), 34.3% (temp 0.6).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Direct inference (no self-critique; same model) reported: MATH = 80.2% (temp 0.0), 78.8% (temp 0.1), 77.5% (temp 0.3), 75.2% (temp 0.6); Minerva-Math = 42.3% (temp 0.0), 38.9% (temp 0.1), 37.7% (temp 0.3), 34.1% (temp 0.6).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering: explicit combined solution+critique prompt template (single-pass template in Appendix A.4.1); correction is produced by the same model in the same inference call. Sampling temperature varied to study effects.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>No evidence of improvement; single-pass self-critique underperformed direct inference across temperatures on both MATH and Minerva-Math (numerical examples above show lower accuracy with self-critique).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported issues: inconsistent self-evaluation standards (missed errors or false positives), temperature sensitivity (higher temperature degrades iterative stability), regeneration inefficiency (when errors are detected, subsequent corrections often introduce new errors), and increased computational overhead. Authors concluded single-pass self-critique consistently underperformed compared to direct inference.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared directly to (a) direct inference with no self-critique and (b) two-stage self-critique; single-pass performed worse than direct inference and typically slightly worse than two-stage. The paper also contrasts these empirical findings with prior literature that sometimes found benefits from self-refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Table 11 (Appendix A.4) presents temperature ablations showing direct inference (no self-critique) is superior across temperatures; single-pass scores fall below direct inference at each temperature tested (see reported numerical values).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8926.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8926.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Two-stage self-critique</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Two-stage self-critique (separate critique step with iterative regeneration)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An inference-time iterative procedure where the model first generates a solution, then a separate critique step evaluates the solution; if judged incorrect the model regenerates a new solution and repeats up to a fixed maximum number of iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-Math-7B-CFT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B-parameter Qwen2.5-Math model fine-tuned with Critique Fine-Tuning (CFT) on a 50K WebInstruct critique dataset; used for iterative inference experiments described in Appendix A.4.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Two-stage self-critique</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Stage 1: generate a solution (step-by-step). Stage 2: run a separate critique prompt that evaluates the prior solution and outputs 'Correct'/'Incorrect' plus critique points. If 'Incorrect', the process returns to Stage 1 to regenerate; this loop repeats up to 8 attempts (max iterations = 8). Prompts and the loop are defined in Appendix A.4.1.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MATH; Minerva-Math</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>MATH: mathematical problem-solving benchmark; Minerva-Math: more challenging quantitative reasoning set used in evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Example reported results (temperature-dependent): MATH = 77.9% (temp 0.1), 75.8% (temp 0.3), 74.6% (temp 0.6); Minerva-Math = 38.2% (temp 0.1), 35.4% (temp 0.3), 34.6% (temp 0.6).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Direct inference (no self-critique): see values above — e.g., at temp 0.1 direct MATH = 78.8% and Minerva-Math = 38.9%; at temp 0.3 direct MATH = 77.5% and Minerva-Math = 37.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering with an explicit two-stage control flow: separate solution and critique prompts; loop controller enforces regeneration until critique labels solution 'Correct' or max iterations reached. Sampling temperature varied to study effects.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>8</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>No improvement over direct inference; two-stage yields slightly better results than single-pass in some settings but still generally lower than direct inference (numerical comparisons above). The two-stage approach did not produce consistent gains and added cost.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported failures: inconsistent critique judgments (false negatives and false positives), instability with higher sampling temperature, difficulty in effectively fixing identified errors across regenerations (regeneration inefficiency), additional computational cost from iterative attempts, and ultimately poorer end-task accuracy compared to direct inference. Specific failure examples document over-lenient evaluations and false error detections.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to single-pass self-critique and direct inference; two-stage slightly outperformed single-pass in some temperature settings but remained inferior to direct inference. Paper notes contrast with some literature that reports benefits from iterative self-refinement but cites other works that find self-correction unreliable.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Appendix Table 11 reports temperature-variant ablations for both single-pass and two-stage methods; summary: both self-critique approaches decline in accuracy as temperature increases, and neither equals the direct inference baseline across temperatures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Large language models cannot self-correct reasoning yet <em>(Rating: 2)</em></li>
                <li>Generating sequences by learning to self-correct <em>(Rating: 2)</em></li>
                <li>Can large language models really improve by self-critiquing their own plans? <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8926",
    "paper_id": "paper-994a08fd88f8e90affe78e7e518a4fe74f024ca6",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "Single-pass self-critique",
            "name_full": "Single-pass self-critique (generate-and-critique in one pass)",
            "brief_description": "An inference-time procedure where the model generates a solution and immediately critiques that same solution within one inference pass and attempts to correct it if errors are detected.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-Math-7B-CFT",
            "model_description": "7B-parameter Qwen2.5-Math model fine-tuned with Critique Fine-Tuning (CFT) on a 50K WebInstruct critique dataset; standard inference configuration unless otherwise specified.",
            "reflection_method_name": "Single-pass self-critique",
            "reflection_method_description": "The model is prompted to: (1) generate a step-by-step solution, (2) produce a critique of that solution, and (3) if errors are found, provide a corrected solution -- all within one inference call (one generate+critique pass). Prompts combine solution and critique in a single template (see Appendix A.4.1).",
            "task_name": "MATH; Minerva-Math",
            "task_description": "MATH: a benchmark of school/competition-level math problems; Minerva-Math: quantitative reasoning benchmark used for evaluating LLM mathematical problem solving.",
            "performance_with_reflection": "Example reported results (temperature-dependent): MATH = 77.2% (temp 0.1), 76.1% (temp 0.3), 73.5% (temp 0.6); Minerva-Math = 36.7% (temp 0.1), 35.2% (temp 0.3), 34.3% (temp 0.6).",
            "performance_without_reflection": "Direct inference (no self-critique; same model) reported: MATH = 80.2% (temp 0.0), 78.8% (temp 0.1), 77.5% (temp 0.3), 75.2% (temp 0.6); Minerva-Math = 42.3% (temp 0.0), 38.9% (temp 0.1), 37.7% (temp 0.3), 34.1% (temp 0.6).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering: explicit combined solution+critique prompt template (single-pass template in Appendix A.4.1); correction is produced by the same model in the same inference call. Sampling temperature varied to study effects.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "No evidence of improvement; single-pass self-critique underperformed direct inference across temperatures on both MATH and Minerva-Math (numerical examples above show lower accuracy with self-critique).",
            "limitations_or_failure_cases": "Reported issues: inconsistent self-evaluation standards (missed errors or false positives), temperature sensitivity (higher temperature degrades iterative stability), regeneration inefficiency (when errors are detected, subsequent corrections often introduce new errors), and increased computational overhead. Authors concluded single-pass self-critique consistently underperformed compared to direct inference.",
            "comparison_to_other_methods": "Compared directly to (a) direct inference with no self-critique and (b) two-stage self-critique; single-pass performed worse than direct inference and typically slightly worse than two-stage. The paper also contrasts these empirical findings with prior literature that sometimes found benefits from self-refinement.",
            "ablation_study_results": "Table 11 (Appendix A.4) presents temperature ablations showing direct inference (no self-critique) is superior across temperatures; single-pass scores fall below direct inference at each temperature tested (see reported numerical values).",
            "uuid": "e8926.0",
            "source_info": {
                "paper_title": "Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Two-stage self-critique",
            "name_full": "Two-stage self-critique (separate critique step with iterative regeneration)",
            "brief_description": "An inference-time iterative procedure where the model first generates a solution, then a separate critique step evaluates the solution; if judged incorrect the model regenerates a new solution and repeats up to a fixed maximum number of iterations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-Math-7B-CFT",
            "model_description": "7B-parameter Qwen2.5-Math model fine-tuned with Critique Fine-Tuning (CFT) on a 50K WebInstruct critique dataset; used for iterative inference experiments described in Appendix A.4.",
            "reflection_method_name": "Two-stage self-critique",
            "reflection_method_description": "Stage 1: generate a solution (step-by-step). Stage 2: run a separate critique prompt that evaluates the prior solution and outputs 'Correct'/'Incorrect' plus critique points. If 'Incorrect', the process returns to Stage 1 to regenerate; this loop repeats up to 8 attempts (max iterations = 8). Prompts and the loop are defined in Appendix A.4.1.",
            "task_name": "MATH; Minerva-Math",
            "task_description": "MATH: mathematical problem-solving benchmark; Minerva-Math: more challenging quantitative reasoning set used in evaluations.",
            "performance_with_reflection": "Example reported results (temperature-dependent): MATH = 77.9% (temp 0.1), 75.8% (temp 0.3), 74.6% (temp 0.6); Minerva-Math = 38.2% (temp 0.1), 35.4% (temp 0.3), 34.6% (temp 0.6).",
            "performance_without_reflection": "Direct inference (no self-critique): see values above — e.g., at temp 0.1 direct MATH = 78.8% and Minerva-Math = 38.9%; at temp 0.3 direct MATH = 77.5% and Minerva-Math = 37.7%.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering with an explicit two-stage control flow: separate solution and critique prompts; loop controller enforces regeneration until critique labels solution 'Correct' or max iterations reached. Sampling temperature varied to study effects.",
            "number_of_iterations": 8,
            "evidence_for_improvement": "No improvement over direct inference; two-stage yields slightly better results than single-pass in some settings but still generally lower than direct inference (numerical comparisons above). The two-stage approach did not produce consistent gains and added cost.",
            "limitations_or_failure_cases": "Reported failures: inconsistent critique judgments (false negatives and false positives), instability with higher sampling temperature, difficulty in effectively fixing identified errors across regenerations (regeneration inefficiency), additional computational cost from iterative attempts, and ultimately poorer end-task accuracy compared to direct inference. Specific failure examples document over-lenient evaluations and false error detections.",
            "comparison_to_other_methods": "Compared to single-pass self-critique and direct inference; two-stage slightly outperformed single-pass in some temperature settings but remained inferior to direct inference. Paper notes contrast with some literature that reports benefits from iterative self-refinement but cites other works that find self-correction unreliable.",
            "ablation_study_results": "Appendix Table 11 reports temperature-variant ablations for both single-pass and two-stage methods; summary: both self-critique approaches decline in accuracy as temperature increases, and neither equals the direct inference baseline across temperatures.",
            "uuid": "e8926.1",
            "source_info": {
                "paper_title": "Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Large language models cannot self-correct reasoning yet",
            "rating": 2
        },
        {
            "paper_title": "Generating sequences by learning to self-correct",
            "rating": 2
        },
        {
            "paper_title": "Can large language models really improve by self-critiquing their own plans?",
            "rating": 2
        }
    ],
    "cost": 0.01383775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate</h1>
<p>Yubo Wang ${ }^{1}$, Xiang Yue ${ }^{2}$, Wenhu Chen ${ }^{1,3}$<br>${ }^{1}$ Department of Computer Science, University of Waterloo<br>${ }^{2}$ Carnegie Mellon University, Pittsburgh<br>${ }^{3}$ Vector Institute, Toronto</p>
<p>https://tiger-ai-lab.github.io/CritiqueFineTuning/</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Supervised Fine-Tuning (SFT)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Critique Fine-Tuning (CFT)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Q: Your airline is considering replacing its McDonnell Douglas MD-80s with either the Airbus 320ceo or the Airbus 320neo. Both models ... How would you conduct a financial analysis to determine which model is the more costeffective choice in the long run?</td>
<td style="text-align: center;">A: To determine the better financial choice between the Airbus 320ceo and Airbus 320neo ... suggest that the initial ... expected lives.</td>
<td style="text-align: center;">Q: Your airline is considering replacing its McDonnell Douglas MD-80s with either the Airbus 320ceo or the Airbus 320neo. Both <br> A: To determine the better financial choice between the Airbus 320ceo and Airbus 320...</td>
<td style="text-align: center;">C: The financial analysis you propose is thorough and wellstructured... reliable basis for decisionmaking... <br> Conclusion: right</td>
</tr>
<tr>
<td style="text-align: center;">MATH</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">OlympiadBench</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">100</td>
<td style="text-align: center;">a Base SFT-GPT4o SFT-verified $\square$ CFT</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">a Base SFT-GPT4o SFT-verified $\square$ CFT</td>
</tr>
<tr>
<td style="text-align: center;">$80$</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">30.1</td>
</tr>
<tr>
<td style="text-align: center;">$60$</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">30.1</td>
</tr>
<tr>
<td style="text-align: center;">$40$</td>
<td style="text-align: center;">61.7</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">30.1</td>
</tr>
<tr>
<td style="text-align: center;">20 DeepSeek</td>
<td style="text-align: center;">Qwen2.5</td>
<td style="text-align: center;">DeepSeek</td>
<td style="text-align: center;">Qwen2.5 Qwen2.5-Math</td>
</tr>
</tbody>
</table>
<p>Figure 1: Comparison between CFT and SFT on 50K samples from WebInstruct (Yue et al., 2024b). SFT-verified means SFT training on the responses validated by GPT-4o, SFT-GPT4o means SFT training on the responses from GPT-4o. CFT is our approach, which trains on the critique provided by GPT-4o.</p>
<h4>Abstract</h4>
<p>Supervised Fine-Tuning (SFT) is commonly used to train language models to imitate annotated responses for given instructions. In this paper, we propose Critique Fine-Tuning (CFT), a method more effective than SFT for reasoning tasks. Instead of simply imitating correct responses, CFT trains models to critique noisy responses, inspired by human learning processes that emphasize critical thinking, deeper analysis, and nuanced understanding-traits often overlooked by standard SFT. To validate the effectiveness of CFT, we construct multiple critique datasets (e.g., WebInstruct, MetaMath, NuminaMath), where GPT-4o serves as the teacher to generate critiques in the form of ([query; noisy response], critique). Experiments on these datasets demonstrate that CFT consistently outperforms SFT by $4-10 \%$ across six mathematical reasoning benchmarks, and is effective across different base models including Qwen2.5, Qwen2.5-Math, and DeepSeek-Math. Notably, our model Qwen2.5-Math-CFT only requires 1 hour of training on 8 xH 100 over the 50 K examples, yet matches or outperforms strong competitors like Qwen2.5-Math-Instruct on most benchmarks, which use over 2 M samples. Moreover, it matches the performance of SimpleRL, which is a DeepSeek-r1 replication trained with 140x more compute. Experiments on IF_Eval and MT-Bench further demonstrate that CFT can significantly enhance the model's general generation and instructionfollowing capabilities, outperforming the Qwen2.5-Math-Instruct by a large margin. Ablation studies show that CFT is robust to noisy response sources and teacher critique models. These findings highlight that CFT offers a more effective alternative to advance the reasoning of language models.</p>
<h1>1 Introduction</h1>
<p>Recently, large language models (LLMs) (Achiam et al., 2023; Team et al., 2023; Dubey et al., 2024) have shown unprecedented performance on real-world problems. One core techniques is supervised fine-tuning (SFT), which trains LLMs to follow natural language instructions (Wei et al., 2022; Ouyang et al., 2022; Sanh et al., 2022). In the process of SFT, LLMs are forced to imitate the annotated responses. Numerous efforts have been made to build high-quality SFT datasets using approaches like Self-Instruct (Wang et al., 2023b) and Evol-Instruct (Xu et al., 2024) to enhance LLMs' general instruction-following capabilities. More recently, works such as MAmmoTH (Yue et al., 2024a;b), MetaMath (Yu et al., 2024), and WizardCoder (Luo et al., 2024) have employed SFT to improve the targeted capabilities of LLMs in areas like mathematical reasoning, coding, and more. While these approaches have shown significant gains on weaker base models such as Mistral (Jiang et al., 2023) or LLaMA3 (Dubey et al., 2024), diminishing returns become evident as SFT dataset size and quality scale up. This limitation is particularly pronounced for already-powerful base models (non-SFTed), such as Qwen2.5-base (Yang et al., 2024a), Qwen2.5-Math-base (Yang et al., 2024b), or DeepSeek-Coder-V2-base (Guo et al., 2024), which have undergone extensive domain-adaptive pretraining on reasoning-focused corpora comprising hundreds of billions of tokens. Our experiments in section 3 reveal that applying SFT to these models can even degrade performance without stringent quality control.
In this paper, we propose a new learning framework called Critique Fine-Tuning (CFT) as an alternative to traditional SFT. Inspired by human learning-where critical thinking and constructive feedback are vital for improvement-we shift the focus from imitation to critique-based learning. When humans learn, they do not merely replicate provided answers but analyze, critique, and refine them. Similarly, in CFT, the model learns to provide critiques for noisy responses, identify flaws, suggest improvements, and verify correctness. Formally, CFT involves training the model to critique a given query-response pair, maximizing the likelihood $P(c|[x ; y])$, where $c$ is the annotated critique for a queryresponse pair $[x ; y]$. A detailed visualization of CFT is presented in Figure 1.
To validate CFT's effectiveness, we designed a series of experiments using multiple critique datasets, including WebInstruct (Yue et al., 2024b), MetaMathQA (Yu et al., 2024), and NuminaMath (Li et al., 2024b), with critiques synthesized by advanced models such as GPT-4o (Achiam et al., 2023). We applied CFT to strong 7B base language models (non-instruction-tuned), including DeepSeekMath-base (Shao et al., 2024), Qwen2.5 (Yang et al., 2024a), and Qwen2.5-Math (Yang et al., 2024b). These models were compared against strong SFT-trained variants, such as WebInstruct-verified (SFT on GPT-4o-verified responses) and WebInstruct-GPT4o (SFT directly on GPT-4o-generated responses). When evaluated on six math benchmarks, including MATH and AIME24, CFT-trained models consistently outperform the best SFT-trained models by an average of 4-10 absolute points.
We further expanded the evaluation to broader STEM benchmarks, including GPQA (Rein et al., 2023), TheoremQA (Chen et al., 2023), and MMLU-Pro (Wang et al., 2024b). Our results show that the best CFT-trained model, Qwen2.5-Math-CFT, trained on 50K examples, outperformed strong competitors like AceMath (Liu et al., 2024) and Qwen2.5-MathInstruct (Yang et al., 2024b), which were trained on over 2M examples. We also compare Qwen2.5-Math-CFT with SimpleRL (Zeng et al., 2025), an open replication of DeepSeekR1 (Guo et al., 2025) trained with 140x more compute ( 1152 vs 8 H 100 hours). Results show that Qwen2.5-Math-CFT reaches the same average performance across 5 math benchmarks, highlighting the efficiency and effectiveness of CFT for reasoning-focused tasks.
To understand the models' general instruction following abilities, we also evaluate our Qwen2.5-Math-CFT on MT-Bench (Zheng et al., 2023) and IF_Eval (Zhou et al., 2023). We show that our method also enhances general instruction-following abilities. It outperforms our SFTed version and the official Qwen2.5-Math-Instruct by a notable margin.
To better understand different factors of CFT, we conducted comprehensive ablation studies:</p>
<ol>
<li>Robustness to dataset sources: Comparing WebInstruct (Yue et al., 2024b) against MetaMathQA (Yu et al., 2024) and NuminaMath (Li et al., 2024b), we observed that WebInstruct</li>
</ol>
<p>provided a slight advantage $(3 \%+$ ) due to its diversity and broader topic coverage.
2. Robustness to noisy response sources: Experiments with original noisy responses and Qwen2.5-base responses critiqued by GPT-4o showed negligible performance differences.
3. Flexibility to the teacher: Using a weaker critique dataset synthesized by GPT-4o-mini still yielded notable improvements over SFT, despite a $4 \%$ overall score drop.
4. Controlling for token length: Even controlling token length, shorter critique examples significantly outperformed standard SFT ( $55.2 \%$ vs. $50.4 \%$ ), confirming improvements stem from critique-based training rather than increased sequence length.
Our approach not only demonstrates strong improvement on reasoning-focused tasks, but also exhibits notable improvement over general-purpose instruction following tasks. These evidence has shown great potential of CFT to replace SFT in language model training.</p>
<h1>2 Method \&amp; Dataset</h1>
<p>To validate the effectiveness of CFT, we construct several fine-tuning datasets. Most of our experiments are based on WebInstruct, an instruction dataset collected from online educational resources and quiz websites. The dataset undergoes synthetic processing in its pipeline using LLMs to improve solution quality and format consistency.</p>
<h3>2.1 WebInstruct</h3>
<p>WebInstruct spans a wide range of topics, including Mathematics (65\%), Physics (8\%), Chemistry (4\%), Business (10\%), Humanities (4\%), and more. Unlike other datasets, which are primarily derived from math contests and competitions, WebInstruct offers broader topic coverage. The responses in WebInstruct are extracted and refined by large language models such as Qwen-72B (Bai et al., 2023) and Mixtral (Jiang et al., 2024), making them highly prone to noise due to the lack of verification or quality control.
We curate the following subsets from WebInstruct:</p>
<ul>
<li>WebInstruct-SFT: A 50K subset directly sampled from the original WebInstruct dataset. This subset has a very high error ratio (over $50 \%$ ).</li>
<li>WebInstruct-verified: We prompt GPT-4o-1120 to judge the correctness of original WebInstruct answers. We retained the top 50K samples as "verified" SFT data.</li>
<li>WebInstruct-GPT-4o: A 50K subset that reuses questions from WebInstruct-SFT but replaces the answers with those generated by GPT-4o-1120.</li>
<li>WebInstruct-CFT (Ours): A 50K subset derived from WebInstruct-SFT, where GPT-4o1120 provides detailed critiques of the original responses. Approximately $56 \%$ of the responses in this subset are judged as "correct" while the rest are considered "wrong". Despite containing some critique errors introduced by GPT-4o, this dataset is comparable in quality to WebInstruct-GPT-4o.
We compare our CFT datasets with existing SFT datasets in Table 1. As shown, our datasets cover a broader range of topics while being significantly smaller in size, highlighting their efficiency in boosting LLMs' reasoning abilities.</li>
</ul>
<h3>2.2 MetaMath \&amp; NuminaMath</h3>
<p>In addition to WebInstruct, we synthesized critiques for other datasets, including MetaMathQA and NuminaMath. From each dataset, we randomly sampled 50K examples and used GPT-4o to critique the original responses. We then applied CFT to these datasets to demonstrate the generalizability of our approach across other datasets.</p>
<h3>2.3 Training Objective</h3>
<p>Our training objective is straightforward. We concatenate question $x$ and noisy response $y$ as input, then optimize the model parameters to generate critique $c$. Formally, the training loss is: $\operatorname{argmax}_{\theta} \log P(c \mid[x ; y] ; \theta)$ where $\theta$ is the parameters of the language model.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Size</th>
<th>Source or Seed</th>
<th>Discipline</th>
</tr>
</thead>
<tbody>
<tr>
<td>Supervised Fine-Tuning Data</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>WizardMath (Luo et al., 2023)</td>
<td>96K</td>
<td>GSM8K, MATH</td>
<td>Math</td>
</tr>
<tr>
<td>MathInstruct (Yue et al., 2024a)</td>
<td>260K</td>
<td>GSM8K, MATH, etc</td>
<td>Math</td>
</tr>
<tr>
<td>MetaMathQA (Yu et al., 2024)</td>
<td>395K</td>
<td>GSM8K, MATH</td>
<td>Math</td>
</tr>
<tr>
<td>XwinMath (Li et al., 2024a)</td>
<td>1.4M</td>
<td>GSM8K, MATH</td>
<td>Math</td>
</tr>
<tr>
<td>OrcaMath (Mitra et al., 2024)</td>
<td>200K</td>
<td>GSM8K</td>
<td>Math</td>
</tr>
<tr>
<td>NuminaMath (Li et al., 2024b)</td>
<td>860K</td>
<td>GSM8K, MATH, AIME</td>
<td>Math</td>
</tr>
<tr>
<td>AceMath (Liu et al., 2024)</td>
<td>1.6M</td>
<td>GSM8K, MATH, AIME</td>
<td>Math</td>
</tr>
<tr>
<td>OpenMath-2 (Toshniwal et al., 2024)</td>
<td>14M</td>
<td>GSM8K, MATH</td>
<td>Math</td>
</tr>
<tr>
<td>Critique Fine-Tuning Data (Ours)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>CFT</td>
<td>50K</td>
<td>WebInstruct</td>
<td>STEM</td>
</tr>
</tbody>
</table>
<p>Table 1: The comparison of CFT and SFT datasets.</p>
<h1>3 Experiments</h1>
<p>In this section, we will detail our experiments.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">MATH</th>
<th style="text-align: center;">Minerva-Math</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">OlympiadBench</th>
<th style="text-align: center;">AIME24</th>
<th style="text-align: center;">AMC23</th>
<th style="text-align: center;">AVG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">DeepSeek-Math-7B</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">20.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WebInstruct-SFT</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;">16.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WebInstruct-verified-SFT</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">9.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">21.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WebInstruct-GPT4o-SFT</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">11.8</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">8.9</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;">24.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WebInstruct-CFT</td>
<td style="text-align: center;">42.2</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">27.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\Delta=$ CFT- $\mathrm{SFT}_{\text {best }}$</td>
<td style="text-align: center;">6.4</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">3.5</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-7B</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">37.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WebInstruct-SFT</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">20.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WebInstruct-verified-SFT</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">30.1</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">38.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WebInstruct-GPT4o-SFT</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">19.7</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">36.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WebInstruct-CFT</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">88.8</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">48.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\Delta=$ CFT- $\mathrm{SFT}_{\text {best }}$</td>
<td style="text-align: center;">9.6</td>
<td style="text-align: center;">9.5</td>
<td style="text-align: center;">11.4</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">10.4</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Math-7B</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">13.6</td>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;">16.1</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">37.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WebInstruct-SFT</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">35.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WebInstruct-verified-SFT</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">40.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WebInstruct-GPT4o-SFT</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">50.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WebInstruct-CFT</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">90.9</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">57.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\Delta=$ CFT- $\mathrm{SFT}_{\text {best }}$</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">16.6</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">6.7</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance comparison of SFT and CFT on different base models. All the experiments are trained with WebInstruct subset. We select the checkpoint with highest validation score and report their results.</p>
<h3>3.1 Experimental Setup</h3>
<p>Evaluation Datasets We evaluate our method on a wide range of mathematical reasoning benchmarks. For standard mathematical reasoning, we evaluate MATH (Hendrycks et al., 2021), Minerva-Math (Lewkowycz et al., 2022) and GSM8K (Cobbe et al., 2021). To assess performance on more challenging competition-level mathematics, we incorporate AIME 2024, AMC 2023, and OlympiadBench (He et al., 2024) containing various difficulty levels of Mathematical Olympiad problems. We further extend our evaluation to broader STEM reasoning capabilities through TheoremQA (Chen et al., 2023) for mathematical theorem understanding, MMLU-Pro (Wang et al., 2024b) covering physics, chemistry, mathematics, etc., and GPQA (Rein et al., 2023) for complex problems requiring scientific reasoning.
Training Details We evaluate three different SFT training settings and one CFT training setting in our experiments. For the SFT training settings, we explore: (1) SFT: direct training on original dataset, (2) SFT-verified: training on responses validated by GPT-4o, and (3) SFT-GPT-4o: training on responses generated by GPT-4o. For CFT, we train the model using our curated CFT datasets as described in section 2. We use MATH-500 (Lightman et al., 2023b) as our validation set and select the best-performing checkpoint after training on the</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">#Data</th>
<th style="text-align: center;">MATH</th>
<th style="text-align: center;">GPQA</th>
<th style="text-align: center;">TheoremQA</th>
<th style="text-align: center;">MMLU-Pro</th>
<th style="text-align: center;">OlympiadBench</th>
<th style="text-align: center;">AIME24</th>
<th style="text-align: center;">AMC23</th>
<th style="text-align: center;">AVG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Frontier Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4o (2024-08-06)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">81.1</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">54.7</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">9.3</td>
<td style="text-align: center;">47.5</td>
<td style="text-align: center;">51.7</td>
</tr>
<tr>
<td style="text-align: center;">GPT-o1-mini</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">57.2</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">72.1</td>
</tr>
<tr>
<td style="text-align: center;">Other Open-sourced Reasoning LLMs</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Deepseek-Math-7B-Instruct</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">44.3</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">13.6</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">23.9</td>
</tr>
<tr>
<td style="text-align: center;">Mathstral-7B-v0.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">32.2</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">6.7</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">32.9</td>
</tr>
<tr>
<td style="text-align: center;">NuminaMath-7B-CoT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">38.6</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">6.7</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">29.9</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3.1-8B-Instruct</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">6.7</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">30.3</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3.1-70B-Instruct</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">65.7</td>
<td style="text-align: center;">42.2</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">40.4</td>
</tr>
<tr>
<td style="text-align: center;">NuminaMath-72B-CoT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">24.9</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">52.5</td>
<td style="text-align: center;">39.1</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Math-72B-Instruct</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">50.3</td>
<td style="text-align: center;">60.3</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">56.4</td>
</tr>
<tr>
<td style="text-align: center;">Initialized from Qwen2.5-Math-7B-Base</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Math-Base</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">16.1</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">32.7</td>
</tr>
<tr>
<td style="text-align: center;">Eurus-2-SFT</td>
<td style="text-align: center;">230 K</td>
<td style="text-align: center;">62.4</td>
<td style="text-align: center;">32.1</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">44.2</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">30.1</td>
<td style="text-align: center;">34.3</td>
</tr>
<tr>
<td style="text-align: center;">rStar-Math@Greedy</td>
<td style="text-align: center;">747 K</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">47.5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">AceMath-Qwen2.5-Math</td>
<td style="text-align: center;">2.3 M</td>
<td style="text-align: center;">83.1</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">42.2</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">43.0</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Math-7B-Instruct</td>
<td style="text-align: center;">2.5 M</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">31.1</td>
<td style="text-align: center;">37.0</td>
<td style="text-align: center;">39.5</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">44.6</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Math-7B-CFT</td>
<td style="text-align: center;">50 K</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">47.5</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">48.1</td>
</tr>
</tbody>
</table>
<p>Table 3: Performance comparison of our models vs. other reasoning-specialized models. #Data denotes total training set size. We select the checkpoint with highest validation score.
entire dataset for 1 epoch. We maintain consistent hyperparameters across all experiments with a learning rate of $5 \mathrm{e}-6$, a cosine decay learning schedule with a warm-up ratio of 0.1 , and a global batch size of 512 .</p>
<h1>3.2 Main Results (CFT vs. SFT)</h1>
<p>To evaluate the effectiveness of CFT, we compare it with various SFT methods on three 7B-scale base models using mathematical reasoning benchmarks. Table 2 presents the results across different base models and methods. Our key findings are as follows:
Base Model Selection We experiment with three 7B-scale base models: DeepSeek-Math7B, Qwen2.5-7B, and Qwen2.5-Math-7B. Results show that Qwen2.5-Math-7B serves as a stronger foundation, with its base version achieving $37.8 \%$ average accuracy across benchmarks. When enhanced with CFT, it achieves the best performance with $57.1 \%$ average accuracy.
Performance Gains CFT consistently outperforms all SFT baselines across different models. On DeepSeek-Math-7B, it achieves a $3.5 \%$ absolute improvement over the SFT-GPT4o. On Qwen2.5-7B, it demonstrates a substantial $10.4 \%$ improvement over the SFT-verified. On Qwen2.5-Math-7B, it surpasses the strong GPT-4o SFT baseline by $6.7 \%$ over SFT-GPT4o.</p>
<h3>3.3 More Results (CFT Models vs. Existing Models)</h3>
<p>In Table 3, we compare our best CFT-models with other competitive models with different scales. We expanded the evaluation benchmarks to cover broader STEM topics.
Our Qwen2.5-Math-7B-CFT achieves the highest average performance (48.1\%) among 7B-scale models while using significantly less training data (50K samples). Specifically:</p>
<ul>
<li>It substantially outperforms specialized math models such as Deepseek-Math-7B-Instruct (23.9\%), Mathstral-7B (32.9\%), and NuminaMath-7B-CoT (29.9\%).</li>
<li>This strong performance is achieved with remarkably less training data - only 50K samples compared to AceMath-Qwen2.5-Math (2.3M samples) and Qwen2.5-Math-7B-Instruct (2.5M samples), demonstrating the superior data efficiency of our approach. In addition, our method outperforms Eurus-2-SFT (230K) Cui et al. (2025) and rStar-Math@Greedy (747K) Guan et al. (2025) on most tasks.</li>
</ul>
<p>Despite being smaller in scale, our Qwen2.5-Math-7B-CFT demonstrates strong performance compared to larger models. With only 7B parameters, it achieves better average performance (48.1\%) than Llama-3.1-70B-Instruct (40.4\%) and NuminaMath-72B-CoT (39.1\%). When</p>
<p>compared to Qwen2.5-Math-72B-Instruct (56.4\%), our model shows competitive results on several benchmarks (e.g., $67.5 \%$ vs $70.0 \%$ on AMC23) despite using only one-tenth of the parameters and less training data. While frontier closed models like GPT-4o still maintain a performance lead, our results demonstrate that efficient training strategies can help smaller models achieve strong performance with fewer resources.</p>
<h1>3.4 Comparison with RL-based Method</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Data Size</th>
<th style="text-align: center;">GPU Hours</th>
<th style="text-align: center;">MATH-500</th>
<th style="text-align: center;">Minerva-Math</th>
<th style="text-align: center;">OlympiadBench</th>
<th style="text-align: center;">AIME24</th>
<th style="text-align: center;">AMC23</th>
<th style="text-align: center;">AVG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SimpleRL-Zero</td>
<td style="text-align: center;">$8 \mathrm{~K} \times 12$</td>
<td style="text-align: center;">1152</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">37.9</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">48.9</td>
</tr>
<tr>
<td style="text-align: left;">SimpleRL</td>
<td style="text-align: center;">$8 \mathrm{~K}+8 \mathrm{~K} \times 12$</td>
<td style="text-align: center;">1152</td>
<td style="text-align: center;">82.4</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">$\mathbf{5 0 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">CFT</td>
<td style="text-align: center;">50 K</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">50.2</td>
</tr>
</tbody>
</table>
<p>Table 4: Performance comparison with RL-driven methods.</p>
<p>Recently, researchers have shown that reinforcement learning can significantly boost the reasoning capabilities of large language models. Here, we compare with SimpleRL (Zeng et al., 2025), which is an open replication of DeepSeek-R1 (Guo et al., 2025). We consider pure RL-based training (SimpleRL-Zero) and Distill+RL-based training (SimpleRL) as our competitors, both require $32 \times \mathrm{H} 100$ to train for 1.5 days. In contrast, our method only requires $8 \times \mathrm{H} 100$ to train for 1 hour. Furthermore, our approach does not requirea long decoding length leading to higher efficiency.
As shown in Table 4, CFT can improve Qwen2.5-Math-7B-base to the same level as SimpleRL. On several benchmarks like AMC23 and Minverva-Math, CFT can outperform both SimpleRL significantly. The biggest difference is AIME24, which only contains a test set of 30 questions. The accuracy is heavily impacted by the randomness.</p>
<h3>3.5 Ablation Studies</h3>
<p>To understand the impact of different factors in CFT, we conduct several ablation studies:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">MetaMathQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">NuminaMath</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">WebInstruct</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">CFT</td>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">CFT</td>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">CFT</td>
</tr>
<tr>
<td style="text-align: left;">MATH</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">80.2</td>
</tr>
<tr>
<td style="text-align: left;">Minerva-Math</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">42.3</td>
</tr>
<tr>
<td style="text-align: left;">GSM8K</td>
<td style="text-align: center;">79.5</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">88.3</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">90.9</td>
</tr>
<tr>
<td style="text-align: left;">OlympiadBench</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">36.3</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">41.6</td>
</tr>
<tr>
<td style="text-align: left;">AIME24</td>
<td style="text-align: center;">6.7</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">20.0</td>
</tr>
<tr>
<td style="text-align: left;">AMC23</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">67.5</td>
</tr>
<tr>
<td style="text-align: left;">AVG</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">47.3</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">57.1</td>
</tr>
</tbody>
</table>
<p>Table 5: Performance comparison of SFT and CFT with different training datasets on Qwen2.5-Math-7B.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">SFT</th>
<th style="text-align: center;">GPT-4o-mini-CFT</th>
<th style="text-align: center;">GPT-4o-1120-CFT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MATH</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">80.2</td>
</tr>
<tr>
<td style="text-align: left;">Minerva-Math</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">42.3</td>
</tr>
<tr>
<td style="text-align: left;">GSM8K</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">90.9</td>
</tr>
<tr>
<td style="text-align: left;">OlympiadBench</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">41.6</td>
</tr>
<tr>
<td style="text-align: left;">AIME24</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">20.0</td>
</tr>
<tr>
<td style="text-align: left;">AMC23</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">67.5</td>
</tr>
<tr>
<td style="text-align: left;">AVG</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">57.1</td>
</tr>
</tbody>
</table>
<p>Table 7: Performance comparison of CFT using different teacher critique models (GPT-4o and mini) on Qwen2.5-Math-7B.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">Base</th>
<th style="text-align: center;">Self-generated</th>
<th style="text-align: center;">Other-generated</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MATH</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">78.2</td>
<td style="text-align: center;">80.2</td>
</tr>
<tr>
<td style="text-align: left;">Minerva-Math</td>
<td style="text-align: center;">13.6</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">42.3</td>
</tr>
<tr>
<td style="text-align: left;">GSM8K</td>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">90.9</td>
</tr>
<tr>
<td style="text-align: left;">OlympiadBench</td>
<td style="text-align: center;">16.1</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">41.6</td>
</tr>
<tr>
<td style="text-align: left;">AIME24</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">20.0</td>
</tr>
<tr>
<td style="text-align: left;">AMC23</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">67.5</td>
</tr>
<tr>
<td style="text-align: left;">AVG</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">57.1</td>
</tr>
</tbody>
</table>
<p>Table 6: Comparison between self-generated (by Qwen2.5-Math-7B) and original solutions (from WebInstruct) for CFT training.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">IF_Eval</th>
<th style="text-align: center;"></th>
<th style="text-align: left;">MT-Bench</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">strict</td>
<td style="text-align: center;">loose</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5-Math-7B</td>
<td style="text-align: center;">0.266</td>
<td style="text-align: center;">0.291</td>
<td style="text-align: left;">4.79</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5-Math-7B-Instruct</td>
<td style="text-align: center;">0.333</td>
<td style="text-align: center;">0.345</td>
<td style="text-align: left;">5.49</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5-Math-7B-SFT</td>
<td style="text-align: center;">0.315</td>
<td style="text-align: center;">0.330</td>
<td style="text-align: left;">5.23</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5-Math-7B-verified-SFT</td>
<td style="text-align: center;">0.328</td>
<td style="text-align: center;">0.341</td>
<td style="text-align: left;">5.41</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5-Math-7B-GPT4o-SFT</td>
<td style="text-align: center;">0.325</td>
<td style="text-align: center;">0.343</td>
<td style="text-align: left;">5.38</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5-Math-7B-CFT</td>
<td style="text-align: center;">0.335</td>
<td style="text-align: center;">0.362</td>
<td style="text-align: left;">6.49</td>
</tr>
</tbody>
</table>
<p>Table 8: Performance comparison across different models on Instruction Following (IF_Eval, instruction-level) and general instruction alignment (MT-Bench).</p>
<p>Dataset Source We ablate the impact of different training datasets on model performance. As shown in Table 5, when trained with SFT, both MetaMathQA and NuminaMath achieve better performance than WebInstruct ( $47.3 \%$ and $37.5 \%$ vs. $35.1 \%$ on average), indicating their higher data quality. However, when trained with CFT, WebInstruct surprisingly achieves the best performance ( $57.1 \%$ ), outperforming both MetaMathQA and NuminaMath. This suggests that the effectiveness of CFT is not solely determined by the quality of solution data. Instead, by learning to identify and critique incorrect solutions, the model can develop stronger mathematical reasoning capabilities even from imperfect demonstrations, highlighting the robustness and effectiveness of our critique-based learning approach.
Response Source We compare two sources of solutions for CFT training: solutions generated by Qwen2.5-Math-7B itself and original solutions from the WebInstruct dataset. Table 6 shows that using original solutions achieves comparable performance ( $57.1 \%$ vs. $55.1 \%$ on average), with some variation across different benchmarks. The improvement is more noticeable on challenging datasets like Minerva-Math ( $9.2 \%$ increase). These results demonstrate that CFT is robust to different solution sources and can effectively learn from both model-generated and original solutions from the WebInstruct dataset.
Teacher Critique Model To understand the impact of critique model quality on CFT, we compare the performance when using GPT-4o-mini and GPT-4o-1120 as critique models in Table 7. First, we observe that even with a relatively modest critique model GPT-4o-mini, CFT significantly outperforms SFT-verified baseline ( $52.0 \%$ vs. $40.4 \%$ on average), with substantial improvements on MATH ( $11.9 \%$ increase) and Minerva-Math ( $23.9 \%$ increase). This demonstrates the effectiveness of CFT without requiring an extremely powerful critique model. Furthermore, using a stronger critique model GPT-4o-1120 leads to even better performance across all benchmarks ( $57.1 \%$ on average), with notable gains on GSM8K ( $6.4 \%$ increase) and OlympiadBench ( $6.5 \%$ increase). These results confirm that while CFT is effective with modest critique models, stronger critique models can provide more accurate and insightful feedback, leading to better mathematical reasoning capabilities. In the future, we plan to leverage o1 or even o3 as teacher critique model to understand the potential.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">MATH</th>
<th style="text-align: center;">Minerva-Math</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">OlympiadBench</th>
<th style="text-align: center;">AIME24</th>
<th style="text-align: center;">AMC23</th>
<th style="text-align: center;">AVG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Qwen2.5-Math-7B-CFT</td>
<td style="text-align: center;">$\mathbf{8 0 . 2}$</td>
<td style="text-align: center;">$\mathbf{4 2 . 3}$</td>
<td style="text-align: center;">$\mathbf{9 0 . 9}$</td>
<td style="text-align: center;">$\mathbf{4 1 . 6}$</td>
<td style="text-align: center;">$\mathbf{2 0 . 0}$</td>
<td style="text-align: center;">$\mathbf{6 7 . 5}$</td>
<td style="text-align: center;">$\mathbf{5 7 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">Mix 50K CFT + 50K AceMath SFT</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">89.9</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">54.8</td>
</tr>
<tr>
<td style="text-align: left;">CFT + 50K AceMath SFT (two-stage)</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">35.4</td>
<td style="text-align: center;">88.2</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">51.6</td>
</tr>
<tr>
<td style="text-align: left;">AceMath SFT (50K)</td>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">52.5</td>
<td style="text-align: center;">49.7</td>
</tr>
</tbody>
</table>
<p>Table 9: Comparison of strategies for combining CFT with high-quality SFT data</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">MATH</th>
<th style="text-align: center;">Minerva-Math</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">OlympiadBench</th>
<th style="text-align: center;">AIME24</th>
<th style="text-align: center;">AMC23</th>
<th style="text-align: center;">AVG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">WebInstruct-SFT</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">35.1</td>
</tr>
<tr>
<td style="text-align: left;">WebInstruct-verified-SFT</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">40.4</td>
</tr>
<tr>
<td style="text-align: left;">WebInstruct-GPT4o-SFT</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">50.4</td>
</tr>
<tr>
<td style="text-align: left;">WebInstruct-CFT</td>
<td style="text-align: center;">$\mathbf{8 0 . 2}$</td>
<td style="text-align: center;">$\mathbf{4 2 . 3}$</td>
<td style="text-align: center;">$\mathbf{9 0 . 9}$</td>
<td style="text-align: center;">$\mathbf{4 1 . 6}$</td>
<td style="text-align: center;">$\mathbf{2 0 . 0}$</td>
<td style="text-align: center;">$\mathbf{6 7 . 5}$</td>
<td style="text-align: center;">$\mathbf{5 7 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">WebInstruct-CFT-Short</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">40.1</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">55.2</td>
</tr>
</tbody>
</table>
<p>Table 10: Ablation on controlling token length (CFT vs. SFT)
Combination with High-Quality SFT Data We investigate whether combining CFT with high-quality SFT datasets can further improve performance. As shown in Table 9, mixing AceMath SFT data or applying it in a two-stage training setting slightly degrades performance compared to pure CFT training ( $54.8 \%$ and $51.6 \%$ vs. $57.1 \%$ on average, respectively). This indicates that incorporating standard supervised instruction data may conflict with critique-based training objectives. Therefore, purely learning to critique incorrect solutions is more effective for reasoning tasks than jointly learning to imitate high-quality solutions.
Controlling for Token Length Since critique-based training typically involves longer sequences, one might suspect this as the primary reason for its superior performance. To address this concern, we select WebInstruct data with token lengths similar to standard SFT training data and perform critique fine-tuning. The results in Table 10 show that even when controlling token lengths, shorter-length critique data still significantly outperforms SFT</p>
<p>baselines ( $55.2 \%$ vs. $50.4 \%$ on average). This confirms that the advantage of CFT comes from the critique-based learning mechanism rather than merely increased token length.</p>
<h1>General Generation and Instruction-Following Capability</h1>
<p>To assess whether CFT affects the general generation and instruction-following abilities, we evaluate the models on MT-Bench (Zheng et al., 2023), a benchmark for general instruction alignment, and IF_Eval (Zhou et al., 2023), a comprehensive instruction-following evaluation suite. As shown in Table 8, Qwen2.5-Math-7B-CFT achieves a score of 6.49 on MTBench, significantly improving over the base Qwen2.5-Math-7B (4.79) and instruction-tuned Qwen2.5-Math-7B-Instruct (5.49). It also outperforms various SFT approaches, including standard SFT (5.23), verified-SFT (5.41), and GPT4o-SFT (5.38). On IF_Eval, Qwen2.5-Math-7B-CFT demonstrates superior instruction-following ability with the highest scores in both strict mode ( 0.335 ) and loose mode ( 0.362 ), outperforming all other variants including the instruction-tuned model. These comprehensive improvements across different benchmarks demonstrate that critique-based training not only enhances mathematical reasoning but also improves general instruction-following capabilities and text generation quality. This indicates that the critique mechanism helps the model develop more robust general capabilities even when trained primarily on mathematical content, suggesting positive transfer between specialized and general abilities.</p>
<h2>4 Limitations</h2>
<p>The Noisy Critique Data Our ablation study indicates that the quality of critique feedback notably influences the effectiveness of CFT. A manual inspection of 50 critique samples generated by GPT-4o-1120 on WebInstruct reveals that roughly 20\% contain inaccuracies, such as misjudging correct steps, missing errors, or providing imprecise explanations (see Appendix A. 3 for examples). This highlights the importance of using higher-quality critique data to further enhance CFT. Future research could explore automated critique verification methods or develop curated datasets with human-verified critiques to improve mathematical reasoning capabilities.</p>
<h2>Limitations of Self-Critique</h2>
<p>We explored incorporating self-critique mechanisms into our framework, where the model critiques and iteratively refines its own outputs. However, we found these approaches consistently underperformed compared to direct inference. In particular, self-critique suffered from inconsistent critique standards, where the model either overlooked genuine errors or mistakenly flagged correct solutions as incorrect. Additionally, increasing sampling temperatures-necessary to maintain diversity and avoid repetition during iterative refinement-introduced instability, further degrading performance. Due to these challenges, our final CFT implementation relies on direct inference without self-critique. For detailed experimental results, specific methodological comparisons, and further analysis of observed issues, please refer to Appendix A.4.</p>
<h2>5 Related Work</h2>
<h3>5.1 Instruction Tuning</h3>
<p>Instruction tuning is one of the most crucial part of aligning pre-trained language models with human expectations. The current instruction-tuning datasets are either based on (1) human annotation: such as FLAN (Wei et al., 2022), T0 (Sanh et al., 2022), SuperNI (Wang et al., 2022), which compiles large instruction-tuning datasets from existing human-labeled datasets; and (2) model synthesis: such as Self-Instruct (Wang et al., 2023b), WizardLM (Xu et al., 2024), WildChat (Zhao et al., 2024), which creates instruction-tuning datasets by synthesizing from powerful LLMs (Achiam et al., 2023). Both types of instruction datasets have shown great performance improvement of LMs on general evaluation tasks. More recently, Tulu (Wang et al., 2023a) and Tulu-3 (Lambert et al., 2024) have explored how to combine existing post-training data and algorithms to maximize LMs' performance.</p>
<h1>5.2 Mathematical Instruction Tuning</h1>
<p>Taking this further, math-instructed models have been developed to advance LLM performance in the mathematical domain (Luo et al., 2023; Yue et al., 2024a; Yu et al., 2024; Li et al., 2024a; Mitra et al., 2024; Li et al., 2024b). Recently, there has been a wave to scale up the math instruction dataset to millions of examples like MAmmoTH2 (Yue et al., 2024b), Open-MathInstruct (Toshniwal et al., 2024), and AceMath (Liu et al., 2024) and Qwen2.5Math-Instruct (Yang et al., 2024b). These methods have shown tremendous performance gains on math reasoning datasets. However, we also observe diminishing marginal gain by further scaling the instruction data up, suggesting that a more efficient training algorithm is needed. In this paper, we aim to challenge SFT and propose a much more efficient learning algorithm CFT and show similar performance with only 1-10\% of SFT data.</p>
<h3>5.3 Critique Learning</h3>
<p>Teaching AIs to critique has been a long standing goal in the pursuit of AGI.
Self-Correction The concept of 'self-correction' has been emerged as a promising direction in LLMs since 2023. There has been a line of work (Madaan et al., 2024; Welleck et al., 2023; Shinn et al., 2024; Bai et al., 2022; Ganguli et al., 2023; Gou et al., 2023) aiming to use feedback from the model itself to further improve its performance. However, a later work (Huang et al., 2023; Valmeekam et al., 2023) revealed that the self-correction in reasoning is not quite reliable. More recently, with the rise of GPT-o1 (Jaech et al., 2024), LLM self-correction has again demonstrated its potential to improve LLMs' own reasoning capabilities.
Critique Model A critique model differs from self-correction in that it employs a specialized model to provide feedback to another model during generation. In mathematical reasoning, critique models often take the form of reward models. Recent work has explored both outcome-based reward models (Uesato et al., 2022; Yang et al., 2024b) and process-based reward models (Wang et al., 2024a; Lightman et al., 2023a; Yuan et al., 2024) to enhance reasoning capabilities of language models. However, these approaches typically focus on directly predicting reward scores without explicitly providing intermediate reasoning or explanations. The most similar prior work to ours is critique-out-loud (Ankner et al.), which functions solely as a reward estimator rather than directly guiding the generation process.
In contrast, our proposed approach differs substantially from these existing methods. We leverage critique feedback explicitly as a training objective to encourage deeper understanding and reasoning about the given problems. At inference time, the trained model generates responses directly, without involving any explicit critique or iterative refinement steps.</p>
<h2>6 Conclusion</h2>
<p>In this paper, we introduced Critique Fine-Tuning (CFT), a novel paradigm that trains language models to critique and analyze responses rather than imitating them as in traditional SFT. Experiments demonstrated that CFT consistently outperforms SFT by 4-10\% on mathematical reasoning benchmarks, achieves comparable performance to resource-intensive RL methods using significantly fewer training examples ( 50 K vs. $2 \mathrm{M}+$ ) and compute ( 8 H100 GPU-hours), and generalizes effectively to broader STEM domains. Interestingly, even without explicit instruction tuning via traditional SFT or RL, CFT-trained models inherently exhibit strong instruction-following capabilities, challenging conventional assumptions regarding the necessity of imitation-based training for instruction-following tasks. These findings suggest that explicitly teaching models to identify and critique incorrect reasoning can significantly enhance their reasoning and generalization capabilities. Future research directions include improving the quality and coverage of critique data, enabling models to perform self-critique for continual self-improvement, combining CFT with complementary training paradigms like RL, extending the approach to multi-modal scenarios, and further investigating its theoretical foundations. Overall, we believe CFT represents a promising step forward in making language model training more efficient, robust, and effective, potentially reducing the computational and data requirements for developing high-performing models while substantially improving their reasoning and instruction-following abilities.</p>
<h1>References</h1>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.</p>
<p>Zachary Ankner, Mansheej Paul, Brandon Cui, Jonathan Daniel Chang, and Prithviraj Ammanabrolu. Critique-out-loud reward models. In Pluralistic Alignment Workshop at NeurIPS 2024.</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.</p>
<p>Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. Theoremqa: A theorem-driven question answering dataset. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 7889-7901, 2023.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan Yao, Xu Han, Hao Peng, Yu Cheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, and Ning Ding. Process reinforcement through implicit rewards, 2025. URL https://arxiv.org/abs/2502.01456.</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.</p>
<p>Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas I Liao, Kamilė Lukošiūtė, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, et al. The capacity for moral self-correction in large language models. arXiv preprint arXiv:2302.07459, 2023.</p>
<p>Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Nan Duan, Weizhu Chen, et al. Critic: Large language models can self-correct with tool-interactive critiquing. In The Twelfth International Conference on Learning Representations, 2023.</p>
<p>Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. rstar-math: Small llms can master math reasoning with self-evolved deep thinking, 2025. URL https://arxiv.org/abs/2501.04519.</p>
<p>Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. Deepseek-coder: When the large language model meets programming-the rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024.</p>
<p>Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.</p>
<p>Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.</p>
<p>Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. In The Twelfth International Conference on Learning Representations, 2023.</p>
<p>Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.</p>
<p>Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. T $\backslash$ " ulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843-3857, 2022.</p>
<p>Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han Hu, Zheng Zhang, and Houwen Peng. Common 7b language models already possess strong math capabilities. arXiv preprint arXiv:2403.04706, 2024a.</p>
<p>Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Q Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13, 2024b.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050, 2023a.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050, 2023b.</p>
<p>Zihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Acemath: Advancing frontier math reasoning with post-training and reward modeling. arXiv preprint arXiv:2412.15084, 2024.</p>
<p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023.</p>
<p>Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct. In The Twelfth International Conference on Learning Representations, 2024.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. Orca-math: Unlocking the potential of slms in grade school math. arXiv preprint arXiv:2402.14830, 2024.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730-27744, 2022.</p>
<p>David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R Bowman. Gpqa: A graduate-level googleproof q\&amp;a benchmark. arXiv preprint arXiv:2311.12022, 2023.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, et al. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations, 2022.</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.</p>
<p>Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.</p>
<p>Shubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor Gitman. Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data. arXiv preprint arXiv:2410.01560, 2024.</p>
<p>Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.</p>
<p>Karthik Valmeekam, Matthew Marquez, and Subbarao Kambhampati. Can large language models really improve by self-critiquing their own plans? In NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023.</p>
<p>Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 9426-9439, 2024a.</p>
<p>Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, et al. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 5085-5109, 2022.</p>
<p>Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? exploring the state of instruction tuning on open resources. Advances in Neural Information Processing Systems, 36:74764-74786, 2023a.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 13484-13508, 2023b.</p>
<p>Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. MMLU-pro: A more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024b. URL https://openreview.net/forum?id=y1#DM6R2r3.</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022.</p>
<p>Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. Generating sequences by learning to self-correct. In The Eleventh International Conference on Learning Representations, 2023.</p>
<p>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, and Daxin Jiang. WizardLM: Empowering large pre-trained language models to follow complex instructions. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=CfXh93NOgH.</p>
<p>An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024a.</p>
<p>An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5 -math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024b.</p>
<p>Longhui Yu, Weisen Jiang, Han Shi, YU Jincheng, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. In The Twelfth International Conference on Learning Representations, 2024.</p>
<p>Lifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan Liu, and Hao Peng. Free process rewards without process labels. arXiv preprint arXiv:2412.01981, 2024.</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. In The Twelfth International Conference on Learning Representations, 2024a.</p>
<p>Xiang Yue, Tianyu Zheng, Ge Zhang, and Wenhu Chen. MAmmoTH2: Scaling instructions from the web. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024b. URL https://openreview.net/forum?id=yVu5dnPlqA.</p>
<p>Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025.</p>
<p>Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. Wildchat: 1 m chatgpt interaction logs in the wild. In The Twelfth International Conference on Learning Representations, 2024.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:46595$46623,2023$.</p>
<p>Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023.</p>
<h1>A Appendix</h1>
<h2>A. 1 GPT-4o Case Study for Critique Data Generation</h2>
<p>To illustrate our approach for generating critique data, we present an example case using GPT-4o as the critique generator. The following shows the input prompt, student solution, and the resulting critique generated by GPT-4o.</p>
<h2>Input Prompt to GPT-4o</h2>
<p>You are a science expert. A student is trying to solve a question, please explain briefly whether his answer is correct or not. Finally, conclude your judgement with 'Conclusion: right/wrong [END]'
Question:
How does mitosis contribute to the maintenance and continuity of life in eukaryotic organisms?
Solution:
Mitosis plays a crucial role in maintaining and continuing life in eukaryotic organisms by:</p>
<ul>
<li>Growth and Development: Mitosis allows for the production of new cells, enabling organisms to increase in size and develop from a single cell into a multicellular entity. * Tissue Repair and Regeneration: When tissues are damaged, mitosis generates new cells to replace the lost or injured ones, facilitating wound healing and tissue regeneration. * Cell Replacement: Mitosis continually replaces old or dying cells with new ones, maintaining the integrity and function of tissues and organs. * Reproduction: In asexual reproduction, mitosis is the primary mechanism by which new individuals are produced. It also plays a role in sexual reproduction by generating gametes (eggs and sperm) that combine to form a new organism.
Without mitosis, eukaryotic organisms would be unable to grow, repair themselves, or reproduce, ultimately compromising their survival and the continuity of life.</li>
</ul>
<h2>GPT-4o Generated Critique</h2>
<p>The student's answer is mostly correct, but there is a significant error in the explanation regarding the role of mitosis in reproduction. While mitosis is indeed essential for growth, tissue repair, and cell replacement, it is not involved in the production of gametes (eggs and sperm) in sexual reproduction. Gametes are produced through a different process called meiosis, not mitosis. Mitosis is involved in asexual reproduction for some organisms, but it does not play a direct role in sexual reproduction.
The rest of the answer correctly highlights the importance of mitosis in growth, tissue repair, and cell replacement, which are vital for the maintenance and continuity of life.
Conclusion: wrong [END]</p>
<p>This example demonstrates how GPT-4o effectively identifies the conceptual error in the student's answer regarding reproduction. The critique correctly points out that meiosis, not mitosis, is responsible for gamete production, while acknowledging the accurate components of the answer. The structured output concludes with a clear judgment, facilitating further computational processing of the critique data. We also used critique data obtained through this instruction to filter and create the WebInstruct-GPT-4o-verified-SFT dataset.</p>
<h1>A. 2 GPT-4o Case Study for WebInstruct-GPT-4o-SFT dataset Generation</h1>
<p>To illustrate how we generated the WebInstruct-GPT-4o-SFT dataset, we present an example prompt and the corresponding output from GPT-4o.</p>
<h2>Input Prompt to GPT-4o</h2>
<p>You are a science expert. You need to answer a given question clearly and conclude your answer with 'Answer: [YOUR ANSWER]
Question: How do you find the derivative of a quadratic function using the Power Rule?</p>
<h2>GPT-4o Generated Answer</h2>
<p>To find the derivative of a quadratic function using the Power Rule, follow these steps. A quadratic function is generally in the form:
$f(x)=a x^{2}+b x+c$
where $a, b$, and $c$ are constants.
The Power Rule for differentiation states that if you have a function $f(x)=x^{n}$, then its derivative $f^{\prime}(x)$ is given by:
$f^{\prime}(x)=n x^{n-1}$
Apply the Power Rule to each term of the quadratic function:</p>
<ol>
<li>Differentiate $a x^{2}$ : - Here, $n=2$, so using the Power Rule: $\frac{d}{d x}\left(a x^{2}\right)=2 a x^{2-1}=2 a x$</li>
<li>Differentiate $b x$ : - Here, $n=1$, so using the Power Rule: $\frac{d}{d x}(b x)=1 \cdot b x^{1-1}=b$</li>
<li>Differentiate $c$ : - Since $c$ is a constant, its derivative is 0 .</li>
</ol>
<p>Combine these results to find the derivative of the entire quadratic function:
$f^{\prime}(x)=2 a x+b$
Thus, the derivative of the quadratic function $f(x)=a x^{2}+b x+c$ is:
$f^{\prime}(x)=2 a x+b$
Answer: The derivative of a quadratic function $f(x)=a x^{2}+b x+c$ using the Power Rule is $f^{\prime}(x)=2 a x+b$.</p>
<p>This example demonstrates how GPT-4o provides clear, structured mathematical explanations with step-by-step reasoning. The response includes proper mathematical notation, methodical breakdown of the differentiation process, and a concise final answer. These high-quality responses formed the foundation of our WebInstruct-GPT-4o-SFT dataset.</p>
<h2>A. 3 Case Study: Noisy Critique Data</h2>
<p>Here we present examples of noisy critique data and analyze their impact on model performance:</p>
<h1>A.3.1 Case 1: Incorrect Critique of Valid Solution</h1>
<h2>Case 1: Incorrect Critique of Valid Solution</h2>
<h2>Problem</h2>
<p>What is flawed in the counter-proof attempt against the statement that two randomly chosen points on the surface of an N -dimensional unit sphere with a large N are nearly orthogonal? The counter-proof involves selecting a random arc with length in the interval $(0, \pi)$ and placing it on the sphere's surface at a random orientation, claiming that the ends of the arc don't have a high probability of being nearly orthogonal.</p>
<h2>Original Solution</h2>
<p>The flaw in the counter-proof attempt lies in the misunderstanding of the uniform distribution of randomly selected points on the sphere's surface. The original statement assumes that the points are chosen independently and uniformly, whereas the counter-proof proposes a specific distribution by creating a random arc. This distribution does not represent the uniform selection of two points on the sphere.
The Bertrand Paradox, a related concept in probability, demonstrates that different methods of selecting a random chord in a circle can lead to different probabilities for the chord being longer than half the circle's radius. Similarly, in the N -dimensional case, the probability of orthogonality depends on the method of random selection, and the arc method does not refute the original statement about two uniformly chosen points.
To address the issue, one should focus on the probability of two points being nearly orthogonal when they are each chosen independently and uniformly from the sphere's surface, which is the basis of the original statement.</p>
<h2>Incorrect Critique</h2>
<p>Your explanation is on the right track, but it could be clarified further. The key issue with the counter-proof is that it introduces a biased method of selecting points on the sphere. By constructing a random arc and using its endpoints, the counter-proof imposes a specific correlation between the two points. This violates the assumption of independence and uniformity in the original statement.
In high-dimensional spaces (large $N$ ), the geometry of the sphere leads to the phenomenon where two points chosen independently and uniformly are almost always nearly orthogonal. This is a result of the concentration of measure phenomenon, which is a well-established property in high-dimensional geometry. The counter-proof's method of selecting points via an arc does not reflect this uniform and independent selection process, and thus cannot serve as a valid refutation of the original statement.
Conclusion: wrong [END]</p>
<h2>Analysis</h2>
<p>The critique is overly harsh, as the original solution already provides a clear and sufficient answer to the core question.</p>
<h1>A.3.2 Case 2: Missed Errors in Solution</h1>
<h2>Case 2: Missed Errors in Solution</h2>
<h2>Problem</h2>
<p>Find the roots of the following polynomial: $\frac{43 x^{2}}{5}+5 x+\frac{28}{5}$.</p>
<h2>Original Solution</h2>
<p>To find the roots of the polynomial, we can use the quadratic formula:
$x=\frac{-b \pm \sqrt{b^{2}-4 a c}}{2 a}$
where a, $b$, and $c$ are the coefficients of the polynomial.
In this case, we have:
$a=\frac{43}{5}, b=5, c=\frac{28}{5}$
Substituting these values into the quadratic formula, we get: $x=$ $\frac{-5 \pm \sqrt{b^{2}-4\left(\frac{43}{5}\right)\left(\frac{28}{5}\right)}}{2\left(\frac{43}{5}\right)}$
Simplifying this expression, we get:
$x=\frac{-5 \pm \sqrt{25-\frac{4328}{5}}}{2\left(\frac{43}{5}\right)} x=\frac{-5 \pm \sqrt{25-238}}{2\left(\frac{43}{5}\right)} x=\frac{-5 \pm \sqrt{-213}}{2\left(\frac{43}{5}\right)} x=\frac{-5 \pm i \sqrt{213}}{2\left(\frac{43}{5}\right)} x=$ $\frac{-5 \pm i \sqrt{4191}}{86}$
Therefore, the roots of the polynomial are $x=\frac{-5 \pm i \sqrt{4191}}{86}$.</p>
<h2>Inadequate Critique</h2>
<p>Let's verify this step by step:
The quadratic formula $\frac{-b \pm \sqrt{b^{2}-4 a c}}{2 a}$ was correctly applied For the given equation $\frac{43 x^{2}}{5}+5 x+\frac{28}{5}: a=\frac{43}{5} b=5 c=\frac{28}{5}$ When we substitute: $b^{2}=254 a c=4\left(\frac{43}{5}\right)\left(\frac{28}{5}\right)=238 b^{2}-4 a c=25-238=-213$ Therefore: $x=\frac{-5 \pm i \sqrt{213}}{2\left(\frac{43}{5}\right)}=\frac{-5 \pm i \sqrt{4191}}{86}$ The denominator 86 comes from $2 \cdot \frac{43}{5} \cdot \frac{5}{5}=86$, and $\sqrt{213} \cdot \sqrt{19.67} \approx \sqrt{4191}$
Thus, the answer $x=\frac{-5 \pm i \sqrt{4191}}{86}$ must be correct. Crituque Conclusion: correct [END]</p>
<h2>Correct Analysis</h2>
<p>The critical calculation error occurs in step 4: The transformation from $\frac{-5 \pm i \sqrt{213}}{2\left(\frac{43}{5}\right)}$ to $\frac{-5 \pm i \sqrt{4191}}{86}$ is incorrect. 1) While the denominator's 5 was properly handled $\left(2 \cdot \frac{43}{5}=\frac{86}{5}\right)$ 2) It ignored that this coefficient 5 must affect the numerator as well The correct simplification should be: $\frac{-5 \pm i \sqrt{213}}{2\left(\frac{43}{5}\right)}=\frac{-25 \pm 5 i \sqrt{213}}{86}$.</p>
<h2>A. 4 Detailed Analysis of Self-Critique Methods</h2>
<p>We experimented with two self-critique approaches during inference:</p>
<ol>
<li>
<p>Single-pass self-critique: The model generates and critiques its solution within one inference pass, and regenerates if errors are detected.</p>
</li>
<li>
<p>Two-stage self-critique: The model separately evaluates its solutions in a dedicated critique step and iteratively regenerates (up to 8 attempts) until a satisfactory solution is found.</p>
</li>
</ol>
<p>Table 11 shows the comparative performance of these methods across various temperature settings.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Temperature</th>
<th style="text-align: center;">MATH</th>
<th style="text-align: center;">Minerva-Math</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">42.3</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">38.9</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">37.7</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">34.1</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">36.7</td>
</tr>
<tr>
<td style="text-align: left;">Single-pass self-critique</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">35.2</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">34.3</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">38.2</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">75.8</td>
<td style="text-align: center;">35.4</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">34.6</td>
</tr>
</tbody>
</table>
<p>Table 11: Comparison of inference methods across various temperature settings.
Our analysis revealed several issues that limit the effectiveness of self-critique:</p>
<ol>
<li>Inconsistent critique standards: The model often applies inconsistent criteria when evaluating its own work, leading to either missed errors (false negatives) or incorrectly flagging valid solutions as problematic (false positives).</li>
<li>Temperature sensitivity: Higher temperatures introduce variability that compounds across iterations, making the overall process less stable. The single-pass method drops from $77.2 \%$ to $73.5 \%$ on MATH as temperature increases from 0.1 to 0.6 , with similar trends on Minerva-Math.</li>
<li>Regeneration inefficiency: Even when errors are correctly identified, the model often struggles to effectively address the specific issues in subsequent regeneration attempts, sometimes introducing new errors while fixing others.</li>
<li>Computational overhead: The iterative nature of self-critique significantly increases inference time and computational cost, with diminishing returns on performance.</li>
</ol>
<p>The two-stage method performs slightly better than the single-pass approach but still underperforms relative to direct inference. This suggests that while our model benefits from critique-based training, applying self-critique during inference creates additional complexity that the model struggles to navigate effectively.</p>
<h1>A.4.1 Inference Method Prompts</h1>
<p>We present the prompt templates used in our different inference approaches:</p>
<h2>Direct Inference Template</h2>
<h2>Direct Inference Prompt Template</h2>
<p>Please reason step by step, and put your final answer within boxed.
Question: [Problem text here]
Answer: Let's solve this step by step: [Solution steps] Therefore, the final answer is ANSWER.</p>
<h2>Single-pass Self-critique Template</h2>
<h1>Single-pass Self-critique Prompt Template</h1>
<p>Please reason step by step to solve this problem and then critique your solution. If any errors are found, provide a corrected solution. Please put your final answer within $\square$.
Question: [Problem text here]
Answer: Let's solve this first: [Initial solution steps] Therefore, my initial answer is ANSWER.
Critique: [Critique points]
[If errors found: Based on my critique, let me provide a corrected solution: Corrected solution: ...]</p>
<h2>Two-stage Self-critique Template</h2>
<h2>Two-stage Self-critique Process</h2>
<h2>Stage 1 (Solution Generation):</h2>
<p>Please reason step by step, and put your final answer within boxed.
Question: [Problem text here]
Answer: Let's solve this step by step: [Solution steps] Therefore, the final answer is $\overline{A N S W E R}$.</p>
<h2>Stage 2 (Critique):</h2>
<p>Please critique whether the following solution to the question is correct. Question: [Problem text here] Solution: [Previous solution]
Critique: 1. [Critique point 1] 2. [Critique point 2] ...
Critique Conclusion: Correct/Incorrect
If the conclusion is "Incorrect", the process returns to Stage 1 for a new solution attempt. This iterative process continues until either:</p>
<ul>
<li>The critique conclusion becomes "Correct", indicating a satisfactory solution has been found, or</li>
<li>The maximum number of iterations (8) is reached, in which case the last generated solution is used as the final answer.
The complete process can be represented as:
For i in range(1, 9): 1. Generate solution (Stage 1) 2. Critique solution (Stage 2) 3. If critique conclusion is "Correct": Return current solution and exit 4. If i $==8$ : Return current solution 5 . Continue to next iteration</li>
</ul>
<h1>A.4.2 Observed Issues in Self-Critique Methods</h1>
<h2>Common Issues with Self-Critique Methods</h2>
<p>Our analysis revealed several systematic issues with self-critique approaches:</p>
<h2>Inconsistent Self-Evaluation</h2>
<p>We observed the following patterns of inconsistency in self-evaluation:</p>
<ul>
<li>Over-lenient Evaluation: Cases where the model failed to identify clear logical errors in its own reasoning</li>
<li>False Negatives: Instances where correct solutions were incorrectly flagged as erroneous</li>
</ul>
<h2>Specific Failure Cases</h2>
<p>Here we document specific examples where self-critique methods failed:</p>
<h2>Example 1: Over-lenient Evaluation</h2>
<h2>Problem</h2>
<p>Chandra has four bowls. Each one is a different color (red, blue, yellow, green). She also has exactly one glass the same color as each bowl. If she chooses a bowl and a glass from the cupboard, how many pairings are possible? One such pairing is a blue bowl and a yellow glass.</p>
<h2>Original Solution</h2>
<p>To solve this, we need to determine the total number of possible pairings of one bowl and one glass when there are four bowls and four glasses, each uniquely paired by color.
Reasoning: 1. Chandra has ${ }^{<em> </em>} 4$ bowls<strong> (red, blue, yellow, green) and ${ }^{<em> </em>} 4$ glasses</strong> (red, blue, yellow, green). 2. Each bowl matches exactly one glass by color. For example: - The red bowl matches with the red glass - The blue bowl matches with the blue glass - The yellow bowl matches with the yellow glass - The green bowl matches with the green glass 3. Since each bowl must pair with its corresponding glass, there is only ${ }^{<em> </em>} 1$ valid pairing** (a red bowl with a red glass, etc.).</p>
<h2>Self-Critique</h2>
<p>The question asks for the number of possible pairings, but based on the problem setup, only one valid pairing exists. The reasoning above is correct. ${ }^{<em> </em>}$ Conclusion: Correct [END]**</p>
<h1>Example 2: False Error Detection</h1>
<h2>Problem</h2>
<p>Chandra has four bowls. Each one is a different color (red, blue, yellow, green). She also has exactly one glass the same color as each bowl. If she chooses a bowl and a glass from the cupboard, how many pairings are possible? One such pairing is a blue bowl and a yellow glass.</p>
<h2>Original Solution</h2>
<p>To solve this problem, we need to calculate the total number of possible pairings of bowls and glasses. Each bowl can be paired with any of the 4 glasses.
Here are the steps:</p>
<ol>
<li>Chandra has 4 bowls and 4 glasses. 2. Each bowl can be paired with any one of the 4 glasses. 3. Therefore, the total number of pairings is calculated as the product of the number of bowls and the number of glasses:</li>
</ol>
<p>$$
4 \times 4=16
$$</p>
<h2>Self-Critique</h2>
<p>The reasoning provided is incorrect. While the calculation of the total number of pairings as $4 \times 4=16$ is mathematically correct, the explanation is flawed. The problem does not state that each bowl must be paired with a glass of a different color. Therefore, the assumption that each bowl can only be paired with a glass of the same color is invalid.
Conclusion: ${ }^{<em> </em>}$ Critique Conclusion: Incorrect**</p>            </div>
        </div>

    </div>
</body>
</html>
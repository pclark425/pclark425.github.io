<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9286 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9286</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9286</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-279243781</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.05614v1.pdf" target="_blank">Which Prompting Technique Should I Use? An Empirical Investigation of Prompting Techniques for Software Engineering Tasks</a></p>
                <p><strong>Paper Abstract:</strong> A growing variety of prompt engineering techniques has been proposed for Large Language Models (LLMs), yet systematic evaluation of each technique on individual software engineering (SE) tasks remains underexplored. In this study, we present a systematic evaluation of 14 established prompt techniques across 10 SE tasks using four LLM models. As identified in the prior literature, the selected prompting techniques span six core dimensions (Zero-Shot, Few-Shot, Thought Generation, Ensembling, Self-Criticism, and Decomposition). They are evaluated on tasks such as code generation, bug fixing, and code-oriented question answering, to name a few. Our results show which prompting techniques are most effective for SE tasks requiring complex logic and intensive reasoning versus those that rely more on contextual understanding and example-driven scenarios. We also analyze correlations between the linguistic characteristics of prompts and the factors that contribute to the effectiveness of prompting techniques in enhancing performance on SE tasks. Additionally, we report the time and token consumption for each prompting technique when applied to a specific task and model, offering guidance for practitioners in selecting the optimal prompting technique for their use cases.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9286.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9286.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ES-KNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Exemplar Selection KNN</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that retrieves semantically similar in-context exemplars using k-nearest-neighbor over embedding vectors and appends them to the prompt to provide demonstrations tailored to the current instance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Aggregate (DeepSeek-V3, Qwen2.5-Coder-32B-Instruct, Llama-3.3-70B-Instruct, OpenAI o3-mini)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Clone Detection; Code Translation; Assert Generation; Exception Type Prediction (and others where ES-KNN ranked top)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Code understanding and code-generation tasks where exemplar-driven in-context demonstrations can guide the model (metrics include F1, CodeBLEU, BLEU, Accuracy depending on task).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot in-context learning where exemplars are selected by k-nearest-neighbor using code-aware embeddings; top-k nearest samples (excluding current) are inserted into the prompt as demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against many alternative prompting techniques (e.g., Thread/Tree-of-Thought, USC, Self-Refine, baseline simplistic instruction) and a baseline prompt with only a simple instruction + data sample.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Ranked as the top prompting technique (aggregate across four LLMs) for multiple tasks (notably Clone Detection, Code Translation, Assert Generation); metrics for those tasks are task-specific (e.g., Clone Detection uses F1, Code Translation uses CodeBLEU), but the paper reports rank/aggregate superiority rather than per-technique numeric percent improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Outperformed other prompting techniques and the simplistic baseline across many tasks in aggregate; exact metric-by-technique numeric values are not provided in the paper for each model/technique pair.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors attribute ES-KNN's effectiveness to providing semantically similar, task-aligned in-context examples that hint to the LLM how to process the current instance (structured guidance + relevant examples).</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Each prompting technique used 10 prompt template variations; each variation applied to ~38–40 instances; exemplars selected via embeddings (multilingual/code-optimized embedding model) and cosine similarity; four LLMs evaluated; dataset sampling with 95% confidence, 5% margin; parser required special delimiters or entire response used; temperature fixed per model (o3-mini & DeepSeek: 1.0; Llama & Qwen: 0.7).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which Prompting Technique Should I Use? An Empirical Investigation of Prompting Techniques for Software Engineering Tasks', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9286.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9286.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>USC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Universal Self Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensembling prompting technique that generates multiple outputs and uses a metaprompt or selection strategy to choose the most consistent answer among them to improve reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Aggregate (DeepSeek-V3, Qwen2.5-Coder-32B-Instruct, Llama-3.3-70B-Instruct, OpenAI o3-mini)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code Question Answering (Code QA); Code Generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generative and QA-style code tasks where multiple candidate outputs can be compared or aggregated (metrics include Accuracy for Code QA, CodeBLEU for generation).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Generate multiple candidate responses (via repeated decoding) and apply a meta-selection prompt/consistency heuristic to pick the most consistent/majority answer.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against single-shot or single-response chain-of-thought and other prompting techniques like ES-KNN, Self-Refine, Thread/Tree-of-Thought variants, baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported as the best-performing technique for Code QA and Code Generation for most LLMs in the study (aggregate ranking), with improved reliability vs. single-response prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Better than many individual prompting strategies on Code QA/Code Generation in aggregate; numeric effect sizes per metric are not given for each technique in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Combining multiple answers and selecting the most consistent one mitigates generation errors and increases output reliability, at the expense of increased inference time and computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>USC requires generating multiple candidate outputs per prompt (increasing tokens and time); the authors normalized performance by token count and response time to produce resource-efficiency rankings and found USC often incurred higher time cost due to the multiple generations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which Prompting Technique Should I Use? An Empirical Investigation of Prompting Techniques for Software Engineering Tasks', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9286.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9286.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Thread Of Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A stepwise decomposition prompting technique that instructs the model to work through a problem step by step, pausing to summarize and analyze substeps before proceeding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Aggregate (DeepSeek-V3, Qwen2.5-Coder-32B-Instruct, Llama-3.3-70B-Instruct, OpenAI o3-mini)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Defect Detection (DD)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Code understanding classification-style task where the model must identify defects in code (metric: Accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Decomposition/stepwise reasoning prompt (break code into components, consider each component sequentially, summarize/assess before final decision).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to other reasoning techniques (e.g., Tree-of-Thought, Chain-of-Thought variants, exemplar-based few-shot, baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported as the most effective prompting technique for Defect Detection across the tested LLMs (described as 'outstanding performance' for this task), outperforming alternatives in aggregate ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Better than alternative formats on Defect Detection in aggregate; explicit numeric accuracy differences by technique are not provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Breaking code into components and focusing on each component sequentially helps the LLM concentrate attention and increases the chance of identifying defects (structured decomposition aids defect localization).</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>ToT prompts were validated with 10 variations; applied to sampled dataset (~38–40 examples per template); contrastive explanations indicated Structured Guidance as a key factor.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which Prompting Technique Should I Use? An Empirical Investigation of Prompting Techniques for Software Engineering Tasks', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9286.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9286.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SG-ICL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Generated In-Context Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique where the model automatically generates its own in-context exemplars to simulate few-shot learning, avoiding manual selection of demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Aggregate (DeepSeek-V3, Qwen2.5-Coder-32B-Instruct, Llama-3.3-70B-Instruct, OpenAI o3-mini)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Bug Fixing; Code Summarization (and mixed results across other tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generative tasks where exemplar demonstrations can guide transformations (metrics: CodeBLEU, BLEU).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>The model first generates synthetic in-context examples (demonstrations) and then those examples are used in the prompt for the target instance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against manually-selected exemplars (ES-KNN), single-shot baselines, and other reasoning prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>SG-ICL was among the best techniques for Bug Fixing and Code Summarization in some model/task combinations, but also appears as worst-performing for other tasks in Table III (i.e., mixed/variable performance).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Inconsistent: best in some generation tasks (e.g., Bug Fixing for some models) but listed as worst in several tasks as well; paper reports mixed outcomes without single numeric effect sizes per technique.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Automatically-generated demonstrations can help few-shot-style learning, but quality and relevance of generated exemplars vary by task and model; when exemplars are not well-aligned they may harm performance.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Used the same 10 prompt template variations and instance splits; SG-ICL sometimes appeared among worst-performing techniques in Table III, indicating potential negative effects depending on task/model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which Prompting Technique Should I Use? An Empirical Investigation of Prompting Techniques for Software Engineering Tasks', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9286.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9286.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Role Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that instructs the model to adopt a particular role (e.g., 'You are an expert developer / reviewer') to influence style, perspective, and answer framing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI o3-mini (noted), Aggregate</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple SE tasks (notably many wins on o3-mini across tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Various code understanding and generation tasks; within this study RP was highlighted as a top performer particularly for the o3-mini model.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Prompt includes role instruction (explicit role assignment) and then task instructions and data sample.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against exemplar-based few-shot, chain-of-thought, decomposition, ensembling, self-criticism techniques and a baseline simplistic prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>RP was the majority winner for the OpenAI o3-mini model across many SE tasks (i.e., RP often ranked top on o3-mini), and it consistently ranked highly for token-efficiency across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>On lighter-weight model (o3-mini) RP outperformed other prompting techniques for many tasks, suggesting model-dependent prompt effectiveness; exact numeric metrics per task/technique are not enumerated.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Role Prompting is a cost-efficient and token-efficient alternative; lighter-weight models (o3-mini) may respond better to role-based framing than to exemplar-heavy prompts that increase token usage.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>RP was evaluated with the standardized 10 template variations and the same sampled datasets; token-efficiency and time-efficiency analyses showed RP among the most token-efficient techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which Prompting Technique Should I Use? An Empirical Investigation of Prompting Techniques for Software Engineering Tasks', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9286.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9286.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self Refine</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative prompting technique where the model self-evaluates its output and generates refinements in subsequent steps to improve answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Aggregate (DeepSeek-V3, Qwen2.5-Coder-32B-Instruct, Llama-3.3-70B-Instruct, OpenAI o3-mini)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple SE tasks (appears among worst-performing techniques for several tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Intended for generation/refinement tasks (e.g., code generation, summarization), but in this study SR was sometimes associated with poor performance or high cost.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Iterative loop: generate initial response, self-evaluate, and produce a refined output based on self-feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against single-shot prompts, ensembling (USC), exemplar-based few-shot (ES-KNN), and other reasoning/decomposition techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Appears as a worst-performing technique for several tasks in Table III and in some cases underperforms the simplistic baseline; also often incurred higher time costs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Worse than many other prompting techniques and sometimes worse than the baseline; numeric performance deltas not enumerated per task in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Iterative refinement can introduce overhead and may not improve (or can harm) performance when initial outputs are poor or when tasks benefit more from structured guidance and exemplars; also increases inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>SR incrementally produces responses (increasing response time); the study used normalized efficiency metrics (performance ÷ tokens and ÷ response time) and found SR tended to be less time-efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which Prompting Technique Should I Use? An Empirical Investigation of Prompting Techniques for Software Engineering Tasks', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9286.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9286.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Linguistic Features</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linguistic Characteristics of Prompts (MATTR, Token Count, Flesch-Kincaid, Gunning Fog, Flesch Reading Ease)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Quantitative linguistic metrics computed for prompts (lexical diversity, length, and readability) and correlated with task performance to identify relationships between prompt wording/complexity and LLM success.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Aggregate (analysis across techniques and models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code Understanding (group) and Code Generation (group)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Aggregate groups of SE tasks: code understanding tasks (classification/QA) vs. code generation tasks (translation, bug-fixing, summarization).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Not a prompt technique per se; measured properties of the prompt text used for each prompting technique: MATTR (lexical diversity), token count, Flesch-Kincaid Grade, Gunning Fog Index, Flesch Reading Ease.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Significant correlations reported: Lexical diversity (MATTR) positively correlated with performance aggregate r=0.4440, p<0.001; code generation r=0.5229, p<0.001; code understanding r=0.3468, p=0.0088. Token count negatively correlated with performance in code understanding r=-0.2567, p=0.0022 and code generation r=-0.3200, p=0.0030. Readability metrics show task-dependent correlations (see explanation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Higher lexical diversity in prompts generally helps performance; longer prompts tend to hurt performance; readability effects differ by task group — simpler (more readable) prompts help code understanding tasks whereas more complex prompts (higher FK grade / Fog index) correlate positively with performance on code generation tasks; Flesch Reading Ease correlates positively with understanding tasks and negatively with generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Linguistic metrics were averaged across prompts per prompting technique for each SE task; Spearman correlations were computed between averaged metric values and task performance scores (r and p-values reported).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which Prompting Technique Should I Use? An Empirical Investigation of Prompting Techniques for Software Engineering Tasks', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9286.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9286.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Resource Costs (Token & Time)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Token and Time Resource Impacts of Prompting Techniques</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analysis of token consumption and response time per prompting technique and task, normalized by performance to yield resource-efficiency rankings and reveal trade-offs between performance gains and resource costs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Aggregate (DeepSeek-V3, Qwen2.5-Coder-32B-Instruct, Llama-3.3-70B-Instruct, OpenAI o3-mini)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>All evaluated SE tasks with highlights for Code Generation and Bug Fixing</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Measured token savings and time savings when applying prompting techniques and prompt-compression strategies across tasks; reported aggregate and per-model differences.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Not a single prompt format — study measured tokens in prompts, tokens saved through compression strategies, and response times across techniques and models; efficiency normalized as (performance ÷ tokens) and (performance ÷ response_time).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported token savings: Code Generation aggregate token savings = 10,733.42; Bug Fixing aggregate token savings = 8,306.71. Model-specific examples: Llama Code Generation 8,837.50, Qwen 9,278.82, DeepSeek 8,502.98; o3-mini lagged (e.g., 360.08 tokens in some tasks). Time savings: Code Generation average saved seconds per prompt = 231.05s; Bug Fixing = 211.72s; lower savings for Exception Type Prediction (66.68s), Clone Detection (80.31s), Code QA (85.73s).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Performance-maximizing techniques (e.g., ES-KNN) often incur higher token usage due to appended exemplars; RP is more token-efficient; USC and SR improve quality but increase time due to multiple outputs or iterative refinement. Larger models (Llama, DeepSeek) were more amenable to prompt compression and achieved larger token/time savings than the smaller o3-mini.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Efficiency measured by dividing performance metrics by number of tokens and by response time; prompting techniques were ranked by these normalized efficiencies and aggregated across models; token and time statistics are reported in Tables V and VI (aggregate and per-model).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which Prompting Technique Should I Use? An Empirical Investigation of Prompting Techniques for Software Engineering Tasks', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>What makes good in-context examples for GPT-3? <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Universal self-consistency for large language model generation <em>(Rating: 2)</em></li>
                <li>Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator <em>(Rating: 2)</em></li>
                <li>Contrastive explanations for model interpretability <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Prompting or fine-tuning? a comparative study of large language models for taxonomy construction. <em>(Rating: 1)</em></li>
                <li>An empirical comparison of pre-trained models of source code <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9286",
    "paper_id": "paper-279243781",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "ES-KNN",
            "name_full": "Exemplar Selection KNN",
            "brief_description": "A prompting technique that retrieves semantically similar in-context exemplars using k-nearest-neighbor over embedding vectors and appends them to the prompt to provide demonstrations tailored to the current instance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Aggregate (DeepSeek-V3, Qwen2.5-Coder-32B-Instruct, Llama-3.3-70B-Instruct, OpenAI o3-mini)",
            "model_size": null,
            "task_name": "Clone Detection; Code Translation; Assert Generation; Exception Type Prediction (and others where ES-KNN ranked top)",
            "task_description": "Code understanding and code-generation tasks where exemplar-driven in-context demonstrations can guide the model (metrics include F1, CodeBLEU, BLEU, Accuracy depending on task).",
            "presentation_format": "Few-shot in-context learning where exemplars are selected by k-nearest-neighbor using code-aware embeddings; top-k nearest samples (excluding current) are inserted into the prompt as demonstrations.",
            "comparison_format": "Compared against many alternative prompting techniques (e.g., Thread/Tree-of-Thought, USC, Self-Refine, baseline simplistic instruction) and a baseline prompt with only a simple instruction + data sample.",
            "performance": "Ranked as the top prompting technique (aggregate across four LLMs) for multiple tasks (notably Clone Detection, Code Translation, Assert Generation); metrics for those tasks are task-specific (e.g., Clone Detection uses F1, Code Translation uses CodeBLEU), but the paper reports rank/aggregate superiority rather than per-technique numeric percent improvements.",
            "performance_comparison": "Outperformed other prompting techniques and the simplistic baseline across many tasks in aggregate; exact metric-by-technique numeric values are not provided in the paper for each model/technique pair.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Authors attribute ES-KNN's effectiveness to providing semantically similar, task-aligned in-context examples that hint to the LLM how to process the current instance (structured guidance + relevant examples).",
            "null_or_negative_result": false,
            "experimental_details": "Each prompting technique used 10 prompt template variations; each variation applied to ~38–40 instances; exemplars selected via embeddings (multilingual/code-optimized embedding model) and cosine similarity; four LLMs evaluated; dataset sampling with 95% confidence, 5% margin; parser required special delimiters or entire response used; temperature fixed per model (o3-mini & DeepSeek: 1.0; Llama & Qwen: 0.7).",
            "uuid": "e9286.0",
            "source_info": {
                "paper_title": "Which Prompting Technique Should I Use? An Empirical Investigation of Prompting Techniques for Software Engineering Tasks",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "USC",
            "name_full": "Universal Self Consistency",
            "brief_description": "An ensembling prompting technique that generates multiple outputs and uses a metaprompt or selection strategy to choose the most consistent answer among them to improve reliability.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Aggregate (DeepSeek-V3, Qwen2.5-Coder-32B-Instruct, Llama-3.3-70B-Instruct, OpenAI o3-mini)",
            "model_size": null,
            "task_name": "Code Question Answering (Code QA); Code Generation",
            "task_description": "Generative and QA-style code tasks where multiple candidate outputs can be compared or aggregated (metrics include Accuracy for Code QA, CodeBLEU for generation).",
            "presentation_format": "Generate multiple candidate responses (via repeated decoding) and apply a meta-selection prompt/consistency heuristic to pick the most consistent/majority answer.",
            "comparison_format": "Compared against single-shot or single-response chain-of-thought and other prompting techniques like ES-KNN, Self-Refine, Thread/Tree-of-Thought variants, baseline.",
            "performance": "Reported as the best-performing technique for Code QA and Code Generation for most LLMs in the study (aggregate ranking), with improved reliability vs. single-response prompts.",
            "performance_comparison": "Better than many individual prompting strategies on Code QA/Code Generation in aggregate; numeric effect sizes per metric are not given for each technique in the main text.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Combining multiple answers and selecting the most consistent one mitigates generation errors and increases output reliability, at the expense of increased inference time and computational cost.",
            "null_or_negative_result": false,
            "experimental_details": "USC requires generating multiple candidate outputs per prompt (increasing tokens and time); the authors normalized performance by token count and response time to produce resource-efficiency rankings and found USC often incurred higher time cost due to the multiple generations.",
            "uuid": "e9286.1",
            "source_info": {
                "paper_title": "Which Prompting Technique Should I Use? An Empirical Investigation of Prompting Techniques for Software Engineering Tasks",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "ToT",
            "name_full": "Thread Of Thought",
            "brief_description": "A stepwise decomposition prompting technique that instructs the model to work through a problem step by step, pausing to summarize and analyze substeps before proceeding.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Aggregate (DeepSeek-V3, Qwen2.5-Coder-32B-Instruct, Llama-3.3-70B-Instruct, OpenAI o3-mini)",
            "model_size": null,
            "task_name": "Defect Detection (DD)",
            "task_description": "Code understanding classification-style task where the model must identify defects in code (metric: Accuracy).",
            "presentation_format": "Decomposition/stepwise reasoning prompt (break code into components, consider each component sequentially, summarize/assess before final decision).",
            "comparison_format": "Compared to other reasoning techniques (e.g., Tree-of-Thought, Chain-of-Thought variants, exemplar-based few-shot, baseline).",
            "performance": "Reported as the most effective prompting technique for Defect Detection across the tested LLMs (described as 'outstanding performance' for this task), outperforming alternatives in aggregate ranking.",
            "performance_comparison": "Better than alternative formats on Defect Detection in aggregate; explicit numeric accuracy differences by technique are not provided in the paper.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Breaking code into components and focusing on each component sequentially helps the LLM concentrate attention and increases the chance of identifying defects (structured decomposition aids defect localization).",
            "null_or_negative_result": false,
            "experimental_details": "ToT prompts were validated with 10 variations; applied to sampled dataset (~38–40 examples per template); contrastive explanations indicated Structured Guidance as a key factor.",
            "uuid": "e9286.2",
            "source_info": {
                "paper_title": "Which Prompting Technique Should I Use? An Empirical Investigation of Prompting Techniques for Software Engineering Tasks",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "SG-ICL",
            "name_full": "Self-Generated In-Context Learning",
            "brief_description": "A prompting technique where the model automatically generates its own in-context exemplars to simulate few-shot learning, avoiding manual selection of demonstrations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Aggregate (DeepSeek-V3, Qwen2.5-Coder-32B-Instruct, Llama-3.3-70B-Instruct, OpenAI o3-mini)",
            "model_size": null,
            "task_name": "Bug Fixing; Code Summarization (and mixed results across other tasks)",
            "task_description": "Generative tasks where exemplar demonstrations can guide transformations (metrics: CodeBLEU, BLEU).",
            "presentation_format": "The model first generates synthetic in-context examples (demonstrations) and then those examples are used in the prompt for the target instance.",
            "comparison_format": "Compared against manually-selected exemplars (ES-KNN), single-shot baselines, and other reasoning prompts.",
            "performance": "SG-ICL was among the best techniques for Bug Fixing and Code Summarization in some model/task combinations, but also appears as worst-performing for other tasks in Table III (i.e., mixed/variable performance).",
            "performance_comparison": "Inconsistent: best in some generation tasks (e.g., Bug Fixing for some models) but listed as worst in several tasks as well; paper reports mixed outcomes without single numeric effect sizes per technique.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Automatically-generated demonstrations can help few-shot-style learning, but quality and relevance of generated exemplars vary by task and model; when exemplars are not well-aligned they may harm performance.",
            "null_or_negative_result": true,
            "experimental_details": "Used the same 10 prompt template variations and instance splits; SG-ICL sometimes appeared among worst-performing techniques in Table III, indicating potential negative effects depending on task/model.",
            "uuid": "e9286.3",
            "source_info": {
                "paper_title": "Which Prompting Technique Should I Use? An Empirical Investigation of Prompting Techniques for Software Engineering Tasks",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "RP",
            "name_full": "Role Prompting",
            "brief_description": "A prompting technique that instructs the model to adopt a particular role (e.g., 'You are an expert developer / reviewer') to influence style, perspective, and answer framing.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "OpenAI o3-mini (noted), Aggregate",
            "model_size": null,
            "task_name": "Multiple SE tasks (notably many wins on o3-mini across tasks)",
            "task_description": "Various code understanding and generation tasks; within this study RP was highlighted as a top performer particularly for the o3-mini model.",
            "presentation_format": "Prompt includes role instruction (explicit role assignment) and then task instructions and data sample.",
            "comparison_format": "Compared against exemplar-based few-shot, chain-of-thought, decomposition, ensembling, self-criticism techniques and a baseline simplistic prompt.",
            "performance": "RP was the majority winner for the OpenAI o3-mini model across many SE tasks (i.e., RP often ranked top on o3-mini), and it consistently ranked highly for token-efficiency across tasks.",
            "performance_comparison": "On lighter-weight model (o3-mini) RP outperformed other prompting techniques for many tasks, suggesting model-dependent prompt effectiveness; exact numeric metrics per task/technique are not enumerated.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Role Prompting is a cost-efficient and token-efficient alternative; lighter-weight models (o3-mini) may respond better to role-based framing than to exemplar-heavy prompts that increase token usage.",
            "null_or_negative_result": false,
            "experimental_details": "RP was evaluated with the standardized 10 template variations and the same sampled datasets; token-efficiency and time-efficiency analyses showed RP among the most token-efficient techniques.",
            "uuid": "e9286.4",
            "source_info": {
                "paper_title": "Which Prompting Technique Should I Use? An Empirical Investigation of Prompting Techniques for Software Engineering Tasks",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "SR",
            "name_full": "Self Refine",
            "brief_description": "An iterative prompting technique where the model self-evaluates its output and generates refinements in subsequent steps to improve answers.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Aggregate (DeepSeek-V3, Qwen2.5-Coder-32B-Instruct, Llama-3.3-70B-Instruct, OpenAI o3-mini)",
            "model_size": null,
            "task_name": "Multiple SE tasks (appears among worst-performing techniques for several tasks)",
            "task_description": "Intended for generation/refinement tasks (e.g., code generation, summarization), but in this study SR was sometimes associated with poor performance or high cost.",
            "presentation_format": "Iterative loop: generate initial response, self-evaluate, and produce a refined output based on self-feedback.",
            "comparison_format": "Compared against single-shot prompts, ensembling (USC), exemplar-based few-shot (ES-KNN), and other reasoning/decomposition techniques.",
            "performance": "Appears as a worst-performing technique for several tasks in Table III and in some cases underperforms the simplistic baseline; also often incurred higher time costs.",
            "performance_comparison": "Worse than many other prompting techniques and sometimes worse than the baseline; numeric performance deltas not enumerated per task in the paper.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Iterative refinement can introduce overhead and may not improve (or can harm) performance when initial outputs are poor or when tasks benefit more from structured guidance and exemplars; also increases inference time.",
            "null_or_negative_result": true,
            "experimental_details": "SR incrementally produces responses (increasing response time); the study used normalized efficiency metrics (performance ÷ tokens and ÷ response time) and found SR tended to be less time-efficient.",
            "uuid": "e9286.5",
            "source_info": {
                "paper_title": "Which Prompting Technique Should I Use? An Empirical Investigation of Prompting Techniques for Software Engineering Tasks",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Prompt Linguistic Features",
            "name_full": "Linguistic Characteristics of Prompts (MATTR, Token Count, Flesch-Kincaid, Gunning Fog, Flesch Reading Ease)",
            "brief_description": "Quantitative linguistic metrics computed for prompts (lexical diversity, length, and readability) and correlated with task performance to identify relationships between prompt wording/complexity and LLM success.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Aggregate (analysis across techniques and models)",
            "model_size": null,
            "task_name": "Code Understanding (group) and Code Generation (group)",
            "task_description": "Aggregate groups of SE tasks: code understanding tasks (classification/QA) vs. code generation tasks (translation, bug-fixing, summarization).",
            "presentation_format": "Not a prompt technique per se; measured properties of the prompt text used for each prompting technique: MATTR (lexical diversity), token count, Flesch-Kincaid Grade, Gunning Fog Index, Flesch Reading Ease.",
            "comparison_format": null,
            "performance": "Significant correlations reported: Lexical diversity (MATTR) positively correlated with performance aggregate r=0.4440, p&lt;0.001; code generation r=0.5229, p&lt;0.001; code understanding r=0.3468, p=0.0088. Token count negatively correlated with performance in code understanding r=-0.2567, p=0.0022 and code generation r=-0.3200, p=0.0030. Readability metrics show task-dependent correlations (see explanation).",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Higher lexical diversity in prompts generally helps performance; longer prompts tend to hurt performance; readability effects differ by task group — simpler (more readable) prompts help code understanding tasks whereas more complex prompts (higher FK grade / Fog index) correlate positively with performance on code generation tasks; Flesch Reading Ease correlates positively with understanding tasks and negatively with generation tasks.",
            "null_or_negative_result": false,
            "experimental_details": "Linguistic metrics were averaged across prompts per prompting technique for each SE task; Spearman correlations were computed between averaged metric values and task performance scores (r and p-values reported).",
            "uuid": "e9286.6",
            "source_info": {
                "paper_title": "Which Prompting Technique Should I Use? An Empirical Investigation of Prompting Techniques for Software Engineering Tasks",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Resource Costs (Token & Time)",
            "name_full": "Token and Time Resource Impacts of Prompting Techniques",
            "brief_description": "Analysis of token consumption and response time per prompting technique and task, normalized by performance to yield resource-efficiency rankings and reveal trade-offs between performance gains and resource costs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Aggregate (DeepSeek-V3, Qwen2.5-Coder-32B-Instruct, Llama-3.3-70B-Instruct, OpenAI o3-mini)",
            "model_size": null,
            "task_name": "All evaluated SE tasks with highlights for Code Generation and Bug Fixing",
            "task_description": "Measured token savings and time savings when applying prompting techniques and prompt-compression strategies across tasks; reported aggregate and per-model differences.",
            "presentation_format": "Not a single prompt format — study measured tokens in prompts, tokens saved through compression strategies, and response times across techniques and models; efficiency normalized as (performance ÷ tokens) and (performance ÷ response_time).",
            "comparison_format": null,
            "performance": "Reported token savings: Code Generation aggregate token savings = 10,733.42; Bug Fixing aggregate token savings = 8,306.71. Model-specific examples: Llama Code Generation 8,837.50, Qwen 9,278.82, DeepSeek 8,502.98; o3-mini lagged (e.g., 360.08 tokens in some tasks). Time savings: Code Generation average saved seconds per prompt = 231.05s; Bug Fixing = 211.72s; lower savings for Exception Type Prediction (66.68s), Clone Detection (80.31s), Code QA (85.73s).",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Performance-maximizing techniques (e.g., ES-KNN) often incur higher token usage due to appended exemplars; RP is more token-efficient; USC and SR improve quality but increase time due to multiple outputs or iterative refinement. Larger models (Llama, DeepSeek) were more amenable to prompt compression and achieved larger token/time savings than the smaller o3-mini.",
            "null_or_negative_result": false,
            "experimental_details": "Efficiency measured by dividing performance metrics by number of tokens and by response time; prompting techniques were ranked by these normalized efficiencies and aggregated across models; token and time statistics are reported in Tables V and VI (aggregate and per-model).",
            "uuid": "e9286.7",
            "source_info": {
                "paper_title": "Which Prompting Technique Should I Use? An Empirical Investigation of Prompting Techniques for Software Engineering Tasks",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "What makes good in-context examples for GPT-3?",
            "rating": 2,
            "sanitized_title": "what_makes_good_incontext_examples_for_gpt3"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Universal self-consistency for large language model generation",
            "rating": 2,
            "sanitized_title": "universal_selfconsistency_for_large_language_model_generation"
        },
        {
            "paper_title": "Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator",
            "rating": 2,
            "sanitized_title": "selfgenerated_incontext_learning_leveraging_autoregressive_language_models_as_a_demonstration_generator"
        },
        {
            "paper_title": "Contrastive explanations for model interpretability",
            "rating": 2,
            "sanitized_title": "contrastive_explanations_for_model_interpretability"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Prompting or fine-tuning? a comparative study of large language models for taxonomy construction.",
            "rating": 1,
            "sanitized_title": "prompting_or_finetuning_a_comparative_study_of_large_language_models_for_taxonomy_construction"
        },
        {
            "paper_title": "An empirical comparison of pre-trained models of source code",
            "rating": 1,
            "sanitized_title": "an_empirical_comparison_of_pretrained_models_of_source_code"
        }
    ],
    "cost": 0.01819425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Which Prompting Technique Should I Use? An Empirical Investigation of Prompting Techniques for Software Engineering Tasks
5 Jun 2025</p>
<p>Enio Garcia 
Santana Junior 
Federal University of Bahia (UFBA)
Brazil</p>
<p>Gabriel Benjamin 
Federal University of Bahia (UFBA)
Brazil</p>
<p>Melissa Araujo 
Federal University of Bahia (UFBA)
Brazil</p>
<p>Harrison Santos 
Federal University of Bahia (UFBA)
Brazil</p>
<p>David Freitas 
Federal University of Bahia (UFBA)
Brazil</p>
<p>Eduardo Almeida 
Federal University of Bahia (UFBA)
Brazil</p>
<p>Paulo Anselmo Da Mota 
Silveira Neto 
Federal Rural University of Pernambuco (UFRPE)
Brazil</p>
<p>Jiawei Li 
University of California
IrvineUCI)USA</p>
<p>Jina Chun 
University of California
IrvineUCI)USA</p>
<p>Iftekhar Ahmed 
University of California
IrvineUCI)USA</p>
<p>Which Prompting Technique Should I Use? An Empirical Investigation of Prompting Techniques for Software Engineering Tasks
5 Jun 2025B52C92952D09FA88347A269758B5BB88arXiv:2506.05614v1[cs.SE]Prompt EngineeringLarge Language ModelsSoftware Engineering Tasks
A growing variety of prompt engineering techniques has been proposed for Large Language Models (LLMs), yet systematic evaluation of each technique on individual software engineering (SE) tasks remains underexplored.In this study, we present a systematic evaluation of 14 established prompt techniques across 10 SE tasks using four LLM models.As identified in the prior literature, the selected prompting techniques span six core dimensions (Zero-Shot, Few-Shot, Thought Generation, Ensembling, Self-Criticism, and Decomposition).They are evaluated on tasks such as code generation, bug fixing, and code-oriented question answering, to name a few.Our results show which prompting techniques are most effective for SE tasks requiring complex logic and intensive reasoning versus those that rely more on contextual understanding and example-driven scenarios.We also analyze correlations between the linguistic characteristics of prompts and the factors that contribute to the effectiveness of prompting techniques in enhancing performance on SE tasks.Additionally, we report the time and token consumption for each prompting technique when applied to a specific task and model, offering guidance for practitioners in selecting the optimal prompting technique for their use cases.</p>
<p>I. INTRODUCTION</p>
<p>Emerging Large Language Models (LLMs) have rapidly advanced beyond the capabilities of earlier smaller models, thanks to breakthroughs in deep neural architectures, access to large-scale training corpora in natural language and source code, and substantial computational power [1].Simply through prompting without any model training, LLMs have already shown state-of-the-art performance in automating various Software Engineering (SE) tasks, such as code translation [2], [3], commit message generation [4], [5], and program repair [6], [7].Despite these promising developments, it has become increasingly evident that even small changes in how a prompt is formulated can drastically alter an LLM's output [8].In automating SE tasks by prompting LLMs, the adopted prompts have the potential to impact various quality aspects ranging from code correctness and readability to the efficiency of bugfixing suggestions [9]- [11].</p>
<p>To guide LLMs toward more reliable, accurate, and contextaware responses, an array of prompt engineering [9] tech-niques have been proposed by researchers in recent years.However, it remains unknown which prompt engineering technique would benefit a specific SE task the most, leaving practitioners uncertain about how to compose the prompts to suit their needs.In this work, we address this gap by systematically evaluating the effectiveness of fourteen widely used prompt engineering techniques [9] within ten SE tasks [12], spanning an extensive range of code understanding and generation tasks.This prompted us to ask our first research question:</p>
<p>RQ1: How do different prompting techniques impact the performance of SE tasks?</p>
<p>We analyzed the linguistic features of these prompts to gain a deeper understanding of the characteristics that distinguish prompting techniques that lead to performance improvements from those that do not in different SE tasks.This includes examining aspects such as Lexical Diversity, Token Count, Flesch-Kincaid Grade Level, Gunning Fog Index, and Flesch Reading Ease.These linguistic features help identify how prompt clarity and complexity relate to LLM performance in SE tasks.Lexical Diversity reflects vocabulary richness, Token Count captures prompt length, and Flesch-Kincaid Grade Level, Gunning Fog Index, and Flesch Reading Ease assess readability and complexity.Together, they reveal whether more concise, readable, or varied prompts are associated with improved task outcomes.Identifying significant linguistic similarities among high-performing prompting strategies can reveal useful insights for systematically designing more effective prompts for SE automation.In addition to linguistic features, we investigated other potential factors contributing to the success of prompting techniques by leveraging contrastive explanation [13].Specifically, we queried the LLMs to compare successful and less successful prompts, aiming to understand what the model perceives as key differentiators in performance.Thus, we formulated the following research questions:</p>
<p>RQ2: What linguistic features of prompting techniques are associated with improved performance on SE tasks?RQ3: What factors, according to LLMs, contribute to the effectiveness of prompting techniques in SE tasks?</p>
<p>Since large language models (LLMs) require substantial computational resources and can incur high financial costs, particularly when accessed via commercial APIs, software practitioners must carefully balance the trade-off between performance improvements and resource consumption [14].</p>
<p>Recent studies indicate that seemingly minor variations in prompts can result in disproportionately large differences in resource usage, including increased token counts and longer inference times.Therefore, understanding how different prompting techniques impact both performance and resource costs is essential for making informed decisions in practical SE scenarios.This motivation leads to our next research question:</p>
<p>RQ4: How are resource costs associated with the performance of prompting techniques in SE tasks?</p>
<p>Based on our findings, we constructed composite prompts by combining the prompting techniques that consistently yield performance improvements in SE tasks.We aimed to investigate whether integrating all positively impactful factors could lead to further performance gains.Ultimately, we aim to enable SE practitioners and researchers to leverage LLMs more effectively and accurately, advancing more robust and autonomous SE practices.This motivated our final research question:</p>
<p>Overall, this paper makes the following contributions:</p>
<p>• A comprehensive benchmark of 14 prompt-engineering techniques evaluated on 10 SE tasks and 3 LLM families, totaling more than 2k prompts.• A systematic investigation that discloses the factors associated with a prompting technique's success across tasks and models.II.RELATED WORK LLMs in Software Engineering.Recent LLMs have billions of parameters and are trained with large-scale datasets of natural language and source code, showing state-of-theart understanding and generation capabilities.Consequently, researchers have increasingly adopted them to conduct various SE tasks ranging from code translation [2], [3], commit message generation [4], [5], code review [15], [16], and program repair [6], [7] to name a few.In addition to research adoption, practitioners also integrate LLMs into software development activities.Recent years have witnessed numerous LLM-based AI code assistants, such as Github Copilot [17], JetBrains AI assistant [18], and Visual Studio IntelliCode [19].Sergeyuk et al. [20] have found that developers tend to delegate certain SE tasks to those LLM-based AI code assistants, including test case and documentation generation.</p>
<p>Techniques to Improve LLM Performance.Various techniques exist to improve the performance of LLMs when automating a task.Historically, fine-tuning with domain-specific datasets has been the go-to approach to apply the LLMs to a downstream task [21].While fine-tuning can improve a generic LLM's performance on the task, it requires heavy computational resources and curated datasets, making it an expensive and time-consuming approach.However, recent works have [11], [22] suggested that simply prompting the LLMs with well-designed prompt engineering techniques can be equally or more effective in certain conditions.For example, Shang et al. [23] compared fine-tuning and prompt-based methods for generating unit tests, showing that carefully structured prompts sometimes achieve comparable results.Similarly, Chen et al. [24] demonstrated that prompting LLMs outperformed finetuning in taxonomy-building tasks, and the performance gap widened when the LLMs were fine-tuned with limited data.Moreover, [9] offers a survey of 58 techniques, illustrating how prompt formulation can dramatically affect outcomes.This highlights the growing recognition that the way prompts are structured can significantly influence the quality of the generated output, making prompt engineering a crucial skill for developers and researchers.Despite increased interest in prompt-based methods with LLMs, best practices remain inconsistent, especially in automating software development and maintenance activities.In this study, we aim to fill this gap by providing insights into how different SE tasks would benefit from different prompting techniques so that researchers and practitioners can better utilize the power of the LLMs to automate these SE tasks.</p>
<p>Comparisons of LLM Approaches.Prior works have investigated the effectiveness of different pre-trained LLMs in performing SE tasks.For instance, Niu et al. [12] evaluated 19 pre-trained models on 13 SE tasks, highlighting the correlation between LLMs of different architectures and their performance on SE tasks.Similarly, a survey on code generation [25] analyzed multiple LLM-based solutions and ranked their performance across standard benchmarks.Alizadeh et al. [26] extended this line of work by analyzing the energy-accuracy trade-off of 18 LLM families, demonstrating that larger models and higher energy budgets do not always yield superior results and that no single model dominates across diverse SE tasks.While these comparisons have shown the importance of selecting the right model, they rarely revealed how the wording and structure of the prompt itself can influence the quality of the generated content, leading to an oversight, as even a well-trained model can underperform if not guided by well-constructed instructions formatted by an appropriate prompting technique [27].It remains unclear how to adapt each task-specific scenario via prompting to maximize SE task performance, an aspect we address in this work.</p>
<p>In addition, we note that while both fine-tuning and prompt engineering have been studied in the domain of software engineering, no comprehensive evaluation has yet examined how different prompt engineering techniques perform across SE tasks.This comparative analysis can highlight the strengths and weaknesses of various prompting techniques, offering practical guidelines for developers and researchers alike.To fill this gap, our research systematically compares recognized prompting techniques drawn from the literature on LLMs and SE across diverse scenarios such as bug fixing, test generation, and code summarization.</p>
<p>Explaining the Prompt Technique.Researchers have explored various metrics to explain model predictions and gain insights into their behavior.One such metric is contrastive explanation, which seeks to clarify why a model predicts a specific outcome for a given input instead of an alternative outcome.In the NLP domain, prior studies have employed contrastive explanation to better understand model decisions in tasks such as natural language inference [13], question answering [28], and grammatical acceptability [29].These studies demonstrate that contrastive explanations can be more effective in revealing model reasoning than simply prompting the model for a rationale.In this work, we adopt the practices proposed in [13] to examine, given a prompt, task instance, and model-generated output, why one prompt technique leads to better performance than another.Our work is the first to investigate why certain prompts perform better in various SE tasks leveraging contrastive explanations.</p>
<p>III. METHODOLOGY</p>
<p>In this section, we introduce the methodology of our experiments for answering our research questions.</p>
<p>A. Software Engineering Tasks</p>
<p>To make our findings more generalizable and comprehensive, we targeted a broad range of Software Engineering (SE) tasks curated by Niu et al (Table I).[12].These tasks have been widely explored and automated by researchers using LLMs, demonstrating various characteristics in terms of inputoutput modalities.They have also been grouped into code understanding and code generation tasks.Due to the limited financial budget, we could not run our experiments on all the datasets associated with these SE tasks.Thus, we conducted random sampling on each dataset with a 95% confidence level and a 5% margin of error.</p>
<p>The original study from which we derived our task set included a total of 13 tasks.For the purposes of our investigation, we refined this list to 10 tasks.Specifically, we excluded Codeto-Code Retrieval and Code Search as these tasks are primarily designed for code embedding models [30], [31] rather than generative models where the design of the prompting techniques can impact the performance.Additionally, we excluded the Code Completion task due to challenges in reproducing the required dataset format using the implementation provided in the replication package [12].In total, we obtained 10 sampled datasets.Table I lists the sample size for each dataset for an SE task.</p>
<p>B. Prompting Techniques</p>
<p>A prompting technique is a blueprint that describes how to structure a prompt that serves as the input query to the LLMs.It may incorporate conditional logic, parallelism, or other architectural considerations to obtain responses from the LLMs that better align with the user intent and task objectives [9].</p>
<p>Schulhoff et al. [9] curated a catalog of 46 prompting techniques in literature.To ensure that prompting techniques can be readily applied to the selected SE tasks, we devised a set of exclusion criteria to filter out those that were not relevant or applicable.This filtering process was conducted independently by two researchers, master's students with more than two years of experience in prompt engineering.In cases where disagreement arose, a third researcher with extensive experience in LLMs and software development was consulted to resolve the divergence and reach a consensus.The whole process was conducted through a negotiated agreement [42].These filtering criteria are: (1) Only one prompting technique was retained if multiple prompting techniques function similarly (i.e,.K-Nearest Neighbor [43] and Vote-K [44] both are few-shot techniques that differ only on the method used to choose the examples); (2) the prompting technique cannot be an ensemble of multiple other techniques (i.e,.DENSE [45]);</p>
<p>(3) Since our goal was to examine the effectiveness of the prompt content and structure, the prompting technique cannot rely on external task-specific tools (i.e., ReAct [46]).</p>
<p>After filtering, 14 out of the 46 prompting techniques were adopted in our analysis.The list of all 46 prompting techniques and the rationale for whether they were included or not is available in our replication package 1 .We describe each of the 14 techniques as follows:</p>
<p>1) Exemplar Selection KNN [43] (ES-KNN): Selects exemplars using a k-nearest neighbor approach to enrich the prompt context.In detail, the data samples from the full original dataset (excluding the current data sample to be queried) are transformed into vector embeddings using a code-optimized embedding model [47].We selected the most widely adopted and topperforming embedding model available at the time of our experiment.Depending on the SE task, since the data samples may consist of source code, natural language, or both, we selected a multilingual embedding model that was trained with both natural language and multiple programming languages.We then computed cosine similarity between the current sample and encoded data samples to retrieve the top-k nearest neighbor samples, which were selected as in-context exemplars to be added to the prompt.We selected the most freqency used and best performing 2) Few Shot Contrastive CoT [48] (CCoT): Uses both correct and incorrect chain-of-thought examples to refine reasoning steps.3) Tree Of Thought [49] (TroT): Structures multiple branching reasoning paths to explore diverse solution strategies for complex design problems.4) Self Ask [50] (SA): Makes the model generate its own follow-up questions before answering, which helps it break down and solve complex problems step by step.5) Universal Self Consistency [51] (USC): Combines multiple answers from the model and uses a metaprompt to pick the most consistent one, improving output reliability.6) Self Refine [52] (SR): Iteratively improves initial responses by self-evaluating and updating code explanations or solutions.7) Self-Generated In-Context Learning [53] (SG-ICL):</p>
<p>Automatically generates in-context exemplars to simulate few-shot learning, streamlining prompt formulation for coding tasks.8) Thread Of Thought [54] (ToT): This technique guides the LLMs to work through a problem step by step, focusing on breaking down a large or complex task into smaller, manageable parts.For example, instead of solving everything at once, the model is told to pause, summarize, and analyze each step before moving to the next step, making the reasoning process more organized and clear.9) Step Back Prompting [55] (SBP): With step back prompting, the model is first asked to examine the problem as a whole and think about the key ideas or main facts, before drafting the solution.This helps the LLMs to plan ahead and avoid jumping into details.10) Emotional Prompting [56] (EP): Incorporates affective language to shape engaging and empathetic responses, useful in writing user-friendly documentation or error messages.11) Style Prompting [57] (SP): Directs the model to adopt a specific tone or format, ensuring that generated code comments and documentation align with a desired style.12) Rephrase and Respond [58]</p>
<p>C. Prompt Validation</p>
<p>We conducted a structured prompt validation process [61] to reduce the potential impact of prompt wording on the task performance.For each prompting technique, we constructed ten prompt variation templates with different synonyms and phrasings, using OpenAI's ChatGPT [62] as a tool to ensure no loss in text quality [63], [64] (e.g., eliminating grammatical errors, maintaining sentence coherence, and preserving the original intent of the technique), as well as to maximize variability across the generated prompts.Figure 1 contains two examples of prompt template variations.Then six researchers, who were also the authors of this study, participated in the manual process of reviewing these templates to ensure that the semantic meaning of the prompts remained consistent.To detail, a pair of researchers was assigned 4 to 5 prompting techniques.Each researcher independently reviewed the variation templates for their assigned technique to determine whether the prompts conformed to the descriptions provided by scientific literature.A variation template was accepted only if both researchers in the pair agreed.In cases of disagreement, the template was discarded, and new variation was generated and reviewed until ten acceptable template variations were finalized.</p>
<p>The average Cohen's kappa for inter-rater reliability across all prompting techniques was kappa=0.45.</p>
<p>D. Language Model Selection</p>
<p>To ensure the generalizability and comprehensiveness of our findings, we employed a diverse set of LLMs.These models have been developed by different organizations, focusing on various architectures and training objectives.Since the selected SE tasks involve not only natural language but also source code, we chose the LLMs that show stateof-the-art performance on widely adopted code generation benchmarks, such as EvalPlus [65].Consequently, we selected DeepSeek-V3 [66], Qwen2.5-Coder-32B-Instruct[67], Llama-3.3-70B-Instruct[68], and OpenAI o3-mini [69].</p>
<p>E. Result Collection</p>
<p>The results were obtained by applying each prompting technique to all ten selected SE tasks (Table I).For each prompting technique being applied to an SE task, we divided the dataset evenly and ensured all prompt template variations were applied as close to the same number of data instances as possible (between 38 and 40 instances per variation).These prompt template variations (Section III-C) were then sent to the LLMs for solving the SE task.Specifically we used together.aiAPI [70] for the open source language models: DeepSeek-V3, Qwen2.5-Coder-32B-Instruct and Llama-3.3-70B-Instruct, and for the o3-mini we used the OpenAi API.The raw model responses were subsequently processed using task-specific extraction scripts.Our parser aimed to extract only the relevant answer field from the model's output according to the expected output format for each SE task.To ensure the parser did not artificially inflate performance, we only extracted answers delimited by special markers that were specified in the prompt instructions.If the model correctly used the specified delimiters, our parser would return only the content within those delimiters.If the model response failed to include the required delimiters, we included the entire response in our analysis.We included the entire model response as the answer for evaluation.This process ensured that the extraction step reflected the actual performance of the LLMs in adhering to prompt specifications and instructions.After this cleaning step, the extracted responses were used to compute each SE task's evaluation metrics (Table I).</p>
<p>In addition, we constructed a baseline to demonstrate the effectiveness of the prompting techniques under study.Specifically, the baseline only prompts the LLMs with a simplistic instruction to complete the SE task along with the data sample.For each of the ten SE tasks, we averaged performance metrics across the four LLMs to have an overview of the effectiveness of the prompting techniques.Then, we ranked the prompting techniques to identify the most effective one for the SE tasks.</p>
<p>The ranking was determined by aggregating the performance metric for each technique across four LLMs.The metrics for the SE tasks are included in Table I.We describe them in the following:</p>
<p>F. Linguistic Metrics</p>
<p>To reveal characteristics that have the potential to impact the performance of the SE tasks when prompting LLMs, we selected the following linguistic metrics that are relevant to prompt engineering:</p>
<p>• Lexical Diversity (MATTR) assesses the variety of vocabulary used in the prompt by calculating the Moving-Average Type-Token Ratio (MATTR) [71].• Flesch-Kincaid Grade Level evaluates the readability of a prompt by estimating the U.S. school grade level necessary for its comprehension [72].• Gunning Fog Index calculates the years of formal education a reader needs to understand the prompt [73].</p>
<p>• Token Count calculates the number of tokens in the prompt.</p>
<p>• Flesch Reading Ease rates prompt readability on a scale from 0 (very difficult) to 100 (very easy), with higher scores indicating easier-to-read prompts.This metric is widely used in software documentation and requirements engineering studies [74], [75].We calculated these metrics for all the prompts in this study and averaged the metric values of all the prompts using a prompting technique for each of the selected SE task.The goal was to collect the linguistic characteristics of different prompting techniques for the SE tasks.To investigate their association with the prompting techniques' effectiveness in the SE tasks, we conducted Spearman correlation [76] between the averaged values of these linguistic metrics and the performance scores of the SE tasks (Table I).</p>
<p>G. Contrastive Explanation</p>
<p>To further investigate why a specific prompting technique shows better performance in an SE task than other techniques, we used contrastive explanation [13] that attempts to uncover the factors associated with the model's decisions.Contrastive explanations provide interpretable results similar to what a human would explain a decision, which inherently answers the question "why P, rather than Q?" [77].Since the decisions the LLMs make are often complex, the complete explanations can be complicated and uninterpretable [78].Contrastive explanations omit the causal attributes or factors common to both P and Q, making the explanations easy to understand [13].</p>
<p>To utilize contrastive explanations to explain the superiority of a prompting technique in solving the SE tasks, we prompted the LLMs using in-context learning [79].To be more specific, for each SE task, we included the definitions of the prompting technique that performed the best and the worst (Section III-E) and prompted the LLM to generate contrastive explanations.In total, we prompted the LLM (i.e., the same LLM that had generated the responses for the SE task) to obtain comprehensive explanations regarding the potential factors that were responsible for a prompting technique's superior performance compared to worst prompting technique.The process was repeated for all four LLM models used in our experiment.</p>
<p>To ensure the quality and interpretability of the explanations, we also attached four demonstration examples (following the best practice of in-context learning in software engineering research [79]), each consisting of the queries constructed by the two prompting techniques and their responses.To make the responses of the two prompting techniques more distinguishable for the LLM to generate explanations, we only included examples whose responses by the superior prompting technique were significantly better than those of the worst prompting technique (For understanding tasks, the response of the superior prompting technique is a correct prediction while that of the compared technique is not.For generation tasks, we ensure the metric scores of the two techniques have the most difference).Due to the space limit, we include the complete prompt for all the SE tasks in our replication package.To ensure the quality of the prompt to extract explanations, we followed the best practices [80] and examined the responses after executing the composed prompts ten times.</p>
<p>We collected all the contrastive explanation responses provided by the four LLM models.Then, for each SE task, the responses across the models were combined for manual evaluation.To detail, we used a card sorting method [81] to conduct a qualitative analysis of these responses.Two researchers participated in this analysis.Each of them evaluated the explanations and categorized them into codes.They then met to discuss these codes, which were further grouped into high-level categories.Finally, the researchers refined the categories and organized them into meaningful themes that can sufficiently explain why a certain technique outperformed the others.</p>
<p>IV. RESULTS</p>
<p>A. (RQ1) How do different prompting techniques impact the performance of SE tasks?</p>
<p>Table II shows the best-performing prompting technique for each SE task using different LLMs.To have an overview of the effectiveness of the prompting techniques for each task, we also provided the prompting techniques that showed the best performance, averaged across all the LLMs in the Aggregate column.We note that while no single prompting technique emerged as universally optimal across all tasks, there are certain prompting techniques that tend to show superior effectiveness than others.For example, for tasks such as Clone Detection, Code Translation, and Assert Generation, ES-KNN consistently outperforms other prompting techniques across all four selected LLMs.This suggests that semantically similar incontext demonstration examples in the prompt may be able to hint the LLMs how to process the current data sample to achieve the goals of the SE tasks properly.In addition, USC demonstrates the best performance in Code QA and Code Generation for most of the LLMs, which indicates that providing the model with structured examples or encouraging exploration of multiple solution pathways can mitigate errors in the generated output.It is worth noting that ToT presents outstanding performance for the task of Defect Detection.It prompts the LLMs to engage in a process where the code is broken down into components, and each component is examined step by step to identify the potential defects.Such a process may push the LLMs to focus on specific components one at a time, increasing the chance of identifying defects.</p>
<p>As for the four selected LLMs, DeepSeek-V3, Qwen2.5-Coder-32B-Instruct, and Llama-3.3-70Bshow nearly consistent results regarding the prompting technique that is the most effective across all SE tasks (i.e., ES-KNN is the most effective one across all these three models in Clone Detection, Exception Type Prediction, Code Translation, and Assert Generation).However, the outperforming prompting techniques that are applied to OpenAI o3-mini are different for most SE tasks, with RP being the majority winner.</p>
<p>We also include the worst-performing prompting techniques for the SE tasks in Table III.Table IV lists the prompting techniques along with the average values of the evaluation metrics across the four LLMs for the ten SE tasks in this study.We also included the aggregate performance of the two task groups: code understanding and generation tasks, which are displayed in z-score [82] format to make comparing different metrics possible.Here, a z-score indicates how many standard deviations a particular value is from the mean, allowing for a standardized comparison of performance across different metrics and scales.</p>
<p>It shows that the worst-performing prompting techniques even underperform the baseline (Section III-E) where simplistic instruction is used.This finding points out that complex prompting techniques would not necessarily improve performance when applied to achieve certain goals.Our results reveal compatibilities of various prompting techniques to different SE tasks, providing practical guidelines to researchers and software developers.These guidelines can be essential to ensure software quality since LLMs are increasingly being used to automate software development and maintenance activities [20].</p>
<p>Observation 1: No single prompt technique consistently outperforms others across all SE tasks.Certain prompting techniques can negatively impact their performance.</p>
<p>B. (RQ2) What linguistic features of prompting techniques are associated with improved performance on SE tasks?</p>
<p>To examine how linguistic characteristics of prompts relate to task performance, we analyzed selected linguistic metrics (Section III-F) across various task groups.Note that the tasks are grouped into code understanding tasks and code generation tasks following [12] (Table I).Our goal was to identify potential associations between these features and SE task performance.The results are summarized below:</p>
<p>Lexical diversity, measured by the Moving-Average Type-Token Ratio (MATTR), shows a strong positive correlation with performance across all tasks (aggregate: r = 0.4440, p &lt; 0.001).This correlation is particularly pronounced in code  generation tasks (r = 0.5229, p &lt; 0.001) and also significant in code understanding tasks (r = 0.3468, p = 0.0088).These results suggest that prompts with a rich vocabulary and a diverse use of words can enhance model performance.</p>
<p>Token count exhibits a negative correlation with performance in both code understanding (r = −0.2567,p = 0.0022) and code generation tasks (r = −0.3200,p = 0.0030).This indicates that longer prompts may not necessarily lead to better outcomes.</p>
<p>Readability metrics show varying correlations depending on the SE tasks.In code understanding tasks, Flesch-Kincaid Grade Level scores correlate negatively with task performance (r = −0.2974,p = 0.0260), suggesting that simpler prompts are more effective.However, in code generation tasks, there's a positive correlation (r = 0.2975, p = 0.0060), implying that more complex prompts may benefit these tasks.</p>
<p>Similarly, the Gunning Fog Index shows a negative correlation in code understanding tasks (r = −0.3795,p = 0.0039) and a positive correlation in code generation tasks (r = 0.2938, p = 0.0067).</p>
<p>However, The Flesch Reading Ease score shows different correlations.It correlates positively with task performance in code understanding tasks (r = 0.2797, p = 0.0369) and negatively with code generation tasks (r = −0.3366,p = 0.0017).This would indicate that readability impacts performance differently depending on the task characteristics, but different metrics demonstrate different correlation patterns.We argue that future research should further investigate how the readability of the prompts impacts the performance of the LLMs in the SE tasks.Therefore, our results underscore the importance of tailoring prompt characteristics to the specific nature of the task.Enhancing lexical diversity appears universally beneficial, while optimizing readability requires further investigation.</p>
<p>Observation 2: Linguistic features of prompts exhibit significant and various correlations with SE task performance.</p>
<p>C. (RQ3) What factors, according to LLMs, contribute to the effectiveness of prompting techniques in SE tasks</p>
<p>To understand why a specific prompting technique outperforms others in SE tasks, we used contrastive explanations to examine the additional factors contributing to the effectiveness of each technique.Table IV presents the categorized factors of the best prompting technique in contrast to the worst techniques, which was manually evaluated as outlined in III-G.For example, in the code translation task, the best prompting technique (ES-KNN) included structured guidance and in-context examples, in contrast to the worst technique (Three-of-Thought).Categories of factors in Table IV represent additional factors incorporated into the best prompting technique that contributed to superior task performance.The findings from the contrastive explanation analysis suggest that Structured Guidance and In-Context Examples are the most prevalent factors among the best-performing techniques.Furthermore, these common factors consistently appear across the majority of the SE tasks included in our experiment.This indicates that, for most SE tasks, effective prompts that enhance task performance tend to provide the model with structured guidance and relevant in-context examples.Below is the list of all categories for the factors identified for the best prompting techniques based on the manual analysis explained in Section III-G  To investigate how the resource costs impact the performance of the prompting techniques, we examine how resourceefficient each prompting technique is in conducting the SE tasks.To do so, we considered two resource measurements, namely, the number of tokens in the prompt and the response time.We normalize the efficiency of each prompting technique by dividing the values of the performance evaluation metrics by the number of used tokens and the response time.Finally, the prompting techniques are ranked based on the resourceefficiency.We present the most and least resource-efficient prompting techniques in Table VII and VIII, along with the original best and worst prompting techniques in Section IV-A.</p>
<p>Our findings reveal that, while ES-KNN remains at the top for many tasks in Section IV-A, it is not the most tokenefficient one.The additional context and examples provided by ES-KNN increase token consumption, making it less suitable in settings where minimizing token usage is critical.In contrast, RP consistently ranks at or near the top for token efficiency across most tasks.A similar shift occurs when response time is considered.Techniques such as ES-KNN and SA are frequently among the fastest, while approaches like USC and SR consistently fall to the bottom due to the computational overhead associated with their iterative reasoning or the generation of multiple responses.Despite performance gain in several code generation tasks such as Code QA and Code Generation, they tend to incur additional time costs to obtain the responses.</p>
<p>Table VI presents the average number of seconds saved per prompt.Similar to token savings, Code Generation (231.05s) and Bug Fixing (211.72s)achieved the largest average time savings across models, reinforcing that longer and more complex tasks benefit most from prompt compression.The tasks that benefit least from prompting across models are Exception type prediction (66.68s),Clone detection (80.31s), and Code QA (85.73s).This suggests that prompting techniques provide less efficiency gain for tasks that may be more recognition-or classification-based, rather than generative in nature.</p>
<p>Observation 4: Prompting techniques like ES-KNN deliver the highest performance and speed but include more tokens in the prompts, while techniques such as USC boost performance but consume more time to obtain a response.</p>
<p>V. DISCUSSION</p>
<p>Our results show that prompting effectiveness varies by task and LLM, with no single technique consistently outperforming others.Exemplar Selection KNN was strong across tasks, while Role Prompting offered a cost-efficient alternative.Notably, the o3-mini model showed different performance patterns.These findings have several implications.First, they highlight the importance of prompt-model alignment, suggesting that prompt engineering should be adaptive, not one-sizefits-all.Practitioners should empirically validate prompting strategies when switching models, even within the same model family.Second, the variation in performance across tasks reinforces the need for task-aware prompting, where the design of the prompt accounts for the structure, complexity, and requirements of the underlying task.Finally, the deviations seen with o3-mini raise concerns about prompt robustness across model scales.This suggests that lighter-weight models may require distinct prompting techniques or additional tuning to match the effectiveness seen in larger models.These insights emphasize the value of prompt evaluation frameworks that account for model-task combinations and point to future research opportunities in automated prompt selection methods.</p>
<p>Our contrastive explanation analysis revealed that prompting techniques like Exemplar Selection KNN outperform others by offering clear structural guidance and relevant in-context examples, enhancing performance across most SE tasks.These findings indicate that effective prompt design requires more than wording-it demands well-structured guidance and tailored examples aligned with the task.Prioritizing prompt clarity and minimizing ambiguity can boost model robustness and efficiency, helping practitioners develop more effective prompting strategies to maximize LLM performance in diverse software engineering tasks.Moreover, these insights suggest that adaptive prompt engineering, which dynamically incorporates task-specific structures and examples, could further improve results.This also highlights the potential for automated prompt optimization tools to assist developers in crafting prompts that align with task requirements, ultimately accelerating development workflows and reducing trial-anderror in prompt design.</p>
<p>Our analysis of token savings and time savings reveals that prompt optimization offers significant efficiency gains across SE tasks, with clear variation based on task type and LLM architecture.Tasks such as Code Generation and Bug Fixing exhibited the highest average token savings, exceeding 8,000 tokens per prompt.This suggests that these token-intensive tasks benefit substantially from prompt compression strategies, with potential reductions in inference cost and latency.These are critical considerations in real-world applications that utilize commercial LLMs with token-based billing.</p>
<p>We also observed considerable variation in token and time savings across LLMs.Larger models like Llama and DeepSeek consistently achieved higher savings, indicating they are more amenable to prompt compression, possibly due to differences in tokenization schemes, context window capacity, or internal architectural design.In contrast, o3-mini showed relatively limited token savings across most tasks, suggesting a constrained capacity for compression.These findings reinforce that prompt optimization strategies should be model-aware, as techniques effective on one LLM may not generalize to another.</p>
<p>Task-specific trends also emerged.For example, Assert Generation and Exception Type classification yielded lower aggregate token savings, implying that prompts for these tasks may already be concise, or that further compression risks omitting critical context.Conversely, Code QA with Llama showed exceptionally high savings, warranting further investigation into whether such tasks include redundancies or patterns that facilitate aggressive but safe compression.Some tasks, like Exception Type and Clone Detection, consistently had lower time savings across models, suggesting their input prompts may already be compact or that compression has limited impact on runtime.</p>
<p>From a practical standpoint, these findings emphasize that prompt design should carefully balance efficiency and performance.While reducing token usage is desirable, it must not come at the expense of output quality, particularly for complex tasks like Bug Fixing and Code Translation, which may require more detailed and nuanced input.The observed differences across tasks and models highlight an opportunity for automated, context-aware prompt rewriting tools that tailor prompt structure dynamically to maximize utility and efficiency.</p>
<p>VI. THREATS TO VALIDITY</p>
<p>In this section, we outline the potential threats to the validity of our study.</p>
<p>Construct Validity: To mitigate bias in manual evaluation for prompting technique selection (Section III-B and prompt variation template selection (Section III-C), the researchers independently conducted the manual analysis.Following opencoding practices [83], discrepancies were resolved through negotiated agreement.Moreover, to ensure the quality of model-generated responses through contrastive explanations, we adhered to established prompting practices [13].</p>
<p>Internal Validity: To ensure that task performance reflects solely the effectiveness of the prompting technique rather than prompt wording or model temperature, we used a fixed default temperature value (1 for o3-mini and Deepseek, 0.7 for Llama and Qwen) for all LLM and we employed the prompt validation process as outlined in Section III-C.In addition, we adopted the dataset used by a prior study [9].We used the same sampled dataset for each SE task to compare the performance of different prompting techniques across four LLM models.</p>
<p>External Validity: This study uses four LLMs to investigate 10 software engineering (SE) tasks and 14 prompting techniques.While our findings lay a foundation for the systematic evaluation of prompting techniques across diverse SE tasks, they may not be fully generalizable beyond the specific tasks, datasets, prompting techniques, and LLMs used in this work.To minimize the threat pertaining to LLMs, we selected models with distinct training objectives (general-purpose vs. code-specialized) and model availability (opensource vs. proprietary models) and strong performance on the widely-used EvalPlus leaderboard [65].</p>
<p>VII. CONCLUSION AND FUTURE WORK</p>
<p>In this study, we conducted systematic evaluation of 14 prompt engineering techniques across 10 SE tasks using four LLMs.Our results offer concrete insights into which techniques yield highest performance gains and where resource overheads may present practical limitations in real-world deployments.The findings of our work provide empirical evidence to guide practitioners and researchers for selecting optimal prompting techniques that are best aligned with taskspecific objectives and real-world operational constraints such as execution time and token usage.</p>
<p>Future work should extend beyond the linguistic characteristics of natural language prompts to explore additional dimensions, such as properties of prompt embeddings.Further research is needed to investigate strategies to optimize the prompt structure in accordance with ask-specific requirements and dataset characteristics.All research artifacts are available on our companion website.[84]</p>
<p>Fig. 1 .
1
Fig. 1.Example of two variations of the emotion prompting template for the task Mutant Generation.</p>
<p>TABLE I SUMMARY
I
OF SELECTED SE TASKS AND EVALUATION METRICS USED
Task (Abbreviation)CategoryDatasetMetric(s)Sample sizeDefect Detection (DD)</p>
<p>TABLE II BEST
II
PROMPTING TECHNIQUES FOR EACH MODEL AND AGGREGATE
TaskAggregateQwenDeepSeekLlamao3-miniUnderstanding TasksES-KNNES-KNNES-KNNES-KNNRPDefect detectionToTToTToTToTRPClone detectionES-KNNES-KNNES-KNNES-KNN ES-KNNException typeES-KNNES-KNNES-KNNES-KNNRRCode QAUSCSG-ICLUSCUSCRPGeneration TasksES-KNNES-KNNES-KNNES-KNN ES-KNNCode translationES-KNNES-KNNES-KNNES-KNN ES-KNNBug fixingControlSG-ICLSG-ICLSG-ICLControlMutant generationES-KNNRPES-KNNES-KNNRPAssert generationES-KNNES-KNNES-KNNES-KNN ES-KNNCode summarizationControlSG-ICLES-KNNControlSG-ICLCode generationUSCUSCSG-ICLUSCUSC</p>
<p>TABLE III WORST
III
PROMPTING TECHNIQUES FOR EACH MODEL AND AGGREGATE
TaskAggregateQwenDeepSeekLlamao3-miniUnderstanding TasksSRSRCCoTSRSBPDefect detectionEPRREPEPSBPClone detectionSRSG-ICLSG-ICLUSCSRException typeRRSRToTSRSRCode QATroTTroTTroTControlControlGeneration TasksSG-ICLSG-ICLToTSG-ICLSRCode translationSG-ICLSG-ICLUSCTroTUSCBug fixingRRToTRRSRSAMutant generationToTToTSG-ICLUSCSRAssert generationSRTroTSG-ICLUSCSRCode summarizationRRRRRRSARRCode generationES-KNNTroTTroTTroTES-KNN</p>
<p>TABLE IV COMPARISON
IV
OF PROMPTING TECHNIQUES WITH ASSOCIATED REASON CATEGORIES
BestControl PromptWorstTaskStdTechnique z-scorez-scoreTechnique z-score Best vs Worst Contrastive ExplanationUnderstanding TasksES-KNN1.120.29ToT-0.66Defect detection3.60ToT73.6666.22SR59.91Structured Guidance, RobustnessClone detection1.17ES-KNN68.6066.03SG-ICL63.45Structured Guidance, EfficiencyException type7.16ES-KNN82.5078.16ToT52.04Structured Guidance, In-Context ExampleCode QA1.39USC55.6750.99TroT51.44Ambiguity Reduction, Structured GuidanceGeneration TasksES-KNN1.830.80TroT-2.11Code translation6.30ES-KNN42.0830.19TroT13.39Structured Guidance, In-Context ExampleBug fixing4.69SG-ICL36.0236.26TroT16.45In-Context Example, RobustnessMutant generation19.59ES-KNN69.9367.98TroT16.44Structured Guidance, In-Context ExampleAssert generation15.23ES-KNN65.4425.24TroT0.92Structured Guidance, In-Context ExampleCode summarization1.51SG-ICL4.154.16ToT0.45Structured Guidance, In-Context ExampleCode generation3.04USC24.4423.18TroT13.45Ambiguity Reduction, EfficiencyObservation 3: Prompting techniques show greatereffectiveness when they include structured guidancealigned with task objectives and relevant examples.D. (RQ4) How are resource costs associated with the perfor-mance of prompting techniques in SE tasks?</p>
<p>Table V
Vpresents the mean number of tokens saved perprompt and an aggregate average. Code generation shows thehighest token savings overall (Aggregate: 10,733.42), espe-cially for Llama (8837.50), Qwen (9278.82), and DeepSeek(8502.98). Bug fixing also has very high token savings (Ag-gregate: 8306.71), with DeepSeek (8744.93), Llama (7621.01),and Qwen (7698.30). However, o3-mini lags far behind(360.08). Code QA achieves an exceptionally high savingswith Llama (19485.09), much higher than any other model onany task, which may indicate unusually long original promptsor strong compressibility. o3-mini consistently shows the low-
https://github.com/prompt-study/prompt-tasks-study/tree/main
Code Understanding Devign [32] Acc 391 Clone Detection (CD) Code Understanding BigCloneBench [33] F1 390 Exception Type Prediction (ET) Code Understanding Kanade et al. [32] Acc 380 Code Question Answering (QA) Code Understanding CoSQA [34], CodeSearchNet [35] Acc 382 Code Translation (CT) Code Generation CodeTrans [35] CodeBLEU [36] 378 Bug Fixing (BF) Code Generation BFP [37] CodeBLEU [36] 390 Mutant Generation (MG) Code Generation GM [38] BLEU [39] 390 Assert Generation (AG) Code Generation ATLAS [40] BLEU [39] 390 Code Summarization (SM) Code Generation DeepCom [35] BLEU [39] 390 Code Generation (CG) Code Generation CONCODE [41] CodeBLEU [36] 391 Code summarization RR / ToT / ToT RR / ToT / ToT RR / ToT / ToT SA / ToT / ToT RR / USC / ToT Code generation ES-KNN / SR / SR TroT / SR / SR TroT / SR / SR TroT / TroT / SR ES-KNN / USC / USC
Attention is all you need. A Vaswani, Advances in Neural Information Processing Systems. 201730</p>
<p>Lost in translation: A study of bugs introduced by large language models while translating code. R Pan, A R Ibrahimzada, R Krishna, D Sankar, L P Wassi, M Merler, B Sobolev, R Pavuluri, S Sinha, R Jabbarvand, Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. the IEEE/ACM 46th International Conference on Software Engineering2024</p>
<p>Transagents: Build your translation company with language agents. M Wu, J Xu, L Wang, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations2024</p>
<p>Only diff is not enough: Generating commit messages leveraging reasoning and action of large language model. J Li, D Faragó, C Petrov, I Ahmed, Proceedings of the ACM on Software Engineering. 12024FSE</p>
<p>Consider what humans consider: Optimizing commit message leveraging contexts considered by human. arXiv:2503.119602025arXiv preprint</p>
<p>Towards an understanding of large language models in software engineering tasks. S Hou, Y Liu, J Lee, 42nd International Conference on Software Engineering (ICSE). 2024</p>
<p>Aligning the objective of llm-based program repair. J Xu, Y Fu, S H Tan, P He, arXiv:2404.088772024arXiv preprint</p>
<p>Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting. M Sclar, Y Choi, Y Tsvetkov, A Suhr, International Conference on Learning Representations (ICLR). 2024</p>
<p>The prompt report: A systematic survey of prompt engineering techniques. S S , 2025</p>
<p>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. P Liu, W Yuan, J Fu, Z Jiang, H Hayashi, G Neubig, 10.1145/3560815ACM Comput. Surv. 559Jan. 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Proceedings of the 36th International Conference on Neural Information Processing Systems, ser. NIPS '22. the 36th International Conference on Neural Information Processing Systems, ser. NIPS '22Red Hook, NY, USACurran Associates Inc2022</p>
<p>An empirical comparison of pre-trained models of source code. C Niu, C Li, V Ng, D Chen, J Ge, B Luo, 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE2023</p>
<p>A Jacovi, S Swayamdipta, S Ravfogel, Y Elazar, Y Choi, Y Goldberg, arXiv:2103.01378Contrastive explanations for model interpretability. 2021arXiv preprint</p>
<p>Llm in a flash: Efficient large language model inference with limited memory. K Alizadeh, S I Mirzadeh, D Belenko, S Khatamifard, M Cho, C C Del Mundo, M Rastegari, M Farajtabar, 2024</p>
<p>Exploring the potential of chatgpt in automated code refinement: An empirical study. Q Guo, J Cao, X Xie, S Liu, X Li, B Chen, X Peng, Proceedings of the 46th IEEE/ACM International Conference on Software Engineering. the 46th IEEE/ACM International Conference on Software Engineering2024</p>
<p>Harnessing large language models for curated code reviews. O B Sghaier, M Weyssow, H Sahraoui, arXiv:2502.034252025arXiv preprint</p>
<p>Ai that builds with you. G Copilot, 2023</p>
<p>Jetbrains ai: Optimize your workflow. with ai built for you. J Ai, 2024</p>
<p>Jtype less, code more. visual studio intellicode brings ai assistance directly into your personal development flow. V S Intellicode, 2024</p>
<p>Using ai-based coding assistants in practice: State of affairs, perceptions, and ways forward. A Sergeyuk, Y Golubev, T Bryksin, I Ahmed, Information and Software Technology. 1781076102025</p>
<p>Large language model fine-tuning with low-rank adaptation: A performance exploration. B Hanindhito, B Patel, L K John, 10.1145/3676151.3719377Proceedings of the 16th ACM/SPEC International Conference on Performance Engineering (ICPE '25). the 16th ACM/SPEC International Conference on Performance Engineering (ICPE '25)Toronto, ON, CanadaACM2025</p>
<p>Language models are few-shot learners. T B E A Brown, Proceedings of the 34th International Conference on Neural Information Processing Systems, ser. NIPS '20. the 34th International Conference on Neural Information Processing Systems, ser. NIPS '20Curran Associates Inc2020</p>
<p>A large-scale empirical study on fine-tuning large language models for unit testing. Y Shang, Q Zhang, C Fang, S Gu, J Zhou, Z Chen, 2024</p>
<p>Prompting or fine-tuning? a comparative study of large language models for taxonomy construction. B Chen, F Yi, D Varró, 2023 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C). IEEE2023</p>
<p>A survey on large language models for code generation. J Jiang, F Wang, J Shen, S Kim, S Kim, arXiv:2406.005152024arXiv preprint</p>
<p>Language models in software development tasks: An experimental analysis of energy and accuracy. N Alizadeh, B Belchev, N Saurabh, P Kelbert, F Castor, 2025</p>
<p>Prompting is programming: A query language for large language models. P Hase, I Purohit, M Bansal, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2023</p>
<p>Distinguish before answer: Generating contrastive explanation as knowledge for commonsense question answering. Q Chen, G Xu, M Yan, J Zhang, F Huang, L Si, Y Zhang, arXiv:2305.081352023arXiv preprint</p>
<p>Interpreting language models with contrastive explanations. K Yin, G Neubig, arXiv:2202.104192022arXiv preprint</p>
<p>Codexembed: A generalist embedding model family for multiligual and multi-task code retrieval. Y Liu, R Meng, S Joty, S Savarese, C Xiong, Y Zhou, S Yavuz, arXiv:2411.126442024arXiv preprint</p>
<p>voyage-code-2: Elevate your code retrieval. V Ai, </p>
<p>. Online, </p>
<p>Learning and evaluating contextual embedding of source code. A Kanade, P Maniatis, G Balakrishnan, K Shi, Proceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine Learning2020ser. ICML'20. JMLR.org</p>
<p>Towards a big data curated benchmark of inter-project code clones. J Svajlenko, J F Islam, I Keivanloo, C K Roy, M M Mia, 2014 IEEE International Conference on Software Maintenance and Evolution. 2014</p>
<p>CoSQA: 20,000+ web queries for code search and question answering. J Huang, D Tang, L Shou, M Gong, K Xu, D Jiang, M Zhou, N Duan, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. C Zong, F Xia, W Li, R Navigli, the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational LinguisticsAug. 20211</p>
<p>Codexglue: A machine learning benchmark dataset for code understanding and generation. S L , 2021</p>
<p>Codebleu: a method for automatic evaluation of code synthesis. S Ren, D Guo, S Lu, L Zhou, S Liu, D Tang, N Sundaresan, M Zhou, A Blanco, S Ma, 2020</p>
<p>An empirical study on learning bug-fixing patches in the wild via neural machine translation. M Tufano, C Watson, G Bavota, M D Penta, M White, D Poshyvanyk, ACM Transactions on Software Engineering and Methodology (TOSEM). 2842019</p>
<p>A generative and mutational approach for synthesizing bugexposing test cases to guide compiler fuzzing. G Ye, T Hu, Z Tang, Z Fan, S H Tan, B Zhang, W Qian, Z Wang, Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering2023</p>
<p>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W Zhu, 2002</p>
<p>On learning meaningful assert statements for unit test cases. C Watson, M Tufano, K Moran, G Bavota, D Poshyvanyk, 10.1145/3377811.3380429Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, ser. ICSE '20. the ACM/IEEE 42nd International Conference on Software Engineering, ser. ICSE '20New York, NY, USAAssociation for Computing Machinery2020</p>
<p>Mapping language to code in programmatic context. S Iyer, I Konstas, A Cheung, L Zettlemoyer, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. D Riloff, J Chiang, J Hockenmaier, Tsujii, the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsOct.-Nov. 2018</p>
<p>Qualitative content analysis. J Forman, L Damschroder, Empirical methods for bioethics: A primer. Emerald Group Publishing Limited2007</p>
<p>What makes good in-context examples for GPT-3?. J Liu, D Shen, Y Zhang, B Dolan, L Carin, W Chen, The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, E. Agirre. Ireland Dublin, Online , Association for Computational LinguisticsDeeLIO 2022. May 2022Proceedings of Deep Learning Inside Out</p>
<p>Selective annotation makes language models better few-shot learners. H Su, J Kasai, C H Wu, W Shi, T Wang, J Xin, R Zhang, M Ostendorf, L Zettlemoyer, N A Smith, T Yu, 2022</p>
<p>Exploring demonstration ensembling for in-context learning. M Khalifa, L Logeswaran, M Lee, H Lee, L Wang, arXiv:2308.087802023arXiv preprint</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, International Conference on Learning Representations (ICLR). 2023</p>
<p>jina-embeddings-v2-base-code. H Face, 2025</p>
<p>Contrastive chain-of-thought prompting. Y K Chia, G Chen, L A Tuan, S Poria, L Bing, 2023</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T L Griffiths, Y Cao, K Narasimhan, 2023</p>
<p>Measuring and narrowing the compositionality gap in language models. O Press, M Zhang, S Min, L Schmidt, N A Smith, M Lewis, 2023</p>
<p>Universal self-consistency for large language model generation. X Chen, R Aksitov, U Alon, J Ren, K Xiao, P Yin, S Prakash, C Sutton, X Wang, D Zhou, 2023</p>
<p>Self-refine: Iterative refinement with self-feedback. A M , 2023</p>
<p>Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator. H J Kim, H Cho, J Kim, T Kim, K M Yoo, S Lee, 2022</p>
<p>Thread of thought unraveling chaotic contexts. Y Zhou, X Geng, T Shen, C Tao, G Long, J.-G Lou, J Shen, 2023</p>
<p>Take a step back: Evoking reasoning via abstraction in large language models. H S Zheng, S Mishra, X Chen, H.-T Cheng, E H Chi, Q V Le, D Zhou, 2024</p>
<p>Large language models understand and can be enhanced by emotional stimuli. C Li, J Wang, Y Zhang, K Zhu, W Hou, J Lian, F Luo, Q Yang, X Xie, 2023</p>
<p>Bounding the capabilities of large language models in open text generation with prompt constraints. A Lu, H Zhang, Y Zhang, X Wang, D Yang, 2023</p>
<p>Rephrase and respond: Let large language models ask better questions for themselves. Y Deng, W Zhang, Z Chen, Q Gu, 2024</p>
<p>Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. Z M Wang, 2024</p>
<p>Large language models as analogical reasoners. M Yasunaga, X Chen, Y Li, P Pasupat, J Leskovec, P Liang, E H Chi, D Zhou, 2024</p>
<p>Prompt engineering: How prompt vocabulary affects domain knowledge. D Schreiter, Georg-August-Universität Göttingen, 2024Ph.D. dissertation</p>
<p>Chatgpt: Optimizing language models for dialogue. Openai, 2023</p>
<p>Investigating the accuracy of chatgpt as a writing error correction tool. W Alsaweed, S Aljebreen, International Journal of Computer-Assisted Language Learning and Teaching. 142024</p>
<p>Is chatgpt a highly fluent grammatical error correction system? a comprehensive evaluation. T Fang, S Yang, K Lan, D F Wong, J Hu, L S Chao, Y Zhang, 2023</p>
<p>Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. J Liu, C S Xia, Y Wang, L Zhang, Proceedings of the 37th International Conference on Neural Information Processing Systems, ser. NIPS '23. the 37th International Conference on Neural Information Processing Systems, ser. NIPS '23Red Hook, NY, USACurran Associates Inc2023</p>
<p>Deepseek-v3 technical report. A Liu, 2025</p>
<p>B H , Qwen2.5-coder technical report. 2024</p>
<p>Llama 3.3 70b instruct model card. A I Meta, 20243-70B-Instruct</p>
<p>Openai o3-mini system card. Openai, 2025</p>
<p>Together models. Together, Ai, 2025</p>
<p>Cutting the gordian knot: The moving-average type-token ratio (mattr). M A Covington, J D Mcfall, Journal of Quantitative Linguistics. 1722010</p>
<p>Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel. J P Kincaid, R P Fishburne, R L Rogers, B S Chissom, Naval Technical Training Command Millington TN Research Branch. 1975TechRep. 8-75</p>
<p>The technique of clear writing. R Gunning, 1952No Title)</p>
<p>A new readability yardstick. R Flesch, Journal of applied psychology. 3232211948</p>
<p>Quality control in software documentation: Measurement of text comprehensibility. F Lehner, Information &amp; Management. 2531993</p>
<p>The proof and measurement of association between two things. C Spearman, The American Journal of Psychology. 1511904</p>
<p>Logic and causal attribution. D J Hilton, Attitudes and Attribution: A Symposium in Honour of Jos Jaspars. Sheffield, EnglandNew York University PressApr 3-5, 1986. 1988Annual Conference of the British Psychological Society</p>
<p>Towards faithfully interpretable nlp systems: How should we define and evaluate faithfulness. A Jacovi, Y Goldberg, arXiv:2004.036852020arXiv preprint</p>
<p>What makes good in-context demonstrations for code intelligence tasks with llms?. S Gao, X.-C Wen, C Gao, W Wang, H Zhang, M R Lyu, 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE2023</p>
<p>Advancing conversational ai: Best practices in prompt engineering for enhanced chatbot performance. S Goriparthi, Journal ID. 62028020</p>
<p>Card-sorting: From text to themes. T Zimmermann, Perspectives on data science for software engineering. Elsevier2016</p>
<p>. G L Iverson, Scores, 10.1007/978-0-387-79948-3_12632011SpringerNew York, NY; New York</p>
<p>Open coding descriptions. B G Glaser, Hon , An International Journal. 151495838392016Grounded Theory Review</p>
<p>GitHub -prompt-study/prompt-tasks-study -github.com. 31- 05-2025</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8380 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8380</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8380</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-10c86505de83647c7b4157595ab10f64e97c94ef</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/10c86505de83647c7b4157595ab10f64e97c94ef" target="_blank">On the Ability and Limitations of Transformers to Recognize Formal Languages</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work systematically study the ability of Transformers to model such languages as well as the role of its individual components in doing so, and provides insights on therole of self-attention mechanism in modeling certain behaviors and the influence of positional encoding schemes on the learning and generalization abilities.</p>
                <p><strong>Paper Abstract:</strong> Transformers have supplanted recurrent models in a large number of NLP tasks. However, the differences in their abilities to model different syntactic properties remain largely unknown. Past works suggest that LSTMs generalize very well on regular languages and have close connections with counter languages. In this work, we systematically study the ability of Transformers to model such languages as well as the role of its individual components in doing so. We first provide a construction of Transformers for a subclass of counter languages, including well-studied languages such as n-ary Boolean Expressions, Dyck-1, and its generalizations. In experiments, we find that Transformers do well on this subclass, and their learned mechanism strongly correlates with our construction. Perhaps surprisingly, in contrast to LSTMs, Transformers do well only on a subset of regular languages with degrading performance as we make languages more complex according to a well-known measure of complexity. Our analysis also provides insights on the role of self-attention mechanism in modeling certain behaviors and the influence of positional encoding schemes on the learning and generalization abilities of the model.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8380.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8380.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer-counting-via-attention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer indirect counting via self-attention aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transformers implement counting-like arithmetic indirectly by encoding per-symbol counter updates in value/embedding vectors and using (often uniform) self-attention aggregation to compute a (counter value)/(length) ratio which an FFN decodes to detect nonzero counters or allowed-next symbols.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Encoder-only Transformer (small models used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only Transformer (masked self-attention for left-to-right character prediction) with up to 4 layers, up to 4 heads, intermediate vector dimensions from 2 to 32; trained on character-prediction objective (MSE on k-hot next-character labels), RMSProp optimizer, small parameter counts (117 to ~17k).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>counting / depth-tracking (Dyck-1, Shuffle-Dyck), counter-value computation for n-ary Boolean expressions (counter increments depend on operator arity), length-normalized counter ratios (counter/length).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Per-symbol embeddings/value vectors encode incremental updates (+m / -m) for counters (e.g., open bracket = +1 in one coordinate, close = -1). Self-attention (often with keys set to zero in constructions or learned models exhibiting near-uniform attention) aggregates these value vectors across the prefix to produce components proportional to the cumulative counter value divided by prefix length; an FFN (with ReLU) then extracts nonnegative coordinates to implement zero-checks and next-token validity.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Visualization of self-attention block outputs (per-coordinate time series), inspection of learned value and embedding vectors (signs and magnitudes), attention-map visualization (shows near-uniform weights), correlational analysis (Pearson correlation between self-attention outputs and theoretical counter/length ratios), architectural interventions in proofs (set K matrix to zero, Q/V to identity) and ablation-like experiments comparing positional encoding schemes and layer counts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Empirical results demonstrating the mechanism: Shuffle-2 (Transformer with positional masking): Bin1 [1-50]: 100.0%; Bin2 [51-100]: 100.0%; Bin3 [101-150]: 93.0%. Shuffle-2 with absolute encodings: 100.0 / 85.2 / 63.3; relative encodings: 100.0 / 51.6 / 3.8. BoolExp-3 (positional masking): 100.0 / 100.0 / 99.8; absolute: 100.0 / 90.6 / 51.3; relative: 100.0 / 96.0 / 68.4. a^n b^n c^n (positional masking): 100.0 / 100.0 / 100.0; absolute: 100.0 / 62.1 / 5.3; relative: 100.0 / 31.3 / 22.0. Correlation evidence: Pearson r ≈ 0.99 between certain self-attention coordinates and depth/length or counter/length ratios.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Dependence on positional encoding scheme and length-extrapolation: models using absolute or relative positional encodings often fail to generalize to longer lengths (sharp accuracy drops across bins). Failure when task requires resets (single-layer models), modular counting or periodicity (Parity, (aa)*, (abab)*), and when inputs are unary (softmax aggregation yields constant self-attention outputs across timesteps).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Strong empirical visualizations: per-coordinate outputs of self-attention track theoretical depth/length and counter/length ratios with very high Pearson correlation (~0.99); inspection of value vectors showing opposite signs for open vs close brackets and magnitudes proportional to operator arity for Boolean expressions; attention maps showing nearly uniform attention weights consistent with the construction; theoretical construction (setting K=0, V/Q = identity) demonstrating how exact arithmetic-like ratios can be formed.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Transforms fail on modular/periodic arithmetic problems (Parity, (aa)*), even for short lengths, and fail to robustly extrapolate when explicit positional encodings at test time differ from training; single-layer Transformer provably cannot implement certain reset behaviors (Reset-Dyck-1) because scoring/value for reset symbol is independent of preceding inputs under single-layer masked attention; some tasks require multi-layer architectures or specific positional encodings to succeed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Ability and Limitations of Transformers to Recognize Formal Languages', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8380.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8380.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Positional-encoding-dependence</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dependence of arithmetic-like behavior on positional encodings and masking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The ability of Transformers to implement counting/periodicity tasks strongly depends on how positional information is provided: positional masking alone, absolute embeddings, relative encodings, or custom periodic encodings produce qualitatively different abilities to learn and generalize arithmetic-like tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Encoder-only Transformer (same experimental variants described above)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Small Transformer models tested with three positional-information regimes: (1) only positional masking (no explicit position embeddings), (2) absolute (fixed) positional encodings, (3) relative positional encodings (Transformer-XL style); also experiments with trainable fixed-length position embeddings and custom periodic encodings (e.g., cos(n*pi)).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>length-dependent counting, depth-capacity languages (D_n), periodicity/modular counting tasks ((aa)*, (aaaa)*, Parity, (abab)*), and reset sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>When only positional masking is provided, Transformers can sometimes succeed because ordering information comes solely from causal masking and learned symbol/value patterns; explicit positional encodings change prefix representations and can break length extrapolation if the model wasn't exposed to those encodings during training. Custom periodic positional embeddings (e.g., cos(n*pi)) can encode the necessary periodicity for modular tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Training/evaluation comparisons across models with positional masking only vs absolute encodings vs relative encodings vs custom periodic encodings; visualization of learned positional embedding components (e.g., coordinate resembling cos(n*pi)); ablations removing residual connections to test reliance on memorized positional encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Concrete numbers: (a a)^*: positional masking: 0.0% (Bin0/Bin1); absolute encoding: 1.3% / 0.0%; relative encoding: 0.6% / 0.0%; cos(n*pi) encoding: 100.0% / 100.0%; trainable fixed-length positional embeddings: 100.0% / 0.0% (doesn't extrapolate). For Shuffle/BoolExp tasks, positional masking gave best extrapolation (examples in other entry).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Standard absolute/relative encodings often lead to poor generalization to longer sequences (test-time positional encodings differ from training); trainable finite-length embeddings can learn appropriate periodic patterns but do not extrapolate to unseen lengths; certain periodic tasks require encoding with correct period (cos(n*pi) worked for period-2 but not period-4 tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical comparisons across encoding schemes showing stark differences in performance (see above numbers); visualization of learned positional embedding coordinate that resembles cos(n*pi) on (aa)* task; theoretical argument that softmax aggregation makes self-attention outputs constant for unary inputs if no position signal is present, preventing discrimination of even/odd positions.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Even when a custom positional encoding enables a specific modular task (e.g., cos(n*pi) for (aa)*), the same encoding fails for other moduli (e.g., period-4). Trainable positional embeddings that fit training lengths may not extrapolate to longer lengths, causing brittle generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Ability and Limitations of Transformers to Recognize Formal Languages', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8380.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8380.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Single-layer-reset-limitation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single-layer Transformer inability to implement reset operations (Reset-Dyck-1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single-layer Transformer with masked self-attention cannot implement counter reset behavior required by Reset-Dyck-1 because the attention scoring and value for a reset symbol are independent of preceding inputs in this architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Single-layer masked encoder Transformer (1 layer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Single-layer Transformer with masked self-attention; experiments compare 1-layer vs 2-layer performance and positional encoding variants.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>resettable counting (Reset-Dyck-1): ability to ignore prior prefix and restart counting upon encountering a reset symbol.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>In a single-layer masked Transformer the attention scoring function for a token (reset symbol) cannot depend on earlier prefix in the manner required to zero-out or negate prior contributions; therefore the model cannot make the reset symbol's scoring or value vector reflect the need to ignore previous inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Architectural intervention: compare 1-layer vs 2-layer models; theoretical proof (Lemma C.3) showing scoring/value independence; empirical evaluation (Table 2) measuring accuracies across bins with different layer counts and positional schemes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reset-Dyck-1 results (Table 2): 1-layer positional masking: Bin0 45.1%, Bin1 38.9%; 2-layer positional masking: 100.0% / 99.2%. 1-layer with positional encoding: 55.8% / 37.9%; 2-layer: 100.0% / 99.6%. LSTM baseline: 100.0% / 100.0% for both layer settings reported.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Single-layer models systematically fail on examples where a reset occurs at different positions (the model's final output is the same regardless of reset position under single-layer masked attention); therefore cannot distinguish sequences that should be accepted vs rejected depending on reset placement.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Lemma C.3 formal argument in paper; empirical corroboration by large performance gap between 1-layer and 2-layer models in Table 2; explanation that multi-layer models can make scoring/value vectors for the reset token depend on preceding inputs (enabling reset behavior).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Although 2-layer Transformers can succeed on Reset-Dyck-1, this shows that depth (multiple layers) is necessary; therefore architectural depth is an important determinant and single-layer negative results do not extend to deeper networks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Ability and Limitations of Transformers to Recognize Formal Languages', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8380.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8380.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM-counter-simulation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LSTM simulation of counters (LSTM baseline performance)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LSTMs (small architectures used as baselines) reliably implement counting and modular behaviors by updating internal cell states, achieving near-perfect generalization on many counter and regular languages that challenge Transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM (small baseline models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Small LSTM models tuned as baselines (sizes chosen to be small to avoid memorization); trained on the same character-prediction task; used as reference for comparing Transformer performance.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>direct counting via internal memory (a^n b^n, a^n b^n c^n, Dyck-1, Shuffle-Dyck, Tomita regular languages, Parity and periodic/simple modular tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>LSTM cell state acts as counters (increment/decrement or flip between values), enabling direct implementation of modular counting (e.g., flipping between two values for parity) and reset via forget gate.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Used as empirical baseline rather than internal probing in this paper; references and prior works (Weiss et al., Suzgun et al.) provide mechanistic analyses showing LSTMs simulate counter machines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Consistently near-perfect performance across many tasks in this paper: Shuffle-2 LSTM: Bin1/Bin2/Bin3 = 100.0% / 100.0% / 100.0%; BoolExp-3 LSTM: 100.0 / 100.0 / 99.7%; a^n b^n c^n LSTM: 100.0 / 100.0 / 97.8%; Tomita grammars: LSTM achieved 100.0% across bins for all Tomita languages tested.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Paper reports LSTMs generally succeed on the considered tasks; prior literature indicates LSTMs can be sensitive to precision/scale and training regimes for very long lengths, but within practical finite-length windows used here they perform robustly.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical baseline results in this paper show LSTMs generalize perfectly on both counter and regular languages where Transformers sometimes fail; prior referenced works (Weiss et al., Suzgun et al.) provided constructive and probing evidence that LSTM internal states simulate counters.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Although LSTMs perform well on evaluated tasks in this work, the paper notes theoretical limitations tied to finite precision and that Transformers are expressive in other ways (Turing-completeness results), so this is not a universal statement about superiority in all regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Ability and Limitations of Transformers to Recognize Formal Languages', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LSTM networks can perform dynamic counting <em>(Rating: 2)</em></li>
                <li>On the practical computational power of finite precision RNNs for language recognition <em>(Rating: 2)</em></li>
                <li>Theoretical limitations of self-attention in neural sequence models <em>(Rating: 2)</em></li>
                <li>On the turing completeness of modern neural network architectures <em>(Rating: 1)</em></li>
                <li>On the computational power of transformers and its implications in sequence modeling <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8380",
    "paper_id": "paper-10c86505de83647c7b4157595ab10f64e97c94ef",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "Transformer-counting-via-attention",
            "name_full": "Transformer indirect counting via self-attention aggregation",
            "brief_description": "Transformers implement counting-like arithmetic indirectly by encoding per-symbol counter updates in value/embedding vectors and using (often uniform) self-attention aggregation to compute a (counter value)/(length) ratio which an FFN decodes to detect nonzero counters or allowed-next symbols.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Encoder-only Transformer (small models used in experiments)",
            "model_description": "Encoder-only Transformer (masked self-attention for left-to-right character prediction) with up to 4 layers, up to 4 heads, intermediate vector dimensions from 2 to 32; trained on character-prediction objective (MSE on k-hot next-character labels), RMSProp optimizer, small parameter counts (117 to ~17k).",
            "arithmetic_task_type": "counting / depth-tracking (Dyck-1, Shuffle-Dyck), counter-value computation for n-ary Boolean expressions (counter increments depend on operator arity), length-normalized counter ratios (counter/length).",
            "mechanism_or_representation": "Per-symbol embeddings/value vectors encode incremental updates (+m / -m) for counters (e.g., open bracket = +1 in one coordinate, close = -1). Self-attention (often with keys set to zero in constructions or learned models exhibiting near-uniform attention) aggregates these value vectors across the prefix to produce components proportional to the cumulative counter value divided by prefix length; an FFN (with ReLU) then extracts nonnegative coordinates to implement zero-checks and next-token validity.",
            "probing_or_intervention_method": "Visualization of self-attention block outputs (per-coordinate time series), inspection of learned value and embedding vectors (signs and magnitudes), attention-map visualization (shows near-uniform weights), correlational analysis (Pearson correlation between self-attention outputs and theoretical counter/length ratios), architectural interventions in proofs (set K matrix to zero, Q/V to identity) and ablation-like experiments comparing positional encoding schemes and layer counts.",
            "performance_metrics": "Empirical results demonstrating the mechanism: Shuffle-2 (Transformer with positional masking): Bin1 [1-50]: 100.0%; Bin2 [51-100]: 100.0%; Bin3 [101-150]: 93.0%. Shuffle-2 with absolute encodings: 100.0 / 85.2 / 63.3; relative encodings: 100.0 / 51.6 / 3.8. BoolExp-3 (positional masking): 100.0 / 100.0 / 99.8; absolute: 100.0 / 90.6 / 51.3; relative: 100.0 / 96.0 / 68.4. a^n b^n c^n (positional masking): 100.0 / 100.0 / 100.0; absolute: 100.0 / 62.1 / 5.3; relative: 100.0 / 31.3 / 22.0. Correlation evidence: Pearson r ≈ 0.99 between certain self-attention coordinates and depth/length or counter/length ratios.",
            "error_types_or_failure_modes": "Dependence on positional encoding scheme and length-extrapolation: models using absolute or relative positional encodings often fail to generalize to longer lengths (sharp accuracy drops across bins). Failure when task requires resets (single-layer models), modular counting or periodicity (Parity, (aa)*, (abab)*), and when inputs are unary (softmax aggregation yields constant self-attention outputs across timesteps).",
            "evidence_for_mechanism": "Strong empirical visualizations: per-coordinate outputs of self-attention track theoretical depth/length and counter/length ratios with very high Pearson correlation (~0.99); inspection of value vectors showing opposite signs for open vs close brackets and magnitudes proportional to operator arity for Boolean expressions; attention maps showing nearly uniform attention weights consistent with the construction; theoretical construction (setting K=0, V/Q = identity) demonstrating how exact arithmetic-like ratios can be formed.",
            "counterexamples_or_challenges": "Transforms fail on modular/periodic arithmetic problems (Parity, (aa)*), even for short lengths, and fail to robustly extrapolate when explicit positional encodings at test time differ from training; single-layer Transformer provably cannot implement certain reset behaviors (Reset-Dyck-1) because scoring/value for reset symbol is independent of preceding inputs under single-layer masked attention; some tasks require multi-layer architectures or specific positional encodings to succeed.",
            "uuid": "e8380.0",
            "source_info": {
                "paper_title": "On the Ability and Limitations of Transformers to Recognize Formal Languages",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "Positional-encoding-dependence",
            "name_full": "Dependence of arithmetic-like behavior on positional encodings and masking",
            "brief_description": "The ability of Transformers to implement counting/periodicity tasks strongly depends on how positional information is provided: positional masking alone, absolute embeddings, relative encodings, or custom periodic encodings produce qualitatively different abilities to learn and generalize arithmetic-like tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Encoder-only Transformer (same experimental variants described above)",
            "model_description": "Small Transformer models tested with three positional-information regimes: (1) only positional masking (no explicit position embeddings), (2) absolute (fixed) positional encodings, (3) relative positional encodings (Transformer-XL style); also experiments with trainable fixed-length position embeddings and custom periodic encodings (e.g., cos(n*pi)).",
            "arithmetic_task_type": "length-dependent counting, depth-capacity languages (D_n), periodicity/modular counting tasks ((aa)*, (aaaa)*, Parity, (abab)*), and reset sequences.",
            "mechanism_or_representation": "When only positional masking is provided, Transformers can sometimes succeed because ordering information comes solely from causal masking and learned symbol/value patterns; explicit positional encodings change prefix representations and can break length extrapolation if the model wasn't exposed to those encodings during training. Custom periodic positional embeddings (e.g., cos(n*pi)) can encode the necessary periodicity for modular tasks.",
            "probing_or_intervention_method": "Training/evaluation comparisons across models with positional masking only vs absolute encodings vs relative encodings vs custom periodic encodings; visualization of learned positional embedding components (e.g., coordinate resembling cos(n*pi)); ablations removing residual connections to test reliance on memorized positional encodings.",
            "performance_metrics": "Concrete numbers: (a a)^*: positional masking: 0.0% (Bin0/Bin1); absolute encoding: 1.3% / 0.0%; relative encoding: 0.6% / 0.0%; cos(n*pi) encoding: 100.0% / 100.0%; trainable fixed-length positional embeddings: 100.0% / 0.0% (doesn't extrapolate). For Shuffle/BoolExp tasks, positional masking gave best extrapolation (examples in other entry).",
            "error_types_or_failure_modes": "Standard absolute/relative encodings often lead to poor generalization to longer sequences (test-time positional encodings differ from training); trainable finite-length embeddings can learn appropriate periodic patterns but do not extrapolate to unseen lengths; certain periodic tasks require encoding with correct period (cos(n*pi) worked for period-2 but not period-4 tasks).",
            "evidence_for_mechanism": "Empirical comparisons across encoding schemes showing stark differences in performance (see above numbers); visualization of learned positional embedding coordinate that resembles cos(n*pi) on (aa)* task; theoretical argument that softmax aggregation makes self-attention outputs constant for unary inputs if no position signal is present, preventing discrimination of even/odd positions.",
            "counterexamples_or_challenges": "Even when a custom positional encoding enables a specific modular task (e.g., cos(n*pi) for (aa)*), the same encoding fails for other moduli (e.g., period-4). Trainable positional embeddings that fit training lengths may not extrapolate to longer lengths, causing brittle generalization.",
            "uuid": "e8380.1",
            "source_info": {
                "paper_title": "On the Ability and Limitations of Transformers to Recognize Formal Languages",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "Single-layer-reset-limitation",
            "name_full": "Single-layer Transformer inability to implement reset operations (Reset-Dyck-1)",
            "brief_description": "A single-layer Transformer with masked self-attention cannot implement counter reset behavior required by Reset-Dyck-1 because the attention scoring and value for a reset symbol are independent of preceding inputs in this architecture.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Single-layer masked encoder Transformer (1 layer)",
            "model_description": "Single-layer Transformer with masked self-attention; experiments compare 1-layer vs 2-layer performance and positional encoding variants.",
            "arithmetic_task_type": "resettable counting (Reset-Dyck-1): ability to ignore prior prefix and restart counting upon encountering a reset symbol.",
            "mechanism_or_representation": "In a single-layer masked Transformer the attention scoring function for a token (reset symbol) cannot depend on earlier prefix in the manner required to zero-out or negate prior contributions; therefore the model cannot make the reset symbol's scoring or value vector reflect the need to ignore previous inputs.",
            "probing_or_intervention_method": "Architectural intervention: compare 1-layer vs 2-layer models; theoretical proof (Lemma C.3) showing scoring/value independence; empirical evaluation (Table 2) measuring accuracies across bins with different layer counts and positional schemes.",
            "performance_metrics": "Reset-Dyck-1 results (Table 2): 1-layer positional masking: Bin0 45.1%, Bin1 38.9%; 2-layer positional masking: 100.0% / 99.2%. 1-layer with positional encoding: 55.8% / 37.9%; 2-layer: 100.0% / 99.6%. LSTM baseline: 100.0% / 100.0% for both layer settings reported.",
            "error_types_or_failure_modes": "Single-layer models systematically fail on examples where a reset occurs at different positions (the model's final output is the same regardless of reset position under single-layer masked attention); therefore cannot distinguish sequences that should be accepted vs rejected depending on reset placement.",
            "evidence_for_mechanism": "Lemma C.3 formal argument in paper; empirical corroboration by large performance gap between 1-layer and 2-layer models in Table 2; explanation that multi-layer models can make scoring/value vectors for the reset token depend on preceding inputs (enabling reset behavior).",
            "counterexamples_or_challenges": "Although 2-layer Transformers can succeed on Reset-Dyck-1, this shows that depth (multiple layers) is necessary; therefore architectural depth is an important determinant and single-layer negative results do not extend to deeper networks.",
            "uuid": "e8380.2",
            "source_info": {
                "paper_title": "On the Ability and Limitations of Transformers to Recognize Formal Languages",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "LSTM-counter-simulation",
            "name_full": "LSTM simulation of counters (LSTM baseline performance)",
            "brief_description": "LSTMs (small architectures used as baselines) reliably implement counting and modular behaviors by updating internal cell states, achieving near-perfect generalization on many counter and regular languages that challenge Transformers.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LSTM (small baseline models)",
            "model_description": "Small LSTM models tuned as baselines (sizes chosen to be small to avoid memorization); trained on the same character-prediction task; used as reference for comparing Transformer performance.",
            "arithmetic_task_type": "direct counting via internal memory (a^n b^n, a^n b^n c^n, Dyck-1, Shuffle-Dyck, Tomita regular languages, Parity and periodic/simple modular tasks).",
            "mechanism_or_representation": "LSTM cell state acts as counters (increment/decrement or flip between values), enabling direct implementation of modular counting (e.g., flipping between two values for parity) and reset via forget gate.",
            "probing_or_intervention_method": "Used as empirical baseline rather than internal probing in this paper; references and prior works (Weiss et al., Suzgun et al.) provide mechanistic analyses showing LSTMs simulate counter machines.",
            "performance_metrics": "Consistently near-perfect performance across many tasks in this paper: Shuffle-2 LSTM: Bin1/Bin2/Bin3 = 100.0% / 100.0% / 100.0%; BoolExp-3 LSTM: 100.0 / 100.0 / 99.7%; a^n b^n c^n LSTM: 100.0 / 100.0 / 97.8%; Tomita grammars: LSTM achieved 100.0% across bins for all Tomita languages tested.",
            "error_types_or_failure_modes": "Paper reports LSTMs generally succeed on the considered tasks; prior literature indicates LSTMs can be sensitive to precision/scale and training regimes for very long lengths, but within practical finite-length windows used here they perform robustly.",
            "evidence_for_mechanism": "Empirical baseline results in this paper show LSTMs generalize perfectly on both counter and regular languages where Transformers sometimes fail; prior referenced works (Weiss et al., Suzgun et al.) provided constructive and probing evidence that LSTM internal states simulate counters.",
            "counterexamples_or_challenges": "Although LSTMs perform well on evaluated tasks in this work, the paper notes theoretical limitations tied to finite precision and that Transformers are expressive in other ways (Turing-completeness results), so this is not a universal statement about superiority in all regimes.",
            "uuid": "e8380.3",
            "source_info": {
                "paper_title": "On the Ability and Limitations of Transformers to Recognize Formal Languages",
                "publication_date_yy_mm": "2020-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LSTM networks can perform dynamic counting",
            "rating": 2
        },
        {
            "paper_title": "On the practical computational power of finite precision RNNs for language recognition",
            "rating": 2
        },
        {
            "paper_title": "Theoretical limitations of self-attention in neural sequence models",
            "rating": 2
        },
        {
            "paper_title": "On the turing completeness of modern neural network architectures",
            "rating": 1
        },
        {
            "paper_title": "On the computational power of transformers and its implications in sequence modeling",
            "rating": 1
        }
    ],
    "cost": 0.01845175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>On the Ability and Limitations of Transformers to Recognize Formal Languages</h1>
<p>Satwik Bhattamishra Kabir Ahuja ${ }^{\ominus *}$ Navin Goyal ${ }^{\text {® }}$<br>${ }^{\text {M }}$ Microsoft Research India<br>${ }^{\text {® }}$ Udaan.com<br>{t-satbh, navingo}@microsoft.com<br>kabir.ahuja@udaan.com</p>
<h4>Abstract</h4>
<p>Transformers have supplanted recurrent models in a large number of NLP tasks. However, the differences in their abilities to model different syntactic properties remain largely unknown. Past works suggest that LSTMs generalize very well on regular languages and have close connections with counter languages. In this work, we systematically study the ability of Transformers to model such languages as well as the role of its individual components in doing so. We first provide a construction of Transformers for a subclass of counter languages, including well-studied languages such as $n$-ary Boolean Expressions, Dyck-1, and its generalizations. In experiments, we find that Transformers do well on this subclass, and their learned mechanism strongly correlates with our construction. Perhaps surprisingly, in contrast to LSTMs, Transformers do well only on a subset of regular languages with degrading performance as we make languages more complex according to a well-known measure of complexity. Our analysis also provides insights on the role of self-attention mechanism in modeling certain behaviors and the influence of positional encoding schemes on the learning and generalization abilities of the model.</p>
<h2>1 Introduction</h2>
<p>Transformer (Vaswani et al., 2017) is a selfattention based architecture which has led to state-of-the-art results across various NLP tasks (Devlin et al., 2019; Liu et al., 2019; Radford et al., 2018). Much effort has been devoted to understand the inner workings and intermediate representations of pre-trained models; Rogers et al. (2020) is a recent survey. However, our understanding of their practical ability to model different behaviors relevant to sequence modeling is still nascent.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Counter languages form a strict superset of regular languages, and are a strict subset of contextsensitive languages. Counter and context-free languages have a nonempty intersection and neither set is contained in the other.</p>
<p>On the other hand, a long line of research has sought to understand the capabilities of recurrent neural models such as the LSTMs (Hochreiter and Schmidhuber, 1997) . Recently, Weiss et al. (2018), Suzgun et al. (2019a) showed that LSTMs are capable of recognizing counter languages such as Dyck-1 and $a^{n} b^{n}$ by learning to perform counting like behavior. Suzgun et al. (2019a) showed that LSTMs can recognize shuffles of multiple Dyck1 languages, also known as Shuffle-Dyck. Since Transformer based models (e.g., GPT-2 and BERT) are not equipped with recurrence and start computation from scratch at each step, they are incapable of directly maintaining a counter. Moreover, it is known that theoretically RNNs can recognize any regular language in finite precision, and LSTMs work well for this task in practical settings. However, Transformer's ability to model such properties in practical settings remains an open question.</p>
<p>Prior to the current dominance of Transformers for NLP tasks, recurrent models like RNN-based models such as LSTMs were the most common choice, and their computational capabilities have</p>
<p>been studied for decades, e.g., (Kolen and Kremer, 2001). In this work, we investigate the ability of Transformers to express, learn, and generalize on certain counter and regular languages. Formal languages provide us a controlled setting to study a network's ability to model different syntactic properties in isolation and the role of its individual components in doing so.</p>
<p>Recent work has demonstrated close connections between LSTMs and counter automata. Hence, we seek to understand the capabilities of Transformers to model languages for which the abilities of LSTMs are well understood. We first show that Transformers are expressive enough to recognize certain counter languages like Shuffle-Dyck and $n$-ary Boolean Expressions by using self-attention mechanism to implement the relevant counter operations in an indirect manner. We then extensively evaluate the model's learning and generalization abilities on such counter languages and find that models generalize well on such languages. Visualizing the intermediate representations of these models shows strong correlations with our proposed construction. Although Transformers can generalize well on some popularly used counter languages, we observe that they are limited in their ability to recognize others. We find a clear contrast between the performance of Transformers and LSTMs on regular languages (a subclass of counter languages). Our results indicate that, in contrast to LSTMs, Transformers achieve limited performance on languages that involve modeling periodicity, modular counting, and even simpler star-free variants of Dyck-1, which they were able to recognize effortlessly. Our analysis provides insights about the significance of different components, namely self-attention, positional encoding, and the number of layers. Our results also show that positional masking and positional encoding can both aid in generalization and training, but in different ways. We conduct extensive experiments on over 25 carefully chosen formal languages. Our results are perhaps the first indication of the limitations of Transformers for practical-sized problems that are, in a precise sense, very simple, and in particular, easy for recurrent models.</p>
<h2>2 Related Work</h2>
<p>Numerous works, e.g., Suzgun et al. (2019b); Sennhauser and Berwick (2018); Skachkova et al. (2018), have attempted to understand the capabil-
ities and inner workings of recurrent models by empirically analyzing them on formal languages. Weiss et al. (2018) showed that LSTMs are capable of simulating counter operations and explored their practical ability to recognize languages like $a^{n} b^{n}$ and $a^{n} b^{n} c^{n}$. Suzgun et al. (2019a) further showed that LSTMs can learn to recognize Dyck-1 and Shuffle-Dyck and can simulate the behavior of $k$-counter machines. Theoretical connections of recurrent models have been established with counter languages (Merrill, 2019; Merrill et al., 2020; Merrill, 2020). It has also been shown that RNN based models can recognize regular languages (Kolen and Kremer, 2001; Korsky and Berwick, 2019) and efforts have been made to extract DFAs from RNNs trained to recognize regular languages (Weiss et al., 2019; Wang et al., 2018b; Michalenko et al., 2019). We are not aware of such studies for Transformers.</p>
<p>Recently, researchers have sought to empirically analyze different aspects of the Transformer trained on practical NLP tasks such as the information contained in intermediate layers (Rogers et al., 2020; Reif et al., 2019; Warstadt et al., 2019). Voita et al. (2019) studied the role of different types of attention heads. Yang et al. (2019); Tsai et al. (2019) examined the ability of the model to learn order information via different positional encoding schemes. Complementary to these, our work is focused on analyzing Transformer's ability to model particular behaviors that could be relevant to modeling linguistic structure. Recently, it has been shown that Transformers are Turing-complete (Pérez et al., 2019; Bhattamishra et al., 2020) and are universal approximators of sequence-to-sequence functions given arbitrary precision (Yun et al., 2020). Hahn (2020) shows that Transformers cannot recognize languages Parity and Dyck-2. However, these results only apply to very long words, and their applicability to practical-sized inputs is not clear (indeed, we will see different behavior for practicalsized input). Moreover, these results concern the expressive power of Transformers and do not apply to learning and generalization abilities. Thus Transformers' ability to model formal languages requires further investigation.</p>
<h2>3 Definitions</h2>
<p>We consider the Transformer as used in popular pretrained LM models such as BERT and GPT, which is the encoder-only model of the original seq-to-seq architecture (Vaswani et al., 2017). The encoder</p>
<p>consists of multiple layers with two blocks each: (1) self-attention block, (2) a feed-forward network (FFN). For $1 \leq i \leq n$, at the $i$-th step, the model takes as input the sequence $s_{1}, s_{2}, \ldots, s_{i}$ where $s \in \Sigma$ and generates the output vector $\boldsymbol{y}<em i="i">{i}$. Each input $s</em>}$ is first converted into an embedding vector using the function $f_{e}: \Sigma \rightarrow \mathbb{R}^{d_{\text {model }}}$ and usually some form of positional encoding is added to yield the final input vector $\boldsymbol{x<em _model="{model" _text="\text">{i}$. The embedding dimension $d</em>}}$ is also the dimension of intermediate vectors of the network. Let $\boldsymbol{X<em 1="1">{i}:=\left(\boldsymbol{x}</em>\right)$ for $i \geq 1$.}, \ldots, \boldsymbol{x}_{i</p>
<p>In the self-attention block, the input vectors undergo linear transformations $Q(\cdot), K(\cdot)$, and $V(\cdot)$ yielding the corresponding query, key and value vectors, respectively. The self-attention mechanism takes as input a query vector $Q\left(\boldsymbol{x}<em i="i">{i}\right)$, key vectors $K\left(\boldsymbol{X}</em>}\right)$, and value vectors $V\left(\boldsymbol{X<em i="i">{i}\right)$. An attentionhead denoted by $\operatorname{Att}\left(Q\left(\boldsymbol{x}</em>}\right), K\left(\boldsymbol{X<em i="i">{i}\right), V\left(\boldsymbol{X}</em>}\right)\right)$, is a vector $\boldsymbol{a<em j="1">{i}=\sum</em>}^{i} \alpha_{j} \boldsymbol{v<em 1="1">{j}$, where $\left(\alpha</em>}, \ldots, \alpha_{i}\right)=$ $\operatorname{softmax}\left(\left\langle Q\left(\boldsymbol{x<em 1="1">{i}\right), K\left(\boldsymbol{x}</em>}\right)\right\rangle, \ldots,\left\langle Q\left(\boldsymbol{x<em i="i">{i}\right), K\left(\boldsymbol{x}</em>\right)\right\rangle\right)$.</p>
<p>The output of a layer denoted by $\boldsymbol{z}<em i="i">{i}$ is computed by $\boldsymbol{z}</em>}=O\left(\boldsymbol{a<em i="i">{i}\right)$ where $1 \leq i \leq n$ and $O(\cdot)$ typically denotes an FFN with ReLU activation. The complete $L$-layer model is a repeated application of the single-layer model described above, which produces a vector $\boldsymbol{z}</em>}^{L}$ at its final layer where $L$ denotes the last layer. The final output is obtained by applying a projection layer with some normalization or an FFN over the vectors $\boldsymbol{z<em i="i">{i}^{L}$ 's and is denoted by $\boldsymbol{y}</em>\right)$. Residual connections and layer normalization are also applied to aid the learning process of the network.}=F\left(\boldsymbol{z}_{i}^{L</p>
<p>In an LM setting, when the Transformer processes the input sequentially, each input symbol can only attend over itself and the previous inputs, masking is applied over the inputs following it. Note that, providing positional information in this form via masked self-attention is also referred to as positional masking (Vaswani et al., 2017; Shen et al., 2018). A Transformer model without positional encoding and positional masking is orderinsensitive.</p>
<h3>3.1 Formal Languages</h3>
<p>Formal languages are abstract models of the syntax of programming and natural languages; they also relate to cognitive linguistics, e.g., Jäger and Rogers (2012); Hahn (2020) and references therein.
Counter Languages. These are languages recognized by a deterministic counter automaton (DCA), that is, a DFA with a finite number of unbounded
counters (Fischer et al., 1968). The counters can be incremented/decremented by constant values and can be reset to 0 (details in App. B.1). The commonly used counter languages to study sequence models are Dyck-1, $a^{\mathrm{n}} b^{\mathrm{n}}$, and $a^{\mathrm{n}} b^{\mathrm{n}} c^{\mathrm{n}}$. Several works have explored the ability of recurrent models to recognize these languages as well as their underlying mechanism to do so. We include them in our analysis as well as some general form of counter languages such as Shuffle-Dyck (as used in Suzgun et al. (2019a)) and $n$-ary Boolean Expressions. The language Dyck-1 over alphabet $\Sigma={[,]}$ consists of balanced parentheses defined by derivation rules $S \rightarrow[S] \mid S S \mid \epsilon$. Shuffle-Dyck is a family of languages containing shuffles of Dyck-1 languages. Shuffle- $k$ denotes the shuffle of $k$ Dyck-1 languages: it contains $k$ different types of brackets, where each type of bracket is required to be wellbalanced, but their relative order is unconstrained. For instance, a Shuffle-2 language over alphabet $\Sigma={[,],(,)}$ contains the words $(])]$ and $[(())$ but not $])[$ (. We also consider $n$-ary Boolean Expressions (hereby BoolExp- $n$ ), which are a family of languages of valid Boolean expressions (in the prefix notation) parameterized by the number of operators and their individual arities. For instance, an expression with unary operator $\sim$ and binary operator $\wedge$ contains the word ' $\wedge \sim 01$ ' but not ' $\sim 10$ ' (formal definitions in App. B).</p>
<p>Note that, although languages such as Dyck-1 and $a^{\mathrm{n}} b^{\mathrm{n}}$ are context-free, a DCA with a single counter is sufficient to recognize Dyck-1 and $a^{\mathrm{n}} b^{\mathrm{n}}$. Similarly, a DCA with two single-turn counters can recognize $a^{\mathrm{n}} b^{\mathrm{n}} c^{\mathrm{n}}$. On the other hand, recognizing Shuffle-Dyck requires multiple multi-turn counters, where for a given type of bracket, its corresponding counter is incremented or decremented by 1 . Hence, it represents a more general form of counter languages. Similarly, recognizing BoolExp requires a 1-counter DCA with the counter updates depending on the operator: a ternary operator will increment the counter by 2 ( $=$ arity -1 ) whereas a unary operator will increment it by 0 . Figure 1 shows the relationship between counter languages and other classes of formal languages.
Regular Languages. Regular languages, perhaps the best studied class of formal languages, form a subclass of counter languages ${ }^{1}$. They neatly divide</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>into two subclasses: star-free and non-star-free. Star-free languages can be described by regular expressions formed by union, intersection, complementation, and concatenation operators but not the Kleene star (*). Like regular languages, star-free languages are surprisingly rich with algebraic, logical, and multiple other characterizations and continue to be actively researched, e.g., (McNaughton and Papert, 1971; Jäger and Rogers, 2012). They form a simpler subclass of regular languages where the notion of simplicity can be made precise in various ways, e.g. they are first-order logic definable and cannot represent languages that require modular counting.</p>
<p>We first consider Tomita grammars containing 7 regular languages representable by DFAs of small sizes, a popular benchmark for evaluating recurrent models and extracting DFA from trained recurrent models (see, e.g., Wang et al. (2018a)). Tomita grammars contain both star-free and non-star-free languages. We further investigate some non-star-free languages such as $(a a)^{<em>}$, Parity and $(a b a b)^{</em>}$. Parity contains words over ${0,1}$ with an even number of 1 's. Similarly $(a a)^{<em>}$ and $(a b a b)^{</em>}$ require modeling periodicity.</p>
<p>On the other hand, the seemingly similar looking language $(a b)^{<em>}$ is star-free: $(a b)^{</em>}=\left(b \emptyset^{c}+\varnothing^{c} a+\right.$ $\left.\varnothing^{c} a a \emptyset^{c}+\varnothing^{c} b b \emptyset^{c}\right)^{c}$, where $c$ denotes set complementation, and thus $\varnothing^{c}=\Sigma^{<em>}$. The dot-depth of a star-free language is a measure of nested concatenation or sequentiality required in a star-free regular expression (formal definition in App. B.2). We define a family $\mathcal{D}<em 1="1">{0}, \mathcal{D}</em>}, \ldots$ of star-free languages. For $n \geq 0$, the language $\mathcal{D<em n="n">{n}$ over $\Sigma={a, b}$ is defined inductively as follows: $\mathcal{D}</em> b\right)^{}=\left(a \mathcal{D}_{n-1</em>}$ where $\mathcal{D}<em 1="1">{0}=\epsilon$, the empty word. Thus $\mathcal{D}</em>=(a b)^{<em>}$ and $\mathcal{D}_{2}=\left(a(a b)^{</em>} b\right)^{*}$. Language $\mathcal{D}_{n}$ is known to have dot-depth $n$.</p>
<p>The list of all considered languages and their definitions are provided in the App. B.</p>
<h2>4 Expressiveness Results</h2>
<p>Proposition 4.1. There exists a Transformer as defined in Section 3 that can recognize the family of languages Shuffle-Dyck.</p>
<p>Proof. Let $s_{1}, s_{2}, \ldots, s_{n}$ denote a sequence $w \in$ Shuffle- $k$ over the alphabet $\Sigma=$ $\left{\left.\left[{ }<em k-1="k-1">{0}, \ldots,\right|</em>,\right]<em k-1="k-1">{0}, \ldots,\right|</em>$ is defined as follows.}}$. The language Shuffle1 is equivalent to Dyck-1. For any Shuffle- $k$ language, consider a model with $d_{\text {model }}=2 k$, where the embedding function $f_{e</p>
<p>For each type of open bracket [ $j$ where,$0 \leq j&lt;k$, the vector $f_{e}\left(\left[<em e="e">{j}\right)\right.$ has the value +1 and -1 at the indices $2 j$ and $2 j+1$, respectively. It has the value 0 at the rest of the indices. Similarly for each closing bracket, the vector $f</em>\left(\left[<em e="e">{j}\right)\right.$ has the value -1 and +1 at the indices $2 j$ and $2 j+1$, and it has the value 0 at the rest of the indices. For Dyck-1, this would lead to $f</em>}([)=[+1,-1]^{T}$ and $f_{e}([)=[-1,+1]^{T}$ (with $d_{\text {model }}=2$ ). We use a single-layer Transformer where we set the matrix corresponding to linear transformation for key vectors to be null matrix, that is $K(\boldsymbol{x})=\mathbf{0}$ for all $\boldsymbol{x}$. This will lead to equal attention weights for all inputs. The matrices corresponding to $Q(\cdot)$ and $V(\cdot)$ are set to Identity. Thus, $\operatorname{Att}\left(Q\left(\boldsymbol{x<em i="i">{i}\right), K\left(\boldsymbol{X}</em>}\right), V\left(\boldsymbol{X<em j="1">{i}\right)\right)=\frac{1}{i} \sum</em>}^{i} \boldsymbol{v<em i="i">{j}$ for $1 \leq i \leq n$. Hence, at the $i$-th step, the selfattention block produces a vector $\boldsymbol{a}</em>}$ which has the values $\frac{\sigma\left([j)-\sigma\left([j)\right.\right.}{i}$ at indices $2 j$ and the values $\frac{\sigma\left([j)-\sigma\left([j)\right.\right.}{i}$ at indices $2 j+1$, where $\sigma(s)$ denotes the number of occurrence of the symbol $s$. For instance, in Dyck-1, if in the first $i$ inputs, there are $\sigma(\mid)$ open brackets and $\sigma(\mid)$ closing brackets, then $\boldsymbol{a<em i="i">{i}=\left[\frac{\sigma(\mid)-\sigma(\mid)}{i}, \frac{\sigma(\mid)-\sigma(\mid)}{i}\right]^{T}$, where $i=\sigma(\mid)+\sigma(\mid)$. In $\boldsymbol{a}</em>$, the value $\sigma(\mid)-\sigma(\mid)$ represents the depth (difference between the number of open and closing brackets) of the Dyck-1 word at index $i$. Hence, the first coordinate is the ratio of the depth of the Dyck-1 word and its length at that index, while the other coordinate is its negative.</p>
<p>We then apply a simple FFN with ReLU activation over the vector $\boldsymbol{a}<em i="i">{i}$. The vector $\boldsymbol{z}</em>}=$ $\operatorname{ReLU}\left(\mathbf{I} \boldsymbol{a<em i="i">{i}\right)$. The even indices of the vector $\boldsymbol{z}</em>$ must never be nonzero, and the values of all coordinates must be zero at the last step to ensure the number of open and closing brackets are the same.}$ will be nonzero if the number of open brackets of the corresponding type is greater than the number of closing brackets. A similar statement holds for the odd indices. Thus, for a given word to be in Shuffle$k$, the values at odd indices of the vector $\boldsymbol{z}_{i</p>
<p>For an input sequence $s_{1}, s_{2}, \ldots, s_{n}$, the model will produce $\boldsymbol{z}<em n="n">{1}, \ldots, \boldsymbol{z}</em>}$ based on the construction specified above. A word $w$ belongs to language Shuffle- $k$ if $\boldsymbol{z<em n="n">{i, 2 j+1}=0$ for all $1 \leq i \leq n, 0 \leq j&lt;$ $k$ and $\boldsymbol{z}</em>$ and does not belong to the language otherwise. This can be easily implemented by an additional layer of self-attention and feedforward network to classify a given sequence.}=\mathbf{0</p>
<p>The bottleneck for precision in the construc-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Language</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Bin-1 Accuracy $[1,50]^{\dagger}$</th>
<th style="text-align: center;">Bin-2 Accuracy $[51,100]^{\dagger}$</th>
<th style="text-align: center;">Bin-3 Accuracy $[101,150]^{\dagger}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Shuffle-2</td>
<td style="text-align: center;">LSTM (Baseline)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Absolute Positional Encodings)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">85.2</td>
<td style="text-align: center;">63.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Relative Positional Encodings)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">3.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Only Positional Masking)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">93.0</td>
</tr>
<tr>
<td style="text-align: center;">BoolExp-3</td>
<td style="text-align: center;">LSTM (Baseline)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">99.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Absolute Positional Encodings)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;">51.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Relative Positional Encodings)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">96.0</td>
<td style="text-align: center;">68.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Only Positional Masking)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">99.8</td>
</tr>
<tr>
<td style="text-align: center;">$a^{n} b^{n} c^{n}$</td>
<td style="text-align: center;">LSTM (Baseline)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">97.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Absolute Positional Encodings)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">62.1</td>
<td style="text-align: center;">5.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Relative Positional Encodings)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">31.3</td>
<td style="text-align: center;">22.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Only Positional Masking)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
</tbody>
</table>
<p>Table 1: The performance of Transformers and LSTMs on the respective counter languages. Refer to section 6 for details. Performance on other counter languages such as Shuffle-4 and Shuffle-6 are listed in Table 8 in appendix.
tion above is the calculation of values of the form $\frac{\sigma(\mid t-\sigma())}{t}$ in the vector $\boldsymbol{\alpha}_{i}$. Since in a finite precision setting with $r$ bits, this can be computed up to a value exponential in $r$, our proof entails that Transformers can recognize languages in Shuffle-Dyck for lengths exponential in the number of bits.</p>
<p>Using a similar logic, one can also show that Transformers can recognize the family of languages BoolExp- $n$ (refer to Lemma C.2). By setting the value vectors according to the arities of the operators, the model can obtain the ratio of the counter value of the underlying automata and the length of the input at each step via self-attention. Although the above construction is specific to these language families, we provide a proof for a more general but restricted subclass of Counter Languages in the appendix (refer to Lemma C.1). The above construction serves to illustrate how Transformers can recognize such languages by indirectly doing relevant computations. As we will later see, this will also help us interpret how trained models recognize such languages.</p>
<h2>5 Experimental Setup</h2>
<p>In our experiments, we consider 27 formal languages belonging to different parts in the hierarchy of counter and regular languages. For each language, we generate samples within a fixed-length window for our training set and generate multiple validation sets with different windows of length to evaluate the model's generalization ability.</p>
<p>For most of the languages, we generate 10 k samples for our training sets within lengths 1 to 50 and create different validation sets containing samples with distinct but contiguous windows of length. The number of samples in each validation set is</p>
<p>2 k , and the width of each window is about 50 . For languages that have very few positive examples in a given window of length, such as $(a b)^{*}$ and $a^{n} b^{n} c^{n}$, we train on all positive examples within the training window. Similarly, each validation set contains all possible strings of the language for a particular range. Table 6 in appendix lists the dataset statistics of all 27 formal languages we consider. ${ }^{2}$. We have made our source code available at https://github.com/satwik77/Transformer-Formal-Languages.</p>
<h3>5.1 Training details</h3>
<p>We train the model on character prediction task as introduced in Gers and Schmidhuber (2001) and as used in Suzgun et al. (2019b,a). Similar to an LM setup, the model is only presented with positive samples from the given language. For an input sequence $s_{1}, s_{2}, \ldots, s_{n}$, the model receives the sequence $s_{1}, \ldots, s_{i}$ for $1 \leq i \leq n$ at each step $i$ and the goal of the model is to predict the next set of legal/valid characters in the $(i+1)^{t h}$ step. From here onwards, we say a model can recognize a language if it can perform the character prediction task perfectly.</p>
<p>The model assigns a probability to each character in the vocabulary of the language corresponding to its validity in the next time-step. The output can be represented by a $k$-hot vector where each coordinate corresponds to a character in the vocabulary of the language. The output is computed by applying a sigmoid activation over the scores assigned by the model for each character. Following Suzgun et al. (2019b,a), the learning objective of</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>the model is to minimize the mean-squared error between the predicted probabilities and the $k$-hot labels. ${ }^{3}$ During inference, we use a threshold of 0.5 to obtain the predictions of the model. For a test sample, the model's prediction is considered to be correct if and only if its output at every step is correct. Note that, this is a relatively stringent metric as a correct prediction is obtained only when the output is correct at every step. The accuracy of the model over test samples is the fraction of total samples predicted correctly ${ }^{4}$. Similar to Suzgun et al. (2019a) we consider models of small sizes to prevent them from memorizing the training set and make it feasible to visualize the model. In our experiments, we consider Transformers with up to 4 layers, 4 heads and the dimension of the intermediate vectors within 2 to 32 . We extensively tune the model across various hyperparameter settings. We also examine the influence of providing positional information in different ways such as absolute encodings, relative encodings (Dai et al., 2019) and using only positional masking without any explicit encodings.</p>
<h2>6 Results on Counter Languages</h2>
<p>We evaluated the performance of the model on 9 counter languages. Table 1 shows the performance of different models described above on some representative languages. We also include the performance of LSTMs as a baseline. We found that Transformers of small size (single head and single layer) can generalize well on some general form of counter languages such as Shuffle-Dyck and BoolExp- $n$. Surprisingly, we observed this behavior when the network was not provided any form of explicit positional encodings, and positional information was only available in the form of masking. For models with positional encoding, the lack of the ability to generalize to higher lengths could be attributed to the fact that the model has never been trained on some of the positional encodings that it receives at test time. On the other hand, the model without any explicit form of positional encoding is less susceptible to such issues if it is capable of performing the task and was found to generalize well across various hyperparameter settings.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Values of different coordinates of the output of self-attention block of the models trained on Shuffle2 and BoolExp-3. The dotted lines are the scaled depth to length ratios for Shuffle-2 and scaled counter value to length ratios for BoolExp-3. We observe a near perfect Pearson correlation coefficent of 0.99 between outputs of self attention block and the DL and CL ratios.</p>
<h3>6.1 Role of Self-Attention</h3>
<p>In order to check our hypothesis in Sec. 4, we visualize certain attributes of trained models that generalize well on Shuffle-2 and BoolExp-3. ${ }^{5}$ Our construction in Sec. 4 recognizes sequences in Shuffle-Dyck by computing the depth to length ratio of the input at each step via self-attention mechanism. For BoolExp- $n$, the model can achieve the task similarly by computing the corresponding counter value divided by length (refer to Lemma C.2). Interestingly, upon visualizing the outputs of the self-attention block for a model trained on Shuffle-2, we found a strong correlation of its elements with the depth to length ratio. As shown in Fig. 2a, different coordinates of the output vector of the self-attention block contain computations corresponding to different counters of the Shuffle-2 language. We observe the same behavior for models trained on Shuffle-4 language (refer to Figure 5 in appendix). Similarly, upon visualizing a model trained on Boolean Expressions with 3 operators, we found strong correlation ${ }^{6}$ between its elements and the ratio of the counter value and length of the input (refer to Figure 2b). This indicates that the model learns to recognize inputs by carrying out the required computation in an indirect manner,</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">1-Layer</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">2-Layer</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model Type</td>
<td style="text-align: center;">Bin 0</td>
<td style="text-align: center;">Bin 1</td>
<td style="text-align: center;">Bin 0</td>
<td style="text-align: center;">Bin 1</td>
</tr>
<tr>
<td style="text-align: center;">Positional Masking</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">99.2</td>
</tr>
<tr>
<td style="text-align: center;">Positional Encoding</td>
<td style="text-align: center;">55.8</td>
<td style="text-align: center;">37.9</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">99.6</td>
</tr>
<tr>
<td style="text-align: center;">LSTM</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
</tbody>
</table>
<p>Table 2: Results on language Reset-Dyck-1 with different number of layers.
as described in our construction. Additionally, for both models, we found that the attention weights of the self-attention block were uniformly distributed (refer to Figure 4 in appendix). Further, on inspecting the embedding and value vectors of the open and closing brackets, we found that their respective coordinates were opposite in sign and similar in magnitude. As opposed to Shuffle-Dyck, for BoolExp- $n$, the magnitudes of the elements in the value vectors were according to their corresponding arity. For instance, the magnitude for a ternary operator was (almost) thrice the magnitude for a unary operator (refer to Figure 3 in appendix). These observations are consistent with our construction, indicating that the model uses its value vectors to determine the counter updates and then at each step, aggregates all the values to obtain a form of the final counter value in an indirect manner. This is complementary to LSTMs, which can simulate the behavior of $k$-counters more directly by making respective updates to its cell states upon receiving each input (Suzgun et al., 2019a).</p>
<h3>6.2 Limitations of the Single-Layer Transformer</h3>
<p>Although we observed that single-layer Transformers are easily able to recognize some of the popularly studied counter languages, at the same time, it is not necessarily true for counter languages that require reset operations. We define a variant of the Dyck-1 language. Let Reset-Dyck-1 be the language defined over the alphabet $\Sigma=$ ${[,], #}$, where # denotes a symbol that resets the counter. Words in Reset-Dyck-1 have the form $\Sigma^{*} # v$, where the string $v$ belongs to Dyck1. When the machine encounters the reset symbol #, it must ignore all the previous input, reset the counter to 0 and go to start state. It is easy to show that this cannot be directly implemented with a single layer self-attention network with positional masking (Lemma C. 3 in Appendix). The key limitation for both with and without encodings is the fact that for a single layer network the scor-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Language</th>
<th style="text-align: center;">StarFree</th>
<th style="text-align: center;">Transformer</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LSTM</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Bin 0</td>
<td style="text-align: center;">Bin 1</td>
<td style="text-align: center;">Bin 0</td>
<td style="text-align: center;">Bin 1</td>
</tr>
<tr>
<td style="text-align: center;">Tomita 1</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">Tomita 2</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">Tomita 3</td>
<td style="text-align: center;">$#$</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">10.8</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">Tomita 4</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">Tomita 5</td>
<td style="text-align: center;">$#$</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">Tomita 6</td>
<td style="text-align: center;">$#$</td>
<td style="text-align: center;">88.8</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">Tomita 7</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
</tbody>
</table>
<p>Table 3: Results on Tomita grammar
ing function $\left\langle Q\left(\boldsymbol{x}<em _35_="#">{n}\right), K\left(\boldsymbol{x}</em>$. LSTMs, on the other hand, can emulate the reset operation using forget gate.}\right)\right\rangle$ and the value vector corresponding to the reset symbol is independent of the preceding inputs which it is supposed to negate (reset). The same limitation does not hold for multilayer networks where the value vector, as well as the scoring function for the reset symbol, are dependent on its preceding inputs. On evaluating the model on data generated from such a language, we found that single-layer networks are unable to perform well in contrast to networks with two layers (Table 2) ${ }^{7</p>
<h2>7 Results on Regular Languages</h2>
<p>We first examine the popular benchmark of Tomita grammars. While the LSTMs generalize perfectly on all 7 languages, Transformers are unable to generalize on 3 languages, all of which are non-starfree. Note that, all star-free languages in Tomita grammar have dot-depth 1. Recognizing non-starfree languages requires modeling properties such as periodicity and modular counting. Consequently, we evaluate the model on some of the simplest non-star-free languages such as the languages $(a a)^{<em>}$ and Parity. We find that they consistently fail to learn or generalize on such languages, whereas LSTMs of very small sizes perform flawlessly. Table 4 lists the performance on some non-star-free languages. Note that LSTMs can easily recognize such simple non-star-free languages considered here by using its internal memory and recurrence ${ }^{8}$. However, doing the same task via self-attention mechanism without using any internal memory could be highly non-trivial and potentially impossible. Languages such as $(a a)^{</em>}$ and Parity are among the simplest</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Transformer</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LSTM</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Language</td>
<td style="text-align: center;">Property</td>
<td style="text-align: center;">Bin 0</td>
<td style="text-align: center;">Bin 1</td>
<td style="text-align: center;">Bin 0</td>
<td style="text-align: center;">Bin 1</td>
</tr>
<tr>
<td style="text-align: center;">Parity</td>
<td style="text-align: center;">non-SF</td>
<td style="text-align: center;">68.7 (23.0)</td>
<td style="text-align: center;">0.0 (0.0)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">$(a a)^{*}$</td>
<td style="text-align: center;">non-SF</td>
<td style="text-align: center;">100 (1.3)</td>
<td style="text-align: center;">0.0 (0.0)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">$(a b a b)^{*}$</td>
<td style="text-align: center;">non-SF</td>
<td style="text-align: center;">100.0 (9.9)</td>
<td style="text-align: center;">5.4 (0.0)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{D}_{1}$</td>
<td style="text-align: center;">depth-1</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{D}_{2}$</td>
<td style="text-align: center;">depth-2</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{D}_{3}$</td>
<td style="text-align: center;">depth -4</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
</tbody>
</table>
<p>Table 4: Results on non-star-free languages (non-SF) and the language $\mathcal{D}_{n}$. The values in parenthesis correspond to the scores obtained for a model without residual connections. This is to prevent the model from solving the task by memorizing the positional encodings and study the ability of self-attention mechanism to solve the task.
non-star-free languages, and hence limitations in recognizing such languages carry over to a larger class of languages. The results above may suggest that the star-free languages are precisely the regular languages recognizable by Transformers. As we will see in the next section, this is not so.</p>
<h3>7.1 Necessity of Positional Encodings</h3>
<p>The architecture of Transformer imposes limitations for recognizing certain types of languages. Although Transformers seem to generalize well when they are capable of performing a task with only positional masking, they are incapable of recognizing certain types of languages without explicit positional encodings. We consider the family of star-free languages $\mathcal{D}<em n="n">{n}$ defined in Sec. 3.1. Note that the task of recognizing $\mathcal{D}</em>}$ is equivalent to recognizing Dyck-1 with maximum depth $n$, where the symbols $a$ and $b$ in $\mathcal{D<em n="n">{n}$ are analogous to open and closing brackets in Dyck-1 respectively. The primary difference between recognizing $\mathcal{D}</em>}$ and Dyck-1 is that in case of $\mathcal{D<em n="n">{n}$, when the input reaches the maximum depth $n$, the model must predict $a$ (the open bracket) as invalid for the next character, whereas in Dyck-1, open brackets are always allowed. We show that although Transformers with only positional masking can generalize well on Dyck-1, they are incapable of recognizing the language $\mathcal{D}</em>}$ for $n&gt;1$. The limitation arises from the fact that when the model receives a sequence of only $a$ 's, then due to the softmax based aggregation, the output of the self-attention block $\boldsymbol{a<em 1="1">{i}$ will be a constant vector, implying that the output of the feed-forward will also be a constant vector, that is, $\boldsymbol{z}</em>}=\boldsymbol{z<em n="n">{2}=\ldots=\boldsymbol{z}</em>$, if the input begins with $n$ consecutive $a s$, then, since the model cannot distinguish between}$. In case of languages such as $\mathcal{D}_{n</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">$(a a)^{*}$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$(a a a a)^{*}$</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Encoding Scheme</td>
<td style="text-align: center;">Bin 0</td>
<td style="text-align: center;">Bin 1</td>
<td style="text-align: center;">Bin 0</td>
<td style="text-align: center;">Bin 1</td>
</tr>
<tr>
<td style="text-align: center;">Positional Masking</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">Absolute Encoding</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">6.7</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">Relative Encoding</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">$\cos (n \pi)$</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">Trainable Embedding</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">0.0</td>
</tr>
</tbody>
</table>
<p>Table 5: Performance of transformer based models on $(a a)^{<em>}$ and $(a a a a)^{</em>}$, for different types of position encoding schemes. To separately study the effect of different position encodings on the self attention mechanism, we do not include residual connections in the models studied here.
the $n$-th $a$ and the preceding $a$ 's, the model cannot recognize the language $\mathcal{D}<em n="n">{n}$. This limitation does not exist if the model is provided explicit positional encoding. Upon evaluating Transformers with positional encodings on instances of the language $\mathcal{D}</em>$, we found that the models are able to generalize to a certain extent on strings within the same lengths as seen during training but fail to generalize on higher lengths (Table 4). It is perhaps surprising that small and simpler self-attention networks can generalize very well on languages such as Dyck-1 but achieve limited performance on a language that belongs to a much simpler class such as star-free.</p>
<p>Similarly, since $(a a)^{<em>}$, is a unary language (alphabet size is 1 ), the model will always receive the same character at each step. Hence, for a model with only positional masking, the output vector will be the same at every step, making it incapable of recognizing the language $(a a)^{</em>}$. For the language Parity, when the input word contains only 1 's, the task reduces to recognizing $(11)^{*}$ and hence a model without positional encodings is incapable of recognizing Parity even for very small lengths regardless of the size of the network (refer to Lemma C.4). We find it surprising that for Parity, which is permutation invariant, positional encodings are necessary for transformers to recognize them even for very small lengths.</p>
<h3>7.2 Influence of Custom Positional Encodings</h3>
<p>The capability and complexity of the network could significantly depend on the positional encoding scheme. For instance, for language $(a a)^{*}$, the ability of a self-attention network to recognize it depends solely on the positional encoding. Upon evaluating with standard absolute and relative encoding schemes, we observe that the model is unable to learn or generalize well. At the same time, it is easy to show that if $\cos (n \pi)$, which has a period of two</p>
<p>is used as positional encoding, the self-attention mechanism can easily achieve the task which we also observe when we empirically evaluated with such an encoding. However, the same encoding would not work for a language such as $(a a a a)^{*}$, which has a periodicity of four. Table 5 shows the performance of the model with different types of encodings. When we used fixed-length trainable positional embeddings, the obtained learned embeddings were very similar to the $\cos (n \pi)$ form; however, such embeddings cannot be used for sequences of higher lengths. This also raises the need for better learnable encodings schemes that can extrapolate to variable lengths of inputs not seen during training data such as (Liu et al., 2020).</p>
<p>Our experiments on over 15 regular languages seem to indicate that Transformers are able to generalize on star-free languages within dot-depth 1 but have difficulty with higher dot-depths or more complex classes like non-star-free languages. Table 9 in Appendix lists results on all considered regular languages.</p>
<h2>8 Discussion</h2>
<p>We showed that Transformers can easily generalize on certain counter languages such as Shuffle-Dyck and Boolean Expressions in a manner similar to our proposed construction. Our visualizations imply that Transformers do so with a generalizable mechanism instead of overfitting on some statistical regularities. Similar to natural languages, Boolean Expressions consist of recursively nested hierarchical constituents. Recently, Papadimitriou and Jurafsky (2020) showed that pretraining LSTMs on formal languages like Shuffle-Dyck transfers to LM performance on natural languages. At the same time, our results show clear limitations of Transformers compared to LSTMs on a large class of regular languages. Evidently, the performance and capabilities of Transformers heavily depend on architectural constituents e.g., the positional encoding schemes and the number of layers. Recurrent models have a more automata-like structure wellsuited for counter and regular languages, whereas self-attention networks' structure is very different, which seems to limit their abilities for the considered tasks.</p>
<p>Our work poses a number of open questions. Our results are consistent with the hypothesis that Transformers generalize well for star-free languages with dot-depth 1, but not for higher depths. Clarifying
this hypothesis theoretically and empirically is an attractive challenge. What does the disparity between the performance of Transformers on natural and formal languages indicate about the complexity of natural languages and their relation to linguistic analysis? (See also Hahn (2020)). Another interesting direction would be to understand whether certain modifications or recently proposed variants of Transformers improve their performance on formal languages. Regular and counter languages model some aspects of natural language while contextfree languages model other aspects such as hierarchical dependencies. Although our results have some implications on them, we leave a detailed study on context-free languages for future work.</p>
<h2>Acknowledgements</h2>
<p>We thank the anonymous reviewers for their constructive comments and suggestions. We would also like to thank our colleagues at Microsoft Research and Michael Hahn for their valuable feedback and helpful discussions.</p>
<h2>References</h2>
<p>Satwik Bhattamishra, Arkil Patel, and Navin Goyal. 2020. On the computational power of transformers and its implications in sequence modeling. arXiv preprint arXiv:2006.09286.</p>
<p>Rina Cohen and Janusz Brzozowski. 1971. Dot-depth of star-free events. Journal of Computer and System Sciences, 5:1-16.</p>
<p>Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. 2019. Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2978-2988, Florence, Italy. Association for Computational Linguistics.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Volker Diekert and Paul Gastin. 2008. First-order definable languages. In Logic and Automata: History and Perspectives [in Honor of Wolfgang Thomas], volume 2 of Texts in Logic and Games, pages 261306. Amsterdam University Press.</p>
<p>Patrick C Fischer, Albert R Meyer, and Arnold L Rosenberg. 1968. Counter machines and counter languages. Mathematical systems theory, 2(3):265283.</p>
<p>Felix A Gers and E Schmidhuber. 2001. Lstm recurrent networks learn simple context-free and contextsensitive languages. IEEE Transactions on Neural Networks, 12(6):1333-1340.</p>
<p>Michael Hahn. 2020. Theoretical limitations of selfattention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156-171.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735-1780.</p>
<p>Gerhard Jäger and James Rogers. 2012. Formal language theory: refining the chomsky hierarchy. Philosophical Transactions of the Royal Society B: Biological Sciences, 367(1598):1956-1970.</p>
<p>John F Kolen and Stefan C Kremer. 2001. A field guide to dynamical recurrent networks. John Wiley \&amp; Sons.</p>
<p>Samuel A Korsky and Robert C Berwick. 2019. On the computational power of rnns. arXiv preprint arXiv:1906.06349.</p>
<p>Xuanqing Liu, Hsiang-Fu Yu, Inderjit Dhillon, and Cho-Jui Hsieh. 2020. Learning to encode position for transformer with continuous dynamical model. arXiv preprint arXiv:2003.09229.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Robert McNaughton and Seymour A. Papert. 1971. Counter-Free Automata (M.I.T. Research Monograph No. 65). The MIT Press.</p>
<p>William Merrill. 2019. Sequential neural networks as automata. In Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges, pages 1-13, Florence. Association for Computational Linguistics.</p>
<p>William Merrill. 2020. On the linguistic capacity of real-time counter automata. arXiv preprint arXiv:2004.06866.</p>
<p>William Merrill, Gail Weiss, Yoav Goldberg, Roy Schwartz, Noah A Smith, and Eran Yahav. 2020. A formal hierarchy of rnn architectures. arXiv preprint arXiv:2004.08500.</p>
<p>Joshua J. Michalenko, Ameesh Shah, Abhinav Verma, Swarat Chaudhuri, and Ankit B. Patel. 2019. Finite automata can be linearly decoded from languagerecognizing RNNs. In International Conference on Learning Representations.</p>
<p>Isabel Papadimitriou and Dan Jurafsky. 2020. Pretraining on non-linguistic structure as a tool for analyzing learning bias in language models. arXiv preprint arXiv:2004.14601.</p>
<p>Jorge Pérez, Javier Marinković, and Pablo Barceló. 2019. On the turing completeness of modern neural network architectures. In International Conference on Learning Representations.</p>
<p>Jean-ric Pin. 2017. The dot-depth hierarchy, 45 years later. In The Role of Theory in Computer Science, pages 177-201.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openaiassets/researchcovers/languageunsupervised/language understanding paper. pdf.</p>
<p>Emily Reif, Ann Yuan, Martin Wattenberg, Fernanda B Viegas, Andy Coenen, Adam Pearce, and Been Kim. 2019. Visualizing and measuring the geometry of bert. In Advances in Neural Information Processing Systems, pages 8592-8600.</p>
<p>Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020. A primer in bertology: What we know about how bert works. arXiv preprint arXiv:2002.12327.</p>
<p>Luzi Sennhauser and Robert Berwick. 2018. Evaluating the ability of LSTMs to learn context-free grammars. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 115-124, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Shirui Pan, and C. Zhang. 2018. Disan: Directional self-attention network for rnn/cnn-free language understanding. In AAAI.</p>
<p>Natalia Skachkova, Thomas Trost, and Dietrich Klakow. 2018. Closing brackets with recurrent neural networks. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 232-239, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Howard Straubing. 1994. Finite Automata, Formal Logic, and Circuit Complexity. Birkhauser Verlag, CHE.</p>
<p>Mirac Suzgun, Yonatan Belinkov, Stuart Shieber, and Sebastian Gehrmann. 2019a. LSTM networks can perform dynamic counting. In Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges, pages 44-54, Florence. Association for Computational Linguistics.</p>
<p>Mirac Suzgun, Yonatan Belinkov, and Stuart M. Shieber. 2019b. On evaluating the generalization of LSTM models in formal languages. In Proceedings of the Society for Computation in Linguistics (SCiL) 2019, pages 277-286.</p>
<p>Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. 2019. Transformer dissection: An unified understanding for transformer's attention via the lens of kernel. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4344-4353, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008.</p>
<p>Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. 2019. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5797-5808, Florence, Italy. Association for Computational Linguistics.</p>
<p>Qinglong Wang, Kaixuan Zhang, Alexander G. Ororbia II, Xinyu Xing, Xue Liu, and C. Lee Giles. 2018a. A comparative study of rule extraction for recurrent neural networks. CoRR, abs/1801.05420v2.</p>
<p>Qinglong Wang, Kaixuan Zhang, II Ororbia, G Alexander, Xinyu Xing, Xue Liu, and C Lee Giles. 2018b. A comparative study of rule extraction for recurrent neural networks. arXiv preprint arXiv:1801.05420.</p>
<p>Alex Warstadt, Yu Cao, Ioana Grosu, Wei Peng, Hagen Blix, Yining Nie, Anna Alsop, Shikha Bordia, Haokun Liu, Alicia Parrish, Sheng-Fu Wang, Jason Phang, Anhad Mohananey, Phu Mon Hiut, Paloma Jeretic, and Samuel R. Bowman. 2019. Investigating BERT's knowledge of language: Five analysis methods with NPIs. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2877-2887, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Gail Weiss, Yoav Goldberg, and Eran Yahav. 2018. On the practical computational power of finite precision RNNs for language recognition. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 740-745, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Gail Weiss, Yoav Goldberg, and Eran Yahav. 2019. Learning deterministic weighted automata with queries and counterexamples. In H. Wallach, H. Larochelle, A. Beygelzimer, F. AlcheBuc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8560-8571. Curran Associates, Inc.</p>
<p>Baosong Yang, Longyue Wang, Derek F. Wong, Lidia S. Chao, and Zhaopeng Tu. 2019. Assessing
the ability of self-attention networks to learn word order. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3635-3644, Florence, Italy. Association for Computational Linguistics.</p>
<p>Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. 2020. Are transformers universal approximators of sequence-to-sequence functions? In International Conference on Learning Representations.</p>
<h2>A Roadmap</h2>
<p>The appendix is organized as follows. In section B we first provide formal definitions of the key languages used in our investigation in the main paper. In sections B. 1 and B.2, we also provide the formal definitions of automata, star-free languages and the dot-depth hierarchy. In section C, we provide the details of all our expressiveness results. Section D contains the details of our experimental setup which could be relevant for reproducibility of the results and includes a thorough discussion of the choice of character prediction task. The list of all the formal languages we have considered, their dataset statistics as well as the results are provided in section D.</p>
<h2>B Definitions</h2>
<p>In this section, we provide formal definitions of some of the languages used in our analysis. In counter languages, we first define the family of shuffled Dyck-1 languages. The language Dyck-1 is a simple context-free language that can also be recognized by a counter automaton with a single counter. We generate the data for Dyck-1 based on the following PCFG,</p>
<p>$$
S \rightarrow \begin{cases}(S) &amp; \text { with probability } p \ S S &amp; \text { with probability } q \ \varepsilon &amp; \text { with probability } 1-(p+q)\end{cases}
$$</p>
<p>where $0&lt;p, q&lt;1$ and $(p+q)&lt;1$. We use 0.5 as the value of $p$ and 0.25 as the value for $q$.
Shuffle-Dyck. We now define the Shuffle-Dyck language introduced and described in (Suzgun et al., 2019a). We first define the shuffling operation formally. The shuffling operation $|$ : $\Sigma^{<em>} \times \Sigma^{</em>} \rightarrow \mathcal{P}\left(\Sigma^{*}\right)$ can be inductively defined as follows: ${ }^{9}$</p>
<ul>
<li>$u | \varepsilon=\varepsilon | u={u}$</li>
<li>$\alpha u | \beta v=\alpha(u | \beta v) \cup \beta(\alpha u | v)$
for any $\alpha, \beta \in \Sigma$ and $u, v \in \Sigma^{*}$. For instance, the shuffle of $a b$ and $c d$ is</li>
</ul>
<p>$$
a b | c d={a b c d, a c b d, a c d b, c a d d, c a d b, c d a b}
$$</p>
<p>There is a natural extension of the shuffling operation $|$ to languages. The shuffle of two languages</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>$\mathcal{L}<em 2="2">{1}$ and $\mathcal{L}</em>}$, denoted $\mathcal{L<em 2="2">{1} | \mathcal{L}</em>}$, is the set of all possible interleavings of the elements of $\mathcal{L<em 2="2">{1}$ and $\mathcal{L}</em>$, respectively, that is:</p>
<p>$$
\mathcal{L}<em 2="2">{1} | \mathcal{L}</em>}=\bigcup_{u \in \mathcal{L<em 2="2">{1}, v \in \mathcal{L}</em> u | v
$$}</p>
<p>Given a language $\mathcal{L}$, we define its self-shuffling $\mathcal{L} |^{2}$ to be $\mathcal{L} | \sigma(\mathcal{L})$, where $\sigma$ is an isomorphism on the vocabulary of $\mathcal{L}$ to a disjoint vocabulary. More generally, we define the $k$-self-shuffle</p>
<p>$$
\mathcal{L} |^{k}= \begin{cases}{\varepsilon} &amp; \text { if } k=0 \ \mathcal{L}|\sigma(\mathcal{L}|^{k-1}) &amp; \text { otherwise }\end{cases}
$$</p>
<p>We use Shuffle- $k$ to denote the shuffle of $k$ Dyck1 languages (Dyck-1 $\left.|^{k}\right)$ each with its own brackets. Shuffle-1 is the same as Dyck-1. For instance the language Shuffle-2 is the shuffle of Dyck-1 over alphabet $\Sigma={(),)}$ and another Dyck-1 over the alphabet $\Sigma={],}$. Hence the resulting Shuffle-2 language is defined over alphabet $\Sigma={],,(),)}$ and contains words such as ( $]$ ) and $[(())$ ) but not $])[$ (. This is different from the context-free language Dyck-2 in which ( $]$ ) belongs to the language but ( $]$ )] does not. Similar to (Suzgun et al., 2019a) we generate the training data by generating sequence for Dyck- $n$ but by providing the correct target values for the character prediction task.
$n$-ary Boolean Expressions. We now define the family of languages $n$-ary Boolean Expressions parameterized by the number and arities of its operators. An instance of the language contains operators of different arities and as shown in (Fischer et al., 1968), these languages can be recognized by counter-machines with a single counter. However as opposed to Dyck-1 the values with which the counters will be incremented or decremented will depend on the arity of its operator. A language with $n$ operators can be defined by the following derivation rules</p>
<div class="codehilite"><pre><span></span><code><span class="o">&lt;</span><span class="nf">exp</span><span class="o">&gt;</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="o">&lt;</span><span class="n">VALUE</span><span class="o">&gt;</span>
<span class="o">&lt;</span><span class="nf">exp</span><span class="o">&gt;</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="o">&lt;</span><span class="n">UNARY</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&lt;</span><span class="nf">exp</span><span class="o">&gt;</span>
<span class="o">&lt;</span><span class="nf">exp</span><span class="o">&gt;</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="o">&lt;</span><span class="n">BINARY</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&lt;</span><span class="nf">exp</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&lt;</span><span class="nf">exp</span><span class="o">&gt;</span>
<span class="p">...</span>
<span class="o">&lt;</span><span class="nf">exp</span><span class="o">&gt;</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="o">&lt;</span><span class="n">n</span><span class="o">-</span><span class="n">ARY</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&lt;</span><span class="nf">exp</span><span class="o">&gt;</span><span class="w"> </span><span class="p">.</span><span class="w"> </span><span class="p">.</span><span class="w"> </span><span class="o">&lt;</span><span class="nf">exp</span><span class="o">&gt;</span>
</code></pre></div>

<p>Tomita Grammars Tomita Grammars are 7 regular langauges defined on the alphabet $\Sigma={0,1}$. Tomita-1 has the regular expression $1^{<em>}$ i.e. the strings containing only 1's and no 0 s are allowed. Tomita-2 is defined by the regular expression $(10)^{</em>}$. Tomita-3 accepts the strings where odd number</p>
<p>of consecutive 1 s are always followed by an even number of 0 s . Tomita-4 accepts the strings that do not contain 3 consecutive 0 s . In Tomita-5 only the strings containing an even number of 0 s and even number of 1 s are allowed. In Tomita-6 the difference in the number of 1 s and 0 s should be divisible by 3 and finally, Tomita-7 has the regular expression $0^{<em>} 1^{</em>} 0^{<em>} 1^{</em>}$.</p>
<p>We note that Tomita $2=\mathcal{D}_{1}=(01)^{<em>}$ and that the very simple language ${0,1,2}^{</em>} 02^{*}$ has dotdepth 2 (Cohen and Brzozowski, 1971).</p>
<h2>B. 1 Counter Automata</h2>
<p>We define the general counter machine following (Fischer et al., 1968). We are concerned with realtime counter machines here in which the number of computation steps is bounded by the number of inputs similar to how we use sequence models in practice. The machine has a finite number of unbounded counters and it modifies it by adding or subtracting values or resetting the counter value to 0 . For $m \in \mathbb{Z}$, let $+m$ denote the function $x \mapsto x+m$. Let $\times 0$ denote the constant zero function $x \mapsto 0$.</p>
<p>Definition B. 1 (General counter machine (Fischer et al., 1968)). A $k$-counter machine is a tuple $\left\langle\Sigma, Q, q_{0}, u, \delta, F\right\rangle$ with</p>
<ol>
<li>A finite alphabet $\Sigma$</li>
<li>A finite set of states $Q$</li>
<li>An initial state $q_{0}$</li>
<li>A counter update function</li>
</ol>
<p>$$
\begin{array}{r}
u: \Sigma \times Q \times{0,1}^{k} \rightarrow \
({+m: m \in \mathbb{Z}} \cup{\times 0})^{k}
\end{array}
$$</p>
<ol>
<li>A state transition function</li>
</ol>
<p>$$
\delta: \Sigma \times Q \times{0,1}^{k} \rightarrow Q
$$</p>
<ol>
<li>An acceptance mask</li>
</ol>
<p>$$
F \subseteq Q \times{0,1}^{k}
$$</p>
<p>A machine processes an input string $x$ one token at a time. For each token, we use $u$ to update the counters and $\delta$ to update the state according to the current input token, the current state, and a finite mask of the current counter values.</p>
<p>For a vector $\boldsymbol{v}$, let $z(\boldsymbol{v})$ denote the broadcasted "zero-check" function, i.e. $z(\boldsymbol{v})<em i="i">{i}$ is 0 if $v</em> \in \Sigma$, we define the transition}=0$ or 1 otherwise. Let $\langle q, \boldsymbol{c}\rangle \in Q \times \mathbb{Z}^{k}$ be a configuration of machine $M$. Upon reading input $x_{t</p>
<p>$$
\langle q, \boldsymbol{c}\rangle \rightarrow_{x_{t}}\left\langle\delta\left(x_{t}, q, z(\boldsymbol{c})\right), u\left(x_{t}, q, z(\boldsymbol{c})\right)(\boldsymbol{c})\right\rangle
$$</p>
<p>For any string $x \in \Sigma^{*}$ with length $n$, a counter machine accepts $x$ if there exist states $q_{1}, . ., q_{n}$ and counter configurations $\boldsymbol{c}<em n="n">{1}, . ., \boldsymbol{c}</em>$ such that</p>
<p>$$
\left\langle q_{0}, \mathbf{0}\right\rangle \rightarrow_{x_{1}}\left\langle q_{1}, \boldsymbol{c}<em x__2="x_{2">{1}\right\rangle \rightarrow</em>\right\rangle \in F
$$}} . . \rightarrow_{x_{n}}\left\langle q_{n}, \boldsymbol{c}_{n</p>
<p>A counter machines accepts a language $L$ if, for each $x \in \Sigma^{*}$, it accepts $x$ iff $x \in L$. Refer to (Merrill, 2020) for more details on counter machines, variants and their properties.</p>
<h2>B. 2 Star-free regular languages and the dot-depth hierarchy</h2>
<p>Star-free regular languages (defined in the main paper) are a simpler subclass of regular languages; they have regular expressions without Kleene star (but use set complementation). The set of star-free languages is further stratified by the dot-depth hierarchy, which is a hierarchy of families of languages whose union is the family of star-free languages. Informally, the position of a language in this hierarchy is a measure of the number of nested concatenations or sequentiality required to express the language in a star-free regular expression. Both the star-free regular languages as well as the dot-depth hierarchy are well-studied with rich connections and multiple (equivalent) definitions. For more information, see e.g. (McNaughton and Papert, 1971; Cohen and Brzozowski, 1971; Straubing, 1994; Diekert and Gastin, 2008; Jäger and Rogers, 2012; Pin, 2017).</p>
<p>To define the dot-depth hierarchy, we first define Boolean and concatenation closures of language families. For a language family $\mathcal{L}$ over a finite alphabet $\Sigma=\left{a_{1}, \ldots, a_{k}\right}$, its Boolean closure $\mathcal{B} \mathcal{L}$ is the set of languages obtained by applying Boolean operators (union, intersection and set complementation w.r.t. $\Sigma^{*}$ ) to the languages in $\mathcal{L}$. In other words, $\mathcal{B} \mathcal{L}$ is the smallest family of languages containing $\mathcal{L}$ and closed under Boolean operations: if $L_{1}, L_{2} \in \mathcal{L}$ then $L_{1} \cap L_{2} \in \mathcal{B} \mathcal{L}$ and $L_{1} \cup L_{2} \in \mathcal{B} \mathcal{L}$ and $L_{1}^{c}, L_{2}^{c} \in \mathcal{B} \mathcal{L}$. Similarly, define the concatenation closure of $\mathcal{L}$ as the smallest family of languages containing $\mathcal{L}$ and closed under concatenation: if $L_{1}, L_{2} \in \mathcal{L}$ then $L_{1} L_{2} \in \mathcal{M} \mathcal{L}$.</p>
<p>We begin with the class $\mathcal{E}$ of basic languages consisting of $\left{a_{1}\right}, \ldots\left{a_{k}\right},{\epsilon}, \emptyset$. By alternately applying the operators $\mathcal{B}$ and $\mathcal{M}$ to $\mathcal{E}$ we can define the hierarchy</p>
<p>$$
\mathcal{E} \subseteq \mathcal{M} \mathcal{E} \subseteq \mathcal{B} \mathcal{M} \mathcal{E} \subseteq \mathcal{M} \mathcal{B} \mathcal{M} \mathcal{E} \subseteq \ldots
$$</p>
<p>Let $\mathcal{B}<em 0="0">{0}=\mathcal{B} \mathcal{M} \mathcal{E}$. The dot-depth hierarchy is the sequence of families of languages $\mathcal{B}</em>} \subseteq \mathcal{B<em n_1="n+1">{1} \subseteq \ldots$ defined inductively by $\mathcal{B}</em>}=\mathcal{B} \mathcal{M} \mathcal{B<em 0="0">{n}$ for $n \geq 0$. It is known that all the inclusions in $\mathcal{B}</em>} \subseteq \mathcal{B<em n="n">{1} \subseteq \ldots$ are strict and is exemplified by the languages $\mathcal{D}</em>$ first, but these have only minor effects on the overall concept and results.}$ (see Pin (2017)). Minor variations in the definition exist in the literature; in particular, we could have applied the operator $\mathcal{B</p>
<h2>C Expressiveness Results</h2>
<p>We define a weaker version of counter automata which are restricted in a certain sense. Then, we show that Transformers are at least as powerful as such automata.</p>
<p>Definition C. 1 (Simplified and Stateless counter machine). We define a counter machine to be simplified and stateless if $u$ and $\delta$ have the following form,</p>
<p>$$
\begin{gathered}
u: \Sigma \rightarrow{+m: m \in \mathbb{Z}}^{k} \
\delta: \Sigma \rightarrow Q
\end{gathered}
$$</p>
<p>This implies that the machine can have $k$ counters. The counters can be incremented or decremented by any values but it will only depend on the input symbol. Similarly, the state transition will also depend on the current input. A string $x \in \Sigma^{<em>}$ will be accepted if $\left\langle q_{n}, z\left(\boldsymbol{c}<em C="C" L="L" R="R">{n}\right)\right\rangle \in F$. We use $L</em>$ to denote the class of languages recognized by such a counter machine. The above language is similar to $\Sigma$-restricted counter machine defined in (Merrill et al., 2020).
Lemma C.1. Transformers can recognize $L_{R C L}$.
Proof. Let $s_{1}, s_{2}, \ldots, s_{n}$ denote a sequence $w \in$ $\Sigma^{</em>}$. If the counter machine has $k$ counters, then let the dimension of intermediate vectors $d_{\text {model }}=$ $2 k+|\Sigma|$. The first $2 k$ dimensions will be reserved for counter related operations and then $|Q|$ dimensions will be reserved to obtain the state vector. The embedding vector $\boldsymbol{x}<em 2="2" j:="j:" j_1="j+1">{i}$ of each symbol will have 0 s in the first $2 k$ dimensions and the last $|\Sigma|$ dimensions will have the one-hot encoding representation of the symbol. For a $k$ counter machine the value
vectors would have a subvector of dimension 2 reserved for computations pertaining to each of the counter. That is, $\boldsymbol{x}</em>}$ will be reserved for the $j$ th counter where $0 \leq j&lt;k$. For any given input symbol $s$, if $u(s)$ has counter operation of $+m$ at the $j$ th counter, then the value will be such that $\boldsymbol{v}$ will contain $+m$ at index $2 j$ and $-m$ at index $2 j+1$ upto index $2 k$. The last $|\Sigma|$ dimensions will have the value 0 in the value vectors. This can be easily obtained by a linear transformation $V($.$) over one-hot encodings. The linear transfor-$ m tion K($.$) to obtain the key vectors will lead to$ zero vectors and hence all inputs will have equal attention weights. The linear transformation $V($.$) to obtain the value vectors \boldsymbol{v<em i="i">{i}$ will be identity function. Hence the output of the self-attention block along with residual connection will be of the form $\boldsymbol{a}</em>}=\frac{1}{i} \sum_{t=1}^{i} \boldsymbol{v<em i="i">{t}+\boldsymbol{x}</em>$.</p>
<p>The last $|\Sigma|$ dimensions of the vector $\boldsymbol{a}<em i="i">{i}$ will have one-hot encoding of the input vector at $i$-th step. The one-hot encoding of the input can be easily mapped to the one-hot encoding for the corresponding state using a simple FFN. Additionally, this will ensure that, at the $i$-th step, the output of the self-attention block $\boldsymbol{a}</em>$. It is easy to implement the zero check function with a simple linear layer over the output vector. The network accepts an input sequence $w$ when the values in the output vector corresponding to each counter and state at the $n$-th correspond to that required for the final state.}$ will have the value $\frac{c_{j}}{i}$ at indices $2 j$, where $c_{j}$ denotes the counter value of the counter automata representing the language. Similarly, the odd indices $2 j+1$ will have the value $-\frac{c_{j}}{i}$. After applying a simple feed-forward network with ReLU activation, we obtain the output vector $\boldsymbol{z}_{i</p>
<p>We next show that $n$-ary Boolean Expressions can be recognized by Transformers with a similar construction.</p>
<p>Lemma C.2. Transformers can recognize n-ary Boolean Expressions.</p>
<p>Proof. Let $L_{m}$ denote a language of type $n$-ary Boolean Expressions with $m$ operators defined over the alphabet $\Sigma$. Consider a single layer Transformer network with $d_{\text {model }}=2$. Let $s_{0}, s_{1}, \ldots, s_{n}$ be sequence $w$ where $w \in \Sigma^{*}$. Let $s_{0}$ be a special start symbol with embedding $f_{e}=[+1,-1]$. The embeddings of each input symbol $s \in \Sigma$ are defined as follows, $f_{e}(s)=$</p>
<p>$[+(r-1),-(r-1)]$ where $r$ denotes the arity of the symbol. The arity of values such as 0 and 1 is taken as 0 . Similar to the previous construction, the key values are null and hence attention weights are uniform leading to $\boldsymbol{a}<em t="1">{i}=\frac{1}{i} \sum</em>}^{i} \boldsymbol{v<em i="i">{t}$. Hence the output of the self-attention block will be $\boldsymbol{a}</em>}=\left[\frac{c_{j}}{i},-\frac{c_{j}}{i}\right]$, where $c_{j}$ denotes the counter value of the automata representing the language. Essentially, for each operator, the value added to the counter is equal to its arity subtracted by 1 . For each value such as 0 and 1 , the counter value is decremented by 1 . We then apply a simple FFN with ReLU activation to obtain the output vector $\boldsymbol{z<em i="i">{i}=\operatorname{ReLU}\left(\mathbf{I} \boldsymbol{a}</em>\right)$.</p>
<p>An input sequence $w$ belongs to the language $L_{m}$ if the second coordinate of the output is zero at every step, that is, $\boldsymbol{z}<em n="n">{i, 2}=0$ for $0 \leq i \leq n$ and $\boldsymbol{z}</em>$.}=\mathbf{0</p>
<p>Let Reset-Dyck-1 be a language defined over alphabet $\Sigma={[,], 1}$, where 1 denotes a symbol that requires a reset operation. Words in Reset-Dyck-1 have the form $\Sigma^{*} 1 v$, where the string $v$ belongs to Dyck-1. So essentially, when the machine encounters the reset symbol 1 , it has to ignore all the previous inputs, reset the counter to 0 and go to start state.
Lemma C.3. A single-layer Transformer with only positional masking cannot recognize the language Reset-Dyck-1.</p>
<p>Proof. The proof is straightforward. Let $s_{1}, s_{2}, \ldots, s_{n}$ be an input sequence $w$. Let $s_{r}$ denote the $r$-th symbol where the reset symbol occurs. It is easy to see that the scoring function $\left\langle\boldsymbol{q}<em r="r">{n}, K\left(\boldsymbol{v}</em>$ will remain the same regardless of the position of the reset symbol and hence by contradiction, it cannot recognize such a language.}\right)\right\rangle$ is independent of the position as well as the inputs before the reset symbol which are relevant for the reset operation. Consider the case where the first half of the input contains a sequence of open and closing brackets such that it does not belong to Dyck-1 and the second half contains a sequence that belongs to Dyck-1. If the reset symbol occurs after the first half of the sequence, then the word belongs to Dyck-1 and if it occurs in the beginning then it does not belong to the language Dyck-1. However, by construction, the output of the model $\boldsymbol{z}_{n</p>
<p>The above limitation does not exist if there is a two layer network. The scoring function as well as
value vector of the reset symbol will be dependent of the inputs that precede it. Hence it is not necessary that a two layer network will not be able to recognize such a language. Indeed, as shown in the main paper, the 2-layer Transformer performs well on Reset-Dyck-1.
Lemma C.4. Transformers with only positional masking cannot recognize the language $(a a)^{<em>}$.
Proof. Let $s_{1}, s_{2}, \ldots, s_{n}$ be an input sequence $w$ where $w \in a^{</em>}$. Since it is a unary language, the input at each step will be the same symbol and hence the embedding as well as query, key and value vectors will be the same. Since all the value vectors are the same, regardless of the attention weights, the output of the self-attention vector $\boldsymbol{a}<em 1="1">{i}$ will be a constant vector at each timestep. This implies that the output vectors $\boldsymbol{z}</em>}=\boldsymbol{z<em n="n">{2}=\ldots=\boldsymbol{z}</em>$.}$. Inductively, it is easy to see that regardless of the number of layers this phenomenon will carry forward and hence the output vector at each timestep will be the same. Thus, the network cannot distinguish output at even steps and odd steps which is necessary to recognize the language $(a a)^{*</p>
<p>For parity, in the case where the input consists of only 1 s , the problem reduces to recognizing $(11)^{*}$. Hence it follows from the above result that a network without positional encoding cannot recognize parity even for minimal lengths.</p>
<h2>D Experiments</h2>
<h2>D. 1 Discussion on Character Prediction Task</h2>
<p>As described in section 5.1, we use character prediction task in our experiments to evaluate the model's ability to recognize a language. In character prediction task the model is only presented with positive samples from a given language and its goal is to predict the next set of valid characters. During inference, the model predicts the next set of legal characters at each step and a prediction is considered to be correct if and only if the model's output at every step is correct. The character prediction task is similar to predicting which of the input characters are allowed to make a transition in a given automaton such that it leads to a non-dead state. If an input character is not among the legal characters, that implies the underlying automaton will transition to a dead state and regardless of the following characters, the input word will never be accepted. When the end-of-sequence symbol is allowed as</p>
<p>one of the next set of legal characters, it implies that the underlying automaton is in the final state and the input can be accepted.</p>
<p>Character prediction and classification. If a model can perform character prediction task perfectly, then it can also perform classification in the following way. For an input sequence $s_{1}, s_{2}, \ldots, s_{n}$, the model receives the sequence $s_{1}, \ldots, s_{i}$ for $1 \leq i \leq n$ at each step $i$ and model predicts the set of valid characters in the $(i+1)^{t h}$ position. If the next character is among the model's predicted set of valid characters at each step $i$ and the end of symbol character is allowed at the $n$-th step, then the word is accepted and if any character is not within the model's predicted set of valid characters, then the word is rejected. One of the primary reason for the choice of character prediction task is that it is arguably more robust than the standard classification task. The metric for character prediction task is relatively stringent and the model is required to model the underlying mechanism as opposed to just one label in standard classification. Note that the null accuracy (accuracy when all the predictions are replaced by a single label) is $50 \%$ if the distribution of labels is balanced (higher otherwise), on the other hand the null accuracy of character prediction task is close to 0 . Additionally, in case of classification, depending on how the positive or negative data are generated, the model may also be biased to predict based on some statistical regularites instead of modeling the actual mechanism. In (Weiss et al., 2019), they find that LSTMs trained to recognize Dyck-1 via classification on randomly sampled data do not learn the correct mechanism and fail on adversarially generated samples. On the other hand, Suzgun et al. (2019a) show that LSTMs trained to recognize Dyck-1 via character prediction task learn to perform the correct mechanism required to do the task.</p>
<p>Character prediction and language modelling. The character prediction task has clear connections with Language modelling. If a model can perform language modelling perfectly, then it can perform character prediction task in the following way. For an input sequence $s_{1}, s_{2}, \ldots, s_{n}$, the model receives the sequence $s_{1}, \ldots, s_{i}$, for $1 \leq i \leq n$ at each step $i$ and predicts a distribution over the vocabulary. Mapping all the characters for which the model assigns a nonzero probability to 1 and mapping to 0 for all characters that are assigned
zero probability will reduce it to character prediction task. However, there are a few issues with using language modelling in our formal language setting. Firstly, as mentioned in (Suzgun et al., 2019a), the task of recognizing a language is not inherently probabilistic. Our goal here is to understand whether a network can or cannot model a particular language. Using language modelling will require us to impose a distribution arbitrarily for the given setting. More importantly, in character prediction task, some signals are explicitly provided. In the case of language modelling, we may just have to rely on the model to pick up those nuanced signals. For instance, in the language $\mathcal{D}<em n="n">{n}$, when the input reaches the maximum depth $n$, in character prediction task it is explicitly provided the target value that $a$ is not allowed anymore whereas in language modelling the model is expected to assign zero probability to $a$ at the maximum depth based on the fact that it will never see a word depth more than $n$ in the training data. This phenomenon has major issues. For instance, when we consider Dyck-1 in practical setting, we can only provide it with limited data which implies there will be a sequence with a maximum finite depth. In this scenario, a language model trained on such data may learn the Dyck-1 language or the language $\mathcal{D}</em>$ with that particular maximum depth. This limitation does not exist in the character prediction task where the signal is explicitly provided during training.</p>
<h2>D. 2 Experimental Details</h2>
<p>We use 4 NVIDIA Tesla P100 GPUs each with 16 GB memory to run our experiments, and train and evaluate our models on about 9 counter languages and 18 regular languages. The important details of all of these languages like the training and test sizes and the lengths of the strings considered, have been summarized in Table 6. In all of our experiments, the first bin always has the same length range as the training set, i.e. if the training set contains strings with lengths in range $[2,50]$, then the strings in the first test bin will also lie in the same range. Width of bin is the difference between upper and lower limits of the string lengths that lie in that bin. All the test bins are taken to be disjoint from each other. Hence, if we have 3 bins with a width of 50 and the training range is $[2,50]$, then the length ranges for the test bins will be $[2,50],[52,100]$ and $[102,150]$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Training Data</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Test Data</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Language</td>
<td style="text-align: center;">Size</td>
<td style="text-align: center;">Length <br> Range</td>
<td style="text-align: center;">Size per <br> Bin</td>
<td style="text-align: center;">Length <br> Range</td>
<td style="text-align: center;">Number <br> of Bins</td>
<td style="text-align: center;">Bin <br> Width</td>
</tr>
<tr>
<td style="text-align: center;">Counter Languages</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Shuffle-2</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">$[2,50]$</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">$[2,150]$</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">Shuffle-4</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">$[2,100]$</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">$[2,200]$</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">Shuffle-6</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">$[2,1000]$</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">$[2,200]$</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">Boolean-3</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">$[2,50]$</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">$[2,150]$</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">Boolean-5</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">$[2,50]$</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">$[2,150]$</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">$a^{n} b^{n}$</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">$[2,100]$</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">$[2,300]$</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">$a^{n} b^{n} c^{n}$</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">$[3,150]$</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">$[3,450]$</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">150</td>
</tr>
<tr>
<td style="text-align: center;">$a^{n} b^{n} c^{n} d^{n}$</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">$[4,200]$</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">$[4,600]$</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">200</td>
</tr>
<tr>
<td style="text-align: center;">Dyck-1</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">$[2,50]$</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">$[2,150]$</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">Regular Languages</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Tomita 1</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">$[2,50]$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">$[2,100]$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">Tomita 4</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">$[2,50]$</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">$[2,100]$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">Tomita 7</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">$[2,50]$</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">$[2,100]$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">Tomita 2</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">$[2,50]$</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">$[2,100]$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">$a a^{<em>} b b^{</em>} c c^{<em>} d d^{</em>} e e^{*}$</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">$[5,200]$</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">$[5,300]$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">${a, b}^{<em>} d{b, c}^{</em>}$</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">$[1,50]$</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">$[1,100]$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">${0,1,2}^{<em>} 02^{</em>}$</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">$[2,50]$</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">$[2,100]$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{D}_{2}$</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">$[2,100]$</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">$[2,200]$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{D}_{3}$</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">$[2,100]$</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">$[2,200]$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{D}_{4}$</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">$[2,100]$</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">$[2,200]$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{D}_{12}$</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">$[2,100]$</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">$[2,200]$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Parity</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">$[2,50]$</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">$[2,100]$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">$(a a)^{*}$</td>
<td style="text-align: center;">250</td>
<td style="text-align: center;">$[2,500]$</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">$[2,600]$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">$(a a a a)^{*}$</td>
<td style="text-align: center;">125</td>
<td style="text-align: center;">$[4,500]$</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">$[4,600]$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">$(a b a b)^{*}$</td>
<td style="text-align: center;">125</td>
<td style="text-align: center;">$[4,500]$</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">$[4,600]$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Tomita 3</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">$[2,50]$</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">$[2,100]$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">Tomita 5</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">$[2,50]$</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">$[2,100]$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">Tomita 6</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">$[2,50]$</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">$[2,100]$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">50</td>
</tr>
</tbody>
</table>
<p>Table 6: Statistics of different datasets used in the experiments. Note that the width of the first bin is always defined by the training set (see D), and hence can be different from the widths of other bins reported in Bin Width column. As an example, for $(a a)^{*}$, the first bin will have a length range of $[2,500]$ and $[502,600]$ for the second bin.</p>
<p>For each of these languages, we extensively tune on a bunch of different architectural and optimization related hyperparameters. Table 7 lists the hyperparameters considered in our experiments and the bounds for each of them. This corresponds to about 162 different configurations for tuning transformers (for a hidden size of 3, 4 heads are not allowed) and 40 configurations for LSTMs . Over all the languages and hyperparameters there were a minimum of 117 parameters and a maximum of 17,888 parameters for the models that we considered. We use a grid search procedure to tune the hyperparameters. While reporting the accuracy scores for a given language, we compute the mean of the top 5 accuracies, corresponding to all hyperparameter configurations. For some experiments we had to consider the hyperparameters lying outside of the values specified in Table 7. As an instance, we considered 4 layer transformers in the cases where the training accuracies obtained
were low for single and two layered networks and reported the results accordingly.</p>
<p>For training our models we used RMSProp optimizer with the smoothing constant $\alpha=0.99$. In our initial few experiments we also tried Stochastic Gradient Descent with learning rate decay and Adam Optimizer, but decided to go ahead with RMSProp as it outperformed SGD in majority of experiments and gave similar performance as Adam but needed fewer hyperparameters. For each language we train models corresponding to each language for 100 epochs and a batch size of 32 . In case of convergence, i.e. perfect accuracies for all the bins, before completion of all epochs, we stop the training process early. The results of our experiments on counter and regular languages are provided in Tables 8 and 9 respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyperparameter</th>
<th style="text-align: center;">Bounds</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Hidden Size</td>
<td style="text-align: center;">$[3,32]$</td>
</tr>
<tr>
<td style="text-align: center;">Heads</td>
<td style="text-align: center;">$[1,4]$</td>
</tr>
<tr>
<td style="text-align: center;">Number of Layers</td>
<td style="text-align: center;">$[1,2]-[1,4]$</td>
</tr>
<tr>
<td style="text-align: center;">Learning Rate</td>
<td style="text-align: center;">$[1 \mathrm{e}-2,1 \mathrm{e}-3]$</td>
</tr>
<tr>
<td style="text-align: center;">Position Encoding</td>
<td style="text-align: center;">[Absolute, Relative, Positional</td>
</tr>
<tr>
<td style="text-align: center;">Scheme</td>
<td style="text-align: center;">Masking]</td>
</tr>
</tbody>
</table>
<p>Table 7: Different hyperparameters and the values considered for each of them. Note that certain parameters like Heads and Position Encoding Scheme are only relevant for Transformer based models and not for LSTMs. We considered upto 4 layers transformers in the cases where the training accuracies obtained were low for single and two layered networks and reported the results accordingly.</p>
<h2>E Plots</h2>
<p>We visualize different aspects of the trained models to understand how they achieve a particular task and if the learned behaviour resembles our constructions. Figure 3 shows the value vectors corresponding to the trained models on Shuffle-2 and Boolean-3 Language. We also visualize the attention weights corresponding to these two models in Figure 4. Similar to the self-attention output visualizations for Shuffle-2 and Boolean-3 in the main paper, we visualize these values for a model trained on Shuffle-4 in Figure 5 and again, find close correlations with the depth to length ratios of different types of brackets in the language. Finally, in Figure 6, we visualize a component of the learned position embeddings vectors and found a similar behaviour to $\cos (n \pi)$ agreeing with our hypothesis.</p>
<p>Value Vectors</p>
<table>
<thead>
<tr>
<th style="text-align: center;">-</th>
<th style="text-align: center;">0.74</th>
<th style="text-align: center;">-2.5</th>
<th style="text-align: center;">-24</th>
<th style="text-align: center;">1.8</th>
<th style="text-align: center;">9</th>
<th style="text-align: center;">-2.8</th>
<th style="text-align: center;">-0.95</th>
<th style="text-align: center;">10</th>
<th style="text-align: center;">-26</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-0.78</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">-24</td>
<td style="text-align: center;">-2.1</td>
<td style="text-align: center;">-9.4</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">-10</td>
<td style="text-align: center;">27</td>
</tr>
<tr>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-21</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">-26</td>
<td style="text-align: center;">-3.3</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">-19</td>
<td style="text-align: center;">4.6</td>
<td style="text-align: center;">1.4</td>
</tr>
<tr>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-22</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">-1.2</td>
<td style="text-align: center;">-26</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">-4.1</td>
<td style="text-align: center;">-26</td>
<td style="text-align: center;">-4.2</td>
<td style="text-align: center;">-1.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">8</td>
</tr>
</tbody>
</table>
<p>(a)</p>
<p>Value Vectors</p>
<table>
<thead>
<tr>
<th style="text-align: center;">-0.1</th>
<th style="text-align: center;">-0.052</th>
<th style="text-align: center;">-0.14</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">-0.31</td>
<td style="text-align: center;">-0.3</td>
<td style="text-align: center;">-0.31</td>
</tr>
<tr>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">-0.15</td>
<td style="text-align: center;">-0.5</td>
</tr>
</tbody>
</table>
<p>(b)</p>
<p>Figure 3: Plot of value vectors of transformer based models trained on Shuffle-2 3a and Boolean-3 language 3b. The Shuffle-2 model had a hidden size of 8 and boolean- 3 model had a hidden size of 3 . The x -axis corresponds to different components of the value vectors for both models. Shuffle-2 language consisted of square and round brackets, while for Boolean-3 we considered 3 operators namely: $\sim$ a unary operator, + a binary operator and finally, $&gt;$ which is a ternary operator..</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Language</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Bin-1 Accuracy $[1,50]^{+}$</th>
<th style="text-align: center;">Bin-2 Accuracy $[51,100]^{+}$</th>
<th style="text-align: center;">Bin-3 Accuracy $[101,150]^{+}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Dyck-1</td>
<td style="text-align: center;">LSTM (Baseline)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Absolute Positional Encodings)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Relative Positional Encodings)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">91.0</td>
<td style="text-align: center;">60.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Only Positional Masking)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">Shuffle-2</td>
<td style="text-align: center;">LSTM (Baseline)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Absolute Positional Encodings)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">85.2</td>
<td style="text-align: center;">63.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Relative Positional Encodings)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">3.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Only Positional Masking)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">93.0</td>
</tr>
<tr>
<td style="text-align: center;">Shuffle-4</td>
<td style="text-align: center;">LSTM (Baseline)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">99.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Absolute Positional Encodings)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">46.6</td>
<td style="text-align: center;">20.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Relative Positional Encodings)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">57.2</td>
<td style="text-align: center;">5.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Only Positional Masking)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">98.8</td>
</tr>
<tr>
<td style="text-align: center;">Shuffle-6</td>
<td style="text-align: center;">LSTM (Baseline)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">99.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Absolute Positional Encodings)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">16.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Relative Positional Encodings)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">59.1</td>
<td style="text-align: center;">5.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Only Positional Masking)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">94.0</td>
</tr>
<tr>
<td style="text-align: center;">Boolean Expressions (3)</td>
<td style="text-align: center;">LSTM (Baseline)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">99.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Absolute Positional Encodings)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;">51.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Relative Positional Encodings)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">96.0</td>
<td style="text-align: center;">68.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Only Positional Masking)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">99.8</td>
</tr>
<tr>
<td style="text-align: center;">Boolean Expressions (5)</td>
<td style="text-align: center;">LSTM (Baseline)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">96.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Absolute Positional Encodings)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">84.3</td>
<td style="text-align: center;">40.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Relative Positional Encodings)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">32.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Only Positional Masking)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">99.0</td>
</tr>
<tr>
<td style="text-align: center;">$\alpha^{m} b^{n}$</td>
<td style="text-align: center;">LSTM (Baseline)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">99.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Absolute Positional Encodings)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Relative Positional Encodings)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">$\alpha^{m} b^{n} c^{n}$</td>
<td style="text-align: center;">Transformer (Only Positional Masking)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Relative Positional Encodings)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">$\alpha^{m} b^{n} c^{n} d^{n}$</td>
<td style="text-align: center;">Transformer (Absolute Positional Encodings)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">62.1</td>
<td style="text-align: center;">5.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Relative Positional Encodings)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">31.3</td>
<td style="text-align: center;">22.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Only Positional Masking)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">$\alpha^{m} b^{n} c^{n} d^{n}$</td>
<td style="text-align: center;">LSTM (Baseline)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">99.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Absolute Positional Encodings)</td>
<td style="text-align: center;">88.45</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Relative Positional Encodings)</td>
<td style="text-align: center;">41.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer (Only Positional Masking)</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">99.4</td>
</tr>
</tbody>
</table>
<p>Table 8: The performance of Transformers and LSTMs on the respective counter languages. Refer to section 6 in the main paper for details.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Attention maps for models trained on Shuffle-2 and Boolean-3 languages. Similar to our constructions for recognizing these languages, we observe nearly uniform attention weights in both cases</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Language</th>
<th style="text-align: center;">Property</th>
<th style="text-align: center;"><code>[x]</code></th>
<th style="text-align: center;">Transformer (Only Positional Masking)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Transformer (w Position Encodings)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LSTM</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Bin 0</td>
<td style="text-align: center;">Bin 1</td>
<td style="text-align: center;">Bin 0</td>
<td style="text-align: center;">Bin 1</td>
<td style="text-align: center;">Bin 0</td>
<td style="text-align: center;">Bin 1</td>
</tr>
<tr>
<td style="text-align: center;">Tomita 1</td>
<td style="text-align: center;">SF</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">Tomita 4</td>
<td style="text-align: center;">SF</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">24.1</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(LT-b)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Tomita 7</td>
<td style="text-align: center;">SF</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">Tomita $2=$</td>
<td style="text-align: center;">SF</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{D}_{1}=(01)^{*}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$a a^{<em>} b b^{</em>} c c^{<em>} d d^{</em>} e e^{*}$</td>
<td style="text-align: center;">SF</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">${a, b}^{<em>} d{b, c}^{</em>}$</td>
<td style="text-align: center;">SF</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">${0,1,2}^{<em>} 02^{</em>}$</td>
<td style="text-align: center;">SF</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{D}_{2}$</td>
<td style="text-align: center;">SF</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{D}_{3}$</td>
<td style="text-align: center;">SF</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">4.2</td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">8.5</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{D}_{4}$</td>
<td style="text-align: center;">SF</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{D}_{12}$</td>
<td style="text-align: center;">SF</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">16.5</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">Parity</td>
<td style="text-align: center;">non-SF</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">$(a a)^{*}$</td>
<td style="text-align: center;">non-SF</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">$(a a a a)^{*}$</td>
<td style="text-align: center;">non-SF</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">$(a b a b)^{*}$</td>
<td style="text-align: center;">non-SF</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">Tomita 3</td>
<td style="text-align: center;">non-SF</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">10.8</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">Tomita 5</td>
<td style="text-align: center;">non-SF</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">Tomita 6</td>
<td style="text-align: center;">non-SF</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">88.8</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
</tbody>
</table>
<p>Table 9: Summary of results on Regular Languages. The languages are arranged in an increasing order of their complexities.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Values of four different coordinates of the output of self-attention block. The model is trained to recognize Shuffle-4. The dotted lines are the scaled depth to length ratio for the four types of bracket provided for reference.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: The values of coordiante 3 of the learned position encodings on the language $(a a)^{*}$. The variation in the encodings resemble a periodic behaviour similar to $\cos (n \pi)$</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{9}$ We abuse notation by allowing a string to stand for the singleton containing that string. $\epsilon$ is the empty string.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{5}$ We take the model with the smallest number of parameters that generalized well making it feasible for us to visualize it.
${ }^{6}$ The Pearson correlation of values were $\sim 0.99$&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
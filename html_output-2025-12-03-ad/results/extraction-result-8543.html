<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8543 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8543</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8543</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-7451a65b0478c12d073a9799102d42f4e2c768a5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7451a65b0478c12d073a9799102d42f4e2c768a5" target="_blank">CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work evaluates four strong VLMs on CAPTURe, finding that models perform worse with occlusion, suggesting that VLMs are also deficient in inferring unseen spatial relationships: even the strongest VLMs like GPT-4o fail to count with occlusion.</p>
                <p><strong>Paper Abstract:</strong> Recognizing and reasoning about occluded (partially or fully hidden) objects is vital to understanding visual scenes, as occlusions frequently occur in real-world environments and act as obstacles for spatial comprehension. To test models'ability to reason about multiple occluded objects, we introduce a novel task, Counting Amodally for Patterns Through Unseen REgions (CAPTURe), which requires a model to count objects arranged in a pattern by inferring how the pattern continues behind an occluder (an object which blocks parts of the scene). CAPTURe requires both recognizing visual patterns and reasoning, making it a useful testbed for evaluating vision-language models (VLMs) on whether they understand occluded patterns and possess spatial understanding skills. By requiring models to reason about occluded objects, CAPTURe also tests VLMs'ability to form world models that would allow them to fill in missing information. CAPTURe consists of two parts: (1) CAPTURe-real, with manually filtered images of real objects in patterns and (2) CAPTURe-synthetic, a controlled diagnostic with generated patterned images. We evaluate four strong VLMs (GPT-4o, Intern-VL2, Molmo, and Qwen2-VL) on CAPTURe, finding that models struggle to count on both occluded and unoccluded patterns. Crucially, we find that models perform worse with occlusion, suggesting that VLMs are also deficient in inferring unseen spatial relationships: even the strongest VLMs like GPT-4o fail to count with occlusion. In contrast, we find that humans achieve very little error on CAPTURe. We also find that providing auxiliary information of occluded object locations increases performance, underscoring that the model error comes both from an inability to handle occlusion as well as difficulty in counting in images. Code and data: https://github.com/atinpothiraj/CAPTURe</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8543.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8543.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial multimodal vision-language model (VLM) from OpenAI used as a strong baseline for visual reasoning; evaluated in this paper by pairing its vision encoder with instructions to perform amodal counting in patterned scenes with occlusion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hello gpt-4o</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal vision-language model (VLM) from OpenAI used to interpret images and generate textual counts; coupled with prompt selection and an external answer extractor to produce single-number outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>CAPTURE (Amodal pattern counting)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Pattern completion / amodal counting: images contain regular arrangements (grids, circles, triangles) of objects with an occluding box; task requires inferring how the pattern continues behind occluder (spatial world modeling + counting).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Model shown an image (occluded or unoccluded) and asked via prompt to count total objects assuming pattern continues behind occluder; free-form text output allowed, answers extracted by a Llama 3.1 8B-based answer extractor; ten prompts validated and best selected per model; sMAPE used as metric.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Prompt engineering (best of 10 prompts), free-form textual reasoning, optional Chain-of-Thought (CoT) experiments (CoT generally reduced performance), temperature backoff (iteratively lowered to reduce incoherent responses), oracle experiments (textual coordinates of visible/all objects), inpainting pipeline (FLUX.1-Fill) used as predicted auxiliary input; no explicit symbolic Sudoku-like solver used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>CAPTURE_real (unoccluded) sMAPE = 13.34%; CAPTURE_real (occluded) sMAPE = 14.75% (+1.41%). CAPTURE_synthetic (unoccluded) sMAPE = 5.90%; CAPTURE_synthetic (occluded) sMAPE = 9.71% (+3.81%). With All Object Coordinate Oracle: sMAPE = 2.93% (−11.82 vs occluded). With Visible Object Coordinate Oracle: sMAPE = 9.20% (−5.55). With Inpainting: sMAPE = 15.89% (+1.14).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Model shows ability to recognize patterns (pattern-classification accuracy high, see Table 4). Performance degrades with occlusion (worse sMAPE), indicating limitations in forming/amodal world models; providing all object coordinates (oracle) dramatically reduces error to near-human levels, implying that when given explicit spatial coordinates GPT-4o can leverage spatial reasoning/arithmetics in text; inpainting provided mixed/insufficient improvement indicating imperfect world modeling from visual input.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared to humans (CAPTURE_real occluded sMAPE 3.79%) GPT-4o is ~3–4x worse on real data; CountGD (vision-only counting model trained on FSC-147) outperforms GPT-4o on unoccluded data and sometimes on occluded but suffers larger relative drop with occlusion; hybrids (CountGD visible-count fed to VLM prompt) reduce VLM error but often still worse than CountGD alone.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails more often when occluded object count increases; produces incoherent/skipped outputs (assigned worst sMAPE); chain-of-thought often reduces performance; inpainting sometimes mis-synthesizes patterns causing worse results; when tasked to count 'only occluded objects' error much higher (occluded-only sMAPE = 26.13%), revealing inability to isolate and infer occluded quantity reliably; bias to predict round/common numbers noted in confusion analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8543.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8543.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Intern-VL2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Intern-VL2-Llama3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source large vision-language model (VLM) architecture combining InternVL visual backbone with an Llama3 language backbone; evaluated on CAPTURE for amodal counting and spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Intern-VL2-Llama3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>VLM that pairs a scaled vision foundation model (Intern-VL2) with an Llama3 language backbone; chosen for strong VLM benchmark performance; used with prompt selection and long output budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>CAPTURE (Amodal pattern counting)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Pattern completion / amodal counting; requires inferring positions/number of occluded objects from visible pattern segments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Same CAPTURE setup: image + textual prompt instructing to count as if occluder not present; best prompt selected from 10; free-form output extracted via answer extractor; sMAPE metric; oracle and inpainting auxiliary experiments conducted.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Prompt selection, optional CoT (found to hurt performance), temperature backoff to reduce incoherent outputs; oracle experiments (visible/all coordinates) and inpainting pipeline used for ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>CAPTURE_real (unoccluded) sMAPE = 26.17%; CAPTURE_real (occluded) sMAPE = 32.90% (+6.73). CAPTURE_synthetic (unoccluded) sMAPE = 16.44%; CAPTURE_synthetic (occluded) sMAPE = 17.57% (+1.13). With All Coordinates Oracle: sMAPE = 17.48% (−15.42). With Visible Oracle: sMAPE = 25.13% (−7.77). With Inpainting: sMAPE = 31.12% (−1.78).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Pattern recognition accuracy (Table 4) is moderate but drops substantially under occlusion for InternVL2; oracle coordinates substantially improve results indicating the language backbone can perform counting when provided explicit spatial data; however high errors on occluded-only counting (sMAPE = 75.82%) show weak ability to infer occluded counts from visual cues alone.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Performs worse than GPT-4o and CountGD on unoccluded data; shows larger deterioration with occlusion relative to some models; hybrid with CountGD reduces error but remains suboptimal compared to CountGD alone.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Very poor at counting only occluded objects (extremely high sMAPE), significant drop in pattern-recognition accuracy under occlusion, CoT does not help, temperature backoff yields only modest gains; indicates weaknesses in visual-to-world-model integration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8543.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8543.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Molmo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Molmo 7B-D</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal model fine-tuned for pointing and grounding (text-to-2D coordinate), enabling direct point-and-count style behavior; evaluated on CAPTURE to test whether grounding aids amodal counting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Molmo 7B-D</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B-parameter multimodal model trained on many examples grounding text to 2D image coordinates; has explicit point-and-count capabilities that could benefit counting tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>CAPTURE (Amodal pattern counting)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Pattern completion / amodal counting requiring inference about occluded pattern continuation and counting of visible+occluded items.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Same CAPTURE prompting framework; Molmo prompts slightly different (first state the pattern then give final count). Molmo excluded from some oracle experiments due to prompt length limits.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Direct pointing/grounding capability used implicitly; prompt asking to state pattern then count; temperature backoff and prompt selection used; excluded from some long-text oracle tests due to prompt truncation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>CAPTURE_real (unoccluded) sMAPE = 25.90%; CAPTURE_real (occluded) sMAPE = 32.49% (+6.59). CAPTURE_synthetic (unoccluded) sMAPE = 8.40%; CAPTURE_synthetic (occluded) sMAPE = 17.73% (+9.33). (Molmo excluded from All/Visible Coordinate oracle tests due to prompt length limits.)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Despite being trained to ground text to coordinates, Molmo still shows substantial error under occlusion; on synthetic unoccluded split it does well (8.40%) but occlusion greatly increases errors, suggesting grounding ability alone does not solve amodal world modeling from partial views.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Molmo does not outperform GPT-4o on occluded CAPTURE_real; on synthetic unoccluded data Molmo is competitive but occlusion causes large relative degradation; temperature backoff sometimes produced greater improvements for Molmo than other models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Prompt-length limit prevents use of long-text oracle coordinate inputs; performs very poorly on occluded-only counting task (occluded-only sMAPE = 96.79%), indicating inability to infer occluded counts despite point-and-count training; biased predictions and overconfidence noted.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8543.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8543.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2-VL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2-VL-7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter VLM focused on high-resolution perception; evaluated on CAPTURE for spatial inference under occlusion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2-VL-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-language model emphasizing perception at variable resolutions; used with prompt selection and long output budgets in CAPTURE evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>CAPTURE (Amodal pattern counting)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Pattern-based amodal counting: requires extrapolating regular spatial patterns behind occluders and counting total objects.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>CAPTURE prompting (count as-if occluder not present), free-form output, answer extraction by external Llama 3.1 8B extractor; CoT and temperature backoff experiments performed; oracles and inpainting ablations applied.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Prompt engineering, CoT experiments (CoT often hurts), temperature backoff to reduce skipped outputs; used inpainting pipeline (FLUX.1-Fill) as predicted auxiliary input which gave modest improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>CAPTURE_real (unoccluded) sMAPE = 18.96%; CAPTURE_real (occluded) sMAPE = 29.33% (+10.37). CAPTURE_synthetic (unoccluded) sMAPE = 6.63%; CAPTURE_synthetic (occluded) sMAPE = 11.74% (+5.11). With All Coordinates Oracle: sMAPE = 9.62% (−19.71). With Visible Oracle: sMAPE = 17.70% (−11.63). With Inpainting: sMAPE = 22.64% (−6.69).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Shows relatively strong pattern recognition (Table 4) and benefits from circular arrangements; inpainting and oracle experiments reduce error, demonstrating partial use of spatial cues but inability to fully form accurate amodal world models from the raw image alone; occluded-only counting sMAPE = 32.89% indicates difficulty isolating occluded counts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Performs worse than GPT-4o on CAPTURE_real occluded; benefits by combining with CountGD oracles and inpainting but still falls short of human performance; oracle coordinate inputs yield large error reductions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Large sMAPE increase with occlusion, overconfidence in some occluded examples, biased predictions to particular numbers; inpainting improves but does not fully close gap to unoccluded performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8543.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8543.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MiniCPM-o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MiniCPM-o 2.6</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A compact multimodal VLM (MiniCPM series) evaluated in CAPTURE to assess small-model spatial reasoning and counting under occlusion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Minicpm-v: A gpt-4v level mllm on your phone.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MiniCPM-o 2.6</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Lightweight multimodal model (approx. 2.6B parameters) designed for efficient multimodal inference; included to assess how smaller VLMs handle amodal counting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2.6B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>CAPTURE (Amodal pattern counting)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Pattern completion + counting under occlusion: requires spatial extrapolation from partial observations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Standard CAPTURE prompts (best-of-10 chosen), free-form outputs extracted by answer-extractor, sMAPE metrics; CoT and temperature backoff explored.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Prompt tuning and temperature backoff to reduce incoherent answers; no special grounding capabilities like Molmo; oracle/inpainting experiments reported in aggregate analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>CAPTURE_real (unoccluded) sMAPE = 23.84%; CAPTURE_real (occluded) sMAPE = 30.08% (+6.24). CAPTURE_synthetic (unoccluded) sMAPE = 17.06%; CAPTURE_synthetic (occluded) sMAPE = 19.00% (+1.94).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Performs worse than larger VLMs; pattern recognition and occlusion handling limited; temperature backoff yields moderate improvements for some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Worse than GPT-4o and some other VLMs on most splits; hybrid with CountGD improves performance but does not reach CountGD unoccluded accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High sMAPE on occluded splits; struggles more when occluded objects increase; susceptible to incoherent outputs that are penalized heavily by sMAPE.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8543.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8543.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Kimi-VL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Kimi-VL-A3B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language model included in the CAPTURE evaluation to expand architectural diversity; used to measure occlusion and counting capability in a mid-sized VLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Kimi-VL technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Kimi-VL-A3B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Kimi-VL family model (technical report cited); included as a representative VLM with its own vision encoder and language backbone; used with the CAPTURE prompt/answer-extractor pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>CAPTURE (Amodal pattern counting)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Pattern-based amodal counting requiring spatial extrapolation across occluded regions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Standard CAPTURE prompting; free-form generation; answer extraction via Llama 3.1 8B; sMAPE evaluation across occluded/unoccluded real and synthetic splits.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Prompt selection, temperature backoff experiments; oracle/inpainting ablations applied in aggregate analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>CAPTURE_real (unoccluded) sMAPE = 23.48%; CAPTURE_real (occluded) sMAPE = 25.96% (+2.48). CAPTURE_synthetic (unoccluded) sMAPE = 16.91%; CAPTURE_synthetic (occluded) sMAPE = 18.07% (+1.16).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Shows modest degradation with occlusion (smaller Δ than some models), suggesting some robustness but still substantial gap to humans and oracle-assisted performance; benefits from oracle coordinates in aggregate analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Performs similarly to other mid-sized VLMs and worse than GPT-4o; hybrid approaches and oracle inputs improve performance but remain below human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Limited ability to infer occluded-only counts; still prone to the same failure modes (incoherent outputs, biases, overconfidence) reported across VLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8543.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8543.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CountGD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CountGD (multi-modal open-world counting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art object-detection-based counting method trained on FSC-147 used as a vision-only baseline and oracle for visible-object counts in CAPTURE evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Countgd: Multi-modal open-world counting.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CountGD</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Object-detection-based counting system (trained on FSC-147) designed to count visible objects from images; does not attempt amodal completion and thus cannot infer occluded objects without additional reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>CAPTURE (Amodal pattern counting) — used as baseline/hybrid component</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Counting visible objects in images (vision-only), not an amodal completion solver; spatially grounded detection/counting but no occlusion inference.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Applied to CAPTURE images (subset from FSC-147 test set where CountGD had training overlap) to count visible objects; outputs provided as auxiliary visible counts to VLMs in hybrid experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Object detection and counting trained on FSC-147; used as oracle of visible counts for hybrids; no textual reasoning component.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>On evaluation subset, CountGD unoccluded sMAPE = 3.15%; CountGD occluded sMAPE = 10.34% (deterioration of 7.19 percentage points due to occlusion). Hybrid (CountGD visible count + VLM prompt) reduced some VLM errors but hybrid often remained worse than CountGD alone.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Strong at counting visible objects (low unoccluded sMAPE) but performance degrades in presence of occlusion because it does not infer hidden objects; its failure modes illustrate the gap between detection-based counting and amodal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Outperforms all tested VLMs on unoccluded CAPTURE images; on occluded images its error increases but it still can outperform some VLMs; combining CountGD visible counts with VLMs improved VLM performance but hybrid still sometimes trailed CountGD alone.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Cannot infer or reason about occluded objects — error scales with number of occluded objects; not a solution for amodal counting on its own.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Spartqa: A textual question answering benchmark for spatial reasoning <em>(Rating: 2)</em></li>
                <li>Unibench: Visual reasoning requires rethinking vision-language beyond scaling <em>(Rating: 2)</em></li>
                <li>Occ-mllm: Empowering multimodal large language model for the understanding of occluded objects <em>(Rating: 2)</em></li>
                <li>Countgd: Multi-modal open-world counting <em>(Rating: 2)</em></li>
                <li>Countnet3d: A 3d computer vision approach to infer counts of occluded objects <em>(Rating: 2)</em></li>
                <li>pix2gestalt: Amodal segmentation by synthesizing wholes <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8543",
    "paper_id": "paper-7451a65b0478c12d073a9799102d42f4e2c768a5",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o",
            "brief_description": "A commercial multimodal vision-language model (VLM) from OpenAI used as a strong baseline for visual reasoning; evaluated in this paper by pairing its vision encoder with instructions to perform amodal counting in patterned scenes with occlusion.",
            "citation_title": "Hello gpt-4o",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Multimodal vision-language model (VLM) from OpenAI used to interpret images and generate textual counts; coupled with prompt selection and an external answer extractor to produce single-number outputs.",
            "model_size": null,
            "puzzle_name": "CAPTURE (Amodal pattern counting)",
            "puzzle_type": "Pattern completion / amodal counting: images contain regular arrangements (grids, circles, triangles) of objects with an occluding box; task requires inferring how the pattern continues behind occluder (spatial world modeling + counting).",
            "task_setup": "Model shown an image (occluded or unoccluded) and asked via prompt to count total objects assuming pattern continues behind occluder; free-form text output allowed, answers extracted by a Llama 3.1 8B-based answer extractor; ten prompts validated and best selected per model; sMAPE used as metric.",
            "mechanisms_or_strategies": "Prompt engineering (best of 10 prompts), free-form textual reasoning, optional Chain-of-Thought (CoT) experiments (CoT generally reduced performance), temperature backoff (iteratively lowered to reduce incoherent responses), oracle experiments (textual coordinates of visible/all objects), inpainting pipeline (FLUX.1-Fill) used as predicted auxiliary input; no explicit symbolic Sudoku-like solver used.",
            "performance_metrics": "CAPTURE_real (unoccluded) sMAPE = 13.34%; CAPTURE_real (occluded) sMAPE = 14.75% (+1.41%). CAPTURE_synthetic (unoccluded) sMAPE = 5.90%; CAPTURE_synthetic (occluded) sMAPE = 9.71% (+3.81%). With All Object Coordinate Oracle: sMAPE = 2.93% (−11.82 vs occluded). With Visible Object Coordinate Oracle: sMAPE = 9.20% (−5.55). With Inpainting: sMAPE = 15.89% (+1.14).",
            "evidence_of_spatial_reasoning": "Model shows ability to recognize patterns (pattern-classification accuracy high, see Table 4). Performance degrades with occlusion (worse sMAPE), indicating limitations in forming/amodal world models; providing all object coordinates (oracle) dramatically reduces error to near-human levels, implying that when given explicit spatial coordinates GPT-4o can leverage spatial reasoning/arithmetics in text; inpainting provided mixed/insufficient improvement indicating imperfect world modeling from visual input.",
            "comparisons": "Compared to humans (CAPTURE_real occluded sMAPE 3.79%) GPT-4o is ~3–4x worse on real data; CountGD (vision-only counting model trained on FSC-147) outperforms GPT-4o on unoccluded data and sometimes on occluded but suffers larger relative drop with occlusion; hybrids (CountGD visible-count fed to VLM prompt) reduce VLM error but often still worse than CountGD alone.",
            "limitations_or_failure_cases": "Fails more often when occluded object count increases; produces incoherent/skipped outputs (assigned worst sMAPE); chain-of-thought often reduces performance; inpainting sometimes mis-synthesizes patterns causing worse results; when tasked to count 'only occluded objects' error much higher (occluded-only sMAPE = 26.13%), revealing inability to isolate and infer occluded quantity reliably; bias to predict round/common numbers noted in confusion analyses.",
            "uuid": "e8543.0",
            "source_info": {
                "paper_title": "CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Intern-VL2",
            "name_full": "Intern-VL2-Llama3-8B",
            "brief_description": "An open-source large vision-language model (VLM) architecture combining InternVL visual backbone with an Llama3 language backbone; evaluated on CAPTURE for amodal counting and spatial reasoning.",
            "citation_title": "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.",
            "mention_or_use": "use",
            "model_name": "Intern-VL2-Llama3-8B",
            "model_description": "VLM that pairs a scaled vision foundation model (Intern-VL2) with an Llama3 language backbone; chosen for strong VLM benchmark performance; used with prompt selection and long output budgets.",
            "model_size": "8B",
            "puzzle_name": "CAPTURE (Amodal pattern counting)",
            "puzzle_type": "Pattern completion / amodal counting; requires inferring positions/number of occluded objects from visible pattern segments.",
            "task_setup": "Same CAPTURE setup: image + textual prompt instructing to count as if occluder not present; best prompt selected from 10; free-form output extracted via answer extractor; sMAPE metric; oracle and inpainting auxiliary experiments conducted.",
            "mechanisms_or_strategies": "Prompt selection, optional CoT (found to hurt performance), temperature backoff to reduce incoherent outputs; oracle experiments (visible/all coordinates) and inpainting pipeline used for ablations.",
            "performance_metrics": "CAPTURE_real (unoccluded) sMAPE = 26.17%; CAPTURE_real (occluded) sMAPE = 32.90% (+6.73). CAPTURE_synthetic (unoccluded) sMAPE = 16.44%; CAPTURE_synthetic (occluded) sMAPE = 17.57% (+1.13). With All Coordinates Oracle: sMAPE = 17.48% (−15.42). With Visible Oracle: sMAPE = 25.13% (−7.77). With Inpainting: sMAPE = 31.12% (−1.78).",
            "evidence_of_spatial_reasoning": "Pattern recognition accuracy (Table 4) is moderate but drops substantially under occlusion for InternVL2; oracle coordinates substantially improve results indicating the language backbone can perform counting when provided explicit spatial data; however high errors on occluded-only counting (sMAPE = 75.82%) show weak ability to infer occluded counts from visual cues alone.",
            "comparisons": "Performs worse than GPT-4o and CountGD on unoccluded data; shows larger deterioration with occlusion relative to some models; hybrid with CountGD reduces error but remains suboptimal compared to CountGD alone.",
            "limitations_or_failure_cases": "Very poor at counting only occluded objects (extremely high sMAPE), significant drop in pattern-recognition accuracy under occlusion, CoT does not help, temperature backoff yields only modest gains; indicates weaknesses in visual-to-world-model integration.",
            "uuid": "e8543.1",
            "source_info": {
                "paper_title": "CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Molmo",
            "name_full": "Molmo 7B-D",
            "brief_description": "A multimodal model fine-tuned for pointing and grounding (text-to-2D coordinate), enabling direct point-and-count style behavior; evaluated on CAPTURE to test whether grounding aids amodal counting.",
            "citation_title": "Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models.",
            "mention_or_use": "use",
            "model_name": "Molmo 7B-D",
            "model_description": "7B-parameter multimodal model trained on many examples grounding text to 2D image coordinates; has explicit point-and-count capabilities that could benefit counting tasks.",
            "model_size": "7B",
            "puzzle_name": "CAPTURE (Amodal pattern counting)",
            "puzzle_type": "Pattern completion / amodal counting requiring inference about occluded pattern continuation and counting of visible+occluded items.",
            "task_setup": "Same CAPTURE prompting framework; Molmo prompts slightly different (first state the pattern then give final count). Molmo excluded from some oracle experiments due to prompt length limits.",
            "mechanisms_or_strategies": "Direct pointing/grounding capability used implicitly; prompt asking to state pattern then count; temperature backoff and prompt selection used; excluded from some long-text oracle tests due to prompt truncation.",
            "performance_metrics": "CAPTURE_real (unoccluded) sMAPE = 25.90%; CAPTURE_real (occluded) sMAPE = 32.49% (+6.59). CAPTURE_synthetic (unoccluded) sMAPE = 8.40%; CAPTURE_synthetic (occluded) sMAPE = 17.73% (+9.33). (Molmo excluded from All/Visible Coordinate oracle tests due to prompt length limits.)",
            "evidence_of_spatial_reasoning": "Despite being trained to ground text to coordinates, Molmo still shows substantial error under occlusion; on synthetic unoccluded split it does well (8.40%) but occlusion greatly increases errors, suggesting grounding ability alone does not solve amodal world modeling from partial views.",
            "comparisons": "Molmo does not outperform GPT-4o on occluded CAPTURE_real; on synthetic unoccluded data Molmo is competitive but occlusion causes large relative degradation; temperature backoff sometimes produced greater improvements for Molmo than other models.",
            "limitations_or_failure_cases": "Prompt-length limit prevents use of long-text oracle coordinate inputs; performs very poorly on occluded-only counting task (occluded-only sMAPE = 96.79%), indicating inability to infer occluded counts despite point-and-count training; biased predictions and overconfidence noted.",
            "uuid": "e8543.2",
            "source_info": {
                "paper_title": "CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Qwen2-VL",
            "name_full": "Qwen2-VL-7B",
            "brief_description": "A 7B-parameter VLM focused on high-resolution perception; evaluated on CAPTURE for spatial inference under occlusion.",
            "citation_title": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution.",
            "mention_or_use": "use",
            "model_name": "Qwen2-VL-7B",
            "model_description": "Vision-language model emphasizing perception at variable resolutions; used with prompt selection and long output budgets in CAPTURE evaluation.",
            "model_size": "7B",
            "puzzle_name": "CAPTURE (Amodal pattern counting)",
            "puzzle_type": "Pattern-based amodal counting: requires extrapolating regular spatial patterns behind occluders and counting total objects.",
            "task_setup": "CAPTURE prompting (count as-if occluder not present), free-form output, answer extraction by external Llama 3.1 8B extractor; CoT and temperature backoff experiments performed; oracles and inpainting ablations applied.",
            "mechanisms_or_strategies": "Prompt engineering, CoT experiments (CoT often hurts), temperature backoff to reduce skipped outputs; used inpainting pipeline (FLUX.1-Fill) as predicted auxiliary input which gave modest improvements.",
            "performance_metrics": "CAPTURE_real (unoccluded) sMAPE = 18.96%; CAPTURE_real (occluded) sMAPE = 29.33% (+10.37). CAPTURE_synthetic (unoccluded) sMAPE = 6.63%; CAPTURE_synthetic (occluded) sMAPE = 11.74% (+5.11). With All Coordinates Oracle: sMAPE = 9.62% (−19.71). With Visible Oracle: sMAPE = 17.70% (−11.63). With Inpainting: sMAPE = 22.64% (−6.69).",
            "evidence_of_spatial_reasoning": "Shows relatively strong pattern recognition (Table 4) and benefits from circular arrangements; inpainting and oracle experiments reduce error, demonstrating partial use of spatial cues but inability to fully form accurate amodal world models from the raw image alone; occluded-only counting sMAPE = 32.89% indicates difficulty isolating occluded counts.",
            "comparisons": "Performs worse than GPT-4o on CAPTURE_real occluded; benefits by combining with CountGD oracles and inpainting but still falls short of human performance; oracle coordinate inputs yield large error reductions.",
            "limitations_or_failure_cases": "Large sMAPE increase with occlusion, overconfidence in some occluded examples, biased predictions to particular numbers; inpainting improves but does not fully close gap to unoccluded performance.",
            "uuid": "e8543.3",
            "source_info": {
                "paper_title": "CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "MiniCPM-o",
            "name_full": "MiniCPM-o 2.6",
            "brief_description": "A compact multimodal VLM (MiniCPM series) evaluated in CAPTURE to assess small-model spatial reasoning and counting under occlusion.",
            "citation_title": "Minicpm-v: A gpt-4v level mllm on your phone.",
            "mention_or_use": "use",
            "model_name": "MiniCPM-o 2.6",
            "model_description": "Lightweight multimodal model (approx. 2.6B parameters) designed for efficient multimodal inference; included to assess how smaller VLMs handle amodal counting.",
            "model_size": "2.6B",
            "puzzle_name": "CAPTURE (Amodal pattern counting)",
            "puzzle_type": "Pattern completion + counting under occlusion: requires spatial extrapolation from partial observations.",
            "task_setup": "Standard CAPTURE prompts (best-of-10 chosen), free-form outputs extracted by answer-extractor, sMAPE metrics; CoT and temperature backoff explored.",
            "mechanisms_or_strategies": "Prompt tuning and temperature backoff to reduce incoherent answers; no special grounding capabilities like Molmo; oracle/inpainting experiments reported in aggregate analyses.",
            "performance_metrics": "CAPTURE_real (unoccluded) sMAPE = 23.84%; CAPTURE_real (occluded) sMAPE = 30.08% (+6.24). CAPTURE_synthetic (unoccluded) sMAPE = 17.06%; CAPTURE_synthetic (occluded) sMAPE = 19.00% (+1.94).",
            "evidence_of_spatial_reasoning": "Performs worse than larger VLMs; pattern recognition and occlusion handling limited; temperature backoff yields moderate improvements for some cases.",
            "comparisons": "Worse than GPT-4o and some other VLMs on most splits; hybrid with CountGD improves performance but does not reach CountGD unoccluded accuracy.",
            "limitations_or_failure_cases": "High sMAPE on occluded splits; struggles more when occluded objects increase; susceptible to incoherent outputs that are penalized heavily by sMAPE.",
            "uuid": "e8543.4",
            "source_info": {
                "paper_title": "CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Kimi-VL",
            "name_full": "Kimi-VL-A3B",
            "brief_description": "A vision-language model included in the CAPTURE evaluation to expand architectural diversity; used to measure occlusion and counting capability in a mid-sized VLM.",
            "citation_title": "Kimi-VL technical report",
            "mention_or_use": "use",
            "model_name": "Kimi-VL-A3B",
            "model_description": "Kimi-VL family model (technical report cited); included as a representative VLM with its own vision encoder and language backbone; used with the CAPTURE prompt/answer-extractor pipeline.",
            "model_size": "3B",
            "puzzle_name": "CAPTURE (Amodal pattern counting)",
            "puzzle_type": "Pattern-based amodal counting requiring spatial extrapolation across occluded regions.",
            "task_setup": "Standard CAPTURE prompting; free-form generation; answer extraction via Llama 3.1 8B; sMAPE evaluation across occluded/unoccluded real and synthetic splits.",
            "mechanisms_or_strategies": "Prompt selection, temperature backoff experiments; oracle/inpainting ablations applied in aggregate analyses.",
            "performance_metrics": "CAPTURE_real (unoccluded) sMAPE = 23.48%; CAPTURE_real (occluded) sMAPE = 25.96% (+2.48). CAPTURE_synthetic (unoccluded) sMAPE = 16.91%; CAPTURE_synthetic (occluded) sMAPE = 18.07% (+1.16).",
            "evidence_of_spatial_reasoning": "Shows modest degradation with occlusion (smaller Δ than some models), suggesting some robustness but still substantial gap to humans and oracle-assisted performance; benefits from oracle coordinates in aggregate analyses.",
            "comparisons": "Performs similarly to other mid-sized VLMs and worse than GPT-4o; hybrid approaches and oracle inputs improve performance but remain below human baseline.",
            "limitations_or_failure_cases": "Limited ability to infer occluded-only counts; still prone to the same failure modes (incoherent outputs, biases, overconfidence) reported across VLMs.",
            "uuid": "e8543.5",
            "source_info": {
                "paper_title": "CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "CountGD",
            "name_full": "CountGD (multi-modal open-world counting)",
            "brief_description": "A state-of-the-art object-detection-based counting method trained on FSC-147 used as a vision-only baseline and oracle for visible-object counts in CAPTURE evaluations.",
            "citation_title": "Countgd: Multi-modal open-world counting.",
            "mention_or_use": "use",
            "model_name": "CountGD",
            "model_description": "Object-detection-based counting system (trained on FSC-147) designed to count visible objects from images; does not attempt amodal completion and thus cannot infer occluded objects without additional reasoning.",
            "model_size": null,
            "puzzle_name": "CAPTURE (Amodal pattern counting) — used as baseline/hybrid component",
            "puzzle_type": "Counting visible objects in images (vision-only), not an amodal completion solver; spatially grounded detection/counting but no occlusion inference.",
            "task_setup": "Applied to CAPTURE images (subset from FSC-147 test set where CountGD had training overlap) to count visible objects; outputs provided as auxiliary visible counts to VLMs in hybrid experiments.",
            "mechanisms_or_strategies": "Object detection and counting trained on FSC-147; used as oracle of visible counts for hybrids; no textual reasoning component.",
            "performance_metrics": "On evaluation subset, CountGD unoccluded sMAPE = 3.15%; CountGD occluded sMAPE = 10.34% (deterioration of 7.19 percentage points due to occlusion). Hybrid (CountGD visible count + VLM prompt) reduced some VLM errors but hybrid often remained worse than CountGD alone.",
            "evidence_of_spatial_reasoning": "Strong at counting visible objects (low unoccluded sMAPE) but performance degrades in presence of occlusion because it does not infer hidden objects; its failure modes illustrate the gap between detection-based counting and amodal reasoning.",
            "comparisons": "Outperforms all tested VLMs on unoccluded CAPTURE images; on occluded images its error increases but it still can outperform some VLMs; combining CountGD visible counts with VLMs improved VLM performance but hybrid still sometimes trailed CountGD alone.",
            "limitations_or_failure_cases": "Cannot infer or reason about occluded objects — error scales with number of occluded objects; not a solution for amodal counting on its own.",
            "uuid": "e8543.6",
            "source_info": {
                "paper_title": "CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Spartqa: A textual question answering benchmark for spatial reasoning",
            "rating": 2
        },
        {
            "paper_title": "Unibench: Visual reasoning requires rethinking vision-language beyond scaling",
            "rating": 2
        },
        {
            "paper_title": "Occ-mllm: Empowering multimodal large language model for the understanding of occluded objects",
            "rating": 2
        },
        {
            "paper_title": "Countgd: Multi-modal open-world counting",
            "rating": 2
        },
        {
            "paper_title": "Countnet3d: A 3d computer vision approach to infer counts of occluded objects",
            "rating": 2
        },
        {
            "paper_title": "pix2gestalt: Amodal segmentation by synthesizing wholes",
            "rating": 1
        }
    ],
    "cost": 0.017936999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>CAPTURE: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting</h1>
<p>Atin Pothiraj Elias Stengel-Eskin Jaemin Cho Mohit Bansal<br>UNC Chapel Hill<br>{atin, esteng, jmincho, mbansal}@cs.unc.edu</p>
<h4>Abstract</h4>
<p>Recognizing and reasoning about occluded (partially or fully hidden) objects is vital to understanding visual scenes, as occlusions frequently occur in real-world environments and act as obstacles for spatial comprehension. To test models' ability to reason about multiple occluded objects, we introduce a novel task, Counting Amodally for Patterns Through Unseen REgions (CAPTURE), which requires a model to count objects arranged in a pattern by inferring how the pattern continues behind an occluder (an object which blocks parts of the scene). CAPTURE requires both recognizing visual patterns and reasoning, making it a useful testbed for evaluating vision-language models (VLMs) on whether they understand occluded patterns and possess spatial understanding skills. By requiring models to reason about occluded objects, CAPTURE also tests VLMs' ability to form world models that would allow them to fill in missing information. CAPTURE consists of two parts: (1) CAPTURE ${ }^{\text {real }}$, with manually filtered images of real objects in patterns and (2) CAPTURE ${ }^{\text {synthetic }}$, a controlled diagnostic with generated patterned images. We evaluate four strong VLMs (GPT-4o, Intern-VL2, Molmo, and Qwen2VL) on CAPTURE, finding that models struggle to count on both occluded and unoccluded patterns. Crucially, we find that models perform worse with occlusion, suggesting that VLMs are also deficient in inferring unseen spatial relationships: even the strongest VLMs like GPT-4o fail to count with occlusion. In contrast, we find that humans achieve very little error on CAPTURE. We also find that providing auxiliary information of occluded object locations increases performance, underscoring that the model error comes both from an inability to handle occlusion as well as difficulty in counting in images. ${ }^{1}$</p>
<h2>1. Introduction</h2>
<p>Inferring what lies behind different objects in occluded scenes is crucial for human perception, as it allows us to</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. CAPTURE example with an output from GPT-4o. While people can easily infer the missing number of cups and correctly reason over occluded patterns, models generally struggle to reason over these occluded scenes.
maintain a coherent understanding of our environment even when parts are hidden. The human visual system accomplishes this by integrating past experiences, context, and sensory inputs to reconstruct incomplete scenes [19, 27, 30, 45]. Meanwhile, recent advancements in vision-language models (VLMs) - especially in terms of visual and spatial reasoning - raise the question of whether these systems can perform similar inferential tasks. One way of measuring such capabilities is through amodal completion - the task of inferring the invisible parts of partially occluded objects; here, vision-only models are typically evaluated via dense prediction tasks like object segmentation and image inpainting [5]. However, this format is not well-suited for assessing VLMs, whose outputs consist of text tokens rather than pixel-level predictions. This raises a critical question: How can we quantify the ability of VLMs to form spatial world modeling [17] in the presence of occlusion?</p>
<p>To address this, we introduce CAPTURE, Counting Amodally for Patterns Through Unseen REgions, a novel benchmark that tests a VLM's world modeling and spatial reasoning abilities through the task of amodal counting, where models are prompted to count occluded objects</p>
<p>by amodally completing a pattern. CAPTURE focuses on counting as it provides an objective and easy-to-verify output by comparing predicted counts with ground truth values. Moreover, patterned objects appear in various real-world domains, especially in man-made environments like parking lots, cities, and warehouses, where counting objects is often required. Fig. 1 illustrates the CAPTURE task. We show a VLM an image where objects are placed in a regular pattern (e.g., a 4x4 grid) with some objects occluded, and ask the model to count the total number of objects in the image assuming that the pattern continues behind the occlusion. The task requires handling occlusion, pattern recognition, and counting skills that exist in humans from a fairly young age [27, 30, 45], thus humans can easily answer such questions - indeed, we find that people can complete CAPTURE tasks with almost no error.</p>
<p>CAPTURE consists of two subsets: CAPTURE ${ }^{\text {real }}$ and CAPTURE ${ }^{\text {synthetic }}$. As shown in Fig. 2, CAPTURE ${ }^{\text {real }}$ contains real-world images and tests the ability of models to perform amodal counting in naturalistic contexts, while CAPTURE ${ }^{\text {synthetic }}$ allows us to analyze specific factors by controlling different variables like color, shape, and number of objects. All images in CAPTURE contain a pattern of objects and a manually annotated occluding black box covering some objects. CAPTURE ${ }^{\text {real }}$ contains 924 images with a diverse range of settings and objects, covering 92 different object types, while CAPTURE ${ }^{\text {synthetic }}$ contains 1250 images across multiple attribute classes.</p>
<p>By combining vision encoders with large language models (LLMs), VLMs have the potential to reason in a zeroshot way about visual inputs. To put this ability to the test and measure VLMs' ability to reason about missing visual information, we evaluate four strong recent VLMs (GPT4o, InternVL2, Molmo, and Qwen2VL) on CAPTURE. Our experiment results (Sec. 4) show that models generally struggle with the multiple aspects of the task, with high error rates on both CAPTURE ${ }^{\text {real }}$ and CAPTURE ${ }^{\text {synthetic }}$ for occluded and unoccluded images. In contrast, we find that humans can perform the task easily: whereas model performance deteriorates as more objects in images are occluded, humans complete the task almost perfectly. We also compare VLMs to a vision-only model trained to count visible objects; while this model generally outperforms VLMs, its error is directly tied to the number of occluded objects - the more objects are occluded, the higher its error will be.</p>
<p>By objectively measuring VLMs' spatial reasoning capabilities under occlusion, CAPTURE highlights an unexpected weakness in VLMs. We analyze this weakness by providing the model with additional clues and information. Specifically, we test to what degree the VLMs' failure stems from an inability to integrate visual information by providing it with a text-based representation of the visible objects in the image in the form of object coordinates; here, VLMs
perform substantially better, indicating that their poor performance on CAPTURE stems partly from an inability to count objects in images, rather than an inability to count more generally. Our findings align with previous work, which similarly finds that VLMs struggle to count in images [22, 33, 42]. We also test the degree to which VLM errors stem from an inability to form a world model by providing it with auxiliary information (the coordinates of the occluded objects in text, or inpainting the occluded regions). We find that VLMs perform substantially better with this auxiliary information, suggesting that VLMs are partly limited by their inability to imagine the missing visual information. Addressing these gaps is critical for VLMs to function effectively in real-world scenarios, where visual reasoning often involves occlusions - whether counting stadium seats, components on production lines, or buildings in neighborhoods. We hope that our work will foster future research on improving the world modeling capabilities of VLMs.</p>
<h2>2. CAPTURE</h2>
<h3>2.1. Task Overview</h3>
<p>Input/output formulation. CAPTURE tests VLMs on occlusion reasoning, pattern recognition, and counting of both visible and occluded objects. VLMs already achieve high accuracy in classifying single, occluded objects [20]. Thus, we also argue that VLMs have the potential to perform well on CAPTURE's challenging task because their proficiency in handling occlusion ought to enable them to recognize occluded objects and reason accordingly. All images in CAPTURE contain a pattern. This makes the task solvable for models and people - if the objects were not placed in a pattern, it would be unreasonable to expect models to infer the position of the occluded objects. For example, given an image of a random pile of coins with a region occluded, it is not easy to infer whether the occluded region contains no coins or contains roughly the same amount as the rest of the pile. For this task, the patterns considered are all regular and fairly small, e.g. grids, circles, triangles, and other regular shapes - see Fig. 2 for further examples. The last step of CAPTURE is counting, asking the model to provide an objectively measurable output. In addition to VLMs, we also test CountGD [3], a state-of-the-art object detection-based counting method, finding that it fails to account for the occluded scenario, as its training entails solely predicting the visible, unoccluded objects in the image.</p>
<p>Metric. We use symmetric mean percent error (sMAPE) as the primary metric. sMAPE is given by:</p>
<p>$$
\operatorname{sMAPE}=100 \cdot \frac{1}{n} \sum_{i=1}^{n} \frac{\left|y_{i}-\hat{y}<em i="i">{i}\right|}{\left|y</em>
$$}\right|+\left|\hat{y}_{i}\right|</p>
<p>where $y_{i}$ represents the actual values, $\hat{y}_{i}$ represents the pre-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Example images with GPT-4o responses to CAPTURE ${ }^{\text {real }}$ and CAPTURE ${ }^{\text {synthetic }}$ occluded splits.
dicted values, and $n$ is the number of observations. sMAPE is capped at $100 \%$, providing a fixed range. This makes sMAPE ideal for challenging tasks like ours, as we can penalize responses that fail to produce an answer with a maximum error of $100 \%$. For a justification of sMAPE over other metrics, see Appendix A.1.</p>
<h3>2.2. Dataset</h3>
<p>CAPTURE ${ }^{\text {real }}$. We introduce a set of real images with patterns to test amodal counting in naturalistic settings. The original images and annotations come from the FSC-147 dataset [37], a diverse counting dataset with manual annotations for the number of target objects and all object bounding boxes in each image. FSC-147 contains a diverse array of objects, with 6146 real-world images across 147 object categories. We filter FSC-147 for images that contain identifiable and regular patterns of objects and manually overlay a black box to occlude some objects, resulting in 924 images. Filtering is first performed with GPT-4o and then manually verified; we also manually verify that determining objects despite the occlusion is feasible. For each example, we maintain both occluded and unoccluded versions. Further details on CAPTURE ${ }^{\text {real }}$ can be found in Appendix B.</p>
<p>CAPTURE ${ }^{\text {synthetic }}$. While CAPTURE ${ }^{\text {real }}$ makes CAPTURE more applicable to real-world scenarios, each image is unique, making the data less controlled and challenging to draw clear conclusions about model performance. Images without background distractors, texture variance, and other potential visual obstacles provide a more controlled version of the task. Therefore, we create CAPTURE ${ }^{\text {synthetic }}$ to examine the task in a fully controlled environment. CAPTURE ${ }^{\text {synthetic }}$ comprises 1250 images of simple objects in patterns, where different variables are held constant or changed. We vary the following features:</p>
<ol>
<li>Object count: varies from 5 to 15 .</li>
<li>Object: can be either dots or squares.</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">CAPTURE ${ }^{\text {real }}$</th>
<th style="text-align: center;">CAPTURE ${ }^{\text {synthetic }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"># Images</td>
<td style="text-align: center;">924</td>
<td style="text-align: center;">1250</td>
</tr>
<tr>
<td style="text-align: left;"># Object Types</td>
<td style="text-align: center;">92</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">Avg. Occluded Obj.</td>
<td style="text-align: center;">13.97</td>
<td style="text-align: center;">2.73</td>
</tr>
<tr>
<td style="text-align: left;">Avg. Total Obj.</td>
<td style="text-align: center;">61.45</td>
<td style="text-align: center;">10.00</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Diverse Objects/Settings</td>
<td style="text-align: center;">Confounder-free</td>
</tr>
<tr>
<td style="text-align: left;">Strengths</td>
<td style="text-align: center;">Naturalistic</td>
<td style="text-align: center;">Controllable Attributes</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Realistic Context</td>
<td style="text-align: center;">Uniformly Distributed</td>
</tr>
</tbody>
</table>
<p>Table 1. Statistics and strengths for CAPTURE splits.
3. Arrangement/shape: can be a rectangle, circle, or pyramid (where feasible based on object count).
4. Location: we consider five positions on the page: center, top-left, top-right, bottom-left, or bottom-right.
5. Color: we randomly choose one of 5 colors for all the objects in an image.
The CAPTURE ${ }^{\text {synthetic }}$ data is split similarly to the CAPTURE ${ }^{\text {real }}$ data; each configuration has a variant with an overlaid occluding box and one without.</p>
<h3>2.3. Statistics and Examples</h3>
<p>Fig. 2 shows examples from CAPTURE ${ }^{\text {real }}$ and CAPTURE ${ }^{\text {synthetic }}$ paired with their corresponding answers from GPT-4o and their ground truth answers. These examples show the range of objects and patterns in the dataset and highlight the task's feasibility for humans. Tab. 1 reports summary statistics for CAPTURE, including the number of images and object types, as well as the mean number of occluded and total objects in both splits of CAPTURE. The number of objects in CAPTURE ${ }^{\text {real }}$ is shown in Fig. 3, where most images have between 0 and 30 objects. On CAPTURE ${ }^{\text {synthetic }}$, the maximum number of objects is 15 , and CAPTURE ${ }^{\text {synthetic }}$ images generally have 1-6 occluded objects (shown in Fig. 4, as further occlusion could make the count unresolvable).</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. # of objects in CAPTURE ${ }^{\text {real }}$ images.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. # of occluded objects in CAPTURE ${ }^{\text {synthetic }}$ images.</p>
<h2>3. Experiment Setup</h2>
<h3>3.1. Models</h3>
<p>We experiment with GPT-4o [28], Intern-VL2-Llama3-8B [9, 10], Qwen2-VL-7B [41], MiniCPM-o 2.6 [47], and Kimi-VL-A3B [40] for their high scores on other VLM tasks [29]. We add Molmo 7B-D [13], because of its ability to "point and count," giving it a potential advantage on CAPTURE. Specifically, Molmo is trained on millions of examples that directly ground text to 2D coordinates (or "points") in images. This allows Molmo to directly point to image coordinates and count more easily by pointing to several objects. All the VLMs feature a different language backbone and vision encoder to provide broad coverage of model architectures. To evaluate models, we provide the model with the name of the specific object to be counted and the explicit instruction to count fully visible objects and objects behind the occluding box (in the occluded images). For each model, we test ten prompts on a validation set of 100 images, selecting the best prompt for each model in each dataset section (CAPTURE ${ }^{\text {real }} /$ CAPTURE ${ }^{\text {synthetic }}$ ) and for each environment (occluded/unoccluded). We provide the selected prompts in Appendix D.</p>
<h3>3.2. Answer Generation and Extraction</h3>
<p>Given the complex nature of CAPTURE, we allow models to generate open-ended responses and then subsequently extract answers. Further details (including the maximum number of tokens) can be found in Appendix A.2.</p>
<p>Answer extraction. Empirically, we found that constraining the output to a specific format for ease of analysis neg-
atively impacted benchmark performance. Therefore, we instead prompt models to generate freely and extract the final output number using a separate answer extractor based on Llama 3.1 8B [1]. This answer extractor takes the output from the model as input and prompts it to extract a single number representing the final answer. The answer extractor also identifies if an output failed to converge on a singular number answer and assigns a label to these examples. We mark such incomplete/incoherent model generations as 'skipped' questions and when calculating the error later, these responses are assigned the worst possible sMAPE score ( $100 \%$ ). The answer extractor outputs were manually verified on 1000 outputs, and the extractor was found to be $100 \%$ accurate.</p>
<p>Human and object detection baselines. We also report the performance of humans and a recent counting model (CountGD [3]) as baselines to establish a point of reference for model performance. To confirm that humans can perform the CAPTURE task, we provided 100 randomly selected occluded examples each from the CAPTURE ${ }^{\text {real }}$ and CAPTURE ${ }^{\text {synthetic }}$ subsets to 3 undergraduate students with no prior knowledge of the task.</p>
<h2>4. Results and Analysis</h2>
<h3>4.1. Main Results on CAPTURE ${ }^{\text {real }}$</h3>
<p>Models consistently struggle with counting and perform worse on occluded images. We run the VLMs on the occluded and unoccluded versions of CAPTURE to discern whether occlusion significantly impacts model performance. Tab. 2 shows that all models struggle with counting generally, performing poorly on both splits. Moreover, we see that every model performs better on the unoccluded images. On average, the models perform $6.28 \%$ worse in CAPTURE ${ }^{\text {real }}$ occluded images and $4.85 \%$ worse in CAPTURE ${ }^{\text {synthetic }}$ occluded images (in terms of absolute sMAPE), indicating increased difficulty from a standard counting task. The best model for both splits, GPT-4o, has an error rate of $14.75 \%$ on CAPTURE ${ }^{\text {real }}$ and a lower error rate of $9.71 \%$ on CAPTURE ${ }^{\text {synthetic }}$. Across both the real and synthetic split, GPT-4o's error increases with occlusion, by $1.41 \%$ on the real data and $3.81 \%$ on the synthetic split. Interestingly, despite its fine-tuning on counting tasks, Molmo exhibits a sizable error rate of $32.5 \%$ on CAPTURE ${ }^{\text {real }}$ occluded images. The high error rates of VLMs indicate limited capabilities in visual understanding under occlusions, pattern recognition, and counting. We further analyze the source of these errors with oracle experiments in Sec. 4.3.</p>
<p>Humans complete the task with almost no error. Tab. 3, evaluated on a 100 -example subset of each split, confirms that humans complete the task with ease despite occlusion,</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Error (\%) [ $\downarrow$ ]</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>CAPTURE ${ }^{\text {real }}$</td>
<td></td>
<td>CAPTURE ${ }^{\text {synthetic }}$</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Original</td>
<td>w/ Occlusion $(\Delta)$</td>
<td>Original</td>
<td>w/ Occlusion $(\Delta)$</td>
</tr>
<tr>
<td>GPT-4o</td>
<td>13.34</td>
<td>$14.75(+1.41)$</td>
<td>5.90</td>
<td>$9.71(+3.81)$</td>
</tr>
<tr>
<td>InternVL2</td>
<td>26.17</td>
<td>$32.90(+6.73)$</td>
<td>16.44</td>
<td>$17.57(+1.13)$</td>
</tr>
<tr>
<td>Molmo</td>
<td>25.90</td>
<td>$32.49(+6.59)$</td>
<td>8.40</td>
<td>$17.73(+9.33)$</td>
</tr>
<tr>
<td>Qwen2VL</td>
<td>18.96</td>
<td>$29.33(+10.37)$</td>
<td>6.63</td>
<td>$11.74(+5.11)$</td>
</tr>
<tr>
<td>MiniCPM-o 2.6</td>
<td>23.84</td>
<td>$30.08(+6.24)$</td>
<td>17.06</td>
<td>$19.00(+1.94)$</td>
</tr>
<tr>
<td>Kimi-VL-A3B</td>
<td>23.48</td>
<td>$25.96(+2.48)$</td>
<td>16.91</td>
<td>$18.07(+1.16)$</td>
</tr>
<tr>
<td>Avg. of 6 VLMs</td>
<td>21.95</td>
<td>$27.59(+5.64)$</td>
<td>11.89</td>
<td>$15.64(+3.75)$</td>
</tr>
</tbody>
</table>
<p>Table 2. Results across VLMs on all splits of CAPTURE, with average error for each column. Metric: sMAPE (lower is better).</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Error (\%) [ $\downarrow$ ]</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>CAPTURE ${ }^{\text {real }}$</td>
<td>CAPTURE ${ }^{\text {synthetic }}$</td>
</tr>
<tr>
<td>(Baseline)</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Human</td>
<td>3.79</td>
<td>0.92</td>
</tr>
<tr>
<td>(VLMs)</td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-4o</td>
<td>14.75</td>
<td>9.71</td>
</tr>
<tr>
<td>InternVL2</td>
<td>32.90</td>
<td>17.57</td>
</tr>
<tr>
<td>Molmo</td>
<td>32.49</td>
<td>17.73</td>
</tr>
<tr>
<td>Qwen2VL</td>
<td>29.33</td>
<td>11.74</td>
</tr>
<tr>
<td>Avg. of 4 VLMs</td>
<td>27.37</td>
<td>14.19</td>
</tr>
</tbody>
</table>
<p>Table 3. Human baseline vs VLMs on CAPTURE ${ }^{\text {real }}$ and CAPTURE ${ }^{\text {synthetic }}$ (occluded split). Metric: sMAPE (lower is better).
with an sMAPE of $3.79 \%$ on CAPTURE ${ }^{\text {real }}$ and $0.92 \%$ on CAPTURE ${ }^{\text {synthetic }}$. On the same subset of examples, models performed 7 times worse on CAPTURE ${ }^{\text {real }}$ and 14 times worse on CAPTURE ${ }^{\text {synthetic }}$ than humans, underscoring the gap between VLMs and humans in this task.</p>
<p>Object detection-based baseline outperforms VLMs. We attempt the task with a strong object detection-based model to highlight that a standard counting approach will experience a greater loss going from unoccluded to occluded environments, as it cannot capture any occluded objects, i.e. cannot reason. We choose CountGD [3], the top solution for unoccluded counting on FSC-147, on which it was trained. Because we draw our images from FSC-147's train and test sets, and CountGD trains on FSC-147, we only evaluate CountGD on the subset of our data sourced from the FSC-147 test set, consisting of 149 images.</p>
<p>We find that CountGD deteriorates by $\mathbf{7 . 1 9 \%}$ on occluded images, increasing from $3.15 \%$ sMAPE to $10.34 \%$ as observed in Fig. 5. As expected, CountGD outperforms all VLMs on the unoccluded split as it is trained for counting on FSC-147. CountGD also outperforms the VLMs on the occluded split, reinforcing that only counting the visible objects is a hard-to-beat baseline. However, the drop in performance with occlusion is greater than the average
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. VLM vs. VLM + CountGD hybrid on questions from the CAPTURE ${ }^{\text {real }}$ (occluded split) that are not in CountGD training set. Metric: sMAPE (lower is better).</p>
<p>VLM's drop, highlighting a disadvantage of non-reasoning solutions on CAPTURE: their error is necessarily tied directly to the number of occluded objects and they cannot address the task on their own, whereas a VLM might be able to infer missing objects via reasoning.</p>
<p>Hybrid VLM counting systems improve performance. Finding that CountGD is far better at counting visible objects than VLMs, we leverage the advantage that CountGD has by feeding its visible object count information to the VLMs as part of the prompt. As expected, Fig. 5 illustrates that there is a considerable decrease in error when CountGD and the VLMs are combined. However, this hybrid system still performs worse than CountGD alone, indicating VLMs are still subpar even at counting just occluded objects (as further reinforced by Appendix C.3).</p>
<h3>4.2. Effect of Data Factors on VLM Performance</h3>
<p>Here, we use the CAPTURE ${ }^{\text {synthetic }}$ data (which can be controlled precisely and fully annotated ) to examine which features correlate with model performance. We test the effect of the following variables on final performance: (1) Increasing the number of occluded objects; (2) Varying the pattern. We also investigate whether models can classify patterns, and to what degree models can predict the number of occluded objects only (rather than the total).</p>
<p>Models perform worse when more dots are occluded. In Fig. 6 (right), we observe that error increases with re-</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Effect of number of total objects in the image and number of occluded objects on sMAPE from CAPTURE® ${ }^{\text {synthetic }}$ (occluded split). Metric: sMAPE (lower is better).</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Effect of pattern type in CAPTURE ${ }^{\text {synthetic }}$ (occluded split) on sMAPE. Metric: sMAPE (lower is better).
spect to the number of occluded dots. However, Fig. 6 (left) also shows that performance is less affected by the total number of dots. This suggests that the task difficulty is more closely correlated with the difficulty of occlusion - i.e. the difficulty of the world modeling task - rather than the complexity of the pattern. Some models, such as GPT-4o, deviate from this trend, which has lower error on specific numbers. We further explore model bias in Appendix C.5.</p>
<p>Performance depends on pattern type. The controllability of CAPTURE ${ }^{\text {synthetic }}$ allows us to measure the effect of pattern type on performance. In Fig. 7, we find that model performance differs across shapes with some regularity: objects arranged in a circle generally have lower sMAPE than other shapes, across all models. Qwen2VL has an espe-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Accuracy (\%) [ $\uparrow$ ]</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">w/ Occlusion $(\Delta)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4o</td>
<td style="text-align: center;">84.00</td>
<td style="text-align: center;">$78.52(-5.48)$</td>
</tr>
<tr>
<td style="text-align: left;">InternVL2</td>
<td style="text-align: center;">68.52</td>
<td style="text-align: center;">$47.48(-21.04)$</td>
</tr>
<tr>
<td style="text-align: left;">Molmo</td>
<td style="text-align: center;">80.70</td>
<td style="text-align: center;">$65.22(-15.48)$</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2VL</td>
<td style="text-align: center;">88.35</td>
<td style="text-align: center;">$86.43(-1.92)$</td>
</tr>
<tr>
<td style="text-align: left;">Avg. of 4 VLMs</td>
<td style="text-align: center;">80.39</td>
<td style="text-align: center;">$69.41(-10.98)$</td>
</tr>
</tbody>
</table>
<p>Table 4. VLM accuracy in identifying the correct pattern in CAPTURE ${ }^{\text {synthetic. }}$. Metric: accuracy (higher is better).
cially large decrease in error when given circular arrangements compared to rectangles or triangles.</p>
<p>Models can identify patterns. To determine how much model errors can be attributed to a lack of pattern recognition ability, we formulate a separate task where models must recognize the pattern in the image on CAPTURE ${ }^{\text {synthetic }}$. Here, we frame the task as multiple-choice, asking the model to select from the pattern types available (rectangle, triangle, or circle). Table 4 illustrates that all perform substantially better than random at this task, with most models except InternVL2 achieving accuracy above $80 \%$ in the unoccluded setting. As expected, the patterns were easier to identify in unoccluded scenarios, with models suffering an average accuracy drop of $10.95 \%$ in the occluded setting. Notably, GPT-4o and Qwen2VL have a fairly small drop in performance, suggesting they can generally capture the pattern even in the presence of occlusion.</p>
<h3>4.3. Analysis with Auxiliary Information</h3>
<p>In Sec. 4.1, we see that models broadly struggle with amodal counting. Here, we seek to disentangle whether this problem results from a failure to reason, the absence of a world model, or both by giving VLMs two different types of auxiliary information: oracle information and predicted information. Oracle information is ground truth and is directly pulled from CAPTURE's metadata, e.g., object locations. Predicted information generates new information from a completely separate model and gives it to the VLM. This information is not ground truth and is sourced from an external model, such as an image inpainting model, rather than the VLM. By giving the model auxiliary information in the form of reasoning and spatial clues, we can establish how much of each model's error results from an inability to handle occlusion rather than an inability to recognize and count visible objects.</p>
<p>Oracle setup. We test two oracles for CAPTURE ${ }^{\text {real }}$ 's occluded split based on its constituent subtasks: counting the visible objects and inferring/counting occluded objects. Both oracles provide the VLM with text-based coordinates</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Example image and text inputs for experiments with auxiliary information experiments (Sec. 4.3). Blue eyes indicate objects for which the All Object Coordinate Oracle or Visible Object Coordinate Oracle extracts coordinates. The brighter part of the image represents the area which Inpainting Pipeline fills in. Example prompts are shown in italics. Blue eye overlays and faded parts of images are for demonstration purposes and are not passed with the image.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Original</th>
<th style="text-align: center;">w/ Occlusion</th>
<th style="text-align: center;">Oracle Information</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Predicted Information</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">+ All Coordinates $(\Delta)$</td>
<td style="text-align: center;">+ Visible $(\Delta)$</td>
<td style="text-align: center;">+ Inpainting $(\Delta)$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4o</td>
<td style="text-align: center;">13.34</td>
<td style="text-align: center;">14.75</td>
<td style="text-align: center;">2.93 (-11.82)</td>
<td style="text-align: center;">9.20 (-5.55)</td>
<td style="text-align: center;">15.89 (+1.14)</td>
</tr>
<tr>
<td style="text-align: center;">InternVL2</td>
<td style="text-align: center;">26.17</td>
<td style="text-align: center;">32.90</td>
<td style="text-align: center;">17.48 (-15.42)</td>
<td style="text-align: center;">25.13 (-7.77)</td>
<td style="text-align: center;">31.12 (-1.78)</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2VL</td>
<td style="text-align: center;">18.96</td>
<td style="text-align: center;">29.33</td>
<td style="text-align: center;">9.62 (-19.71)</td>
<td style="text-align: center;">17.70 (-11.63)</td>
<td style="text-align: center;">22.64 (-6.69)</td>
</tr>
<tr>
<td style="text-align: center;">Avg. of 3 VLMs</td>
<td style="text-align: center;">19.49</td>
<td style="text-align: center;">25.66</td>
<td style="text-align: center;">10.01 (-15.65)</td>
<td style="text-align: center;">17.34 (-8.32)</td>
<td style="text-align: center;">23.22 (-2.44)</td>
</tr>
</tbody>
</table>
<p>Table 5. Effect of auxiliary information on occluded CAPTURE ${ }^{\text {red }}$. $\Delta=$ (Auxiliary Information) - (w/ Occlusion). Metric: sMAPE.
of objects in the image, simplifying the visual task by assuming the VLM effectively has a perfect visual system that can recognize and localize objects in the image. The first oracle, the Visible Object Coordinate Oracle, gives the VLM the coordinates of all unoccluded objects (encoded as text, as seen in Fig. 8) and instructs the model to estimate the number of occluded objects, count the number of visible object coordinates, and add the two. In other words, the model is given oracle information about what objects are visible, thus also revealing key information about the pattern. The second oracle, the All Object Coordinate Oracle, instead gives the model the coordinates of all objects. Here, the model only needs to count the coordinates in the prompt, eliminating the need to reason on the visual input. Note that Molmo is excluded in these tests because it contains a prompt limit that would truncate the list of coordinates. An example of the oracle inputs can be seen in Fig. 8.</p>
<p>Prediction setup. In this setting, we provide the VLM with an external world model representation predicted by another model. Specifically, we develop the Inpainting Pipeline to fill in the occluded region via a diffusion-based inpainting model and pass the inpainted image to the VLMs. For the inpainting model, we choose FLUX.1-Fill [dev], whose backbone FLUX. 1 [dev] [21] is a top public model in the Text to Image Model Arena [7]. An example input to the VLM can be seen on the far-right of Fig. 8.</p>
<p>Providing visible or all object coordinates improves performance substantially. The results in Tab. 2 indicate that models struggle on CAPTURE, which requires identifying a pattern and counting both visible and occluded objects. Moreover, models generally struggle with counting even in unoccluded settings. Both oracles simplify the counting task: All Object Coordinate Oracle reduces the task to simply counting coordinates with no reasoning involved, and Visible Object Coordinate Oracle similarly simplifies the task for visible objects, while still requiring inferring occluded objects. Additionally, under Visible Object Coordinate Oracle, recognizing the pattern shifts from a visual reasoning task to an augmented math problem. Instead of visually reasoning about where objects are located, the VLM considers what patterns the coordinates could make. Translating this task into a text problem results in an average increase of $15 \%$ with all objects coordinate oracle; the errors LLMs make here are due to an inability to count in the text prompt, as opposed to weaknesses in handling occlusion (since all object coordinates are given), and the strongest</p>
<p>model, GPT-4o, achieves minimal error here. We also obtain an average increase of $8 \%$ with the visible objects coordinate oracle (shown in Tab. 5), possibly because it allows the more powerful LLM backbone (which is far larger than the vision model in all models tested) to complete the counting task. Taken together, these results suggest that there is much room for improvement in visual world modeling beyond text-based reasoning of VLMs.</p>
<p>Providing diffusion-based inpainting improves performance marginally. Similar to the object coordinate oracles, the Inpainting Pipeline (rightmost columns in Fig. 8 and Tab. 5) eliminates the need for world modeling and provides VLMs with an approximation of the image behind the occluder. With the inpainted images, VLM error decreases by almost $2 \%$ for InternVL2 and $7 \%$ for Qwen2VL compared to the original occluded images. GPT-4o's error increases on inpainted images by a small margin; we hypothesize that this may be because GPT-4o has one of the better world models (based on its superior performance), and thus does not improve further with the inpainted images. Moreover, every VLM still falls short of its unoccluded image performance, indicating that the diffusion model is not a perfect world model. Qualitatively, we find that the inpainting model sometimes fails to output the correct pattern.</p>
<h2>5. Related Work</h2>
<p>Spatial reasoning in visual question answering. Past work measures the spatial reasoning capabilities of VLMs in the form of visual question answering (VQA) [4, 16] benchmarks. SpartQA [26] asks VLMs to identify the spatial relation (e.g., above, behind, left of) between objects in synthetically created 2D images from NLVR [39]. More recent benchmarks test similar spatial relation understanding with real images [2, 24, 36]. While this past work asks models to provide a text description for a relation between two fully observed objects, CAPTURE measures the world modeling from a partially observed scene, thus requiring the handling of occlusion, pattern recognition, and counting. Together, these constitute a stricter test of spatial reasoning than typical VQA settings.</p>
<p>Amodal completion. Occlusions are common in natural scenes, and vision solutions for amodal completion have made significant progress in infilling occlusions [6, 38, 46]. The amodal completion task has evolved from simply completing a shape to filling in appearance (e.g., texture, color, etc.) to finally dealing with fine-grained order perception (multiple stacked occluded objects) [5]. Specifically in Qiu and Di [34], VLMs classify the hidden objects and extract fine details from occluded items. CAPTURE, however, presents a unique category of patterned amodal counting which requires inferring fully occluded objects based on a
pattern rather than inferring occluded object wholes based on object parts. In other words, previous work has only attempted tasks that require amodal completion for one object at a time [31, 38, 46], whereas CAPTURE handles multiple objects. Multi-object amodal completion is crucial because in cluttered scenes, entire groups of objects are often occluded. Moreover, the output space of CAPTURE is language (rather than filling pixels).</p>
<p>Counting with vision-and-language models. Within the task of counting, the most similar application to CAPTURE is dense counting, where the objects to be counted occlude each other. There are many practical applications of such a task, like counting cells on a crowded slide [8], determining crop yields from densely-packed fields [43], or crowd counting [14, 44, 48]. Liang et al. [23] improved crowd counting with an augmented CLIP [35], i.e. also using VLMs for counting. Additionally, Jenkins et al. [18] introduced an amodal counting benchmark, presenting an occluded 3D counting task where models must count objects on retail shelves. However, our work differs in many ways, as Jenkins et al. [18] only counts retail shelves and uses LiDAR input. More broadly, dense counting focuses on overlapping objects rather than on counting objects arranged into patterns, which is the focus of CAPTURE.</p>
<h2>6. Conclusion</h2>
<p>We introduced CAPTURE, a novel benchmark for amodal counting that measures spatial reasoning capabilities under occlusion. CAPTURE is designed to assess VLMs' ability to form a robust world model and use that model for visual reasoning skills under occlusion. By testing counting, we cast the problem as a measurable task with an objective correct answer that also has real-world utility as VLMs become more broadly adopted. Our results suggest that VLMs struggle to combine reasoning, counting, and world modeling with low performance on occluded and unoccluded images. Our analysis indicates that models improve with oracle information about visible objects (simplifying the reasoning/counting tasks) and predicted information about the occluded objects (also simplifying world modeling), pointing to directions of model improvement.</p>
<h2>Acknowledgments</h2>
<p>This work was supported by DARPA ECOLE Program No. HR00112390060, NSF-CAREER Award 1846185, NSF-AI Engage Institute DRL-2112635, DARPA Machine Commonsense (MCS) Grant N66001-19-2-4031, ARO Award W911NF2110220, ONR Grant N00014-23-1-2356, Microsoft Accelerate Foundation Models Research (AFMR) grant program, and a Bloomberg Data Science PhD Fellowship. The views contained in this article are those of the authors and not of the funding agency.</p>
<h2>References</h2>
<p>[1] AI@Meta. Llama 3.1 model card. Github Model Card, 2024. 4
[2] Haider Al-Tahan, Quentin Garrido, Randall Balestriero, Diane Bouchacourt, Caner Hazirbas, and Mark Ibrahim. Unibench: Visual reasoning requires rethinking visionlanguage beyond scaling. arXiv preprint arXiv:2408.04810, 2024. 8
[3] Niki Amini-Naieni, Tengda Han, and Andrew Zisserman. Countgd: Multi-modal open-world counting. arXiv preprint arXiv:2407.04619, 2024. 2, 4, 5
[4] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 24252433, 2015. 8
[5] Jiayang Ao, Qiuhong Ke, and Krista A Ehinger. Image amodal completion: A survey. Computer Vision and Image Understanding, 229:103661, 2023. 1, 8
[6] Jiayang Ao, Yanbei Jiang, Qiuhong Ke, and Krista A Ehinger. Open-world amodal appearance completion. arXiv preprint arXiv:2411.13019, 2024. 8
[7] Artificial Analysis. Text to image model arena, 2025. Accessed: April 10, 2025. 7
[8] Soumen Bera. Partially occluded object detection and counting. In Proceedings of the 2015 Third International Conference on Computer, Communication, Control and Information Technology (C3IT), pages 1-6. IEEE, 2015. 8
[9] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. 4
[10] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 4
[11] Davide Chicco, Matthijs J Warrens, and Giuseppe Jurman. The coefficient of determination r-squared is more informative than smape, mae, mape, mse and rmse in regression analysis evaluation. Peerj computer science, 7:e623, 2021. 11
[12] Nikolas Coupland. How frequent are numbers? Language \&amp; Communication, 31(1):27-37, 2011. 13
[13] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. 4
[14] Zheyi Fan, Zihao Song, Di Wu, and Yixuan Zhu. Multibranch segmentation-guided attention network for crowd counting. Journal of Visual Communication and Image Representation, 97:103964, 2023. 8
[15] Benito E Flores. A pragmatic view of accuracy measurement in forecasting. Omega, 14(2):93-98, 1986. 11
[16] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904-6913, 2017. 8
[17] David Ha and Jürgen Schmidhuber. Recurrent world models facilitate policy evolution. Advances in neural information processing systems, 31, 2018. 1
[18] Porter Jenkins, Kyle Armstrong, Stephen Nelson, Siddhesh Gotad, J Stockton Jenkins, Wade Wilkey, and Tanner Watts. Countnet3d: A 3d computer vision approach to infer counts of occluded objects. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 3008-3017, 2023. 8
[19] Gaetano Kanizsa, Paolo Legrenzi, and Paolo Bozzi. Organization in vision : essays on gestalt perception. Praeger, 1979. 1
[20] Kaleb Kassaw, Francesco Luzi, Leslie M Collins, and Jordan M Malof. Are deep learning models robust to partial object occlusion in visual recognition tasks? arXiv preprint arXiv:2409.10775, 2024. 2
[21] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 7
[22] Baiqi Li, Zhiqiu Lin, Wenxuan Peng, Jean de Dieu Nyandwi, Daniel Jiang, Zixian Ma, Simran Khanuja, Ranjay Krishna, Graham Neubig, and Deva Ramanan. Naturalbench: Evaluating vision-language models on natural adversarial samples. arXiv preprint arXiv:2410.14669, 2024. 2
[23] Dingkang Liang, Jiahao Xie, Zhikang Zou, Xiaoqing Ye, Wei Xu, and Xiang Bai. Crowdclip: Unsupervised crowd counting via vision-language model. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2893-2903, 2023. 8
[24] Fangyu Liu, Guy Edward Toh Emerson, and Nigel Collier. Visual spatial reasoning. Transactions of the Association for Computational Linguistics, 2023. 8
[25] Baraka Jacob Maiseli. Optimum design of chamfer masks using symmetric mean absolute percentage error. EURASIP Journal on Image and Video Processing, 2019(1):74, 2019. 11
[26] Roshanak Mirzaee and Hossein Rajaby. Spartqa: A textual question answering benchmark for spatial reasoning. In The 2021 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL2021), 2021. 8
[27] Ingrid R Olson, J Christopher Gatenby, Hoi-Chung Leung, Pawel Skudlarski, and John C Gore. Neuronal representation of occluded objects in the human brain. Neuropsychologia, 42(1):95-104, 2004. 1, 2
[28] OpenAI. Hello gpt-4o, 2024. 4
[29] OpenCompass Team. Openvlm leaderboard. https:// huggingface.co/spaces/opencompass/open_ vlm_leaderboard, 2024. Accessed: 2024-11-13. 4
[30] Yumiko OTSUKA, So KANAZAWA, and Masami K YAMAGUCHI. Development of modal and amodal completion in infants. Perception (London. Print), 35(9):1251-1264, 2006. 1,2</p>
<p>[31] Ege Ozguroglu, Ruoshi Liu, Dídac Surís, Dian Chen, Achal Dave, Pavel Tokmakov, and Carl Vondrick. pix2gestalt: Amodal segmentation by synthesizing wholes. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3931-3940. IEEE Computer Society, 2024. 8
[32] Max Peeperkorn, Tom Kouwenhoven, Dan Brown, and Anna Jordanous. Is temperature the creativity parameter of large language models? arXiv preprint arXiv:2405.00492, 2024. 12
[33] Muhammad Fetrat Qharabagh, Mohammadreza Ghofrani, and Kimon Fountoulakis. Lvlm-count: Enhancing the counting ability of large vision-language models. arXiv preprint arXiv:2412.00686, 2024. 2
[34] Wenmo Qiu and Xinhan Di. Occ-mllm: Empowering multimodal large language model for the understanding of occluded objects. arXiv preprint arXiv:2410.01261, 2024. 8
[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PmLR, 2021. 8
[36] Navid Rajabi and Jana Kosecka. Gsr-bench: A benchmark for grounded spatial reasoning evaluation via multimodal llms. arXiv preprint arXiv:2406.13246, 2024. 8
[37] Viresh Ranjan, Udbhav Sharma, Thu Nguyen, and Minh Hoai. Learning to count everything. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3394-3403, 2021. 3
[38] Kaziwa Saleh, Sándor Szénási, and Zoltán Vámossy. Mask guided gated convolution for amodal content completion. In 2024 IEEE 22nd Jubilee International Symposium on Intelligent Systems and Informatics (SISY), pages 000321-000326. IEEE, 2024. 8
[39] Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. A corpus of natural language for visual reasoning. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 217223, Vancouver, Canada, 2017. Association for Computational Linguistics. 8
[40] Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, Congcong Wang, Dehao Zhang, Dikang Du, Dongliang Wang, Enming Yuan, Enzhe Lu, Fang Li, Flood Sung, Guangda Wei, Guokun Lai, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haoning Wu, Haotian Yao, Haoyu Lu, Heng Wang, Hongcheng Gao, Huabin Zheng, Jiaming Li, Jianlin Su, Jianzhou Wang, Jiaqi Deng, Jiezhong Qiu, Jin Xie, Jinhong Wang, Jingyuan Liu, Junjie Yan, Kun Ouyang, Liang Chen, Lin Sui, Longhui Yu, Mengfan Dong, Mengnan Dong, Nuo Xu, Pengyu Cheng, Qizheng Gu, Runjie Zhou, Shaowei Liu, Sihan Cao, Tao Yu, Tianhui Song, Tongtong Bai, Wei Song, Weiran He, Weixiao Huang, Weixin Xu, Xiaokun Yuan, Xingcheng Yao, Xingzhe Wu, Xinxing Zu, Xinyu Zhou, Xinyuan Wang, Y. Charles, Yan Zhong, Yang Li, Yangyang Hu, Yanru Chen, Yejie Wang, Yibo Liu, Yibo Miao, Yidao Qin, Yimin Chen,</p>
<p>Yiping Bao, Yiqin Wang, Yongsheng Kang, Yuanxin Liu, Yulun Du, Yuxin Wu, Yuzhi Wang, Yuzi Yan, Zaida Zhou, Zhaowei Li, Zhejun Jiang, Zheng Zhang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Zijia Zhao, and Ziwei Chen. Kimi-VL technical report, 2025. 4
[41] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 4
[42] Wei-Yao Wang, Zhao Wang, Helen Suzuki, and Yoshiyuki Kobayashi. Seeing is understanding: Unlocking causal attention into modality-mutual attention for multimodal llms. arXiv preprint arXiv:2503.02597, 2025. 2
[43] Yiding Wang, Yuxin Qin, and Jiali Cui. Occlusion robust wheat ear counting algorithm based on deep learning. Frontiers in Plant Science, 12:645899, 2021. 8
[44] Yongjie Wang, Feng Wang, and Dongyang Huang. Dualbranch counting method for dense crowd based on selfattention mechanism. Expert Systems with Applications, 236:121272, 2024. 8
[45] Karen Wynn. Children's understanding of counting. Cognition, 36(2):155-193, 1990. 1, 2
[46] Katherine Xu, Lingzhi Zhang, and Jianbo Shi. Amodal completion via progressive mixed context diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9099-9109, 2024. 8
[47] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: A gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. 4
[48] Lifang Zhou, Songlin Rao, Weisheng Li, Bo Hu, and Bo Sun. Multi-branch progressive embedding network for crowd counting. Image and Vision Computing, page 105140, 2024. 8</p>
<h2>Appendix</h2>
<h2>A. Implementation Details</h2>
<h2>A.1. Metric Details</h2>
<p>We use symmetric mean percent error (sMAPE) as the primary metric for our benchmarks due to its resistance to bias for under/over predictions and small/large ground truths [25]. The standard metric for a counting benchmark is mean average error (MAE). MAE is popular, but heavily penalizes predictions that deviate by a small margin from big ground truths, highlighting the necessity for a metric that gives equal weighting to all questions. Mean average percent error (MAPE) initially seems appealing but is disproportionally inflated for small ground truths and is biased towards overpredictions. Mean square error (MSE) and root mean square error (RMSE) are also commonly used but are very sensitive to outliers because they square the error. Intuitively, performing well on almost all questions and poorly on a small subset should score better than consistently being wrong. Among commonly-used metrics, sMAPE is the only metric that evaluates performance in relation to the distribution of ground truth elements [11]. There are two common definitions [15] for sMAPE, but we use the one that scales to $100 \%$. sMAPE is given by:</p>
<p>$$
\operatorname{sMAPE}=100 \cdot \frac{1}{n} \sum_{i=1}^{n} \frac{\left|y_{i}-\hat{y}<em i="i">{i}\right|}{\left|y</em>
$$}\right|+\left|\hat{y}_{i}\right|</p>
<p>where $y_{i}$ represents the actual values, $\hat{y}_{i}$ represents the predicted values, and $n$ is the number of observations. sMAPE is capped at $100 \%$, providing a finite scoring range. This feature is ideal for challenging tasks like ours, as it penalizes model responses that fail to produce an answer.</p>
<h2>A.2. Output Tokens</h2>
<p>To maximize the VLM's chance at success, we allocate a high number of output tokens to generate a rationale and output. This varies per model. We give 4000 tokens to InternVL2, 2000 tokens to Molmo, and 8192 tokens to Qwen2VL, following their max output lengths. For GPT4 o, we use the default of 4096 tokens.</p>
<h2>B. CAPTURE Dataset Creation Details</h2>
<p>The following expands upon Sec. 2.2. While FSC-147, a diverse counting dataset with manual annotations, is a strong starting point, it cannot immediately be adapted to our task. To make the task of amodal counting solvable, our dataset requires images with patterns in them. A person (or model) can infer how the pattern would continue and thus accurately predict the total number. For questions to be answerable, the dataset's images must be filtered down to represent patterns a model or person could recognize.</p>
<p>Our filtering process follows two stages. First, we prompt GPT-4o to determine whether the objects were arranged in a pattern. Second, if the model responded with "no", the images were immediately discarded. If the model output was "yes", the log probability of the token is stored. Empirically, we found that higher log probability values (i.e. higher confidence scores) corresponded to more welldefined patterns in the image. Thus, we use the log probabilities for filtering.</p>
<p>Specifically, let $P_{\text {yes }}$ be the log probability of the "yes" token and $T$ denote the threshold for determining how welldefined a pattern is. ${ }^{2}$ To filter the images based on pattern rigidity, we apply the following condition: $e^{P_{\mathrm{yes}}} \geq T$. This inequality yields 991 images from the original dataset ( $16.12 \%$ ). Next, we manually filter each of the selected images to ensure that they indeed contain patterns and feature a countable number of objects, excluding 34 images. Afterward, we manually place a "fair" occluding box in each image, i.e. a box that leaves sufficient portions of the pattern visible, such that the pattern can still be inferred from the unoccluded portions of the image. Occluding boxes were also chosen with varying positions and sizes in the image.</p>
<h2>C. Additional Analysis</h2>
<p>Here we provide additional experiments that attempt to either increase model performance on CAPTURE or dissect the reasons behind poor model performance. Chain-of-Thought inhibits model performance, while temperature backoff slightly improves performance. Additionally, we find that models struggle at counting just occluded objects, are overconfident in occluded settings, and are biased to predict specific numbers.</p>
<h2>C.1. Chain-of-Thought reduces model performance</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">CAPTURE $^{\text {real }}$</th>
<th style="text-align: center;">CAPTURE $^{\text {synthetic }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4o</td>
<td style="text-align: center;">14.75</td>
<td style="text-align: center;">9.71</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4o w/ CoT</td>
<td style="text-align: center;">14.94</td>
<td style="text-align: center;">7.73</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2</td>
<td style="text-align: center;">29.33</td>
<td style="text-align: center;">11.74</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2 w/ CoT</td>
<td style="text-align: center;">31.57</td>
<td style="text-align: center;">37.81</td>
</tr>
</tbody>
</table>
<p>Table 6. CoT experiments (metric: sMAPE).</p>
<p>During development, we experimented with several common strategies including CoT. In Tab. 6, we find that CoT reduces model performance except in the occluded synthetic scenario, most likely because the included examples are very similar to the test prompt.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Error (\%) ( $\downarrow$ )</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Real</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Synthetic</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Unoccluded</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Occluded</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Unoccluded</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Occluded</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">w/ backoff ( $\Delta$ )</td>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">w/ backoff ( $\Delta$ )</td>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">w/ backoff ( $\Delta$ )</td>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">w/ backoff ( $\Delta$ )</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4o</td>
<td style="text-align: center;">13.34</td>
<td style="text-align: center;">$12.57(-0.77)$</td>
<td style="text-align: center;">14.75</td>
<td style="text-align: center;">$14.39(-0.36)$</td>
<td style="text-align: center;">5.90</td>
<td style="text-align: center;">$5.93(+0.03)$</td>
<td style="text-align: center;">9.71</td>
<td style="text-align: center;">$9.23(-0.48)$</td>
</tr>
<tr>
<td style="text-align: center;">InternVL2</td>
<td style="text-align: center;">26.17</td>
<td style="text-align: center;">$27.09(+0.92)$</td>
<td style="text-align: center;">32.90</td>
<td style="text-align: center;">$32.37(-0.53)$</td>
<td style="text-align: center;">16.44</td>
<td style="text-align: center;">$15.59(-0.85)$</td>
<td style="text-align: center;">17.57</td>
<td style="text-align: center;">$16.24(-1.33)$</td>
</tr>
<tr>
<td style="text-align: center;">Molmo</td>
<td style="text-align: center;">25.90</td>
<td style="text-align: center;">$21.23(-4.67)$</td>
<td style="text-align: center;">32.49</td>
<td style="text-align: center;">$28.17(-4.32)$</td>
<td style="text-align: center;">8.40</td>
<td style="text-align: center;">$2.88(-5.52)$</td>
<td style="text-align: center;">17.73</td>
<td style="text-align: center;">$15.85(-1.88)$</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2VL</td>
<td style="text-align: center;">18.96</td>
<td style="text-align: center;">$19.40(+0.44)$</td>
<td style="text-align: center;">29.33</td>
<td style="text-align: center;">$28.47(-0.86)$</td>
<td style="text-align: center;">6.63</td>
<td style="text-align: center;">$6.66(+0.03)$</td>
<td style="text-align: center;">11.74</td>
<td style="text-align: center;">$11.51(-0.23)$</td>
</tr>
<tr>
<td style="text-align: center;">Avg. of 4 VLMs</td>
<td style="text-align: center;">21.09</td>
<td style="text-align: center;">$20.07(-1.02)$</td>
<td style="text-align: center;">27.37</td>
<td style="text-align: center;">$25.85(-1.52)$</td>
<td style="text-align: center;">9.34</td>
<td style="text-align: center;">$7.76(-1.58)$</td>
<td style="text-align: center;">14.19</td>
<td style="text-align: center;">$13.21(-0.98)$</td>
</tr>
</tbody>
</table>
<p>Table 7. Comparison of models on CAPTURE across four scenarios (CAPTURE ${ }^{\text {real }}$ vs. CAPTURE ${ }^{\text {synthetic }}$, Unoccluded vs. Occluded). "Original" indicates no backoff; "w/ backoff" indicates applying backoff, with $\Delta=$ (w/ backoff) - (Original). Negative $\Delta$ values indicate an improvement.</p>
<h2>C.2. Temperature backoff slightly improves model performance</h2>
<p>To improve VLM performance on CAPTURE, we address a trend we established during early testing. Most of the time, the VLM fails by reaching an incorrect answer. Sometimes, however, our benchmark can cause VLMs to produce a long and irrelevant response that strays from the original prompt, leading to the worst possible sMAPE score (100\%).</p>
<p>To reduce the number of skipped questions, we experiment with temperature backoff, which iteratively decreases the sampling temperature. Because the answer extractor can immediately identify an incoherent output, we can regenerate the response with a lower temperature to get the model to answer the task properly. Consistent with our findings, Peeperkorn et al. [32] also finds that lower temperatures increase coherence in VLMs, thereby enhancing their chances of maintaining relevance to the prompt. Therefore, temperature backoff gives VLMs a better chance of achieving higher scores. Each time the answer extractor returns an empty answer because the VLMs produced an incoherent answer, we reduce the temperature by 0.1 (starting from 1.0) until it reaches 0.0 , at which point the example is skipped.</p>
<p>Models perform slightly better with temperature backoff. We introduced temperature backoff to reduce model incoherence, and it performed fairly well. As shown in Tab. 7 (bottom), this method slightly improves performance across each model, resulting in an average error reduction of $5.78 \%$ in CAPTURE $^{\text {real }}$ and $5.45 \%$ in CAPTURE ${ }^{\text {synthetic }}$. Temperature backoff essentially allows the model to reattempt the question if it fails to respond to the prompt. Similar to previous results, positive results from reattempts highlight VLMs' weak reasoning abilities.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>| Model | Error (\%) [ $\downarrow$ ] |  |
| :-- | :--: | :--: |
|  | All Objects | Only Occluded |
| GPT-4o | 14.75 | $26.13(+11.38)$ |
| InternVL2 | 32.90 | $75.82(+42.92)$ |
| Molmo | 32.49 | $96.79(+64.30)$ |
| Qwen2VL | 29.33 | $32.89(+3.56)$ |
| Avg. of 4 VLMs | 27.37 | $57.91(+30.54)$ |</p>
<p>Table 8. VLM sMAPE for counting all objects and counting only the occluded objects in CAPTURE ${ }^{\text {real }}$. Metric: sMAPE (lower is better).</p>
<h2>C.3. Models struggle at counting just occluded objects</h2>
<p>We separately test whether models can count only the occluded objects (not including the visible objects) in an image. Here, as Tab. 8 demonstrates, the models perform especially poorly in this task, with high error rates across all models. Therefore, we can conclude that occlusion and counting are uniquely difficult for the VLMs, and that the drop in performance between unoccluded and occluded settings in Tab. 2 is likely due to a poor ability to count occluded objects.</p>
<h2>C.4. Models are overconfident in occluded settings</h2>
<p>We test the uncertainty with two different methods of obtaining confidence on Qwen2VL. In the first method, we prompt Qwen2VL for its confidence in the answer. For the second method, we generate 20 responses for every question in our VQA and calculate the confidence as the percentage of times the most common answer was generated. These results can be seen in Fig. 9 and Fig. 10 respectively. In both reliability curves, there is a slight trend that the model's confidence is negatively correlated with the error, which is the desired outcome. In CAPTURE ${ }^{\text {real }}$, how-</p>
<p>ever, the correlation is much stronger. While the models are somewhat calibrated (with generally lower confidence on higher-error examples, there are still outliers in prompted confidence for CAPTURE ${ }^{\text {real }}$ occluded and sampled confidence for CAPTURE ${ }^{\text {synthetic }}$ occluded. This indicates that not only do the models perform worse under occlusion, but they can also be overconfident.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. Reliability curve of prompting model for confidence vs. sMAPE.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10. Reliability curve of sampling model for confidence vs. sMAPE.</p>
<h2>C.5. Models are biased to predict specific numbers.</h2>
<p>To examine where models frequently err, we generated a confusion matrix for every model based on CAPTURE ${ }^{\text {synthetic }}$ results (shown in Appendix C.5). The y-axis represents the ground truth values and the x -axis represents the model's answers. We find that models often over-predict
numbers associated with common counts in real life: GPT40 tends to predict numbers like $8,9,10$, and 12 , which are all non-prime numbers (i.e. can be arranged into a grid) and common groupings of objects. For example, 12 is a common grouping (dozens) and allows arrangements into $3 \times 4$ or $2 \times 6$ grids. InternVL and Qwen2VL over-predict 5 and 10, aligning with how humans conceptualize numbers. Indeed, Coupland [12] found that numbers 5, 10, 20, and other round numbers appear disproportionally more in online texts. Molmo has no correlation with these factors, possibly due to its unique "point and count" ability.</p>
<h2>D. VLM Prompts</h2>
<p>We use a 100 -example validation set for each setting to select the best prompt, which we report below.</p>
<h2>Prompt for GPT-40 on CAPTURE ${ }^{\text {real }}$ unoccluded split.</h2>
<p>Count the exact number of [object] in the image. Assume the pattern of [object] continues behind any black box. Provide the total number of [object] as if the black box were not there.</p>
<h2>Prompt for InternVL2 on CAPTURE ${ }^{\text {real }}$ unoccluded split.</h2>
<p>Your task is to count objects in the image. First, state what the pattern is, then give your final count.</p>
<h2>Prompt for Molmo on CAPTURE ${ }^{\text {real }}$ unoccluded split.</h2>
<p>Count the exact number of [object] in the image. Only count [object] that are visible within the frame. If [object] are partially in the frame (i.e. if any part of [object] are visible), count it.</p>
<h2>Prompt for Qwen2VL on CAPTURE ${ }^{\text {real }}$ unoccluded split.</h2>
<p>Count the exact number of [object] in the image. Assume the pattern of [object] continues behind any black box. Provide the total number of [object] as if the black box were not there. Only count [object] that are visible within the frame (or would be visible without the occluding box). If [object] are partially in the frame (i.e. if any part of [object] are visible), count it. If the [object] would be partially in the frame without the occluding box, count it.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11. Confusion matrix: predicted vs. ground truth counts for CAPTURE ${ }^{\text {real }}$ 's occluded split.</p>
<h2>Prompt for GPT-4o, InternVL2, and Qwen2VL on CAPTURE ${ }^{\text {real }}$ occluded split.</h2>
<p>Count the exact number of [object] in the image. Assume the pattern of [object] continues behind any black box. Provide the total number of [object] as if the black box were not there. Only count [object] that are visible within the frame (or would be visible without the occluding box). If [object] are partially in the frame (i.e. if any part of [object] are visible), count it. If the [object] would be partially in the frame without the occluding box, count it. Molmo: Your task is to count objects in the image. Assume the pattern of [object] continues behind the black box. First, state what the pattern is, then give your final count.</p>
<h2>Prompt for Molmo on CAPTURE ${ }^{\text {real }}$ occluded split.</h2>
<p>Your task is to count objects in the image. Assume the pattern of [object] continues behind the black box. First, state what the pattern is, then give your final count.</p>
<h2>Prompt for GPT-4o on CAPTURE ${ }^{\text {realistic }}$ unoccluded split.</h2>
<p>Your task is to count objects in the image. First, state what the pattern is, then give your final count.</p>
<h1>Prompt for InternVL2 on CAPTURE ${ }^{\text {synthetic }}$ unoc-</h1>
<p>cluded split.</p>
<p>Count the exact number of [dot shape]s in the image. Only count [dot shape]s that are visible within the frame. If [dot shape]s are partially in the frame (i.e. if any part of [dot shape]s are visible), count it.</p>
<h2>Prompt for Molmo on CAPTURE ${ }^{\text {synthetic }}$ unoc-</h2>
<p>cluded split.</p>
<p>Count the exact number of [dot shape]s in the image. Only count [dot shape]s that are visible within the frame.</p>
<h2>Prompt for Qwen2VL on CAPTURE ${ }^{\text {synthetic }}$ unoc-</h2>
<p>cluded split.</p>
<p>Count the exact number of [dot shape]s in the image. Assume the pattern of [dot shape]s continues behind any black box. Provide the total number of [dot shape]s as if the black box were not there. Only count [dot shape]s that are visible within the frame (or would be visible without the occluding box). If [dot shape]s are partially in the frame (i.e. if any part of [dot shape]s are visible), count it. If the [dot shape]s would be partially in the frame without the occluding box, count it.</p>
<h2>Prompt for GPT-40 and Molmo on CAPTURE ${ }^{\text {synthetic }}$ occluded split.</h2>
<p>Your task is to count objects in the image. Assume the pattern of [dot shape]s continues behind the black box. First, state what the pattern is, then give your final count.</p>
<h2>Prompt for InternVL2 and Qwen2VL on CAPTURE ${ }^{\text {synthetic }}$ occluded split.</h2>
<p>Count the exact number of [dot shape]s in the image. Assume the pattern of [dot shape]s continues behind any black box. Provide the total number of [dot shape]s as if the black box were not there. Only count [dot shape]s that are visible within the frame (or would be visible without the occluding box). If [dot shape]s are partially in the frame (i.e. if any part of [dot shape]s are visible), count it. If the [dot shape]s would be partially in the frame without the occluding box, count it.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ We set $T=0.9999$ based on manual evaluation, finding it resulted in fewer false positives.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
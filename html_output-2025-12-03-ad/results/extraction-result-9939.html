<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9939 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9939</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9939</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-5d12dfd7278cb8da26f9fd1956cad3c15cea9863</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5d12dfd7278cb8da26f9fd1956cad3c15cea9863" target="_blank">WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> WildBench is introduced, an automated evaluation framework designed to benchmark large language models (LLMs) using challenging, real-world user queries and develops two metrics, WB-Reward and WB-Score, which are computable using advanced LLMs such as GPT-4-turbo.</p>
                <p><strong>Paper Abstract:</strong> We introduce WildBench, an automated evaluation framework designed to benchmark large language models (LLMs) using challenging, real-world user queries. WildBench consists of 1,024 tasks carefully selected from over one million human-chatbot conversation logs. For automated evaluation with WildBench, we have developed two metrics, WB-Reward and WB-Score, which are computable using advanced LLMs such as GPT-4-turbo. WildBench evaluation uses task-specific checklists to evaluate model outputs systematically and provides structured explanations that justify the scores and comparisons, resulting in more reliable and interpretable automatic judgments. WB-Reward employs fine-grained pairwise comparisons between model responses, generating five potential outcomes: much better, slightly better, slightly worse, much worse, or a tie. Unlike previous evaluations that employed a single baseline model, we selected three baseline models at varying performance levels to ensure a comprehensive pairwise evaluation. Additionally, we propose a simple method to mitigate length bias, by converting outcomes of ``slightly better/worse'' to ``tie'' if the winner response exceeds the loser one by more than $K$ characters. WB-Score evaluates the quality of model outputs individually, making it a fast and cost-efficient evaluation metric. WildBench results demonstrate a strong correlation with the human-voted Elo ratings from Chatbot Arena on hard tasks. Specifically, WB-Reward achieves a Pearson correlation of 0.98 with top-ranking models. Additionally, WB-Score reaches 0.95, surpassing both ArenaHard's 0.91 and AlpacaEval2.0's 0.89 for length-controlled win rates, as well as the 0.87 for regular win rates.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9939.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9939.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Length bias (LLM-as-judge)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Preference for longer outputs by LLM evaluators (length bias)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The phenomenon where LLMs used as judges tend to prefer longer model outputs, leading to biased win/tie decisions unless corrected; WildBench documents this bias and applies a length-penalty mitigation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Open-ended dialogue / instruction-following evaluation (WildBench tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4-Turbo (primary judge); also Claude-3 and others considered</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Chain-of-thought style prompts plus instance-specific checklists used by GPT-4-Turbo to compare responses pairwise (WB-Reward) or to score individually (WB-Score); length-penalty rule converts 'slightly better/worse' to 'tie' when winner is longer by > K characters.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Reference human judgments: Chatbot Arena large-scale online human preference votes (Hard-English split, May 20, 2024) used as ground-truth for correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson correlation between automatic metric and Chatbot Arena Elo; reported values relevant to length-bias discussion: WB-Reward P-Cor_top = 0.98, WB-Score = 0.95; ablation on length-penalty K values showed K=500 maximized correlation with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Unmitigated LLM judges favor longer responses, causing inflated 'slightly better' outcomes based on length rather than substantive quality; without correction this degrades alignment with human preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Cites prior work (Dubois et al., 2024) documenting the preference for longer outputs; WildBench implements a length-penalty (tie conversion when length difference > K) and reports that varying K (100,200,500,1000,∞) changes correlation, with K=500 best — indicating raw LLM judgments would be biased by length.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>When mitigated (length-penalty with K=500), WildBench metrics still achieve very strong correlation with human Elo (WB-Reward 0.98, WB-Score 0.95), showing the bias can be reduced effectively; also individual cases (similar-length models) show ranking not driven by length.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Sections 1 (Intro), 3.2 (Mitigating length bias), 4.1 (Are longer responses always better?), 4.3 (Length penalties).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9939.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9939.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-preference / self-selection bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM evaluators favoring their own generations (self-preference/self-selection bias)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The tendency for an LLM used as a judge to recognize and prefer outputs similar to or produced by itself, which can bias evaluation results; WildBench acknowledges this risk and uses checklists and mixed-judge signals to mitigate it.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Open-ended generation / pairwise model comparison</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4-Turbo as primary judge; experiments referenced using GPT-4, Claude 3 Opus, and Mistral-Large as judges</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Judges instructed with checklists and CoT-style prompts; evaluation aggregates comparisons against three baseline models to reduce bias.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human preference data from Chatbot Arena (used as comparison/ground-truth for correlations).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Correlation metrics to human Elo (Pearson, Spearman, Kendall) used to validate mitigations; no single numeric 'self-preference' measure reported in WildBench, but prior work is cited.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Unmitigated LLM-as-judge can prefer outputs similar to its own generation style, harming fairness and fidelity to human preference; this can distort relative rankings among systems.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>The paper cites Panickssery et al. (2024) documenting LLM judges favoring their own outputs; WildBench counters this by generating instance-specific checklists using multiple LLMs and by combining judgments against multiple baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>WildBench reports that using multiple LLM judges (GPT-4, Claude-3-Opus, Mistral-Large) produced very similar rankings (minimal influence), suggesting that with their checklist and baseline-mixing approach the self-preference problem can be largely mitigated in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Sections 3.1 (checklists), 3.2 (baseline LLMs), 4.3 (Do multiple LLMs as judges help?), Related Works (Panickssery et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9939.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9939.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ambiguity / interpretability loss without structured checklists</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Noisy, ambiguous, or uninterpretable judgments from LLM-as-judge without instance-specific checklists</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLM judgments can be noisy or hard to interpret if evaluators are not given a consistent, structured evaluation standard; WildBench introduces per-instance checklists and CoT-style analysis to restore interpretability and improve agreement with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Open-ended response evaluation (dialogue, writing, coding, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4-Turbo (used to create and follow per-instance checklists); checklists also produced by GPT-4-Turbo and Claude-3-Opus combination.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>For each test query, generate 5–10 interpretable checklist questions (combined from multiple LLMs and reviewed by humans) and prompt the judge to produce step-by-step (CoT) analyses before final decision.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human judges in Chatbot Arena provide preference votes without necessarily following the same structured checklist; WildBench uses Arena Elo as reference.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Ablation: WB-Score Pearson correlation with human preferences drops from 0.925 (with checklists) to 0.905 (without checklists) — direct evidence that lack of structure degrades agreement with humans.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Without structured checklists and stepwise reasoning prompts, LLM-as-judge evaluations become more ambiguous/noisy and less aligned with human judgments; interpretability of judgments (reasons) is reduced.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Quantified ablation: WB-Score correlation falls (P-Cor_all) from 0.925 to 0.905 when checklists are removed, indicating degraded fidelity to human preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>With checklists and CoT-style prompts, WildBench achieves high agreement with human Elo (WB-Reward 0.98, WB-Score 0.95), showing structured prompting can largely recover human-aligned judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Sections 3.1 (Instance-Specific Checklists) and 4.3 (Ablation on checklists).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9939.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9939.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Coarseness & trade-offs of pairwise LLM judging</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Coarseness of pairwise LLM-based comparisons and cost/efficiency trade-offs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pairwise comparisons using LLM judges can be coarse (few outcome categories) and more expensive; WildBench mitigates coarseness by using three baselines and fine-grained labels, and provides a faster individual-scoring alternative (WB-Score).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Pairwise comparison evaluation for open-ended outputs</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4-Turbo for pairwise WB-Reward comparisons (using labels A++, A+, A=B, B+, B++), with three different baseline models (GPT-4-Turbo-0429, Claude-3-Haiku, Llama-2-70B-chat) aggregated.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Structured checklist + CoT prompt; WB-Reward assigns granular reward values (+1,+0.5,0,-0.5,-1) per pair; length-penalty applied to adjust ties.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human pairwise voting (Chatbot Arena) used as comparison; Arena Elo derived from many human pairwise votes.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Correlation to human Arena Elo: WB-Reward Pearson correlations up to 0.98 (top models). WB-Score (individual scoring) P-Cor ~0.95.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Pairwise LLM judging can be coarse and costly relative to per-instance scoring; relying solely on pairwise LLM judgments without multiple baselines or fine-grained outcome labels risks missing nuance or produces noisy comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Paper notes 'Pairwise evaluations can be coarse' and justifies using three baselines and five outcome categories; also reports WB-Reward is 3–4× more expensive than WB-Score, indicating a practical cost trade-off versus human evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Despite coarseness/cost, WB-Reward with multiple baselines and checklist-guided CoT achieves very high correlation with human judgments (0.98), and WB-Score offers a cheaper, faster alternative (claimed ∼ $5 / 10 minutes) with still strong human agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Sections 3.2 (WB-Reward design), 3.3 (WB-Score), 4.3 (Pairwise vs individual trade-offs).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Length-controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators <em>(Rating: 2)</em></li>
                <li>LLM evaluators recognize and favor their own generations <em>(Rating: 2)</em></li>
                <li>Judging LLM-as-a-judge with MT-Bench and Chatbot Arena <em>(Rating: 2)</em></li>
                <li>AlpacaEval: An Automatic Evaluator of Instruction-Following Models <em>(Rating: 1)</em></li>
                <li>From live data to high-quality benchmarks: The Arena-Hard pipeline <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9939",
    "paper_id": "paper-5d12dfd7278cb8da26f9fd1956cad3c15cea9863",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "Length bias (LLM-as-judge)",
            "name_full": "Preference for longer outputs by LLM evaluators (length bias)",
            "brief_description": "The phenomenon where LLMs used as judges tend to prefer longer model outputs, leading to biased win/tie decisions unless corrected; WildBench documents this bias and applies a length-penalty mitigation.",
            "citation_title": "WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild",
            "mention_or_use": "use",
            "task_domain": "Open-ended dialogue / instruction-following evaluation (WildBench tasks)",
            "llm_judge_model": "GPT-4-Turbo (primary judge); also Claude-3 and others considered",
            "llm_judge_setup": "Chain-of-thought style prompts plus instance-specific checklists used by GPT-4-Turbo to compare responses pairwise (WB-Reward) or to score individually (WB-Score); length-penalty rule converts 'slightly better/worse' to 'tie' when winner is longer by &gt; K characters.",
            "human_evaluation_setup": "Reference human judgments: Chatbot Arena large-scale online human preference votes (Hard-English split, May 20, 2024) used as ground-truth for correlation.",
            "agreement_metric": "Pearson correlation between automatic metric and Chatbot Arena Elo; reported values relevant to length-bias discussion: WB-Reward P-Cor_top = 0.98, WB-Score = 0.95; ablation on length-penalty K values showed K=500 maximized correlation with human judgments.",
            "losses_identified": "Unmitigated LLM judges favor longer responses, causing inflated 'slightly better' outcomes based on length rather than substantive quality; without correction this degrades alignment with human preferences.",
            "examples_of_loss": "Cites prior work (Dubois et al., 2024) documenting the preference for longer outputs; WildBench implements a length-penalty (tie conversion when length difference &gt; K) and reports that varying K (100,200,500,1000,∞) changes correlation, with K=500 best — indicating raw LLM judgments would be biased by length.",
            "counterexamples_or_caveats": "When mitigated (length-penalty with K=500), WildBench metrics still achieve very strong correlation with human Elo (WB-Reward 0.98, WB-Score 0.95), showing the bias can be reduced effectively; also individual cases (similar-length models) show ranking not driven by length.",
            "paper_reference": "Sections 1 (Intro), 3.2 (Mitigating length bias), 4.1 (Are longer responses always better?), 4.3 (Length penalties).",
            "uuid": "e9939.0",
            "source_info": {
                "paper_title": "WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Self-preference / self-selection bias",
            "name_full": "LLM evaluators favoring their own generations (self-preference/self-selection bias)",
            "brief_description": "The tendency for an LLM used as a judge to recognize and prefer outputs similar to or produced by itself, which can bias evaluation results; WildBench acknowledges this risk and uses checklists and mixed-judge signals to mitigate it.",
            "citation_title": "WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild",
            "mention_or_use": "use",
            "task_domain": "Open-ended generation / pairwise model comparison",
            "llm_judge_model": "GPT-4-Turbo as primary judge; experiments referenced using GPT-4, Claude 3 Opus, and Mistral-Large as judges",
            "llm_judge_setup": "Judges instructed with checklists and CoT-style prompts; evaluation aggregates comparisons against three baseline models to reduce bias.",
            "human_evaluation_setup": "Human preference data from Chatbot Arena (used as comparison/ground-truth for correlations).",
            "agreement_metric": "Correlation metrics to human Elo (Pearson, Spearman, Kendall) used to validate mitigations; no single numeric 'self-preference' measure reported in WildBench, but prior work is cited.",
            "losses_identified": "Unmitigated LLM-as-judge can prefer outputs similar to its own generation style, harming fairness and fidelity to human preference; this can distort relative rankings among systems.",
            "examples_of_loss": "The paper cites Panickssery et al. (2024) documenting LLM judges favoring their own outputs; WildBench counters this by generating instance-specific checklists using multiple LLMs and by combining judgments against multiple baselines.",
            "counterexamples_or_caveats": "WildBench reports that using multiple LLM judges (GPT-4, Claude-3-Opus, Mistral-Large) produced very similar rankings (minimal influence), suggesting that with their checklist and baseline-mixing approach the self-preference problem can be largely mitigated in practice.",
            "paper_reference": "Sections 3.1 (checklists), 3.2 (baseline LLMs), 4.3 (Do multiple LLMs as judges help?), Related Works (Panickssery et al.).",
            "uuid": "e9939.1",
            "source_info": {
                "paper_title": "WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Ambiguity / interpretability loss without structured checklists",
            "name_full": "Noisy, ambiguous, or uninterpretable judgments from LLM-as-judge without instance-specific checklists",
            "brief_description": "LLM judgments can be noisy or hard to interpret if evaluators are not given a consistent, structured evaluation standard; WildBench introduces per-instance checklists and CoT-style analysis to restore interpretability and improve agreement with human judgments.",
            "citation_title": "WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild",
            "mention_or_use": "use",
            "task_domain": "Open-ended response evaluation (dialogue, writing, coding, etc.)",
            "llm_judge_model": "GPT-4-Turbo (used to create and follow per-instance checklists); checklists also produced by GPT-4-Turbo and Claude-3-Opus combination.",
            "llm_judge_setup": "For each test query, generate 5–10 interpretable checklist questions (combined from multiple LLMs and reviewed by humans) and prompt the judge to produce step-by-step (CoT) analyses before final decision.",
            "human_evaluation_setup": "Human judges in Chatbot Arena provide preference votes without necessarily following the same structured checklist; WildBench uses Arena Elo as reference.",
            "agreement_metric": "Ablation: WB-Score Pearson correlation with human preferences drops from 0.925 (with checklists) to 0.905 (without checklists) — direct evidence that lack of structure degrades agreement with humans.",
            "losses_identified": "Without structured checklists and stepwise reasoning prompts, LLM-as-judge evaluations become more ambiguous/noisy and less aligned with human judgments; interpretability of judgments (reasons) is reduced.",
            "examples_of_loss": "Quantified ablation: WB-Score correlation falls (P-Cor_all) from 0.925 to 0.905 when checklists are removed, indicating degraded fidelity to human preferences.",
            "counterexamples_or_caveats": "With checklists and CoT-style prompts, WildBench achieves high agreement with human Elo (WB-Reward 0.98, WB-Score 0.95), showing structured prompting can largely recover human-aligned judgments.",
            "paper_reference": "Sections 3.1 (Instance-Specific Checklists) and 4.3 (Ablation on checklists).",
            "uuid": "e9939.2",
            "source_info": {
                "paper_title": "WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Coarseness & trade-offs of pairwise LLM judging",
            "name_full": "Coarseness of pairwise LLM-based comparisons and cost/efficiency trade-offs",
            "brief_description": "Pairwise comparisons using LLM judges can be coarse (few outcome categories) and more expensive; WildBench mitigates coarseness by using three baselines and fine-grained labels, and provides a faster individual-scoring alternative (WB-Score).",
            "citation_title": "WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild",
            "mention_or_use": "use",
            "task_domain": "Pairwise comparison evaluation for open-ended outputs",
            "llm_judge_model": "GPT-4-Turbo for pairwise WB-Reward comparisons (using labels A++, A+, A=B, B+, B++), with three different baseline models (GPT-4-Turbo-0429, Claude-3-Haiku, Llama-2-70B-chat) aggregated.",
            "llm_judge_setup": "Structured checklist + CoT prompt; WB-Reward assigns granular reward values (+1,+0.5,0,-0.5,-1) per pair; length-penalty applied to adjust ties.",
            "human_evaluation_setup": "Human pairwise voting (Chatbot Arena) used as comparison; Arena Elo derived from many human pairwise votes.",
            "agreement_metric": "Correlation to human Arena Elo: WB-Reward Pearson correlations up to 0.98 (top models). WB-Score (individual scoring) P-Cor ~0.95.",
            "losses_identified": "Pairwise LLM judging can be coarse and costly relative to per-instance scoring; relying solely on pairwise LLM judgments without multiple baselines or fine-grained outcome labels risks missing nuance or produces noisy comparisons.",
            "examples_of_loss": "Paper notes 'Pairwise evaluations can be coarse' and justifies using three baselines and five outcome categories; also reports WB-Reward is 3–4× more expensive than WB-Score, indicating a practical cost trade-off versus human evaluations.",
            "counterexamples_or_caveats": "Despite coarseness/cost, WB-Reward with multiple baselines and checklist-guided CoT achieves very high correlation with human judgments (0.98), and WB-Score offers a cheaper, faster alternative (claimed ∼ $5 / 10 minutes) with still strong human agreement.",
            "paper_reference": "Sections 3.2 (WB-Reward design), 3.3 (WB-Score), 4.3 (Pairwise vs individual trade-offs).",
            "uuid": "e9939.3",
            "source_info": {
                "paper_title": "WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Length-controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators",
            "rating": 2
        },
        {
            "paper_title": "LLM evaluators recognize and favor their own generations",
            "rating": 2
        },
        {
            "paper_title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena",
            "rating": 2
        },
        {
            "paper_title": "AlpacaEval: An Automatic Evaluator of Instruction-Following Models",
            "rating": 1
        },
        {
            "paper_title": "From live data to high-quality benchmarks: The Arena-Hard pipeline",
            "rating": 1
        }
    ],
    "cost": 0.01579275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild</h1>
<p>Bill Yuchen Lin ${ }^{\circ}$<br>Yuntian Deng ${ }^{\text { }}$ Khyathi Chandu ${ }^{\text { }}$ Faeze Brahman ${ }^{\text { }}$ Abhilasha Ravichander ${ }^{\text {® }}$<br>Valentina Pyatkin ${ }^{\circ}$ Nouha Dziri ${ }^{\circ}$ Ronan Le Bras ${ }^{\circ}$ Yejin Choi ${ }^{\circ}{ }^{\circ}$<br>${ }^{\circ}$ Allen Institute for AI University of Washington<br>https://hf.co/spaces/allenai/WildBench</p>
<h4>Abstract</h4>
<p>We introduce WildBench, an automated evaluation framework designed to benchmark large language models (LLMs) using challenging, real-world user queries. WildBENCH consists of 1,024 examples carefully selected from over one million human-chatbot conversation logs. For automated evaluation with WILDBENCH, we have developed two metrics, WB-Reward and WB-Score, which are computable using advanced LLMs such as GPT-4-turbo. WILDBENCH evaluation uses taskspecific checklists to evaluate model outputs systematically and provides structured explanations that justify the scores and comparisons, resulting in more reliable and interpretable automatic judgments. WB-Reward employs fine-grained pairwise comparisons between model responses, generating five potential outcomes: much better, slightly better, slightly worse, much worse, or a tie. Unlike previous evaluations that employed a single baseline model, we selected three baseline models at varying performance levels to ensure a comprehensive pairwise evaluation. Additionally, we propose a simple method to mitigate length bias by converting outcomes of "slightly better/worse" to "tie" if the winner's response exceeds the loser's by more than $K$ characters. WB-Score evaluates the quality of model outputs individually, making it a fast and cost-efficient evaluation metric. WILDBENCH results demonstrate a strong correlation with the human-voted Elo ratings from Chatbot Arena on hard tasks. Specifically, WB-Reward achieves a Pearson correlation of 0.98 with top-ranking models. Additionally, WB-Score reaches 0.95 , surpassing both ArenaHard's 0.91 and AlpacaEval2.0's 0.89 for length-controlled win rates, as well as the 0.87 for regular win rates.</p>
<h2>1 INTRODUCTION</h2>
<p>Large language models (LLMs) have become integral to a wide range of real-world applications due to their strong generalization capabilities across diverse tasks. However, effectively evaluating their performance remains a challenging problem, particularly when striving for an automated and cost-effective solution. Traditional benchmarking datasets like MMLU (Li et al., 2023a) focus primarily on assessing the reasoning abilities of LLMs using multiple-choice questions, which fall short in evaluating the more open-ended problems that real-world users pose. Chatbot Arena (Chiang et al., 2024) provides an online platform where human preferences are collected to judge pairs of model outputs, subsequently ranking LLMs using Elo ratings. While this human-based evaluation method offers valuable insights into user preferences, it has notable limitations, such as high labor costs, the inability to deliver real-time results, a lack of data transparency, and the challenge of fairly evaluating all models with the same data.
Several automated benchmarks such as AlpacaEval (Li et al., 2023b), MT-bench (Zheng et al., 2024), and ArenaHard (Li et al., 2024) employ advanced LLMs like GPT-4-Turbo to assess the quality of model responses. Comparative analyses of these benchmarks are presented in Table 1 and Figure 3. These existing benchmarks exhibit significant shortcomings in task composition and skill coverage, particularly in mirroring the natural distribution of real-world user tasks. MT-bench, comprising</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Example tasks sampled from AlpacaEval (Li et al., 2023b) and WildBench. Tasks in WildBENCH are more diverse and challenging, which are collected from real users in the wild. Complex real-user tasks usually have multiple constraints and require higher-order reasoning skills, which are well represented in WildBENCH.
only 80 hand-crafted examples, lacks sufficient breadth for a comprehensive evaluation. Meanwhile, AlpacaEval, with 805 tasks derived from multiple alignment datasets, includes relatively simple tasks, such as "What is the capital of Australia?" and suffers from low task diversity; for instance, over 20 tasks redundantly assess recipe generation skills (e.g., "can you provide a recipe for ...?"). We show a few examples in Figure 1 to illustrate the differences between AlpacaEval and our WildBENCH.</p>
<p>AlpacaEval mostly focuses on information-seeking tasks, containing merely 6\% coding and 3\% mathematics tasks. Conversely, ArenaHard, sampling 500 tasks from ChatbotArena, displays an excessive concentration on coding and debugging tasks, accounting for over $57 \%$ of its content. Most existing benchmarks do not sufficiently challenge the models with the varied and unexpected nature of user inquiries in practical settings, thus limiting their overall effectiveness in providing a holistic evaluation. This issue highlights the necessity for more comprehensive benchmarks that can better simulate the wide range of tasks from real users.</p>
<p>In this paper, we introduce WildBENCH, an automated evaluation framework designed for assessing LLMs using complex tasks from real-world users. The examples in WildBENCH are periodically updated, with the current version (V2) comprising 1,024 tasks carefully curated from real user-chatbot dialogs provided by the AI2's WildChat project (Zhao et al., 2024). We engage multiple advanced LLMs to process a filtered selection from WildChat, tasking them with the analysis of the requisite knowledge and skills for each task and subsequently labeling the difficulty level. Tasks considered as easy by all models are excluded. We ensure the distribution of tasks mirrors the original WildChat data, such that the task distribution of WildBENCH is still natural (Figure 3). Additionally, all finalized tasks undergo manual review. Further details are provided in Section 2.</p>
<p>As shown in Figure 1, WildBENCH presents a significantly harder challenge due to the complexity, depth, and realism of the tasks involved. WildBENCH is sourced from real-world user interactions and has been carefully curated to ensure diversity and challenge. The tasks in WildBENCH typically demand higher-order reasoning, such as writing and/or debugging code with specific constraints, creative writing with multiple constraints on the style and content, or designing a software system with complex requirements. These tasks often require critical thinking, creativity, and technical expertise, making WildBENCH substantially more challenging than AlpacaEval, where simpler, factual, or surface-level tasks dominate.</p>
<p>WildBENCH evaluation is illustrated in Figure 4. To design a reliable automatic evaluation, we employ two key designs for using LLMs as judges. Drawing inspiration from how humans evaluate responses to open-ended questions, we develop task-specific checklists. These checklists guide LLMs in generating consistent and reliable judgments, with each checklist comprising questions focused on specific criteria. Similar to the zero-shot Chain-of-Thoughts (CoT) prompting (Kojima et al., 2022), we prompt LLMs to provide step-by-step, structured analyses of each LLM response. This method encourages a detailed, fine-grained evaluation process, culminating in a well-justified final decision.</p>
<p>We employ two primary metrics: WB-Reward for pairwise comparisons and WB-Score for individual scoring. WB-Reward is based on pairwise comparisons between LLMs, with five possible outcomes: "A is much/slightly better/worse than B" or "Tie." Notably, we used three baseline models to compare with each testing model instead of using a single baseline model, as most prior works do. This approach provides a more comprehensive assessment based on different levels of model performance.</p>
<p>WB-Score measures the quality of each model’s generation individually, offering a quicker and more cost-effective evaluation. To mitigate the bias towards longer outputs, a common issue in LLM-as-a-judge evaluations <em>Dubois et al. (2024)</em>, we introduced a simple length-penalty method, converting slight wins/losses to ties when the winner’s output is significantly longer than the loser’s.</p>
<p>Both metrics have demonstrated strong correlations with human judgments, evidenced by a Pearson correlation of 0.98 for WB-Reward and 0.95 for WB-Score against the human-voted Elo rating from Chatbot Arena on the top-ranking models. These scores significantly surpass other benchmarks, such as ArenaHard<em>Li et al. (2024)</em>’s 0.91 and AlpacaEval2.0’s 0.87 (0.89 for the length-controlled version) <em>Li et al. (2023b); Dubois et al. (2024)</em>, validating WILDBENCH’s effectiveness and alignment with human-based evaluation. More details are shown in Table 3 in Section 4.</p>
<h1>2 WILDBENCH DATA CURATION</h1>
<p>In this section, we describe the data curation process for the tasks used to evaluate LLMs in WILDBENCH. Our goal is to ensure that the selected tasks not only represent real-world use cases but are also challenging enough to distinguish the varying capabilities of LLMs.</p>
<p>Table 1: Statistical comparison of LLM alignment benchmarks. Length are in characters.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>#Tasks</th>
<th>#Turns</th>
<th>ChatHistory</th>
<th>QueryLen</th>
<th>PromptLen</th>
<th>RealUser</th>
<th>TaskTag</th>
<th>Evaluation</th>
</tr>
</thead>
<tbody>
<tr>
<td>MT-Bench</td>
<td>80</td>
<td>2</td>
<td>✔Dynamic</td>
<td>202.2</td>
<td>Dynamic</td>
<td>✖</td>
<td>✔</td>
<td>Score</td>
</tr>
<tr>
<td>AlpacaEval</td>
<td>805</td>
<td>1</td>
<td>✖</td>
<td>164.9</td>
<td>164.9</td>
<td>✖</td>
<td>✖</td>
<td>Pair (ref=1)</td>
</tr>
<tr>
<td>ArenaHard</td>
<td>500</td>
<td>1</td>
<td>✖</td>
<td>406.4</td>
<td>406.4</td>
<td>✔</td>
<td>✖</td>
<td>Pair (ref=1)</td>
</tr>
<tr>
<td>WILDBENCH</td>
<td>1,024</td>
<td>$\leq$ 5</td>
<td>✔Static</td>
<td>978.5</td>
<td>3402.1</td>
<td>✔✔</td>
<td>✔</td>
<td>Score+Pair (ref=3)</td>
</tr>
</tbody>
</table>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Distribution of query lengths in AlpacaEval, ArenaHard, and WildBench.</p>
<h2>2.1 Mining Challenging Tasks from WildChat</h2>
<p>We sourced tasks from the WildChat dataset <em>Zhao et al. (2024)</em>, which comprises one million human-chatbot conversations from real users. This dataset is particularly suited for conversion into an evaluation benchmark because it contains a diverse array of tasks that users expect LLMs to perform, such as writing assistance, coding, mathematics, data analysis, role playing, and planning.</p>
<p>Basic filtering. To control the quality and diversity of the selected tasks, we applied several filtering steps. First, we removed user queries that were either too short (less than 10 tokens) or excessively long (more than 3,000 tokens). We also excluded conversations with more than five user-chatbot turns to maintain focus and coherence in the tasks, as conversations exceeding five turns tend to contain multiple topics. Furthermore, we focused on English data and filtered out non-English tasks. Since our focus is more on evaluating the capabilities of LLMs rather than content moderation, we also removed toxic conversations. To ensure task diversity, we used sentence embeddings from SentenceBERT <em>Reimers &amp; Gurevych (2019)</em> to calculate the cosine similarity between queries, discarding those with a high similarity score above 0.9. The threshold is determined by manual inspection. Lastly, to further enhance task diversity, we used a diverse user pool by retaining only the last conversation for each unique device, thus removing tasks from the same user that might require similar underlying skills.</p>
<p>Difficulty annotation. To identify challenging tasks that can distinguish the performance of different LLMs, we used GPT-4-Turbo <em>OpenAI (2023)</em>, Claude-3-Sonnet, and Opus <em>Anthropic (2024)</em> to</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Distribution of task categories in AlpacaEval, ArenaHard, and WildBench.
analyze the required background knowledge and reasoning capabilities for each task. These models assigned a difficulty rating on a five-point scale (from "very easy" to "very hard"). Tasks rated as "very easy" or "easy" by all models were excluded. From the remaining pool, we randomly sampled 1,500 tasks to ensure that the distribution of task categories is similar to the original dataset.</p>
<p>Human annotation. To improve the quality of selected tasks, human annotation was used for quality control. We first used GPT-4-Turbo to summarize the intent of each query. These summaries were then used to help human reviewers remove nonsensical tasks. Finally, we retained 1,024 tasks for WildBench. We also manually reviewed the tasks to ensure that they were challenging and diverse, covering a wide range of task categories. For the checklist questions, we verified that they were clear, interpretable, and relevant to the evaluation of LLM responses.</p>
<p>Dynamic updates and data leakage prevention. WildBENCH is designed to be a dynamic benchmark that is updated regularly to reflect new types of user interactions. In fact, we have already released two versions of the benchmark (V1 in 2024 March and V2 in 2024 May), with similar curation process but on different iterations of WildChat data. To prevent potential data leakage for LLMs that use WildChat as part of their training or alignment, we coordinated with the WildChat team to ensure that the tasks we sample will not be publicly available in the WildChat dataset.</p>
<h1>2.2 WildBench Statistics</h1>
<p>To better understand the composition of our evaluation, we analyze basic statistics and task categories.
Basic statistics. Table 1 compares the statistics of WildBench to existing benchmarks AlpacaEval (Li et al., 2023b; Dubois et al., 2024), MT-Bench (Zheng et al., 2024), and ArenaHard (Li et al., 2024). Among these benchmarks, only ArenaHard and WildBench are sourced from user queries in the wild ("RealUser"), rather than being curated by experts or through crowdsourcing. The difference between ArenaHard and our WildBench is that our data distribution aligns with real users' task categories, rather than overly focusing on coding and debugging as ArenaHard does.</p>
<p>Long-context tasks. WildBench includes conversation histories of up to four turns per conversation, reflecting complex and extended user interactions that are facilitated by recent advancements in LLMs, with over 20\% of conversations having more than two or more turns as shown in Figure 8. Additionally, as shown in Figure 2, WildBENCH has longer query lengths, attributable to the extensive context provided by real user interactions captured in the dataset. This is because that GPT-4-Turbo, one of the chatbots behind WildChat, supports up to 128 K context tokens and 4 K output tokens. This capability exemplifies the importance of a dynamic, in-the-wild benchmark: as models evolve, they unlock new user applications. Thanks to these realistic user activities, WildBENCH is a more suitable benchmark for testing the long-context problem solving abilities of LLMs.</p>
<p>Task categories. To enable a fine-grained analysis of LLM capabilities across varied tasks, we categorize the tasks into 12 categories based on previous analysis of ShareGPT queries (Ouyang et al., 2023) and our intent annotation of the tasks. Detailed descriptions about the 12 task categories are shown in Appendix A. The distribution of the task categories is shown in Figure 3. In this figure, we also compare to AlpacaEval and ArenaHard. Notably, WildBENCH is more balanced compared to AlpacaEval and ArenaHard, which have over 50\% of their tasks in Information seeking and Coding \&amp; Debugging categories, respectively.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Evaluation framework for WILDBENCH. There are two metrics: WB-Score for individual evaluation and WB-Reward for pairwise evaluation. The checklist is used to guide the evaluation process. The length penalty is used to mitigate the length bias. WB-Reward and WB-Score both have strong correlations with human-based ranking of LLMs on Chatbot Arena.</p>
<h1>3 Automatic Evaluation with WildBENCH</h1>
<p>In this section, we introduce the evaluation process of LLMs using WILDBENCH. We first explain how we generate a checklist for each test query to enhance interpretability and reduce evaluation ambiguity in WILDBENCH. Then, we introduce two automatic metrics: WILDBENCH-Score and WILDBENCH-Reward. Finally, we discuss how we mitigate the length bias in the evaluation process.</p>
<h3>3.1 Instance-Specific Checklists</h3>
<p>Powerful LLMs have been widely used as judges to evaluate the quality of LLM outputs in many automatic evaluation methods, such as AlpacaEval (Li et al., 2023b). However, even asking humans to judge which of the given two model outputs is better can be subjective and ambiguous. Moreover, such judgements provide limited information about the quality of the models. Without a constant, interpretable, and comprehensive evaluation standard, the results can be noisy and hard to interpret.</p>
<p>To address this issue, we generate a checklist for each test query in WILDBENCH to comprehensively evaluate the responses of different models. The checklist consists of 5-10 questions that are designed to be interpretable and easy to verify. We combine the responses of GPT-4-Turbo and Claude-3-Opus to finalize the checklists, thereby mitigating the bias of using a single LLM as the evaluator. These checklists have been manually reviewed and are used as part of the prompts for LLM judges to evaluate the responses of different models. An example of the checklist can be found in Figure 4. Taking the G20 example in Figure 1, here is a subset of checklist questions for the task:</p>
<h2>Example checklist for the G20 task example in Figure 1.</h2>
<p>$\checkmark$ Does the essay contain more than 1200 words as requested by the user?
$\checkmark$ Is the language of the essay beautiful and poetic, incorporating extensive vocabulary as specified?
$\checkmark$ Does the essay include a significant amount of factual and empirical data related to the impact of the G20 summit on the global economy, trade, and development?
$\checkmark$ Are there references to the role of young people in shaping the future of the world within the context of the G20 summit?
$\checkmark$ Does the essay include ancient Indian historical references as requested by the user?
$\checkmark$ Is the essay structured in a clear and logical manner, facilitating an easy understanding of the discussed topics?</p>
<h3>3.2 Pairwise Evaluation with WB-Reward Metric</h3>
<p>WB-Reward is based on pairwise evaluation, which uses a GPT-4-Turbo judge to compare the responses of two LLMs to determine which one performs better on a given task, using a structured</p>
<p>checklist to guide the comparison. This metric provides straightforward comparisons among models and the intermediate outcomes of win/lose rates are easy to interpret.</p>
<p>Step-by-step evaluation process. In Figure 4, we detail the step-by-step evaluation process for pairwise comparison. First, we provide a chain of evaluation questions to guide the LLM judge to analyze the user query and the conversation history. The LLM then evaluates the two responses and also analyze where and why one is better than the other. Finally, we ask the LLM to make a final judgment on which response is better and why. This method is inspired by the evaluation process in human evaluation, where human judges are asked to provide detailed feedback on the quality of the responses before making a final decision. The full evaluation prompt can be found at Appendix D</p>
<p>WB-Reward metric. To compute the WB-Reward for a test model X against a baseline model Y, we assign rewards based on the comparison result: +1 if X is much better than $\mathrm{Y},+0.5$ if X is slightly better than Y, 0 for a tie, -0.5 for X is slightly worse than Y , and -1 for X is much worse than Y .</p>
<p>Baseline LLMs for pairwise evaluation. Using a single baseline model for pairwise evaluation can lead to noisy and biased evaluations. To mitigate this issue, we use three baseline models (GPT-4-Turbo-0429, Claude-3-Haiku, and Llama-2-70B-chat (Touvron et al., 2023)) to compute the rewards for each model. Our metric WB-Reward (Mix) is the average of the rewards from these three baselines on 1024 examples, providing a more robust performance evaluation on WILEBENCH.</p>
<p>Mitigating length bias with a margin for ties. Previous studies have shown that LLM judges tend to prefer longer responses (Dubois et al., 2024). To mitigate this bias, we propose a simple and intuitive length penalty method. If the winning response is longer than the losing one by a certain threshold ( K characters), we convert Slightly Win/Slightly Lose to a Tie. $K$ can be customized via our leaderboard web-page for personalized configuration. Setting $K=\infty$ will disable the length penalty. We designed this feature to support a more personalized and flexible leaderboard. For example, users who prefer shorter and more concise outputs can set a smaller K if they do not prioritize correlating perfectly with the general human-based model rankings on ChatbotArena. This choice allows for a customized leaderboard experience depending on user preferences.</p>
<h1>3.3 IndividuAl Evaluation with WB-Score Metric</h1>
<p>Although pairwise evaluation provides a direct comparison between LLMs, it is usually more expensive and time-consuming than grading each individual LLM generation. To individually evaluate the performance of each model on WILDBENCH, we prompt GPT-4-Turbo to assign a score from 1 to 10 for each model's response. The full evaluation prompt can be found at Appendix E.</p>
<p>Score definition. To ensure a stable and consistent evaluation, we ask GPT-4-Turbo to evaluate the quality of each response based on the checklist and provide detailed strengths and weakness of each output before giving a score from 1 to 10 . The scores are defined as follows:</p>
<ul>
<li>Score 1-2: The response is very poor and does not make sense at all.</li>
<li>Score 3-4: The response is poor and does not help the user solve the problem meaningfully.</li>
<li>Score 5-6: The response is fair but has issues (e.g., factual errors, hallucinations, missing key information).</li>
<li>Score 7-8: The response is good but could be improved.</li>
<li>Score 9-10: The response is perfect and provides helpful information to solve the problem.</li>
</ul>
<p>Score rescaling. The WILDBENCH-Score is calculated as the average of the scores on all examples tested, where each score is first subtracted by 5 and then multiplied by 2 (i.e., $S^{\prime}=(S-5) \times 2$ ). A score of 5 represents a borderline acceptable response, so this rescaling can help to better differentiate the performance of models that can effectively solve the tasks.</p>
<h2>4 ReSults \&amp; ANALYSIS</h2>
<p>We analyze the performance of different models on WILDBENCH. We first present the leaderboard analysis, then examine the length bias issue in the evaluation process, and finally discuss the correlation between WILDBENCH-Score and ChatbotArena Elo rating.</p>
<p>Leaderboard features. In Table 2, we present a subset of the results from our live leaderboard demo. For the most up-to-date results and more interactive features, such as customizing length penalties and viewing the detailed task-wise performance of each model, please refer to our live leaderboard. Our</p>
<p>Table 2: Evaluation results (subset) of LLMs using WildBENCH and other benchmarks. Please refer to Figure 6-7 and demo website to view and interact with the full results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Model names</th>
<th style="text-align: center;">WB-Reward (no length penalty)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">WB- <br> Score</th>
<th style="text-align: center;">Arena <br> Elo</th>
<th style="text-align: center;">Arena- <br> Hard</th>
<th style="text-align: center;">Alpaca <br> LC</th>
<th style="text-align: center;">$\begin{aligned} &amp; \text { Eval2 } \ &amp; \text { WR } \end{aligned}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mix $\triangleleft$ GPT4T $\triangleleft$ Haiku $\triangleleft$ Llama2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">GPT-4o-0513 1</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">46.3</td>
<td style="text-align: center;">59.3</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">1293</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">51.3</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">GPT-4-Turbo-0409 2</td>
<td style="text-align: center;">34.6</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">45.3</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">1251</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">46.1</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">GPT-4-Turbo-0125 3</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">$-4.4$</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">1239</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Gemini-1.5-Pro 4</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">$-4.4$</td>
<td style="text-align: center;">37.9</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">55.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Llama-3-70B-Inst</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">$-19$</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">60.4</td>
<td style="text-align: center;">1213</td>
<td style="text-align: center;">41.1</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">33.2</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Claude 3 Opus 5</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">$-20.4$</td>
<td style="text-align: center;">34.3</td>
<td style="text-align: center;">46.3</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">1232</td>
<td style="text-align: center;">60.4</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">29.1</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">Gemini-1.5-Flash 6</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">$-16.6$</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">53.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">Yi-1.5-34B-Chat</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">$-18.3$</td>
<td style="text-align: center;">24.1</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">Llama3-Inst-8B-SimPO</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">$-22.5$</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">53.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;">40.5</td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">Claude 3 Sonnet 7</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">$-31.6$</td>
<td style="text-align: center;">19.4</td>
<td style="text-align: center;">33.9</td>
<td style="text-align: center;">55.5</td>
<td style="text-align: center;">1187</td>
<td style="text-align: center;">46.8</td>
<td style="text-align: center;">34.9</td>
<td style="text-align: center;">25.6</td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">Qwen1.5-72B-Chat</td>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;">$-34.8$</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">1143</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">26.5</td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">Command-R-Plus 8</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">$-36.3$</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">1155</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">* Claude 3 Haiku 9</td>
<td style="text-align: center;">$-8.5$</td>
<td style="text-align: center;">$-46.9$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">1169</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">Mistral-Large 10</td>
<td style="text-align: center;">$-10.5$</td>
<td style="text-align: center;">$-48.1$</td>
<td style="text-align: center;">$-4$</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">54.2</td>
<td style="text-align: center;">1158</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">21.4</td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">StarlingLM-7B-beta</td>
<td style="text-align: center;">$-11.9$</td>
<td style="text-align: center;">$-48.7$</td>
<td style="text-align: center;">$-5$</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">46.8</td>
<td style="text-align: center;">1111</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">Llama-3-8B-Inst</td>
<td style="text-align: center;">$-14.6$</td>
<td style="text-align: center;">$-49.8$</td>
<td style="text-align: center;">$-9.7$</td>
<td style="text-align: center;">15.7</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">1144</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">22.6</td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">Command-R 11</td>
<td style="text-align: center;">$-16$</td>
<td style="text-align: center;">$-48.4$</td>
<td style="text-align: center;">$-12.7$</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">1106</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">26</td>
<td style="text-align: center;">Mixtral-8x7B-Inst</td>
<td style="text-align: center;">$-18.8$</td>
<td style="text-align: center;">$-53.4$</td>
<td style="text-align: center;">$-13.5$</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">1114</td>
<td style="text-align: center;">23.4</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">18.3</td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">DBRX Inst</td>
<td style="text-align: center;">$-21.6$</td>
<td style="text-align: center;">$-57.3$</td>
<td style="text-align: center;">$-16.3$</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">48.9</td>
<td style="text-align: center;">1106</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">18.4</td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">Yi-1.5-6B-Chat</td>
<td style="text-align: center;">$-24.3$</td>
<td style="text-align: center;">$-55$</td>
<td style="text-align: center;">$-19.9$</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">30</td>
<td style="text-align: center;">Mistral-7B-Inst-v0.2</td>
<td style="text-align: center;">$-25$</td>
<td style="text-align: center;">$-58.1$</td>
<td style="text-align: center;">$-22.4$</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">1071</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">17.1</td>
<td style="text-align: center;">14.7</td>
</tr>
<tr>
<td style="text-align: center;">32</td>
<td style="text-align: center;">Tulu-2-dpo-70b</td>
<td style="text-align: center;">$-25.4$</td>
<td style="text-align: center;">$-59.3$</td>
<td style="text-align: center;">$-20.3$</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">45.2</td>
<td style="text-align: center;">1099</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">16.0</td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">* Llama-2-70B-chat</td>
<td style="text-align: center;">$-26.8$</td>
<td style="text-align: center;">$-56.9$</td>
<td style="text-align: center;">$-23.6$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">1070</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">13.9</td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">Qwen1.5-7B-Chat</td>
<td style="text-align: center;">$-27$</td>
<td style="text-align: center;">$-57.7$</td>
<td style="text-align: center;">$-23$</td>
<td style="text-align: center;">$-0.2$</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">1059</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">11.8</td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">Phi-3-medium-128k</td>
<td style="text-align: center;">$-33.3$</td>
<td style="text-align: center;">$-66.4$</td>
<td style="text-align: center;">$-30$</td>
<td style="text-align: center;">$-3.6$</td>
<td style="text-align: center;">42.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">36</td>
<td style="text-align: center;">GPT-3.5-turbo-0125</td>
<td style="text-align: center;">$-33.5$</td>
<td style="text-align: center;">$-66.3$</td>
<td style="text-align: center;">$-30$</td>
<td style="text-align: center;">$-4.1$</td>
<td style="text-align: center;">42.1</td>
<td style="text-align: center;">1105</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">38</td>
<td style="text-align: center;">Llama-2-7B-chat</td>
<td style="text-align: center;">$-48$</td>
<td style="text-align: center;">$-71.8$</td>
<td style="text-align: center;">$-44.6$</td>
<td style="text-align: center;">$-27.8$</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">1012</td>
<td style="text-align: center;">4.6</td>
<td style="text-align: center;">5.4</td>
<td style="text-align: center;">5.0</td>
</tr>
<tr>
<td style="text-align: center;">39</td>
<td style="text-align: center;">Gemma-7B-it</td>
<td style="text-align: center;">$-57$</td>
<td style="text-align: center;">$-78.4$</td>
<td style="text-align: center;">$-55.8$</td>
<td style="text-align: center;">$-36.8$</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">1047</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">6.9</td>
</tr>
<tr>
<td style="text-align: center;">40</td>
<td style="text-align: center;">Gemma-2B-it</td>
<td style="text-align: center;">$-74.1$</td>
<td style="text-align: center;">$-87.8$</td>
<td style="text-align: center;">$-73.6$</td>
<td style="text-align: center;">$-60.8$</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">980</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">5.4</td>
<td style="text-align: center;">3.4</td>
</tr>
</tbody>
</table>
<p>live leaderboard also supports exploring data and comparing model outputs side by side to understand the strengths and weaknesses of each model.</p>
<p>By using three baseline models of varying performance levels (GPT-4-Turbo $&gt;$ Claude 3 Haiku $&gt;$ Llama-2-70B-chat), we observe that the tested models can be naturally grouped into three tiers based on their performance. Tier 1 models outperform Claude 3 Haiku, Tier 2 models outperform Llama-2-70B-chat but are worse than Claude 3 Haiku, and Tier 3 models are worse than Llama-2-70B-chat.</p>
<h1>4.1 Leaderboard Analysis</h1>
<p>Where are the gaps between models? A unique feature of the WILDBENCH leaderboard is the ability to compare models across different task categories, which enables us to identify the strengths and weaknesses of each model on different types of tasks. In Figure 5, we select a set of popular models for analysis: Llama-3-8B-Inst (Meta, 2023), Llama-3-8B-Inst-SimPO (Meng et al., 2024b), Yi-1.5-34B-chat (AI et al., 2024), Llama-3-70B-Inst, GPT-4-Turbo-0409, and Claude 3 Opus. We show their performance in WB-Score across five task categories (merged from the 12 categories shown in Figure 3). Larger models like GPT-4-Turbo-0409 and Claude 3 Opus perform well across all task categories, while open LLMs like Llama-3-8B-Inst and Yi-1.5-34B-chat show weaker performance on coding and math-related tasks.</p>
<p>Will an 8B model outperform a 70B model? On the AlpacaEval-2.0 leaderboard, Llama-3-8B-Inst-SimPO (LC=44.7\%) significantly outperforms Llama-3-70B-Inst (LC=34.4\%) (Meng et al., 2024a), which is surprising and differs from our results. As shown in both Table 2 and Figure 5, our results indicate that Llama-3-8B-Inst-SimPO is generally still worse than Yi-34B-chat and Llama-3-70B-Inst. However, on information-seeking and creative tasks, Llama-3-8B-Inst-SimPO performs comparably to Llama-3-70B-Inst. Thus, we believe AlpacaEval's evaluation results underestimate the performance of Llama-3-70B-Inst due to task selection bias in addition to the weakness of their evaluation prompting method. While the performance of Llama-3-8B-Inst-SimPO is not as good as it</p>
<p>Table 3: Correlation with Chatbot ArenaElo Elo (Hard-En-240520) of alignment benchmarks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">P- $\operatorname{Cor}_{\text {top }}$</th>
<th style="text-align: center;">P- $\operatorname{Cor}_{\text {all }}$</th>
<th style="text-align: center;">S- $\operatorname{Cor}_{\text {all }}$</th>
<th style="text-align: center;">K- $\operatorname{Cor}_{\text {all }}$</th>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">P- $\operatorname{Cor}_{\text {top }}$</th>
<th style="text-align: center;">P- $\operatorname{Cor}_{\text {all }}$</th>
<th style="text-align: center;">S- $\operatorname{Cor}_{\text {all }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ArenaElo (Hard-En)</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">Avg Length</td>
<td style="text-align: center;">0.472</td>
<td style="text-align: center;">0.554</td>
<td style="text-align: center;">0.376</td>
</tr>
<tr>
<td style="text-align: center;">Arena-Hard</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.925</td>
<td style="text-align: center;">0.965</td>
<td style="text-align: center;">0.890</td>
<td style="text-align: center;">WB-Reward ${ }_{\text {Bam }}^{\text {Bama }}$</td>
<td style="text-align: center;">0.976</td>
<td style="text-align: center;">0.965</td>
<td style="text-align: center;">0.965</td>
</tr>
<tr>
<td style="text-align: center;">AlpacaEval2-LC</td>
<td style="text-align: center;">0.892</td>
<td style="text-align: center;">0.951</td>
<td style="text-align: center;">0.924</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">WB-Reward ${ }_{\text {All }}^{\text {Bab }}$</td>
<td style="text-align: center;">0.974</td>
<td style="text-align: center;">0.961</td>
<td style="text-align: center;">0.965</td>
</tr>
<tr>
<td style="text-align: center;">AlpacaEval2</td>
<td style="text-align: center;">0.865</td>
<td style="text-align: center;">0.952</td>
<td style="text-align: center;">0.960</td>
<td style="text-align: center;">0.868</td>
<td style="text-align: center;">WB-Reward ${ }_{\text {Bam }}^{\text {Baka }}$</td>
<td style="text-align: center;">0.985</td>
<td style="text-align: center;">0.974</td>
<td style="text-align: center;">0.982</td>
</tr>
<tr>
<td style="text-align: center;">WB-Score</td>
<td style="text-align: center;">0.955</td>
<td style="text-align: center;">0.940</td>
<td style="text-align: center;">0.943</td>
<td style="text-align: center;">0.846</td>
<td style="text-align: center;">WB-Reward ${ }_{\text {Bam }}^{\text {Bama }}$</td>
<td style="text-align: center;">0.977</td>
<td style="text-align: center;">0.969</td>
<td style="text-align: center;">0.961</td>
</tr>
<tr>
<td style="text-align: center;">WB-Reward ${ }_{\text {Bam }}^{\text {Baka }}$</td>
<td style="text-align: center;">0.984</td>
<td style="text-align: center;">0.973</td>
<td style="text-align: center;">0.978</td>
<td style="text-align: center;">0.912</td>
<td style="text-align: center;">WB-Reward ${ }_{\text {Bam }}^{\text {Baka }}$</td>
<td style="text-align: center;">0.992</td>
<td style="text-align: center;">0.973</td>
<td style="text-align: center;">0.969</td>
</tr>
<tr>
<td style="text-align: center;">WB-Reward ${ }_{\text {Bam }}^{\text {Baka }}$</td>
<td style="text-align: center;">0.984</td>
<td style="text-align: center;">0.976</td>
<td style="text-align: center;">0.974</td>
<td style="text-align: center;">0.912</td>
<td style="text-align: center;">WB-Reward ${ }_{\text {Bam }}^{\text {Baka }}$</td>
<td style="text-align: center;">0.973</td>
<td style="text-align: center;">0.976</td>
<td style="text-align: center;">0.974</td>
</tr>
</tbody>
</table>
<p>seems on AlpacaEval-2.0, it is indeed the best 8B model in our evaluation and outperforms some other larger models. Interestingly, Llama-3-8B-Inst-SimPO consistently improves the performance of Llama-3-8B-Inst on all task categories, resulting in a similar shape on the radar plot in Figure 5.</p>
<p>Are longer responses always better? WILDBench is robust to length bias. For example, Llama-2-70B-chat and Llama-3-70B-Inst have similar output lengths ( 2,965 vs 2,983 chars), yet Llama-3-70B-Inst ranks 5th while Llama-2-70B-chat ranks 33rd on the leaderboard of 40 models. Additionally, Yi-1.5-6B’s output length is the 4th longest among the 40 models ( 3,322 characters), but it ranks 29th on the leaderboard. This suggests that the WILDBENCH evaluation is not biased towards longer responses, with response quality being the most important factor in the evaluation process. Additionally, we use a length penalty to ensure that longer responses are not always favored, and users can customize the length penalty to adjust the trade-off between response length and quality according to their needs. This feature is available on our live leaderboard and is illustrated in Figure 6.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Performance breakdown by task category of 6 models on WILDBENCH.</p>
<h1>4.2 Correlation to Human Judgment</h1>
<p>To analyze how well WILDBENCH evaluation
correlates with human judgment, we compare our results to the ChatbotArena Elo rating generated by large-scale online human evaluations. Focusing on hard prompts, we use the Elo ratings from the Hard-English version released on May 20, 2024.</p>
<p>We compare our WB-Reward and WB-Score with three other metrics: AlpacaEval winrate (WR), length-controlled winrate (LC), and ArenaHard scores. We use three correlation metrics: Pearson correlation (P-Cor), Spearman correlation (S-Cor), and Kendall's tau correlation (K-Cor). To ensure a fair comparison, we consider all models that have all four metrics available in Table 2, which results in 14 models. To distinguish the top-performing models, we also consider the top 6 models, denoting their correlation metrics as P -Cor ${ }<em _all="{all" _text="\text">{\text {top }}$, and P -Cor ${ }</em>$ respectively. The reason why we care about the correlation on top-ranking models is that models released in the future are likely to compete with the top models, so the Pearson correlation in this range is more important from the perspective of predicting the future application of a metric. The analysis results are shown in Table 3.}</p>
<p>Both WB-Reward and WB-Score show strong correlations with the human-based Elo rating, particularly for the top-performing models, achieving the best correlation among all other automatic metrics. Among using different baseline models for pairwise evaluation, we find that using Haiku as the baseline model yields the best correlation. These results suggest that the WILDBENCH evaluation correlates well with human judgment in ranking model performance as an automatic metric.</p>
<h1>4.3 Ablation Studies and Discussions.</h1>
<p>Checklists. In our ablation study on the impact of checklists, we compared model performance with and without checklists by removing the associated parts from the prompt templates. The results indicate that incorporating checklists improves the final correlation with human preferences. Specifically, the WB-Score without checklists achieves a Pearson correlation of 0.905 (for all models), which is lower than the 0.925 correlation achieved when using checklists.</p>
<p>Length penalties. We experimented with different $K(100,200,500,1000, \inf )$ in the length penalty method. We found that $K=500$ is the best choice, as it achieves the highest correlation with human judgments. This result suggests that the length penalty method is effective in mitigating the length bias in LLM evaluations.</p>
<p>Do multiple LLMs as judges help? How much do multiple LLMs help? We experimented with using GPT-4, Claude 3 Opus, and Mistral-Large as LLM judges. Our experiments revealed that these LLM judges produced very similar results, thereby exerting minimal influence on the final relative ranking of LLMs. Considering to reduce the cost of evaluation and faster turnaround time, we recommend using a single LLM as a judge in practice. In the future versions, we will explore more efficient ways to use multiple LLMs as judges, for example, by using different judge LLMs for different tasks that are best suited to their strengths.</p>
<p>Data distribution. How do we explain that WildBench has a different distribution compared to ChatbotArena's platform but still shows a strong correlation, even better than ArenaHard? The objective of WildBench is to evaluate LLMs on challenging tasks from real users. The ArenaElo we use for comparison is derived from the hard-English split in ChatbotArena, where human users submit tasks and vote. Thus, both WildBench and ChatbotArena aim to address the same goal. While it is practically impossible to match the exact distribution of users and tasks between the two-given that WildChat users are anonymous and ChatbotArena does not publicize its data-both are sourced from real users on the web. Consequently, this represents the best possible approach for correlating our LLM ratings with human-based ratings.</p>
<p>Two complementary metrics: WB-Reward \&amp; WB-Score. Both metrics use checklists and a CoT-style prompt for evaluation, utilizing the same testing data. The key differences are in their methodologies: WB-Score: Evaluates each model's outputs individually on a scale of 1-10, with detailed explanations for each score (see Appendix); WB-Reward: Compares a model's outputs to those of three baseline models at different performance levels for a comprehensive evaluation. Pairwise evaluations can be coarse, but using three baseline models and refined pairwise choices (e.g., much better or slightly better) mitigates this. WB-Score provides a universal score comparable across models using the same evaluation templates and checklists. Additionally, WB-Score is cheaper and faster to run ( 10 minutes, $\$ 5$ ) compared to WB-Reward, which requires 3-4 times the cost due to multiple baselines. Both metrics have their strengths and weaknesses. We use both to build our official leaderboard, allowing users to choose the most suitable metrics for their experiments.</p>
<h2>5 Related Works</h2>
<p>Close-ended benchmarks. Close-ended benchmarks typically consist of multiple-choice questions and have been widely used to evaluate LLMs authors (2022). For example, MMLU (Hendrycks et al., 2020) includes multi-choice questions across various subject areas. Its variants include CMMLU (Li et al., 2023a) for Chinese, KMMLU (Son et al., 2024) for Korean, and MMLU-Pro (Wang et al., 2024) for more challenging evaluation. GPQA (Rein et al., 2023) is another close-ended benchmark designed to be challenging even for humans with internet access. Specialized benchmarks with ground-truth answers, such as GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), also fall into this category. While these benchmarks focus on close-form answers, our work evaluates LLMs' ability to generate free-form responses and engage in conversations with users.</p>
<p>Expert-curated and crowdsourced data. Several open-ended generation benchmarks rely on data curated by human experts or crowdsourcing workers. For instance, MT-Bench (Zheng et al., 2024) manually creates examples for predefined categories. AlpacaEval (Li et al., 2023b) is based on author-written examples (Dubois et al., 2023; Taori et al., 2023; Wang et al., 2022), which primarily consists of simple instructions such as rewriting tasks.</p>
<p>In-the-wild data. A key feature of our work is that its underlying data is sourced from real-world use cases, ensuring alignment with actual LLM use cases. Notable benchmarks using real-world data include ChatbotArena (Zheng et al., 2024; Chiang et al., 2024), where users input their questions and choose the better response from two LLMs. However, ChatbotArena relies on extensive human feedback. WildVision (Lu et al., 2024) is a similar project but designed for vision language models. ArenaHard (Li et al., 2024) is another work that selects user queries from ChatbotArena to construct a benchmark for automatic evaluation.</p>
<p>Evaluation methods. Evaluating open-ended generation poses challenges due to the lack of a single valid ground truth. Human evaluation, though reliable, is expensive and time-consuming. To reduce costs and enable fast evaluation, powerful LLMs are often used as judges, as seen in benchmarks like MT-Bench, AlpacaEval, ArenaHard, and our own. Evaluation methods include single-system grading, which assigns scores to individual outputs, and pairwise comparisons, which compare outputs of two systems to compute win rates. Pairwise comparisons, while more expensive, can highlight subtle differences across systems (Zheng et al., 2024). To mitigate self-selection bias where an LLM prefers its own outputs (Panickssery et al., 2024), we use checklists generated from multiple LLMs, similar to InfoBench (Qin et al., 2024). In addition, we ask LLM judges generate structured explanations that enable human verification for further calibration, inspired by Just-Eval (Lin et al., 2023). There are also local evaluators that can be used to evaluate LLMs with our WILDBENCH with open-weight LLMs, such as TIGERScore (Jiang et al., 2023) and Prometheus (Kim et al., 2024).</p>
<p>Data leakage prevention. Publicly available benchmarks risk contamination from LLMs trained on such data. GPQA includes a special string to help LLM developers filter out its data (Rein et al., 2023), yet indirect leakage through cited examples remains possible. To mitigate this, we reserve a subset of WildChat that is never released publicly, which keeps its expert-curated evaluation data private. However, WILDBENCH provides a public validation set and details the benchmark construction process for greater transparency.</p>
<p>Other dimensions for evaluation. While our focus is on evaluating LLM capabilities, other evaluation dimensions, such as safety (Mazeika et al., 2024; Jiang et al., 2024), fairness (Gallegos et al., 2024), logical reasoning (Lin et al., 2024), agentic planning (Liu et al., 2023; Mialon et al., 2023; Lin et al., 2022), and hallucination detection (Min et al., 2023; Mishra et al., 2024; Hong et al., 2024), are equally important.</p>
<h1>6 CONCLUSION and Future DireCTIONS</h1>
<p>In this work, we introduced WILDBENCH, a benchmark designed to evaluate LLMs using realworld user queries. An important feature of WILDBENCH data is the nature of in-the-wild user queries with natural task distribution. To evaluate LLM performance using the collected data, we introduced a CoT-like LLM-as-judge method to improve the interpretability of evaluations and reduce ambiguity. We also incorporated a length penalty method to mitigate the length bias in LLM-as-judge evaluations. Experiments show that our primary metrics, WB-Reward and WB-Score, have very strong correlations with human judgments, surpassing existing evaluations.</p>
<p>We present extensive experiments and analyses, showcasing the performance of a wide range of 40 LLMs, including both proprietary and public ones, on the WILDBENCH benchmark. By providing a detailed breakdown of scores across different task categories, WILDBENCH offers insights on the strengths and weaknesses of different models. By introducing WILDBENCH, we aim to provide a realistic, dynamic, and contamination-resilient evaluation framework that accurately reflects the capabilities of LLMs. We will actively maintain the project for continually evaluating new LLMs with unseen tasks over time.</p>
<h2>REFERENCES</h2>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Anthropic. The claude 3 model family: Opus, sonnet, haiku. https://www-cdn. anthropic. com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3. pdf, 2024.</p>
<p>The BigBench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. ArXiv, abs/2206.04615, 2022. URL https://api. semanticscholar. org/CorpusID:263625818.</p>
<p>Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: An open platform for evaluating llms by human preference, 2024.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387, 2023.</p>
<p>Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori B. Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators, 2024.</p>
<p>Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K. Ahmed. Bias and fairness in large language models: A survey, 2024.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.</p>
<p>Giwon Hong, Aryo Pradipta Gema, Rohit Saxena, Xiaotang Du, Ping Nie, Yu Zhao, Laura PerezBeltrachini, Max Ryabinin, Xuanli He, Clémentine Fourrier, and Pasquale Minervini. The hallucinations leaderboard - an open effort to measure hallucinations in large language models, 2024.</p>
<p>Dongfu Jiang, Yishan Li, Ge Zhang, Wenhao Huang, Bill Yuchen Lin, and Wenhu Chen. Tigerscore: Towards building explainable metric for all text generation tasks. Transactions on Machine Learning Research, 2023.</p>
<p>Liwei Jiang, Kavel Rao, Seungju Han, Allyson Ettinger, Faeze Brahman, Sachin Kumar, Niloofar Mireshghallah, Ximing Lu, Maarten Sap, Yejin Choi, and Nouha Dziri. Wildteaming at scale: From in-the-wild jailbreaks to (adversarially) safer language models. ArXiv, abs/2406.18510, 2024. URL https://api. semanticscholar.org/CorpusID:270738096.</p>
<p>Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. Prometheus 2: An open source language model specialized in evaluating other language models. arXiv preprint arXiv:2405.01535, 2024.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. ArXiv, abs/2205.11916, 2022. URL https://api. semanticscholar.org/CorpusID:249017743.</p>
<p>Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese. arXiv preprint arXiv:2306.09212, 2023a.</p>
<p>Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. From live data to high-quality benchmarks: The arena-hard pipeline, April 2024. URL https://lmsys.org/blog/2024-04-19-arena-hard/.</p>
<p>Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 2023b.</p>
<p>Bill Yuchen Lin, Chengsong Huang, Qian Liu, Wenda Gu, Sam Sommerer, and Xiang Ren. On grounded planning for embodied tasks with language models. ArXiv, abs/2209.00465, 2022. URL https://api.semanticscholar.org/CorpusID:251979509.</p>
<p>Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Raghavi Chandu, Chandra Bhagavatula, and Yejin Choi. The unlocking spell on base llms: Rethinking alignment via in-context learning. ArXiv, abs/2312.01552, 2023. URL https://api. semanticscholar.org/CorpusID:265608902.</p>
<p>Bill Yuchen Lin, Ronan Le Bras, and Yejin Choi. Zebralogic: Benchmarking the logical reasoning ability of language models, 2024. URL https://hf.co/spaces/allenai/ ZebraLogicBench-Leaderboard.</p>
<p>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Yuxian Gu, Hangliang Ding, Kai Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Shengqi Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating llms as agents. ArXiv, abs/2308.03688, 2023. URL https: //api.semanticscholar.org/CorpusID:260682249.</p>
<p>Yujie Lu, Dongfu Jiang, Wenhu Chen, William Yang Wang, Yejin Choi, and Bill Yuchen Lin. Wildvision: Evaluating vision-language models in the wild with human preferences. arXiv preprint arXiv:2406.11069, 2024.</p>
<p>Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, David Forsyth, and Dan Hendrycks. Harmbench: A standardized evaluation framework for automated red teaming and robust refusal, 2024.</p>
<p>Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with a reference-free reward. 2024a. URL https://api.semanticscholar.org/CorpusID: 269983560 .</p>
<p>Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with a referencefree reward, 2024b.</p>
<p>Meta. Introducing Meta Llama 3: The most capable openly available LLM to date. https: //ai.meta.com/blog/meta-llama-3/, 2023.</p>
<p>Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann André LeCun, and Thomas Scialom. Gaia: a benchmark for general ai assistants. ArXiv, abs/2311.12983, 2023. URL https://api.semanticscholar.org/CorpusID:265351664.</p>
<p>Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. arXiv preprint arXiv:2305.14251, 2023.</p>
<p>Abhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham Neubig, Yulia Tsvetkov, and Hannaneh Hajishirzi. Fine-grained hallucination detection and editing for language models, 2024.</p>
<p>OpenAI. Gpt-4 technical report, 2023.
Siru Ouyang, Shuohang Wang, Yang Liu, Ming Zhong, Yizhu Jiao, Dan Iter, Reid Pryzant, Chenguang Zhu, Heng Ji, and Jiawei Han. The shifted and the overlooked: A task-oriented investigation of user-GPT interactions. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https://openreview.net/forum?id=qS1ip2dGH0.</p>
<p>Arjun Panickssery, Samuel R. Bowman, and Shi Feng. Llm evaluators recognize and favor their own generations, 2024.</p>
<p>Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei Liu, Pengfei Liu, and Dong Yu. Infobench: Evaluating instruction following ability in large language models, 2024.</p>
<p>Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks, 2019.</p>
<p>David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: A graduate-level google-proof q\&amp;a benchmark, 2023.</p>
<p>Guijin Son, Hanwool Lee, Sungdong Kim, Seungone Kim, Niklas Muennighoff, Taekyoon Choi, Cheonbok Park, Kang Min Yoo, and Stella Biderman. Kmmlu: Measuring massive multitask language understanding in korean. arXiv preprint arXiv:2402.11548, 2024.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.</p>
<p>Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark, 2024.</p>
<p>Wenting Zhao, Xiang Ren, John Frederick Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. Wildchat: 1 m chatgpt interaction logs in the wild. 2024. URL https://api.semanticscholar . org/CorpusID:269390491.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 2024.</p>
<h1>Appendix</h1>
<h2>A Task Categories</h2>
<p>In Section 2.2 we mentioned that tasks are categorized into 12 categories to enable fine-grained analysis of LLM capabilities. The definition of these task categories are as follows.</p>
<ul>
<li>Information seeking - Users ask for specific information or facts about various topics.</li>
<li>Reasoning - Queries require logical thinking, problem-solving, or processing of complex ideas.</li>
<li>Planning - Users need assistance in creating plans or strategies for activities and projects.</li>
<li>Editing - Involves editing, rephrasing, proofreading, or other tasks related to the composition of general written content.</li>
<li>Coding \&amp; Debugging - Users seek help with writing, reviewing, or fixing code in programming.</li>
<li>Math - Queries related to mathematical concepts, problems, and calculations.</li>
<li>Role playing - Users engage in scenarios requiring ChatGPT to adopt a character or persona.</li>
<li>Data Analysis - Requests involve interpreting data, statistics, or performing analytical tasks.</li>
<li>Creative Writing - Users seek assistance with crafting stories, poems, or other creative texts.</li>
<li>Advice seeking - Users ask for recommendations or guidance on various personal or professional issues.</li>
<li>Brainstorming - Involves generating ideas, creative thinking, or exploring possibilities.</li>
<li>Others - Any queries that do not fit into the above categories or are of a miscellaneous nature.</li>
</ul>
<p>We consolidate the original categories into five major groups for easier task-wise analysis. Specifically, we combine "Information seeking" and "Advice seeking" into "Info Seeking"; "Math" and "Data Analysis" into "Math \&amp; Data"; and "Reasoning" and "Planning" into "Reasoning \&amp; Planning." The remaining types are grouped under "Creative Tasks." These consolidated groups are illustrated in Figure 5.</p>
<p>Please note that the following links are allenai for double-blind review, which we will update after the review process. The supplementary zip file contains the source code for the evaluation scripts, the leaderboard, and the data.</p>
<h2>B MORE INFORMATION ON WILDBENCH DATA</h2>
<p>The distribution of the number of turns in WILDBENCH can be found in Figure 8. The dataset documentation, metadata, and the public subset of WILDBENCH can be found at https://huggingface. co/datasets/allenai/WildBench/viewer/v2. We release the data under AI2's ImpACT license as a low-risk artifact, and we bear all responsibility in case of rights violations. We will ensure that the dataset will be available for a long time and maintain the data by continuously updating it.</p>
<p>Figure 8: Distribution of the number of turns in WildBench.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<h2>C MORE INFORMATION ON WILDBENCH EVALUATION</h2>
<p>Our evaluation results on the public subset of WILDBENCH can be reproduced using evaluation scripts available at https://github.com/allenai/WildBench/. We have included generation script for each model under the folder https://github.com/allenai/WildBench/tree/ main/scripts, and the scripts for evaluating generations can be found at https://github. com/allenai/WildBench/tree/main/evaluation.</p>
<h2>D Prompt Template for Pairwise Evaluation Metric WB-Reward</h2>
<p>The prompt template for pairwise evaluation is shown below. It can be divided into three sections: the first section provides the high-level instruction, the task to be tested, and two model outputs; the</p>
<p>second section specifies the checklist and the rules; and the last section instructs the LLM judge to follow the step-by-step evaluation process as detailed in Section 3.2</p>
<div class="codehilite"><pre><span></span><code><span class="o">#</span><span class="w"> </span><span class="n">Instruction</span>
<span class="n">You</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">expert</span><span class="w"> </span><span class="n">evaluator</span><span class="o">.</span><span class="w"> </span><span class="n">Your</span><span class="w"> </span><span class="n">task</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">evaluate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">quality</span><span class="w"> </span><span class="k">of</span>
<span class="err">\</span><span class="n">hookrightarrow</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">responses</span><span class="w"> </span><span class="n">generated</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">two</span><span class="w"> </span><span class="n">AI</span><span class="w"> </span><span class="n">models</span><span class="o">.</span><span class="w"> </span><span class="n">We</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">provide</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="k">with</span>
<span class="err">\</span><span class="n">hookrightarrow</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">user</span><span class="w"> </span><span class="n">query</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">pair</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">AI</span><span class="o">-</span><span class="n">generated</span><span class="w"> </span><span class="n">responses</span><span class="w"> </span><span class="o">(</span><span class="n">Response</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="ow">and</span>
<span class="err">\</span><span class="n">hookrightarrow</span><span class="w"> </span><span class="n">B</span><span class="o">).</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">first</span><span class="w"> </span><span class="n">read</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">user</span><span class="w"> </span><span class="n">query</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">conversation</span>
<span class="err">\</span><span class="n">hookrightarrow</span><span class="w"> </span><span class="n">history</span><span class="w"> </span><span class="n">carefully</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">analyzing</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">task</span><span class="o">,</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="n">evaluate</span><span class="w"> </span><span class="n">the</span>
<span class="err">\</span><span class="n">hookrightarrow</span><span class="w"> </span><span class="n">quality</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">responses</span><span class="w"> </span><span class="n">based</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">rules</span><span class="w"> </span><span class="n">provided</span><span class="w"> </span><span class="n">below</span><span class="o">.</span>
<span class="o">#</span><span class="w"> </span><span class="n">Conversation</span><span class="w"> </span><span class="n">between</span><span class="w"> </span><span class="n">User</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">AI</span>
<span class="o">##</span><span class="w"> </span><span class="n">History</span>
<span class="o">&lt;|</span><span class="n">begin_of_history</span><span class="o">|&gt;</span>
<span class="o">{$</span><span class="n">history</span><span class="o">}</span>
<span class="o">&lt;|</span><span class="n">end_of_history</span><span class="o">|&gt;</span>
<span class="o">##</span><span class="w"> </span><span class="n">Current</span><span class="w"> </span><span class="n">User</span><span class="w"> </span><span class="n">Query</span>
<span class="o">&lt;|</span><span class="n">begin_of_query</span><span class="o">|&gt;</span>
<span class="o">{$</span><span class="n">user_query</span><span class="o">}</span>
<span class="o">&lt;|</span><span class="n">end_of_query</span><span class="o">|&gt;</span>
<span class="o">##</span><span class="w"> </span><span class="n">Response</span><span class="w"> </span><span class="n">A</span>
<span class="o">&lt;|</span><span class="n">begin_of_response_A</span><span class="o">|&gt;</span>
<span class="o">{$</span><span class="n">candidate_A</span><span class="o">}</span>
<span class="o">&lt;|</span><span class="n">end_of_response_A</span><span class="o">|&gt;</span>
<span class="o">##</span><span class="w"> </span><span class="n">Response</span><span class="w"> </span><span class="n">B</span>
<span class="o">&lt;|</span><span class="n">begin_of_response_B</span><span class="o">|&gt;</span>
<span class="o">{$</span><span class="n">candidate_B</span><span class="o">}</span>
<span class="o">&lt;|</span><span class="n">end_of_response_B</span><span class="o">|&gt;</span>
</code></pre></div>

<h1>Evaluation</h1>
<div class="codehilite"><pre><span></span><code><span class="err">##</span><span class="w"> </span><span class="nx">Checklist</span>
<span class="p">&lt;</span><span class="o">|</span><span class="nx">begin_of_checklist</span><span class="o">|</span><span class="p">&gt;</span>
<span class="p">{</span><span class="err">$</span><span class="nx">checklist</span><span class="p">}</span>
<span class="p">&lt;</span><span class="o">|</span><span class="nx">end_of_checklist</span><span class="o">|</span><span class="p">&gt;</span>
<span class="nx">Please</span><span class="w"> </span><span class="nx">use</span><span class="w"> </span><span class="nx">this</span><span class="w"> </span><span class="nx">checklist</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">guide</span><span class="w"> </span><span class="nx">your</span><span class="w"> </span><span class="nx">evaluation</span><span class="p">,</span><span class="w"> </span><span class="nx">but</span><span class="w"> </span><span class="nx">do</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="nx">limit</span>
<span class="err">\</span><span class="nx">hookrightarrow</span><span class="w"> </span><span class="nx">your</span><span class="w"> </span><span class="nx">assessment</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">checklist</span><span class="p">.</span>
<span class="err">##</span><span class="w"> </span><span class="nx">Rules</span>
<span class="nx">You</span><span class="w"> </span><span class="nx">should</span><span class="w"> </span><span class="nx">compare</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">above</span><span class="w"> </span><span class="nx">two</span><span class="w"> </span><span class="nx">responses</span><span class="w"> </span><span class="nx">based</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="nx">your</span><span class="w"> </span><span class="nx">analysis</span><span class="w"> </span><span class="nx">of</span>
<span class="err">\</span><span class="nx">hookrightarrow</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">user</span><span class="w"> </span><span class="nx">queries</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">conversation</span><span class="w"> </span><span class="nx">history</span><span class="p">.</span><span class="w"> </span><span class="nx">You</span><span class="w"> </span><span class="nx">should</span><span class="w"> </span><span class="nx">first</span>
<span class="err">\</span><span class="nx">hookrightarrow</span><span class="w"> </span><span class="nx">write</span><span class="w"> </span><span class="nx">down</span><span class="w"> </span><span class="nx">your</span><span class="w"> </span><span class="nx">analysis</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">checklist</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">you</span><span class="w"> </span><span class="nx">used</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">the</span>
<span class="err">\</span><span class="nx">hookrightarrow</span><span class="w"> </span><span class="nx">evaluation</span><span class="p">,</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="nx">provide</span><span class="w"> </span><span class="nx">your</span><span class="w"> </span><span class="nx">assessment</span><span class="w"> </span><span class="nx">according</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">the</span>
<span class="err">\</span><span class="nx">hookrightarrow</span><span class="w"> </span><span class="nx">checklist</span><span class="p">.</span><span class="w"> </span><span class="nx">There</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">five</span><span class="w"> </span><span class="nx">choices</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">give</span><span class="w"> </span><span class="nx">your</span><span class="w"> </span><span class="k">final</span><span class="w"> </span><span class="nx">assessment</span><span class="p">:</span>
<span class="err">\</span><span class="nx">hookrightarrow</span><span class="w"> </span><span class="p">[</span><span class="s">&quot;A++&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;A+&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;A=B&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;B+&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;B++&quot;</span><span class="p">],</span><span class="w"> </span><span class="nx">which</span><span class="w"> </span><span class="nx">correspond</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">the</span>
<span class="err">\</span><span class="nx">hookrightarrow</span><span class="w"> </span><span class="nx">following</span><span class="w"> </span><span class="nx">meanings</span><span class="p">:</span>
<span class="o">-</span><span class="w"> </span><span class="err">`</span><span class="nx">A</span><span class="o">++</span><span class="err">`</span><span class="p">:</span><span class="w"> </span><span class="nx">Response</span><span class="w"> </span><span class="nx">A</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">much</span><span class="w"> </span><span class="nx">better</span><span class="w"> </span><span class="nx">than</span><span class="w"> </span><span class="nx">Response</span><span class="w"> </span><span class="nx">B</span><span class="p">.</span>
<span class="o">-</span><span class="w"> </span><span class="err">`</span><span class="nx">A</span><span class="o">+</span><span class="err">`</span><span class="p">:</span><span class="w"> </span><span class="nx">Response</span><span class="w"> </span><span class="nx">A</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">only</span><span class="w"> </span><span class="nx">slightly</span><span class="w"> </span><span class="nx">better</span><span class="w"> </span><span class="nx">than</span><span class="w"> </span><span class="nx">Response</span><span class="w"> </span><span class="nx">B</span><span class="p">.</span>
<span class="o">-</span><span class="w"> </span><span class="err">`</span><span class="nx">A</span><span class="p">=</span><span class="nx">B</span><span class="err">`</span><span class="p">:</span><span class="w"> </span><span class="nx">Response</span><span class="w"> </span><span class="nx">A</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">B</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">same</span><span class="w"> </span><span class="nx">quality</span><span class="p">.</span><span class="w"> </span><span class="nx">Please</span><span class="w"> </span><span class="nx">use</span><span class="w"> </span><span class="nx">this</span>
<span class="err">$\</span><span class="nx">hookrightarrow</span><span class="err">$</span><span class="w"> </span><span class="kd">choice</span><span class="w"> </span><span class="nx">sparingly</span><span class="p">.</span>
<span class="o">-</span><span class="w"> </span><span class="err">`</span><span class="nx">B</span><span class="o">+</span><span class="err">`</span><span class="p">:</span><span class="w"> </span><span class="nx">Response</span><span class="w"> </span><span class="nx">B</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">only</span><span class="w"> </span><span class="nx">slightly</span><span class="w"> </span><span class="nx">better</span><span class="w"> </span><span class="nx">than</span><span class="w"> </span><span class="nx">Response</span><span class="w"> </span><span class="nx">A</span><span class="p">.</span>
<span class="o">-</span><span class="w"> </span><span class="err">`</span><span class="nx">B</span><span class="o">++</span><span class="err">`</span><span class="p">:</span><span class="w"> </span><span class="nx">Response</span><span class="w"> </span><span class="nx">B</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">much</span><span class="w"> </span><span class="nx">better</span><span class="w"> </span><span class="nx">than</span><span class="w"> </span><span class="nx">Response</span><span class="w"> </span><span class="nx">A</span><span class="p">.</span>
</code></pre></div>

<h2>Output Format</h2>
<p>First, please output your analysis for each model response, and then
\hookrightarrow \text { summarize your assessment to three aspects: "reason A=B", "reason}
\hookrightarrow A&gt;B", and "reason B&gt;A", and finally make your choice for the final
\hookrightarrow \text { assessment.}
Please provide your evaluation results in the following json format by
\hookrightarrow \text { filling in the placeholders in []:}
...
{
    "analysis of A": "[analysis of Response A]",
    "analysis of B": "[analysis of Response B]",
    "reason of A=B": "[where Response A and B perform equally well]",
    "reason of A&gt;B": "[where Response A is better than Response B]",
    "reason of B&gt;A": "[where Response B is better than Response A]",
    "choice": "[A++ or A+ or A=B or B+ or B++]",
}
...</p>
<div class="codehilite"><pre><span></span><code><span class="err">#</span><span class="w"> </span><span class="nt">E</span><span class="w"> </span><span class="nt">Prompt</span><span class="w"> </span><span class="nt">Template</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="nt">Individual</span><span class="w"> </span><span class="nt">Evaluation</span><span class="w"> </span><span class="nt">Metric</span><span class="w"> </span><span class="nt">WB-Score</span><span class="w"> </span>

<span class="nt">The</span><span class="w"> </span><span class="nt">prompt</span><span class="w"> </span><span class="nt">template</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="nt">individual</span><span class="w"> </span><span class="nt">evaluation</span><span class="w"> </span><span class="nt">is</span><span class="w"> </span><span class="nt">shown</span><span class="w"> </span><span class="nt">below</span><span class="o">.</span><span class="w"> </span><span class="nt">It</span><span class="w"> </span><span class="nt">can</span><span class="w"> </span><span class="nt">be</span><span class="w"> </span><span class="nt">similarly</span><span class="w"> </span><span class="nt">divided</span><span class="w"> </span><span class="nt">into</span><span class="w"> </span><span class="nt">three</span><span class="w"> </span><span class="nt">sections</span><span class="o">:</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">first</span><span class="w"> </span><span class="nt">section</span><span class="w"> </span><span class="nt">provides</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">high-level</span><span class="w"> </span><span class="nt">instruction</span><span class="o">,</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">task</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">be</span><span class="w"> </span><span class="nt">tested</span><span class="o">,</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">model</span><span class="w"> </span><span class="nt">output</span><span class="o">;</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">second</span><span class="w"> </span><span class="nt">section</span><span class="w"> </span><span class="nt">specifies</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">checklist</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">rules</span><span class="o">;</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">last</span><span class="w"> </span><span class="nt">section</span><span class="w"> </span><span class="nt">instructs</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">LLM</span><span class="w"> </span><span class="nt">judge</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">follow</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">step-by-step</span><span class="w"> </span><span class="nt">evaluation</span><span class="w"> </span><span class="nt">process</span><span class="w"> </span><span class="nt">as</span><span class="w"> </span><span class="nt">detailed</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">Section</span><span class="w"> </span><span class="nt">3</span><span class="p">.</span><span class="nc">3</span><span class="o">.</span>
</code></pre></div>

<h1>Instruction</h1>
<p>You are an expert evaluator. Your task is to evaluate the quality of
\hookrightarrow the responses generated by AI models.
We will provide you with the user query and an AI-generated responses.
You should first read the user query and the conversation history
\hookrightarrow carefully for analyzing the task, and then evaluate the quality of
\hookrightarrow the responses based on and rules provided below.</p>
<h1>Conversation between User and AI</h1>
<h2>History</h2>
<p>&lt;|begin_of_history|&gt;
{$history}
&lt;|end_of_history|&gt;</p>
<h2>Current User Query</h2>
<p>&lt;|begin_of_query|&gt;
{$user_query}
&lt;|end_of_query|&gt;</p>
<h2>AI Response</h2>
<p>&lt;|begin_of_response|&gt;
{$model_output}
&lt;|end_of_response|&gt;</p>
<div class="codehilite"><pre><span></span><code>
</code></pre></div>

<h1>Evaluation</h1>
<h2>Checklist</h2>
<p>&lt;|begin_of_checklist|&gt;
{$checklist}
&lt;|end_of_checklist|&gt;
Please use this checklist to guide your evaluation, but do not limit
\hookrightarrow \text { your assessment to the checklist.</p>
<h2>Rules</h2>
<p>You should compare the above response based on your analysis of the
\hookrightarrow \text { user queries and the conversation history.}
You should first write down your analysis and the checklist that you
\hookrightarrow used for the evaluation, and then provide your assessment according
\hookrightarrow to the checklist.
The scores are in the range of 1-10, where 1 means the response is very
\hookrightarrow \text { poor and 10 means the response is perfect.}
Here are more detailed criteria for the scores:
- Score 1-2: The response is very poor and does not make sense at all.
- Score 3-4: The response is poor and does help user solve the problem
\hookrightarrow in a meaningful way.
- Score 5-6: The response is fair but has some issues (e.g., factual
\hookrightarrow errors, hallucinations, missing key information).
- Score 7-8: The response is good enough but could be improved in some
\hookrightarrow ways.
- Score 9-10: The response is perfect and provides helpful information
\hookrightarrow that can help user solve the problem.</p>
<div class="codehilite"><pre><span></span><code>
</code></pre></div>

<h2>Output Format</h2>
<p>First, please output your analysis for the model response, and then
\hookrightarrow \text { summarize your assessment to two aspects: "strengths" and}
\hookrightarrow \text { "weaknesses"; Finally, please write down your rating for the}
\hookrightarrow \text { assessment.}
Please provide your evaluation results in the following json format by
\hookrightarrow \text { filling in the placeholders in []:}
...
{
    "strengths": "[analysis for the strengths of the response]",
    "weaknesses": "[analysis for the weaknesses of the response]",
    "score": "[1-10]"
}
~
```</p>
<h1>F FULL WILDBENCH LEADERBOARD</h1>
<p>The full WILDBENCH leaderboard as of Jun 5, 2024 can be found in Figure 6; The updated leaderboard as of Sept 1, 2024 can be found in Figure 7. Note that we used a new metric named WB-Elo that is based on merging WB-Reward and WB-Score to a collection of pairwise comparisons and perform Elo rating updates on top of existing LMSYS Elo rating, thus we can have a faster and more stable leaderboard update. You can view and interact with the latest results on our leaderboard on our website at https://huggingface.co/spaces/allenai/WildBench</p>
<p>WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 6: Leaderboard of WildBench (2024 Jun 5th)</p>
<table>
<thead>
<tr>
<th>-</th>
<th>Model</th>
<th>-</th>
<th>wB-Elo (LE)</th>
<th>-</th>
<th>Info Seek</th>
<th>-</th>
<th>Creative</th>
<th>Code &amp; Debug</th>
<th>Math &amp; Data</th>
<th>Reason &amp; Plan</th>
<th>Score</th>
<th>wB-Elo (Max)</th>
<th>Len</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>@ got-Ax-2024-05-12</td>
<td></td>
<td>1227.1</td>
<td>58.6</td>
<td></td>
<td>59.1</td>
<td>60.5</td>
<td>57.3</td>
<td>60.2</td>
<td>59.3</td>
<td>1236.7</td>
<td>3723</td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>@ Claude_3_5_Sorvet</td>
<td></td>
<td>1215.4</td>
<td>55.5</td>
<td></td>
<td>55.6</td>
<td>56.5</td>
<td>50.2</td>
<td>55.6</td>
<td>54.7</td>
<td>1221.9</td>
<td>2911</td>
<td></td>
</tr>
<tr>
<td>3</td>
<td>@ Gemini_1_5_Fon</td>
<td></td>
<td>1214.6</td>
<td>52.2</td>
<td></td>
<td>55.1</td>
<td>55.2</td>
<td>48.6</td>
<td>53.7</td>
<td>53</td>
<td>1220.3</td>
<td>3247</td>
<td></td>
</tr>
<tr>
<td>4</td>
<td>@ got-A-turbo-2024-04-09</td>
<td></td>
<td>1209.6</td>
<td>57.2</td>
<td></td>
<td>58.7</td>
<td>55.1</td>
<td>51</td>
<td>56.2</td>
<td>55.2</td>
<td>1217.1</td>
<td>3093</td>
<td></td>
</tr>
<tr>
<td>5</td>
<td>@ Yi-Large-Freelem</td>
<td></td>
<td>1208.9</td>
<td>57.7</td>
<td></td>
<td>57.6</td>
<td>54.3</td>
<td>51.9</td>
<td>56.6</td>
<td>55.3</td>
<td>1214.1</td>
<td>3512</td>
<td></td>
</tr>
<tr>
<td>6</td>
<td>@ ZeroGens-12-Chat_10628</td>
<td></td>
<td>1199.1</td>
<td>52.7</td>
<td></td>
<td>56.4</td>
<td>55</td>
<td>51.4</td>
<td>54.8</td>
<td>54</td>
<td>1207.2</td>
<td>3252</td>
<td></td>
</tr>
<tr>
<td>7</td>
<td>@ got-A-SLID-screview</td>
<td></td>
<td>1197.3</td>
<td>54.4</td>
<td></td>
<td>57.6</td>
<td>52.9</td>
<td>45.8</td>
<td>53.5</td>
<td>52.3</td>
<td>1205.9</td>
<td>3335</td>
<td></td>
</tr>
<tr>
<td>8</td>
<td>@ Claude_3_Doux</td>
<td></td>
<td>1196.3</td>
<td>53.5</td>
<td></td>
<td>53</td>
<td>53.3</td>
<td>46.7</td>
<td>52.5</td>
<td>51.7</td>
<td>1202.5</td>
<td>2685</td>
<td></td>
</tr>
<tr>
<td>9</td>
<td>@ Gemini_1_5_Flash</td>
<td></td>
<td>1192</td>
<td>48.7</td>
<td></td>
<td>51.7</td>
<td>48.7</td>
<td>45.3</td>
<td>50.8</td>
<td>48.9</td>
<td>1196.8</td>
<td>3654</td>
<td></td>
</tr>
<tr>
<td>10</td>
<td>@ Liana-2-708-Instruct</td>
<td></td>
<td>1187.5</td>
<td>52.3</td>
<td></td>
<td>54.3</td>
<td>44.7</td>
<td>42.1</td>
<td>50.1</td>
<td>47.8</td>
<td>1193.6</td>
<td>3046</td>
<td></td>
</tr>
<tr>
<td>11</td>
<td>@ ZeroGens-12-Lama_10614</td>
<td></td>
<td>1184.9</td>
<td>48</td>
<td></td>
<td>40.8</td>
<td>48.9</td>
<td>46.4</td>
<td>47.2</td>
<td>45.7</td>
<td>1175.9</td>
<td>2580</td>
<td></td>
</tr>
<tr>
<td>12</td>
<td>@ Yi-Large</td>
<td></td>
<td>1181.8</td>
<td>51</td>
<td></td>
<td>51.8</td>
<td>47.7</td>
<td>44.5</td>
<td>51.3</td>
<td>48.9</td>
<td>1186.4</td>
<td>3095</td>
<td></td>
</tr>
<tr>
<td>13</td>
<td>@ Athera-708</td>
<td></td>
<td>1180.7</td>
<td>60.8</td>
<td></td>
<td>60.4</td>
<td>59</td>
<td>57.1</td>
<td>61</td>
<td>59.5</td>
<td>1198.3</td>
<td>3175</td>
<td></td>
</tr>
<tr>
<td>14</td>
<td>@ Avontino-A-2608-Inst</td>
<td></td>
<td>1178.6</td>
<td>53</td>
<td></td>
<td>53.3</td>
<td>46.3</td>
<td>40.8</td>
<td>49.2</td>
<td>47.7</td>
<td>1181.3</td>
<td>2754</td>
<td></td>
</tr>
<tr>
<td>15</td>
<td>@ Lemma-2-278-11</td>
<td></td>
<td>1176.4</td>
<td>50.5</td>
<td></td>
<td>53.6</td>
<td>47</td>
<td>43.9</td>
<td>50.6</td>
<td>48.5</td>
<td>1181</td>
<td>2924</td>
<td></td>
</tr>
<tr>
<td>16</td>
<td>@ Mistral-Large-2</td>
<td></td>
<td>1176.3</td>
<td>57.4</td>
<td></td>
<td>58.9</td>
<td>53.8</td>
<td>52.7</td>
<td>57.2</td>
<td>55.6</td>
<td>1190.5</td>
<td>3503</td>
<td></td>
</tr>
<tr>
<td>17</td>
<td>@ Claude_3_Sorvet</td>
<td></td>
<td>1174.7</td>
<td>47.1</td>
<td></td>
<td>46.3</td>
<td>46.1</td>
<td>40.6</td>
<td>47.4</td>
<td>45.5</td>
<td>1176.4</td>
<td>2670</td>
<td></td>
</tr>
<tr>
<td>18</td>
<td>@ got-Ac-mini-2024-07-18</td>
<td></td>
<td>1173.5</td>
<td>57.4</td>
<td></td>
<td>60.1</td>
<td>57.2</td>
<td>54</td>
<td>58.2</td>
<td>57.1</td>
<td>1193.2</td>
<td>3648</td>
<td></td>
</tr>
<tr>
<td>19</td>
<td>@ Omnii-728-Instruct</td>
<td></td>
<td>1172.3</td>
<td>49.5</td>
<td></td>
<td>49.9</td>
<td>39.8</td>
<td>41</td>
<td>46.8</td>
<td>44.5</td>
<td>1176.7</td>
<td>2856</td>
<td></td>
</tr>
<tr>
<td>20</td>
<td>@ Reko_Gaze</td>
<td></td>
<td>1170.4</td>
<td>52.3</td>
<td></td>
<td>55.5</td>
<td>40.6</td>
<td>40.2</td>
<td>48</td>
<td>45.9</td>
<td>1174.1</td>
<td>2592</td>
<td></td>
</tr>
<tr>
<td>21</td>
<td>@ gamma-2-70-11-11695</td>
<td></td>
<td>1166.6</td>
<td>56.5</td>
<td></td>
<td>58</td>
<td>50.9</td>
<td>48.6</td>
<td>55.6</td>
<td>53.3</td>
<td>1186.5</td>
<td>4277</td>
<td></td>
</tr>
<tr>
<td>22</td>
<td>@ gamma-2-70-11-1870</td>
<td></td>
<td>1166.6</td>
<td>58.2</td>
<td></td>
<td>59.1</td>
<td>50.5</td>
<td>47.1</td>
<td>55.5</td>
<td>53.2</td>
<td>1184.4</td>
<td>3982</td>
<td></td>
</tr>
<tr>
<td>23</td>
<td>@ Yi-L.5-308-Chat</td>
<td></td>
<td>1159.6</td>
<td>50.3</td>
<td></td>
<td>53.5</td>
<td>42.1</td>
<td>39.4</td>
<td>48.1</td>
<td>45.6</td>
<td>1164.4</td>
<td>3523</td>
<td></td>
</tr>
<tr>
<td>24</td>
<td>@ Claude_3_Holdu</td>
<td></td>
<td>1159.1</td>
<td>45.3</td>
<td></td>
<td>42.9</td>
<td>37</td>
<td>31.4</td>
<td>41.3</td>
<td>38.9</td>
<td>1159.3</td>
<td>2601</td>
<td></td>
</tr>
<tr>
<td>25</td>
<td>@ Mistral-Homo-Inst_11281</td>
<td></td>
<td>1158.6</td>
<td>51.9</td>
<td></td>
<td>54.6</td>
<td>39.7</td>
<td>35.6</td>
<td>47.4</td>
<td>44.4</td>
<td>1166.9</td>
<td>3318</td>
<td></td>
</tr>
<tr>
<td>26</td>
<td>@ Mistral-Large</td>
<td></td>
<td>1157</td>
<td>46.1</td>
<td></td>
<td>49.7</td>
<td>33.7</td>
<td>30.9</td>
<td>41.8</td>
<td>38.9</td>
<td>1159.5</td>
<td>2514</td>
<td></td>
</tr>
<tr>
<td>27</td>
<td>@ Lemma-2-98-11</td>
<td></td>
<td>1156.4</td>
<td>49</td>
<td></td>
<td>51</td>
<td>36.7</td>
<td>36.4</td>
<td>46.7</td>
<td>42.7</td>
<td>1159.9</td>
<td>2802</td>
<td></td>
</tr>
<tr>
<td>28</td>
<td>@ Command-9-Flux</td>
<td></td>
<td>1151.4</td>
<td>49.2</td>
<td></td>
<td>52.6</td>
<td>28.4</td>
<td>23.5</td>
<td>41.9</td>
<td>36.8</td>
<td>1153.3</td>
<td>3293</td>
<td></td>
</tr>
<tr>
<td>29</td>
<td>@ GLM-A-98-Chat</td>
<td></td>
<td>1148.5</td>
<td>46.3</td>
<td></td>
<td>47.8</td>
<td>35.4</td>
<td>29.8</td>
<td>42.5</td>
<td>39.1</td>
<td>1153.9</td>
<td>3692</td>
<td></td>
</tr>
<tr>
<td>30</td>
<td>@ Magpie-88-Align-v5_1</td>
<td></td>
<td>1148.4</td>
<td>48.9</td>
<td></td>
<td>49.2</td>
<td>33.7</td>
<td>29.8</td>
<td>42.7</td>
<td>39.3</td>
<td>1154.8</td>
<td>3107</td>
<td></td>
</tr>
<tr>
<td>31</td>
<td>@ Yi-L.5-98-Chat</td>
<td></td>
<td>1148</td>
<td>42.6</td>
<td></td>
<td>45.6</td>
<td>35</td>
<td>32.2</td>
<td>42.4</td>
<td>38.7</td>
<td>1154.2</td>
<td>3468</td>
<td></td>
</tr>
<tr>
<td>32</td>
<td>@ Liana2-Inst-88-11695</td>
<td></td>
<td>1147.5</td>
<td>47.9</td>
<td></td>
<td>50.6</td>
<td>31.8</td>
<td>24</td>
<td>40.9</td>
<td>37</td>
<td>1152.2</td>
<td>2541</td>
<td></td>
</tr>
<tr>
<td>33</td>
<td>@ Liana2-Inst-88-11695-v5_2</td>
<td></td>
<td>1147.4</td>
<td>47.9</td>
<td></td>
<td>51.8</td>
<td>31.5</td>
<td>24.4</td>
<td>40.7</td>
<td>37.2</td>
<td>1151.3</td>
<td>2533</td>
<td></td>
</tr>
<tr>
<td>34</td>
<td>@ Omnii_5-728-Chat</td>
<td></td>
<td>1147.4</td>
<td>48.2</td>
<td></td>
<td>50.4</td>
<td>35.4</td>
<td>29.8</td>
<td>42.5</td>
<td>39.9</td>
<td>1150</td>
<td>2392</td>
<td></td>
</tr>
<tr>
<td>35</td>
<td>@ Liana2-Inst-88-11695-ExFS</td>
<td></td>
<td>1145.5</td>
<td>47.3</td>
<td></td>
<td>49.1</td>
<td>28.6</td>
<td>21.2</td>
<td>39.5</td>
<td>35</td>
<td>1147.3</td>
<td>2480</td>
<td></td>
</tr>
<tr>
<td>36</td>
<td>@ GLM_1Liana2-88-Inst-</td>
<td></td>
<td>1144</td>
<td>46.1</td>
<td></td>
<td>51.1</td>
<td>27.3</td>
<td>23.5</td>
<td>39.8</td>
<td>35.3</td>
<td>1148.9</td>
<td>2913</td>
<td></td>
</tr>
<tr>
<td>37</td>
<td>@ Phi-3-motion-1286</td>
<td></td>
<td>1139.5</td>
<td>35.7</td>
<td></td>
<td>33.2</td>
<td>18.2</td>
<td>23</td>
<td>32.3</td>
<td>27.3</td>
<td>1128.2</td>
<td>2849</td>
<td></td>
</tr>
<tr>
<td>38</td>
<td>@ Liana-3-88-Instruct</td>
<td></td>
<td>1139.5</td>
<td>39.3</td>
<td></td>
<td>43.6</td>
<td>22</td>
<td>17</td>
<td>34.4</td>
<td>29.2</td>
<td>1138.6</td>
<td>2975</td>
<td></td>
</tr>
<tr>
<td>39</td>
<td>@ Hermes-2-Theta-Liana-3-88</td>
<td></td>
<td>1137.4</td>
<td>41.6</td>
<td></td>
<td>39.8</td>
<td>23.1</td>
<td>18.7</td>
<td>33.7</td>
<td>29.6</td>
<td>1137.7</td>
<td>2742</td>
<td></td>
</tr>
<tr>
<td>40</td>
<td>@ Sterling-LM-78-beta-ExFS</td>
<td></td>
<td>1136</td>
<td>42.9</td>
<td></td>
<td>44.3</td>
<td>25.3</td>
<td>18.6</td>
<td>36.3</td>
<td>31.6</td>
<td>1137.8</td>
<td>2835</td>
<td></td>
</tr>
<tr>
<td>41</td>
<td>@ GLM_1Dashar-78-10621</td>
<td></td>
<td>1134.3</td>
<td>41</td>
<td></td>
<td>44.7</td>
<td>11</td>
<td>12.7</td>
<td>31.6</td>
<td>25.1</td>
<td>1126.5</td>
<td>2823</td>
<td></td>
</tr>
<tr>
<td>42</td>
<td>@ Reko_Flash</td>
<td></td>
<td>1132.7</td>
<td>41.5</td>
<td></td>
<td>42.4</td>
<td>22.1</td>
<td>20.5</td>
<td>35</td>
<td>30.4</td>
<td>1132.1</td>
<td>2103</td>
<td></td>
</tr>
<tr>
<td>43</td>
<td>@ Gamma-2-28-11</td>
<td></td>
<td>1129.7</td>
<td>39.9</td>
<td></td>
<td>43.6</td>
<td>17.9</td>
<td>15.8</td>
<td>33.8</td>
<td>27.8</td>
<td>1128.8</td>
<td>3589</td>
<td></td>
</tr>
<tr>
<td>44</td>
<td>@ got-3_5-turbo-0121</td>
<td></td>
<td>1129.2</td>
<td>36.5</td>
<td></td>
<td>37.4</td>
<td>26.5</td>
<td>21.6</td>
<td>33.4</td>
<td>30</td>
<td>1122.7</td>
<td>1844</td>
<td></td>
</tr>
<tr>
<td>45</td>
<td>@ 1000_Instruct</td>
<td></td>
<td>1128.5</td>
<td>41.1</td>
<td></td>
<td>42.3</td>
<td>26.4</td>
<td>24.5</td>
<td>36.2</td>
<td>32.6</td>
<td>1129.4</td>
<td>2576</td>
<td></td>
</tr>
<tr>
<td>46</td>
<td>@ Neo-78-Instruct-ExFS</td>
<td></td>
<td>1126.6</td>
<td>34.9</td>
<td></td>
<td>38.5</td>
<td>12.8</td>
<td>12.6</td>
<td>28.7</td>
<td>23.1</td>
<td>1116</td>
<td>4107</td>
<td></td>
</tr>
<tr>
<td>47</td>
<td>@ Neo-78-Instruct</td>
<td></td>
<td>1126.2</td>
<td>36.3</td>
<td></td>
<td>39.5</td>
<td>14</td>
<td>15</td>
<td>31.4</td>
<td>25</td>
<td>1122.1</td>
<td>3735</td>
<td></td>
</tr>
<tr>
<td>48</td>
<td>@ SterlingLM-78-beta</td>
<td></td>
<td>1126.2</td>
<td>41.9</td>
<td></td>
<td>43.8</td>
<td>24.4</td>
<td>17</td>
<td>34.1</td>
<td>30.2</td>
<td>1126.3</td>
<td>2797</td>
<td></td>
</tr>
<tr>
<td>49</td>
<td>@ Command-8</td>
<td></td>
<td>1125.6</td>
<td>44.1</td>
<td></td>
<td>47.4</td>
<td>19.3</td>
<td>16</td>
<td>34.6</td>
<td>29.5</td>
<td>1125.3</td>
<td>2919</td>
<td></td>
</tr>
<tr>
<td>50</td>
<td>@ Mistral-8x78-Instruct</td>
<td></td>
<td>1124.7</td>
<td>41.9</td>
<td></td>
<td>42.8</td>
<td>25</td>
<td>22.1</td>
<td>34.6</td>
<td>31.5</td>
<td>1123.4</td>
<td>2653</td>
<td></td>
</tr>
<tr>
<td>51</td>
<td>@ Yi-L.5-68-Chat</td>
<td></td>
<td>1122.7</td>
<td>31.4</td>
<td></td>
<td>31.1</td>
<td>16.6</td>
<td>16.8</td>
<td>27.3</td>
<td>23.3</td>
<td>1110.3</td>
<td>3899</td>
<td></td>
</tr>
<tr>
<td>52</td>
<td>@ Tulu-2-don-70b</td>
<td></td>
<td>1121</td>
<td>40.7</td>
<td></td>
<td>42.7</td>
<td>20.7</td>
<td>14.8</td>
<td>32.3</td>
<td>28</td>
<td>1119.1</td>
<td>2908</td>
<td></td>
</tr>
<tr>
<td>53</td>
<td>@ Reko_5dgo</td>
<td></td>
<td>1120.8</td>
<td>34.4</td>
<td></td>
<td>36.2</td>
<td>13.5</td>
<td>8.9</td>
<td>25</td>
<td>21.3</td>
<td>1112.2</td>
<td>2417</td>
<td></td>
</tr>
<tr>
<td>54</td>
<td>@ Mistral-78-Instruct-v5_2</td>
<td></td>
<td>1105</td>
<td>40.1</td>
<td></td>
<td>42.1</td>
<td>18.4</td>
<td>10.1</td>
<td>30.1</td>
<td>25.6</td>
<td>1104.1</td>
<td>2832</td>
<td></td>
</tr>
<tr>
<td>55</td>
<td>@ Liana-2-708-1061</td>
<td></td>
<td>1101.9</td>
<td>38.3</td>
<td></td>
<td>48</td>
<td>9.3</td>
<td>4.2</td>
<td>26.8</td>
<td>20.7</td>
<td>1099.2</td>
<td>3138</td>
<td></td>
</tr>
<tr>
<td>56</td>
<td>@ Omnii_5-78-Chat</td>
<td></td>
<td>1092.7</td>
<td>34</td>
<td></td>
<td>38.3</td>
<td>14.9</td>
<td>11.9</td>
<td>28.9</td>
<td>23.4</td>
<td>1091.1</td>
<td>2519</td>
<td></td>
</tr>
<tr>
<td>57</td>
<td>@ Hermes-2-Mistral-8x78-1870</td>
<td></td>
<td>1085.8</td>
<td>39.8</td>
<td></td>
<td>37.9</td>
<td>26</td>
<td>21.8</td>
<td>34.2</td>
<td>30.7</td>
<td>1083.6</td>
<td>2874</td>
<td></td>
</tr>
<tr>
<td>58</td>
<td>@ Phi-3-mini-1286</td>
<td></td>
<td>1082.1</td>
<td>28.6</td>
<td></td>
<td>30.6</td>
<td>21.6</td>
<td>18.6</td>
<td>28.1</td>
<td>24.7</td>
<td>1074.5</td>
<td>2435</td>
<td></td>
</tr>
<tr>
<td>59</td>
<td>@ Gamma-78-11</td>
<td></td>
<td>1079.2</td>
<td>12.7</td>
<td></td>
<td>21.2</td>
<td>1.8</td>
<td>-3.7</td>
<td>10.2</td>
<td>6.6</td>
<td>1054.5</td>
<td>1726</td>
<td></td>
</tr>
<tr>
<td>60</td>
<td>@ Liana-2-78-1061</td>
<td></td>
<td>1052.5</td>
<td>27.7</td>
<td></td>
<td>29.8</td>
<td>-6.8</td>
<td>-7.2</td>
<td>15.4</td>
<td>8.3</td>
<td>1044</td>
<td>2985</td>
<td></td>
</tr>
<tr>
<td>61</td>
<td>@ Gamma-28-11</td>
<td></td>
<td>1011.8</td>
<td>-2.1</td>
<td></td>
<td>7.2</td>
<td>-17.7</td>
<td>-18.6</td>
<td>-5.8</td>
<td>-9.7</td>
<td>981.8</td>
<td>1590</td>
<td></td>
</tr>
</tbody>
</table>
<p>Figure 7: Leaderboard of WildBench (2024 Sept 1st)</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ol>
<li>AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation models by 01.ai, 2024.</li>
</ol>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
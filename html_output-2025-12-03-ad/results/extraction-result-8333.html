<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8333 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8333</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8333</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-277322494</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.19990v2.pdf" target="_blank">LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?</a></p>
                <p><strong>Paper Abstract:</strong> Multi-step spatial reasoning entails understanding and reasoning about spatial relationships across multiple sequential steps, which is crucial for tackling complex real-world applications, such as robotic manipulation, autonomous navigation, and automated assembly. To assess how well current Multimodal Large Language Models (MLLMs) have acquired this fundamental capability, we introduce LEGO-Puzzles , a scalable benchmark designed to evaluate both spatial understanding and sequential reasoning in MLLMs through LEGO-based tasks. LEGO-Puzzles consists of 1,100 carefully curated visual question-answering (VQA) samples spanning 11 distinct tasks, ranging from basic spatial understanding to complex multi-step reasoning. Based on LEGO-Puzzles, we conduct a comprehensive evaluation of state-of-the-art MLLMs and uncover significant limitations in their spatial reasoning capabilities: even the most powerful MLLMs can answer only about half of the test cases, whereas human participants achieve over 90% accuracy. In addition to VQA tasks, we evaluate MLLMs’ abilities to generate LEGO images following assembly illustrations. Our experiments show that only Gemini-2.0-Flash and GPT-4o exhibit a limited ability to follow these instructions, while other MLLMs either replicate the input image or generate completely irrelevant outputs. Overall, LEGO-Puzzles exposes critical deficiencies in existing MLLMs’ spatial understanding and sequential reasoning capabilities, and</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8333.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8333.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary multimodal large language model evaluated in this paper; one of the top-performing models on LEGO-Puzzles VQA but still substantially below human performance and exhibits degradation on multi-step spatial tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary multimodal LLM evaluated in zero-shot setting on LEGO-based spatial VQA and image-generation tasks; interprets semantic content and can generate new images rather than editing inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>LEGO-Puzzles (LEGO assembly VQA & Generation)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>3D spatial manipulation and multi-step assembly reasoning (visual question answering and image-generation of intermediate assembly states).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot multiple-choice VQA across 11 tasks (Height, Adjacency, Rotation, Multiview, Next-Step, Dependency, Rotation Status, Position, Backwards, Ordering, Outlier). Also Next-k-Step ablation (k = 1..5) with and without Chain-of-Thought (CoT) prompt 'think step by step', and image-generation evaluation scored by human experts (0-3 scale for Appearance and Instruction Following).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard zero-shot prompting; experiments testing Chain-of-Thought (CoT) prompting for multi-step Next-k-Step tasks; model appears to reconstruct scenes conceptually for generation rather than editing input images.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall accuracy on LEGO-Puzzles reported as 57.7% (one of top performers among evaluated models); human experts ≈93.6%. In Next-k-Step, accuracy declines as k increases; under CoT prompting for long k steps, accuracy dropped to ~5% (worse than random 25%). Image-generation: semantic relevance sometimes preserved but appearance fidelity lower than best proprietary model (Gemini-2.0-Flash).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Qualitative and quantitative results show partial spatial reasoning ability (best among models evaluated on some tasks) but strong evidence of failure modes: inability to reliably track iterative spatial transformations over multiple steps, reliance on 2D image priors, and dramatic performance drop with increasing k indicating poor iterative visual memory.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared directly to Gemini-2.0-Flash (54.0%) and to human experts (≈93.6%); GPT-4o is highest- or near-highest-performing model on many VQA tasks but underperforms humans by ~30+ percentage points. Under CoT prompting, showed limited benefit (improvement for k=1 but catastrophic drop for k≥2).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Severe degradation with increasing multi-step depth (Next-k-Step); with CoT prompting can perform far below random on longer sequences (5% reported). Tends to generate semantically relevant but visually divergent images (low appearance fidelity), and inconsistent instruction following for fine-grained assembly edits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8333.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8333.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-2.0-Flash</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-2.0-Flash</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary multimodal MLLM evaluated on LEGO-Puzzles that shows strong image-editing consistency and among top VQA performance, but still well below human proficiency and exhibits multi-step degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-2.0-Flash</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary multimodal model evaluated zero-shot on LEGO-Puzzles VQA and image-generation tasks; demonstrates stronger appearance fidelity when performing instruction-driven image edits compared to other evaluated models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>LEGO-Puzzles (LEGO assembly VQA & Generation)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>3D spatial manipulation and sequential assembly reasoning (VQA and image editing/generation).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot multiple-choice VQA across the 11 LEGO tasks; Next-k-Step experiments for multi-step sequential reasoning; image-generation tests where model must produce intermediate assembly images following instructions; human scoring for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard zero-shot prompting; tested with and without CoT; model appears to apply an image-editing style generation that preserves input appearance identity rather than fully reconstructing a new image.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall accuracy on LEGO-Puzzles reported as 54.0% (one of the top proprietary models). In Next-k-Step, performance declines as k increases (no exact per-k numbers given, but trend explicitly reported). In image-generation, highest appearance and instruction-following scores among evaluated models (qualitative human-rated 0–3 scale, exact numbers not provided in text).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Better appearance consistency and structured edits indicate stronger spatial consistency mechanisms compared to other models; nevertheless still far below human accuracy, and shows inability to fully solve long-range sequential reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Directly compared to GPT-4o (GPT-4o slightly higher overall accuracy), open-source models (Gemini-2.0-Flash outperforms most open-source models on both VQA and generation), and human experts (≈93.6%). CoT prompting provided no obvious benefit for Gemini-2.0-Flash in Next-k-Step.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Decline in multi-step Next-k-Step tasks as k increases; fails to reach human-level performance; limited benefit from CoT prompting for true step-by-step reasoning; still struggles with relative relationships and rotation perception tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8333.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8333.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-2.5-72B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen-2.5-72B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large open/proprietary vision-language model (reported 72B) evaluated on LEGO-Puzzles that shows stable performance across increasing multi-step reasoning depth in Next-k-Step tests, unlike most other models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-2.5-72B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-language model (72B) evaluated zero-shot on LEGO-Puzzles VQA; included in Next-k-Step experiments and shown to maintain relatively consistent accuracy as k increases.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>LEGO-Puzzles (LEGO assembly VQA & Next-k-Step)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>3D assembly multi-step reasoning and VQA.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot multiple-choice VQA and Next-k-Step experiments (k up to 7) with and without CoT prompting; evaluated on standard LEGO-Puzzles tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard prompting; tested with CoT; model responses did not demonstrably use CoT to produce improved stepwise decomposition (CoT provided no obvious benefit).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In Next-k-Step experiments, Qwen-2.5-72B achieved stable and relatively consistent accuracy scores around ~65% across all values of k (even up to k=7). Overall VQA performance: among top open-source models but specific overall accuracy not explicitly enumerated in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Stability across increasing k suggests better iterative handling of multi-step assembly sequences compared to other models; however, no internal probing or ablation reported—evidence based on task performance trends.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared to GPT-4o and Gemini-2.0-Flash: Qwen is more stable across iterative steps though its absolute top-line VQA accuracies are lower than GPT-4o/Gemini; compared to InternVL-2.5-78B which performs near-random, Qwen substantially outperforms on Next-k-Step.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Although stable on Next-k-Step, Qwen did not show obvious benefits from CoT prompting and may still be below human accuracy on the full LEGO-Puzzles suite; paper does not claim human-level performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8333.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8333.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InternVL-2.5-78B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InternVL-2.5-78B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large vision-language model (reported 78B) evaluated here; demonstrates near-random or poor performance on multi-step LEGO assembly reasoning and catastrophic failures under CoT for longer sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Internvl-2.5-78B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-language model (78B) evaluated zero-shot across LEGO-Puzzles tasks and Next-k-Step experiments; tested both with and without CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>78B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>LEGO-Puzzles (VQA & Next-k-Step)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>3D assembly sequential reasoning and VQA.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot VQA on LEGO-Puzzles; Next-k-Step experiments with k up to 5, comparing standard prompting vs CoT ('think step by step').</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard prompting and CoT variants; model sometimes showed short-term CoT benefit for k=1 but not for larger k.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Overall performance close to random guessing in several tasks. In Next-k-Step, accuracy close to random guessing; under CoT prompting for longer k steps accuracy dropped dramatically (reported 10% in one experiment when facing longer k steps, below 25% random).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Performance indicates lack of reliable multi-step spatial reasoning and inability to maintain coherent intermediate state across steps; occasional improvement for k=1 suggests limited short-range reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Underperforms GPT-4o, Gemini-2.0-Flash, and Qwen-2.5-72B on multi-step tasks. CoT improved k=1 but caused catastrophic degradation for k≥2, unlike some other models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Catastrophic failure under CoT for longer multi-step sequences (accuracy as low as 10%); inconsistent generation behavior and inability to follow structured multi-step assembly reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8333.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8333.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Emu3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Emu3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source next-token-prediction based MLLM evaluated on LEGO-Puzzles that displayed severe response biases (e.g., defaulting to a single option) and poor sequence-generation performance on ordering tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Emu3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source multimodal model (next-token-prediction) included in the evaluation; evaluated zero-shot on LEGO-Puzzles VQA and ordering tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>LEGO-Puzzles (VQA: Ordering task)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Multi-step sequencing (ordering of assembly steps) and 3D assembly reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot multiple-choice prompts asking for ordered sequences (e.g., 'BACD'), explicit instruction about output format included; evaluated for ability to output valid constrained sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard prompting; no explicit CoT benefit reported; model displayed biased output strategies rather than genuine stepwise reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Severe failure in Ordering: e.g., Emu3 answered 'B' for 98 out of 100 test cases (near-constant single-letter bias), effectively scoring near 0 on the Ordering task.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>No convincing evidence of spatial reasoning; persistent single-label bias indicates inability to follow structured sequential reasoning prompts and suggests training or decoding issues rather than spatial understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared unfavorably to proprietary models (Gemini, GPT-4o) and several other open-source models; contrasted with human experts who perform well on ordering tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Output bias (repeated single choice), inability to generate valid multi-element orderings, likely poor constrained-output handling and lack of genuine stepwise dependency reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8333.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8333.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Emu2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Emu2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source multimodal model evaluated for image-generation on LEGO tasks; preserves some appearance identity but fails at instruction-following in generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Emu2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source multimodal model supporting image output evaluated on generation tasks (Rotation*, Multiview*, Position*, Dependency*, Next-Step*).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>LEGO-Puzzles (Generation tasks: Rotation*, Multiview*, Position*, Dependency*, Next-Step*)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Image generation/editing for 3D assembly steps and viewpoint changes.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Given assembly illustration and instructions, model must generate intermediate assembly images; generation quality scored by human experts on Appearance (App) and Instruction Following (IF) from 0–3.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Model seems oriented to image reconstruction/preservation rather than following sequential assembly instructions for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Human-evaluated: Emu2 shows some capability in preserving visual appearance (App) but fails entirely in instruction-following (IF score near 0 in many cases); exact numeric scores not provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Preservation of appearance suggests visual consistency, but failure to follow instructions indicates lack of reasoning about spatial transformations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared to Gemini-2.0-Flash (which has both App and IF strengths) and GPT-4o (which semantically reconstructs scenes), Emu2 is worse at following stepwise instructions despite appearance fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Treats tasks as image reconstruction rather than reasoning-driven image generation; cannot perform sequential modifications required by assembly tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8333.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8333.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GILL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GILL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source multimodal generation model evaluated on image-generation for LEGO tasks; performs poorly, often producing irrelevant outputs and nearly zero instruction-following scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GILL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source multimodal image-generation-capable model evaluated on LEGO-Puzzles generation tasks; supports long-range sequence input and image output.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>LEGO-Puzzles (Generation tasks: Rotation*, Multiview*, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Instruction-driven image generation for 3D assembly sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot generation from assembly illustrations; human expert scoring on Appearance and Instruction Following (0–3).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Appears to rely on generic image-generation capabilities without robust instruction-following or sequential-edit mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Human-evaluated scores near zero for instruction-following; generated images often unrelated to expected results (exact numbers not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>No evidence of reliable spatial reasoning in generation; fails to produce relevant intermediate assembly states.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Performs worse than proprietary models (Gemini-2.0-Flash, GPT-4o) and even worse than some other open-source models like Emu2 in terms of instruction following.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Nearly complete failure to follow instructions and generate relevant outputs; instruction-following scores close to zero.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8333.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8333.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Anole</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anole</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source autoregressive multimodal model evaluated for interleaved image-text generation on LEGO generation tasks, performing very poorly in instruction-following and generation relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Anole</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source autoregressive native large multimodal model supporting interleaved image-text generation; evaluated on LEGO-Puzzles image-generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>LEGO-Puzzles (Generation tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>3D assembly sequential image generation/editing.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot generation from assembly instructions/images; human scoring for Appearance and Instruction Following.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Autoregressive interleaved image-text generation without demonstrated capacity for stepwise spatial editing in this benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported to perform worst among generation models evaluated (GILL and Anole perform the worst), with instruction-following scores close to zero; generated images often unrelated to expected results.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>No substantive evidence of spatial reasoning in generated outputs; failures indicate inability to follow sequential spatial instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Worse than proprietary editors (Gemini-2.0-Flash) and worse than Emu2 in appearance preservation; similar poor behavior as GILL.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fail to generate relevant outputs in nearly all cases; instruction-following almost absent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8333.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8333.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Open-source MLLMs (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open-source Multimodal Large Language Models (aggregate group)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Collection of evaluated open-source MLLMs (e.g., VILA1.5-13B, Idefics3-8B, DeepSeek-VL2, Pixtral-12B, LLaVA-OneVision-7B, EMU3, Emu2, GILL, Anole) that largely perform poorly on LEGO-Puzzles, many near or below random baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various open-source MLLMs (VILA1.5-13B, Idefics3-8B, DeepSeek-VL2, Pixtral-12B, LLaVA-OneVision-7B, EMU3, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A set of open-source vision-language models evaluated in zero-shot setting on LEGO-Puzzles VQA and some on generation; specific architectures vary by model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>LEGO-Puzzles (VQA & some Generation)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>3D assembly spatial understanding and sequential reasoning VQA; some image generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot multiple-choice VQA across 11 tasks; generation evaluation for subset of tasks with human scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard prompting; some models tested with CoT; many open-source models show output biases or inability to follow structured prompts (e.g., defaulting to single letter), and poor instruction-following in generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Majority of open-source models perform at or below random baseline on many tasks (text reports: 20 models, 14 open-source; many below random; 4/14 open-source models scored zero on Ordering). Examples: Emu3 answered 'B' for 98/100 cases; 11/20 models scored lower than random on Height task.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Aggregate results demonstrate weak spatial reasoning: widespread failures on height perception (2D vs 3D illusions), ordering and multi-step tasks; little evidence that open-source models track multi-step spatial state robustly.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Consistently underperform proprietary models (Gemini-2.0-Flash, GPT-4o). Human experts far outperform open-source group (humans ≈93.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Frequent failures: answer-format/response biases in ordering tasks, inability to follow structured prompts, poor height/rotation perception (reliance on 2D priors), and near-zero instruction-following in generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>DSRBench: A comprehensive 3d spatial reasoning benchmark <em>(Rating: 2)</em></li>
                <li>SAT: Spatial aptitude training for multimodal language models <em>(Rating: 2)</em></li>
                <li>CAD-GPT: Synthesising cad construction sequence with spatial reasoning-enhanced multimodal llms <em>(Rating: 2)</em></li>
                <li>Coarse correspondences boost spatial-temporal reasoning in multimodal language models <em>(Rating: 1)</em></li>
                <li>Manipllm: Embodied multimodal large language model for object-centric robotic manipulation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8333",
    "paper_id": "paper-277322494",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o",
            "brief_description": "A proprietary multimodal large language model evaluated in this paper; one of the top-performing models on LEGO-Puzzles VQA but still substantially below human performance and exhibits degradation on multi-step spatial tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Proprietary multimodal LLM evaluated in zero-shot setting on LEGO-based spatial VQA and image-generation tasks; interprets semantic content and can generate new images rather than editing inputs.",
            "model_size": null,
            "puzzle_name": "LEGO-Puzzles (LEGO assembly VQA & Generation)",
            "puzzle_type": "3D spatial manipulation and multi-step assembly reasoning (visual question answering and image-generation of intermediate assembly states).",
            "task_setup": "Zero-shot multiple-choice VQA across 11 tasks (Height, Adjacency, Rotation, Multiview, Next-Step, Dependency, Rotation Status, Position, Backwards, Ordering, Outlier). Also Next-k-Step ablation (k = 1..5) with and without Chain-of-Thought (CoT) prompt 'think step by step', and image-generation evaluation scored by human experts (0-3 scale for Appearance and Instruction Following).",
            "mechanisms_or_strategies": "Standard zero-shot prompting; experiments testing Chain-of-Thought (CoT) prompting for multi-step Next-k-Step tasks; model appears to reconstruct scenes conceptually for generation rather than editing input images.",
            "performance_metrics": "Overall accuracy on LEGO-Puzzles reported as 57.7% (one of top performers among evaluated models); human experts ≈93.6%. In Next-k-Step, accuracy declines as k increases; under CoT prompting for long k steps, accuracy dropped to ~5% (worse than random 25%). Image-generation: semantic relevance sometimes preserved but appearance fidelity lower than best proprietary model (Gemini-2.0-Flash).",
            "evidence_of_spatial_reasoning": "Qualitative and quantitative results show partial spatial reasoning ability (best among models evaluated on some tasks) but strong evidence of failure modes: inability to reliably track iterative spatial transformations over multiple steps, reliance on 2D image priors, and dramatic performance drop with increasing k indicating poor iterative visual memory.",
            "comparisons": "Compared directly to Gemini-2.0-Flash (54.0%) and to human experts (≈93.6%); GPT-4o is highest- or near-highest-performing model on many VQA tasks but underperforms humans by ~30+ percentage points. Under CoT prompting, showed limited benefit (improvement for k=1 but catastrophic drop for k≥2).",
            "limitations_or_failure_cases": "Severe degradation with increasing multi-step depth (Next-k-Step); with CoT prompting can perform far below random on longer sequences (5% reported). Tends to generate semantically relevant but visually divergent images (low appearance fidelity), and inconsistent instruction following for fine-grained assembly edits.",
            "uuid": "e8333.0",
            "source_info": {
                "paper_title": "LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Gemini-2.0-Flash",
            "name_full": "Gemini-2.0-Flash",
            "brief_description": "A proprietary multimodal MLLM evaluated on LEGO-Puzzles that shows strong image-editing consistency and among top VQA performance, but still well below human proficiency and exhibits multi-step degradation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini-2.0-Flash",
            "model_description": "Proprietary multimodal model evaluated zero-shot on LEGO-Puzzles VQA and image-generation tasks; demonstrates stronger appearance fidelity when performing instruction-driven image edits compared to other evaluated models.",
            "model_size": null,
            "puzzle_name": "LEGO-Puzzles (LEGO assembly VQA & Generation)",
            "puzzle_type": "3D spatial manipulation and sequential assembly reasoning (VQA and image editing/generation).",
            "task_setup": "Zero-shot multiple-choice VQA across the 11 LEGO tasks; Next-k-Step experiments for multi-step sequential reasoning; image-generation tests where model must produce intermediate assembly images following instructions; human scoring for generation.",
            "mechanisms_or_strategies": "Standard zero-shot prompting; tested with and without CoT; model appears to apply an image-editing style generation that preserves input appearance identity rather than fully reconstructing a new image.",
            "performance_metrics": "Overall accuracy on LEGO-Puzzles reported as 54.0% (one of the top proprietary models). In Next-k-Step, performance declines as k increases (no exact per-k numbers given, but trend explicitly reported). In image-generation, highest appearance and instruction-following scores among evaluated models (qualitative human-rated 0–3 scale, exact numbers not provided in text).",
            "evidence_of_spatial_reasoning": "Better appearance consistency and structured edits indicate stronger spatial consistency mechanisms compared to other models; nevertheless still far below human accuracy, and shows inability to fully solve long-range sequential reasoning.",
            "comparisons": "Directly compared to GPT-4o (GPT-4o slightly higher overall accuracy), open-source models (Gemini-2.0-Flash outperforms most open-source models on both VQA and generation), and human experts (≈93.6%). CoT prompting provided no obvious benefit for Gemini-2.0-Flash in Next-k-Step.",
            "limitations_or_failure_cases": "Decline in multi-step Next-k-Step tasks as k increases; fails to reach human-level performance; limited benefit from CoT prompting for true step-by-step reasoning; still struggles with relative relationships and rotation perception tasks.",
            "uuid": "e8333.1",
            "source_info": {
                "paper_title": "LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Qwen-2.5-72B",
            "name_full": "Qwen-2.5-72B",
            "brief_description": "A large open/proprietary vision-language model (reported 72B) evaluated on LEGO-Puzzles that shows stable performance across increasing multi-step reasoning depth in Next-k-Step tests, unlike most other models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen-2.5-72B",
            "model_description": "Vision-language model (72B) evaluated zero-shot on LEGO-Puzzles VQA; included in Next-k-Step experiments and shown to maintain relatively consistent accuracy as k increases.",
            "model_size": "72B",
            "puzzle_name": "LEGO-Puzzles (LEGO assembly VQA & Next-k-Step)",
            "puzzle_type": "3D assembly multi-step reasoning and VQA.",
            "task_setup": "Zero-shot multiple-choice VQA and Next-k-Step experiments (k up to 7) with and without CoT prompting; evaluated on standard LEGO-Puzzles tasks.",
            "mechanisms_or_strategies": "Standard prompting; tested with CoT; model responses did not demonstrably use CoT to produce improved stepwise decomposition (CoT provided no obvious benefit).",
            "performance_metrics": "In Next-k-Step experiments, Qwen-2.5-72B achieved stable and relatively consistent accuracy scores around ~65% across all values of k (even up to k=7). Overall VQA performance: among top open-source models but specific overall accuracy not explicitly enumerated in main text.",
            "evidence_of_spatial_reasoning": "Stability across increasing k suggests better iterative handling of multi-step assembly sequences compared to other models; however, no internal probing or ablation reported—evidence based on task performance trends.",
            "comparisons": "Compared to GPT-4o and Gemini-2.0-Flash: Qwen is more stable across iterative steps though its absolute top-line VQA accuracies are lower than GPT-4o/Gemini; compared to InternVL-2.5-78B which performs near-random, Qwen substantially outperforms on Next-k-Step.",
            "limitations_or_failure_cases": "Although stable on Next-k-Step, Qwen did not show obvious benefits from CoT prompting and may still be below human accuracy on the full LEGO-Puzzles suite; paper does not claim human-level performance.",
            "uuid": "e8333.2",
            "source_info": {
                "paper_title": "LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "InternVL-2.5-78B",
            "name_full": "InternVL-2.5-78B",
            "brief_description": "A large vision-language model (reported 78B) evaluated here; demonstrates near-random or poor performance on multi-step LEGO assembly reasoning and catastrophic failures under CoT for longer sequences.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Internvl-2.5-78B",
            "model_description": "Vision-language model (78B) evaluated zero-shot across LEGO-Puzzles tasks and Next-k-Step experiments; tested both with and without CoT prompting.",
            "model_size": "78B",
            "puzzle_name": "LEGO-Puzzles (VQA & Next-k-Step)",
            "puzzle_type": "3D assembly sequential reasoning and VQA.",
            "task_setup": "Zero-shot VQA on LEGO-Puzzles; Next-k-Step experiments with k up to 5, comparing standard prompting vs CoT ('think step by step').",
            "mechanisms_or_strategies": "Standard prompting and CoT variants; model sometimes showed short-term CoT benefit for k=1 but not for larger k.",
            "performance_metrics": "Overall performance close to random guessing in several tasks. In Next-k-Step, accuracy close to random guessing; under CoT prompting for longer k steps accuracy dropped dramatically (reported 10% in one experiment when facing longer k steps, below 25% random).",
            "evidence_of_spatial_reasoning": "Performance indicates lack of reliable multi-step spatial reasoning and inability to maintain coherent intermediate state across steps; occasional improvement for k=1 suggests limited short-range reasoning.",
            "comparisons": "Underperforms GPT-4o, Gemini-2.0-Flash, and Qwen-2.5-72B on multi-step tasks. CoT improved k=1 but caused catastrophic degradation for k≥2, unlike some other models.",
            "limitations_or_failure_cases": "Catastrophic failure under CoT for longer multi-step sequences (accuracy as low as 10%); inconsistent generation behavior and inability to follow structured multi-step assembly reasoning.",
            "uuid": "e8333.3",
            "source_info": {
                "paper_title": "LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Emu3",
            "name_full": "Emu3",
            "brief_description": "An open-source next-token-prediction based MLLM evaluated on LEGO-Puzzles that displayed severe response biases (e.g., defaulting to a single option) and poor sequence-generation performance on ordering tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Emu3",
            "model_description": "Open-source multimodal model (next-token-prediction) included in the evaluation; evaluated zero-shot on LEGO-Puzzles VQA and ordering tasks.",
            "model_size": null,
            "puzzle_name": "LEGO-Puzzles (VQA: Ordering task)",
            "puzzle_type": "Multi-step sequencing (ordering of assembly steps) and 3D assembly reasoning.",
            "task_setup": "Zero-shot multiple-choice prompts asking for ordered sequences (e.g., 'BACD'), explicit instruction about output format included; evaluated for ability to output valid constrained sequences.",
            "mechanisms_or_strategies": "Standard prompting; no explicit CoT benefit reported; model displayed biased output strategies rather than genuine stepwise reasoning.",
            "performance_metrics": "Severe failure in Ordering: e.g., Emu3 answered 'B' for 98 out of 100 test cases (near-constant single-letter bias), effectively scoring near 0 on the Ordering task.",
            "evidence_of_spatial_reasoning": "No convincing evidence of spatial reasoning; persistent single-label bias indicates inability to follow structured sequential reasoning prompts and suggests training or decoding issues rather than spatial understanding.",
            "comparisons": "Compared unfavorably to proprietary models (Gemini, GPT-4o) and several other open-source models; contrasted with human experts who perform well on ordering tasks.",
            "limitations_or_failure_cases": "Output bias (repeated single choice), inability to generate valid multi-element orderings, likely poor constrained-output handling and lack of genuine stepwise dependency reasoning.",
            "uuid": "e8333.4",
            "source_info": {
                "paper_title": "LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Emu2",
            "name_full": "Emu2",
            "brief_description": "An open-source multimodal model evaluated for image-generation on LEGO tasks; preserves some appearance identity but fails at instruction-following in generation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Emu2",
            "model_description": "Open-source multimodal model supporting image output evaluated on generation tasks (Rotation*, Multiview*, Position*, Dependency*, Next-Step*).",
            "model_size": null,
            "puzzle_name": "LEGO-Puzzles (Generation tasks: Rotation*, Multiview*, Position*, Dependency*, Next-Step*)",
            "puzzle_type": "Image generation/editing for 3D assembly steps and viewpoint changes.",
            "task_setup": "Given assembly illustration and instructions, model must generate intermediate assembly images; generation quality scored by human experts on Appearance (App) and Instruction Following (IF) from 0–3.",
            "mechanisms_or_strategies": "Model seems oriented to image reconstruction/preservation rather than following sequential assembly instructions for generation.",
            "performance_metrics": "Human-evaluated: Emu2 shows some capability in preserving visual appearance (App) but fails entirely in instruction-following (IF score near 0 in many cases); exact numeric scores not provided in text.",
            "evidence_of_spatial_reasoning": "Preservation of appearance suggests visual consistency, but failure to follow instructions indicates lack of reasoning about spatial transformations.",
            "comparisons": "Compared to Gemini-2.0-Flash (which has both App and IF strengths) and GPT-4o (which semantically reconstructs scenes), Emu2 is worse at following stepwise instructions despite appearance fidelity.",
            "limitations_or_failure_cases": "Treats tasks as image reconstruction rather than reasoning-driven image generation; cannot perform sequential modifications required by assembly tasks.",
            "uuid": "e8333.5",
            "source_info": {
                "paper_title": "LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "GILL",
            "name_full": "GILL",
            "brief_description": "An open-source multimodal generation model evaluated on image-generation for LEGO tasks; performs poorly, often producing irrelevant outputs and nearly zero instruction-following scores.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GILL",
            "model_description": "Open-source multimodal image-generation-capable model evaluated on LEGO-Puzzles generation tasks; supports long-range sequence input and image output.",
            "model_size": null,
            "puzzle_name": "LEGO-Puzzles (Generation tasks: Rotation*, Multiview*, etc.)",
            "puzzle_type": "Instruction-driven image generation for 3D assembly sequences.",
            "task_setup": "Zero-shot generation from assembly illustrations; human expert scoring on Appearance and Instruction Following (0–3).",
            "mechanisms_or_strategies": "Appears to rely on generic image-generation capabilities without robust instruction-following or sequential-edit mechanisms.",
            "performance_metrics": "Human-evaluated scores near zero for instruction-following; generated images often unrelated to expected results (exact numbers not provided).",
            "evidence_of_spatial_reasoning": "No evidence of reliable spatial reasoning in generation; fails to produce relevant intermediate assembly states.",
            "comparisons": "Performs worse than proprietary models (Gemini-2.0-Flash, GPT-4o) and even worse than some other open-source models like Emu2 in terms of instruction following.",
            "limitations_or_failure_cases": "Nearly complete failure to follow instructions and generate relevant outputs; instruction-following scores close to zero.",
            "uuid": "e8333.6",
            "source_info": {
                "paper_title": "LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Anole",
            "name_full": "Anole",
            "brief_description": "An open-source autoregressive multimodal model evaluated for interleaved image-text generation on LEGO generation tasks, performing very poorly in instruction-following and generation relevance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Anole",
            "model_description": "Open-source autoregressive native large multimodal model supporting interleaved image-text generation; evaluated on LEGO-Puzzles image-generation tasks.",
            "model_size": null,
            "puzzle_name": "LEGO-Puzzles (Generation tasks)",
            "puzzle_type": "3D assembly sequential image generation/editing.",
            "task_setup": "Zero-shot generation from assembly instructions/images; human scoring for Appearance and Instruction Following.",
            "mechanisms_or_strategies": "Autoregressive interleaved image-text generation without demonstrated capacity for stepwise spatial editing in this benchmark.",
            "performance_metrics": "Reported to perform worst among generation models evaluated (GILL and Anole perform the worst), with instruction-following scores close to zero; generated images often unrelated to expected results.",
            "evidence_of_spatial_reasoning": "No substantive evidence of spatial reasoning in generated outputs; failures indicate inability to follow sequential spatial instructions.",
            "comparisons": "Worse than proprietary editors (Gemini-2.0-Flash) and worse than Emu2 in appearance preservation; similar poor behavior as GILL.",
            "limitations_or_failure_cases": "Fail to generate relevant outputs in nearly all cases; instruction-following almost absent.",
            "uuid": "e8333.7",
            "source_info": {
                "paper_title": "LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Open-source MLLMs (aggregate)",
            "name_full": "Open-source Multimodal Large Language Models (aggregate group)",
            "brief_description": "Collection of evaluated open-source MLLMs (e.g., VILA1.5-13B, Idefics3-8B, DeepSeek-VL2, Pixtral-12B, LLaVA-OneVision-7B, EMU3, Emu2, GILL, Anole) that largely perform poorly on LEGO-Puzzles, many near or below random baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Various open-source MLLMs (VILA1.5-13B, Idefics3-8B, DeepSeek-VL2, Pixtral-12B, LLaVA-OneVision-7B, EMU3, etc.)",
            "model_description": "A set of open-source vision-language models evaluated in zero-shot setting on LEGO-Puzzles VQA and some on generation; specific architectures vary by model.",
            "model_size": null,
            "puzzle_name": "LEGO-Puzzles (VQA & some Generation)",
            "puzzle_type": "3D assembly spatial understanding and sequential reasoning VQA; some image generation tasks.",
            "task_setup": "Zero-shot multiple-choice VQA across 11 tasks; generation evaluation for subset of tasks with human scoring.",
            "mechanisms_or_strategies": "Standard prompting; some models tested with CoT; many open-source models show output biases or inability to follow structured prompts (e.g., defaulting to single letter), and poor instruction-following in generation.",
            "performance_metrics": "Majority of open-source models perform at or below random baseline on many tasks (text reports: 20 models, 14 open-source; many below random; 4/14 open-source models scored zero on Ordering). Examples: Emu3 answered 'B' for 98/100 cases; 11/20 models scored lower than random on Height task.",
            "evidence_of_spatial_reasoning": "Aggregate results demonstrate weak spatial reasoning: widespread failures on height perception (2D vs 3D illusions), ordering and multi-step tasks; little evidence that open-source models track multi-step spatial state robustly.",
            "comparisons": "Consistently underperform proprietary models (Gemini-2.0-Flash, GPT-4o). Human experts far outperform open-source group (humans ≈93.6%).",
            "limitations_or_failure_cases": "Frequent failures: answer-format/response biases in ordering tasks, inability to follow structured prompts, poor height/rotation perception (reliance on 2D priors), and near-zero instruction-following in generation.",
            "uuid": "e8333.8",
            "source_info": {
                "paper_title": "LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "DSRBench: A comprehensive 3d spatial reasoning benchmark",
            "rating": 2,
            "sanitized_title": "dsrbench_a_comprehensive_3d_spatial_reasoning_benchmark"
        },
        {
            "paper_title": "SAT: Spatial aptitude training for multimodal language models",
            "rating": 2,
            "sanitized_title": "sat_spatial_aptitude_training_for_multimodal_language_models"
        },
        {
            "paper_title": "CAD-GPT: Synthesising cad construction sequence with spatial reasoning-enhanced multimodal llms",
            "rating": 2,
            "sanitized_title": "cadgpt_synthesising_cad_construction_sequence_with_spatial_reasoningenhanced_multimodal_llms"
        },
        {
            "paper_title": "Coarse correspondences boost spatial-temporal reasoning in multimodal language models",
            "rating": 1,
            "sanitized_title": "coarse_correspondences_boost_spatialtemporal_reasoning_in_multimodal_language_models"
        },
        {
            "paper_title": "Manipllm: Embodied multimodal large language model for object-centric robotic manipulation",
            "rating": 1,
            "sanitized_title": "manipllm_embodied_multimodal_large_language_model_for_objectcentric_robotic_manipulation"
        }
    ],
    "cost": 0.016511249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?
25 Mar 2025</p>
<p>Kexian Tang tangkexian@pjlab.org.cn 
Tongji University</p>
<p>Simons Institute
BerkeleyUC</p>
<p>Junyao Gao gaojunyao@pjlab.org.cn 
Tongji University</p>
<p>Simons Institute
BerkeleyUC</p>
<p>Yanhong Zeng zengyanhong@pjlab.org.cn 
Tongji University</p>
<p>Haodong Duan duanhaodong@pjlab.org.cn 
Tongji University</p>
<p>Yanan Sun sunyanan@pjlab.org.cn 
Tongji University</p>
<p>Zhening Xing xingzhening@pjlab.org.cn 
Tongji University</p>
<p>Wenran Liu liuwenran@pjlab.org.cn 
Tongji University</p>
<p>Kaifeng Lyu kaifenglyu@berkeley.edu 
Kai Chen chenkai@pjlab.org.cn 
Tongji University</p>
<p>C Multiview </p>
<p>Shanghai AI Laboratory</p>
<p>LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?
25 Mar 2025AAD7CAB5F2ED238DCF38702581D6EE9FarXiv:2503.19990v1[cs.AI]A. 30°B . 60°C . 90°D . 120°R otation <image 1> <image 2> This is the target assembly image: <image 1>Which option (ABCor D) correctly shows the correct assembly step? Options: A. C. B. D. <image 1>
Multi-step spatial reasoning entails understanding and reasoning about spatial relationships across multiple sequential steps, which is crucial for tackling complex realworld applications, such as robotic manipulation, autonomous navigation, and automated assembly.To assess how well current Multimodal Large Language Models (MLLMs) have acquired this fundamental capability, we introduce LEGO-Puzzles, a scalable benchmark designed to evaluate both spatial understanding and sequential reasoning in MLLMs through LEGO-based tasks.LEGO-Puzzles consists of 1,100 carefully curated visual questionanswering (VQA) samples spanning 11 distinct tasks, ranging from basic spatial understanding to complex multi-step reasoning.Based on LEGO-Puzzles, we conduct a comprehensive evaluation of state-of-the-art MLLMs and uncover significant limitations in their spatial reasoning capabilities: even the most powerful MLLMs can answer only about half of the test cases, whereas human participants achieve over 90% accuracy.In addition to VQA tasks, we evaluate MLLMs' abilities to generate LEGO images following assembly illustrations.Our experiments show that only Gemini-2.0-Flashand GPT-4o exhibit a limited ability to follow these instructions, while other MLLMs either replicate the input image or generate completely irrelevant outputs.Overall, LEGO-Puzzles exposes critical deficiencies in existing MLLMs' spatial understanding and sequential reasoning capabilities, and underscores the need for further advancements in multimodal spatial reasoning.</p>
<p>Introduction</p>
<p>Spatial intelligence [5] has attracted growing attention due to its significance in various applications, including robotics control [22,28], autonomous driving [18,52], and automated assembly [12].These complex real-world applications inherently require advanced multi-step spatial reasoning capabilities, which involve perceiving 3D-aware spatial relationships and reasoning about them across multiple sequential steps [5,44,58].With the rapid advancement of Large Language Models (LLMs) [3,17,41,49], Multimodal Large Language Models (MLLMs) [8,34,42,48,50] have also witnessed significant progress in perceiving visual information and interacting with humans through natural language.While MLLMs have made remarkable strides in fundamental tasks such as object recognition [14,29] and optical character recognition [16,36,39,46], existing evaluations [31,37] suggest that their spatial reasoning abilities are still limited.</p>
<p>Research on evaluating MLLMs' multi-step spatial reasoning capabilities remains largely unexplored.Existing studies primarily focus on assessing the spatial understanding capability, which pertains to the comprehension of a static scene.Some works [21,30,53] employ synthetic environments to render multiple simple 3D objects and then query the spatial relationships between them.However, such question-answering (QA) tasks tend to be overly simplistic for MLLMs to handle, lacking the diversity and complexity of real-world scenarios.Other studies [31,38] construct spatial understanding tasks based on natural images, but this approach often involves manual annotations, which may limit scalability.Moreover, most existing evaluations rarely evaluate reasoning over sequences of spatial transformations or actions, leaving the multi-step aspect of spatial reasoning largely unaddressed.</p>
<p>In this work, we take inspiration from a common recreational activity, LEGO construction, to design a comprehensive evaluation framework for assessing the multi-step spatial reasoning capabilities of MLLMs.The assembly process of a complete LEGO model typically encompasses dozens or even hundreds of discrete construction steps, providing an ideal foundation for testing sequential reasoning abilities.Each step requires accurate comprehension of geometry, orientation, and connection mechanisms of LEGO pieces to successfully follow the provided illustrations.Based on publicly available LEGO projects with detailed step-by-step assembly instructions, we introduce LEGO-Puzzles, a novel benchmark specifically engineered to evaluate MLLMs' multi-step spatial reasoning capabilities.In total, LEGO-Puzzles encompasses a diverse collection of over 1,100 carefully curated visual questionanswering (VQA) pairs spanning 11 distinct tasks, which fall into three major categories.First, we develop a set of fundamental tests to assess MLLMs' basic spatial understanding capabilities, including recognition of height relationships, rotational transformations, adjacency patterns, and viewpoints within 3D space.Building upon this foundation, we construct both single-step and multi-step sequential reasoning evaluations based on LEGO assembly sequences to examine models' sequential reasoning ability.These advanced tests include identifying the configuration of intermediate assembly states (single-step) or determining the correct order of multiple intermediate LEGO states (multi-step).</p>
<p>LEGO-Puzzles offers several distinctive advantages compared to existing spatial understanding benchmarks: 1) Enhanced visual richness.Unlike synthetic datasets such as CLEVR [21,30], which utilize rendered primitive shapes, LEGO-based questions present significantly greater visual complexity and diversity.2) Superior scalability.A single LEGO assembly instruction manual can generate hundreds of unique evaluation questions, which enables efficient benchmark expansion with minimal additional resource investment.</p>
<p>Leveraging LEGO-Puzzles, we conduct comprehensive evaluations of 20 state-of-the-art MLLMs, including proprietary models such as GPT-4o and Gemini-2.0-Flash,as well as leading open-source alternatives [8,43,50,56].Our experimental results reveal a substantial gap between current MLLMs and human-level proficiency.Even the strongest models struggle with basic spatial understanding tasks, such as accurately identifying the height of LEGO pieces and determining adjacency relationships in 3D space.Among open-source models, only a few achieve performance notably above random guessing across different tasks.</p>
<p>Beyond VQA tasks, LEGO-Puzzles also enables the assessment of spatially grounded image generation.For instance, given an assembly illustration, an MLLM is tasked with generating an image of the intermediate state following the specified assembly operation.In these generation tests, most of the evaluated models fail completely, either disregarding the provided instructions or generating images that are entirely irrelevant to the intended LEGO configuration.</p>
<p>In summary, our novel benchmark LEGO-Puzzles provides a comprehensive evaluation of the spatial understanding and sequential reasoning capabilities of MLLMs.Our main contributions are as follows:</p>
<p>• A novel benchmark for spatial understanding.Based on LEGO constructions, our benchmark LEGO-Puzzles offers natural and diverse test cases for evaluating the spatial understanding capabilities of MLLMs, with improved visual richness and scalability over existing datasets.• Evaluation of multi-step spatial reasoning.Built upon LEGO's step-by-step building process, LEGO-Puzzles is the first benchmark explicitly designed to assess multistep spatial reasoning, where each task requires reasoning over up to 7 LEGO construction steps.• Comprehensive assessment on visual question answering and image generation.LEGO-Puzzles assesses the spatial reasoning capability of LLMs across both VQA and image generation tasks, providing a comprehensive assessment of their ability to comprehend and process spatial information in a human-like manner.</p>
<p>Related Work</p>
<p>General Multi-Modal Evaluation Benchmarks.Recent years have seen significant advancements in multimodal large language models (MLLMs), accompanied by a surge in benchmark datasets evaluating their visual understanding.Several comprehensive benchmarks have been introduced to assess various multimodal capabilities.MME [14] provides a systematic evaluation of 14 image-centric tasks, revealing persistent challenges such as object hallucination and spatial reasoning failures.MMBench [37] introduces a bilingual multiple-choice format for fine-grained multimodal assessment.Moving beyond static images, SEED-Bench [25] evaluates generative comprehension across 19K Q&amp;A pairs spanning both image and video reasoning, showing that temporal understanding remains a major limitation.For expert-level reasoning, MMMU [60] presents a discipline-specific benchmark across 183 subtopics, revealing substantial knowledge gaps even in leading MLLMs even in leading MLLMs, such as GPT-4o and Gemini.Overall, these benchmarks reveal that while MLLMs have made progress, they still struggle with spatial understanding, temporal coherence, multimodal integration, and highlevel reasoning, presenting clear directions for future research.</p>
<p>Visual-Spatial Understanding in MLLMs.Multimodal large language models (MLLMs) have made significant strides in vision-and-language tasks, yet they still struggle with 3D spatial understanding.Benchmarks such as 3DSRBench [38] show that even the most advanced models achieve only 45-50% accuracy on 3D spatial tasks and experience substantial performance drops under unusual camera angles.To enhance spatial reasoning, several studies have explored Chain-of-Thought (CoT) prompting.For example, Park et al. [44] demonstrate that combining CoT with explicit image-to-text conversion can improve gener-alization from simple to hard visual reasoning tasks.However, beyond such tailored interventions, traditional CoT prompting alone has generally failed to improve spatial reasoning performance [58].In response, alternative approaches have emerged.Spatially enriched datasets, such as Spatial Aptitude Training (SAT) [45], significantly boost zero-shot performance across real-image benchmarks.Architectural innovations like CAD-GPT [51], which embeds 3D coordinates into language representations, and MVoT [27], which introduces visual sketching during inference, further expand the solution space.Additionally, lightweight strategies like Coarse Correspondences [33] improve spatial understanding without requiring model fine-tuning.Despite these advances, achieving human-level 3D spatial reasoning in MLLMs remains an open challenge.</p>
<p>LEGO-Puzzles</p>
<p>In this section, we introduce LEGO-Puzzles, a diverse and comprehensive benchmark designed to evaluate the multi-step spatial reasoning capability of MLLMs in detail.Specifically, we first introduce the motivation and definition of each task in Section 3.1.Then, we introduce our dataset curation process, including data collection, question-answer generation, and quality control, in Section 3.2.</p>
<p>Task Definition</p>
<p>To enhance the evaluation of multi-step spatial reasoning for Multimodal Large Language Models (MLLMs), we define three primary categories of tasks based on insights from cognitive psychology and human experiences in developing relevant skills [5,40,55].Using LEGO building as a concrete example of how humans develop spatial intelligence, we find that individuals typically engage in the following processes: First, they must understand the spatial relationships between each LEGO piece and how these pieces relate from different perspectives in 3D space.Next, they need to reason through the dependencies and assembly logic of each block at every step of the building process.Finally, they extend their reasoning to multi-step reasoning across the entire assembly sequence.To achieve this, our tasks range from fundamental spatial understanding (36.4%) to single-step sequential reasoning (36.4%) and, ultimately, to multi-step sequential reasoning (27.3%), as illustrated in Figure 1.Below, we provide further details on each task.In conclusion, LEGO-Puzzles consists of over 1,100 visual question-answering (VQA) pairs derived from 407 LEGO building instructions, encompassing 11 tasks across spatial understanding, single-step and multi-step sequential reasoning.In addition to VQA tasks, We further extend several sub-tasks of spatial understanding (Rotation<em> and Multiview</em>) and single-step sequential reasoning (Position<em>, Dependency</em> and Next-Step*) to include image generation following [57], as part of the visual Generation evaluation of MLLMs.</p>
<p>Dataset Curation</p>
<p>As illustrated in Figure 3, our pipeline consists of three key steps: data collection, question-answer generation, and quality control.This design ensures the scalability, accuracy, and reliability of our data.Data Collection.Data collection consists of three stages.First, we collect a diverse set of open-source LEGO source files from the Internet, which include comprehensive stepby-step LEGO building instructions, visualizations, and the required LEGO pieces for each step.Notably, the camera perspective remains consistent across all steps within a specific instruction, ensuring temporal, spatial, and logical coherence throughout the building process.Second, we render LEGO building instruction files as PDF files using a publicly available rendering software, Studio1 .This tool enables us to adjust default rendering settings to construct tasks that evaluate spatial relationships at varying levels of complexity.Specifically, for the Rotation and Multiview tasks, we utilize the Persistence of Vision Raytracer (POV-Ray) style and modify the lighting strength to generate realistic LEGO images from different angles.For the Backward task, we also edit attributes such as color, quantity, and assembly positions of pieces to create erroneous images.Finally, we use PDF-Extract2 to extract all LEGO pieces and objects of interest from the rendered PDF files.All images are systematically organized according to a uni-  Our question-answer template includes instructions, questions, and answers.Here, we provide an example from the Position task for reference.</p>
<p>fied naming standard and prepared for question-answer generation across different tasks.</p>
<p>Question-Answer Generation.To ensure the scalability of our pipeline, we design several task-specific templates for question-answer generation.See Figure 4 for an example.</p>
<p>Each data example includes an instruction, a question, and an answer.To meet the requirements of different tasks, we create LEGO sequences of varying lengths.Note that the image token <image x> serves as a placeholder for the corresponding image input here.Quality Control.We implement a rigorous human review process to maintain high quality and minimize errors.Specifically, we carefully examine the consistency between LEGO objects in source files and rendered PDF files, conducting checks for duplication, adherence to standards, and formatting in the generated images.Additionally, we apply cross-validation to ensure that each question-answer pair aligns with its task-specific template and that the image annotations are accurate, as verified by multiple annotators.This rigorous process ensures high-quality and reliable evaluation.VILA1.5-13B[32], Idefics3-8B [24], DeepSeek-VL2-[Tiny/Small] [56], Pixtral-12B [1], LLaVA-OneVision-7B [26], and EMU3 [54].For proprietary models, we evaluate Claude-3.5-Sonnet[2], Gemini-1.5-Flash,Gemini-1.5-Pro,Gemini-2.0-Flash[48], GPT-4o (20241120), and GPT-4o-mini [42].For the additional image Generation evaluation, we evaluate the open-source models Emu2 [47], GILL [23], and Anole [9], as well as the proprietary models GPT-4o and Gemini-2.0-Flash,all of which support longrange sequence input and image output.Moreover, all evaluations are conducted in a zero-shot setting for a fair comparison.</p>
<p>Evaluation on LEGO-Puzzles</p>
<p>Baselines.We provide two baselines for comparison:</p>
<p>• Random indicates the accuracy of random selection for each question, assuming equal probability for all options.• p-value-based critical value indicates the minimum accuracy required to statistically surpass random guessing at a given significance level (p = 0.05).Evaluation Metrics.We calculate the accuracy (%) of verbal answers to multiple-choice questions using exact matching as our primary metric, following Duan et al. [11].When models fail to generate an answer in the required format, we utilize the ChatGPT-0125 [13] method from VLMEvalKit [11] as a fallback option.Additionally, we randomly select 20 questions from each task to create LEGO-Puzzles-Lite, resulting in a total of 220 question-answer pairs.This dataset is designed to investigate the performance gap between human intelligence, as evaluated by additional human annotators, and current models in patial Understanding and Sequential Reasoning.In the Generation evaluation, traditional metrics such as FID [20], CLIPScore [15,19], and X-IQE [6] are inadequate for assessing interleaved outputs in visual answers.Therefore, we enlist human experts to evaluate performance based on appearance similarity and instruction following, using a scoring scale from 0 to 3.This approach is necessary due to biases present in VLM-based scoring [35].</p>
<p>Main Results</p>
<p>We include evaluation results for spatial understanding, sequential reasoning, and generation in Table 1, Table 2, and Table 3.We summarize key findings as below.Challenges of LEGO-Puzzles.Our findings indicate that human experts consistently achieve significantly higher overall performance (93.6%), as shown in Table 2.In contrast, current MLLMs fall short, with even the most advanced models, Gemini-2.0-Flashand GPT-4o, trailing over 30% behind human performance across all tasks.This persistent gap highlights the need for comprehensive and substantial improvements in our LEGO-Puzzles.</p>
<p>Gap between Open-source and Proprietary Models.</p>
<p>There is a significant gap between open-source and proprietary MLLMs in both spatial understanding and sequential reasoning abilities.Most open-source MLLMs perform   Table 3. Evaluation on Generation.We conduct human-based evaluation to assess the "Appearance" (App) and "Instruction Following" (IF) scores of Gemini-2.0-Flash,GPT-4o, Emu2, GILL, and Anole, using a scoring scale from 0 to 3 for both dimensions.</p>
<p>20 models performing below Random in both tasks.Conversely, most models achieve accuracy above the critical threshold in the Multiview task, indicating that existing MLLMs possess basic spatial modeling capabilities.However, MLLMs' performance is notably weaker in multi-step sequential reasoning tasks such as Ordering and Outlier compared to single-step spatial reasoning tasks like Dependency and Next-Step.This disparity highlights the models' limitations in capturing long-range dependencies and executing effective sequential reasoning.</p>
<p>In conclusion, our LEGO-Puzzles highlights both the spatial understanding and sequential reasoning abilities of MLLMs.Results in Table 1 demonstrate that GPT-4o achieves the highest performance.However, the overall results suggest significant room for improvement, particularly in domains involving relative relationships, rotation perception, and long-range sequential reasoning.</p>
<p>Image Generation Evaluation</p>
<p>As mentioned in Section 3.1, we evaluate image generation ability across several tasks related to spatial understanding (Rotation<em> and Multiview</em>) and single-step sequential reasoning (Position<em>, Dependency</em> and Next-Step*) as part of the Generation assessment.As shown in Table 3, all MLLMs struggle to simultaneously maintain appearance identity and strictly adhere to user instructions when generating image answers across all tasks.This poor performance indicates that existing MLLMs are ineffective at visualizing spatial understanding and sequential reasoning capabilities, underscoring the challenge of integrating multimodal information effectively.</p>
<p>Exploring Multi-Step Sequential Reasoning</p>
<p>Experiments in Section 4.2 show that current MLLMs perform poorly when extending single-step sequential reasoning QA to multi-step tasks.To further investigate the underlying reasons for these performance variations in sequential reasoning tasks, we design a fine-grained sequential reasoning task called Next-k-Step, which explicitly controls the number of steps required to complete the task.Experimental Setup.Next-k-Step builds upon our singlestep sequential reasoning task, Next-Step, and requires MLLMs to identify the correct LEGO object by sequentially adding k additional LEGO pieces to the current LEGO object.We set k = 1, 2, 3, 4, 5 and construct 20 test cases for each k value.Specifically, we input current LEGO object (x 1 ), next k LEGO pieces (x 2 , x 3 , . . ., x k+1 ) and the target LEGO object (x k+2 ), along with the corresponding text instructions into MLLMs, excepting them to generate the correct answer from four options (A, B, C, D).Additionally, to investigate the effectiveness of the widely adopted Chain of Thought (CoT) approach from the LLM community in enhancing multi-step sequential reasoning, we designed experiments comparing model performance under two conditions: standard prompting (without CoT) and explicit step-by-step reasoning (with CoT).We conduct experiments using the four top-performing models on the Next-</p>
<p>Step task, GPT-4o, Gemini-2.0-Flash,Qwen-2.5-72B, and Internvl-2.5-78B.</p>
<p>Performance Degradation when k Increases.As shown in Table 4, the relationship between accuracy and the number of reasoning steps varies across models.GPT-4o and Gemini-2.0-Flashexhibit a clear performance decline as k increases.These results align with the performance discrepancy between single-step and multi-step sequential reasoning in Table 1, further demonstrating that current MLLMs struggle to handle multi-step sequential relationships requiring iterative reasoning.A key challenge lies in the accumulation of errors as reasoning steps increase.Each intermediate inference introduces potential deviations that can compound over multiple steps, leading to significant inconsistencies in the final predictions.Additionally, MLLMs might lack an explicit visual memory mechanism compared to LLMs in language memory, making it difficult to coherently track and integrate positional changes throughout the reasoning process.Surprisingly, we observe that Qwen-2.5-72Bachieves stable and relatively consistent accuracy scores (around 65.0%) across all values of k, even when k is increased to seven.And for Internvl-2.5-78B,accuracy scores are close to random guessing.Limited Effectiveness of Chain-of-Thought (CoT).By applying CoT prompting, we observe significant improvements when k = 1 for GPT-4o and InternVL-2.5-78B.However, this effect diminishes for k ≥ 2, where accuracy even declines dramatically.This is because these MLLMs perform worse than random guessing (25%) when dealing with longer k steps, with accuracies of 5% for GPT-4o and 10% for InternVL-2.5-78B.For other MLLMs like Gemini-2.0-Flashand Qwen-2.5-72B,CoT prompting does not provide obvious benefits, as they fail to perform genuine stepby-step reasoning in their CoT responses.</p>
<p>Task PCC P-value Height 0.93 0.00723 Adjacency 0.98 0.00046</p>
<p>Consistency Compared with Natural Dataset</p>
<p>Besides its high scalability as a virtual framework, LEGO-Puzzles also demonstrates strong consistency with the natural environment.To verify this, we compare LEGO-Puzzles with 3DSRBench [38], which includes several similar tasks (Height in LEGO-Puzzles and 3DSRBench, Adjacency in LEGO-Puzzles and Location in 3DSRBench) but focuses on real-world domain images.Specifically, we evaluate all proprietary models tested on LEGO-Puzzles within the 3DSRBench dataset and compute the Pearson correlation coefficient [10] to measure the accuracy correlation between these two benchmarks.The results in Table 5 indicate that model performance on LEGO-Puzzles reliably reflects trends observed in natural data.</p>
<p>Task Similarity</p>
<p>In this subsection, we analyze task similarity in our benchmark by calculating the average rank correlation between each and all others, as proposed by Zhang et al. [61].The similarity score for each task is derived from the average correlation between its ranking and those of all other tasks, using three metrics: Spearman Rank Correlation Coefficient (SRCC), Pearson Linear Correlation Coefficient (PLCC), and R² Score (R-squared Coefficient of Determination).</p>
<p>The results in Figure 5 show that only a few task pairs have strong correlations, such as Next Step to Dependency, Multi-step to Single-step Sequential Reasoning, and Spatial Understanding.This is because they either share similar image inputs or integrate step-wise logical progression and spatial comprehension.However, most tasks exhibit moderate to low correlations, ensuring benchmark diversity.More than half of the task pairs have an SRCC between 0.3 and 0.6, indicating limited dependency among tasks.</p>
<p>Overall, our benchmark offers a balanced assessment of MLLMs across various reasoning skills.While some tasks show strong correlations due to conceptual overlap, the majority remain sufficiently independent, providing a comprehensive and distinctive evaluation framework.</p>
<p>Error Analysis</p>
<p>In this section, we conduct a detailed error analysis during the evaluation process of our benchmark, providing insights into model behaviors and shortcomings.Failure in Ordering Task.As shown in Table 1, several open-source models (4/14) completely fail in the Ordering task, with scoring zero.Ordering task requires MLLMs to enable multi-step reasoning ability.Despite explicitly specifying the required answer format in the prompt (e.g., a sequence such as "BACD"), some models were unable to generate a valid response and instead produced arbitrary outputs.For instance, several models (InternVL2.5-8B,Emu3, MiniCPM-V-2-6, and LLava-OneVision-7B) exhibited strong biases, frequently defaulting to a single letter rather than a complete sequence.In extreme cases, models provided nearly identical responses across all test instances, such as Emu3, which answered "B" for 98 out of 100 test cases, demonstrating a lack of genuine reasoning ability.These results indicate that many open-source models struggle with sequence generation and constrained output formatting, suggesting potential issues in their ability to follow structured prompts for reasoning tasks.These biases in responses further suggest that models may be overly reliant on spurious correlations in training data rather than understanding the stepwise dependencies of a given sequence.Challenges in Height Perception: 2D vs. 3D Understanding.In the Height task, we observe that most models (11/20) achieve scores lower than Random.We provide some failure cases in Figure 6, which exhibit noticeable 2D and 3D optical illusions.Since MLLMs are primarily trained on images with a predominantly 2D viewpoint, the discrepancy between 2D and 3D spatial understanding in our Height task often causes MLLMs to answer questions based on a 2D projection rather than a true 3D perspective.Even when we construct instruction prompts that explicitly require MLLMs to comprehend 3D spatial relationships, GPT-4o, the top-performing model, still fails to achieve human-level performance.This observation highlights the tendency of MLLMs to rely on 2D spatial priors during inference, suggesting the need for further research in 3D understanding training.Weak Appearance Consistency and Instruction Following in Image Generation.Our evaluation of MLLMgenerated images reveals substantial differences in instruction following and reasoning-based image synthesis when processing sequential visual inputs.As shown in Table 7, open-source models struggle significantly in both appearance consistency (App) and instruction-following (IF), while proprietary models demonstrate varying degrees of success.Among the proprietary models, Gemini-2.0-Flashexhibits the strongest performance in both appearance and instruction adherence.It effectively follows input constraints and maintains high appearance fidelity, often editing</p>
<p>Height</p>
<p>Question: You will be provided with the current assembly state image <image 1>, the target assembly state image <image 2>, and four step images labeled A, B, C, and D. Your goal is to arrange the four step images in the correct order that transitions from the current state to the target state.</p>
<p>Options:</p>
<p>A. the given image rather than generating a completely new one.This suggests that Gemini-2.0-Flashhas a stronger spatial consistency mechanism, enabling it to make precise modifications while preserving structural coherence.For GPT-4o, the results suggests that GPT-4o may not directly edit the input image but instead interpret its semantic content and generate a new image based on textual understanding.The differences in appearance fidelity and instruction following indicate that GPT-4o's generation process might involve reconstructing the scene conceptually rather than modifying the original image step by step.While this allows it to maintain conceptual relevance, its output often deviates in style and structure from the original input, leading to lower appearance fidelity compared to Gemini-2.0-Flash.Moreover, its instruction following ability remains incon- sistent, particularly for tasks requiring fine-grained reasoning.For open-source models, Emu2 exhibits some capability in preserving visual appearance but fails entirely in instruction-following, treating the task as mere image reconstruction rather than reasoning-based generation.It struggles with spatial dependencies and sequential modifications, making it ineffective for reasoning-intensive tasks.GILL and Anole perform the worst, failing to generate relevant outputs in nearly all cases.Their instruction-following scores are close to zero, and their generated images are often completely unrelated to the expected result.This highlights a fundamental limitation in their ability to process sequential visual transformations, making them unsuitable for complex, instruction-driven image generation.We provide some failure cases in Figure 7.These findings emphasize the fundamental challenges in spatial and sequential reasoning within open-source MLLMs.While Gemini-2.0-Flashshows the most precise adherence to instructions and im-age editing capabilities, GPT-4o tends to generate semantically relevant but visually divergent outputs.Open-source models, by contrast, still lack robust mechanisms for sequential reasoning, underscoring the need for advancements in instruction-following and reasoning-aware image generation techniques.</p>
<p>Answer</p>
<p>Conclusion</p>
<p>We introduce LEGO-Puzzles, a novel benchmark specifically designed to evaluate spatial understanding, as well as single-step and multi-step sequential reasoning in MLLMs.</p>
<p>Inspired by human cognitive patterns in LEGO construction, we create a dataset that includes over 1,100 carefully curated visual question-answering (VQA) samples across 11 distinct tasks, providing diverse scenarios to assess multimodal visual reasoning.We conduct comprehensive experiments with 20 advanced MLLMs, revealing substantial performance gaps compared to humans, particularly in extended sequential reasoning and the generation of spatially coherent visual outputs.These findings underscore the urgent need to enhance the spatial understanding and sequential reasoning capabilities of multimodal AI.</p>
<p>Task 1 :
1
Spatial Understanding.(1) Height: Distinguish the relative heights of LEGO objects.(2) Adjacency: Determine whether LEGO objects are adjacent or separated.(3) Rotation: Calculate the angle of rotation between a LEGO object and its corresponding rotated version.(4) Multiview: Predict the current LEGO status from different viewpoints.Task 2: Single-step Sequential Reasoning.(5) Rotation Status: Assess the rotation status of the next LEGO pieces during assembly.(6) Position: Identify the</p>
<p>Figure 1 .
1
Figure 1.Problem Statistics in LEGO-Puzzles.correct assembly position for the next LEGO pieces.(7) Next-Step: Determine the next LEGO status based on the current status and the upcoming pieces.(8) Dependency: Identify the necessary LEGO pieces required to transition from the current to the next status.Task 3: Multi-Step Sequential Reasoning.(9) Backwards: Identify the correct LEGO status in the assembly pipeline of the LEGO object.(10) Ordering: Predict the correct assembly order of the provided final LEGO images.(11) Outlier: Detect the LEGO status that does not belong to the provided assembly sequence.In conclusion, LEGO-Puzzles consists of over 1,100 visual question-answering (VQA) pairs derived from 407 LEGO building instructions, encompassing 11 tasks across spatial understanding, single-step and multi-step sequential reasoning.In addition to VQA tasks, We further extend several sub-tasks of spatial understanding (Rotation<em> and Multiview</em>) and single-step sequential reasoning (Position<em>, Dependency</em> and Next-Step*) to include image generation following[57], as part of the visual Generation evaluation of MLLMs.</p>
<p>Question:Figure 2 .Figure 3 .
23
Figure 2. Task examples of LEGO-Puzzles.From left to right, the columns represent tasks in Spatial Understanding, Single-Step Sequential Reasoning, and Multi-Step Sequential Reasoning.Note: The questions above are slightly simplified for clarity and brevity.</p>
<p>Figure 4 .
4
Figure 4. Task-specific template.Our question-answer template includes instructions, questions, and answers.Here, we provide an example from the Position task for reference.</p>
<p>Table 1 .
1
Full Evaluation Results of 18 MLLMs on LEGO-Puzzles.Dark Gray indicates the best performance for each task among all models and Light Gray indicates the best result among open-source model.We also highlight the top three models based on their overall performance, using Dark Green , Medium Green , and Light Green , respectively.Models Spatial Understanding Single-Step Reasoning Multi-Step Reasoning Overall Height Adjacency Rotation Multiview Next-Step Dependency Rotation Stat.Position Backwards Ordering Outlier LEGO-Puzzles-Lite Human proficiency 70.</p>
<p>Table 4 .
4
Flash Qwen-2.5-72BInternvl-2.5-78B w/o CoT w.CoT w/o CoT w.CoT w/o CoT w.CoT w/o CoT w.CoT Evaluation on Next-k-Step.k represents the number of steps, and CoT refers to adding a "Think step by step before answering" instruction in QA pairs, similar to those in LLMs.</p>
<p>Figure 5 .
5
Figure5.Task Similarity Heatmap.The heatmap illustrates the pairwise correlation between tasks in our benchmark, measured using SRCC, PLCC, and R² scores.</p>
<p>Figure 6 .
6
Figure 6.Visualization of sample failure cases in Height and Ordering.The Ground Truth answer is marked in blue, while the MLLM's answer is marked in red.Note: The questions above are slightly simplified for clarity and brevity.</p>
<p>Question:Figure 7 .
7
Figure 7. Qualitative image generation results for Rotation<em> and Multiview</em> tasks.Note: The questions above are slightly simplified for clarity and brevity.</p>
<p>Task-Specific Template (here for task Position) Instruction:</p>
<p>You are a LEGO 3D assembly position analyzer.Your primary task is to determine the correct assembly point of a given LEGO piece based on the current state and the next part to be installed.You will be provided with images representing the current state (x_0), the part to install (x_1), the state after installation (x_2), and installation options (A, B, C, D).Your goal is to analyze the given images and determine which of the four options (A, B, C, D) shows the correct assembly point for the next part.Your answer should be based solely on the provided LEGO 3D data, without any additional assumptions.Keep your responses clear, direct, and focused on the question.Please respond with only the letter corresponding to your choice (A, B, C, or D).
Question: Based on the current state (x_0), the next part (x_1) to install,and the state after installation (x_2), which of the following images showsthe correct installation point? Current state (x_0):<image 1>Part to install(x_1):<image 2>State after installation (x_2):<image 3> Options: \nA.<image 4>B.<image 5>C.<image 6>D.<image 7>Please select thecorrect answer from the options above.\nAnswer: Ground Truth</p>
<p>Table 2 .
2
Comparing Top-Performing MLLMs with Human Proficiency on LEGO-Puzzles-Lite.The best results are marked in bold.The top three overall performances are highlighted in Dark Green , Medium Green , and Light Green , respectively.
only marginally better than Random, while leading propri-interplay between 2D and 3D perspectives, most modelsetary models, such as Gemini-2.0-Flash and GPT-4o, ex-(11/20) perform worse than Random, with even humanhibit strong spatial reasoning capabilities, achieving overallexperts achieving significantly lower scores than in otheraccuracies of 54.0% and 57.7%, respectively.Model Performance in Different Tasks. In the Heighttask, where height relationships are complicated by the
tasks.In the Rotation and Rotation Status tasks, our findings indicate that models exhibit limited sensitivity to rotationrelated recognition, achieving low scores, with 7 out of</p>
<p>Table 5 .
5
Pearson Correlation Coefficients (PCC) and P-values for Height and Adjacency Tasks</p>
<p>.47 0.37 0.38 0.46 0.39 0.39 0.28 0.35 0.37 0.27 0.47 1.00 0.36 0.40 0.62 0.69 0.25 0.47 0.36 0.57 0.59 0.37 0.36 1.00 0.53 0.69 0.60 0.15 0.71 0.63 0.78 0.63 0.38 0.40 0.53 1.00 0.75 0.86 0.45 0.63 0.48 0.56 0.58 0.46 0.62 0.69 0.75 1.00 0.89 0.48 0.74 0.66 0.73 0.77 .470.44 0.39 0.45 0.39 0.27 0.46 0.37 0.53 0.32 0.47 1.00 0.47 0.31 0.60 0.58 0.13 0.54 0.41 0.59 0.51 0.44 0.47 1.00 0.51 0.72 0.55 0.12 0.69 0.58 0.77 0.60 0.39 0.31 0.51 1.00 0.71 0.81 0.43 0.52 0.39 0.56 0.57 0.45 0.60 0.72 0.71 1.00 0.89 0.39 0.79 0.66 0.82 0.78 .220.20 0.15 0.21 0.15 0.08 0.21 0.14 0.28 0.10 0.22 1.00 0.22 0.10 0.36 0.34 0.02 0.29 0.16 0.35 0.26 0.20 0.22 1.00 0.26 0.53 0.30 0.02 0.47 0.34 0.59 0.36 0.15 0.10 0.26 1.00 0.50 0.66 0.18 0.27 0.16 0.31 0.33 0.21 0.36 0.53 0.50 1.00 0.80 0.15 0.62 0.43 0.67 0.61
SRCC dimensions redundancy mapPLCC dimensions redundancy mapR² dimensions redundancy mapHeight Adjacency Rotation Multiview Next Step Dependency1.00 00.39 0.69 0.60 0.86 0.89 1.00 0.39 0.70 0.61 0.77 0.73Height Adjacency Rotation Multiview Next Step Dependency1.00 00.39 0.58 0.55 0.81 0.89 1.00 0.40 0.68 0.60 0.72 0.70Height Adjacency Rotation Multiview Next Step Dependency1.00 00.15 0.34 0.30 0.66 0.80 1.00 0.16 0.47 0.36 0.52 0.48Rotation Status0.39 0.25 0.15 0.45 0.48 0.39 1.00 0.27 0.49 0.14 0.51Rotation Status0.27 0.13 0.12 0.43 0.39 0.40 1.00 0.17 0.47 0.17 0.49Rotation Status0.08 0.02 0.02 0.18 0.15 0.16 1.00 0.03 0.22 0.03 0.24Position0.28 0.47 0.71 0.63 0.74 0.70 0.27 1.00 0.71 0.81 0.65Position0.46 0.54 0.69 0.52 0.79 0.68 0.17 1.00 0.78 0.87 0.63Position0.21 0.29 0.47 0.27 0.62 0.47 0.03 1.00 0.61 0.76 0.40Backwards0.35 0.36 0.63 0.48 0.66 0.61 0.49 0.71 1.00 0.63 0.63Backwards0.37 0.41 0.58 0.39 0.66 0.60 0.47 0.78 1.00 0.68 0.58Backwards0.14 0.16 0.34 0.16 0.43 0.36 0.22 0.61 1.00 0.47 0.33ordering0.37 0.57 0.78 0.56 0.73 0.77 0.14 0.81 0.63 1.00 0.73ordering0.53 0.59 0.77 0.56 0.82 0.72 0.17 0.87 0.68 1.00 0.82ordering0.28 0.35 0.59 0.31 0.67 0.52 0.03 0.76 0.47 1.00 0.67Outlier0.27 0.59 0.63 0.58 0.77 0.73 0.51 0.65 0.63 0.73 1.00Outlier0.32 0.51 0.60 0.57 0.78 0.70 0.49 0.63 0.58 0.82 1.00Outlier0.10 0.26 0.36 0.33 0.61 0.48 0.24 0.40 0.33 0.67 1.00HeightAdjacencyRotationMultiviewNext StepDependencyRotation StatusPositionBackwardsorderingOutlierHeightAdjacencyRotationMultiviewNext StepDependencyRotation StatusPositionBackwardsorderingOutlierHeightAdjacencyRotationMultiviewNext StepDependencyRotation StatusPositionBackwardsorderingOutlier
https://www.bricklink.com/v3/studio/download. page
https://github.com/opendatalab/PDF-Extract-Kit</p>
<p>Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, arXiv:2410.07073Saurabh Garg, Theophile Gervet, et al. Pixtral 12b. 2024arXiv preprint</p>
<p>The claude 3 model family: Opus, sonnet, haiku. Anthropic, 2024</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Xingxuan Jianwei, Yichang Zhang, Zhenru Zhang, Chang Zhang, Jingren Zhou, Zhou, arXiv:2309.16609Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. 2023arXiv preprint</p>
<p>Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, arXiv:2502.13923Jun Tang, et al. Qwen2. 5-vl technical report. 2025arXiv preprint</p>
<p>Frames of mind: The theory of multiple intelligences. Marc H Bornstein, Journal of Aesthetic Education. 20231986</p>
<p>X-iqe: explainable image quality evaluation for text-to-image generation with visual large language models. Yixiong Chen, Li Liu, Chris Ding, arXiv:2305.108432023arXiv preprint</p>
<p>Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Zhaoyang Hao Tian, Liu, arXiv:2412.052712024arXiv preprint</p>
<p>How far are we to gpt-4v? closing the gap to commercial multimodal models with opensource suites. Zhe Chen, Weiyun Wang, Shenglong Hao Tian, Zhangwei Ye, Erfei Gao, Wenwen Cui, Kongzhi Tong, Jiapeng Hu, Zheng Luo, Ji Ma, Jiaqi Ma, Xiaoyi Wang, Hang Dong, Hewei Yan, Conghui Guo, Botian He, Zhenjiang Shi, Chao Jin, Bin Xu, Xingjian Wang, Wei Wei, Wenjian Li, Bo Zhang, Pinlong Zhang, Licheng Cai, Xiangchao Wen, Min Yan, Lewei Dou, Xizhou Lu, Tong Zhu, Dahua Lu, Yu Lin, Jifeng Qiao, Wenhai Dai, Wang, 20241</p>
<p>Anole: An open, autoregressive, native large multimodal models for interleaved image-text generation. Ethan Chern, Jiadi Su, Yan Ma, Pengfei Liu, arXiv:2407.061352024arXiv preprint</p>
<p>Pearson correlation coefficient. Noise reduction in speech processing. Israel Cohen, Yiteng Huang, Jingdong Chen, Jacob Benesty, Jacob Benesty, Jingdong Chen, Yiteng Huang, Israel Cohen, 2009</p>
<p>Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, Proceedings of the 32nd ACM international conference on multimedia. the 32nd ACM international conference on multimedia2024</p>
<p>Multimodal perception-fusion-control and human-robot collaboration in manufacturing: A review. Jianguo Duan, Liwen Zhuang, Qinglei Zhang, Ying Zhou, Jiyun Qin, The International Journal of Advanced Manufacturing Technology. 13232024</p>
<p>Gpt-3: Its nature, scope, limits, and consequences. Minds and Machines. Luciano Floridi, Massimo Chiriatti, 202030</p>
<p>Mme: A comprehensive evaluation benchmark for multimodal large language models. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Rongrong Ji, ArXiv, abs/2306.1339420231</p>
<p>Styleshot: A snapshot on any style. Junyao Gao, Yanchen Liu, Yanan Sun, Yinhao Tang, Yanhong Zeng, Kai Chen, Cairong Zhao, arXiv:2407.014142024arXiv preprint</p>
<p>Faceshot: Bring any character into life. Junyao Gao, Yanan Sun, Fei Shen, Xin Jiang, Zhening Xing, Kai Chen, Cairong Zhao, arXiv:2503.007402025arXiv preprint</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Drivemllm: A benchmark for spatial understanding with multimodal large language models in autonomous driving. Xianda Guo, Ruijun Zhang, Yiqun Duan, Yuhang He, Chenming Zhang, Shuai Liu, Long Chen, arXiv:2411.131122024arXiv preprint</p>
<p>Clipscore: A reference-free evaluation metric for image captioning. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, Yejin Choi, arXiv:2104.087182021arXiv preprint</p>
<p>Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter, 201730</p>
<p>Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, Ross Girshick, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition20171</p>
<p>Openvla: An open-source vision-language-action model. Jin Moo, Karl Kim, Siddharth Pertsch, Ted Karamcheti, Ashwin Xiao, Suraj Balakrishna, Rafael Nair, Ethan Rafailov, Grace Foster, Pannag Lam, Sanketi, arXiv:2406.092462024arXiv preprint</p>
<p>Generating images with multimodal language models. Jing Yu Koh, Daniel Fried, Russ R Salakhutdinov, Advances in Neural Information Processing Systems. 202336</p>
<p>Building and better understanding visionlanguage models: insights and future directions. Andrés Hugo Laurenc ¸on, Victor Marafioti, Léo Sanh, Tronchon, Workshop on Responsibly Building the Next Generation of Multimodal Foundational Models. 2024</p>
<p>Seed-bench: Benchmarking multimodal llms with generative comprehension. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, Ying Shan, arXiv:2307.161252023arXiv preprint</p>
<p>Llava-onevision: Easy visual task transfer. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, arXiv:2408.033262024arXiv preprint</p>
<p>Imagine while reasoning in space: Multimodal visualization-ofthought. Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulić, Furu Wei, arXiv:2501.075422025arXiv preprint</p>
<p>Manipllm: Embodied multimodal large language model for object-centric robotic manipulation. Xiaoqi Li, Mingxu Zhang, Yiran Geng, Haoran Geng, Yuxing Long, Yan Shen, Renrui Zhang, Jiaming Liu, Hao Dong, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, Ji-Rong Wen, arXiv:2305.10355Evaluating object hallucination in large vision-language models. 2023arXiv preprint</p>
<p>Super-clevr: A virtual benchmark to diagnose domain robustness in visual reasoning. Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin Van Durme, Alan L Yuille, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition20231</p>
<p>Reasoning paths with reference objects elicit quantitative spatial reasoning in large vision-language models. Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, David Acuna, arXiv:2409.097882024arXiv preprint</p>
<p>Vila: On pre-training for visual language models. Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, Song Han, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2024</p>
<p>Benlin Liu, Yuhao Dong, Yiqin Wang, Zixian Ma, Yansong Tang, Luming Tang, Yongming Rao, Wei-Chiu Ma, Ranjay Krishna, arXiv:2408.00754Coarse correspondences boost spatial-temporal reasoning in multimodal language models. 2024arXiv preprint</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, arXiv:2304.08485Visual instruction tuning. 2023arXiv preprint</p>
<p>Holistic evaluation for interleaved text-and-image generation. Minqian Liu, Zhiyang Xu, Zihao Lin, Trevor Ashby, Joy Rimchala, Jiaxin Zhang, Lifu Huang, arXiv:2406.146432024arXiv preprint</p>
<p>On the hidden mystery of ocr in large multimodal models. Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, arXiv:2305.078952023arXiv preprint</p>
<p>Mmbench: Is your multi-modal model an all-around player?. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, European conference on computer vision. Springer20241</p>
<p>Wufei Ma, Haoyu Chen, Guofeng Zhang, Celso M De Melo, Alan Yuille, Jieneng Chen, arXiv:2412.078253DSRBench: A comprehensive 3d spatial reasoning benchmark. 2024. 1, 2, 8arXiv preprint</p>
<p>Ocr-vqa: Visual question answering by reading text in images. Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, Anirban Chakraborty, 2019 international conference on document analysis and recognition (ICDAR). IEEE2019</p>
<p>Early education for spatial intelligence: Why, what, and how. S Nora, Andrea Newcombe, Frick, 201043</p>
<p>. Openai, Chatgpt, </p>
<p>. ArXiv, abs/2303.08774OpenAI. Gpt-4 technical report. 152023</p>
<p>Unveiling the potential of end-side large language models. Openbmb, Minicpm, 2024</p>
<p>Generalizing from simple to hard visual reasoning: Can we mitigate modality imbalance in vlms?. Simon Park, Abhishek Panigrahi, Yun Cheng, Dingli Yu, Anirudh Goyal, Sanjeev Arora, arXiv:2501.0266920251arXiv preprint</p>
<p>Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan A Plummer, Ranjay Krishna, Kuo-Hao Zeng, Kate Saenko, arXiv:2412.07755SAT: Spatial aptitude training for multimodal language models. 2024arXiv preprint</p>
<p>Towards vqa models that can read. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, Marcus Rohrbach, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Generative multimodal models are in-context learners. Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, arXiv:2312.132862023arXiv preprint</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.11805202315arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, arXiv:2409.12191202415arXiv preprint</p>
<p>CAD-GPT: Synthesising cad construction sequence with spatial reasoning-enhanced multimodal llms. Siyu Wang, Cailian Chen, Xinyi Le, Qimin Xu, Lei Xu, Yanzhou Zhang, Jie Yang, Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). the AAAI Conference on Artificial Intelligence (AAAI)2025</p>
<p>Aligning multi-modal large language models with behavioral planning states for autonomous driving. Wenhai Wang, Jiangwei Xie, Chuanyang Hu, Haoming Zou, Jianan Fan, Wenwen Tong, Yang Wen, Silei Wu, Hanming Deng, Zhiqi Li, arXiv:2312.092452023arXiv preprint</p>
<p>3d-aware visual question answering about parts, poses and occlusions. Xingrui Wang, Wufei Ma, Zhuowan Li, Adam Kortylewski, Alan L Yuille, Advances in Neural Information Processing Systems. 202336</p>
<p>Emu3: Next-token prediction is all you need. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, arXiv:2409.188692024arXiv preprint</p>
<p>What is sequential reasoning in childhood?. Marcy Willard, 2022</p>
<p>Deepseek-vl2: Mixture-ofexperts vision-language models for advanced multimodal understanding. Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, arXiv:2412.10302202425arXiv preprint</p>
<p>Peng Xia, Siwei Han, Shi Qiu, Yiyang Zhou, Zhaoyang Wang, Wenhao Zheng, Zhaorun Chen, Chenhang Cui, Mingyu Ding, Linjie Li, arXiv:2410.10139Massive multimodal interleaved comprehension benchmark for large vision-language models. 2024arXiv preprint</p>
<p>Thinking in space: How multimodal large language models see, remember, and recall spaces. Jihan Yang, Shusheng Yang, Anjali W Gupta, Rilyn Han, Li Fei-Fei, Saining Xie, arXiv:2412.14171202413arXiv preprint</p>
<p>Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, arXiv:2408.01800Minicpm-v: A gpt-4v level mllm on your phone. 2024arXiv preprint</p>
<p>Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Zicheng Zhang, Xiangyu Zhao, Xinyu Fang, Chunyi Li, Xiaohong Liu, Xiongkuo Min, Haodong Duan, Kai Chen, Guangtao Zhai, arXiv:2501.13953Redundancy principles for mllms benchmarks. 2025arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
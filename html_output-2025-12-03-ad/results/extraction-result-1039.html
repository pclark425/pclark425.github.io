<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1039 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1039</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1039</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-208139239</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1911.07450v2.pdf" target="_blank">Unsupervised Reinforcement Learning of Transferable Meta-Skills for Embodied Navigation</a></p>
                <p><strong>Paper Abstract:</strong> Visual navigation is a task of training an embodied agent by intelligently navigating to a target object (e.g., television) using only visual observations. A key challenge for current deep reinforcement learning models lies in the requirements for a large amount of training data. It is exceedingly expensive to construct sufficient 3D synthetic environments annotated with the target object information. In this paper, we focus on visual navigation in the low-resource setting, where we have only a few training environments annotated with object information. We propose a novel unsupervised reinforcement learning approach to learn transferable meta-skills (e.g., bypass obstacles, go straight) from unannotated environments without any supervisory signals. The agent can then fast adapt to visual navigation through learning a high-level master policy to combine these meta-skills, when the visual-navigation-specified reward is provided. Experimental results show that our method significantly outperforms the baseline by 53.34% relatively on SPL, and further qualitative analysis demonstrates that our method learns transferable motor primitives for visual navigation.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1039.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1039.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ULTRA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unsupervised Reinforcement Learning of TRAnsferable meta-skills (ULTRA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised curriculum-based adversarial meta-reinforcement learning framework that trains a hierarchical embodied agent to discover transferable sub-policies (motor primitives) in unannotated 3D indoor scenes and then fast-adapt to target-driven visual navigation using a reinitialized master policy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ULTRA meta-learner (hierarchical embodied agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A simulated embodied agent with a shared hierarchical policy: task-specific master policy (φ) learned per task and shared sub-policies (θ) encoding meta-skills. Training uses unsupervised curriculum-based adversarial task generation, actor-critic (A3C) updates for master/sub-policies, and a Reptile-style meta-update for sub-policies; meta-testing fine-tunes only the master policy on a few labeled scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>AI2-THOR (indoor scene suite)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Photo-realistic indoor 3D scenes across four room categories (kitchens, living rooms, bedrooms, bathrooms). The environment yields high-dimensional egocentric RGB observations, a discrete action set (MoveAhead, RotateLeft, RotateRight, LookDown, LookUp, Done), diverse object layouts and placements, and long-horizon navigation tasks (optimal path lengths variable; emphasis on L ≥ 5 trajectories).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Characterized by large visual/state space and long-horizon navigation: metrics used include optimal path length (L, with analysis on L ≥ 5), task difficulty measured by meta-learner success rate r, number of primitive steps executed (n) by generator, and trajectory length generated by the task generator; also noted qualitatively as larger state space than prior tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Measured by number/diversity of training/test scenes (120 scenes total; authors use 60 meta-training, 20 validation, 20 meta-testing), diversity objective D computed as sum over timesteps of KL-divergences between the current task-generator policy and previous policies (Π) to encourage varied target observations/trajectories, and by variety across room categories and object placements.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate; Success weighted by Path Length (SPL); averaged reward during meta-testing; learning speed (iterations to converge).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ULTRA achieves a 53.34% relative improvement in SPL over the baseline (A3C learned-from-scratch baseline) and requires roughly one-third the number of iterations to converge compared to the baseline; ablation: removing hierarchical policy reduced success rate by 3.47 percentage points and SPL by 0.93 points. Exact absolute SPL or success rates not reported numerically in the paper text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper explicitly discusses trade-offs: high environment complexity and diversity (large state space, varied object layouts) make unsupervised skill discovery challenging for generic intrinsic-motivation methods; ULTRA addresses this by (1) generating a curriculum of tasks of increasing difficulty and diversity, and (2) using a hierarchical policy and meta-reinforcement learning to produce transferable sub-policies. The authors argue that curriculum + hierarchy mitigate meta-overfitting and enable learning under high complexity/high variation where prior unsupervised methods (e.g., DIAYN, Curiosity) fail or perform poorly.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Tested implicitly: AI2-THOR (high complexity & high variation) — ULTRA shows the reported 53.34% relative SPL improvement over baseline and faster convergence (~1/3 iterations).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum-based adversarial task generation (task-generator vs meta-learner), hierarchical policy (master + shared sub-policies), meta-reinforcement learning with Reptile-style updates for sub-policies, actor-critic (A3C) for within-task updates; unsupervised pretraining on unannotated scenes followed by few-shot supervised fine-tuning of master policy on labeled scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Yes — evaluated on held-out meta-testing scenes (20 scenes) after unsupervised meta-training on 60 scenes. ULTRA fast-adapts with higher learning speed and higher navigation performance (success rate and SPL) than baselines, particularly on long trajectories (L ≥ 5). The learned sub-policies show consistent behaviors across different scenes (qualitative motor primitives).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Higher sample efficiency vs baseline: meta-testing uses only a few labeled scenes (5 scenes per room type) and ULTRA converges in approximately one-third the iterations required by A3C-from-scratch; exact number of episodes/interactions not reported numerically in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Unsupervised curriculum-based adversarial generation + hierarchical meta-RL can discover transferable motor primitives for embodied navigation in complex, varied indoor scenes. 2) ULTRA significantly improves navigation efficiency (53.34% relative SPL) and learning speed (≈1/3 iterations) versus baselines. 3) Hierarchical decomposition reduces meta-overfitting (ablation shows dropping hierarchy lowers success/SPL). 4) Diversity encouragement in task generation yields varied curricula that cover broader state space. 5) Prior unsupervised methods (DIAYN, Curiosity) underperform in these complex/diverse visual-navigation environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unsupervised Reinforcement Learning of Transferable Meta-Skills for Embodied Navigation', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1039.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1039.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DIAYN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diversity Is All You Need (DIAYN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised skill-discovery algorithm that learns diverse, distinguishable skills by maximizing mutual information between latent skill variable and states, used here as an unsupervised baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Diversity is all you need: Learning skills without a reward function</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DIAYN-trained agent (unsupervised skill agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent trained with DIAYN-style intrinsic objectives to discover diverse skills without external rewards; applied as a baseline (pretrained on meta-training data and then fine-tuned on meta-testing).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>AI2-THOR (as applied baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same AI2-THOR indoor scenes used by the paper: photo-realistic, multi-room types with large state space.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Same indicators as main experiments: large state space and long-horizon paths (L ≥ 5 highlighted). The paper notes DIAYN's difficulty in these more complex/diverse visual navigation environments.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>High variation across scenes/room types and object placements; DIAYN was applied across the same set of meta-training scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Learning speed and success rate when fine-tuned for navigation; compared on averaged reward and SPL.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported qualitatively as performing poorly: DIAYN learns useful skills in other domains but 'does not perform well on visual navigation' here and 'learns slower than A3C (learn from scratch)'; no precise numeric values given in text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper states DIAYN's ineffectiveness is attributable to the higher complexity and diversity (larger state space) of visual navigation environments compared to prior tasks where DIAYN succeeded; thus DIAYN struggles in high-complexity/high-variation settings without task-specific curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Performed poorly in the paper's experimental (high complexity, high variation) AI2-THOR setting — learning slower than A3C and failing to produce useful transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Unsupervised skill discovery (mutual-information objective) applied across meta-training scenes, then fine-tuned for navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>When fine-tuned for the navigation task in held-out scenes, DIAYN-based pretraining did not yield effective fast adaptation compared to ULTRA and was slower than A3C-from-scratch.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Lower sample efficiency in this domain compared to ULTRA and even compared to A3C-from-scratch (qualitative statement only).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DIAYN, while effective in some robotic simulation tasks, fails to transfer effectively for complex, diverse visual-navigation in AI2-THOR without a curriculum or hierarchical meta-learning; environment complexity/diversity limits its utility here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unsupervised Reinforcement Learning of Transferable Meta-Skills for Embodied Navigation', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1039.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1039.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Curiosity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curiosity-driven exploration by self-supervised prediction (Curiosity)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intrinsic-reward exploration method where the agent is rewarded by prediction error of a learned forward/inverse dynamics model in feature space; used here as an unsupervised baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Curiosity-driven exploration by self-supervised prediction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Curiosity-driven agent (intrinsic-motivation baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent trained with an intrinsic curiosity reward (prediction error in visual feature space) to encourage exploration; pretrained on meta-training scenes and then fine-tuned on navigation tasks as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>AI2-THOR (as applied baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same set of photo-realistic indoor scenes with diverse layouts and objects.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Large visual/state space and long-horizon navigation tasks; the paper attributes limited improvement from curiosity to environmental complexity and diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>High (diverse scenes and object placements across meta-training/test splits).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Averaged reward, success rate, SPL, learning speed.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Curiosity baseline yields limited improvement over A3C-from-scratch (qualitative); exact numeric performance values not provided in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper argues that curiosity-based intrinsic rewards provide limited benefit in high-complexity, high-variation visual navigation environments because the state space is larger and more diverse than prior tasks where curiosity helped exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Limited improvement over A3C-from-scratch in AI2-THOR (high complexity & high variation).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Intrinsic-motivation / curiosity-driven exploration pretraining followed by fine-tuning on navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>When fine-tuned on navigation tasks, curiosity-based pretraining provided limited gains and did not match ULTRA's performance.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Noted as less sample-efficient than ULTRA in this domain (qualitative statement).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Curiosity-driven exploration alone is insufficient to provide the transferable meta-skills needed for efficient target-driven navigation in highly complex and varied indoor visual environments without additional structure (curriculum/hierarchy).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unsupervised Reinforcement Learning of Transferable Meta-Skills for Embodied Navigation', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1039.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1039.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>A3C-from-scratch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Asynchronous Advantage Actor-Critic (learned from scratch)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard A3C-based end-to-end policy learning (LSTM-A3C architecture) trained directly on the supervised visual-navigation reward at meta-testing, used as primary baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Asynchronous methods for deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>A3C (learn-from-scratch embodied agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A standard end-to-end reinforcement learning agent using A3C (LSTM-A3C variant) trained from scratch on the few labeled meta-testing scenes without ULTRA's unsupervised pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>AI2-THOR (indoor navigation tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Photo-realistic indoor scenes; same complexity/variation as used for ULTRA evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Large state space, long-horizon tasks; measured by success rate and SPL especially for trajectories with optimal path length L ≥ 5.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Trained/tested on limited labeled scenes (few-shot: 5 scenes per room type) — variation in meta-testing limited by few-shot regime but environment itself remains diverse.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (limited labeled examples at meta-test)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate; SPL; iterations to converge; averaged reward.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Baseline that ULTRA is compared to: ULTRA outperforms A3C-from-scratch with 53.34% relative SPL improvement and faster convergence; A3C-from-scratch converges in ~3x the iterations relative to ULTRA (qualitative/numeric ratio reported). Exact absolute numbers not provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>A3C-from-scratch struggles to generalize given few labeled scenes and high environment complexity/variation; ULTRA's pretraining helps overcome these limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Performed worse than ULTRA in high-complexity/high-variation AI2-THOR tests; required more iterations and achieved lower SPL.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Single-task supervised RL on meta-testing data (learn-from-scratch), no unsupervised pretraining or curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>When trained from scratch on the small set of labeled meta-testing scenes, A3C converged slower and achieved lower navigation efficiency (SPL) than ULTRA; did not generalize as well under few-shot labeled data.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Lower sample efficiency compared to ULTRA; requires roughly three times more iterations to converge (paper states ULTRA needs one-third the iterations of baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A3C trained from scratch on few labeled scenes is outperformed by ULTRA in both sample efficiency and final navigation efficiency in complex/varied indoor environments; highlights benefit of unsupervised meta-skill pretraining and hierarchical decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unsupervised Reinforcement Learning of Transferable Meta-Skills for Embodied Navigation', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1039.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1039.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random policy baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple baseline where the agent takes uniformly random actions at every timestep; used to provide a lower-bound performance reference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Random policy agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent executes random actions from the action set each timestep; baseline for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>AI2-THOR (indoor navigation tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same photo-realistic indoor scenes; random actions do not leverage any learned behaviors and thus perform poorly.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>N/A for learning baseline; environment complexity still high but agent does not adapt.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>High variation of environment; random policy not sensitive to variation.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate; SPL</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Baseline worst-case performance (qualitative; no numeric values provided in text).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Not analyzed specifically; random policy unsurprisingly fails as environment complexity/variation increases.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Very poor (not quantified).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>None (random actions).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Serves as a lower bound; confirms that learned policies must exploit structure to perform in complex/varied navigation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unsupervised Reinforcement Learning of Transferable Meta-Skills for Embodied Navigation', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Diversity is all you need: Learning skills without a reward function <em>(Rating: 2)</em></li>
                <li>Curiosity-driven exploration by self-supervised prediction <em>(Rating: 2)</em></li>
                <li>Unsupervised meta-learning for reinforcement learning <em>(Rating: 2)</em></li>
                <li>Intrinsic motivation and automatic curricula via asymmetric self-play <em>(Rating: 2)</em></li>
                <li>Reptile: a scalable metalearning algorithm <em>(Rating: 2)</em></li>
                <li>AI2-THOR: An Interactive 3D Environment for Visual AI <em>(Rating: 2)</em></li>
                <li>Asynchronous methods for deep reinforcement learning <em>(Rating: 1)</em></li>
                <li>Learning to navigate in complex environments <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1039",
    "paper_id": "paper-208139239",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "ULTRA",
            "name_full": "Unsupervised Reinforcement Learning of TRAnsferable meta-skills (ULTRA)",
            "brief_description": "An unsupervised curriculum-based adversarial meta-reinforcement learning framework that trains a hierarchical embodied agent to discover transferable sub-policies (motor primitives) in unannotated 3D indoor scenes and then fast-adapt to target-driven visual navigation using a reinitialized master policy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ULTRA meta-learner (hierarchical embodied agent)",
            "agent_description": "A simulated embodied agent with a shared hierarchical policy: task-specific master policy (φ) learned per task and shared sub-policies (θ) encoding meta-skills. Training uses unsupervised curriculum-based adversarial task generation, actor-critic (A3C) updates for master/sub-policies, and a Reptile-style meta-update for sub-policies; meta-testing fine-tunes only the master policy on a few labeled scenes.",
            "agent_type": "simulated agent",
            "environment_name": "AI2-THOR (indoor scene suite)",
            "environment_description": "Photo-realistic indoor 3D scenes across four room categories (kitchens, living rooms, bedrooms, bathrooms). The environment yields high-dimensional egocentric RGB observations, a discrete action set (MoveAhead, RotateLeft, RotateRight, LookDown, LookUp, Done), diverse object layouts and placements, and long-horizon navigation tasks (optimal path lengths variable; emphasis on L ≥ 5 trajectories).",
            "complexity_measure": "Characterized by large visual/state space and long-horizon navigation: metrics used include optimal path length (L, with analysis on L ≥ 5), task difficulty measured by meta-learner success rate r, number of primitive steps executed (n) by generator, and trajectory length generated by the task generator; also noted qualitatively as larger state space than prior tasks.",
            "complexity_level": "high",
            "variation_measure": "Measured by number/diversity of training/test scenes (120 scenes total; authors use 60 meta-training, 20 validation, 20 meta-testing), diversity objective D computed as sum over timesteps of KL-divergences between the current task-generator policy and previous policies (Π) to encourage varied target observations/trajectories, and by variety across room categories and object placements.",
            "variation_level": "high",
            "performance_metric": "Success rate; Success weighted by Path Length (SPL); averaged reward during meta-testing; learning speed (iterations to converge).",
            "performance_value": "ULTRA achieves a 53.34% relative improvement in SPL over the baseline (A3C learned-from-scratch baseline) and requires roughly one-third the number of iterations to converge compared to the baseline; ablation: removing hierarchical policy reduced success rate by 3.47 percentage points and SPL by 0.93 points. Exact absolute SPL or success rates not reported numerically in the paper text.",
            "complexity_variation_relationship": "Yes — the paper explicitly discusses trade-offs: high environment complexity and diversity (large state space, varied object layouts) make unsupervised skill discovery challenging for generic intrinsic-motivation methods; ULTRA addresses this by (1) generating a curriculum of tasks of increasing difficulty and diversity, and (2) using a hierarchical policy and meta-reinforcement learning to produce transferable sub-policies. The authors argue that curriculum + hierarchy mitigate meta-overfitting and enable learning under high complexity/high variation where prior unsupervised methods (e.g., DIAYN, Curiosity) fail or perform poorly.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Tested implicitly: AI2-THOR (high complexity & high variation) — ULTRA shows the reported 53.34% relative SPL improvement over baseline and faster convergence (~1/3 iterations).",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum-based adversarial task generation (task-generator vs meta-learner), hierarchical policy (master + shared sub-policies), meta-reinforcement learning with Reptile-style updates for sub-policies, actor-critic (A3C) for within-task updates; unsupervised pretraining on unannotated scenes followed by few-shot supervised fine-tuning of master policy on labeled scenes.",
            "generalization_tested": true,
            "generalization_results": "Yes — evaluated on held-out meta-testing scenes (20 scenes) after unsupervised meta-training on 60 scenes. ULTRA fast-adapts with higher learning speed and higher navigation performance (success rate and SPL) than baselines, particularly on long trajectories (L ≥ 5). The learned sub-policies show consistent behaviors across different scenes (qualitative motor primitives).",
            "sample_efficiency": "Higher sample efficiency vs baseline: meta-testing uses only a few labeled scenes (5 scenes per room type) and ULTRA converges in approximately one-third the iterations required by A3C-from-scratch; exact number of episodes/interactions not reported numerically in the text.",
            "key_findings": "1) Unsupervised curriculum-based adversarial generation + hierarchical meta-RL can discover transferable motor primitives for embodied navigation in complex, varied indoor scenes. 2) ULTRA significantly improves navigation efficiency (53.34% relative SPL) and learning speed (≈1/3 iterations) versus baselines. 3) Hierarchical decomposition reduces meta-overfitting (ablation shows dropping hierarchy lowers success/SPL). 4) Diversity encouragement in task generation yields varied curricula that cover broader state space. 5) Prior unsupervised methods (DIAYN, Curiosity) underperform in these complex/diverse visual-navigation environments.",
            "uuid": "e1039.0",
            "source_info": {
                "paper_title": "Unsupervised Reinforcement Learning of Transferable Meta-Skills for Embodied Navigation",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "DIAYN",
            "name_full": "Diversity Is All You Need (DIAYN)",
            "brief_description": "An unsupervised skill-discovery algorithm that learns diverse, distinguishable skills by maximizing mutual information between latent skill variable and states, used here as an unsupervised baseline.",
            "citation_title": "Diversity is all you need: Learning skills without a reward function",
            "mention_or_use": "use",
            "agent_name": "DIAYN-trained agent (unsupervised skill agent)",
            "agent_description": "Agent trained with DIAYN-style intrinsic objectives to discover diverse skills without external rewards; applied as a baseline (pretrained on meta-training data and then fine-tuned on meta-testing).",
            "agent_type": "simulated agent",
            "environment_name": "AI2-THOR (as applied baseline)",
            "environment_description": "Same AI2-THOR indoor scenes used by the paper: photo-realistic, multi-room types with large state space.",
            "complexity_measure": "Same indicators as main experiments: large state space and long-horizon paths (L ≥ 5 highlighted). The paper notes DIAYN's difficulty in these more complex/diverse visual navigation environments.",
            "complexity_level": "high",
            "variation_measure": "High variation across scenes/room types and object placements; DIAYN was applied across the same set of meta-training scenes.",
            "variation_level": "high",
            "performance_metric": "Learning speed and success rate when fine-tuned for navigation; compared on averaged reward and SPL.",
            "performance_value": "Reported qualitatively as performing poorly: DIAYN learns useful skills in other domains but 'does not perform well on visual navigation' here and 'learns slower than A3C (learn from scratch)'; no precise numeric values given in text.",
            "complexity_variation_relationship": "Paper states DIAYN's ineffectiveness is attributable to the higher complexity and diversity (larger state space) of visual navigation environments compared to prior tasks where DIAYN succeeded; thus DIAYN struggles in high-complexity/high-variation settings without task-specific curriculum.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Performed poorly in the paper's experimental (high complexity, high variation) AI2-THOR setting — learning slower than A3C and failing to produce useful transfer.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Unsupervised skill discovery (mutual-information objective) applied across meta-training scenes, then fine-tuned for navigation.",
            "generalization_tested": true,
            "generalization_results": "When fine-tuned for the navigation task in held-out scenes, DIAYN-based pretraining did not yield effective fast adaptation compared to ULTRA and was slower than A3C-from-scratch.",
            "sample_efficiency": "Lower sample efficiency in this domain compared to ULTRA and even compared to A3C-from-scratch (qualitative statement only).",
            "key_findings": "DIAYN, while effective in some robotic simulation tasks, fails to transfer effectively for complex, diverse visual-navigation in AI2-THOR without a curriculum or hierarchical meta-learning; environment complexity/diversity limits its utility here.",
            "uuid": "e1039.1",
            "source_info": {
                "paper_title": "Unsupervised Reinforcement Learning of Transferable Meta-Skills for Embodied Navigation",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Curiosity",
            "name_full": "Curiosity-driven exploration by self-supervised prediction (Curiosity)",
            "brief_description": "An intrinsic-reward exploration method where the agent is rewarded by prediction error of a learned forward/inverse dynamics model in feature space; used here as an unsupervised baseline.",
            "citation_title": "Curiosity-driven exploration by self-supervised prediction",
            "mention_or_use": "use",
            "agent_name": "Curiosity-driven agent (intrinsic-motivation baseline)",
            "agent_description": "Agent trained with an intrinsic curiosity reward (prediction error in visual feature space) to encourage exploration; pretrained on meta-training scenes and then fine-tuned on navigation tasks as a baseline.",
            "agent_type": "simulated agent",
            "environment_name": "AI2-THOR (as applied baseline)",
            "environment_description": "Same set of photo-realistic indoor scenes with diverse layouts and objects.",
            "complexity_measure": "Large visual/state space and long-horizon navigation tasks; the paper attributes limited improvement from curiosity to environmental complexity and diversity.",
            "complexity_level": "high",
            "variation_measure": "High (diverse scenes and object placements across meta-training/test splits).",
            "variation_level": "high",
            "performance_metric": "Averaged reward, success rate, SPL, learning speed.",
            "performance_value": "Curiosity baseline yields limited improvement over A3C-from-scratch (qualitative); exact numeric performance values not provided in main text.",
            "complexity_variation_relationship": "Paper argues that curiosity-based intrinsic rewards provide limited benefit in high-complexity, high-variation visual navigation environments because the state space is larger and more diverse than prior tasks where curiosity helped exploration.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Limited improvement over A3C-from-scratch in AI2-THOR (high complexity & high variation).",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Intrinsic-motivation / curiosity-driven exploration pretraining followed by fine-tuning on navigation.",
            "generalization_tested": true,
            "generalization_results": "When fine-tuned on navigation tasks, curiosity-based pretraining provided limited gains and did not match ULTRA's performance.",
            "sample_efficiency": "Noted as less sample-efficient than ULTRA in this domain (qualitative statement).",
            "key_findings": "Curiosity-driven exploration alone is insufficient to provide the transferable meta-skills needed for efficient target-driven navigation in highly complex and varied indoor visual environments without additional structure (curriculum/hierarchy).",
            "uuid": "e1039.2",
            "source_info": {
                "paper_title": "Unsupervised Reinforcement Learning of Transferable Meta-Skills for Embodied Navigation",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "A3C-from-scratch",
            "name_full": "Asynchronous Advantage Actor-Critic (learned from scratch)",
            "brief_description": "Standard A3C-based end-to-end policy learning (LSTM-A3C architecture) trained directly on the supervised visual-navigation reward at meta-testing, used as primary baseline.",
            "citation_title": "Asynchronous methods for deep reinforcement learning",
            "mention_or_use": "use",
            "agent_name": "A3C (learn-from-scratch embodied agent)",
            "agent_description": "A standard end-to-end reinforcement learning agent using A3C (LSTM-A3C variant) trained from scratch on the few labeled meta-testing scenes without ULTRA's unsupervised pretraining.",
            "agent_type": "simulated agent",
            "environment_name": "AI2-THOR (indoor navigation tasks)",
            "environment_description": "Photo-realistic indoor scenes; same complexity/variation as used for ULTRA evaluation.",
            "complexity_measure": "Large state space, long-horizon tasks; measured by success rate and SPL especially for trajectories with optimal path length L ≥ 5.",
            "complexity_level": "high",
            "variation_measure": "Trained/tested on limited labeled scenes (few-shot: 5 scenes per room type) — variation in meta-testing limited by few-shot regime but environment itself remains diverse.",
            "variation_level": "medium (limited labeled examples at meta-test)",
            "performance_metric": "Success rate; SPL; iterations to converge; averaged reward.",
            "performance_value": "Baseline that ULTRA is compared to: ULTRA outperforms A3C-from-scratch with 53.34% relative SPL improvement and faster convergence; A3C-from-scratch converges in ~3x the iterations relative to ULTRA (qualitative/numeric ratio reported). Exact absolute numbers not provided in text.",
            "complexity_variation_relationship": "A3C-from-scratch struggles to generalize given few labeled scenes and high environment complexity/variation; ULTRA's pretraining helps overcome these limitations.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Performed worse than ULTRA in high-complexity/high-variation AI2-THOR tests; required more iterations and achieved lower SPL.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Single-task supervised RL on meta-testing data (learn-from-scratch), no unsupervised pretraining or curriculum.",
            "generalization_tested": true,
            "generalization_results": "When trained from scratch on the small set of labeled meta-testing scenes, A3C converged slower and achieved lower navigation efficiency (SPL) than ULTRA; did not generalize as well under few-shot labeled data.",
            "sample_efficiency": "Lower sample efficiency compared to ULTRA; requires roughly three times more iterations to converge (paper states ULTRA needs one-third the iterations of baselines).",
            "key_findings": "A3C trained from scratch on few labeled scenes is outperformed by ULTRA in both sample efficiency and final navigation efficiency in complex/varied indoor environments; highlights benefit of unsupervised meta-skill pretraining and hierarchical decomposition.",
            "uuid": "e1039.3",
            "source_info": {
                "paper_title": "Unsupervised Reinforcement Learning of Transferable Meta-Skills for Embodied Navigation",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Random",
            "name_full": "Random policy baseline",
            "brief_description": "A simple baseline where the agent takes uniformly random actions at every timestep; used to provide a lower-bound performance reference.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Random policy agent",
            "agent_description": "Agent executes random actions from the action set each timestep; baseline for comparison.",
            "agent_type": "simulated agent",
            "environment_name": "AI2-THOR (indoor navigation tasks)",
            "environment_description": "Same photo-realistic indoor scenes; random actions do not leverage any learned behaviors and thus perform poorly.",
            "complexity_measure": "N/A for learning baseline; environment complexity still high but agent does not adapt.",
            "complexity_level": "high",
            "variation_measure": "High variation of environment; random policy not sensitive to variation.",
            "variation_level": "high",
            "performance_metric": "Success rate; SPL",
            "performance_value": "Baseline worst-case performance (qualitative; no numeric values provided in text).",
            "complexity_variation_relationship": "Not analyzed specifically; random policy unsurprisingly fails as environment complexity/variation increases.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Very poor (not quantified).",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "None (random actions).",
            "generalization_tested": false,
            "generalization_results": "",
            "sample_efficiency": "Not applicable.",
            "key_findings": "Serves as a lower bound; confirms that learned policies must exploit structure to perform in complex/varied navigation tasks.",
            "uuid": "e1039.4",
            "source_info": {
                "paper_title": "Unsupervised Reinforcement Learning of Transferable Meta-Skills for Embodied Navigation",
                "publication_date_yy_mm": "2020-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Diversity is all you need: Learning skills without a reward function",
            "rating": 2,
            "sanitized_title": "diversity_is_all_you_need_learning_skills_without_a_reward_function"
        },
        {
            "paper_title": "Curiosity-driven exploration by self-supervised prediction",
            "rating": 2,
            "sanitized_title": "curiositydriven_exploration_by_selfsupervised_prediction"
        },
        {
            "paper_title": "Unsupervised meta-learning for reinforcement learning",
            "rating": 2,
            "sanitized_title": "unsupervised_metalearning_for_reinforcement_learning"
        },
        {
            "paper_title": "Intrinsic motivation and automatic curricula via asymmetric self-play",
            "rating": 2,
            "sanitized_title": "intrinsic_motivation_and_automatic_curricula_via_asymmetric_selfplay"
        },
        {
            "paper_title": "Reptile: a scalable metalearning algorithm",
            "rating": 2,
            "sanitized_title": "reptile_a_scalable_metalearning_algorithm"
        },
        {
            "paper_title": "AI2-THOR: An Interactive 3D Environment for Visual AI",
            "rating": 2,
            "sanitized_title": "ai2thor_an_interactive_3d_environment_for_visual_ai"
        },
        {
            "paper_title": "Asynchronous methods for deep reinforcement learning",
            "rating": 1,
            "sanitized_title": "asynchronous_methods_for_deep_reinforcement_learning"
        },
        {
            "paper_title": "Learning to navigate in complex environments",
            "rating": 1,
            "sanitized_title": "learning_to_navigate_in_complex_environments"
        }
    ],
    "cost": 0.014763749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Unsupervised Reinforcement Learning of Transferable Meta-Skills for Embodied Navigation
18 Nov 2019</p>
<p>Juncheng Li junchengli@zju.edu.cn 
Zhejiang University</p>
<p>Xin Wang xwang@cs.ucsb.edu 
University of California
Santa Barbara</p>
<p>Siliang Tang siliang@zju.edu.cn 
Zhejiang University</p>
<p>Haizhou Shi shihaizhou@zju.edu.cn 
Zhejiang University</p>
<p>Fei Wu wufei@zju.edu.cn 
Zhejiang University</p>
<p>Yueting Zhuang yzhuang@zju.edu.cn 
Zhejiang University</p>
<p>William Yang Wang william@cs.ucsb.edu 
University of California
Santa Barbara</p>
<p>Unsupervised Reinforcement Learning of Transferable Meta-Skills for Embodied Navigation
18 Nov 201960F00EC5181A3225964CD86BF65E578FarXiv:1911.07450v1[cs.CV]
Visual navigation is a task of training an embodied agent by intelligently navigating to a target object (e.g., television) using only visual observations.A key challenge for current deep reinforcement learning models lies in the requirements for a large amount of training data.It is exceedingly expensive to construct sufficient 3D synthetic environments annotated with the target object information.In this paper, we focus on visual navigation in the lowresource setting, where we have only a few training environments annotated with object information.We propose a novel unsupervised reinforcement learning approach to learn transferable meta-skills (e.g., bypass obstacles, go straight) from unannotated environments without any supervisory signals.The agent can then fast adapt to visual navigation through learning a high-level master policy to combine these meta-skills, when the visual-navigation-specified reward is provided.Evaluation in the AI2-THOR [16] environments shows that our method significantly outperforms the baseline by 53.34% relatively on SPL, and further qualitative analysis demonstrates that our method learns transferable motor primitives for visual navigation.</p>
<p>Introduction</p>
<p>Visual navigation is a task of training an embodied agent that can intelligently navigate to an instance of an object according to the natural-language name of the object.In addition to being a fundamental scientific goal in computer vision and artificial intelligence, navigation in a 3D environment is a crucial skill for the embodied agent.This task may benefit many practical applications where an embodied agent improves the quality of life and augments human capability, such as in-home robots, personal assistants, and hazard removal robots.</p>
<p>Recently, various deep reinforcement learning (RL) approaches [42,24,40,39,31,44,45,11,21,46,19] have been proposed to improve the navigation models.They formulate the problem as the Partially Observable Markov Decision Process (POMDP) and train end-to-end policy networks to map observations to actions.However, deep RL methods are usually data inefficient and require a large amount of training data.In order to train these deep models, we need to construct a sufficient number of 3D synthetic environments and annotate the object information, which is exceedingly expensive, time-consuming, and even infeasible in real-world applications.Furthermore, it is hard for the trained embodied agent to transfer to different environments.</p>
<p>It is worth noticing that when humans encounter a new task, they can quickly learn to solve it by transferring the meta-skills learned in a wide variety of tasks throughout their lives.This stands in stark contrast with the current deep reinforcement learning-based navigation methods, where the policy networks are learned from scratch.Instead, humans have an inherent ability to transfer knowledge across tasks and cross-utilize their knowledge, which offloads the burden of a large number of training samples.</p>
<p>Inspired by this fact, we seek the help of both metalearning [26,7] that learn quickly using a small amount of data and transfer learning [37,41] that accelerate learning a new task through transferring knowledge from a related task that is already learned.In our work, we frame low-resource visual navigation as a meta-learning problem.At the meta-training phase, the environments are not annotated with object information, and we assume access to a set of tasks that we refer to as the meta-training tasks.From these tasks, the embodied agent (we call it as meta-learner) then learns a set of transferable sub-policies, each of which corresponds to a specific meta-skill (also called as motor primitives, e.g., bypass obstacles, go straight) by perform- ing a sequence of primitive actions.At the meta-testing phase, a few annotated environments with hand-specified rewards for visual navigation are provided.As illustrated in Figure 1, after learning transferable sub-policies from meta-training scenes, the agent is solely required to learn a new master policy to combine the sub-policies such that it can fast adapt to visual navigation.During meta-training, the master policy is task-specific, and the sub-policies are shared for all tasks across scenes.The master policy determines the execution order of the sub-policies and is optimized to fast adapt to each meta-training task.The subpolicies are optimized for performance across tasks using gradient-based meta-learning algorithms [26,7].The hierarchical architecture [9,35,2,8] that separates the entire policy into the task-specific part and task-agnostic part can also avoid meta-overfitting: typical gradient-based metalearning algorithms can easily result in overfitting since the entire network is updated on just a few samples.</p>
<p>However, typical meta-learning methods [26,7] require a sufficient number of hand-designed tasks for meta-training, which is not practical for an embodied agent.In this paper, we then propose a novel unsupervised reinforcement learning approach that automatically generate a curriculum of tasks without manual task definition.In our Unsupervised reinforcement Learning of TRAnsferable meta-skills (ULTRA) framework, the agent can efficiently learn transferable meta-skills and thus fast adapt to the new task by leveraging the meta-skills when entering a new environment.The main body of the framework is what we call the curriculum-based adversarial training process, where one agent (task generator) generates a curriculum of tasks with increasing difficulty, no supervisory signal from the environment provided.The other agent (meta-learner) learns the meta-skills by accomplishing the generated tasks.After this unsupervised adversarial training process, the metalearner can fast adapt to the new visual navigation task by just learning a new master policy to combine the learned meta-skills.</p>
<p>Our experimental results show that our method significantly outperforms the baseline by 53.34% on SPL.More-over, further ablation study demonstrates the effectiveness of the adversarial training process and the hierarchical policy.Additionally, by qualitatively visualizing the behavior of the sub-policies, we find that the sub-policies show consistent motor primitives.</p>
<p>In summary, our contributions are mainly four-fold:</p>
<p>• We propose a novel ULTRA framework to learn transferable meta-skills via unsupervised reinforcement learning.</p>
<p>• The hierarchical policy of meta-learner separates the entire policy into the task-specific part and taskagnostic part, which reduces the probability of metaoverfitting and promises a faster convergence.</p>
<p>• Instead of manually designing tasks, we propose a novel curriculum-based adversarial training strategy, where the task generator automatically proposes increasingly difficult tasks to the meta-learner.Further, we define a diversity measure to encourage the task generator to generate more diverse tasks.</p>
<p>• We perform our experiments in low-resource setting, and experimental results show that our method significantly outperforms the baseline by 53.34% relatively on SPL and requires only one-third number of iterations to converge, compared with the baseline.</p>
<p>Related Work</p>
<p>Visual Navigation.Traditional navigation methods [3,5,14,17,20,36] typically employ geometric reasoning on a given occupancy map of the environment.They perform path planning [4,13,18]to decide which actions the robot performs.Recently, many deep reinforcement learning (DRL) approaches [42,24,31,44,45,11,21,46] have been proposed.They formulate the problem as the partially Observable Markov Decision Process (POMDP) and endto-end learn policies network for visual navigation.While these methods achieve great improvement, it is difficult to apply them to real-world situations since these DRL methods require a large number of training episodes and annotated environment information, which is time-consuming and exceedingly expensive.In our work, we focus on developing an unsupervised reinforcement learning method in the low-resource setting.Meta-Learning.Meta-learning, also known as learning to learn, optimizes for the ability to learn new tasks quickly and efficiently, using experience from learning multiple tasks.There are three common types of methods: 1) metricbased methods [32,34,38] that learn an efficient distance metric; 2) memory-based methods [22,25,27,30] that learn to store experience using external or internal memory; and 3) gradient-based methods [26,7,12,29,9] model parameters explicitly for fast learning.Our method relies on a gradient-based meta-learning algorithm called Reptile [26].</p>
<p>The Reptile algorithm is aimed to learn a good parameter initialization during the meta-training process, where a large number of related tasks are provided.Thus, in the meta-testing process, the model can achieve good performance on new tasks after only a few gradient updates.An important difference is that our method does not require a large number of hand-designed tasks at the meta-training stage.Instead, we propose a curriculum-based adversarial training process that automates the meta-training process without any supervision.Intrinsic Motivation-Based Exploration.Intrinsic motivation or curiosity called by psychologists have been widely used to train an agent to explore the environment and create environment priors without external supervision.There are mainly two categories of intrinsic reward: 1) incentivize the agent to explore "novel" states [6,10,33]; and 2) incentivize the agent to perform actions that reduce its predictive uncertainty of the environment [28].Sukhbaatar et al. [33] introduce an adversarial training approach to unsupervised exploration, where one model proposes tasks and the other learns to complete it.In their work, the model for completing the tasks shares the whole parameters during training, and use the parameters as initialization for the downstream task.However, our work differs as we treat the adversarial training process as a sequence of independent meta-training tasks, and each task holds independent task-specific parameters.Also, there is no communication between two agents, whereas, in our work, the generator sends the target observation to the metalearner, which contains the task information.</p>
<p>Gupta et al. [10] propose an unsupervised meta-learning method based on a recently proposed unsupervised exploration technique [6].They use the heuristic method to define intrinsic reward (i.e.random discriminator, entropy-based method ), which automates the task generation process during meta-training.There is no distinct task definition, and such a heuristic method is inefficient as the environment of visual navigation is complex and diverse.Our work instead introduces a curriculum-based adversarial training, which is more interpretable and efficient.</p>
<p>Method</p>
<p>Overview</p>
<p>As mentioned above, we frame low-resource visual navigation as a meta-learning problem.At meta-training phrase, the environments are unannotated, and no hand-designed reward is provided.The agent learns transferable meta-skills via our ULTRA from these scenes.At meta-testing phrase, a few annotated environments and corresponding visualnavigation-specified rewards are provided, and the agent needs to fast adapt to visual navigation.In this section, we Our goal is to use unsupervised reinforcement learning to learn transferable meta-skills that can be utilized by the embodied agent to quickly master visual navigation in indoor 3D scenes.During the curriculum-based adversarial training process, the task generator automatically proposes a curriculum of tasks, and the meta-learner learns to complete these tasks.Specifically, the architecture of the metalearner is the shared hierarchical policy.For each task generated by the task generator, the meta-learner first reinitialize the master policy and learns to combine the sub-policies to complete the task.After adapting the master policy to the new task, the meta-reinforcement learning algorithm is applied to optimized the sub-policies to excellent performance across tasks.As illustrated in figure 1, our ULTRA framework mainly consists of three components: Curriculum-Based Adversarial Task Generation: The curriculum-based adversarial training procedure contains a task generator and a meta-learner.The task generator automatically generates a curriculum of tasks without any supervisory signal from the environment, and the meta-learner tries to complete them.Shared Hierarchical Policy: The architecture of the metalearner is the shared hierarchical policy, which contains a master policy and a set of sub-policies.At each mastertimestep, the master policy first selects a sub-policy to be activated, and then the chosen sub-policy performs primitive actions.The master policy is learned from scratch for each task and encodes the task-specific information.The sub-policies are shared and encapsulate meta-skills that can be transferred across all tasks.Meta-Reinforcement Learning: The meta-reinforcement learning algorithm is to optimize the parameters of the subpolicies across all tasks.</p>
<p>Curriculum-Based Adversarial Task Generation</p>
<p>In this setting, we have two agents: a task generator and a meta-learner.As shown in Figure 2, during each iteration, the task generator starts at the initial state s 0 , performs a sequence of actions, and finally stops at state s T .Then, it sends its egocentric observation at the final state s T to the meta-learner.Given the observation o T at final state s T , the goal of the meta-learner is to reach s T from s 0 , which we call as a task.We initialize the meta-learner at state s 0 , let it learn on this task for multiple episodes, and compute the success rate r.After that, the task generator proposes a new task, and the meta-learner repeats the above process.</p>
<p>The above adversarial training process does not involve any manually-designed tasks.The task generator automatically generates a curriculum of tasks for the meta-learner to complete.As the tasks become more and more complicated, the meta-learner needs to learn transferable sub-policies that are corresponding to meaningful motor primitives, so that it can fast adapt to the new tasks by learning a reinitialized master policy to combine the sub-policies.</p>
<p>Our goal is to automatically generate a curriculum of diverse tasks, where we first start with an easy task and then gradually increase the task difficulty.The reward function of the task generator consists of three components: a final reward based on the success rate, an intermediate reward that penalizes the task generator for taking too many steps, and a diversity measure that measures the diversity of the tasks.Success Rate: We use the success rate of the meta-learner after multiple episodes to measure the difficulty of the task and give the generator a final reward.The final reward is defined as:
R f = k * (1 − r)(1)
where k is a scaling factor, and r is the success rate.</p>
<p>Step Efficiency: At each timestep, the task generator will receive a negative constant intermediate reward.We penalize the task generator for taking too many steps, which encourages it to generate the easiest task that the meta-learner can not complete.In the first few iterations, the task generator can propose tasks by performing a small number of steps.Then as the capabilities of the meta-learner increase, more steps will be taken to generate more difficult tasks (qualitative examples in Figure 2).Task Diversity: In order to explore wider state spaces for our meta-learner to build a better visual and physical understanding of the environment, we add an additional item in the task generator's reward function to encourage it to generate more diverse tasks.Formally, let π denote the current policy, and π denote a previous policy.The diversity measure D can be written as:
D = st∈τ π ∈Π D KL (π (•|s t )||π(•||s t )) (2)
where τ is the trajectory from the current episode, Π is the set of prior polices.We save the previous policy corresponding to the last four episodes in the set Π. We use KL-divergence to measure the difference between the current policy and the previous policies.The task diversity is aimed to incentivize the task generator to generate more diverse tasks that cover a larger state space of the environment.Formally, the task generator's total reward R G can be written as:
R G = k * (1−r)−λ * n+η * st∈τ π ∈Π D KL (π (•|s t )||π(•||s t ))
(3) where λ and η are weight hyper-parameters, and n is the number of actions that the task generator executes.</p>
<p>For meta-learner, we use the shared hierarchical policy.We train it using actor-critic methods [23] with rewards function that incentivizes it to reach the target.</p>
<p>Shared Hierarchical Policy</p>
<p>The shared hierarchical policy decomposes long-term planning into two different time-scales.At master-timestep, the master policy chooses a specific sub-policy from a set of sub-policies and then gives control to the sub-policy.As in [9], the sub-policy executes fixed N timesteps primitive actions(e.g.MoveAhead, RotateLeft) before returning control back to the master policy.</p>
<p>Formally, let φ denote the parameters of the master policy, and θ = {θ 1 , θ 2 , ..., θ K } denote the parameters of the K sub-policies.φ is the task-specific parameters, that is learned from scratch for each task.θ shared between all tasks and switched between by task-specific master policies.For each task generated by the task generator during for w = 0, 1, ...W (warmup period) do 10:</p>
<p>collect rollout τ w i using π M φi,θ 11:
φ i ←− φ i + α∇ φ J(τ w i , π M φi,θ ) 12:
end for 13: θ = θ 14:</p>
<p>for j = 0, 1, ...J (joint update period) do 15:</p>
<p>collect rollout τ j i using π M φi, θ 16:
φ i ←− φ i + α∇ φ J(τ j i , π M φi, θ ) 17: θ ←− θ + α∇ θ J(τ j i , π M φi, θ ) 18:
end for 19:
θ ←− θ + β( θ − θ) 20:
Evaluate R G as Eq 3 and update π G µ 21:</p>
<p>if len(Π) == 4 then</p>
<p>22:</p>
<p>Π.pop(0)</p>
<p>23:</p>
<p>end if</p>
<p>24:</p>
<p>Π.append(µ) 25: end while the adversarial training process, φ is randomly initialized at first and then optimized to maximize the total reward over multiple episodes, given fixed shared parameters θ.</p>
<p>After fine-tuning the task-specific parameters φ to the task (called warm-up period), we take a joint update period, where both θ and φ are updated.The task-specific φ is optimized towards the current task, but the shared θ is optimized to excellent performance across tasks using gradient-based meta-learning algorithms.The details are discussed in the Sec 3.4.</p>
<p>Meta-Reinforcement Learning on the Proposed Tasks</p>
<p>Inspired by meta-learning algorithms [26,7,12,29,9] that leverage experience across many tasks to learn new tasks quickly and efficiently, our method automatically learns transferable meta-skills from a curriculum of tasks generated in the adversarial training process.Background on Gradient-Based Meta-Learning: Our method is inspired by prior work on a first-order gradientbased meta-learning algorithm called Reptile [26].The Reptile algorithm is aimed to learn the initialization of a neural network model, which can fast adapt to a new task.The Reptile algorithm repeatedly samples a task, training on it, and moving the initialization towards the trained weights on that task.</p>
<p>Formally, let θ denote the parameters of the network, τ denote a sampled task, corresponding to loss L τ , and θ denote the updated parameters after K steps of gradient descent on L τ .The update rule of the Reptile algorithm is as follows:
θ ←− θ + β( θ − θ)(4)
where the (θ − θ) can be treated as a gradient that can be plugged into an adaptive algorithm such as Adam [15].If we define K = 1, then this algorithm corresponds to joint training on the expected loss E τ [L τ ].However, we perform multiple gradient updates, such that the update includes important terms from second-and-higher derivatives of L τ .Hence, the Reptile converges to a solution that is very different from the joint training.</p>
<p>For Visual Navigation, our goal is for the agent to learn transferable meta-skills from the unsupervised adversarial training process.Therefore, we apply the Reptile algorithm to update the hierarchical police of the meta-learner.Different from the original Reptile algorithm that computes second-and-higher derivatives to update the whole parameters, we just apply it to update the parameters of the subpolicies and fix them during the test.Also, we treat (θ − θ) as a gradient and use SGD to update it.</p>
<p>Algorithms 1 details our ULTRA that consists of four phases.Firstly, the task generator proposes a task.Secondly, the meta-learner joins in a warm-up period to finetune the master policy.Thirdly, the meta-learner takes a joint update period where both the master policy and subpolicies are updated.Finally, the task generator is updated based on the success rate of the meta-learner and repeats the above procedure.</p>
<p>Formally, let π G µ denote the policy of the task generator parameterized by µ, and π M φi,θ denote the policy of the meta-learner parameterized by task-specific parameters φ i and shared parameters θ = {θ 1 , θ 2 , ..., θ K }.Firstly, we run the task generator and collect a trajectory τ G i (s 0 , s 1 , ..., s T ).We then set the task τ i for the meta-learner by the initial state s 0 , final state s T , and the observation o T at the final state.Secondly, we initialize the meta-learner using the shared sub-policies and the random-initialized master policy.We then run a warmup period to fine-tune the master policy.More specifically, we run the meta-learner for W episodes, and use the collected W trajectories to update the master policy φ i as follows:
φ i ←− φ i + α∇ φ J(τ w i , π M φi,θ )(5)
where J(τ w i , π M φi,θ ) is the objective function of any gradient-based reinforcement learning that uses the w-th trajectory of task τ i produced by policy π M φi,θ to update the master policy φ i .In our work, we use Asynchronous Advantage Actor-Critic(A3C) [23,43].</p>
<p>During the warmup period, the parameters of the shared sub-policies θ are fixed.After fine-tuning the master policy, we enter the joint update period, where we run the hierarchical policy for J episodes, and update both φ i and θ as follows:
φ i ←− φ i + α∇ φ J(τ j i , π M φi, θ )(6)θ ←− θ + α∇ θ J(τ j i , π M φi, θ )(7)
More specifically, we save the value of θ before the joint update period.After J times iterations, we get the updated parameters θ, and then we compute the gradient (θ − θ) and update the shared sub-policies θ using Reptile Algorithm.Finally, we compute the final reward of the task generator based on the success rate r, step efficiency, and the diversity.</p>
<p>Experiments</p>
<p>In our experiments, we aim to (1) evaluate whether the agent can quickly learn visual navigation by leveraging the transferable meta-skills, given only a few training data, (2) determine whether the ULTRA is efficient than other unsupervised RL-based methods [6,10,28], (3) determine whether the hierarchical policy promises a better transfer, and (4) gain insight into how our unsupervised ULTRA works.</p>
<p>Experimental Setup</p>
<p>After learning transferable meta-skills using ULTRA, the agent is required to master visual navigation in a small number of episodes.Visual navigation is a task of training an embodied agent that can intelligently navigate to a specific object chosen by natural language.We can formally define visual navigation in the context of Markov Decision Process (MDP) with the state space S, action space A, transition dynamics T , and reward function R. Let E = {e 1 , e 2 , ..., e n } denote a set of scenes, T = {t 1 , t 2 , ..., t m } denote a set of target objects, and the visual observation s t denote the state at t timestep.An episode can be determined by a scene, a target object, and an initial state.The action set A consists of six unique actions (e.g.MoveAhead, RotateLeft, RotateRight, LookDown, LookUp, Done).The embodied agent is required to figure out the desired action a t at each timestep using only the egocentric RGB images and the language semantics of the target object.If the agent navigates to a position close enough to the target within a certain number of steps, we consider this episode to be successful.</p>
<p>We evaluate our approach in AI2-THOR [16] simulated environment, which is a photo-realistic customizable environment for indoor scenes and contains 120 scenes covering four different room categories: kitchens, living rooms, bedrooms, and bathrooms.We use 60 scenes for meta-training, 20 scenes for meta-testing, 20 scenes for validation, and 20  1: Quantitative results.We compare variations of our method and the baselines on testing data.Additionally, we report the results on trajectories where the optimal path length is at least 5 (L ≥ 5).Our ULTRA significantly outperforms the baselines, especially on L ≥ 5, indicating the superiority of our method on long-term planning.</p>
<p>scenes for testing.During meta-training, object information and hand-specified rewards for visual navigation are not accessible, and the agent performs unsupervised reinforcement learning to learn transferable meta-skills.During meta-testing, we choose the same set of navigational target object classes as [42], and the training reward is specific since the human-annotated labels are available.We then fix the sub-policies, reinitialize the master policy, and train the hierarchical policy using Asynchronous Advantage Actor-Critic(A3C) [23,43].Only a few scenes (5 scenes for each room type) containing labeled object information are provided since we expect the agent has already learned meaningful meta-skills from unsupervised training and want to test whether it can fast adapt to visual navigation given only a few training scenes with task-specific rewards.</p>
<p>Task and Evaluation Metric: We use the averaged rewards on evaluation tasks during the training process to evaluate the learning speed, success rate to evaluate the navigation performance, and the success weighted by Path Length (SPL) [1] to evaluate the navigation efficiency.The SPL is defined as
1 N N i=1 S i li max(pi,li)
, where N is the number of episodes, S i is a binary indicator of success in episode i, l i is the shortest path distance, and p i is the path length.As [42], we report the performance both on all trajectories and trajectories, where the optimal path length is at least 5 (L ≥ 5).Baselines We compare our method with the following baselines: (1) Random policy: The agent randomly execute an action at each timestep; (2) A3C (learn from scratch): The architecture is the same as ours.However, there is no UL-TRA process, and the policy is directly learned from scratch on meta-testing data with visual-navigation-specified rewards.</p>
<p>We also compare to the state-of-the-art unsupervised RL-based methods: We report the rewards averaged across 10 evaluation tasks during meta-testing.After learning meta-skills using unsupervised meta-reinforcement learning, our ULTRA can fast adapt to visual navigation significantly faster than A3C (learn from scratch) and other state-of-the-art unsupervised RL-based methods.</p>
<p>theoretic objective using a maximum entropy policy.(4)</p>
<p>Curiosity: [28] The agent learns skills motivated by a curiosity reward, which serves as an intrinsic reward and is the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a selfsupervised inverse dynamics model.The core idea of these methods is to define some intrinsic rewards in some way so that the agent can explore the environments itself without any external rewards and draws useful priors from this process.As our ULTRA, DIAYN and Curiosity are both first trained on meta-training data, and then fine-tuned on metatesting data.</p>
<p>Figure 4: Ablation study of the number of the subpolicies.We provide the results when we use different number of sub-policies.</p>
<p>Results</p>
<p>We summarize the results of our ULTRA and the baselines in Table 1.Also, we report the rewards averaged across 10 evaluation tasks during meta-testing in Figure 3. From Figure 3, we observe that our approach can fast adapt to visual navigation, significantly outperforming all baselines not only in learning speed but also in performance.The number of iterations required for convergence of our ULTRA is about one-third of the baselines.Furthermore, as shown in Table 1, our approach achieves the best success rate and SPL, especially when the trajectory length L ≥ 5, indicating the superiority of our method on long-term planning.</p>
<p>While DIAYN can learn useful skills on a variety of simulated robotic tasks studied in [6], it does not perform well on visual navigation: it even learns slower than A3C (learn from scratch), which means that it can not learn useful skills from unsupervised exploring.Also, compared with A3C (learn from scratch), the curiosity method makes limited improvement.We argue that the reason for this phenomenon is due to the complexity and diversity of the visual navigation environment, whose state space is always larger than the previous tasks.</p>
<p>Ablation Study</p>
<p>Effect of Individual Components: We conduct an ablation study to illustrate the effect of the hierarchical policy and the adversarial training in Table 1.We start with the final URLTML model and remove the hierarchical policy and the adversarial training, respectively.</p>
<p>The variation of ours without hierarchical policy uses a typical LSTM-A3C policy that updates the entire network during adversarial meta-training.We fine-tune the learned LSTM-A3C policy on meta-testing data.Removing the hierarchical policy, we notice that the success rate drops 3.47 points, and the SPL drops 0.93 points, indicating that updating the entire policy on a few training samples of each metatraining tasks results in poor transferability than ULTRA on visual navigation.Thus, the hierarchical policy reduces the probability of meta-overfitting.Furthermore, the results of the last row (sample random location as meta-training tasks during unsupervised reinforcement learning) validate the superiority of curriculumbased adversarial training.Ablation of the Number of Sub-Policies: To explore the impact of different numbers of the sub-policies, we modify the number of sub-policies.As illustrated in Figure 4, the success rate and SPL keeps increasing when the number of sub-policies is increased from 4 to 7. When we continue to increase the number of sub-policies, not only does the success rate not improve significantly, but SPL decreases because too many sub-policies results in confusion.In order to guarantee the performance and reduce the computational complexity, we set the number of the sub-policies to 7.</p>
<p>Qualitative Analysis</p>
<p>Visualization of the task generator: For a more intuitive view of how our curriculum-based adversarial training works, we visualize three qualitative examples in Figure 2. In each scenario, the tasks are generated starting from the same location.We can see that the difficulty of the generated tasks, corresponding to the length of the generated trajectories, increases as the serial number of the tasks goes up.Also, we can see that the generated trajectories in each scenario are in different directions, indicating that our task generator proposes diverse meta-training tasks.Behavior of the Sub-Policy: We execute sub-policies separately in different scenes to visualize the learned metaskills.In Figure 5, trajectories shown in each row represent the same sub-policies initialized in different scenes, and trajectories shown in each column represent different sub-policies in the same location.As illustrated in Figure 5, the same sub-policy shows consistent behavior in different scenes.Sub-policy1 always bypasses obstacles and goes straight, sub-policy2 always turns right, and sub-policy3 always turns left.The consistency of the sub-policies demonstrates that our ULTRA has learned meaningful meta-skills.</p>
<p>Conclusions</p>
<p>In this paper, we introduce a novel Unsupervised reinforcement Learning of TRAnsferable meta-skills (UL-TRA) framework that enables the agent to learn transferable meta-skills from the curriculum-based adversarial training process.Experiments show that our method out-performs the baselines by a large margin.Moreover, our method converges faster than baselines.Additionally, we find that the sub-policies show consistent motor primitives (e.g., bypass obstacles, go straight), indicating that the agent learns meaningful meta-skills via unsupervised reinforcement learning.</p>
<p>Figure 1 :
1
Figure1: Overview of our ULTRA framework.The blue part on the left is our adversarial training process, where the task generator automatically proposes a curriculum of increasingly challenging tasks, and the meta-learner learns to complete them.From these tasks, the meta-learner learns a set of transferable sub-policies.Then, on the right part, the meta-learner can fast adapt to visual navigation by just learning a new master policy, given the task-specific external reward.The θ k is corresponding to the parameters of the k-th sub-policy.</p>
<p>Figure 2 :
2
Figure 2: Graphical illustration of the task generator.The generator starts from the same location (denoted by the blue robot icon) and generates tasks for the meta-training.The level of difficulty (represented by the darkness of the path) increases along the training process: at the beginning, the path of generation is short.Thus it is easy for the meta-learner to reach the target; as the meta-learner improves, the task generated becomes more challenging.</p>
<p>Algorithm 1 6 :s * ←− s T 7 :o * ←− o T 8 :
1678
Unsupervised Reinforcement Learning 1: randomly initialize θ, φ, µ 2: Π ←− [ ] 3: while not converged do 4: s 0 ←− e i .startstate 5: collect rollout τ G i (s 0 , s 1 , ..., s T ) using π G µ set task τ i = SetT ask(s 0 , s * , o * ) 9:</p>
<p>Figure 3 :
3
Figure3: Learning curves.We report the rewards averaged across 10 evaluation tasks during meta-testing.After learning meta-skills using unsupervised meta-reinforcement learning, our ULTRA can fast adapt to visual navigation significantly faster than A3C (learn from scratch) and other state-of-the-art unsupervised RL-based methods.</p>
<p>Sub-Policy 1 Sub-Policy 2 Sub-Policy 3 Figure 5 :
1235
Figure 5: Visualization of the Sub-Policies.We illustrate the trajectories of some sub-policies.Each row represents the same sub-policy initialized in different scenes, while each column represents different sub-policies in the same location.Our sub-policies show consistent behaviors, that are corresponding to some meta-skills (sub-policy1 always bypasses obstacles, sub-policy2 always turns right, and sub-policy3 always turns left).</p>
<p>On evaluation of embodied navigation agents. Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, arXiv:1807.067572018arXiv preprint</p>
<p>The optioncritic architecture. Pierre-Luc Bacon, Jean Harb, Doina Precup, Thirty-First AAAI Conference on Artificial Intelligence. 2017</p>
<p>Vision based mav navigation in unknown and unstructured environments. Michael Blösch, Stephan Weiss, Davide Scaramuzza, Roland Siegwart, 2010 IEEE International Conference on Robotics and Automation. IEEE2010</p>
<p>The complexity of robot motion planning. John Canny, 1988MIT press</p>
<p>Probabilistic appearance based navigation and loop closing. Mark Cummins, Paul Newman, Proceedings 2007 IEEE International Conference on Robotics and Automation. 2007 IEEE International Conference on Robotics and AutomationIEEE2007</p>
<p>Diversity is all you need: Learning skills without a reward function. Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, Sergey Levine, arXiv:1802.060702018arXiv preprint</p>
<p>Modelagnostic meta-learning for fast adaptation of deep networks. Chelsea Finn, Pieter Abbeel, Sergey Levine, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning201770</p>
<p>Carlos Florensa, Yan Duan, Pieter Abbeel, arXiv:1704.03012Stochastic neural networks for hierarchical reinforcement learning. 2017arXiv preprint</p>
<p>Kevin Frans, Jonathan Ho, Xi Chen, Pieter Abbeel, John Schulman, arXiv:1710.09767Meta learning shared hierarchies. 2017arXiv preprint</p>
<p>Unsupervised meta-learning for reinforcement learning. Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn, Sergey Levine, arXiv:1806.046402018arXiv preprint</p>
<p>Rahul Sukthankar, and Jitendra Malik. Cognitive mapping and planning for visual navigation: Supplementary material. Saurabh Gupta, James Davidson, Sergey Levine, 2017</p>
<p>Learning to learn using gradient descent. Sepp Hochreiter, Steven Younger, Peter R Conwell, International Conference on Artificial Neural Networks. Springer2001</p>
<p>Probabilistic roadmaps for path planning in highdimensional configuration spaces. Lydia E Kavraki, Petr Svestka, J-C Latombe, Mark H Overmars, IEEE transactions on Robotics and Automation. 1241996</p>
<p>Autonomous visual navigation of a mobile robot using a humanguided experience. Kiyosumi Kidono, Jun Miura, Yoshiaki Shirai, Robotics and Autonomous Systems. 402-32002</p>
<p>P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980Adam: A method for stochastic optimization. 2014arXiv preprint</p>
<p>Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Vanderbilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, Ali Farhadi, AI2-THOR: An Interactive 3D Environment for Visual AI. arXiv. 2017</p>
<p>Outdoor mapping and navigation using stereo vision. Kurt Konolige, Motilal Agrawal, Robert C Bolles, Cregg Cowan, Martin Fischler, Brian Gerkey, Experimental Robotics. Springer2008</p>
<p>Planning algorithms. Steven M Lavalle, 2006Cambridge university press</p>
<p>Walking with mind: Mental imagery enhanced embodied qa. Juncheng Li, Siliang Tang, Fei Wu, Yueting Zhuang, Proceedings of the 27th ACM International Conference on Multimedia. the 27th ACM International Conference on MultimediaACM2019</p>
<p>Error modeling in stereo navigation. Larry Matthies, Stevena Shafer, IEEE Journal on Robotics and Automation. 331987</p>
<p>Learning to navigate in complex environments. Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew J Ballard, Andrea Banino, Misha Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu, arXiv:1611.036732016arXiv preprint</p>
<p>Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, Pieter Abbeel, arXiv:1707.03141A simple neural attentive meta-learner. 2017arXiv preprint</p>
<p>Asynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, International conference on machine learning. 2016</p>
<p>Visual representations for semantic target driven navigation. Arsalan Mousavian, Alexander Toshev, Marek Fiser, Jana Kosecka, Ayzaan Wahid, James Davidson, arXiv:1805.060662018arXiv preprint</p>
<p>Meta networks. Tsendsuren Munkhdalai, Hong Yu, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning201770</p>
<p>On first-order meta-learning algorithms. Alex Nichol, Joshua Achiam, John Schulman, arXiv:1803.029992018arXiv preprint</p>
<p>Tadam: Task dependent adaptive metric for improved few-shot learning. Boris Oreshkin, Pau Rodríguez López, Alexandre Lacoste, Advances in Neural Information Processing Systems. 2018</p>
<p>Curiosity-driven exploration by self-supervised prediction. Deepak Pathak, Pulkit Agrawal, Alexei A Efros, Trevor Darrell, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition Workshops2017</p>
<p>Optimization as a model for few-shot learning. Sachin Ravi, Hugo Larochelle, 2016</p>
<p>Meta-learning with memory-augmented neural networks. Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, Timothy Lillicrap, International conference on machine learning. 2016</p>
<p>Manolis Savva, X Angel, Alexey Chang, Thomas Dosovitskiy, Vladlen Funkhouser, Koltun, Minos, arXiv:1712.03931Multimodal indoor simulator for navigation in complex environments. 2017arXiv preprint</p>
<p>Prototypical networks for few-shot learning. Jake Snell, Kevin Swersky, Richard Zemel, Advances in Neural Information Processing Systems. 2017</p>
<p>Intrinsic motivation and automatic curricula via asymmetric self-play. Sainbayar Sukhbaatar, Zeming Lin, Ilya Kostrikov, Gabriel Synnaeve, Arthur Szlam, Rob Fergus, arXiv:1703.054072017arXiv preprint</p>
<p>Learning to compare: Relation network for few-shot learning. Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Timothy M Philip Hs Torr, Hospedales, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2018</p>
<p>Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Doina Richard S Sutton, Satinder Precup, Singh, Artificial intelligence. 1121-21999</p>
<p>Learning metric-topological maps for indoor mobile robot navigation. Sebastian Thrun, Artificial Intelligence. 9911998</p>
<p>Transfer learning. Lisa Torrey, Jude Shavlik, Handbook of research on machine learning applications and trends: algorithms, methods, and techniques. IGI Global2010</p>
<p>Matching networks for one shot learning. Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, Advances in neural information processing systems. 2016</p>
<p>Reinforced cross-modal matching and selfsupervised imitation learning for vision-language navigation. Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang, Wang , Lei Zhang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2019</p>
<p>Look before you leap: Bridging model-free and model-based reinforcement learning for planned-ahead vision-and-language navigation. Xin Wang, Wenhan Xiong, Hongmin Wang, William Yang, Wang , Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)2018</p>
<p>A survey of transfer learning. Karl Weiss, Taghi M Khoshgoftaar, Dingding Wang, Journal of Big data. 3192016</p>
<p>Learning to learn how to learn: Self-adaptive visual navigation using meta-learning. Mitchell Wortsman, Kiana Ehsani, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2019</p>
<p>Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation. Yuhuai Wu, Elman Mansimov, Shun Roger B Grosse, Jimmy Liao, Ba, Advances in neural information processing systems. 2017</p>
<p>Building generalizable agents with a realistic and rich 3d environment. Yi Wu, Yuxin Wu, Georgia Gkioxari, Yuandong Tian, arXiv:1801.022092018arXiv preprint</p>
<p>Guided feature transformation (gft): A neural language grounding module for embodied agents. Haonan Yu, Xiaochen Lian, Haichao Zhang, Wei Xu, arXiv:1805.083292018arXiv preprint</p>
<p>Target-driven visual navigation in indoor scenes using deep reinforcement learning. Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei, Ali Farhadi, IEEE international conference on robotics and automation (ICRA). 2017. 2017IEEE</p>            </div>
        </div>

    </div>
</body>
</html>
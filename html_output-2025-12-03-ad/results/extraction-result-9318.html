<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9318 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9318</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9318</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-261064970</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2308.11483v1.pdf" target="_blank">Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated remarkable capabilities in various NLP tasks. However, previous works have shown these models are sensitive towards prompt wording, and few-shot demonstrations and their order, posing challenges to fair assessment of these models. As these models become more powerful, it becomes imperative to understand and address these limitations. In this paper, we focus on LLMs robustness on the task of multiple-choice questions -- commonly adopted task to study reasoning and fact-retrieving capability of LLMs. Investigating the sensitivity of LLMs towards the order of options in multiple-choice questions, we demonstrate a considerable performance gap of approximately 13% to 75% in LLMs on different benchmarks, when answer options are reordered, even when using demonstrations in a few-shot setting. Through a detailed analysis, we conjecture that this sensitivity arises when LLMs are uncertain about the prediction between the top-2/3 choices, and specific options placements may favor certain prediction between those top choices depending on the question caused by positional bias. We also identify patterns in top-2 choices that amplify or mitigate the model's bias toward option placement. We found that for amplifying bias, the optimal strategy involves positioning the top two choices as the first and last options. Conversely, to mitigate bias, we recommend placing these choices among the adjacent options. To validate our conjecture, we conduct various experiments and adopt two approaches to calibrate LLMs' predictions, leading to up to 8 percentage points improvement across different models and benchmarks.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9318.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9318.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot option-order sensitivity (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot sensitivity to answer-option ordering evaluated with GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation showing that mere reordering of multiple-choice options in a zero-shot prompt can cause large swings in GPT-4 accuracy; sensitivity arises despite high absolute accuracy on some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple-choice QA (CSQA, MMLU subsets, Big-Bench Logical Deduction)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice question answering across diverse benchmarks (CSQA commonsense with 5 options; Abstract Algebra, High School Chemistry, Professional Law from MMLU with 4 options; Logical Deduction from Big-Bench with 3 options).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot natural-language prompt: 'Choose the answer to the question only from A, B, C, D, and E choices. Question: {question}. Choices: {options}. Answer:' with the options presented in some order (the experiment systematically reorders options).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Alternative orderings of the same option set (oracle reordering to produce min and max model accuracy across orderings).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported task accuracies vary by benchmark; example observations include tasks where GPT-4 achieves >90% accuracy on average but still exhibits option-order sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Across reorders the performance range (min vs max) produced gaps; the paper reports sensitivity gaps for models across benchmarks ranging approximately from 13% to 75% (zero-shot aggregated statement). Example: even when GPT-4 accuracy >90% on some tasks, a sensitivity gap of 13.1% was observed.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Sensitivity gap (max minus min accuracy over orderings) reported approximately 13%–75% across benchmarks and models in zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Conjecture: sensitivity is caused by the interaction of (1) model uncertainty between the top candidate answers and (2) positional bias where option placement systematically skews selection when uncertain.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Zero-shot prompts as above; sensitivity gap computed as difference between maximum and minimum model performance achievable by reordering options (oracle ordering). Benchmarks: CSQA, Logical Deduction (BB), Abstract Algebra, High School Chemistry, Professional Law. No explicit model parameter counts provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9318.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9318.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot demonstrations effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Impact of adding few-shot demonstrations (similar-instance retrieval) on option-order sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Adding few-shot demonstrations (1/2/5 shots) chosen by nearest-neighbor similarity yields only marginal changes in robustness to option ordering; sensitivity gap generally remains substantial and does not reliably shrink simply by adding more demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple-choice QA (CSQA, Logical Deduction, Abstract Algebra, High School Chemistry, Professional Law)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same MCQ benchmarks as above; few-shot setting where demonstrations are sampled from 100 example pool by semantic similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot in-context learning with 1-shot, 2-shot, and 5-shot demonstrations inserted into the prompt. Demonstrations selected by nearest neighbors via Sentence-RoBERTa embeddings (Euclidean distance).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared across numbers of demonstrations (1/2/5-shot) and across option reorderings (min/max accuracy over reorderings).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Examples from the paper (InstructGPT few-shot): CSQA 1-shot vanilla accuracy 87.2% with observed Min 79.1% and Max 94.3% across orderings; Logical Deduction 1-shot vanilla 92.3% (Min 84.3%, Max 97.3%); other tasks reported similar min/max ranges (see paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Although average accuracy can improve with more demonstrations, the sensitivity gap (range between min and max across reorderings) remains substantial; increasing number of demonstrations does not consistently reduce sensitivity gap.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Demonstrations sometimes shrink the sensitivity gap as average performance improves, but adding more demonstrations does not guarantee reduced sensitivity; no single-shot-to-five-shot effect-size universally eliminates ordering sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Few-shot demonstrations improve overall accuracy but do not remove positional bias; improvements in robustness correlate with improved performance but are not sufficient alone to eliminate order sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Few-shot sampling: randomly choose 100 samples and draw demonstrations from those samples; similarity-based retrieval using Sentence-RoBERTa; experiments run for 1/2/5-shot; sensitivity measured by min/max over reorderings (error bars in figures indicate range).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9318.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9318.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Top-k pruning via model-sorted options</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reducing answer-option complexity by retaining top-k options sorted by model probability</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ask LLM to sort options by estimated probability and retain only top-2 or top-3 options (keeping original order among retained) to test positional bias; results suggest positional bias persists because performance remains nearly unchanged despite removal of unlikely options.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 and InstructGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple-choice QA (same benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>MCQ tasks where, for each instance, the model is first asked to rank options by probability; experiments then keep only the top-2/top-3 options (in original order) and re-evaluate.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Two-step format: (1) prompt the model to sort options in descending order of probability; (2) reduce the option set to top-2 or top-3 while retaining their original order and re-run prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared full-option evaluation vs reduced top-2/top-3 evaluation (with preserved original order of retained items).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Hits@1 correlates with overall accuracy; over 95% of instances where model initially predicted correctly are captured in Hits@2 and 100% in Hits@3 (as reported). After pruning to top-2/top-3, model performance remains nearly unchanged or shows only small incremental changes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Retaining only top-2 or top-3 options produced near-identical performance to the full option set in many cases, indicating positional biases rather than confusion from many low-probability distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>High coverage in Hits@2 (>95%) and Hits@3 (100%); removal of lower-probability options did not materially change accuracy in aggregate.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>If correct answer is among top-k predicted by the model, the remaining performance differences when options are pruned indicate positional bias (placement of top candidates influences final choice when model is uncertain).</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Sorting performed by prompting the LLM to rank options; Hits@ metrics computed; then top-2/top-3 retained in their original relative order and predictions re-evaluated (detailed results in paper Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9318.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9318.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Top-2 placement patterns</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Patterns of top-2 answer placement that amplify or mitigate positional bias</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systematic investigation of how the relative order and gap between the model's top-two candidate answers in the option list affects selection bias; identifies four pattern types and optimal placements for amplifying vs mitigating bias.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 and InstructGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple-choice QA (benchmarks as above)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>MCQ tasks where sensitivity arises from uncertainty among top-2 answers; study focuses on where those top-2 appear among the full list of options and the size of the gap between them.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Controlled reordering experiments fixing the relative placement of the model-identified top-2 choices into particular slots (e.g., first & last, adjacent first two, second & third, etc.) while reordering other options freely.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compare placements designed to amplify bias vs placements designed to mitigate bias; measure coverage of original sensitivity gap by these targeted placements.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table of coverage (paper Table 4) reports how much of the original sensitivity gap is covered by optimal amplifying/mitigating placements. Example percentages (sensitivity-gap coverage for amplifying vs mitigating) include: GPT-4 CSQA amplifying 62.9% / mitigating 22.7%; InstructGPT CSQA amplifying 71.7% / mitigating 38.3%; Logical Deduction GPT-4 amplifying 42.0% / mitigating 10.1%; see paper for full table.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Amplifying patterns cover between ~20% and 72% of original sensitivity gaps across tasks/models; mitigating patterns reduce coverage (i.e., better mitigation) with ranges ~0.9% to 38% depending on task and model.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Amplifying placement (e.g., putting top-2 as first and last options) can recover up to ~72% of the initial sensitivity gap for some tasks/models; mitigating placements (adjacent slots) can reduce the gap substantially (sometimes to <10% coverage).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Best amplification strategy: place the model's top-two candidate answers at the first and last option positions; best mitigation strategy: place top-two candidates adjacent (e.g., first-two or second-third). Differences likely arise from positional priors in model token generation and attention to early/late list positions when uncertain.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Top-2 choices extracted by asking LLMs to sort options; patterns defined by whether first-of-top2 appears earlier/later and size of gap (# of other choices between them). Experiments measured how much of the initial sensitivity gap is covered by fixing top-2 in particular placements (detailed in Table 3 and Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9318.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9318.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Majority-vote calibration (10 reorders)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Calibration by majority voting across predictions on multiple random option-order reorders</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple calibration approach that runs the model on 10 random reorderings of options and takes a majority vote; this bootstrapping reduces sensitivity and improves accuracy up to several percentage points.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 and InstructGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple-choice QA (same benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>MCQ tasks; the model is asked to answer the same question with differently randomized option orders, then predictions are aggregated by majority vote.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Calibration by ensembling: run model on 10 random reorders and take majority vote over predicted choices (simple bootstrapping).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to single-prompt (single ordering) evaluation and to MEC calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Majority-vote across 10 random reorders yielded up to an 8 percentage-point improvement in accuracy on some benchmarks (paper reports 'up to 8 percentage points' improvement overall).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Majority voting consistently improved performance relative to single-order evaluations across benchmarks; GPT-4 remained more stable than InstructGPT even after calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Improvement up to +8 percentage points (task-dependent).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Averaging over multiple random orderings reduces the impact of positional bias that affects individual-order predictions; computational cost is the main drawback.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Majority vote performed over model predictions from 10 random reorderings per instance. References/analogs: Stickland and Murray (2020) ensemble ideas and Stickland cited in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9318.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9318.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MEC calibration (explain-then-predict)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multiple Evidence Calibration (ask-for-explanation before final answer) applied to MCQ</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting the LLM to generate an explanation before producing a final answer (MEC) gave mixed or negative effects: it consistently decreased performance for InstructGPT and had mixed effects for GPT-4, often increasing uncertainty or causing hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 and InstructGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple-choice QA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>MCQ benchmarks where MEC prompts the model to explain its reasoning then choose an answer.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Explain-then-predict (Multiple Evidence Calibration) prompt format adapted from Wang et al. (2023b): ask model to generate explanation prior to the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to vanilla single-shot/ few-shot prompting and to majority-vote calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>InstructGPT: MEC consistently decreased performance (paper reports consistent decreases). GPT-4: MEC produced mixed results — in some benchmarks it improved, in others it underperformed compared to majority voting; overall reliability questioned.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>MEC underperformed majority-vote in several settings; for InstructGPT MEC often reduced accuracy compared to vanilla prompting and majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Negative effect for InstructGPT (consistent decrease); mixed for GPT-4 (varies by benchmark) — no single summary percent given beyond qualitative statements.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Asking for explanations can amplify model uncertainty (and hallucinations) especially when initial confidence is low; this may push the model toward earlier-positioned top choices or result in failure to produce a final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>MEC prompt follows Wang et al. (2023b). Analysis of mispredictions after MEC indicates increased uncertainty and hallucination; paper reports per-benchmark outcomes (see Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Calibrate before use: Improving few-shot performance of language models <em>(Rating: 2)</em></li>
                <li>Large language models are not fair evaluators <em>(Rating: 2)</em></li>
                <li>Diverse ensembles improve calibration <em>(Rating: 2)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 2)</em></li>
                <li>GPT-4 technical report <em>(Rating: 2)</em></li>
                <li>CommonsenseQA: A question answering challenge targeting commonsense knowledge <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9318",
    "paper_id": "paper-261064970",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "Zero-shot option-order sensitivity (GPT-4)",
            "name_full": "Zero-shot sensitivity to answer-option ordering evaluated with GPT-4",
            "brief_description": "Evaluation showing that mere reordering of multiple-choice options in a zero-shot prompt can cause large swings in GPT-4 accuracy; sensitivity arises despite high absolute accuracy on some tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "task_name": "Multiple-choice QA (CSQA, MMLU subsets, Big-Bench Logical Deduction)",
            "task_description": "Multiple-choice question answering across diverse benchmarks (CSQA commonsense with 5 options; Abstract Algebra, High School Chemistry, Professional Law from MMLU with 4 options; Logical Deduction from Big-Bench with 3 options).",
            "presentation_format": "Zero-shot natural-language prompt: 'Choose the answer to the question only from A, B, C, D, and E choices. Question: {question}. Choices: {options}. Answer:' with the options presented in some order (the experiment systematically reorders options).",
            "comparison_format": "Alternative orderings of the same option set (oracle reordering to produce min and max model accuracy across orderings).",
            "performance": "Reported task accuracies vary by benchmark; example observations include tasks where GPT-4 achieves &gt;90% accuracy on average but still exhibits option-order sensitivity.",
            "performance_comparison": "Across reorders the performance range (min vs max) produced gaps; the paper reports sensitivity gaps for models across benchmarks ranging approximately from 13% to 75% (zero-shot aggregated statement). Example: even when GPT-4 accuracy &gt;90% on some tasks, a sensitivity gap of 13.1% was observed.",
            "format_effect_size": "Sensitivity gap (max minus min accuracy over orderings) reported approximately 13%–75% across benchmarks and models in zero-shot.",
            "explanation_or_hypothesis": "Conjecture: sensitivity is caused by the interaction of (1) model uncertainty between the top candidate answers and (2) positional bias where option placement systematically skews selection when uncertain.",
            "null_or_negative_result": false,
            "experimental_details": "Zero-shot prompts as above; sensitivity gap computed as difference between maximum and minimum model performance achievable by reordering options (oracle ordering). Benchmarks: CSQA, Logical Deduction (BB), Abstract Algebra, High School Chemistry, Professional Law. No explicit model parameter counts provided.",
            "uuid": "e9318.0",
            "source_info": {
                "paper_title": "Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Few-shot demonstrations effect",
            "name_full": "Impact of adding few-shot demonstrations (similar-instance retrieval) on option-order sensitivity",
            "brief_description": "Adding few-shot demonstrations (1/2/5 shots) chosen by nearest-neighbor similarity yields only marginal changes in robustness to option ordering; sensitivity gap generally remains substantial and does not reliably shrink simply by adding more demonstrations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructGPT (text-davinci-003)",
            "model_size": null,
            "task_name": "Multiple-choice QA (CSQA, Logical Deduction, Abstract Algebra, High School Chemistry, Professional Law)",
            "task_description": "Same MCQ benchmarks as above; few-shot setting where demonstrations are sampled from 100 example pool by semantic similarity.",
            "presentation_format": "Few-shot in-context learning with 1-shot, 2-shot, and 5-shot demonstrations inserted into the prompt. Demonstrations selected by nearest neighbors via Sentence-RoBERTa embeddings (Euclidean distance).",
            "comparison_format": "Compared across numbers of demonstrations (1/2/5-shot) and across option reorderings (min/max accuracy over reorderings).",
            "performance": "Examples from the paper (InstructGPT few-shot): CSQA 1-shot vanilla accuracy 87.2% with observed Min 79.1% and Max 94.3% across orderings; Logical Deduction 1-shot vanilla 92.3% (Min 84.3%, Max 97.3%); other tasks reported similar min/max ranges (see paper tables).",
            "performance_comparison": "Although average accuracy can improve with more demonstrations, the sensitivity gap (range between min and max across reorderings) remains substantial; increasing number of demonstrations does not consistently reduce sensitivity gap.",
            "format_effect_size": "Demonstrations sometimes shrink the sensitivity gap as average performance improves, but adding more demonstrations does not guarantee reduced sensitivity; no single-shot-to-five-shot effect-size universally eliminates ordering sensitivity.",
            "explanation_or_hypothesis": "Few-shot demonstrations improve overall accuracy but do not remove positional bias; improvements in robustness correlate with improved performance but are not sufficient alone to eliminate order sensitivity.",
            "null_or_negative_result": false,
            "experimental_details": "Few-shot sampling: randomly choose 100 samples and draw demonstrations from those samples; similarity-based retrieval using Sentence-RoBERTa; experiments run for 1/2/5-shot; sensitivity measured by min/max over reorderings (error bars in figures indicate range).",
            "uuid": "e9318.1",
            "source_info": {
                "paper_title": "Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Top-k pruning via model-sorted options",
            "name_full": "Reducing answer-option complexity by retaining top-k options sorted by model probability",
            "brief_description": "Ask LLM to sort options by estimated probability and retain only top-2 or top-3 options (keeping original order among retained) to test positional bias; results suggest positional bias persists because performance remains nearly unchanged despite removal of unlikely options.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 and InstructGPT",
            "model_size": null,
            "task_name": "Multiple-choice QA (same benchmarks)",
            "task_description": "MCQ tasks where, for each instance, the model is first asked to rank options by probability; experiments then keep only the top-2/top-3 options (in original order) and re-evaluate.",
            "presentation_format": "Two-step format: (1) prompt the model to sort options in descending order of probability; (2) reduce the option set to top-2 or top-3 while retaining their original order and re-run prediction.",
            "comparison_format": "Compared full-option evaluation vs reduced top-2/top-3 evaluation (with preserved original order of retained items).",
            "performance": "Hits@1 correlates with overall accuracy; over 95% of instances where model initially predicted correctly are captured in Hits@2 and 100% in Hits@3 (as reported). After pruning to top-2/top-3, model performance remains nearly unchanged or shows only small incremental changes.",
            "performance_comparison": "Retaining only top-2 or top-3 options produced near-identical performance to the full option set in many cases, indicating positional biases rather than confusion from many low-probability distractors.",
            "format_effect_size": "High coverage in Hits@2 (&gt;95%) and Hits@3 (100%); removal of lower-probability options did not materially change accuracy in aggregate.",
            "explanation_or_hypothesis": "If correct answer is among top-k predicted by the model, the remaining performance differences when options are pruned indicate positional bias (placement of top candidates influences final choice when model is uncertain).",
            "null_or_negative_result": false,
            "experimental_details": "Sorting performed by prompting the LLM to rank options; Hits@ metrics computed; then top-2/top-3 retained in their original relative order and predictions re-evaluated (detailed results in paper Table 2).",
            "uuid": "e9318.2",
            "source_info": {
                "paper_title": "Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Top-2 placement patterns",
            "name_full": "Patterns of top-2 answer placement that amplify or mitigate positional bias",
            "brief_description": "Systematic investigation of how the relative order and gap between the model's top-two candidate answers in the option list affects selection bias; identifies four pattern types and optimal placements for amplifying vs mitigating bias.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 and InstructGPT",
            "model_size": null,
            "task_name": "Multiple-choice QA (benchmarks as above)",
            "task_description": "MCQ tasks where sensitivity arises from uncertainty among top-2 answers; study focuses on where those top-2 appear among the full list of options and the size of the gap between them.",
            "presentation_format": "Controlled reordering experiments fixing the relative placement of the model-identified top-2 choices into particular slots (e.g., first & last, adjacent first two, second & third, etc.) while reordering other options freely.",
            "comparison_format": "Compare placements designed to amplify bias vs placements designed to mitigate bias; measure coverage of original sensitivity gap by these targeted placements.",
            "performance": "Table of coverage (paper Table 4) reports how much of the original sensitivity gap is covered by optimal amplifying/mitigating placements. Example percentages (sensitivity-gap coverage for amplifying vs mitigating) include: GPT-4 CSQA amplifying 62.9% / mitigating 22.7%; InstructGPT CSQA amplifying 71.7% / mitigating 38.3%; Logical Deduction GPT-4 amplifying 42.0% / mitigating 10.1%; see paper for full table.",
            "performance_comparison": "Amplifying patterns cover between ~20% and 72% of original sensitivity gaps across tasks/models; mitigating patterns reduce coverage (i.e., better mitigation) with ranges ~0.9% to 38% depending on task and model.",
            "format_effect_size": "Amplifying placement (e.g., putting top-2 as first and last options) can recover up to ~72% of the initial sensitivity gap for some tasks/models; mitigating placements (adjacent slots) can reduce the gap substantially (sometimes to &lt;10% coverage).",
            "explanation_or_hypothesis": "Best amplification strategy: place the model's top-two candidate answers at the first and last option positions; best mitigation strategy: place top-two candidates adjacent (e.g., first-two or second-third). Differences likely arise from positional priors in model token generation and attention to early/late list positions when uncertain.",
            "null_or_negative_result": false,
            "experimental_details": "Top-2 choices extracted by asking LLMs to sort options; patterns defined by whether first-of-top2 appears earlier/later and size of gap (# of other choices between them). Experiments measured how much of the initial sensitivity gap is covered by fixing top-2 in particular placements (detailed in Table 3 and Table 4).",
            "uuid": "e9318.3",
            "source_info": {
                "paper_title": "Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Majority-vote calibration (10 reorders)",
            "name_full": "Calibration by majority voting across predictions on multiple random option-order reorders",
            "brief_description": "A simple calibration approach that runs the model on 10 random reorderings of options and takes a majority vote; this bootstrapping reduces sensitivity and improves accuracy up to several percentage points.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 and InstructGPT",
            "model_size": null,
            "task_name": "Multiple-choice QA (same benchmarks)",
            "task_description": "MCQ tasks; the model is asked to answer the same question with differently randomized option orders, then predictions are aggregated by majority vote.",
            "presentation_format": "Calibration by ensembling: run model on 10 random reorders and take majority vote over predicted choices (simple bootstrapping).",
            "comparison_format": "Compared to single-prompt (single ordering) evaluation and to MEC calibration.",
            "performance": "Majority-vote across 10 random reorders yielded up to an 8 percentage-point improvement in accuracy on some benchmarks (paper reports 'up to 8 percentage points' improvement overall).",
            "performance_comparison": "Majority voting consistently improved performance relative to single-order evaluations across benchmarks; GPT-4 remained more stable than InstructGPT even after calibration.",
            "format_effect_size": "Improvement up to +8 percentage points (task-dependent).",
            "explanation_or_hypothesis": "Averaging over multiple random orderings reduces the impact of positional bias that affects individual-order predictions; computational cost is the main drawback.",
            "null_or_negative_result": false,
            "experimental_details": "Majority vote performed over model predictions from 10 random reorderings per instance. References/analogs: Stickland and Murray (2020) ensemble ideas and Stickland cited in the paper.",
            "uuid": "e9318.4",
            "source_info": {
                "paper_title": "Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "MEC calibration (explain-then-predict)",
            "name_full": "Multiple Evidence Calibration (ask-for-explanation before final answer) applied to MCQ",
            "brief_description": "Prompting the LLM to generate an explanation before producing a final answer (MEC) gave mixed or negative effects: it consistently decreased performance for InstructGPT and had mixed effects for GPT-4, often increasing uncertainty or causing hallucination.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 and InstructGPT",
            "model_size": null,
            "task_name": "Multiple-choice QA",
            "task_description": "MCQ benchmarks where MEC prompts the model to explain its reasoning then choose an answer.",
            "presentation_format": "Explain-then-predict (Multiple Evidence Calibration) prompt format adapted from Wang et al. (2023b): ask model to generate explanation prior to the final answer.",
            "comparison_format": "Compared to vanilla single-shot/ few-shot prompting and to majority-vote calibration.",
            "performance": "InstructGPT: MEC consistently decreased performance (paper reports consistent decreases). GPT-4: MEC produced mixed results — in some benchmarks it improved, in others it underperformed compared to majority voting; overall reliability questioned.",
            "performance_comparison": "MEC underperformed majority-vote in several settings; for InstructGPT MEC often reduced accuracy compared to vanilla prompting and majority voting.",
            "format_effect_size": "Negative effect for InstructGPT (consistent decrease); mixed for GPT-4 (varies by benchmark) — no single summary percent given beyond qualitative statements.",
            "explanation_or_hypothesis": "Asking for explanations can amplify model uncertainty (and hallucinations) especially when initial confidence is low; this may push the model toward earlier-positioned top choices or result in failure to produce a final answer.",
            "null_or_negative_result": true,
            "experimental_details": "MEC prompt follows Wang et al. (2023b). Analysis of mispredictions after MEC indicates increased uncertainty and hallucination; paper reports per-benchmark outcomes (see Table 5).",
            "uuid": "e9318.5",
            "source_info": {
                "paper_title": "Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Calibrate before use: Improving few-shot performance of language models",
            "rating": 2,
            "sanitized_title": "calibrate_before_use_improving_fewshot_performance_of_language_models"
        },
        {
            "paper_title": "Large language models are not fair evaluators",
            "rating": 2,
            "sanitized_title": "large_language_models_are_not_fair_evaluators"
        },
        {
            "paper_title": "Diverse ensembles improve calibration",
            "rating": 2,
            "sanitized_title": "diverse_ensembles_improve_calibration"
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 2,
            "sanitized_title": "training_language_models_to_follow_instructions_with_human_feedback"
        },
        {
            "paper_title": "GPT-4 technical report",
            "rating": 2,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "CommonsenseQA: A question answering challenge targeting commonsense knowledge",
            "rating": 2,
            "sanitized_title": "commonsenseqa_a_question_answering_challenge_targeting_commonsense_knowledge"
        }
    ],
    "cost": 0.014878249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions</p>
<p>Pouya Pezeshkpour 
Megagon Labs
Megagon Labs</p>
<p>Estevam Hruschka estevam@megagon.ai 
Megagon Labs
Megagon Labs</p>
<p>Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions</p>
<p>Large Language Models (LLMs) have demonstrated remarkable capabilities in various NLP tasks. However, previous works have shown these models are sensitive towards prompt wording, and few-shot demonstrations and</p>
<p>their order, posing challenges to fair assessment of these models. As these models become more powerful, it becomes imperative to understand and address these limitations. In this paper, we focus on LLMs robustness on the task of multiple-choice questionscommonly adopted task to study reasoning and fact-retrieving capability of LLMs. Investigating the sensitivity of LLMs towards the order of options in multiple-choice questions, we demonstrate a considerable performance gap of approximately 13% to 75% in LLMs on different benchmarks, when answer options are reordered, even when using demonstrations in a few-shot setting. Through a detailed analysis, we conjecture that this sensitivity arises when LLMs are uncertain about the prediction between the top-2/3 choices, and specific options placements may favor certain prediction between those top choices depending on the question caused by positional bias. We also identify patterns in top-2 choices that amplify or mitigate the model's bias toward option placement. We found that for amplifying bias, the optimal strategy involves positioning the top two choices as the first and last options. Conversely, to mitigate bias, we recommend placing these choices among the adjacent options. To validate our conjecture, we conduct various experiments and adopt two approaches to calibrate LLMs' predictions, leading to up to 8 percentage points improvement across different models and benchmarks.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have demonstrated impressive performance on various tasks, surpassing that of supervised models and, in some cases, even outperforming humans (Chowdhery Figure 1: GPT-4 sensitivity to reordering options: upon changing the order of choices, GPT-4 change its prediction from "hen house" to "outside of bedroom window" (the example is from CSQA dataset). Touvron et al., 2023;OpenAI, 2023). However, despite their impressive capabilities, previous research has highlighted certain limitations. For instance, LLMs have shown significant sensitivity to small changes in the prompt (Zhao et al., 2021;Wang et al., 2023a;Zhu et al., 2023). Therefore, a more comprehensive and conclusive analysis of different aspects that can affect/limit LLMs' performance is crucial for a fair assessment and their successful real-world adoption. One significant limitation lies in the robustness of LLMs concerning the arrangement of various components in a prompt, as it directly impacts the assessment of their capability in understanding and reasoning for specific tasks. Prior research has demonstrated that LLMs exhibit sensitivity to the arrangement of few-shot demonstrations (Zhao et al., 2021) and the order of appearance for responses generated by candidate models when LLMs are used as referees to evaluate quality (Wang et al., 2023b). Given these findings, it becomes pertinent to inquire whether LLMs are also sensitive to the order of elements of the prompts in different tasks. For example, how much does the order of options in multiple-choice question (MCQ) answering tasks can impact the LLMs performance.</p>
<p>In this paper, we investigating the sensitivity of LLMs to the order of options in multiple-choice questions; using it as a proxy to understand LLMs sensitivity to the order of prompt elements in instruction-or demonstration-based paradigm. We demonstrate an example of GPT-4's sensitivity to options order in Figure 1, using a sample from the CSQA benchmark (Talmor et al., 2018). Notably, by merely rearranging the placement of options among choices A, C, and E, GPT-4 incorrectly predicts the answer to be "outside bedroom window". Within this context, we aim to address the following research questions: (1) To what extent do LLMs exhibit sensitivity to the order of options in multiplechoice questions? (2) What factors contribute to LLMs' sensitivity to the order of options? (3) How can we improve LLMs' robustness to the order of options?</p>
<p>To answer the first question, we conducted experiments using GPT-4 (OpenAI, 2023) and In-structGPT (text-davinci-003) (Ouyang et al., 2022) on five different multiple-choice question benchmarks. Surprisingly, we discovered a substantial sensitivity gap of up to 75% in the zero-shot setting. Additionally, in the few-shot setting, we observed that introducing demonstrations to the prompt only led to marginal improvements in LLMs' robustness if their performance increased. Moving on to the second question, we put forth a conjecture that the sensitivity of LLMs stems from their positional bias, wherein they tend to favor certain placements when uncertain about the answer among the top choices. To validate our conjecture, we analyzed instances where the models' predictions changed upon reordering the options. Furthermore, we demonstrated that the complexity of the number of choices, while retaining the top possible answers, had only a gradual impact (if any) on performance improvement.</p>
<p>Additionally, we discerned patterns in the occurrence of top-2 possible choices that influence the model's probability of selecting a particular option or somewhat mitigate LLMs' positional bias. For amplifying bias, we found that the optimal strategy involves positioning the top two choices as the first and last options. Conversely, to mitigate bias, we recommend placing these choices among the adjacent options. To validate our findings, we conducted qualitative evaluations. Addressing the last question, we demonstrated that employing two different calibrating approaches led to a notable improvement in LLMs' performance, up to 8 percentage points. Through these investigations, we contribute to a deeper understanding of how the order of options affects LLMs' decision-making in multiple-choice questions (MCQ) and offer practical solutions, which go beyond simple bootstrapping, to increase their robustness and accuracy in such scenarios.</p>
<p>Background and Experimental Details</p>
<p>This paper focuses on the task of multiple-choice question answering. In multiple-choice questions, the objective is to identify the correct answer to a given question from a set of possible options (an illustration is presented in Figure 1). To address this task using in-context learning models, we present a prompt in the following format: "Choose the answer to the question only from A, B, C, D, and E choices. Question: {question}. Choices: {options}. Answer:" to the models. Also, for few-shot demonstrations we randomly choose 100 samples and draw demonstrations from those samples.</p>
<p>Models: In our evaluations, we employed two widely-used large language models, InstructGPT (text-davinci-003) (Ouyang et al., 2022) and GPT-4 (OpenAI, 2023), to represent LLMs of different sizes. These models were chosen to assess the impact of model size on its robustness to the order of options in MCQ task. data: To investigate the sensitivity of LLMs to the order of options and the reasons behind this phenomenon, we conducted experiments on five distinct MCQ benchmarks. These benchmarks are as follows: CSQA (Talmor et al., 2018): A commonsense multiple-choice question answering dataset, where each question is accompanied by 5 options. Abstract Algebra, High School Chemistry, and Professional Law from the MMLU benchmark (Hendrycks et al., 2020): These benchmarks consist of multiple-choice questions with 4 options provided for each question. And, Logical deduction from the Big-Bench dataset (Srivastava et al., 2022): This benchmark offers multiplechoice questions with 3 options for each question. Our selection of these benchmarks was guided by three specific criteria: (1) Domain diversity: We aimed to investigate the sensitivity to options order across different domains. (2) Varying option numbers: In order to explore the impact of the num- </p>
<p>Sensitivity to Order</p>
<p>In this section, we first investigation the sensitivity of InstructGPT and GPT-4 to the order of options in the zero-shot setting. Then, we set out to determine whether introducing demonstrations to the prompt in the few-shot setting can enhance the models' robustness. To quantify sensitivity, we calculate the sensitivity gap, which is the difference between the maximum and minimum LLMs' performance when using an oracle ordering. In other words, we examine how specific reordering of options affects the models' predictions when the ground truth is known.</p>
<p>Zero-shot Sensitivity</p>
<p>The result of LLMs sensitivity to the order of options is presented in Table 1. Several noteworthy observations emerge from these results: (1) GPT-4 demonstrates significantly lower sensitivity gap compared to InstructGPT. This suggests that GPT-4 is less affected by the rearrangement of options in the prompt, making it more robust in handling such variations.</p>
<p>(2) Even in tasks where GPT-4 achieves high accuracy levels exceeding 90%, we still observe a considerable sensitivity gap of 13.1%. This indicates that even high-performing models are susceptible to changes in options order, which can impact their fair assessment.</p>
<p>(3) Although the sensitivity gap shows some correlation with the models' performance, tasks where LLMs perform poorly do not necessarily exhibit higher sensitivity gaps. This suggests that factors beyond overall accuracy may also influence LLMs' sensitivity to options order.</p>
<p>(4) The domain and the number of options in the MCQ tasks seem to affect the model's performance. However, we do not observe a clear correlation between these factors and the sensitivity gap.</p>
<p>Can Demonstration in Few-shot Setting</p>
<p>Resolve the Sensitivity?</p>
<p>Having demonstrated the high level of sensitivity when zero-shot prompting LLMs, a crucial question that arises is whether adding demonstrations in the few-shot setting to the prompt can enhance the models' robustness. To address this, we select demonstrations in the few-shot setting by sampling the most similar instances. We achieve this by computing the Euclidean distance over vector representations of questions obtained from Sentence-RoBERTa (Reimers and Gurevych, 2019). The result of order sensitivity in the few-shot setting are visualized in Figure 2 (more detailed results is provided in Appendix). Each bar in the figure is accompanied by error bars, representing the range of maximum and minimum model performance achievable by reordering the options, with knowledge of the ground truth. From the results, we make the following observations: Firstly, the sensitivity gap consistently remains substantial even with the inclusion of more demonstrations in the few-shot setting. Furthermore, as performances improve, the sensitivity gap tends to shrink. However, adding more demonstrations does not necessarily lead to a reduction in the sensitivity gap. This highlights the complexity of the relationship between model performance, the presence of demonstrations, and sensitivity to options order in the few-shot setting, i.e., while demonstrations may marginally improve robustness, they do not entirely mitigate the models' sensitivity to options order.  1) The sensitivity gap consistently remains substantial even with the addition of more demonstrations in the few-shot setting.</p>
<p>(2) As performances improve, the sensitivity gap shrinks.</p>
<p>(3) Adding more demonstrations does not necessarily result in a reduction of the sensitivity gap.</p>
<p>Why Do LLMs Show Sensitivity to the Order of Options?</p>
<p>After analyzing instances in which reordering the options resulted in a change in LLMs prediction, we arrive at the following conjecture:</p>
<p>Conjecture 4.1. The sensitivity of LLMs to the order of options in multiple-choice questions arises from the interaction of two colluding forces: (1) Uncertainty of LLMs regarding the correct answer among the top possible choices. And (2) positional bias, leading LLMs to favor specific options based on the order they appear in, depending on the question.</p>
<p>In the subsequent sections, we begin by empirically validating the conjecture. Furthermore, we identify specific patterns in the options that either amplify or mitigate the model's bias towards their placement.</p>
<p>Uncertainty Meets Positional Bias</p>
<p>To empirically validade our conjecture we devise qualitative experiments aimed at verifying each underlying reason behind the order sensitivity.</p>
<p>Uncertainty: We assess the uncertainty of LLMs concerning instances where reordering affects pre-dictions through a three-step analytical approach. It is important to highlight that GPT-4 and Instruct-GPT lack direct confidence measurements, necessitating our indirect analyses to validate our hypothesis.</p>
<p>(1) The sensitivity gap, which comprises instances where reordering changes the prediction, exhibits a strong correlation with the error rate. The correlation plot between sensitivity gap and LLMs error rate on different benchmarks is depicted in Figure 3.</p>
<p>(2) More than 60% of the sensitive samples identified in GPT-4 also exhibit sensitivity in Instruct-GPT. Additionally, many of these instances contain semantically similar top-choice options. For instance, both InstructGPT and GPT-4 consider the question "Most items in retail stores are what even when they are on sale? A) overpriced B) purchase C) expensive D) park E) buying" from CSQA with top choices "overpriced" and "expensive", as sensitive.</p>
<p>(3) To further verify models' uncertainty towards sensitive instances, we conduct a self-verification process by posing the following question to the LLMs: "Can more than one of the choices be a highly probable answer to the question? Please   respond with 'yes' or 'no'. Question: {ques-tion}. Choices: {options}. Answer:". Remarkably, LLMs consistently predict "yes" for over 94% of the sensitive cases across various benchmarks, further confirming their uncertainty in these scenarios.</p>
<p>Positional Bias: We aim to explore the effect of positional bias in LLMs' order sensitivity by reducing sample difficulty, retaining only the top possible choices while preserving their original order of appearance, and eliminating the rest of the options. The goal is to isolate the influence of positional bias, disentangling it from other potential hidden factors impacting order sensitivity. To identify the top possible choices for each question, we ask LLMs to sort the options in descending order of probability for answering the question. We ob-serve that the Hits@1 metric, which measures the accuracy of the gold truth being the first item in the sorted options, closely aligns with LLMs' overall task accuracy. Moreover, over 95% and 100% of instances that LLMs predict correctly are captured in Hits@2 and Hits@3, respectively. The detailed results of Hits@ metrics for both  and InstructGPT are provided in Table 2.</p>
<p>With the successful identification of the top possible choices by asking LLMs to sort the options, we proceed to investigate the impact of removing the least probable choices on the models' performance, aiming to establish the presence of positional bias. The results of retaining only the top-2 and top-3 choices after sorting the options using LLMs themselves, while preserving their original order of appearance, are presented in Table 2. We observe that despite achieving high Hits@2 and Hits@3 scores (covering all the samples where models initially predicted them correctly), LLMs' performance remains nearly unchanged or exhibits incremental improvements or declines. This observation provides further evidence of the impact of positional bias in order sensitivity.</p>
<p>What Patterns Amplify or Mitigate the Positional Bias?</p>
<p>In here we investigate the impact of certain patterns in the options on the intensity of positional bias. We categorize our findings based on number of options and the target large language model. We limit our investigation to the order of the top-2 choices (extracted from the sorted options list) in the options and their impact on the models' prediction to identify influential patterns. We defer further analysis of patterns involving options beyond the top-2 choices to future research.  Our goal is to identify patterns that amplify the positional bias, increasing the probability of the LLM to choose one answer over another based on their position, or mitigating the positional bias, decreasing dependency of the LLM to choose one answer over another based on their position. Upon investigating the order and placement of top-2 choices in instances where reordering change the prediction, we discover four different patterns:</p>
<p>• Pattern 1: First choice in top-2 appear earlier than the second choice in the options, and having less gap (less number of other choices) between them helps the goal more, i.e., to amplify or mitigate the positional bias.</p>
<p>• Pattern 2: First choice in top-2 appear earlier than the second choice in the options, and having more gap between them helps the goal more.</p>
<p>• Pattern 3: First choice in top-2 appear later than the second choice in the options, and having less gap between them helps the goal more.</p>
<p>• Pattern 4: First choice in top-2 appear later than the second choice in the options, and having more gap between them helps the goal more.</p>
<p>The best pattern, along with its best corresponding order instantiation (placement of top-2 choices), for amplifying or mitigating positional bias based on the type of LLMs and the number of options in the multiple-choice question task is presented in Table 3. For instance, to amplify the positional bias between two choices with the objective of increasing the probability of selecting the first choice as the answer for GPT-4, pattern number 2 proves to be the most effective. The ideal instantiation of this pattern is to place the first choice in option A and the second choice in option E. Investigating the positional bias in various LLMs with different numbers of options in the MCQ task reveal interesting findings. In both GPT-4 and Instruct-GPT, the most influential pattern to amplify the bias remains the same while for mitigating bias the best pattern jumps between first and third patterns. Furthermore, there is a notable contrast between InstructGPT and GPT-4 in their reactions to patterns regarding the order of appearance in the top-2 choices or the size of the gap between them. In overall, to mitigate bias, it appears to be more effective for the top-2 choices to either appear in the first two options or in the second and third options. Conversely, for amplifying bias, it is preferable for the top-2 choices to be positioned in the first and last options.</p>
<p>To validate the discovered patterns and their impact on LLMs' order sensitivity, we conducted two sets of experiments. Firstly, to confirm the effectiveness of patterns amplifying positional bias, we selected the best instantiation of each pattern and measured the performance improvement achieved by placing only the top-2 choice (where the ground truth is at top-1, and top-2 is obtained by sorting the options) in that instantiation. Meanwhile, we kept the order of appearance for other choices. Also, we measured the decrease in LLMs' performance by using the reverse instantiation of the pattern. Our goal here, is to assess the extent to which the sensitivity gap identified in Section 3.1 could be achieved simply by utilizing the most impactful placement. As a result, a higher percentage of coverage over the original sensitivity gap here means that the identified pattern did a better job at amplifying bias.</p>
<p>Secondly, to validate the patterns mitigating the bias, we performed a similar experiment as in Section 3.1, but this time, we fixed the top-2 choices in the placements provided in Table 3 and reordered all other options accordingly. The purpose here is to demonstrate how much of the sensitivity gap can be minimized by following identified mitigating patterns. As a result, a lower percentage of coverage over the original sensitivity gap here means that the identified pattern did a better job at mitigating bias.  Table 4: Percentage of initial sensitivity gap covered using the identified patterns to amplify and mitigate positional bias. A higher percentage in amplifying bias and a lower percentage in mitigating bias indicate better performance in this context. Table 1) by the optimal pattern for amplifying and mitigating positional bias, with more detailed results available in Appendix. A higher percentage in amplifying bias and a lower percentage in mitigating bias indicate better performance of the identified pattern. The amplifying patterns demonstrate sensitivity gap coverage ranging from 20% to 72%, while the mitigating bias pattern ranges from 0.9% to 38%. These results validate the effectiveness of the identified pattern for both amplifying and mitigating bias. Additionally, in most cases, the amplifying pattern covers a considerably greater portion of the sensitivity gap comparing to the mitigating pattern. It is important to highlight that the patterns we have identified for amplifying bias can serve as valuable insights for enhancing model performance or launching adversarial attacks against them. Furthermore, the patterns we have established for mitigating bias can play a crucial role in shaping benchmark design and guiding annotating efforts to create less biased benchmarks.</p>
<p>Calibrating LLMs for MCQ Tasks</p>
<p>We conduct an in-depth investigation into how large language models react to changes in the order of options, and investigate the reasons behind their sensitivity to such changes. Through our exploration, we have clearly observed that LLMs are highly responsive to the sequence in which options are presented. This has led us to a critical juncture where we need to focus on methods to improve the models' resilience to variations in options order, ensuring more dependable and trustworthy evaluations.</p>
<p>One potential solution we have considered is the calibration of LLMs predictions. The outcomes of calibrating LLMs predictions to mitigate order sensitivity by taking majority vote over models prediction in 10 random reorders in a simple bootstrapping approach (Stickland and Murray, 2020;Hou et al., 2023), are provided in Table 5. Our analysis has unveiled a significant observation: employing a majority vote approach for evaluating LLMs across a range of benchmarks results in a substantial performance improvement of up to 8 percentage points. Notably, we find that GPT-4 showcases a higher level of stability against changes in options order compared to InstructGPT, a trait that persists even after implementing prediction calibration. Furthermore, while LLMs' performance on benchmarks featuring four options might be somewhat inferior to those with three or five options, GPT-4 displays a greater resilience following prediction calibration. In contrast, InstructGPT demonstrates heightened robustness in specific contexts like CSQA and high school chemistry, where the effects of calibration are more pronounced.</p>
<p>We have also incorporated the approach of Multiple Evidence Calibration (MEC) introduced by Wang et al. (2023b). In their work, they propose to counteract LLMs' sensitivity by prompting the model to generate an explanation before providing its prediction. We adopt their provided prompt for solving MCQ tasks. The impact of applying MEC calibration and its implications for order sensitivity are outlined in Table 5.</p>
<p>The results from InstructGPT performance reveal that the introduction of MEC calibration results in a consistent decrease in model performance. This behavior contradicts the outcomes achieved through majority voting and underscores the unsuitability of MEC calibration for multiple-choice question tasks. In the case of GPT-4, the integration of MEC calibration also yields contrasting outcomes with respect to majority voting, particularly evident in benchmarks such as CSQA, abstract algebra, and high school chemistry. For logical deduction and professional law benchmarks, while both majority voting and MEC calibration result in improving the model performance, the  amount of improvement differs considerably, thus casting doubt on the reliability of the MEC approach in GPT-4 as well. Subsequent analysis of LLMs' mispredictions following MEC calibration unveils a noteworthy trend: the act of asking the models to explain their reasoning amplifies their uncertainty (partly due to hallucination), particularly in instances where the model's initial confidence was not very high. This, in turn, either frequently guides the models to opt for answer among the top choices, which is positioned earlier in the available options or causing the model to not picking any final answer.</p>
<p>Related Work</p>
<p>Large language models (LLMs) show remarkable accomplishments and capabilities on various NLP tasks, including answering multiple-choice questions. In order to ascertain the dependability of LLMs' proficiency, it becomes imperative to delve into the robustness of their performance when subjected to subtle changes in the input.</p>
<p>LLMs and multiple-choice questions In recent years, multiple-choice questions have been introduced as a method for assessing the reasoning and fact-retrieval capabilities of models (Richardson et al., 2013;Talmor et al., 2018;Clark et al., 2020;Hendrycks et al., 2020). Despite the intricate nature of these tasks, significant strides have been made by large language models achieving human-like performances across various MCQ benchmarks (Liévin et al., 2022;Robinson et al., 2022;OpenAI, 2023;Savelka et al., 2023;Anil et al., 2023). However, the ability of these tasks to effectively gauge the reasoning and factual knowledge of LLMs, along with the reliability of the evaluation settings, presents substantial challenges that warrant deeper investigation.</p>
<p>Sensitivity of LLMs</p>
<p>With the growing prominence of LLMs in addressing NLP tasks, significant attention has been devoted to examining the robust-ness and vulnerabilities of these models. These efforts predominantly focus on two distinct levels:</p>
<p>(1) At the instance level, researchers investigate the robustness of LLMs by studying how modifications or adversarial attacks impact individual instances. For example, Zhao et al. (2021) reveal LLMs' sensitivity to prompt choice and demonstrations order in in-context learning (ICL). Hou et al. (2023) show LLMs are sensitive to the order of sequential interaction histories when used as conditions in ranking candidates for recommender systems. Wang et al. (2023a) launch adversarial attacks on LLM predictions through modifications to ICL demonstrations. Wang et al. (2023b) also explore LLMs' susceptibility to the order of response appearances from candidate models when LLMs serve as referees.</p>
<p>(2) At the alignment level, attempts are made to deliberately misalign LLMs to manipulate their behavior, often referred to as "jailbreaking." Perez and Ribeiro (2022); Zou et al. (2023) achieve misalignment by adversarially attacking the prompt. In a similar vein, (Wolf et al., 2023) propose a theoretical framework that exposes limitations in aligning LLMs, demonstrating there exist prompts that can cause models to exhibit any behavior with finite probability. Furthermore, Wei et al. (2023) propose that jailbreaking arises from conflicting objectives and mismatched generalization, utilizing their hypothesis to develop effective jailbreak strategies.</p>
<p>Discussion and Conclusion</p>
<p>We investigate the inherent sensitivity of language models to the arrangement of options in multiplechoice questions. Upon measuring the intensity of LLMs sensitivity, our aim was twofold: to pinpoint the underlying source of this sensitivity and propose potential solutions to enhance the models' robustness. Our evaluations unequivocally reveal that LLMs not only exhibit pronounced sensitivity to options order, but also that this sensitivity diminishes only slightly when demonstrations are integrated into the few-shot setting under specific circumstances. While our primary focus has been on multiplechoice questions, we have also detected a parallel phenomenon-albeit with varying degrees of sensitivity-in other tasks involving multiple fragments (e.g. the options in MCQ) within inputs. This encompasses tasks like odd word detection, sorting lists of items, and ranking documents. While these observations have been noted, further exploration into these tasks is reserved for future efforts.</p>
<p>In seeking to uncover the root cause of order sensitivity, we conjecture that the issue arises from LLMs' positional bias, particularly manifesting in uncertain instances. We verify our conjecture by conducting diverse experiments that highlight impactful patterns that either magnify or mitigate this positional bias. Despite the validation provided through detailed experimentation, a deeper comprehension of the issue's origin necessitates a thorough exploration of the training data.</p>
<p>To improve the robustness of LLMs' sensitivity against options order, we consider two calibration techniques: majority vote and multiple evidence calibration (MEC). While both methods display promising outcomes, contributing to the improvement of model performance, they are not without their respective limitations. Majority voting is computationally expensive, while MEC diverges significantly from majority voting, casting doubts on its applicability to MCQ tasks. As a result, in order to establish a reliable and accurate evaluation framework for LLMs in the context of multiplechoice questions, it is imperative to develop more efficient calibration strategies. Moreover, refining the evaluation metrics holds the potential to improve LLMs' ability to withstand the challenges posed by options order sensitivity. These avenues present opportunities for in-depth exploration in future works.    Table 8: Sensitivity gap after applying the identified patterns to amplify and mitigate positional bias.</p>
<p>Figure 2 :
2Order sensitivity in few-shot setting: The error bars represent the range of minimum and maximum accuracy achievable in each task through oracle reordering. Our observations are as follows: (</p>
<p>Figure 3 :
3Correlation between the sensitivity gap and error rate for GPT-4 and InstructGPT across various multiple-choice question (MCQ) tasks.</p>
<p>Table 2 :
2Assessing sorting options quality with LLMs and analyzing the impact of reducing options complexity on models performance.</p>
<p>Table 3 :
3Optimal patterns and their best order instantiation for amplifying and mitigating positional bias in different LLMs based on available number of options in multiple-choice questions.</p>
<p>Table 4
4presents the percentage of initial sensitivity gap covered (initial sensitivity gaps are fromTasks </p>
<p>GPT-4 
InstructGPT </p>
<p>Amplifying-Bias Mitigating-Bias Amplifying-Bias Mitigating-Bias </p>
<p>CSQA 
62.9 
22.7 
71.7 
38.3 
Logical Deduction 
42.0 
10.1 
61.7 
0.9 
Abstract Algebra 
52.8 
15.1 
35.7 
25.7 
High School Chemistry 
21.5 
22.9 
25.7 
25.7 
Professional Law 
31.5 
9.7 
20.1 
25.9 </p>
<p>Table 5 :
5Impact of calibration methods on LLMs' performance.</p>
<p>Vanila Min Max Vanila Min Max Vanila Min MaxTasks </p>
<p>1-shot 
2-shot 
5-shot </p>
<p>CSQA 
74.2 53.4 92.1 
74.7 60.7 91.6 
76.3 59.3 91.1 
Logical Deduction 
61.0 16.0 97.0 
72.0 17.7 97.3 
64.7 17.3 96.0 
Abstract Algebra 
36.0 
3.0 72.0 
38.0 
9.0 73.0 
34.0 
7.0 73.0 
High School Chemistry 
50.7 19.7 89.7 
52.2 21.6 84.2 
52.7 24.6 83.2 
Professional Law 
52.1 22.3 74.7 
53.3 29.3 75.3 
53.7 25.5 75.2 </p>
<p>Table 6 :
6Few-shot order sensitivity in InstructGPT.Vanila Min Max Vanila Min Max Vanila Min MaxTasks 
1-shot 
2-shot 
5-shot </p>
<p>CSQA 
87.2 79.1 94.3 
86.3 78.2 94.7 
86.7 77.2 93.3 
Logical Deduction 
92.3 84.3 97.3 
94.3 88.3 97.7 
96.3 89.0 99.3 
Abstract Algebra 
58.0 31.0 82.0 
60.0 32.0 79.0 
58.0 34.0 78.0 
High School Chemistry 
72.9 49.2 91.1 
67.1 53.7 90.6 
68.5 47.7 91.6 
Professional Law 
71.2 57.7 80.7 
71.7 59.7 81.3 
70.6 58.4 83.6 </p>
<p>Table 7 :
7Few-shot order sensitivity in GPT-4.Amplifying-Bias Mitigating-Bias Amplifying-Bias Mitigating-BiasTasks 
GPT-4 
InstructGPT </p>
<p>Min 
Max 
Min 
Max 
Min 
Max Min 
Max </p>
<p>CSQA 
-8.0 
+6.4 
-4.8 
+0.4 -16.0 
+14.9 
-7.7 
+8.8 
Logical Deduction 
-3.1 
+2.4 
+1.3 
+1.3 -28.4 
+17.3 +0.7 
+0.7 
Abstract Algebra 
-19.0 
+9.0 
-7.0 
+1.0 -17.0 
+8.0 
-9.0 
+9.0 
High School Chemistry 
-7.0 
+2.0 -11.6 
-2.0 -11.6 
+5.5 
-9.3 
+7.8 
Professional Law 
-3.8 
+4.0 
+3.2 
+5.6 
-6.4 
+3.7 
-7.6 
+5.5 </p>
<p>AcknowledgementsWe would like to thank Sameer Singh for his valuable comments.A Detailed ResultsThe detailed result of order sensitivity in few-shot setting is provided inTables 6 and 7for Instruct-GPT and GPT-4 respectively. Moreover, we present the impact of the identified patterns aimed at amplifying and mitigating positional bias on order sensitivity inTable 8.
. Rohan Anil, M Andrew, Orhan Dai, Melvin Firat, Dmitry Johnson, Alexandre Lepikhin, Siamak Passos, Emanuel Shakeri, Paige Taropa, Zhifeng Bailey, Chen, arXiv:2305.10403arXiv preprintRohan Anil, Andrew M Dai, Orhan Firat, Melvin John- son, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won, Charles Chung, Sebastian Sutton, Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. arXiv preprintAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>From 'f'to 'a'on the ny regents science exams: An overview of the aristo project. Peter Clark, Oren Etzioni, Tushar Khot, Daniel Khashabi, Bhavana Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, Niket Tandon, 41AI MagazinePeter Clark, Oren Etzioni, Tushar Khot, Daniel Khashabi, Bhavana Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, Niket Tandon, et al. 2020. From 'f'to 'a'on the ny regents science exams: An overview of the aristo project. AI Magazine, 41(4):39-53.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, arXiv:2009.03300Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprintDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language under- standing. arXiv preprint arXiv:2009.03300.</p>
<p>Large language models are zero-shot rankers for recommender systems. Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian Mcauley, Wayne Xin Zhao, arXiv:2305.08845arXiv preprintYupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao. 2023. Large language models are zero-shot rankers for recommender systems. arXiv preprint arXiv:2305.08845.</p>
<p>Can large language models reason about medical questions?. Christoffer Egeberg Valentin Liévin, Ole Hother, Winther, arXiv:2207.08143arXiv preprintValentin Liévin, Christoffer Egeberg Hother, and Ole Winther. 2022. Can large language models rea- son about medical questions? arXiv preprint arXiv:2207.08143.</p>
<p>arXiv:2303.08774OpenAI. 2023. Gpt-4 technical report. arXiv preprintOpenAI. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 35Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instruc- tions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.</p>
<p>Fábio Perez, Ian Ribeiro, arXiv:2211.09527Ignore previous prompt: Attack techniques for language models. arXiv preprintFábio Perez and Ian Ribeiro. 2022. Ignore previous prompt: Attack techniques for language models. arXiv preprint arXiv:2211.09527.</p>
<p>Nils Reimers, Iryna Gurevych, arXiv:1908.10084Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprintNils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084.</p>
<p>Mctest: A challenge dataset for the open-domain machine comprehension of text. Matthew Richardson, J C Christopher, Erin Burges, Renshaw, Proceedings of the 2013 conference on empirical methods in natural language processing. the 2013 conference on empirical methods in natural language processingMatthew Richardson, Christopher JC Burges, and Erin Renshaw. 2013. Mctest: A challenge dataset for the open-domain machine comprehension of text. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 193- 203.</p>
<p>Leveraging large language models for multiple choice question answering. Joshua Robinson, Christopher Michael Rytting, David Wingate, arXiv:2210.12353arXiv preprintJoshua Robinson, Christopher Michael Rytting, and David Wingate. 2022. Leveraging large language models for multiple choice question answering. arXiv preprint arXiv:2210.12353.</p>
<p>Large language models (gpt) struggle to answer multiple-choice questions about code. Jaromir Savelka, Arav Agarwal, Christopher Bogart, Majd Sakr, arXiv:2303.08033arXiv preprintJaromir Savelka, Arav Agarwal, Christopher Bogart, and Majd Sakr. 2023. Large language models (gpt) struggle to answer multiple-choice questions about code. arXiv preprint arXiv:2303.08033.</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, R Adam, Adam Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.04615arXiv preprintAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.</p>
<p>Asa Cooper Stickland, Iain Murray, arXiv:2007.04206Diverse ensembles improve calibration. arXiv preprintAsa Cooper Stickland and Iain Murray. 2020. Di- verse ensembles improve calibration. arXiv preprint arXiv:2007.04206.</p>
<p>Commonsenseqa: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, arXiv:1811.00937arXiv preprintAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018. Commonsenseqa: A question answering challenge targeting commonsense knowl- edge. arXiv preprint arXiv:1811.00937.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Naman Baptiste Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprintHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and effi- cient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Jiongxiao Wang, Zichen Liu, Hee Keun, Muhao Park, Chaowei Chen, Xiao, arXiv:2305.14950Adversarial demonstration attacks on large language models. arXiv preprintJiongxiao Wang, Zichen Liu, Keun Hee Park, Muhao Chen, and Chaowei Xiao. 2023a. Adversarial demon- stration attacks on large language models. arXiv preprint arXiv:2305.14950.</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, arXiv:2305.17926Tianyu Liu, and Zhifang Sui. 2023b. Large language models are not fair evaluators. arXiv preprintPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023b. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926.</p>
<p>Alexander Wei, arXiv:2307.02483Nika Haghtalab, and Jacob Steinhardt. 2023. Jailbroken: How does llm safety training fail. arXiv preprintAlexander Wei, Nika Haghtalab, and Jacob Steinhardt. 2023. Jailbroken: How does llm safety training fail? arXiv preprint arXiv:2307.02483.</p>
<p>Fundamental limitations of alignment in large language models. Yotam Wolf, Noam Wies, Yoav Levine, Amnon Shashua, arXiv:2304.11082arXiv preprintYotam Wolf, Noam Wies, Yoav Levine, and Amnon Shashua. 2023. Fundamental limitations of align- ment in large language models. arXiv preprint arXiv:2304.11082.</p>
<p>Calibrate before use: Improving few-shot performance of language models. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh, PMLRInternational Conference on Machine Learning. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improv- ing few-shot performance of language models. In In- ternational Conference on Machine Learning, pages 12697-12706. PMLR.</p>
<p>Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, arXiv:2306.04528Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. arXiv preprintKaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, et al. 2023. Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. arXiv preprint arXiv:2306.04528.</p>
<p>Andy Zou, Zifan Wang, Zico Kolter, arXiv:2307.15043and Matt Fredrikson. 2023. Universal and transferable adversarial attacks on aligned language models. arXiv preprintAndy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrik- son. 2023. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043.</p>            </div>
        </div>

    </div>
</body>
</html>
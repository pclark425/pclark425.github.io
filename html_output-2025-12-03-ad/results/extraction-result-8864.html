<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8864 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8864</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8864</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-264436468</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.15164v2.pdf" target="_blank">LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers</a></p>
                <p><strong>Paper Abstract:</strong> Logical reasoning, i.e., deductively inferring the truth value of a conclusion from a set of premises, is an important task for artificial intelligence with wide potential impacts on science, mathematics, and society. While many prompting-based strategies have been proposed to enable Large Language Models (LLMs) to do such reasoning more effectively, they still appear unsatisfactory, often failing in subtle and unpredictable ways. In this work, we investigate the validity of instead reformulating such tasks as modular neurosymbolic programming, which we call LINC: Logical Inference via Neurosymbolic Computation. In LINC, the LLM acts as a semantic parser, translating premises and conclusions from natural language to expressions in first-order logic. These expressions are then offloaded to an external theorem prover, which symbolically performs deductive inference. Leveraging this approach, we observe significant performance gains on FOLIO and a balanced subset of ProofWriter for three different models in nearly all experimental conditions we evaluate. On ProofWriter, augmenting the comparatively small open-source StarCoder+ (15.5B parameters) with LINC even outperforms GPT-3.5 and GPT-4 with Chain-of-Thought (CoT) prompting by an absolute 38% and 10%, respectively. When used with GPT-4, LINC scores 26% higher than CoT on ProofWriter while performing comparatively on FOLIO. Further analysis reveals that although both methods on average succeed roughly equally often on this dataset, they exhibit distinct and complementary failure modes. We thus provide promising evidence for how logical reasoning over natural language can be tackled through jointly leveraging LLMs alongside symbolic provers. All corresponding code is publicly available at https://github.com/benlipkin/linc</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8864.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8864.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StarCoder+ on FOLIO (LINC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>StarCoder+ (15.5B) evaluated on FOLIO using LINC</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-access code+NL model (StarCoder+) was used as the semantic parser in LINC to translate natural-language premises to first-order logic (FOL) which are then proved by Prover9; evaluated on the FOLIO validation set (182 cleaned examples).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>StarCoder+</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-access model (StarCoder+) derived from StarCoderBase with a GPT-2-like architecture, multi-query attention and finetuned on ~600B tokens of mixed web/code/natural language (includes RefinedWeb, The Stack, Wikipedia); used locally with bf16 precision.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>15.5B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>FOLIO: an expert-written, open-domain natural-language reasoning dataset requiring mapping NL premises to first-order logic and deciding whether a conclusion is True/False/Uncertain under FOL semantics; authors used the cleaned validation set (182 examples).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>LINC neurosymbolic pipeline: LLM as semantic parser (NL → FOL) + external FOL theorem prover (Prover9) + K-way majority voting (K=10). Baselines evaluated in the paper: Naïve (direct label), Scratchpad (LLM generates FOL then LLM decides), Chain-of-Thought (CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>With LINC and 10-way majority voting, StarCoder+ achieved mean accuracy 56.0% on FOLIO (K=10 votes).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>LINC improved StarCoder+'s accuracy by 14.2 percentage points over the best controlled baseline (Chain-of-Thought at 41.8%); Scratchpad and Naïve did not outperform a majority 'Uncertain' prediction for this model on FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>High rate of FOL syntax/compilation errors in model generations (38% of StarCoder+ generations contained syntax errors); failure modes include (L1) not encoding implicit commonsense facts (e.g., that Harry is a person), (L2) lossy representations that conflate predicates (losing granular facts), and (L3) syntax/arities mistakes. LINC tends to predict 'Uncertain' frequently, trading recall for precision (see insights).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>LINC substantially helps a smaller/open model like StarCoder+ on a naturalistic FOL task versus end-to-end prompting; however, the main bottleneck is faithful NL→FOL semantic parsing (syntax errors and representation choices). Majority voting (K=10) meaningfully improves LINC for weaker models by reducing the impact of syntax errors.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8864.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8864.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StarCoder+ on ProofWriter (LINC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>StarCoder+ (15.5B) evaluated on ProofWriter using LINC</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>StarCoder+ used as the NL→FOL semantic parser in LINC and evaluated on a balanced subset of ProofWriter (synthetic, OWA portion) to decide True/False/Uncertain via Prover9 and majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>StarCoder+</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same as above: an open-access 15.5B model finetuned on code+NL; run locally with large context window.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>15.5B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProofWriter (OWA balanced subset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>ProofWriter: synthetically generated natural-language reasoning dataset where conclusions and multi-step proofs are grounded in first-order style rules; authors used a balanced 360-sample subset (balanced by proof depth and labels) under the Open-World Assumption.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>LINC: NL→FOL translations by StarCoder+ followed by Prover9 deduction and K=10 majority voting. Baselines: Naïve, Scratchpad, Chain-of-Thought.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>The paper reports that StarCoder+ augmented with LINC outperforms GPT-3.5 and GPT-4 with Chain-of-Thought on ProofWriter by absolute margins of 38% and 10% respectively (authors highlight dramatic gains for the small open model in this synthetic domain). Exact StarCoder+ LINC accuracy value is reported via figures in the paper (comparison statement given in text).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Scratchpad did not improve over Naïve; LINC provides significant gains across all models on ProofWriter. GPT-3.5+LINC and GPT-4+LINC achieved 96.4% and 98.3% respectively; StarCoder+LINC was called out as outperforming GPT-3.5/ GPT-4 with CoT by 38%/10% respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>ProofWriter is synthetic with short sentences making autoformalization easier; as a result LINC's gains are largest here. Remaining errors for StarCoder+ on ProofWriter were mainly syntax or improper representations rather than lossy translation.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>LINC is particularly effective on synthetic, regularized NL inputs (like ProofWriter) where NL→FOL translation is easy; even a much smaller model can leverage a symbolic prover to surpass much larger models that only use CoT prompting.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8864.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8864.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 on FOLIO (LINC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-16k) evaluated on FOLIO using LINC</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 was used as the semantic parser in the LINC pipeline and compared to Naïve, Scratchpad, and CoT on FOLIO (cleaned validation set); translations passed to Prover9 and aggregated with 10-way majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-16k-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI conversational model checkpoint (gpt-3.5-turbo-16k-0613) used via API with temperature 0.8; exact parameter count not published by authors (paper notes predecessor GPT-3 had 175B parameters but does not assert GPT-3.5 size).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same FOLIO dataset description as above: NL→FOL mapping and truth-evaluation under first-order logic semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>LINC (NL→FOL via GPT-3.5 + Prover9 + K=10 majority voting); baselines Naïve, Scratchpad, Chain-of-Thought were used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On FOLIO, GPT-3.5 with LINC achieved mean accuracy 62.6% (10-way majority vote).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>This was ~7.7 percentage points higher than the closest baseline (CoT at 54.9% average accuracy) for GPT-3.5—i.e., LINC improved performance relative to in-context reasoning baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>GPT-3.5 generated FOL with syntax errors at a reported rate of ~24% across generations; failure modes mirror LINC's general issues (implicit info not encoded, representation choices losing facts). Depth-generalization: baselines drop toward chance with increasing proof depth, while GPT-3.5+LINC remained near-perfect across depths, indicating LINC scales better to longer deductive chains.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>For mid-size/closed models like GPT-3.5, LINC yields substantial gains, enabling near-perfect performance on synthetic ProofWriter and improved generalization across proof depth, suggesting proving is easier when NL→FOL formalization is reliable.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8864.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8864.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 on ProofWriter (LINC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-16k) evaluated on ProofWriter using LINC</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 acted as the NL→FOL translator within LINC and was evaluated on the balanced ProofWriter subset; proofs resolved by Prover9 and majority voting used.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-16k-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same as above; used with token limits and temperature 0.8, invoked via the OpenAI API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProofWriter (OWA balanced subset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Synthetic dataset focusing on deductive reasoning under first-order style rules; balanced sample across proof depths and labels used for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>LINC (GPT-3.5 NL→FOL + Prover9 + K=10 majority voting).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GPT-3.5 with LINC achieved mean accuracy 96.4% on the balanced ProofWriter subset (10-way majority vote).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Substantially outperformed in-context baselines (Naïve, Scratchpad, CoT), which degraded with increasing proof depth; LINC enabled scalability to longer deductive chains for GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Remaining errors in ProofWriter for GPT-3.5 were rare and typically due to NL→FOL representation mistakes or syntax; however, LINC handled irrelevant premises and long chains better than baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>LINC lets an in-context GPT-3.5 model reach near-ceiling performance on syntactic, regular NL reasoning tasks by delegating deduction to a symbolic solver, confirming the value of neurosymbolic decomposition for scoped logical reasoning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8864.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8864.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 on FOLIO (LINC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (gpt-4-0613) evaluated on FOLIO using LINC</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 was used as the NL→FOL semantic parser in LINC and compared directly to Chain-of-Thought and other baselines on the FOLIO validation examples; outputs fed to Prover9 and aggregated via majority vote.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-4 API checkpoint (gpt-4-0613) invoked with temperature 0.8 and token limits; exact parameterization not publicly specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Expert-written NL→FOL mapping dataset requiring nuanced semantic parsing and first-order deductions; used cleaned validation subset (182 examples).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>LINC (GPT-4 NL→FOL + Prover9 + 10-way majority voting) compared against Chain-of-Thought (CoT), Scratchpad, Naïve.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GPT-4 with LINC achieved mean accuracy 72.5% on FOLIO (10-way majority vote); Chain-of-Thought achieved 75.3% on the same dataset. The LINC vs. CoT difference for GPT-4 was not statistically significant (McNemar p = 0.58).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>On FOLIO, GPT-4 CoT slightly outperformed GPT-4 LINC (75.3% vs. 72.5%) but difference not significant. In contrast, on ProofWriter GPT-4+LINC reached 98.3% (see separate entry). Scratchpad did not improve over Naïve in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>GPT-4 generated FOL with syntax errors at ~13% rate; LINC failure modes include not encoding implicit premises or losing information via representation choices. Compared to CoT, LINC had higher precision (93% vs. CoT 81%) but lower recall (60% vs. CoT 75%) on True/False predictions, and predicted 'Uncertain' more often (57% vs. CoT 41%), indicating conservative behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>For an already-strong model like GPT-4, LINC provides complementary behavior to CoT: LINC reduces hallucination/overconfident incorrect True/False predictions (higher precision) but at the cost of more 'Uncertain' responses (lower recall). LINC excels at scaling to longer deductive chains (maintained near-ceiling performance across proof depths), suggesting symbolic proving is robust once NL→FOL formalization is accurate.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8864.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8864.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 on ProofWriter (LINC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (gpt-4-0613) evaluated on ProofWriter using LINC</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 used as the NL→FOL translator in LINC, with Prover9 performing the deduction and K=10 majority voting producing final labels; evaluated on the balanced ProofWriter subset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same as above; used with large token limits and 0.8 temperature via API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ProofWriter (OWA balanced subset)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Synthetic dataset requiring multi-step deductive reasoning over compact, synthetically generated natural-language rules and facts; authors used a 360-sample balanced subset over proof depth and labels.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>LINC (GPT-4 NL→FOL + Prover9 + 10-way majority voting).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GPT-4 with LINC achieved mean accuracy 98.3% on the balanced ProofWriter subset (10-way majority vote).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>This substantially outperforms in-context baselines on ProofWriter and shows that LINC yields near-ceiling performance for a strong model on synthetic first-order tasks. Compared to GPT-4+CoT, LINC provided higher accuracy on ProofWriter (absolute gains reported in figures).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Even for GPT-4, LINC's errors on FOLIO stemmed primarily from NL→FOL translation mistakes (implicit facts not encoded or representation choices losing information); on ProofWriter these issues were rarer due to synthetic regularity.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>LINC plus a strong LLM (GPT-4) achieves near-perfect performance on synthetic FOL tasks and maintains performance across increasing proof depths, supporting the value of delegating deduction to a symbolic prover while using LLMs for autoformalization; however, the quality of autoformalization is the critical limiting factor in naturalistic settings.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Folio: Natural language reasoning with firstorder logic <em>(Rating: 2)</em></li>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning <em>(Rating: 2)</em></li>
                <li>SATLM <em>(Rating: 2)</em></li>
                <li>Show your work: Scratchpads for intermediate computation with language models <em>(Rating: 2)</em></li>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Program-aided language models (PAL) <em>(Rating: 1)</em></li>
                <li>Synchromesh: Reliable code generation from pre-trained language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8864",
    "paper_id": "paper-264436468",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "StarCoder+ on FOLIO (LINC)",
            "name_full": "StarCoder+ (15.5B) evaluated on FOLIO using LINC",
            "brief_description": "Open-access code+NL model (StarCoder+) was used as the semantic parser in LINC to translate natural-language premises to first-order logic (FOL) which are then proved by Prover9; evaluated on the FOLIO validation set (182 cleaned examples).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "StarCoder+",
            "model_description": "Open-access model (StarCoder+) derived from StarCoderBase with a GPT-2-like architecture, multi-query attention and finetuned on ~600B tokens of mixed web/code/natural language (includes RefinedWeb, The Stack, Wikipedia); used locally with bf16 precision.",
            "model_size": "15.5B",
            "reasoning_task_name": "FOLIO",
            "reasoning_task_description": "FOLIO: an expert-written, open-domain natural-language reasoning dataset requiring mapping NL premises to first-order logic and deciding whether a conclusion is True/False/Uncertain under FOL semantics; authors used the cleaned validation set (182 examples).",
            "method_or_approach": "LINC neurosymbolic pipeline: LLM as semantic parser (NL → FOL) + external FOL theorem prover (Prover9) + K-way majority voting (K=10). Baselines evaluated in the paper: Naïve (direct label), Scratchpad (LLM generates FOL then LLM decides), Chain-of-Thought (CoT).",
            "performance": "With LINC and 10-way majority voting, StarCoder+ achieved mean accuracy 56.0% on FOLIO (K=10 votes).",
            "baseline_comparison": "LINC improved StarCoder+'s accuracy by 14.2 percentage points over the best controlled baseline (Chain-of-Thought at 41.8%); Scratchpad and Naïve did not outperform a majority 'Uncertain' prediction for this model on FOLIO.",
            "limitations_or_failures": "High rate of FOL syntax/compilation errors in model generations (38% of StarCoder+ generations contained syntax errors); failure modes include (L1) not encoding implicit commonsense facts (e.g., that Harry is a person), (L2) lossy representations that conflate predicates (losing granular facts), and (L3) syntax/arities mistakes. LINC tends to predict 'Uncertain' frequently, trading recall for precision (see insights).",
            "insights_or_conclusions": "LINC substantially helps a smaller/open model like StarCoder+ on a naturalistic FOL task versus end-to-end prompting; however, the main bottleneck is faithful NL→FOL semantic parsing (syntax errors and representation choices). Majority voting (K=10) meaningfully improves LINC for weaker models by reducing the impact of syntax errors.",
            "uuid": "e8864.0"
        },
        {
            "name_short": "StarCoder+ on ProofWriter (LINC)",
            "name_full": "StarCoder+ (15.5B) evaluated on ProofWriter using LINC",
            "brief_description": "StarCoder+ used as the NL→FOL semantic parser in LINC and evaluated on a balanced subset of ProofWriter (synthetic, OWA portion) to decide True/False/Uncertain via Prover9 and majority voting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "StarCoder+",
            "model_description": "Same as above: an open-access 15.5B model finetuned on code+NL; run locally with large context window.",
            "model_size": "15.5B",
            "reasoning_task_name": "ProofWriter (OWA balanced subset)",
            "reasoning_task_description": "ProofWriter: synthetically generated natural-language reasoning dataset where conclusions and multi-step proofs are grounded in first-order style rules; authors used a balanced 360-sample subset (balanced by proof depth and labels) under the Open-World Assumption.",
            "method_or_approach": "LINC: NL→FOL translations by StarCoder+ followed by Prover9 deduction and K=10 majority voting. Baselines: Naïve, Scratchpad, Chain-of-Thought.",
            "performance": "The paper reports that StarCoder+ augmented with LINC outperforms GPT-3.5 and GPT-4 with Chain-of-Thought on ProofWriter by absolute margins of 38% and 10% respectively (authors highlight dramatic gains for the small open model in this synthetic domain). Exact StarCoder+ LINC accuracy value is reported via figures in the paper (comparison statement given in text).",
            "baseline_comparison": "Scratchpad did not improve over Naïve; LINC provides significant gains across all models on ProofWriter. GPT-3.5+LINC and GPT-4+LINC achieved 96.4% and 98.3% respectively; StarCoder+LINC was called out as outperforming GPT-3.5/ GPT-4 with CoT by 38%/10% respectively.",
            "limitations_or_failures": "ProofWriter is synthetic with short sentences making autoformalization easier; as a result LINC's gains are largest here. Remaining errors for StarCoder+ on ProofWriter were mainly syntax or improper representations rather than lossy translation.",
            "insights_or_conclusions": "LINC is particularly effective on synthetic, regularized NL inputs (like ProofWriter) where NL→FOL translation is easy; even a much smaller model can leverage a symbolic prover to surpass much larger models that only use CoT prompting.",
            "uuid": "e8864.1"
        },
        {
            "name_short": "GPT-3.5 on FOLIO (LINC)",
            "name_full": "GPT-3.5 (gpt-3.5-turbo-16k) evaluated on FOLIO using LINC",
            "brief_description": "GPT-3.5 was used as the semantic parser in the LINC pipeline and compared to Naïve, Scratchpad, and CoT on FOLIO (cleaned validation set); translations passed to Prover9 and aggregated with 10-way majority voting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo-16k-0613)",
            "model_description": "OpenAI conversational model checkpoint (gpt-3.5-turbo-16k-0613) used via API with temperature 0.8; exact parameter count not published by authors (paper notes predecessor GPT-3 had 175B parameters but does not assert GPT-3.5 size).",
            "model_size": null,
            "reasoning_task_name": "FOLIO",
            "reasoning_task_description": "Same FOLIO dataset description as above: NL→FOL mapping and truth-evaluation under first-order logic semantics.",
            "method_or_approach": "LINC (NL→FOL via GPT-3.5 + Prover9 + K=10 majority voting); baselines Naïve, Scratchpad, Chain-of-Thought were used for comparison.",
            "performance": "On FOLIO, GPT-3.5 with LINC achieved mean accuracy 62.6% (10-way majority vote).",
            "baseline_comparison": "This was ~7.7 percentage points higher than the closest baseline (CoT at 54.9% average accuracy) for GPT-3.5—i.e., LINC improved performance relative to in-context reasoning baselines.",
            "limitations_or_failures": "GPT-3.5 generated FOL with syntax errors at a reported rate of ~24% across generations; failure modes mirror LINC's general issues (implicit info not encoded, representation choices losing facts). Depth-generalization: baselines drop toward chance with increasing proof depth, while GPT-3.5+LINC remained near-perfect across depths, indicating LINC scales better to longer deductive chains.",
            "insights_or_conclusions": "For mid-size/closed models like GPT-3.5, LINC yields substantial gains, enabling near-perfect performance on synthetic ProofWriter and improved generalization across proof depth, suggesting proving is easier when NL→FOL formalization is reliable.",
            "uuid": "e8864.2"
        },
        {
            "name_short": "GPT-3.5 on ProofWriter (LINC)",
            "name_full": "GPT-3.5 (gpt-3.5-turbo-16k) evaluated on ProofWriter using LINC",
            "brief_description": "GPT-3.5 acted as the NL→FOL translator within LINC and was evaluated on the balanced ProofWriter subset; proofs resolved by Prover9 and majority voting used.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo-16k-0613)",
            "model_description": "Same as above; used with token limits and temperature 0.8, invoked via the OpenAI API.",
            "model_size": null,
            "reasoning_task_name": "ProofWriter (OWA balanced subset)",
            "reasoning_task_description": "Synthetic dataset focusing on deductive reasoning under first-order style rules; balanced sample across proof depths and labels used for evaluation.",
            "method_or_approach": "LINC (GPT-3.5 NL→FOL + Prover9 + K=10 majority voting).",
            "performance": "GPT-3.5 with LINC achieved mean accuracy 96.4% on the balanced ProofWriter subset (10-way majority vote).",
            "baseline_comparison": "Substantially outperformed in-context baselines (Naïve, Scratchpad, CoT), which degraded with increasing proof depth; LINC enabled scalability to longer deductive chains for GPT-3.5.",
            "limitations_or_failures": "Remaining errors in ProofWriter for GPT-3.5 were rare and typically due to NL→FOL representation mistakes or syntax; however, LINC handled irrelevant premises and long chains better than baselines.",
            "insights_or_conclusions": "LINC lets an in-context GPT-3.5 model reach near-ceiling performance on syntactic, regular NL reasoning tasks by delegating deduction to a symbolic solver, confirming the value of neurosymbolic decomposition for scoped logical reasoning.",
            "uuid": "e8864.3"
        },
        {
            "name_short": "GPT-4 on FOLIO (LINC)",
            "name_full": "GPT-4 (gpt-4-0613) evaluated on FOLIO using LINC",
            "brief_description": "GPT-4 was used as the NL→FOL semantic parser in LINC and compared directly to Chain-of-Thought and other baselines on the FOLIO validation examples; outputs fed to Prover9 and aggregated via majority vote.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-0613)",
            "model_description": "OpenAI's GPT-4 API checkpoint (gpt-4-0613) invoked with temperature 0.8 and token limits; exact parameterization not publicly specified in the paper.",
            "model_size": null,
            "reasoning_task_name": "FOLIO",
            "reasoning_task_description": "Expert-written NL→FOL mapping dataset requiring nuanced semantic parsing and first-order deductions; used cleaned validation subset (182 examples).",
            "method_or_approach": "LINC (GPT-4 NL→FOL + Prover9 + 10-way majority voting) compared against Chain-of-Thought (CoT), Scratchpad, Naïve.",
            "performance": "GPT-4 with LINC achieved mean accuracy 72.5% on FOLIO (10-way majority vote); Chain-of-Thought achieved 75.3% on the same dataset. The LINC vs. CoT difference for GPT-4 was not statistically significant (McNemar p = 0.58).",
            "baseline_comparison": "On FOLIO, GPT-4 CoT slightly outperformed GPT-4 LINC (75.3% vs. 72.5%) but difference not significant. In contrast, on ProofWriter GPT-4+LINC reached 98.3% (see separate entry). Scratchpad did not improve over Naïve in many settings.",
            "limitations_or_failures": "GPT-4 generated FOL with syntax errors at ~13% rate; LINC failure modes include not encoding implicit premises or losing information via representation choices. Compared to CoT, LINC had higher precision (93% vs. CoT 81%) but lower recall (60% vs. CoT 75%) on True/False predictions, and predicted 'Uncertain' more often (57% vs. CoT 41%), indicating conservative behavior.",
            "insights_or_conclusions": "For an already-strong model like GPT-4, LINC provides complementary behavior to CoT: LINC reduces hallucination/overconfident incorrect True/False predictions (higher precision) but at the cost of more 'Uncertain' responses (lower recall). LINC excels at scaling to longer deductive chains (maintained near-ceiling performance across proof depths), suggesting symbolic proving is robust once NL→FOL formalization is accurate.",
            "uuid": "e8864.4"
        },
        {
            "name_short": "GPT-4 on ProofWriter (LINC)",
            "name_full": "GPT-4 (gpt-4-0613) evaluated on ProofWriter using LINC",
            "brief_description": "GPT-4 used as the NL→FOL translator in LINC, with Prover9 performing the deduction and K=10 majority voting producing final labels; evaluated on the balanced ProofWriter subset.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-0613)",
            "model_description": "Same as above; used with large token limits and 0.8 temperature via API.",
            "model_size": null,
            "reasoning_task_name": "ProofWriter (OWA balanced subset)",
            "reasoning_task_description": "Synthetic dataset requiring multi-step deductive reasoning over compact, synthetically generated natural-language rules and facts; authors used a 360-sample balanced subset over proof depth and labels.",
            "method_or_approach": "LINC (GPT-4 NL→FOL + Prover9 + 10-way majority voting).",
            "performance": "GPT-4 with LINC achieved mean accuracy 98.3% on the balanced ProofWriter subset (10-way majority vote).",
            "baseline_comparison": "This substantially outperforms in-context baselines on ProofWriter and shows that LINC yields near-ceiling performance for a strong model on synthetic first-order tasks. Compared to GPT-4+CoT, LINC provided higher accuracy on ProofWriter (absolute gains reported in figures).",
            "limitations_or_failures": "Even for GPT-4, LINC's errors on FOLIO stemmed primarily from NL→FOL translation mistakes (implicit facts not encoded or representation choices losing information); on ProofWriter these issues were rarer due to synthetic regularity.",
            "insights_or_conclusions": "LINC plus a strong LLM (GPT-4) achieves near-perfect performance on synthetic FOL tasks and maintains performance across increasing proof depths, supporting the value of delegating deduction to a symbolic prover while using LLMs for autoformalization; however, the quality of autoformalization is the critical limiting factor in naturalistic settings.",
            "uuid": "e8864.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Folio: Natural language reasoning with firstorder logic",
            "rating": 2
        },
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2
        },
        {
            "paper_title": "Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "rating": 2
        },
        {
            "paper_title": "SATLM",
            "rating": 2
        },
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "Program-aided language models (PAL)",
            "rating": 1
        },
        {
            "paper_title": "Synchromesh: Reliable code generation from pre-trained language models",
            "rating": 1
        }
    ],
    "cost": 0.019236749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers
14 Feb 2024</p>
<p>Theo X Olausson theoxo@mit.edu 
Alex Gu 
Benjamin Lipkin lipkinb@mit.edu 
Cedegao E Zhang cedzhang@mit.edu 
Armando Solar-Lezama 
Joshua B Tenenbaum 
Roger Levy 
LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers
14 Feb 2024CCAAAAF114FE96D63CB4319A00C8914EarXiv:2310.15164v2[cs.CL]
Logical reasoning, i.e., deductively inferring the truth value of a conclusion from a set of premises, is an important task for artificial intelligence with wide potential impacts on science, mathematics, and society.While many prompting-based strategies have been proposed to enable Large Language Models (LLMs) to do such reasoning more effectively, they still * Author order randomized; all reserve the right to list their name first.</p>
<p>appear unsatisfactory, often failing in subtle and unpredictable ways.In this work, we investigate the validity of instead reformulating such tasks as modular neurosymbolic programming, which we call LINC: Logical Inference via Neurosymbolic Computation.In LINC, the LLM acts as a semantic parser, translating premises and conclusions from natural language to expressions in first-order logic.These expressions are then offloaded to an external theorem prover, which symbolically performs deductive inference.Leveraging this approach, we observe significant performance gains on FOLIO and a balanced subset of ProofWriter for three different models in nearly all experimental conditions we evaluate.On ProofWriter, augmenting the comparatively small open-source StarCoder+ (15.5B parameters) with LINC even outperforms GPT-3.5 and GPT-4 with Chain-of-Thought (CoT) prompting by an absolute 38% and 10%, respectively.When used with GPT-4, LINC scores 26% higher than CoT on ProofWriter while performing comparatively on FOLIO.Further analysis reveals that although both methods on average succeed roughly equally often on this dataset, they exhibit distinct and complementary failure modes.We thus provide promising evidence for how logical reasoning over natural language can be tackled through jointly leveraging LLMs alongside symbolic provers.All corresponding code is publicly available. 1</p>
<p>Introduction</p>
<p>Widespread adoption of large language models (LLMs) such as GPT-3 (Brown et al., 2020), GPT-4 (OpenAI, 2023), and PaLM (Chowdhery et al., 2022) have led to a series of remarkable successes in tasks ranging from text summarization to program synthesis.Some of these successes have encouraged the hypothesis that such models are able to flexibly and systematically reason (Huang and Chang, 2022), especially when using prompting strategies that explicitly encourage verbalizing intermediate reasoning steps before generating the final answer (Nye et al., 2021;Wei et al., 2022;Kojima et al., 2022;Wang et al., 2023b).However, this reasoning ability appears to be unreliable for tasks that require reasoning out of domain (Liang et al., 2022;Saparov et al., 2023), understanding negation (Anil et al., 2022), and following long reasoning chains (Dziri et al., 2023).Furthermore, while the standard approach of "scaling up" seems to improve performance across some reasoning domains, other domains, e.g., reasoning involving use of Modus Tollens, show no such improvements (McKenzie et al., 2022).These findings suggest that such models may be relying on approximate heuristics based on surface-level statistical patterns in reasoning tasks, rather than consistent, generalizable representations and strategies (Srivastava et al., 2023;Creswell et al., 2023).</p>
<p>At the same time, the ability to accurately and soundly perform logical reasoning is important for AI and NLP due to its impact on downstream tasks.For example: retrieval-augmented chatbots may become more truthful if it can be verified that their answers logically follow from the retrieved facts; data-driven models capable of logical reasoning may speed up progress across mathematics and the sciences through automated theorem proving and knowledge discovery; and AI tutoring systems which ensure internal logical consistency might Step 1, the LLM semantic parser samples logic formulas expressing estimates of the semantics.It is possible that some of these might contain errors, e.g., the second example shows a syntax error involving an extra parenthesis, whereas the fourth example highlights a semantic error caused by mismatched predicates.In Step 2, these are then each offloaded to an automated theorem prover, filtering out syntax errors, and producing labels for the remaining samples.In Step 3, the remaining candidate outputs are passed through a majority-vote sieve to arrive at the best estimate for a single output label.make for better educational platforms, teaching students to think more clearly and rigorously.The question of how to enable state-of-the-art LLMs to become more reliable logical reasoners is thus one of great importance, with far-reaching implications.</p>
<p>In this work, we analyze LINC: Logical Inference via Neurosymbolic Computation (Fig. 1).In LINC, logical reasoning is tackled through a modular, two-step neurosymbolic process.First, the language model converts the natural language premises and desired conclusion into first-order logic (FOL) expressions (Enderton, 2001;Barker-Plummer et al., 2011).Second, a symbolic FOL theorem prover algorithmically determines the truth value of the conclusion given the formalized premises.In practice, we also incorporate a third majority voting step, which is shown to improve performance.LINC is a natural extension of recent work augmenting lanugage models with symbolic tools such as calculators or interpreters (Schick et al., 2023).</p>
<p>LINC has a key advantage: the language model itself no longer needs to perform any deductive reasoning, which is offloaded to the theorem prover.However, there are also clear drawbacks: the formalization from natural language to first-order logic must perfectly capture all relevant information contained in the premises, and any loss of information in the formalization procedure may lead the solver astray, leading to an incorrect conclusion.As it is not clear whether the task of formalization is more or less difficult than that of end-to-end natural language reasoning, our core interest in this work is to compare and contrast our neurosymbolic approach to existing reasoning strategies like Chain-of-Thought.Our contributions are thus three-fold:</p>
<p>• First, we propose LINC, a two-stage neurosymbolic approach for logical reasoning tasks (Sec.2).</p>
<p>• Second, we compare LINC to three baseline LLM strategies (Fig. 2), across three models (StarCoder+, GPT-3.5, GPT-4) and two datasets (FOLIO and ProofWriter) (Sec.4).We find that LINC significantly improves performance over every baseline in all experimental conditions except for GPT-4 on FOLIO.</p>
<p>LINC: Logical Inference via Neurosymbolic Computation</p>
<p>Our neurosymbolic approach to end-to-end logical reasoning consists of two stages.In the first stage, the LLM acts as a semantic parser, translating NL statements into FOL expressions in our supported logic language.In the second stage, these expressions are parsed from the text generated by the LLM and then get passed to an automated theorem prover; we use Prover9, a high-performance prover widely used in the logic community (Mc-Cune, 2005-2010).The external solver then executes a symbolic deduction algorithm, which either returns a value from the set {True, False, Uncertain} or raises an exception due to improper FOL syntax (e.g., if the model fails to balance parantheses in the formulae).</p>
<p>At its core, the strength of this approach lies in the reformulation of the problem space.End-toend NL-based reasoning allows for operation over a highly flexible expression space, but leaves the LLM with the difficult task of performing explicit deductive inference over expressions in this space.Using LINC, we instead trade off the flexible expression space of NL for syntactically strict logic formulas, allowing us to leverage symbolic algorithms with provable guarantees that the deductive chains will be correct with respect to the semantics of the intermediate representation.Making effective use of this reformulation thus requires the logic expressions generated by the LLM to be 1) syntactically valid, such that they are accepted by the prover, and 2) semantically valid, such that their evaluation results in the correct conclusion.In our experiments, we mitigate these risks by using a Kway majority voting procedure, which is discussed further in Sec. 3.</p>
<p>The significance of this problem space reformulation can-beyond the numerical increases in performance observed across our experimentsperhaps best be seen through an in-depth comparison of how LINC and traditional end-to-end LLM reasoning approaches such as Chain-of-Thought (CoT) fail.To foreshadow our latter analysis, we find that compared to CoT, LINC has worse recall but better precision on True/False predictions.We discuss this further in Sec. 5 and highlight that this suggests that LINC, as well as neurosymbolic computation more generally, has the potential to reduce LLM overconfidence and hallucination.</p>
<p>Experiments</p>
<p>In this section, we present our experimental setup, the models we use, and the three baselines to which we compare LINC.</p>
<p>Datasets: Our experiments use tasks from two existing datasets: FOLIO (Han et al., 2022) and ProofWriter (Tafjord et al., 2021), both of which have been shown to be challenging for off-the-shelf LLMs (Han et al., 2022;Creswell et al., 2023).FO-LIO is an expert-written, open-domain, logically complex and diverse dataset for natural language reasoning with first-order logic.We use its validation set for our evaluation.However, of the 204 samples in the validation set, we discover that 22 have errors (details in Appendix C), leaving us with 182 examples for our evaluation.ProofWriter, meanwhile, is a synthetically generated dataset for logical reasoning over natural language.For our evaluation, we use the OWA (Open-World Assumption) portion of ProofWriter, since this setting best matches that of FOLIO.Since we are running a large number of experiments, we randomly select 360 data points to evaluate on in order to reduce costs.We sample these in such a way that the resulting data set is balanced across both the number of reasoning steps in the shortest ground truth proof (depth 0-5; 50 samples each) and across the three labels (True/False/Uncertain; 120 samples each; 20 each per depth).</p>
<p>In-context learning examples: We hand-pick eight diverse samples from the FOLIO training set to be used as few-shot in-context examples.Because ProofWriter does not come with ground truth FOL statements, we use these eight samples for both evaluations.Compared to FOLIO, questions in ProofWriter generally have more premises per question (in our validation sets: an average of 5. 3 in FOLIO vs. 18.8 in ProofWriter).Thus, our evaluation on FOLIO is an in-distribution task, whereas ProofWriter requires generalizing out-ofdistribution to reasoning over considerably larger sets of premises than are given in the prompt.</p>
<p>Majority voting: K-way majority voting, in which K samples are taken i.i.d.from the model and the mode is used as the final prediction, has previously been shown to improve the performance of prompting-based strategies in logical reasoning tasks (Wang et al., 2023b).We implement such a strategy in our work, with reported accuracies reflecting K=10-way majority voting, unless otherwise stated.In the case of ties between two la- bels, we arbitrarily select the first of the two to have been generated.We report the effect of K on performance across our conditions and briefly discuss trends in Appendix H. Models: We use three models pre-trained on both natural language and code: GPT-3.5 (Ouyang et al., 2022), GPT-42 (OpenAI, 2023), and Star-Coder+3 (Li et al., 2023) with a decoding temperature of T = 0.8 for all experiments.We defer model, hyperparameter, and hardware details to Appendix B. We opt for StarCoder+ for three reasons: firstly, unlike the other models we consider, it is a free, open-access model.Secondly, it has a dataset search functionality4 , with which we verify that FOLIO and ProofWriter are not in StarCoder+'s training set, giving further assurance in the validity of our findings.Thirdly, with its 15.5B parameters it is likely considerably smaller than GPT-3.5 and GPT-45 , allowing us to compare performance at different model scales.</p>
<p>Controlled baselines: We compare LINC to three baselines, which we call Naïve, Scratchpad, and Chain-of-Thought (CoT), as illustrated in Fig. 2. In the Naïve baseline, the model is given the natural language premises and is asked to directly generate the label (True/False/Uncertain).</p>
<p>In the Scratchpad baseline (Nye et al., 2021), the model is asked to first generate FOL expressions corresponding to the premises, and then generate the label.This baseline is thus an ablation of LINC, where we use the LLM instead of Prover9 as the logic solver.Finally, in the CoT baseline, we use the standard technique of CoT prompting (Wei et al., 2022;Kojima et al., 2022;Wang et al., 2023b), where the model is asked to generate step-by-step natural language reasoning to arrive at the conclusion.The prompts we use for all approaches can be found in Appendix D.</p>
<p>Results &amp; Discussion</p>
<p>Our main results are shown in Figure 3.Each bar represents either LINC or one of the three baselines, while each group of bars indicates the language model used (in {StarCoder+, GPT-3.5, GPT-4}).</p>
<p>We note first that in the FOLIO domain (Figure 3a), StarCoder+-the smallest model we experiment with-benefits the most from LINC, achieving a mean accuracy that is 14.2 points higher than the closest controlled baseline (56.0% vs. 41.8% with CoT).We find that Scratchpad, where the intermediate logical formulae are still generated but the call to the symbolic solver is ablated and replaced by the model's own prediction, does not appear to benefit performance; for StarCoder+, neither Scratchpad nor the Naïve baseline perform better than simply deterministically predicting the most common label ("Uncertain").For GPT-3.5, the trend is similar, although the gap between LINC and the closest baseline shrinks (62.6% vs. 54.9%average accuracies).For GPT-4, the trend reverses: LINC underperforms CoT.However, we perform a McNemar's test (McNemar, 1947) to get the pvalue on this GPT-4 LINC vs. CoT comparison, and we find that the difference is not significant (p = 0.58).Meanwhile, for our balanced subset of ProofWriter, we see significant performance gains across the board (Figure 3b); particularly so for GPT-3.5 and GPT-4, which achieve mean accuracies of 96.4% and 98.3% when paired with LINC.</p>
<p>In light of the high accuracies obtained with LINC on ProofWriter, we offer two plausible reasons why LINC is particularly favorable on this dataset.Firstly, ProofWriter is-unlike FOLIOcompletely synthetically generated, with relatively short sentences, perhaps lending itself particularly well to being formalized in FOL.However, it is noteworthy that the Scratchpad mode does not seem to improve performance over the Naïve baseline, indicating that even if the NL-to-FOL task were particularly easy in this domain, this is not something that the model is itself capable of leveraging to improve its predictions.The second reason might be that the baseline strategies struggle in this out-of-distribution setting, in which the model must generalize to a larger set of premises (with potentially longer deductive chains) than those found in the prompt.This distribution shift makes it harder for the model to ignore irrelevant premises in the question and carry out all deductive chains correctly.Meanwhile, with LINC, the symbolic solver robustly handles irrelevant premises and long deductive chains, since the LLM only needs to translate each sentence into FOL.</p>
<p>To test this last explanation further, we plot each model's performance across ProofWriter as a func-tion of the necessary proof depth in Figure 4. We note first that StarCoder+'s performance remains flat and close to chance with all three baseline methods (Figure 4a).Meanwhile, with LINC the performance remains far above chance, although it drops somewhat as necessary proof depth increases; this performance drop suggests that StarCoder+ struggles somewhat with the NL-to-FOL translation task as the problem at hand gets larger.For GPT-3.5, all baselines perform above chance at proof depth 0 (i.e., where the conclusion can immediately be reached from the premises), but then quickly drop back down (Figure 4b).While Chain-of-Thought prompting allows the model to complete some depth-1 tasks, even this strategy then performs equivalently to chance (within 1 standard deviation) for higher depths.When augmented with LINC, however, GPT-3.5 is able to achieve near-perfect performance across all proof depths, providing evidence for the scalability of this approach to longer deductive chains.Finally, for GPT-4 we observe much stronger performance from the baselines; in particular, CoT performs above chance for all proof depths, and all baselines perform well for shallow proofs (Figure 4c).However, even with GPT-4 the performance drops as the necessary proof depth increases with every configuration except for LINC, which performs near or at ceiling through the maximum proof depth available in the dataset.</p>
<p>Error Analysis</p>
<p>Having established that LINC can improve performance in many settings, we now move on to our final research question: How do the failure modes of LINC compare to those of in-context reasoning  methods?We focus on comparing GPT-4+CoT vs. GPT-4+LINC on FOLIO, since their overall performance is very similar (75.3% vs. 72.5% average accuracy).We leave an analysis of StarCoder+'s predictions on FOLIO to Appendix G.</p>
<p>Qualitative Analysis</p>
<p>Qualitatively, we find that LINC and CoT have completely different failure modes.We give a highlevel overview and abbreviated examples of each failure mode here, leaving full detailed examples to Appendix E.</p>
<p>First, we detail the failure modes for LINC: L1: FOL fails to capture implicit information not mentioned in the premises.Often, there is obvious information not explicitly listed in the premises that is necessary to explicitly encode in FOL in order to successfully make a desired deduction.For example, in the snippet below one must encode in FOL the implicit assumption that Harry is a person (Person(Harry)).</p>
<p>Premise 1: When a person reads a book, that person gains knowledge.L2: FOL fails to capture information explicitly mentioned in the premises due to the choice of representation.Even when information is explicitly written in the premises, the choice of how the NL is represented in FOL can lead to lost information.In the example below, the fact that Heinrich was a Nazi German politician is captured by one symbol NaziGermanPolitician, causing the information that he was independently Nazi, German, or a politician to be lost.As a result, LINC predicted Uncertain instead of the ground truth label True.</p>
<p>Premise: Heinrich Scmidt was a Nazi German politician.</p>
<p>FOL: NaziGermanPolitician (HeinrichSchmidt)</p>
<p>Conclusion (Prover9: Uncertain): Heinrich Schmidt was German.</p>
<p>FOL: German(HeinrichSchmidt)</p>
<p>L3: FOL contains syntax errors.Across all generations, we find that the FOL expressions sometimes contain syntax errors: 38% for Star-CoderPlus, 24% for GPT-3.5, and 13% for GPT-4.The most common error is that the same symbol is used with multiple arities.As an example, if Summer(July4) and Loves(Alex, Summer) were both present in a FOL translation, Summer would have a multiple arity violation.</p>
<p>Next, we detail three failure modes for CoT: C1: CoT concludes something different than it suggests.One common mistake we often see with the CoT approach is that it generates a chain of reasoning indicating uncertainty, but then asserts that the conclusion is false.An example of this is shown below, where GPT-4 suggests there is "no evidence" for the conclusion but then deduces it false.</p>
<p>(GPT-4 Incorrect CoT): Let's think step by step.We want to evaluate if platypus are reptiles.We know from the premises that platypus are mammals and that they have no teeth.The fact that some mammals have teeth does not contradict the fact that platypus, although mammals, do not have teeth.Moreover, the statement about humans having teeth is unrelated to the classification of platypus.There's no evidence in the premises that would suggest platypus are reptiles.Therefore, the statement is false.C2: CoT makes incorrect logical deductions.While there are many instances where CoT makes incorrect logical deductions, one common deduction mistake is the structure "if B is true and A → B, then A is true".In the example shown below, we see this incorrect deduction where A is "Mike has an F1 visa" and B is "Mike needs to apply for CPT or OPT if he wants to work in the US".We note that making this type of logical mistake is often called the fallacy of the converse, and that humans are not immune to it, either.</p>
<p>Premise 1: International students in US have either an F1 visa or a J1 visa.</p>
<p>Premise 2: An international student in US with an F1 visa needs to apply for CPT or OPT if the student wants to work in the US.</p>
<p>Premise 3: Mike is an international student.</p>
<p>Premise 4: Mike needs to apply for CPT if he wants to work in the US.(GPT-4 Incorrect CoT): Let's think step by step.We want to evaluate if Mike has an F1 visa.From the premises, we know that Mike is an international student and needs to apply for CPT if he wants to work in the US.We also know that an international student in the US with an F1 visa needs to apply for CPT or OPT if the student wants to work in the US.So, it is clear that Mike has an F1 visa since he fits the criteria.Therefore, the conclusion is true.</p>
<p>Conclusion (Ground</p>
<p>C3: CoT fails to find complex paths of reasoning.We find that with all three models, CoT fails when the path of reasoning necessary to make the deduction is complex.Sometimes, CoT has difficulty getting started, and other times, it gets stuck in the middle of a reasoning chain.False, and 41% Uncertain, while LINC predicts 24% True, 17% False, and 57% Uncertain (with 2% of predictions throwing an error).Notably, we observe that LINC predicts Uncertain much more frequently than CoT (57% vs. 41%).To understand why, note that the translation from natural language to FOL is a lossy process: recall that in L1 and L2, we saw that the information conveyed through the FOL is sometimes a subset of the information in the original premises.Removing pieces of crucial information that were on the critical path to deducing True/False may then leave an uncertain conclusion.At the same time, while the FOL translations sometimes do not retain all of the information in the NL, they rarely contain false information that was not provided in the original premises.Therefore, LINC's precision when predicting True or False is very high (93%) compared to that of CoT (81%), but this comes at the cost of lower recall on True/False predictions (60% for LINC vs. 75% for CoT).</p>
<ol>
<li>
<p>LINC and CoT mispredict on different examples.Earlier in Sec.5.1, we saw that LINC and CoT exhibit different failure modes, which suggests they should fail on different examples.Indeed, we find that this is the case in our experiments on FO-LIO: Figure 5c shows a 2 × 2 confusion matrix which compares whether or not each method's prediction was correct.We observe that out of the 24 + 29 + 21 = 74 samples where at least one method makes an incorrect prediction, only 21 are shared.On a closer examination of these 21 samples, we find that 16 are ambiguous or incorrect in their specification (details in Appendix E.4), so the two methods only agree on 5 well-formed samples.This suggests that LINC and CoT are complementary methods which fail under distinct circumstances.</p>
</li>
<li>
<p>Mispredictions of in-context reasoning baselines are more similar to each other than they are with mispredictions of LINC.As an extension of the previous analysis, we next investigate the correlation between the mispredictions of each pair of methods.To do so, we define a similarity score between two methods A and B as follows: Given a dataset D with N rows and ground truth labels {A i } N i=1 and {B i } N i=1 from two methods A and B, we define
sim D (A, B) ≜ N i=1 1 [A i = B i ̸ = R i ] N i=1 1 [A i ̸ = R i or B i ̸ = R i ]
In words, sim D (A, B) measures the number of instances where A and B are wrong in identical ways vs. the number of instances where at least one of them is wrong.</p>
</li>
</ol>
<p>Figure 5d shows the pairwise similarity between our four methods, highlighting that the similarity between LINC's mispredictions and the other methods' mispredictions (0.14, 0.21, 0.22) is much lower than the similarity between any pair of the in-context reasoning methods (0.52, 0.54, 0.56).These results suggest that for GPT-4 on FOLIO, LINC is the only method we evaluate which significantly alters the ways in which the model fails to reason.</p>
<p>Related Work</p>
<p>Reasoning in LLMs: Our work contributes to the wider literature on eliciting natural language reasoning capabilities in models.Although we have focused here on comparing a neurosymbolic approach to Scratchpad (Nye et al., 2021) and Chain-of-Thought prompting (Wei et al., 2022;Kojima et al., 2022;Wang et al., 2023b), many other similar or related techniques have been developed in recent years; these include least-to-most prompting (Zhou et al., 2023), selection-inference (Creswell et al., 2023), backward chaining (Tafjord et al., 2022;Kazemi et al., 2023), and self-taught reasoning (Zelikman et al., 2022).Some of these techniques have been formalized under the language model cascades framework (Dohan et al., 2022).</p>
<p>Semantic parsing: The notion of a semantic parser rests on a long tradition of research (Kamath and Das, 2019) whose aim is to map fragments of natural language into useful, symbolic meaning representations (Zelle and Mooney, 1996;Zettlemoyer and Collins, 2005;Berant et al., 2013;Liang et al., 2013;Wong et al., 2023).Unlike earlier works in this tradition, we use a language model to generate the semantic parse, which is a method under active investigation in recent years (Shin and Van Durme, 2022;Drozdov et al., 2022;Lu et al., 2022;Wang et al., 2023a).</p>
<p>Neurosymbolic approaches for reasoning: Methods which combine neural networks with symbolic techniques have seen broad uptake in domains adjacent to logical reasoning, such as generating outputs consistent with a pre-existing symbolic knowledge base (Marra et al., 2019;Manhaeve et al., 2018;Zhang et al., 2023a) and performing algorithmic reasoning over symbolically grounded inputs (Ebrahimi et al., 2021;Ibarz et al., 2022;Veličković et al., 2022).As for logical reasoning with LLMs in particular, there have been a few different proposals for when and how to best combine the LLM with a symbolic component.Zhang et al. ( 2022) finetune a language model to synthesize potential facts paired with likelihoods and then use a handwritten differentiable symbolic reasoner in order to deduce other facts.Weir and Van Durme (2022) relax the solver by instead training neural "entailment" models to decide if and how a given inference rule applies at each stage.Concurrently to this work, Logic-LM (Pan et al., 2023) and SATLM (Ye et al., 2023) propose neurosymbolic approaches which have much in common with LINC.However, other than the models and datasets considered, their contributions have a few key differences to ours.First, we place particular emphasis on establishing an in-depth understanding of the relative benefits and drawbacks of a neurosymbolic approach to reasoning when compared to traditional in-context reasoning strategies like Chain-of-Thought.Second, Logic-LM employs a self-refinement strategy, which has shown promise across code generation and NLP tasks (Zhang et al., 2023b;Chen et al., 2023a;Peng et al., 2023;Madaan et al., 2023;Olausson et al., 2023) but which we do not consider here.Third, SATLM studies arithmetic reasoning in addition to logical reasoning, showcasing the versatility of the neurosymbolic approach.Fourth, and finally, we use an FOL representation that we believe is easier for humans to read and models to learn.We highly encourage interested readers to study these two contemporary works in detail.</p>
<p>Autoformalization: The idea of automatically translating natural language into structured symbolic representations that programs can reason about has gained popularity in the domain of formal mathematics, leading to autoformalization systems for several theorem provers including Mizar (Wang et al., 2018(Wang et al., , 2020)), Lean 3 (Azerbayev et al., 2023), andIsabelle (Wu et al., 2022).Outside formal mathematics, autoformalization has also been applied to translating natural language into system specification languages such as temporal logic (Hahn et al., 2022;Cosler et al., 2023;Chen et al., 2023b).</p>
<p>Tool usage: Our work is heavily inspired by recent work on tool usage.The central idea in this line of research is to augment language models with external tools such as calculators, code interpreters and information retrieval systems.We further divide these works into two classes.In the first class, the model does not need to learn how or where to invoke the tool: instead, the tool is predefined and is applied after the generation step finishes.For example, Gao et al. (2023) and Drori et al. (2022) solve mathematical reasoning tasks by generating Python programs and using the Python interpreter as the tool, Liu et al. (2023) approach physical reasoning tasks with a physical simulator as the tool, and Wong et al. ( 2023) tackle cognitivelyinspired probabilistic reasoning tasks with Church (a probabilistic programming language) as the tool.In the second class, the model must learn to invoke the tool by itself, meaning that the model must generate explicit API calls to the tool which are then executed when those calls are decoded (Schick et al., 2023;Thoppilan et al., 2022;Yao et al., 2022;Cheng et al., 2023).Our work belongs to the former class, with the task at hand being logical reasoning and the tool available for use being a FOL solver (Prover9).We refer the reader to Mialon et al. (2023) for a more thorough survey of recent work in the tool-usage literature.</p>
<p>Conclusion</p>
<p>In this work, we present LINC: Logical Inference via Neurosymbolic Computation, a neurosymbolic approach for scalable logical reasoning with large language models.Our experiments show that LINC leads to significant performance gains in nearly every setting we consider, and that it supports generalization to settings where the model has to reason about a much larger set of premises than it is shown in the in-context learning examples.Furthermore, carrying out a quantitative and qualitative analysis of the mistakes made by LINC, we find evidence that it may complement purely in-context reasoning strategies such as Chain-of-Thought prompting, since they differ greatly in the types and frequencies of mistakes made.This work thus supports the efficacy of neurosymbolic approaches to natural language reasoning, setting the stage for continued advances in combining large language models and symbolic reasoning engines; we discuss several promising future directions in Appendix A.</p>
<p>Limitations</p>
<p>Narrow scope of logical reasoning task considered: In this work, we focus exclusively on one aspect of logical reasoning: predicting the truth value of a conclusion given a set of natural language premises.Here, we consider a setting where the premises and conclusion are expressed in relatively short statements, which makes the formalization task tractable.In particular, ProofWriter's natural language statements are synthetically generated, so they can be easily and accurately parsed into FOL.FOLIO reflects a more naturalistic dataset, so we see a higher failure rate in LINC's semantic parsing step.However, the formalization task becomes more difficult if the premises are in longer paragraph form, such as in question answering or contradiction detection from context passages.This is because the same piece of information can be formalized in a variety of ways, and there is a lot of information that must be pragmatically inferred to arrive at the proper conclusion.</p>
<p>Generalizability of qualitative evaluation: While we find that LINC and CoT produce complementary mistakes for our natural language reasoning task, it is unclear if this result also holds true in similar scenarios, such as the ones considered in PAL (Gao et al., 2023), Logic-LM, and SATLM.This is due to the difference in intermediate language and overall logical reasoning task.However, we hypothesize that it will and encourage future investigation in this direction.</p>
<p>More sophisticated reasoning techniques: Recent work has proposed more sophisticated techniques beyond chain-of-thought, such as tree of thoughts (Yao et al., 2023), program of thoughts, (Chen et al., 2022), or using retrieval in chain-ofthought prompting (Yasunaga et al., 2023).These have potential to improve and eliminate some of the failure modes of the traditional CoT method.In addition, ideas such as self-repair may also serve to improve these failure modes.It remains future work to do a more thorough investigation of the efficacy of these techniques, though there is also preliminary evidence that they still lack reasoning capabilities (Huang et al., 2023).</p>
<p>Scalability: It is unclear how well LINC will perform as the number of premises scales.First, one mistake in formalization can lead to an incorrect deduction, and more premises lead to a higher probability of errors.Second, in the deduction stage, while many fast algorithms (e.g., forwardand backward-chaining) exist for logical deduction, the general problem is still NP-hard.Therefore, the theorem prover may take a long time in practice.</p>
<p>Other logics beyond first-order logic: In this work, we exclusively focus on first-order logic.However, FOL is not expressive enough to han-dle problems requiring higher-order logics (Miller and Nadathur, 1986;Higginbotham, 1998).Also, in many settings it is desirable to work with nonclassical logics (Priest, 2008;Burgess, 2009).Alternative theorem provers would be needed for such problems.A method like LINC can be naturally extended to those settings, but exactly how well it works there requires further investigations.</p>
<p>Computational costs: Implementing our approach with both GPT models and the StarCoder+ model requires non-trivial resources.The former requires reliance on costly API requests and the latter dedicated GPUs for inference.Especially as we use majority voting, many generations must be made for each query, increasing the computational requirements.</p>
<p>A Future Directions</p>
<p>Of the error types we catalog in our analysis of LINC, the key opportunity for improvement is more elegant handling of naturalistic language use.While the errors observed with CoT result from faulty deductive inferences, in the case of LINC, all errors have been localized to the semantic parsing procedure.This process flows primarily unimpeded in the evaluation of the synthetic ProofWriter dataset, yet leaves room for improvement with the naturalistic FOLIO.In follow-up work, we hope to deeply explore naturalistic evaluation settings, as when data get the most messy is also where improvements become the most valuable.Here, we propose three strategies for further improvement on naturalistic settings.First, in naturalistic communication, "obvious" information is often left out of explicit productions, left to be inferred in the "common ground" of the communicative act (Grice, 1975;Stalnaker, 2002).Implicit premise rediscovery through controlled exploration on the logical neighborhood of the existing explicit premises promises to be a powerful strategy for improving performance in underspecified settings.</p>
<p>Second, while a number of samples are lost to syntax errors, recent work has proposed restricting the sampling space of an LLM to that which is consistent with term expansions in a context-freegrammar (CFG) (Poesia et al., 2022).Doing so in this setting would eliminate all syntax errors.</p>
<p>Third, sometimes, the translation process to FOL is lossy, throwing away valuable information present in the original sentence.We propose improving the faithfulness of FOL translations by asking the LLM to translate the FOL back to natural language and comparing with the original.Forward translations that rank highly when back-translated would be those which have effectively captured the intricacies of a particular sentence's semantics.</p>
<p>Overall, we believe that shifting to evaluations on more naturalistic datasets, and incorporating strategies such as those presented here, will help pave the path forward for neurosymbolic approaches to formal reasoning.</p>
<p>B Model Details and Parameters</p>
<p>We use a decoding temperature T = 0.8 for all models.For GPT-3.5 and GPT-4, we limit the maximum number of tokens to generate to 1024 for FOLIO and 4096 for ProofWriter (to accommo-date for the previously mentioned larger number of premises involved in a typical question).For the StarCoder+ model, we allow generation up until the 8192 context window length, since this model is run locally.In either case, decoding is halted early whenever the stop token </EVALUATE> is produced.All local experiments were executed on a cluster equipped with NVIDIA A100 GPUs.</p>
<p>GPT Models: We use the gpt-3.5-turbo-16k-0613and gpt-4-0613 checkpoints of the GPT-3.5 (Ouyang et al., 2022) and GPT-4 (OpenAI, 2023) models, respectively, invoking both models via the OpenAI API.</p>
<p>StarCoder+: StarCoder+ (15.5B)6 is a version of StarCoderBase (Li et al., 2023) which has been finetuned on 600B tokens from a combination of (1) Falcon RefinedWeb (Penedo et al., 2023) (filtered version of CommonCrawl), (2) The Stack v1.2 (Kocetkov et al., 2022), and (3) a Wikipedia dataset.Its base model, StarCoderBase, is an open-access model with a GPT-2 architecture using multi-query attention (Shazeer, 2019) and fill-in-the-middle objective (Bavarian et al., 2022).StarCoderBase has a 8192 context window and is trained on 1T code tokens of permissively licensed text from GitHub across 80 programming languages (Li et al., 2023).We use StarCoder+ instead of StarCoderBase because it is finetuned on natural language, which should improve the performance on our task.We run StarCoder+ with bf16 precision to reduce its memory footprint.</p>
<p>C FOLIO Dataset Preprocessing</p>
<p>We use the publicly available FOLIO dataset on https://github.com/Yale-LILY/FOLIO.We choose representative samples from the training split of the dataset to be our few-shot examples and use the validation split of the dataset in our evaluation.The testing split is not publicly available.The original dataset has 204 validation examples.However, we discovered that there are errors in 22 of the samples.We remove these samples for our evaluation and use the remaining 182 examples.The errors as follows:</p>
<p>• In 4 samples, one or more of the ground truth FOL expressions have unbalanced parentheses (samples 3,109,110,111).</p>
<p>• In 8 samples, the label obtained by executing the ground-truth FOL expressions does not match the provided ground truth label.We double-checked this, first by executing the FOL expressions through Prover9 and second by checking it manually.(samples 6, 28, 30, 48, 113, 115, 139, 140).</p>
<p>• In 10 samples, the number of premises does not match the number of FOL expressions (samples 10, 11, 12, 88, 106, 107, 108, 174, 175, 176).</p>
<p>The sample numbers above refer to the line index in the validation file located at https://github.com/Yale-LILY/FOLIO/blob/main/data/v0.0/folio-validation.jsonl.</p>
<p>D FOLIO Few-Shot Prompts</p>
<p>The methodologies we investigate do not require any finetuning on domain-specific data.Instead, we use in-context learning (ICL) with pretrained models.We prompt the model with a set of instructions and 1-8 ICL examples, which adhere to a structured text format designed to scaffold generations and ease postprocessing.In particular, we begin each ICL example with each of the NL premises wrapped in an HTML-style tag <PREMISES>. . .</PREMISES> followed by the NL conclusion wrapped in <CONCLUSION>. . .</CONCLUSION>.The requisite evaluation steps for each evaluation paradigm are then outlined in a subsequent section wrapped <EVALUATE>. . .</EVALUATE>.Following the inclusion of ICL examples, a test example is added, with the <PREMISES> and <CONCLUSION> sections.Then, the <EVALUATE> tag is opened, and the LM is allowed to proceed with causal generation until the </EVALUATE> tag is generated.Upon generation of this stop token, the <EVALUATE> block is segmented for post-processing according to the method being evaluated ({naïve, scratchpad, chain-of-thought, neuro-symbolic}).</p>
<p>For the few-shot examples, we use samples from the publicly available FOLIO training set.We select a set of diverse samples that are balanced across labels.Since the FOLIO training set does not come with FOL expressions for the conclusions or chain of thought prompts, we manually add both for each sample.For the k-shot setting (k&lt;8), we use the first k samples from the following list of sample indices: 126, 24, 61, 276, 149, 262, 264, 684.Here, sample i refers to the ith line in https://github.com/Yale-LILY/FOLIO/blob/main/data/v0.0/folio-train.jsonl.We do not optimize for the choice of few-shot examples, and this is the only set of examples we evaluated with, so it is likely that there exist better choices for few-shot examples that would lead to improved performance across the board.</p>
<p>D.1 FOLIO, 1-shot (baseline)</p>
<p>The following is a first-order logic (FOL) → problem.The problem is to determine whether the → conclusion follows from the premises.The premises are given in the form of a set of → first-order logic sentences.The conclusion is given in the form of a single → first-order logic sentence.The task is to evaluate the conclusion as 'True', → 'False', or 'Uncertain' given the → premises.</p>
<p><PREMISES>
D.2 FOLIO, 1-shot (scratchpad)
The following is a first-order logic (FOL) → problem.The problem is to determine whether the → conclusion follows from the premises.The premises are given in the form of a set of → first-order logic sentences.The conclusion is given in the form of a single → first-order logic sentence.Depending on the context, the phrase "either x or y" could mean x XOR y, x OR y, or be ambiguous.Throughout our experiments, we found that models had many creatively incorrect ways of translating these statements.One reoccurring error was that statements that clearly intended x XOR y (such as, "an animal is either a rabbit or a squirrel") were translated into x OR y.We tried to account for this into account by including multiple samples with this construct in the few shot examples (see Sec. D.4).However, the models still handle this construct inconsistently and incorrectly.</p>
<p>In addition, we find that throughout the FOLIO dataset, by matching the natural language premises to the FOL premises, we find no consistent or predictable pattern as to how "either x or y" statements are translated.For example, "an animal is either a or a squirrel" is translated as all x.Rabbit(x) | Squirrel(x), while we believe this instance should clearly be XOR.Therefore, we believe that some of these samples are inherently ambiguous or malformed.</p>
<p>To The first two occurring in both GPT-3.5 and GPT-4, and the latter only occurs in GPT-3.5 and interestingly, is correct in GPT-4.In Example 1, to make the correct conclusion, we must encode in FOL that Harry is a person (Person(Harry)) and that Walden is a book (Book("Walden")).Harry being a person is implicit, but "Walden" being a book is explicitly mentioned in premise 4 but fails to be explicitly encoded by the model.In Example 2, we must encode that KiKi is an animal to make the correct deduction.One can argue that this example is ambiguous, but from the context, most would make this inference.In Example 3, we need a clause that says LGA and LGA are the same airport (SameAirport(LGA, LGA)).</p>
<p>Example 1 (GPT-4)</p>
<p>(both of which LINC solves correctly).C3: CoT fails to find complex paths of reasoning.We highlight two examples below: in the first example, the ground truth is false.To make this deduction, one must reason that if a Greyhound is a Boeing 707, then it is a plane, which means it is empty, which means it cannot transport multiple passengers, which means it is not an airline, which means there are no Greyhound planes, which is a contradiction.Looking at the CoT generations, the first CoT attempt gives up after failing to find any link between Greyhound and Boeing 707.The second generation attempts to make deductions from the premises.In this case, none of the 10 CoT reasoning chains begin with the correct step of starting from the negation of the conclusion and deducing it false.</p>
<p>In the second example, to make the correct deduction, we need to start from the fact that Rose is young or a student.If Rose is young, then they do not teach, which means they study, which means they are a student, which means they are a human.If Rose is a student, then they are humans.Neither of the CoT generation is able to make progress on the deduction from the information that Rose is young.In addition, the first CoT generation also has a logical error at the last step, where it asserts that "A or False" is False when the truthness of A is uncertain.</p>
<p>E.4 Shared mistakes between GPT-4 CoT and LINC</p>
<p>As shown in Fig. 5c, there are 21 common errors between GPT-4 CoT and GPT-4 LINC.After an indepth analysis of the examples, we see that 16 of these arise due to inherent errors in the dataset:</p>
<p>• 2 of these samples contain the sentence fragment "Either Zaha Hadid's design style or Kelly Wearstler's design style."as a premise.This premise is likely intended to mean all design styles are one of these two styles, but this is hard for the model to grasp from just the fragment (sample 41, 42).</p>
<p>• 2 of these samples contain the sentence fragment "Either female tennis players at Roland Garros 2022 or male tennis players at Roland Garros 2022." as a premise (sample 43, 45).</p>
<p>• 2 of these samples have an ambiguous use of "either": "Ben is either from The Simpsons or In all our experiments, we use 10-way majority voting, inspired by prior work which found that Chain-of-Thought prompting benefited therefrom (Wang et al., 2023b).However, one might wonder how robust the performance gains seen with LINC are to the precise value of K.  shows, for each model equipped with either LINC or Chain-of-Thought, how the accuracy on FO-LIO varies with K ∈ {1, 2, 3, . . ., 10}.We note that, generally speaking, LINC makes good use of increased values of K.This is especially true for the weaker models; these are more prone to generating syntactically invalid FOL expressions, which cause the solver to return an Error.Taking the majority vote over many samples thus lessens the risk of predicting Error, which is of course always the wrong label.Notably, our results do not indicate that CoT benefits from majority voting in this domain.Future work is needed to establish how this relates to the findings in the previously mentioned prior work.</p>
<p>Figure 1 :
1
Figure 1: This figure showcases the essence of our approach.Starting from a problem in natural language, inStep 1, the LLM semantic parser samples logic formulas expressing estimates of the semantics.It is possible that some of these might contain errors, e.g., the second example shows a syntax error involving an extra parenthesis, whereas the fourth example highlights a semantic error caused by mismatched predicates.In Step 2, these are then each offloaded to an automated theorem prover, filtering out syntax errors, and producing labels for the remaining samples.In Step 3, the remaining candidate outputs are passed through a majority-vote sieve to arrive at the best estimate for a single output label.</p>
<p>Figure 3 :
3
Figure 3: Results of each model on the FOLIO and ProofWriter datasets.Accuracies are for bootstrapped 10-way majority vote for all models.Error bars are ±1 bootstrapped standard deviation.Dotted, black line is the accuracy obtained by always guessing the most common label in the dataset.</p>
<p>Figure 4 :
4
Figure4: Accuracy per necessary proof depth in ProofWriter.Accuracies reported are for bootstrapped 10-way majority vote, and shaded areas cover ±1 bootstrapped standard deviation.Black, dotted lines reflect the expected success rate of guessing a random label, which is 1/3 in all subsets per our experiment design.</p>
<p>FOL:</p>
<p>all x.all y.(Person(x) &amp; Reads(x, y) &amp; Book(y) -&gt; Gains(x, Knowledge)) Premise 2: Harry read the book "Walden" by Henry Thoreau.FOL: Reads(Harry, Walden) Conclusion (Prover9: Uncertain): Harry gains knowledge.FOL: Gains(Harry, Knowledge)</p>
<p>Confusion matrix for Chain-of-Thought.Comparing the consistency of LINC vs. Chain-of-Thought.Similarity between incorrect predictions of each method, i.e., (A wrong == B wrong) / (A wrong or B wrong).</p>
<p>Figure 5 :
5
Figure 5: Analyzing and comparing the mistakes made by GPT-4 on the FOLIO dataset.</p>
<p>Truth: Uncertain): Mike has an F1 visa.</p>
<p>to CoT, LINC has worse recall but better precision on True/False predictions.To see this, we plotted the confusion matrices in Figure 5b (CoT) and Figure 5a (LINC).Looking just at the distributions of predicted labels of the two methods, we see that CoT predicts 32% True, 27%</p>
<p>highlight model behavior on these examples, four representative examples from the FO-LIO validation set are shown below; examples have multiple translations because we used temperature T = 0.8.Here, Correct/Incorrect indicate whether the translations match the ground truth (which doesn't necessarily match how we would translate it).Premise: an animal is either a rabbit or a → squirrel (Ground Truth) Translation: all x.(Rabbit(x) | → Squirrel(x)) (Correct) Translation 1 (GPT-3.5):all x. ( → Animal(x) -&gt; (Rabbit(x) | Squirrel(x))) (Incorrect) Translation 2 (StarCoderPlus): (( → Rabbit(Animal) &amp; -Squirrel(Animal)) | (-→ Rabbit(Animal) &amp; Squirrel(Animal))) (Incorrect) Translation 3 (GPT-4): all x.(( → Animal(x) &amp; Rabbit(x)) | (Animal(x) &amp; → Squirrel(x))) Premise: a person either studys or teaches (Ground Truth) Translation: all x.(Study(x) | → Teaches(x)) (Incorrect) Translation 1 (StarCoderPlus): → Studys(Person) | Teaches(Person) (Incorrect) Translation 2 (StarCoderPlus): (( → Study(Person) &amp; -Teach(Person)) | (-Study → (Person) &amp; Teach(Person))) (Correct) Translation 3 (GPT-4): all x.(Studies → (x) | Teaches(x)) Premise: A man is either kind or evil.(Ground Truth) Translation: all x.(Kind(x) &amp; -→ Evil(x)) | (-Kind(x) &amp; Evil(x)) (Incorrect) Translation 1 (GPT-3.5):((Man(x) &amp; -→ Kind(x)) | (-Man(x) &amp; Evil(x))) (Incorrect) Translation 2 (StarCoderPlus): Kind( → AMan) | Evil(AMan) (Incorrect) Translation 3 (StarCoderPlus): (Kind → (x) | Evil(x)) Premise: Ben is either from The Simpsons or → funny.(Ground Truth) Translation: (Simpsons(Ben) &amp; -→ Funny(Ben)) | (-Simpsons(Ben) &amp; Funny(Ben → )) (Correct) Translation 1 (StarCoderPlus): (( → Simpsons(Ben) &amp; -Funny(Ben)) | (-Simpsons → (Ben) &amp; Funny(Ben))) (Incorrect) Translation 2 (GPT-3.5):( → FromTheSimpsons(Ben) | Funny(Ben)) (Incorrect) Translation 3 (GPT-4): → FromTheSimpsons(Ben) | Funny(Ben) E.2 GPT-4 LINC Failure Modes L1: FOL fails to capture implicit information not mentioned in the premises.Three examples of errors from the FOLIO validation set are shown below.</p>
<p>Similarity between incorrect predictions of each method, i.e., (A wrong == B wrong) / (A wrong or B wrong).</p>
<p>Figure 6 :
6
Figure 6: Analyzing and comparing the mistakes made by StarCoder+ on the FOLIO dataset.</p>
<p>on held-out test set StarCoder+-Chain-of-Thought StarCoder+-LINC (ours) GPT-3.5-Chain-of-ThoughtGPT-3.5-LINC(ours) GPT-4-Chain-of-Thought GPT-4-LINC (ours) Chance</p>
<p>Figure 7 :
7
Figure7: Accuracy on FOLIO per value of K (Appendix H).Shaded areas are ±1 standard deviation over 1000 bootstrapped samples.Note that increasing K benefits LINC (solid lines; shading in color) but not CoT (dashed lines; shading in gray) in our experiments on this dataset.</p>
<p>This figure outlines the string concatenation workflow for each of our conditions.We start with the original problem, provide ICL examples through an intermediate markup language, and finally append the problem to evaluate.At this stage, we allow the model to autoregressively sample until producing a stop token.
* N ICLExamples⧺NaïveScratchpadCoTLINC⧺All dogs are mammals.⧺ = string concatenationHarry is a dog.Is Harry a mammal?<EVALUATE>…Figure 2:</p>
<p>Mike is an international student and → needs to apply for CPT if he wants to → work in the US.We also know that an → international student in the US with an → F1 visa needs to apply for CPT or OPT if → the student wants to work in the US.So, → it is clear that Mike has an F1 visa → since he fits the criteria.Therefore, → the conclusion is true.
Example 1Premises:Plungers suck.Vacuums suck.Vampires suck.Space is a vacuum.A duster is a household appliance that doesn't→ suck.Example 2Conclusion: Space is a vampire.Premises: If people perform in school talent shows often, → then they attend and are very engaged(GPT-4 Incorrect CoT): Let's think step by step. → We want to evaluate if Space is a → vampire. We know that Space is a vacuum → and that vacuums suck. However, while it' → s true that vampires also suck, this → doesn't mean that everything that sucks → is a vampire. Therefore, there is no → reason to believe that Space is a vampire → based on these premises, so the → statement is false. ANSWER: False→ with school events. People either perform in school talent shows → often or are inactive and disinterested → members of their community. If people chaperone high school dances, then → they are not students who attend the → school. All people who are inactive and disinterested → members of their community chaperone high → school dances. All young children and teenagers who wish to → further their academic careers andExample 2 Premises: Some mammals have teeth. Platypus have no teeth. Platypus are mammals. Humans have teeth.→ educational opportunities are students → who attend the school. Bonnie either both attends and is very engaged → with school events and is a student who → attends the school, or she neither → attends and is very engaged with school → events nor is a student who attends theConclusion:→ school.Platypus are reptiles.Conclusion (Ground Truth: Uncertain):(GPT-4 Incorrect CoT): Let's think step by step.Bonnie performs in school talent shows often.→ We want to evaluate if platypus are → reptiles. We know from the premises that → platypus are mammals and that they have → no teeth. fact that some mammals have → teeth does not contradict the fact that → platypus, although mammals, do not have → teeth. Moreover, the statement about → humans having teeth is unrelated to the → classification of platypus. There's no → evidence in the premises that would → suggest platypus are reptiles. Therefore, → the statement is false.(GPT-4 Incorrect CoT): Let's think step by step. → We want to evaluate if Bonnie performs → in school talent shows often. Based on → the last premise, if Bonnie is a student → who attends the school and is very → engaged with school events, this is only → possible if she performs in school talent → shows often, as per the first premise. → If she doesn't attend and is not engaged → with school events, she is an inactive → and disinterested member of her community → . But the fourth premise states that→ these individuals chaperone high school→ dances, meaning they cannot be students,→ which would contradict the last premise.Example 1 Premises: International students in US have either an F1→ Therefore, Bonnie performs in school → talent shows often, so the statement is → true.→ visa or a J1 visa.An international student in US with an F1 visa→ needs to apply for CPT or OPT if the→ student wants to work in the US.Mike is an international student.Mike needs to apply for CPT if he wants to work→ in the US.Conclusion (Ground Truth: Uncertain):Mike has an F1 visa.
C2: CoT makes incorrect logical deductions.(GPT-4IncorrectCoT): Let's think step by step.→We want to evaluate if Mike has an F1 → visa.From the premises, we know that →</p>
<p>We want to evaluate if a Greyhound → is a Boeing 707.The first three premises → don't provide any information about → Greyhound or Boeing 707s.The fourth → premise tells us that all Boeing 707s are → planes but doesn't tell us anything → about Greyhound.The fifth premise tells → us that if Greyhound is not an airline, → then there are no Greyhound planes, but → this still doesn't imply that a Greyhound → is a Boeing 707.Therefore, we do not → have enough information to conclude → whether a Greyhound is a Boeing 707 or → not.So, the statement is uncertain.If someone studies, then they are a student.A person either studies or teaches.No young person teaches.Jerry neither teaches nor is a manager.Rose is young or a student.Conclusion (Ground Truth: True) Rose is a human or Jerry is a manager.(GPT-4 Incorrect CoT, 1): Let's think step by → step.We want to evaluate if Rose is a → human or Jerry is a manager.Let's first → look at Rose.From premise 6, we know → that Rose is either young or a student.→ If Rose is a student, then from premise 1 → we know that Rose is a human.If Rose is → young, we don't know from the premises → whether she is a human.Therefore, it is → uncertain whether Rose is a human.Now → let's look at Jerry.From premise 5, we → know that Jerry is not a manager.→ Therefore, Jerry is not a manager.So, it → is uncertain whether Rose is a human, → but we know that Jerry is not a manager, → so the statement is false.(GPT-4 Incorrect CoT, 2): Let's think step by → step.We want to evaluate if Rose is a → human or Jerry is a manager.First, we → know that Jerry neither teaches nor is a → manager, which means Jerry is not a → manager.Therefore, the only way for the → conclusion to be true is if Rose is a → human.We know that Rose is either young → or a student.If she's a student, since → all students are humans, then she is a → human.However, if Rose is young, we don' → t have enough information to determine → whether she's a human.Therefore, it's → uncertain whether Rose is a human or → Jerry is a manager.
Example 1Premises:If something can transport multiple passengers,→ then they are not empty.All airlines can transport multiple passengers.All planes are empty.All Boeing 707s are planes.If Greyhound is not an airline, then there are→ no Greyhound planes.Conclusion (Ground Truth: False)A Greyhound is a Boeing 707.(GPT-4 Incorrect CoT, 1): Let's think step by→ step. We want to evaluate if a Greyhound→ is a Boeing 707. Looking at our premises,→ we have no direct or indirect→ information linking Greyhounds to Boeing→ 707s. Given the information available, we→ cannot make any certain conclusion about→ a Greyhound being 707.→ Therefore, it is uncertain whether a→ Greyhound is a Boeing 707.(GPT-4 Incorrect CoT, 2): Let's think step by→ step. Example 2Premises:All students are humans.
We use gpt-3.5-turbo-16k-0613 and gpt-4-0613.
https://huggingface.co/bigcode/starcoderplus
See https://huggingface.co/spaces/bigcode/ in-the-stack and https://huggingface.co/spaces/ bigcode/search.
Although the exact size of these models has not been made public, their common predecessor GPT-3 was known to have 175B parameters; seeBrown et al. (2020).
https://huggingface.co/bigcode/starcoderplus
9 Acknowledgements T.X.Olausson is supported by the Defense Advanced Research Projects Agency (DARPA) under the ASKEM program, award HR00112220042. A. Gu is supported by the National Science Foundation (NSF) Graduate Research Fellowship under Grant No. 2141064.B. Lipkin and C.E. Zhang are supported by MIT Presidential Fellowships.A. Solar-Lezama is supported by the National Science Foundation (NSF) and Intel Corporation through NSF Grant CCF:2217064.J.B. Tenenbaum is supported by AFOSR Grant #FA9550-22-1-0387 and the MIT-IBM Watson AI Lab.R.P. Levy is supported by a grant from the Simons Foundation to the Simons Center for the Social Brain at MIT.We thank our anonymous reviewers for their insightful feedback and recommendations.We thank the members of the Computer Aided Programming, Computational Psycholinguistics, and Computational Cognitive Science groups for constructive commentary at various stages of this project.We thank Yoon Kim for helpful suggestions and comments on pieces of the initial project proposal.In addition, we thank Zhaofeng Wu and Simeng Han for discussions regarding the FOLIO dataset.All dispensable things are environment-friendly.All woodware is dispensable.All paper is woodware.No good things are bad.All environment-friendly things are good.A worksheet is either paper or is environment-→ friendly.</PREMISES> <CONCLUSION> A worksheet is not dispensable.</CONCLUSION> <EVALUATE> TEXT: All dispensable things are environment-→ friendly.FOL: all x.(Dispensable(x) -&gt; → EnvironmentFriendly(x)) TEXT: All woodware is dispensable.FOL: all x.(Woodware(x) -&gt; Dispensable(x)) TEXT: All paper is woodware.FOL: all x.(Paper(x) -&gt; Woodware(x)) TEXT: No good things are bad.FOL: all x.(Good(x) -&gt; -Bad(x)) TEXT: All environment-friendly things are good.FOL: all x.(EnvironmentFriendly(x) -&gt; Good(x)) TEXT: A worksheet is either paper or is → environment-friendly.The following is a first-order logic(FOL)→ problem.The problem is to determine whether the → conclusion follows from the premises.The premises are given in the form of a set of → first-order logic sentences.The conclusion is given in the form of a single → first-order logic sentence.L2: FOL fails to capture information explicitly mentioned in the premises due to the choice of representation.We show four illustrative examples: the first two examples occur in both GPT-3.5 and GPT-4, while the last two examples only occur in GPT-3.5 and are correct when using GPT-4.The first example is described in the main text.In example 2, we know that Barutin Cove is a cove, but this lost information is not encoded in the FOL, so we cannot conclude Cove(BarutinCove), although this is crucial information for making the deduction.In Example 3, we must deduce that Tom's license plate is a license plate (FromIstanbul(TomPlate)), which is obvious but is not encoded by the model.In the final example, the idea that an object has four sides is expressed both as FourSided(x) and FourSides(x), so the model is unable to make the correct deduction.L3: FOL contains syntax errors.This is one reason that we observe majority voting helps significantly for neurosymbolic methods, as multiple generations increase the likelihood that at one least one FOL representation successfully compiles.As stated in the main text, the most common error is that the same symbol is used with multiple arities.Two examples of this are shown below: in the first example, the primitive Summer is both used as a function representing that a year has a summer (in the first premise) and as a constant representing that a student loves summer.A similar phenomenon occurs in the second example with the symbol Badults.funny," which in this case ambiguously means XOR.We believe this sentence is an unnatural usage of "either" (sample 142, 143).• In 4 samples, there is a name that is implicitly an animal, but this is not clear(sample 126, 127, 128, 199).Also, in samples 126-128, there is a statement "If Rock is neither a fly nor a bird, then Rock neither flies nor breathes."that should likely say "If Rock neither flies nor is a bird, ...".With the original formulation, everything could be uncertain because nothing is known in the case that Rock is a fly (and this is a reasonable interpretation).• In 5 samples, the ground-truth FOL representation of the natural language premise is incorrect, causing the label to be incorrect(samples 29, 79, 85, 86, 87).• There is a sample where the conclusion is likely mis-worded and should be "Barutin Cove is not located in Antarctica." instead of "Barutin is not located in Antarctica."This changes the ground truth label.(sample 121).The remaining 5 samples are examples where both methods fail.In all cases, CoT fails to find the correct reasoning chain, as the premises/reasoning path is convoluted and complex.Meanwhile, LINC fails as follows:• In sample 104, LINC generates separate symbols for NotLiveAtHome and -LiveAtHome, which are semantically equivalent.F Proofwriter StarCoder+ ErrorsWe also analyzed, qualitatively, the errors that Star-Coder+ makes on the ProofWriter dataset.Since premises from the ProofWriter dataset are relatively simple and direct, lossy translation is rarer.Most errors are due to syntax or improper representations.We show five representative errors below:G StarCoderPlus FOLIO Error AnalysisFor StarCoder+, we see a slightly different trend.In Fig.6a, we see the same pattern as for GPT-4, with a majority of uncertain predictions.In Fig.6b, however, we see that CoT for StarCoder+ primarily predicts true.This is likely because the model was trained on much more code than text, and may not have picked up sophisticated textual chain-ofthought reasoning capabilities.In Fig.6c, we can see that the mispredictions between CoT and LINC differ much more for StarCoder+ than GPT-4.Finally, in Fig.6d, we see the same trends as we saw with GPT-4 but more pronounced, as the similarity between mispredictions in LINC and those in the baseline methods is even lower than they were for GPT-4.
Exploring length generalization in large language models. Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, Behnam Neyshabur, Advances in Neural Information Processing Systems. 202235</p>
<p>ProofNet: Autoformalizing and formally proving undergraduate-level mathematics. Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W Ayers, Dragomir Radev, Jeremy Avigad, arXiv:2302.124332023arXiv preprintCited on pg</p>
<p>Center for the Study of Language and Information. David Barker-Plummer, Jon Barwise, John Etchemendy, Language, proof, and logic. 20112 edition. Cited on pg</p>
<p>Efficient training of language models to fill in the middle. Mohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine Mcleavey, Jerry Tworek, Mark Chen, arXiv:2207.142552022arXiv preprintCited on pg. 15</p>
<p>Semantic parsing on Freebase from question-answer pairs. Jonathan Berant, Andrew Chou, Roy Frostig, Percy Liang, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. the 2013 Conference on Empirical Methods in Natural Language ProcessingSeattle, Washington, USAAssociation for Computational Linguistics2013</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033Cited on pg. 1, 4</p>
<p>Philosophical logic. Burgess John, 2009Princeton University Press</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, arXiv:2211.125882022arXiv preprintCited on pg</p>
<p>Teaching large language models to self-debug. Xinyun Chen, Maxwell Lin, Nathanael Schärli, Denny Zhou, arXiv:2304.051282023aarXiv preprintCited on pg. 9</p>
<p>Yongchao Chen, Rujul Gandhi, Yang Zhang, Chuchu Fan, arXiv:2305.07766NL2TL: Transforming natural languages to temporal logics using large language models. 2023barXiv preprintCited on pg. 9</p>
<p>Binding language models in symbolic languages. Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, Tao Yu, The Eleventh International Conference on Learning Representations. 2023Cited on pg. 9</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>nl2spec: Interactively translating unstructured natural language to temporal logics with large language models. Matthias Cosler, Christopher Hahn, Daniel Mendoza, Frederik Schmitt, Caroline Trippel, Computer Aided Verification. ChamSpringer Nature Switzerland2023Cited on pg. 9</p>
<p>Selection-inference: Exploiting large language models for interpretable logical reasoning. Antonia Creswell, Murray Shanahan, Irina Higgins, The Eleventh International Conference on Learning Representations. 2023Cited on pg. 1, 3, 8</p>
<p>. David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, arXiv:2207.10342Jascha Sohl-Dickstein. 8arXiv preprintet al. 2022. Language model cascades. Cited on pg</p>
<p>A neural network solves, explains, and generates university math problems by program synthesis and fewshot learning at human level. Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin Liu, Linda Chen, Sunny Tran, Newman Cheng, Proceedings of the National Academy of Sciences. the National Academy of Sciences2022119e2123433119Cited on pg. 9</p>
<p>Andrew Drozdov, Nathanael Schärli, Ekin Akyürek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, Denny Zhou, arXiv:2209.15003Compositional semantic parsing with large language models. 2022arXiv preprintCited on pg</p>
<p>Nouha Dziri, Ximing Lu, Melanie Sclar, Lorraine Xiang, Liwei Li, Bill Yuchen Jian, Peter Lin, Chandra West, Bhagavatula, Le Ronan, Jena D Bras, Hwang, arXiv:2305.18654Faith and fate: Limits of transformers on compositionality. 2023arXiv preprint</p>
<p>Towards bridging the neurosymbolic gap: Deep deductive reasoners. Monireh Ebrahimi, Aaron Eberhart, Federico Bianchi, Pascal Hitzler, Applied Intelligence. 512021</p>
<p>A mathematical introduction to logic. Herbert B Enderton, 2001ElsevierCited on pg</p>
<p>PAL: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, PMLR.International Conference on Machine Learning. 2023Cited on pg. 9, 10</p>
<p>Logic and conversation. Herbert P Grice, Speech acts. Brill1975</p>
<p>Christopher Hahn, Frederik Schmitt, Julia J Tillman, Niklas Metzger, Julian Siber, Bernd Finkbeiner, arXiv:2206.01962Formal specifications from natural language. 2022arXiv preprintCited on pg. 9</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, arXiv:2209.00840Folio: Natural language reasoning with firstorder logic. 2022arXiv preprint</p>
<p>On higher-order logic and natural language. James Higginbotham, Proceedings of the British Academy. the British Academy199895Cited on pg</p>
<p>Jie Huang, Kevin Chen, -Chuan Chang, arXiv:2212.10403Towards reasoning in large language models: A survey. 2022arXiv preprint</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, arXiv:2310.017982023arXiv preprintCited on pg</p>
<p>Alex Vitvitskyi, Yulia Rubanova, et al. 2022. A generalist neural algorithmic learner. Borja Ibarz, Vitaly Kurin, George Papamakarios, Kyriacos Nikiforou, Mehdi Bennani, Róbert Csordás, Andrew Joseph Dudzik, Matko Bošnjak, PMLR.Learning on Graphs Conference. Cited on pg</p>
<p>A survey on semantic parsing. Aishwarya Kamath, Rajarshi Das, Automated Knowledge Base Construction (AKBC). 2019Cited on pg</p>
<p>LAMBADA: Backward chaining for automated reasoning in natural language. Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu, Deepak Ramachandran, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231Cited on pg</p>
<p>Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, arXiv:2211.15533The Stack: 3 TB of permissively licensed source code. 2022arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235Cited on pg. 1, 4, 8</p>
<p>Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, arXiv:2305.06161Starcoder: may the source be with you!. 2023arXiv preprintCited on pg. 4, 15</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, arXiv:2211.09110Holistic evaluation of language models. Ananya Kumar, et al. 2022arXiv preprint</p>
<p>Learning dependency-based compositional semantics. Percy Liang, Michael I Jordan, Dan Klein, Computational Linguistics. 3922013</p>
<p>Mind's eye: Grounded language model reasoning through simulation. Ruibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush Vosoughi, Claire Cui, Denny Zhou, Andrew M Dai, The Eleventh International Conference on Learning Representations. 2023Cited on pg. 9</p>
<p>Parsing natural language into propositional and first-order logic with dual reinforcement learning. Xuantao Lu, Jingping Liu, Zhouhong Gu, Hanwen Tong, Chenhao Xie, Junyang Huang, Yanghua Xiao, Wenguang Wang, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational Linguistics2022Cited on pg</p>
<p>Self-refine: Iterative refinement with selffeedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, arXiv:2303.176512023arXiv preprintCited on pg. 9</p>
<p>DeepProbLog: Neural probabilistic logic programming. Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, Luc De, Raedt , Advances in neural information processing systems. 201831</p>
<p>Integrating learning and reasoning with deep logic models. Giuseppe Marra, Francesco Giannini, Michelangelo Diligenti, Marco Gori, Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer2019Cited on pg</p>
<p>. W Mccune, 2005-2010. Prover9 and mace4Cited on pg. 3</p>
<p>The inverse scaling prize. Ian Mckenzie, Alexander Lyzhov, Alicia Parrish, Ameya Prabhu, Aaron Mueller, Najoung Kim, Sam Bowman, Ethan Perez, 2022</p>
<p>Note on the sampling error of the difference between correlated proportions or percentages. Quinn Mcnemar, Psychometrika. 1221947</p>
<p>Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Timo Baptiste Rozière, Jane Schick, Asli Dwivedi-Yu, Celikyilmaz, arXiv:2302.07842Augmented language models: a survey. 2023arXiv preprintCited on pg. 9</p>
<p>Some uses of higher-order logic in computational linguistics. Dale A Miller, Gopalan Nadathur, Proceedings of the 24th Annual Meeting on Association for Computational Linguistics. the 24th Annual Meeting on Association for Computational LinguisticsNew York, USAAssociation for Computational Linguistics1986Cited on pg</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, arXiv:2112.00114Show your work: Scratchpads for intermediate computation with language models. 2021arXiv preprintCited on pg. 1, 4, 8</p>
<p>Is self-repair a silver bullet for code generation?. Jeevana Theo X Olausson, Chenglong Priya Inala, Jianfeng Wang, Armando Gao, Solar-Lezama, arXiv:2306.098962023arXiv preprintCited on pg. 9</p>
<p>GPT-4 technical report. 2023OpenAICited on pg. 1, 4, 15</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235Cited on pg. 4, 15</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Yang, Wang , arXiv:2305.122952023arXiv preprintCited on pg. 9</p>
<p>Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, Julien Launay, arXiv:2306.01116The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. 2023arXiv preprint</p>
<p>Check your facts and try again: Improving large language models with external knowledge and automated feedback. Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, Jianfeng Gao, arXiv:2302.128132023arXiv preprintCited on pg. 9</p>
<p>Synchromesh: Reliable code generation from pre-trained language models. Gabriel Poesia, Alex Polozov, Ashish Vu Le, Gustavo Tiwari, Christopher Soares, Sumit Meek, Gulwani, International Conference on Learning Representations. 2022</p>
<p>An introduction to non-classical logic: From if to is. Graham Priest, 2008Cambridge University Press</p>
<p>Testing the general deductive reasoning capacity of large language models using ood examples. Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Padmakumar, Nitish Joshi, Seyed Mehran Kazemi, Najoung Kim, He He, arXiv:2305.152692023arXiv preprint</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.047612023arXiv preprintCited on pg. 2, 9</p>
<p>Fast transformer decoding: One write-head is all you need. Noam Shazeer, arXiv:1911.021502019arXiv preprint</p>
<p>Fewshot semantic parsing with language models trained on code. Richard Shin, Benjamin Van Durme, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, Transactions on Machine Learning Research. 2023Cited on pg. 1</p>
<p>Common ground. Robert Stalnaker, Linguistics and philosophy. 255/62002</p>
<p>ProofWriter: Generating implications, proofs, and abductive statements over natural language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021Cited on pg</p>
<p>Entailer: Answering questions with faithful and truthful chains of reasoning. Oyvind Tafjord, Bhavana Dalvi Mishra, Peter Clark, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>LaMDA: Language models for dialog applications. Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Du, arXiv:2201.082392022arXiv preprintCited on pg. 9</p>
<p>The clrs algorithmic reasoning benchmark. Petar Veličković, Adrià Puigdomènech Badia, David Budden, Razvan Pascanu, Andrea Banino, Misha Dashevskiy, Raia Hadsell, Charles Blundell, PMLR.International Conference on Machine Learning. 2022Cited on pg. 9</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3522 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3522</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3522</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-f0a0e8b6e84207f50db4d24cc4016e40601214ef</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f0a0e8b6e84207f50db4d24cc4016e40601214ef" target="_blank">Faithful Reasoning Using Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The method carries out a beam search through the space of reasoning traces to improve reasoning quality, and generates humanly interpretable reasoning traces whose validity can be checked by the user.</p>
                <p><strong>Paper Abstract:</strong> Although contemporary large language models (LMs) demonstrate impressive question-answering capabilities, their answers are typically the product of a single call to the model. This entails an unwelcome degree of opacity and compromises performance, especially on problems that are inherently multi-step. To address these limitations, we show how LMs can be made to perform faithful multi-step reasoning via a process whose causal structure mirrors the underlying logical structure of the problem. Our approach works by chaining together reasoning steps, where each step results from calls to two fine-tuned LMs, one for selection and one for inference, to produce a valid reasoning trace. Our method carries out a beam search through the space of reasoning traces to improve reasoning quality. We demonstrate the effectiveness of our model on multi-step logical deduction and scientific question-answering, showing that it outperforms baselines on final answer accuracy, and generates humanly interpretable reasoning traces whose validity can be checked by the user.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3522.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3522.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SI (Selection-Inference)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Selection-Inference faithful reasoning backbone</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A forward-chaining architecture that splits each reasoning step into a Selection model (chooses statements from context) and an Inference model (predicts an entailment from that selection), producing connected, human-interpretable reasoning traces and preventing the answer from depending directly on the question.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Selection-Inference (SI) pipeline using Chinchilla LMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pipeline composed of separate fine-tuned 7B-parameter Chinchilla LMs for Selection and Inference; the Selection LM predicts sentence labels from the context, and the Inference LM predicts an entailment given only the selected statements (no access to the question). SI is combined with a Halter LM and a learned Value LM for beam search.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Proof Writer (PW) and EntailmentBankQA (EB)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>PW: synthetic logical deduction problems with 1/2/3/5-step proofs and True/False/Unknown answers; EB (reformulated to EntailmentBankQA): grade-school science QA problems with context, multiple choices and a ground-truth entailment tree (Task 1: relevant facts only; Task 2: includes distractors). Tasks require multi-step deductive/entailment reasoning over provided context.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Architectural constraint enforcing causal structure: separation of Selection and Inference LMs (Inference denied access to the question), selection by sentence-labels to forbid hallucination, a Halter LM to decide when to stop and produce answers, and a learned Value LM to guide beam search over traces.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Proof Writer overall final-answer accuracy: SI + Halter + PW Search = 88.1% (Table 1). Proof Writer per-depth: depth-1 99.4%, depth-2 98.1%, depth-3 92.0%, depth-5 63.4% (SI + Halter + PW Search). EntailmentBankQA: SI + Halter + Search: Task 1 = 83.2%, Task 2 = 72.9% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Proof Writer: best baseline Proof + Answer = 65.4% overall (Table 1); EntailmentBankQA: Proof + Answer = 64.6% Task 1, 7.8% Task 2; EntailmentWriter + Answer = 50.0% Task 1, 35.0% Task 2 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Substantial improvements: on Proof Writer overall +~22.7 percentage points vs Proof + Answer (88.1% vs 65.4%). On PW depth-5 improvement is large (63.4% vs 60.4% for Proof+Answer in some runs, or vs Proof Only ablations). On EntailmentBankQA Task 2 SI+Search reaches 72.9% vs Proof+Answer 7.8% (very large gain), and vs EntailmentWriter+Answer 35.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Correctness of individual inference steps is not guaranteed — Inference LM can still produce incorrect entailments if Selection provides unrelated statements; SI assumes access to a sufficient context (retrieval not integrated); performance still drops for deeper proofs (depth-5) and for the hardest EB examples; halting may return 'Unknown' and model refrains from answering many problems (e.g., SI+Halter returns Unknown often on PW incomplete-context ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablations show search (Value LM guided beam search) and the Halter both improve final-answer accuracy: SI + Halter + Search outperforms SI + Halter without search (Tables 1 and 2). Analyses show SI produces far fewer hallucinated facts (<1%) than baselines and leverages context and its trace more (Tables 3,4,5,6). Inference accuracy when given valid selections is very high (ProofWriter inference accuracy ~99.9–100%, Table 7), indicating the modular design helps step correctness when selection is valid.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Faithful Reasoning Using Large Language Models', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3522.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3522.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Selection LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Selection language model (component of SI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Fine-tuned LM that, given question and current context, selects supporting statements by predicting sentence labels (e.g., 'sent 3'), ensuring selections come only from the provided context and preventing hallucinated facts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Selection LM (7B Chinchilla)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 7B-parameter Chinchilla LM fine-tuned to output sentence labels that refer to context statements; outputs are used to compose 'We know that sent X and sent Y...' inputs for the Inference LM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Proof Writer and EntailmentBankQA (used to produce selection steps)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Selects the subset of context sentences required at each reasoning step (the 'leaves' / supporting facts) for multi-step logical deduction.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Fine-tuning on ground-truth selection labels; syntactic constraint to only output labels (rather than free-text), then re-substituting referenced sentences for inference input.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Selection accuracy evaluated via Jaccard and leaf similarity metrics: SI models achieve higher Jaccard similarity for leaves than baselines on PW and EB (Figures 8 and 9); explicit numeric leaf/jaccard scores vary by depth/task (figures).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline models (Proof+Answer, EntailmentWriter) have lower selection (leaf) similarity (Figures 8a and 9a); exact numeric baselines vary by task and depth (figures).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Selection LM yields notably better selection (leaf Jaccard) than baseline end-to-end models, contributing to higher-quality, connected traces and downstream answer accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Selection model is non-deterministic and can produce syntactically incorrect label outputs (rare failure mode causing apparent 'made-up facts' when label parsing fails). It can also choose incorrect supporting statements, which then cascades to incorrect inferences.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Analyses show the selection step is crucial: SI's advantage over baselines is partly because Selection avoids hallucination and selects correct supporting statements more frequently (Figures 8 and 9). Beam search over selection candidates guided by the Value LM improves overall trace quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Faithful Reasoning Using Large Language Models', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3522.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3522.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inference LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inference language model (component of SI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Fine-tuned LM that, given only the selected context statements (no access to the question), predicts an entailment (inference) that follows logically from those statements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Inference LM (7B Chinchilla)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 7B-parameter Chinchilla LM fine-tuned on (selection -> entailment) pairs from ground-truth proofs; trained to produce valid next-step inferences from selected premises while being denied question input to prevent cheating.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Proof Writer and EntailmentBankQA (inference generation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Generates the next-step logical consequence (an entailment) given a set of premises selected from the context; these inferences are appended to the context and iterated.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Fine-tuning on ground-truth selection->inference training pairs; restricted input (no question) to force inference to depend only on selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>High inference accuracy when fed valid selections: ProofWriter inference accuracy ~100% for depth-1/2/3 and 99.9% for depth-5 (exact string-match, Table 7). EntailmentBankQA ROUGE-L = 0.69, ROUGE1 = 0.69, BLEURT>0.28 = 64% (Table 8).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baselines that predict intermediate outputs (Proof+Answer, EntailmentWriter) often achieve good intermediate-inference quality but can do so while selecting incorrect supports or cheating — making their traces less valid overall (Figures 17,9).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>By training a dedicated Inference LM that lacks access to the question, the paper reports more faithful inferences that, combined with selection restrictions, produce more valid traces and higher final-answer accuracy compared to baseline end-to-end models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Inference correctness depends heavily on the quality of the Selection output; if selection contains unrelated or wrong premises the Inference LM can produce nonsensical or incorrect statements. The approach cannot guarantee logical correctness for every step.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Feeding the inference model with ground-truth (valid) selections yields near-perfect inference accuracy (Table 7), indicating that Selection quality is the primary bottleneck for end-to-end trace correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Faithful Reasoning Using Large Language Models', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3522.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3522.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Halter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Halter (halt model for termination and answering)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage LM component that decides after each SI step whether the current inference suffices to answer the question, and if so, produces the final answer in a way that forces dependence on the trace rather than model priors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Halter LM (7B Chinchilla)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 7B-parameter Chinchilla LM fine-tuned on examples derived from ground-truth proofs to (1) predict 'Yes'/'No' to 'Do you know the answer given {inference}?', and (2) produce the final answer when appropriate; same LM used for both functions with differing prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Proof Writer and EntailmentBankQA (halting and answer formatting)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Determines when to stop iterative reasoning and returns the answer in the required format (e.g., True/False or multiple-choice choice), forcing use of final inference as evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Fine-tuning on halting examples constructed from ground-truth traces; two-stage prompting to first ask if the answer is known and then to select an answer if it is.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Applied to ground-truth proofs Halter accuracy: PW halter performs almost perfectly; EB halter achieves 88.8% accuracy (Section 4.3). Filtering to only 'known' problems yields near-perfect accuracy on PW and ~87.5%/83.7% on EB Task 1/2 (Figure 6).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline models lack a halting mechanism and cannot reliably say 'Unknown'; ground-truth-proof+Halter yields 100% on PW (Tables 1 and 2). Proof Only + Halter without search shows lower final-answer accuracy compared to SI+Halter+Search.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Halter enables the system to abstain (output 'Unknown'), increasing precision: filtering out problems Halter deems unknown boosts reported accuracies significantly (Figure 6). Combined with SI and search, Halter contributes to higher final-answer accuracy vs baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Halter is trained on traces and thus may be overconfident or fail to detect insufficiency in settings unlike training; performance on EB is imperfect (88.8% on ground-truth proofs), and halting introduces an abstention tradeoff (precision vs recall).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablations show SI+Halter+Search outperforms SI without Search and that using Halter to filter increases precision markedly (Figure 6, Tables 1 and 2).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Faithful Reasoning Using Large Language Models', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3522.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3522.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Value LM + Search</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learned value function (LM) used to guide beam search over traces</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-tuned LM that scores partial reasoning traces as 'correct' or 'incorrect' (trained on positive/negative partial traces) and whose log-probability of 'correct' is used as a value to guide beam search over candidate reasoning traces, improving trace quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Value LM (7B Chinchilla) + Beam Search</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B Chinchilla LM fine-tuned to predict whether the current step in a partial reasoning trace is 'correct' or 'incorrect'; used to score and prune candidate traces in a step-level beam search (keep top B traces each step).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Proof Writer and EntailmentBankQA (search over candidate traces)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Guides exploration of the tree of non-deterministic selection/inference outcomes to find high-value reasoning traces that lead to correct final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Train Value LM on positive examples (ground-truth partial traces) and negative examples (swap a supporting statement and re-run inference) and use log p_value('correct'|trace) to score traces during beam search.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Enabling search (Value LM guided beam search) improves final-answer accuracy for both SI and baseline Proof Only models. Example: SI+Halter improves from 78.3% to 88.1% overall on PW when using PW Search (Table 1); similar gains observed on EB Task 2.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Without search, SI + Halter performs worse (e.g., PW overall SI+Halter = 78.3% vs SI+Halter+PW Search = 88.1%). Proof Only + Halter + Search also improves relative to no-search ablations (Tables 1 and 2).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Search provides the most significant improvement on deeper problems and contexts with distractors (e.g., PW depth-5 and EB Task 2). Quantitatively, search often yields double-digit percentage improvements on the hardest subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Value LM's training depends on the quality of generated negative examples (random replacements); value estimates are imperfect and cannot guarantee pruning of all incorrect traces. Beam search increases compute and may still miss correct traces if beam size or value function are miscalibrated.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablation tables show consistent benefit from search for both SI and Proof Only variants; search yields the largest gains on depth-5 PW and EB Task 2 (Tables 1 and 2). Analyses present example high/low scoring traces (Section D).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Faithful Reasoning Using Large Language Models', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3522.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3522.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EntailmentWriter + Answer (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EntailmentWriter (Dalvi et al.) extended to predict answer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline entailment-tree model from Dalvi et al. (2021) fine-tuned to output an entailment tree and then the final answer; used as a non-causal baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Explaining answers with entailment trees.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>EntailmentWriter + Answer (fine-tuned LM, 7B Chinchilla in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Originally an entailment-tree model trained to predict entailment trees given hypothesis and context; in this paper adapted (fine-tuned) to produce answer after final entailment. In the experiments here all models (including this baseline) use 7B Chinchilla LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>EntailmentBankQA and Proof Writer (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Predict entailment trees and answers for multi-step deductive QA; however, its answering component has direct access to the question and can 'cheat' by relying on model priors.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Baseline: fine-tune an LM to predict full entailment trees and then the answer (no explicit causal separation between selection, inference, and answer).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Proof Writer overall: EntailmentWriter + Answer = 53.5% (Table 1). EntailmentBankQA Task 1 = 50.0%, Task 2 = 35.0% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Compared to SI, EntailmentWriter performs substantially worse on final answer accuracy (e.g., 53.5% vs SI 88.1% on PW). EntailmentWriter sometimes avoids hallucination but can still produce non-valid traces for QA because answer generation has access to the question.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Allows the answer stage direct access to the question and so can 'cheat' (use model priors rather than the produced trace), leading to lack of trace faithfulness; overall lower final-answer accuracy, especially on tasks with distractors; poor selection/leaf accuracy compared to SI (Figures 8 and 9).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper analyses demonstrate EntailmentWriter can produce good intermediate outputs but tends to fail at selecting correct supporting statements and relies more on model priors (Table 3, Figures 17 and 9).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Faithful Reasoning Using Large Language Models', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3522.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3522.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Proof + Answer (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Proof + Answer baseline LM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline LM fine-tuned to predict the entire proof (intermediate inferences) followed by the final answer in one sequence; used to compare end-to-end models against SI.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Proof + Answer baseline (7B Chinchilla fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B Chinchilla LM fine-tuned end-to-end to generate the full proof and then the final answer given context and question (the answer stage has access to the question).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Proof Writer and EntailmentBankQA (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>End-to-end generation of proofs and answers for multi-step logical QA; no enforced causal separation between selection/inference/answering.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Fine-tuning on full proof+answer sequences from training data (standard end-to-end approach).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Proof Writer overall final-answer accuracy: Proof + Answer = 65.4% (Table 1). On EntailmentBankQA: Proof + Answer Task1 = 64.6%, Task2 = 7.8% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>SI outperforms Proof + Answer by large margins (e.g., PW overall 88.1% vs 65.4%). Proof+Answer sometimes produces plausible intermediate inferences but frequently selects incorrect supporting statements and hallucinates on EB (Table 5 shows up to 60% made-up facts on EB).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Prone to hallucination (making up facts) especially on EntailmentBankQA (up to ~60% of problems), traces are not guaranteed connected and answers can depend on model priors rather than the provided context; performs very poorly on EB Task 2 with distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>When paired with Halter and Search (Proof Only + Halter + Search), performance improves, showing that search and halting help even end-to-end baselines, but such models still lack the faithfulness guarantees of SI.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Faithful Reasoning Using Large Language Models', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Explaining answers with entailment trees. <em>(Rating: 2)</em></li>
                <li>Proofwriter: Generating implications, proofs, and abductive statements over natural language. <em>(Rating: 2)</em></li>
                <li>Natural language deduction through search over statement compositions <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 1)</em></li>
                <li>STAR: Bootstrapping reasoning with reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3522",
    "paper_id": "paper-f0a0e8b6e84207f50db4d24cc4016e40601214ef",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "SI (Selection-Inference)",
            "name_full": "Selection-Inference faithful reasoning backbone",
            "brief_description": "A forward-chaining architecture that splits each reasoning step into a Selection model (chooses statements from context) and an Inference model (predicts an entailment from that selection), producing connected, human-interpretable reasoning traces and preventing the answer from depending directly on the question.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Selection-Inference (SI) pipeline using Chinchilla LMs",
            "model_description": "Pipeline composed of separate fine-tuned 7B-parameter Chinchilla LMs for Selection and Inference; the Selection LM predicts sentence labels from the context, and the Inference LM predicts an entailment given only the selected statements (no access to the question). SI is combined with a Halter LM and a learned Value LM for beam search.",
            "model_size": "7B",
            "reasoning_task_name": "Proof Writer (PW) and EntailmentBankQA (EB)",
            "reasoning_task_description": "PW: synthetic logical deduction problems with 1/2/3/5-step proofs and True/False/Unknown answers; EB (reformulated to EntailmentBankQA): grade-school science QA problems with context, multiple choices and a ground-truth entailment tree (Task 1: relevant facts only; Task 2: includes distractors). Tasks require multi-step deductive/entailment reasoning over provided context.",
            "method_or_intervention": "Architectural constraint enforcing causal structure: separation of Selection and Inference LMs (Inference denied access to the question), selection by sentence-labels to forbid hallucination, a Halter LM to decide when to stop and produce answers, and a learned Value LM to guide beam search over traces.",
            "performance": "Proof Writer overall final-answer accuracy: SI + Halter + PW Search = 88.1% (Table 1). Proof Writer per-depth: depth-1 99.4%, depth-2 98.1%, depth-3 92.0%, depth-5 63.4% (SI + Halter + PW Search). EntailmentBankQA: SI + Halter + Search: Task 1 = 83.2%, Task 2 = 72.9% (Table 2).",
            "baseline_performance": "Proof Writer: best baseline Proof + Answer = 65.4% overall (Table 1); EntailmentBankQA: Proof + Answer = 64.6% Task 1, 7.8% Task 2; EntailmentWriter + Answer = 50.0% Task 1, 35.0% Task 2 (Table 2).",
            "improvement_over_baseline": "Substantial improvements: on Proof Writer overall +~22.7 percentage points vs Proof + Answer (88.1% vs 65.4%). On PW depth-5 improvement is large (63.4% vs 60.4% for Proof+Answer in some runs, or vs Proof Only ablations). On EntailmentBankQA Task 2 SI+Search reaches 72.9% vs Proof+Answer 7.8% (very large gain), and vs EntailmentWriter+Answer 35.0%.",
            "limitations_or_failures": "Correctness of individual inference steps is not guaranteed — Inference LM can still produce incorrect entailments if Selection provides unrelated statements; SI assumes access to a sufficient context (retrieval not integrated); performance still drops for deeper proofs (depth-5) and for the hardest EB examples; halting may return 'Unknown' and model refrains from answering many problems (e.g., SI+Halter returns Unknown often on PW incomplete-context ablation).",
            "ablation_or_analysis": "Ablations show search (Value LM guided beam search) and the Halter both improve final-answer accuracy: SI + Halter + Search outperforms SI + Halter without search (Tables 1 and 2). Analyses show SI produces far fewer hallucinated facts (&lt;1%) than baselines and leverages context and its trace more (Tables 3,4,5,6). Inference accuracy when given valid selections is very high (ProofWriter inference accuracy ~99.9–100%, Table 7), indicating the modular design helps step correctness when selection is valid.",
            "uuid": "e3522.0",
            "source_info": {
                "paper_title": "Faithful Reasoning Using Large Language Models",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Selection LM",
            "name_full": "Selection language model (component of SI)",
            "brief_description": "Fine-tuned LM that, given question and current context, selects supporting statements by predicting sentence labels (e.g., 'sent 3'), ensuring selections come only from the provided context and preventing hallucinated facts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Selection LM (7B Chinchilla)",
            "model_description": "A 7B-parameter Chinchilla LM fine-tuned to output sentence labels that refer to context statements; outputs are used to compose 'We know that sent X and sent Y...' inputs for the Inference LM.",
            "model_size": "7B",
            "reasoning_task_name": "Proof Writer and EntailmentBankQA (used to produce selection steps)",
            "reasoning_task_description": "Selects the subset of context sentences required at each reasoning step (the 'leaves' / supporting facts) for multi-step logical deduction.",
            "method_or_intervention": "Fine-tuning on ground-truth selection labels; syntactic constraint to only output labels (rather than free-text), then re-substituting referenced sentences for inference input.",
            "performance": "Selection accuracy evaluated via Jaccard and leaf similarity metrics: SI models achieve higher Jaccard similarity for leaves than baselines on PW and EB (Figures 8 and 9); explicit numeric leaf/jaccard scores vary by depth/task (figures).",
            "baseline_performance": "Baseline models (Proof+Answer, EntailmentWriter) have lower selection (leaf) similarity (Figures 8a and 9a); exact numeric baselines vary by task and depth (figures).",
            "improvement_over_baseline": "Selection LM yields notably better selection (leaf Jaccard) than baseline end-to-end models, contributing to higher-quality, connected traces and downstream answer accuracy.",
            "limitations_or_failures": "Selection model is non-deterministic and can produce syntactically incorrect label outputs (rare failure mode causing apparent 'made-up facts' when label parsing fails). It can also choose incorrect supporting statements, which then cascades to incorrect inferences.",
            "ablation_or_analysis": "Analyses show the selection step is crucial: SI's advantage over baselines is partly because Selection avoids hallucination and selects correct supporting statements more frequently (Figures 8 and 9). Beam search over selection candidates guided by the Value LM improves overall trace quality.",
            "uuid": "e3522.1",
            "source_info": {
                "paper_title": "Faithful Reasoning Using Large Language Models",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Inference LM",
            "name_full": "Inference language model (component of SI)",
            "brief_description": "Fine-tuned LM that, given only the selected context statements (no access to the question), predicts an entailment (inference) that follows logically from those statements.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Inference LM (7B Chinchilla)",
            "model_description": "A 7B-parameter Chinchilla LM fine-tuned on (selection -&gt; entailment) pairs from ground-truth proofs; trained to produce valid next-step inferences from selected premises while being denied question input to prevent cheating.",
            "model_size": "7B",
            "reasoning_task_name": "Proof Writer and EntailmentBankQA (inference generation)",
            "reasoning_task_description": "Generates the next-step logical consequence (an entailment) given a set of premises selected from the context; these inferences are appended to the context and iterated.",
            "method_or_intervention": "Fine-tuning on ground-truth selection-&gt;inference training pairs; restricted input (no question) to force inference to depend only on selection.",
            "performance": "High inference accuracy when fed valid selections: ProofWriter inference accuracy ~100% for depth-1/2/3 and 99.9% for depth-5 (exact string-match, Table 7). EntailmentBankQA ROUGE-L = 0.69, ROUGE1 = 0.69, BLEURT&gt;0.28 = 64% (Table 8).",
            "baseline_performance": "Baselines that predict intermediate outputs (Proof+Answer, EntailmentWriter) often achieve good intermediate-inference quality but can do so while selecting incorrect supports or cheating — making their traces less valid overall (Figures 17,9).",
            "improvement_over_baseline": "By training a dedicated Inference LM that lacks access to the question, the paper reports more faithful inferences that, combined with selection restrictions, produce more valid traces and higher final-answer accuracy compared to baseline end-to-end models.",
            "limitations_or_failures": "Inference correctness depends heavily on the quality of the Selection output; if selection contains unrelated or wrong premises the Inference LM can produce nonsensical or incorrect statements. The approach cannot guarantee logical correctness for every step.",
            "ablation_or_analysis": "Feeding the inference model with ground-truth (valid) selections yields near-perfect inference accuracy (Table 7), indicating that Selection quality is the primary bottleneck for end-to-end trace correctness.",
            "uuid": "e3522.2",
            "source_info": {
                "paper_title": "Faithful Reasoning Using Large Language Models",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Halter",
            "name_full": "Halter (halt model for termination and answering)",
            "brief_description": "A two-stage LM component that decides after each SI step whether the current inference suffices to answer the question, and if so, produces the final answer in a way that forces dependence on the trace rather than model priors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Halter LM (7B Chinchilla)",
            "model_description": "A 7B-parameter Chinchilla LM fine-tuned on examples derived from ground-truth proofs to (1) predict 'Yes'/'No' to 'Do you know the answer given {inference}?', and (2) produce the final answer when appropriate; same LM used for both functions with differing prompts.",
            "model_size": "7B",
            "reasoning_task_name": "Proof Writer and EntailmentBankQA (halting and answer formatting)",
            "reasoning_task_description": "Determines when to stop iterative reasoning and returns the answer in the required format (e.g., True/False or multiple-choice choice), forcing use of final inference as evidence.",
            "method_or_intervention": "Fine-tuning on halting examples constructed from ground-truth traces; two-stage prompting to first ask if the answer is known and then to select an answer if it is.",
            "performance": "Applied to ground-truth proofs Halter accuracy: PW halter performs almost perfectly; EB halter achieves 88.8% accuracy (Section 4.3). Filtering to only 'known' problems yields near-perfect accuracy on PW and ~87.5%/83.7% on EB Task 1/2 (Figure 6).",
            "baseline_performance": "Baseline models lack a halting mechanism and cannot reliably say 'Unknown'; ground-truth-proof+Halter yields 100% on PW (Tables 1 and 2). Proof Only + Halter without search shows lower final-answer accuracy compared to SI+Halter+Search.",
            "improvement_over_baseline": "Halter enables the system to abstain (output 'Unknown'), increasing precision: filtering out problems Halter deems unknown boosts reported accuracies significantly (Figure 6). Combined with SI and search, Halter contributes to higher final-answer accuracy vs baselines.",
            "limitations_or_failures": "Halter is trained on traces and thus may be overconfident or fail to detect insufficiency in settings unlike training; performance on EB is imperfect (88.8% on ground-truth proofs), and halting introduces an abstention tradeoff (precision vs recall).",
            "ablation_or_analysis": "Ablations show SI+Halter+Search outperforms SI without Search and that using Halter to filter increases precision markedly (Figure 6, Tables 1 and 2).",
            "uuid": "e3522.3",
            "source_info": {
                "paper_title": "Faithful Reasoning Using Large Language Models",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Value LM + Search",
            "name_full": "Learned value function (LM) used to guide beam search over traces",
            "brief_description": "A fine-tuned LM that scores partial reasoning traces as 'correct' or 'incorrect' (trained on positive/negative partial traces) and whose log-probability of 'correct' is used as a value to guide beam search over candidate reasoning traces, improving trace quality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Value LM (7B Chinchilla) + Beam Search",
            "model_description": "7B Chinchilla LM fine-tuned to predict whether the current step in a partial reasoning trace is 'correct' or 'incorrect'; used to score and prune candidate traces in a step-level beam search (keep top B traces each step).",
            "model_size": "7B",
            "reasoning_task_name": "Proof Writer and EntailmentBankQA (search over candidate traces)",
            "reasoning_task_description": "Guides exploration of the tree of non-deterministic selection/inference outcomes to find high-value reasoning traces that lead to correct final answers.",
            "method_or_intervention": "Train Value LM on positive examples (ground-truth partial traces) and negative examples (swap a supporting statement and re-run inference) and use log p_value('correct'|trace) to score traces during beam search.",
            "performance": "Enabling search (Value LM guided beam search) improves final-answer accuracy for both SI and baseline Proof Only models. Example: SI+Halter improves from 78.3% to 88.1% overall on PW when using PW Search (Table 1); similar gains observed on EB Task 2.",
            "baseline_performance": "Without search, SI + Halter performs worse (e.g., PW overall SI+Halter = 78.3% vs SI+Halter+PW Search = 88.1%). Proof Only + Halter + Search also improves relative to no-search ablations (Tables 1 and 2).",
            "improvement_over_baseline": "Search provides the most significant improvement on deeper problems and contexts with distractors (e.g., PW depth-5 and EB Task 2). Quantitatively, search often yields double-digit percentage improvements on the hardest subsets.",
            "limitations_or_failures": "Value LM's training depends on the quality of generated negative examples (random replacements); value estimates are imperfect and cannot guarantee pruning of all incorrect traces. Beam search increases compute and may still miss correct traces if beam size or value function are miscalibrated.",
            "ablation_or_analysis": "Ablation tables show consistent benefit from search for both SI and Proof Only variants; search yields the largest gains on depth-5 PW and EB Task 2 (Tables 1 and 2). Analyses present example high/low scoring traces (Section D).",
            "uuid": "e3522.4",
            "source_info": {
                "paper_title": "Faithful Reasoning Using Large Language Models",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "EntailmentWriter + Answer (baseline)",
            "name_full": "EntailmentWriter (Dalvi et al.) extended to predict answer",
            "brief_description": "Baseline entailment-tree model from Dalvi et al. (2021) fine-tuned to output an entailment tree and then the final answer; used as a non-causal baseline in experiments.",
            "citation_title": "Explaining answers with entailment trees.",
            "mention_or_use": "use",
            "model_name": "EntailmentWriter + Answer (fine-tuned LM, 7B Chinchilla in experiments)",
            "model_description": "Originally an entailment-tree model trained to predict entailment trees given hypothesis and context; in this paper adapted (fine-tuned) to produce answer after final entailment. In the experiments here all models (including this baseline) use 7B Chinchilla LMs.",
            "model_size": "7B",
            "reasoning_task_name": "EntailmentBankQA and Proof Writer (used as baseline)",
            "reasoning_task_description": "Predict entailment trees and answers for multi-step deductive QA; however, its answering component has direct access to the question and can 'cheat' by relying on model priors.",
            "method_or_intervention": "Baseline: fine-tune an LM to predict full entailment trees and then the answer (no explicit causal separation between selection, inference, and answer).",
            "performance": "Proof Writer overall: EntailmentWriter + Answer = 53.5% (Table 1). EntailmentBankQA Task 1 = 50.0%, Task 2 = 35.0% (Table 2).",
            "baseline_performance": null,
            "improvement_over_baseline": "Compared to SI, EntailmentWriter performs substantially worse on final answer accuracy (e.g., 53.5% vs SI 88.1% on PW). EntailmentWriter sometimes avoids hallucination but can still produce non-valid traces for QA because answer generation has access to the question.",
            "limitations_or_failures": "Allows the answer stage direct access to the question and so can 'cheat' (use model priors rather than the produced trace), leading to lack of trace faithfulness; overall lower final-answer accuracy, especially on tasks with distractors; poor selection/leaf accuracy compared to SI (Figures 8 and 9).",
            "ablation_or_analysis": "Paper analyses demonstrate EntailmentWriter can produce good intermediate outputs but tends to fail at selecting correct supporting statements and relies more on model priors (Table 3, Figures 17 and 9).",
            "uuid": "e3522.5",
            "source_info": {
                "paper_title": "Faithful Reasoning Using Large Language Models",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Proof + Answer (baseline)",
            "name_full": "Proof + Answer baseline LM",
            "brief_description": "Baseline LM fine-tuned to predict the entire proof (intermediate inferences) followed by the final answer in one sequence; used to compare end-to-end models against SI.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Proof + Answer baseline (7B Chinchilla fine-tuned)",
            "model_description": "7B Chinchilla LM fine-tuned end-to-end to generate the full proof and then the final answer given context and question (the answer stage has access to the question).",
            "model_size": "7B",
            "reasoning_task_name": "Proof Writer and EntailmentBankQA (baseline)",
            "reasoning_task_description": "End-to-end generation of proofs and answers for multi-step logical QA; no enforced causal separation between selection/inference/answering.",
            "method_or_intervention": "Fine-tuning on full proof+answer sequences from training data (standard end-to-end approach).",
            "performance": "Proof Writer overall final-answer accuracy: Proof + Answer = 65.4% (Table 1). On EntailmentBankQA: Proof + Answer Task1 = 64.6%, Task2 = 7.8% (Table 2).",
            "baseline_performance": null,
            "improvement_over_baseline": "SI outperforms Proof + Answer by large margins (e.g., PW overall 88.1% vs 65.4%). Proof+Answer sometimes produces plausible intermediate inferences but frequently selects incorrect supporting statements and hallucinates on EB (Table 5 shows up to 60% made-up facts on EB).",
            "limitations_or_failures": "Prone to hallucination (making up facts) especially on EntailmentBankQA (up to ~60% of problems), traces are not guaranteed connected and answers can depend on model priors rather than the provided context; performs very poorly on EB Task 2 with distractors.",
            "ablation_or_analysis": "When paired with Halter and Search (Proof Only + Halter + Search), performance improves, showing that search and halting help even end-to-end baselines, but such models still lack the faithfulness guarantees of SI.",
            "uuid": "e3522.6",
            "source_info": {
                "paper_title": "Faithful Reasoning Using Large Language Models",
                "publication_date_yy_mm": "2022-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Explaining answers with entailment trees.",
            "rating": 2
        },
        {
            "paper_title": "Proofwriter: Generating implications, proofs, and abductive statements over natural language.",
            "rating": 2
        },
        {
            "paper_title": "Natural language deduction through search over statement compositions",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 1
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 1
        },
        {
            "paper_title": "STAR: Bootstrapping reasoning with reasoning",
            "rating": 1
        }
    ],
    "cost": 0.01676125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Faithful Reasoning Using Large Language Models</h1>
<p>Antonia Creswell ${ }^{1}$ and Murray Shanahan ${ }^{1}$<br>${ }^{1}$ DeepMind</p>
<h4>Abstract</h4>
<p>Although contemporary large language models (LMs) demonstrate impressive question-answering capabilities, their answers are typically the product of a single call to the model. This entails an unwelcome degree of opacity and compromises performance, especially on problems that are inherently multi-step. To address these limitations, we show how LMs can be made to perform faithful multi-step reasoning via a process whose causal structure mirrors the underlying logical structure of the problem. Our approach works by chaining together reasoning steps, where each step results from calls to two fine-tuned LMs, one for selection and one for inference, to produce a valid reasoning trace. Our method carries out a beam search through the space of reasoning traces to improve reasoning quality. We demonstrate the effectiveness of our model on multi-step logical deduction and scientific question-answering, showing that it outperforms baselines on final answer accuracy, and generates humanly interpretable reasoning traces whose validity can be checked by the user.</p>
<p>Keywords: Reasoning, Causality, Large Lanauge Models</p>
<h2>1. Introduction</h2>
<p>Among the many tasks that contemporary large language models (LMs) can perform (Alayrac et al., 2022; Nakano et al., 2021; Zeng et al., 2022), question-answering is potentially one of the most useful (Rae et al., 2021). However, the proficiency of these models typically goes hand-inhand with an unacceptable level of opacity. The assumptions behind an answer and the intermediate steps of reasoning that justify it - insofar as these exist - are hidden from the user. This prevents the user from verifying an answer, makes it difficult to debug a model when it gets an answer wrong, and undermines overall trust in the model's responses.</p>
<p>By contrast, a system that reasons faithfully is one whose underlying computations mirror standard definitions of logical validity. Such a system can supply the user with an interpretable reasoning trace, which allows them to understand how the model reached its final answer. Exposing a model's assumptions and reasoning steps (Figure 1) in this way enables the user to spot mistakes the model may have made, and empowers them to decide for themselves whether the model's conclusions are justified.</p>
<p>This provision is especially important given that LMs are trained on human data collected from the internet, which makes them vulnerable to picking up and perpetuating bias (Bender et al., 2021; Betz et al., 2021; Weidinger et al., 2021). Presented with a context of relevant knowledge and a question, an LM may base its answer on information encoded in its weights rather than prioritising the information present in the context (Dasgupta et al., 2022). Without an interpretable reasoning trace, we cannot know how a model has reached its answer. Did the model rely on its priors, which may be biased, or did it obtain an answer by reasoning correctly with relevant knowledge?</p>
<p>In this paper we develop a forward-chaining model that reasons faithfully in the sense defined above (and more formally in Section 2). The backbone of our system, denoted SI, comprises two fine-tuned LMs, one for selection and one for inference. The interleaved operation of these two components has a causal structure (Figure 2) that mirrors the definition of logical validity. This guarantees that the model's answers follow logically from the given context under certain assumptions.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1 | Example input and output from our Faithful Reasoning model.</p>
<p>Two further fine-tuned language models complete our architecture. First, the halter is used to terminate the reasoning process and return an answer in the required format. If the trace does not terminate within a specified number of steps then the answer is considered to be 'Unknown', allowing us to filter model answers and increase answer precision. Second, a learned value function, which assesses the quality of the current reasoning step is deployed to guide a beam search over reasoning traces to enhance their quality and further boost overall performance.</p>
<p>We evaluate our model on two datasets, Proof Writer (Tafjord et al., 2021) and a questionanswering version of EntailmentBank (Dalvi et al., 2021). We shown that our model outperforms baseline models on final answer accuracy and that our proposed halter and search methods also lead to compounding boosts in performance (Tables 2 and 1). We show that in most cases SI produces higher quality reasoning traces than baseline models (Figures 9 and 8). It is less likely to "hallucinate" facts (Table 5), is better able to utilise the context (Table 3) and is more likely to use its trace to answer questions (Table 4). Finally, our model can accurately predict when it knows the answer (Figure 6).</p>
<h2>2. Defining a Valid Reasoning Trace</h2>
<p>In this Section, we formally define the concept of valid forward reasoning in the context of our
framework, adhering closely to textbook definitions from formal logic (e.g. Hamilton (1988)).</p>
<p>Definition 1. A reasoning step is a pair $\langle s, i\rangle$, where $s$ (the selection) is a set of statements and $i$ (the inference) is a statement.</p>
<p>Definition 2. A reasoning trace is a pair $\langle\mathcal{C}, \mathcal{T}\rangle$ where $\mathcal{C}$ (the context) is a set of statements and $\mathcal{T}$ is a sequence of reasoning steps.</p>
<p>Definition 3. A reasoning trace $\langle\mathcal{C}, \mathcal{T}\rangle$, where $\mathcal{T}=$ $\left\langle s_{0}, i_{0}\right\rangle,\left\langle s_{1}, i_{1}\right\rangle, \ldots\left\langle s_{n}, i_{n}\right\rangle$, is connected iff for every reasoning step $\left\langle s_{k}, i_{k}\right\rangle$, for every statement $q$ in the set $s_{k}$ either $q \in \mathcal{C}$ or $q=i_{j}$ for some $j&lt;k$.</p>
<p>Definition 4. A reasoning trace $\langle\mathcal{C}, \mathcal{T}\rangle$, where $\mathcal{T}=$ $r_{0}, r_{1}, \ldots, r_{n}$, is valid if it is connected and each reasoning step $r_{k}=\langle s, i\rangle$ is correct (in the sense that $i$ logically follows from $s$ ).</p>
<p>In the next section, we introduce the components of our architecture and show how it satisfies the requirements of faithful reasoning, under certain assumptions.</p>
<h2>3. Components of a Faithful Reasoning Model</h2>
<p>We begin by introducing Selection-Inference (SI), the step-wise forward reasoning backbone whose causal structure (see Figure 2) satisfies the requirements for producing valid reasoning traces. We then describe a component for halting, which</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2 | Comparing dependencies between inputs and outputs for SI and related models. Inputs - blue circles, LM outputs - purple circles. Order of the letters indicates the order in which the values are predicted. Arrows indicate the dependencies between inputs - the context, C, and question, Q - intermediate outputs - the selection, S, and inference, I, and the final answer, A. SI is the only model where the answer does not have a direct dependency on the question. Note, EntailmentWriter takes the hypothesis and context as input, where the hypothesis depends on the question and answer.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3 | Faithful Reasoning architecture. See Section 3 for details of each component.
looks at the output of a Selection-Inference step and determines if there is sufficient information to answer the question. When there is sufficient information, the model predicts the answer in such a way that it cannot rely on knowledge embedded in its weights, but must depend on the reasoning trace. Finally, we introduce a value function, which is used to perform a step-level beam search on the reasoning traces to find the best candidate for answering the question. A schematic of our model is shown in Figure 3. We now describe each of these components in more detail. Note that (in contrast to Dalvi et al. (2021)) each component in our model is trained in isolation, and at no point do we optimise our pipeline for final answer accuracy.</p>
<h3>3.1. Selection-Inference: Valid Forward Reasoning</h3>
<p>Given a question and a context consisting of a number of statements sufficient to answer the question, we would like our model to produce a sequence of deductive reasoning steps that answers the question (Figure 1). To achieve this, the SI backbone splits each reasoning step in two (Defn. 1). First, given the question, the Selection model chooses a set of statements from the context (the selection). Second, the Inference model predicts an entailment by computing a statement that follows from the selection (the inference). The inference is then added to the context, and that concludes a single step of reasoning. Multiple iterations of SI are carried out to produce a reasoning trace (Defn. 2). The final inference is used to answer the question.</p>
<h3>3.1.1. Selection</h3>
<p>To ensure that the reasoning trace is connected (Defn. 3), the Selection model is obliged to select elements only from the context, and is unable to 'hallucinate' facts. Similar to Tafjord et al. (2021) and Dalvi et al. (2021), we achieve this by training an LM to refer to statements in the context by their sentence labels, for example, 'sent 3'. These are used to compose sentences of the form "X. We know that Y and ... and Z.", where X, Y, and Z are sentence labels (Figure 4). These sentences are passed directly to the inference model.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure $4 \mid$ The Selection model. The role of the Selection model is to take the context and question and select a number of statements from the context to feed to the inference model. It is crucial that the Selection model is not able to 'hallucinate' facts. To achieve this we fine-tune a LM to predict sentence labels, as show in (i). We then extract the only the sentence labels (ii) and compose a sentence (iii). The statements from the context are then substituted back in (iv) resulting in a sentence composed of statements from the context.</p>
<h3>3.1.2. Inference</h3>
<p>To encourage it to produce correct reasoning steps, the Inference model is trained to predict an entailment given only the selection. By not allowing the Inference model access to the question, we prevent it from "cheating" (directly predicting the answer from the question). While we cannot guarantee that every reasoning step is correct, in the sense that the inference logically follows from the selection (Defn. 4), our implementation makes this more likely. Under the assumption that the Inference model produces logically correct inferences, our model is guaranteed to produce valid reasoning traces.</p>
<h3>3.2. Halting: When to Stop Reasoning?</h3>
<p>SI allows us to produce multi-step reasoning traces, but it does not tell us when to stop the
reasoning process. Furthermore, while we may want to use the final inference as the answer, it may not be in a desirable format. For example, if we are asked whether a statement ' $P(X)$ ' is true or false, our final inference my be ' $P(X)$ ' or 'not $P(X)$ ' where $P$ is a predicate and $X$ a constant. Alternatively, we may want to answer multiple-choice questions, which require one answer to be output from a given set of possibilities.</p>
<p>In light of this, we deploy a two-stage Halter (Figure 5), which uses an LM fine-tuned to predict whether the question can be answered given the current inference and the question. If the question cannot be answered, 'Unknown' is returned. Otherwise, the Halter computes an answer, using the same LM, given the final inference and minimal additional information. It is important that the model is obliged to use the final inference, rather than depend on knowledge embed-</p>
<p>ded in its weights. For example, if we are answering a multiple-choice question, we may provide the choices alongside the final inference, and use the model to output the choice that most closely matches that inference.</p>
<p>To determine if the system is ready to answer the question, we provide the Halter with a sentence of the following form: 'Question: {question} Given {inference}. Do you know the answer?'. The output of the Halter LM is then either 'Yes' or 'No'. If the output is 'Yes', the Halter LM is then prompted again to answer the question with a prompt of the following form: 'Given {inference}. Which of the following most closely matches: {choices}? Answer:'. The output is one of the choices.</p>
<p>The Halter is applied after each step of SI to the resulting inference. If the output of the Halter is 'Unknown' then we proceed to another iteration of SI. If the output of the Halter is an answer, then the process is terminated and the answer is returned. If, after a pre-specified number of SI iterations, the system has not halted, it returns the answer 'Unknown' (Alg. 2). An additional benefit of this is that it allows the model to say that it cannot answer the question, rather than making up an answer. We see a notable increase in performance when we remove questions that the model "thinks" it cannot answer (Figure 6). This has significant implications for trust, safety and the deployment of systems in the real world, where precision (rather than recall) is a priority.</p>
<h3>3.3. Search: Finding the Best Trace</h3>
<p>The selection module is non-deterministic, in the sense that it samples from multiple candidate statements, and this induces a tree of potential reasoning traces. We use beam search to explore this tree in order to find high quality traces. To enable this, we introduce a value function which computes the value of adding a reasoning step to the current trace. The value function is a language model, $\mathrm{LM}<em _text="\text" _value="{value">{\text {value }}$, fine-tuned on examples of partial reasoning traces that culminate in a "correct" or "incorrect" next step. A step is considered "correct" if it is both logically valid and is on the ground truth (shortest) reasoning path. A step is
otherwise considered "incorrect".
Assuming that the sum of probabilities for "correct" and "incorrect" is close to one, we can use $\log p</em>$.}}$ ("correct"|reasoning trace) to score reasoning traces as they are being constructed, where $\log p_{\text {value }}$ denotes the distribution over tokens learned by the language model, $\mathrm{LM}_{\text {value }</p>
<p>We use the value function to guide a beam search. Starting from a single empty trace we use SI to produce $P$ candidate steps. We evaluate each of these steps using the value function and keep the top $B&lt;=P$. We use SI again to generate $P$ candidate next steps for each of the $B$ traces, resulting in $B \times P$ traces. These are evaluated using the value function and the best $B$ traces are kept. We continue this process until all the traces have halted.</p>
<h2>4. Experimental Setup and Evaluation of Components</h2>
<p>In this section, we detail how each component in our faithful reasoning model is trained, and evaluate each component is isolation, where possible. We use two challenging reasoning datasets, Proof Writer (Tafjord et al., 2021) and a modified - more challenging - question-answering version of EntailmentBank (Dalvi et al., 2021). We use a 7B parameter Chinchilla language model in each of our components (Hoffmann et al., 2022).</p>
<h3>4.1. Datasets</h3>
<p>We fine-tune language models on examples of ground truth reasoning traces. Two datasets that provide reasoning traces are EntailmentBank (Dalvi et al., 2021) and Proof Writer (PW) (Tafjord et al., 2021) (See Section B.1.1 for details). Proof Writer is a dataset of logical reasoning problems that ask a question whose answer is True or False given a context and provide step-bystep reasoning traces. Problems require 1, 2, 3 or 5 steps of reasoning. EntailmentBank is derived from the ARC (Clark et al., 2018) dataset of grade school science questions. Dalvi et al. (2021) provide a dataset of (context, hypothesis, entailment tree) triples. Dalvi et al. (2021) propose three tasks, Task 1 where the context consists of facts</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5 | The two-stage Halter. First the model determines if a question is answerable given the current inference. If it is, the model combines minimal additional information (that could not be used on its own to answer the question) and predicts the answer.
from WorldTreeV2 (Xie et al., 2020) needed to answer the question, and Task 2 that additionally includes distractors. EntailmentBank is not a QA dataset, rather the task requires predicting the entailment tree given the hypothesis and context. We reformulate the EntailmentBank dataset (taking additional information from the original ARC tasks) into an EntailmentBankQA (EB) dataset by creating a dataset of context, question, choices, answer and a proof derived from the entailment tree. Our task is to predict the answer and proof given the question, context and choices. This task is more similar to the ARC task, however, here we provide the context and predict a reasoning trace that leads to the answer.</p>
<h3>4.2. Selection-Inference</h3>
<p>The selection model is trained on individual steps of reasoning; given the context and any previous inferences the model is trained to predict the sentence labels which refer to statements in the context. The inference model is trained to predict an entailment given a number of statements from the context. Each reasoning step in each training example in the original dataset produces one training data point for selection and one training point for inference. Examples of 〈input, target〉 pairs used to train the LMs are shown in Figures 11 and 12. By training the model to select statements by labels we prevent the model from being able to make up facts that are not present in the context (Tables 6 and Table 5). Tables 7 and 8 show the inference accuracy on the test set.</p>
<h3>4.3. Halter</h3>
<p>We use the ground truth reasoning traces from each dataset to produce training examples for the Halter LM. The halter has two functions, (1) learn when there is sufficient information to answer the question given the current inference (and the question) and (2) answer the question given the current inference and the choices. An example of how data is generated is shown in Figure 13. Each step of reasoning in each problem can be converted into a data point for training. The input has the form 'Question: {question}. Given {inference}. Do you know the answer?'. For intermediate reasoning steps the target is 'No.'. For final reasoning steps the target is 'Yes.'. From these examples, the model can learn whether an inference contains sufficient information to answer the question. We obtain an additional data point for each problem, which is used to train the model to answer a question. The inputs are of the form 'Given {inference}. Which of these most closely matches {choices}?'. The target is the ground truth answer given in the dataset.</p>
<p>We train two halters, one on the PW dataset and the another on the EB dataset. For the PW dataset we use a simplified single step prediction because the question does not contain sufficient information to solve the problem ${ }^{1}$. Specifically, for PW we construct a training dataset where the input has the form 'Given {inference}. {question}'. For each intermediate inference the target is '</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Unknown' while the final inference has the target ' True' or ' False'.</p>
<p>To evaluate each halter independently of the Proof Only baseline or SI model, it is applied it to the ground truth proofs from the test split. Tables 1 and 2 show results for PW and EB respectively. We see that the PW halter performs almost perfectly while the EB halter achieves $88.8 \%$ accuracy.</p>
<p>The Halter endows our model with the desirable property of predicting when it does not know the answer. Figure 6 shows that our halter model can reliably predict when the answer is known. When we filter out the problems where the model does not know the answer, we obtain nearly perfect accuracy on the PW dataset for all depths and $87.5 \%$ \&amp; $83.7 \%$ accuracy on Task 1 and 2 of EB dataset respectively. This has significant implications for the deployment of such models in scenarios where precision matters.</p>
<h3>4.4. Search</h3>
<p>The Value LM is trained to predict whether the current step of a reasoning trace is ' correct' or ' incorrect'. Again, we use the ground truth reasoning traces to construct examples of correct and incorrect partial reasoning traces. Constructing the correct examples is simple; we take a ground truth trace with $N$ steps and construct the following input for all $n \in[1,2, \ldots, N]$, 'Context: {context} Question:{question} Reason:{reason[1:n]} The above reasoning steps are'. The target is ' correct' for all of these examples. To create the negative examples we take each positive example and replace one of the correct supporting statements with a different, randomly chosen statement from the context and use our Inference LM to predict the entailment. These training examples have the target ' incorrect'. Examples for both Proof Writer and EntailmentBank are shown in Figures 15 and 14.</p>
<h2>5. Experiments and Results</h2>
<p>We present results on both Proof Writer (PW) (Tafjord et al., 2021) and EntailmentBankQA (EB). We show that our model achieves $88.1 \%$
and $78.1 \%$ final answer accuracy on PW and EB respectively significantly outperforming baseline models (Table 1 and 2). We also perform an ablation to demonstrate the key role of search in our model (Table 1 and 2). Compared to baseline models, we show that our model often has higher reasoning trace accuracy; this is most evident on the more challenging tasks, for example PW depth-5 and EB Task 2 (Figure 8 and 9). Finally, we evaluate reasoning trace validity (Section 5.4) showing that baseline model are less likely to leverage the context when answering questions (Table 4 and 3) and are more likely to "hallucinate" statements than SI (Table 6 and 5). All results in this paper were obtained using 7B parameter Chinchilla language model models (Hoffmann et al., 2022).</p>
<h3>5.1. Baselines</h3>
<p>We consider three baseline models. A Proof + Answer baseline where the LM is trained to predict the whole proof followed by the answer. A Proof Only baseline where the model is trained to predict only the proof. We use the Proof Only baseline to ablate the SI model by pairing it with our halter and search methods (see Tables 2 and 1). Finally, we include EntailmentWriter + Answer. This is the entailment model of Dalvi et al. (2021), which is fine-tuned to predict an entailment tree alone, extended for question-answering by training the model to predict the answer after the final conclusion.</p>
<p>While EntailmentWriter + Answer and Proof + Answer tend to be very good at predicting the intermediate inferences (See Figures 17a and 9b) they tend to be less good at selecting the correct statements (see Figures 8a and 9a) and overall they perform less well on final answer accuracy (see Table 1 and 2). This suggests that the models are predicting the correct intermediate outputs, without selecting the correct supporting statements and that the models are unable to use the reasoning trace to answer the question. We also see that baseline models, with the exception of EntailmentWriter, often make-up facts when reasoning (see Table 5), suggesting that their traces are not connected and therefore are not valid (Defn. 3). Finally, baseline models leverage information</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6 | Our model accurately predicts when it 'knows' the answer. The 'known only' accuracy is computed after filtering out the answers that are 'Unknown'. The 'all' accuracy is computed on all problems. This property is beneficial for applications that require high precision.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7 | The value function. Given the context, question and a partial reasoning trace the model predicts the log probability that the current step is correct.
in the context less well than our model, see Table 3, and Table 4 suggests that on Proof Writer, SI is the only model to consistently leverage the reasoning trace to answer questions.</p>
<p>On inspection of EntailmentWriter (Dalvi et al., 2021) outputs on the Proof Writer dataset we see that the model often 'cheats', where the final inference helps to answer the question, but does not follow from the previously selected statements. See Section E.1. Our inference model does not have access to the question and therefore does not have the ability to cheat in this way.</p>
<h3>5.2. Final Answer Accuracy</h3>
<p>Tables 1 and 2 show final answer accuracy on the Proof Writer (PW) and EntailmentBankQA (EB) datasets respectively. Each table shows a comparison to baselines as well as an ablation; comparing both SI + Halter and the Proof Only + Halter baseline model with and without search. We see that SI outperforms EntailmentWriter</p>
<ul>
<li>Answer and Proof + Answer baseline models on all PW and EB tasks. We also show that search improves both baseline and SI performance, providing the most significant improvement for problems that require more reasoning steps (PW, depth-5) and on problems with distractors in the context (EB, Task 2)</li>
</ul>
<p>On the EB dataset we see that SI model + Halter + Search yields similar performance to Proof Only + Halter + Search while also providing faithful reasoning traces, which the Proof Only models do not. In fact, Table 5 shows that the Proof Only models are prone to hallucinating facts in up to $40 \%$ of problems, while SI has made up facts to only $1 \%$ of problems ${ }^{2}$. In the next section we look at reasoning trace accuracy.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Experiment</th>
<th style="text-align: center;">depth-1</th>
<th style="text-align: center;">depth-2</th>
<th style="text-align: center;">depth-3</th>
<th style="text-align: center;">depth-5</th>
<th style="text-align: center;">Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Entailment Writer (Dalvi et al., 2021) + Answer</td>
<td style="text-align: center;">$50.4 \%$</td>
<td style="text-align: center;">$55.3 \%$</td>
<td style="text-align: center;">$52.2 \%$</td>
<td style="text-align: center;">$56.0 \%$</td>
<td style="text-align: center;">$53.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Proof + Answer</td>
<td style="text-align: center;">$70.9 \%$</td>
<td style="text-align: center;">$65.0 \%$</td>
<td style="text-align: center;">$65.5 \%$</td>
<td style="text-align: center;">$60.4 \%$</td>
<td style="text-align: center;">$65.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Ground truth proof + Halter</td>
<td style="text-align: center;">$99.9 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Proof Only + Halter</td>
<td style="text-align: center;">$97.0 \%$</td>
<td style="text-align: center;">$93.1 \%$</td>
<td style="text-align: center;">$84.8 \%$</td>
<td style="text-align: center;">$44.6 \%$</td>
<td style="text-align: center;">$79.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Proof Only + Halter + EB Search</td>
<td style="text-align: center;">$99.2 \%$</td>
<td style="text-align: center;">$96.2 \%$</td>
<td style="text-align: center;">$91.4 \%$</td>
<td style="text-align: center;">$54.9 \%$</td>
<td style="text-align: center;">$85.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Proof Only + Halter + PW Search</td>
<td style="text-align: center;">$98.7 \%$</td>
<td style="text-align: center;">$96.0 \%$</td>
<td style="text-align: center;">$90.3 \%$</td>
<td style="text-align: center;">$56.8 \%$</td>
<td style="text-align: center;">$85.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">SI model + Halter</td>
<td style="text-align: center;">$98.3 \%$</td>
<td style="text-align: center;">$94.1 \%$</td>
<td style="text-align: center;">$82.4 \%$</td>
<td style="text-align: center;">$38.4 \%$</td>
<td style="text-align: center;">$78.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">SI model + Halter + EB Search</td>
<td style="text-align: center;">$\mathbf{9 9 . 4 \%}$</td>
<td style="text-align: center;">$98.0 \%$</td>
<td style="text-align: center;">$91.7 \%$</td>
<td style="text-align: center;">$61.7 \%$</td>
<td style="text-align: center;">$\mathbf{8 8 . 0 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">SI model + Halter + PW Search</td>
<td style="text-align: center;">$\mathbf{9 9 . 4 \%}$</td>
<td style="text-align: center;">$\mathbf{9 8 . 1 \%}$</td>
<td style="text-align: center;">$\mathbf{9 2 . 0 \%}$</td>
<td style="text-align: center;">$\mathbf{6 3 . 4 \%}$</td>
<td style="text-align: center;">$\mathbf{8 8 . 1 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 1 | Proof Writer ablation and comparison to baselines. Note that the baseline model does not produce faithful reasoning traces and has access to the question when answering. By contrast, in SI the reasoning is faithful and the answer depends on the reasoning trace. We show results using search with a value function trained on Proof Writer, PW Search, and with a value function trained on EntailmentBank, EB Search.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Task 1</th>
<th style="text-align: center;">Task 2</th>
<th style="text-align: left;">gests that these models are correctly predicting</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Ground truth proof + Halter</td>
<td style="text-align: center;">$88.8 \%$</td>
<td style="text-align: center;">$88.8 \%$</td>
<td style="text-align: left;">the intermediate outputs, but not via the correct</td>
</tr>
<tr>
<td style="text-align: left;">Proof + Answer</td>
<td style="text-align: center;">$64.6 \%$</td>
<td style="text-align: center;">$7.8 \%$</td>
<td style="text-align: left;">reasoning. Note that this evaluation does not</td>
</tr>
<tr>
<td style="text-align: left;">EntailmentWriter* + Answer</td>
<td style="text-align: center;">$50.0 \%$</td>
<td style="text-align: center;">$35.0 \%$</td>
<td style="text-align: left;">consider the ordering of the proof steps which</td>
</tr>
<tr>
<td style="text-align: left;">Proof Only + Halter</td>
<td style="text-align: center;">$78.5 \%$</td>
<td style="text-align: center;">$60.3 \%$</td>
<td style="text-align: left;">may be inflating the perceived performance of</td>
</tr>
<tr>
<td style="text-align: left;">Proof Only + Halter + Search</td>
<td style="text-align: center;">$82.9 \%$</td>
<td style="text-align: center;">$\mathbf{7 6 . 2 \%}$</td>
<td style="text-align: left;">the baseline models since the baseline models are</td>
</tr>
<tr>
<td style="text-align: left;">SI model + Halter</td>
<td style="text-align: center;">$72.4 \%$</td>
<td style="text-align: center;">$55.9 \%$</td>
<td style="text-align: left;">able to cheat by predicting later reasoning steps</td>
</tr>
<tr>
<td style="text-align: left;">SI model + Halter + Search</td>
<td style="text-align: center;">$\mathbf{8 3 . 2 \%}$</td>
<td style="text-align: center;">$72.9 \%$</td>
<td style="text-align: left;">without computing earlier reasoning steps.</td>
</tr>
</tbody>
</table>
<p>Table 2 | EntailmentBankQA ablation and comparison to baselines. Note that the baseline models are not causal. We use 7B parameter LMs for all models. *(Dalvi et al., 2021)</p>
<h3>5.3. Evaluating Reasoning Trace Accuracy</h3>
<p>Here we evaluate the reasoning trace accuracy of each model on the PW and EB datasets, see Figures 8, 17 and 9.</p>
<p>Evaluating reasoning trace accuracy on PW is straightforward since we are able to use exact string match to check whether two strings are the same. We show the Jaccard similarity between predicted and ground truth leaves (i.e the selection, Figure 8a), intermediate outputs (i.e. the inferences, Figure 17a) and steps (i.e. selection and inference, Figure 8b). Results show that SI had the highest Jaccard similarity for leaves and full traces while Proof + Answer and Entailment Writer + Answer have highest Jaccard similarity for intermediate outputs 17a. This sug-</p>
<p>Overall, on the EB dataset, we see that SI outperforms EntailmentWriter + Answer and Proof + Answer baselines on the more challenging task, Task 2, which has distractors in the context. Figure 9 shows the Jaccard similarity between predicted and ground-truth leaves (i.e. the selection) and the as well as the rouge scores between predicted and target intermediate outputs on the EB dataset (additional results in Figure 18).</p>
<p>Note that high baseline performance on the intermediate outputs (Figures 17 and 9) also suggests that the baseline models have been trained well and means that their poor final answer accuracy cannot be attributed to poor training but rather to the baseline models' inability to use the reasoning trace to answer the question.</p>
<h3>5.4. Trace Validity</h3>
<p>While requirements for Defn. 1-3 are satisfied by the causal structure of our underling model (Figure 2). Requirements of correctness, for Defn. 4,</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" />
(a) Jaccard similarity between the predicted and ground-truth selection, referred to as leaves, used to reason. We see that the SI models perform better than Baseline models.
<img alt="img-8.jpeg" src="img-8.jpeg" />
(b) Jaccard similarity between the predicted and ground-truth reasoning steps. We see that the SI models perform better than Baseline models on the more challenging Task 2.</p>
<p>Figure 8 | Evaluating proof steps for Proof Writer. We compute the above values only on problems where the model predicts that the answer is not "Unknown". Additional analysis in Figure 17.
are less strongly enforced. Never the less we show bellow that, unlike baseline models, our model is not able to cheat and therefore, the correctness assumption is more likely to hold. First, however, we demonstrate that while SI satisfies the requirement of being connected, other baseline models fail to do so.</p>
<h3>5.4.1. SI produces connected traces</h3>
<p>For a reasoning trace to be connected it must not hallucinate facts (Defn. 3). Tables 5 and 6 show that while some baseline models fail to satisfy this requirement and often hallucinate facts. For example, the Proof + Answer baseline makes up facts to solve $60 \%$ of EntailmentBankQA problems. On the other hand, SI makes up facts &lt; $1 \%$ of the time, suggesting that $&gt;99 \%$ of traces produced by SI are connected reasoning traces.</p>
<h3>5.4.2. SI produces correct inferences</h3>
<p>Following Defn. 4 for a trace to be valid it must be connected (as above) and the steps must be correct; the inference must follow from the selection. Tables 7 shows that when fed with a valid selection the inference model reliably produces the correct inference. It is harder to evaluate inference accuracy on EntailmentBankQA, however Table 8 suggests that the inference model is accurate, with a RougeL score if 0.69 .</p>
<h3>5.4.3. SI uses its reasoning trace to answer the question</h3>
<p>Unlike baseline models, SI's causal structure (see Figure 2) forces it to use the reasoning trace to answer the question. On the other hand, some baseline models are able to 'cheat', answering questions without reasoning properly over the context. In other words, they depend more on the knowledge embedded in their weights than on the context provided and the reasoning trace constructed. To investigate this, we evaluate performance of a model that is given an incorrect context (a context different from the one needed to solve the problem) and compare this to performance when the model is given the correct context. If a model's answer depends on careful reasoning over the context, then it should be unable to answer the question when provided with a random context.</p>
<p>On the EntailmentBankQA dataset, we use a random context sampled from another problem in the dataset. Table 3 shows that both the Proof + Answer and EntailmentWriter + Answer models are still able to answer $30 \%$ and $23 \%$ of questions respectively, while SI + Halter is only able to answer $9 \%$. We also see that while almost half of the final accuracy could be accounted for by 'cheating' or chance in the baseline models, less that $12.5 \%$ of SI + Halter final accuracy could be attributed to 'cheating' or chance.</p>
<p>On the ProofWriter dataset, we use an incomplete context which consists only of the rules needed to solve the problems but not the facts, making it impossible to construct the correct a valid trace to solve the problem. Table 4 shows model performance and the difference in performance, $\Delta$, between models that use the complete and incomplete context. The $\Delta$ results suggest that SI + Halter is the only model that reliably makes use of the reasoning trace, while the other models rely on taking short cuts. For example, Proof + Answer may be taking short cuts, by looking for rules whose head predicate matches the predicate in the question.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Task 1 <br> random $\downarrow$ <br> context</th>
<th style="text-align: center;">$\Delta \uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SI + Halter</td>
<td style="text-align: center;">$\mathbf{9 . 4 \%}$</td>
<td style="text-align: center;">$\mathbf{6 3 . 0 \%}$</td>
</tr>
<tr>
<td style="text-align: center;">Proof + Answer</td>
<td style="text-align: center;">$30.0 \%$</td>
<td style="text-align: center;">$34.6 \%$</td>
</tr>
<tr>
<td style="text-align: center;">EW* + Answer</td>
<td style="text-align: center;">$23.0 \%$</td>
<td style="text-align: center;">$27.0 \%$</td>
</tr>
</tbody>
</table>
<p>Table 3 | EntailmentBank: Relative performance increase, $\Delta$, when using the correct context as opposed to a random one. We expect models that actively make use of the context to have poor performance when using the random context and a larger performance increases, $\Delta$, when using the correct context compare to when using the incorrect one. (*EW=EntailmentWriter (Dalvi et al., 2021))</p>
<h2>6. Related Work</h2>
<p>While contemporary language models (LMs) are good at many natural language tasks, they often struggle with logical reasoning (Betz et al., 2021; Creswell et al., 2022; Dasgupta et al., 2022; Rae et al., 2021; Zhang et al., 2022). In this section we draw attention to the exciting progress being made towards reasoning using LMs. We highlight several works that use language models to produce reasoning traces (Bostrom et al., 2022; Dalvi et al., 2021; Kojima et al., 2022; Saha et al., 2020; Tafjord et al., 2021; Wei et al., 2022; Zelikman et al., 2022), and assess the reasoning validity of each approach. Finally, we discuss two additional areas of related work, the use of search and the problem of when to stop reasoning.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Depths 1-5 <br> incomplete $\downarrow$ <br> context</th>
<th style="text-align: center;">$\Delta \uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SI + Halter</td>
<td style="text-align: center;">$\mathbf{2 9 . 5 \%}$</td>
<td style="text-align: center;">$\mathbf{4 8 . 8 \%}$</td>
</tr>
<tr>
<td style="text-align: center;">Proof + Answer</td>
<td style="text-align: center;">$61.2 \%$</td>
<td style="text-align: center;">$4.3 \%$</td>
</tr>
<tr>
<td style="text-align: center;">EW* + Answer</td>
<td style="text-align: center;">$53.4 \%$</td>
<td style="text-align: center;">$0.1 \%$</td>
</tr>
</tbody>
</table>
<p>Table 4 | Proof Writer: Relative performance increase, $\Delta$, when using the complete context as opposed to a incomplete (rules only) context. In the ProofWriter dataset information needed to solve the problem may be leaked, in the baseline models, by the rules themselves, without need to do valid reasoning. We expect models that actively make use of the reasoning trace - rather than 'cheating' using short-cuts - to have poor performance when using the incomplete context and to have a larger performance increases, $\Delta$. (*EW=EntailmentWriter (Dalvi et al., 2021)) SI + Halter performance is less than $50 \%$ because in $69.7 \%$ of cases the model correctly predicts that it cannot answer the question. The $\Delta$ results suggests that SI + Halter is the only model that reliably uses the reasoning trace to answer questions.</p>
<h3>6.1. Language Models Are Not Enough</h3>
<p>Recent work on applying language models to reasoning problems has largely concentrated on improving final answer accuracy rather than producing valid, human interpretable reasoning traces that lead to the answers. For example, various methods of prompting (Wei et al., 2022) and iterative fine-tuning (Zelikman et al., 2022) have been used to encourage models to produce reasoning traces, and while this has led to improvement of final answer accuracy these traces do not support our understanding of how the answer was reached.</p>
<p>Kojima et al. (2022) split the reasoning in two parts, first producing a reasoning trace and then predicting the answer given the question and the reason. Similar to our own model Zhou et al. (2022) go one-step further and split each reasoning step in two: first asking an intermediate question and second, answering that question. While the authors suggest that their approach promotes compositional generalisation, unlike our</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" />
(a) Jaccard similarity between the ground truth leaves (e.g. selection) and those used by the model. We see that SI outperforms all of the baseline models on the more challenging task, Task 2.
<img alt="img-10.jpeg" src="img-10.jpeg" />
(b) Rouge score on the intermediate outputs (or inferences) from each step (ignoring order). The baseline models that do not use search or the halter perform poorly on Task 2.</p>
<p>Figure 9 | Evaluating reasoning steps for EntailmentBankQA. We compute the above values only on problems where the model predicts that the answer is not "Unknown". Note, none of these metrics account for order of the reasoning steps.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">Task 1</th>
<th style="text-align: right;">Task 2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Proof + Answer</td>
<td style="text-align: right;">$10 \%$</td>
<td style="text-align: right;">$60 \%$</td>
</tr>
<tr>
<td style="text-align: left;">EntailmentWriter + Answer</td>
<td style="text-align: right;">$3 \%$</td>
<td style="text-align: right;">$\mathbf{0 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">Proof Only + Halter</td>
<td style="text-align: right;">$15 \%$</td>
<td style="text-align: right;">$23 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Proof Only + Halter + Search</td>
<td style="text-align: right;">$18 \%$</td>
<td style="text-align: right;">$40 \%$</td>
</tr>
<tr>
<td style="text-align: left;">SI + Halter</td>
<td style="text-align: right;">$\mathbf{1 \%}$</td>
<td style="text-align: right;">$\mathbf{0 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">SI + Halter + Search</td>
<td style="text-align: right;">$\mathbf{1 \%}$</td>
<td style="text-align: right;">$\mathbf{0 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 5 | EntailmentBankQA: Proportion of problems on which models made-up facts that were not in the context. We see that only SI and EntailmentWriter are able to avoid making up facts.
approach the answering part of the model has full access to the question and therefore the model does not have to rely on the reasoning trace to answer the question. Moreover, unlike our work, the models of Kojima et al. (2022); Wei et al. (2022); Zelikman et al. (2022); Zhou et al. (2022) are not restricted to reasoning over knowledge in the context, but rather have the ability to hallucinate possibly incorrect "knowledge" to support the answer leading to reasoning traces which are not valid and cannot be trusted.</p>
<h3>6.2. Reasoning with Language Models</h3>
<p>The EntailmentBank dataset proposed by Dalvi et al. (2021) has led to several works focused on deriving reasoning traces to backup an answer or hypothesis (Bostrom et al., 2022; Dalvi et al., 2022; Jhamtani and Clark, 2020; Ribeiro et al., 2022). In our work, we focus on answering questions and providing faithful reasoning traces, rather than post-hoc explanations.</p>
<p>With a similar motivation to our own, Gupta et al. (2022) and Nakano et al. (2021) show promising results extracting evidence from a table or the web, respectively, and using this to answer a question or solve a natural language inference (NLI) problem. However, while Gupta et al. (2022) and Nakano et al. (2021) show the evidence used, they do not show how that information was combined to answer the question. In our work, we produce a valid reasoning trace that shows how multiple pieces of knowledge are combined, over several iterations, to answer a question.</p>
<p>Other works have focused on using reasoning to show whether a statement is True or False (Betz</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Defn 1. A step has <br> a selection and an <br> inference.</th>
<th style="text-align: center;">Defn 2. <br> Sequence of <br> reasoning <br> steps</th>
<th style="text-align: center;">Defn 3. Trace <br> is connected</th>
<th style="text-align: center;">Defn 4. <br> Incentive to <br> make correct <br> inference</th>
<th style="text-align: center;">Valid <br> Reasoning</th>
<th style="text-align: center;">Designed for <br> Question <br> Answering</th>
<th style="text-align: center;">Can answer <br> "Unknown"</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Faithful Reasoning <br> (ours)</td>
<td style="text-align: center;">✔</td>
<td style="text-align: center;">✔</td>
<td style="text-align: center;">✔</td>
<td style="text-align: center;">✔</td>
<td style="text-align: center;">✔</td>
<td style="text-align: center;">✔</td>
<td style="text-align: center;">✔</td>
</tr>
<tr>
<td style="text-align: left;">Proof + Answer <br> (baseline)</td>
<td style="text-align: center;">implicitly</td>
<td style="text-align: center;">implicitly</td>
<td style="text-align: center;">$\mathbf{x}$</td>
<td style="text-align: center;">$\mathbf{x}$</td>
<td style="text-align: center;">$\mathbf{x}$</td>
<td style="text-align: center;">✔</td>
<td style="text-align: center;">$\mathbf{x}$</td>
</tr>
<tr>
<td style="text-align: left;">Let's think <br> step-by-step <br> (Kojima et al. 2022)</td>
<td style="text-align: center;">implicitly</td>
<td style="text-align: center;">implicitly</td>
<td style="text-align: center;">$\mathbf{x}$</td>
<td style="text-align: center;">$\mathbf{x}$</td>
<td style="text-align: center;">$\mathbf{x}$</td>
<td style="text-align: center;">✔</td>
<td style="text-align: center;">$\mathbf{x}$</td>
</tr>
<tr>
<td style="text-align: left;">Chain of Thought <br> (Wei et al. 2022)</td>
<td style="text-align: center;">implicitly</td>
<td style="text-align: center;">implicitly</td>
<td style="text-align: center;">$\mathbf{x}$</td>
<td style="text-align: center;">$\mathbf{x}$</td>
<td style="text-align: center;">$\mathbf{x}$</td>
<td style="text-align: center;">✔</td>
<td style="text-align: center;">$\mathbf{x}$</td>
</tr>
<tr>
<td style="text-align: left;">EntailmentWriter <br> (Dalvi et al. 2021)</td>
<td style="text-align: center;">implicitly</td>
<td style="text-align: center;">implicitly</td>
<td style="text-align: center;">$\mathbf{V}$</td>
<td style="text-align: center;">$\mathbf{V}$</td>
<td style="text-align: center;">$\mathbf{x}$</td>
<td style="text-align: center;">Entailment <br> only</td>
<td style="text-align: center;">$\mathbf{x}$</td>
</tr>
<tr>
<td style="text-align: left;">ProofWriter <br> (Tafjord et al. 2020)</td>
<td style="text-align: center;">✔</td>
<td style="text-align: center;">✔</td>
<td style="text-align: center;">✔</td>
<td style="text-align: center;">✔</td>
<td style="text-align: center;">✔</td>
<td style="text-align: center;">T/F only</td>
<td style="text-align: center;">✔</td>
</tr>
<tr>
<td style="text-align: left;">Bostrom et al. 2022</td>
<td style="text-align: center;">$\underset{\text { inefficient and limited }}{\text { selection step }}$</td>
<td style="text-align: center;">✔</td>
<td style="text-align: center;">✔</td>
<td style="text-align: center;">✔</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Entailment <br> only</td>
<td style="text-align: center;">$\mathbf{x}$</td>
</tr>
</tbody>
</table>
<p>Figure 10 | Comparison between Faithful Reasoning and other related works.
et al., 2021; Tafjord et al., 2021). In Proof Writer, Tafjord et al. (2021) train an LM to enumerate implications (and corresponding reasoning steps) given a hypothesis. A valid reasoning trace can be constructed from these outputs. However, this approach is limited to answering questions whose answer is True, False or Unknown, and a reasoning trace must be constructed post-hoc.</p>
<p>Finally, while several works have informally introduced the notion of faithful reasoning (Bostrom et al., 2022; Gupta et al., 2022; Kumar and Talukdar, 2020), we have related this more precisely to the definition of valid reasoning in logic.</p>
<h3>6.3. Using Search for Reasoning Problems</h3>
<p>The notion of valid and invalid reasoning traces have also been explored in the context of search. Jhamtani and Clark (2020) develop datasets of valid and invalid reasoning traces for grade school science questions. These can be used to train models to detect valid reasoning traces. However, it can be expensive to collect both valid and invalid reasoning traces hence they collect only shallow traces and their traces do not include intermediate inferences. Instead, we show how, given a valid reasoning trace, we can generate many invalid reasoning traces that can be used to finetune a value function and used to guide search. Also, rather than learning a verifier that evaluates
a whole trace (Cobbe et al., 2021; Jhamtani and Clark, 2020; Nye et al., 2022), we train a model on partial reasoning traces, resulting in a model more similar to a value function which assesses the "value" of the current reasoning step, which can be used for step-level search.</p>
<p>Bostrom et al. (2022) also use step-level search to determine whether a hypothesis is entailed by a set of statements. While we perform a beam search, using a learned value function, to find high-quality reasoning traces, Bostrom et al. (2022) depend on exhaustive search to evaluate all possible pairs of statements to use for selection. Unlike Bostrom et al. (2022), our selection step is not limited to selecting just two statements. This allows us to more efficiently solve Proof Writer tasks whose rules may be conditioned on multiple statements.</p>
<h3>6.4. The Problem of When to Stop Reasoning</h3>
<p>The problem of when to "stop" rarely features in the deep learning literature because our models typically answer problems in a single step. However, there are some exceptions. A simple example is text synthesis with large language models where the model has to determine when to stop producing tokens. This is often handled by a special 'End Of Sequence' token (Graves, 2013). Other examples in the deep learning literature draw random variables from a parameterised distribution</p>
<p>to predict when to stop reasoning (Banino et al., 2021; Graves, 2016).</p>
<p>Related work by Kadavath et al. (2022) also investigates when LMs "know" the answer. Their model proposes a number of candidates, and predicts whether each candidate is the answer to the question or not. Additionally, Bostrom et al. (2022) tackle the less challenging problem of determining whether an inference matches a goal state.</p>
<p>In summary, current work focuses on True/False/NLI tasks (Bostrom et al., 2022; Dalvi et al., 2021; Tafjord et al., 2021) while our work tackles question-answering. This is not a trivial difference. In question-answering, there is less information with which to construct the reasoning trace, since the "goal" is not known, and learning when to terminate is also more challenging. Moreover, current work leverages reasoning traces to boost performance - rather than to aid explainability or build trust - allowing for hallucination of "knowledge" during reasoning (Kojima et al., 2022; Wei et al., 2022). Furthermore, some existing approaches still allow the opportunity for "cheating" Dalvi et al. (2021); Wei et al. (2022) by providing the answering part of the model with direct access to the question ${ }^{3}$. Finally, unlike most other models (Dalvi et al., 2021; Wei et al., 2022), the causal structure of our model (see Figure 10) mirrors the requirements for validity, see Table 10. Other approaches that do satisfy validity have their own limitations, as detailed above.</p>
<h2>7. Limitations</h2>
<p>The causal structure of our model mirrors the requirements for producing a valid trace (Defn. 4). Requirements for a connected reasoning trace (Defn. 3) are guaranteed by design (Section 3.1.1). Unavoidably, given our use of LMs, we cannot guarantee that all reasoning steps will be logically correct (Defn. 4). However, our architecture is designed to encourage logical correctness by preventing models from 'cheating'. For exam-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>ple, if the Selection model selects two unrelated statements, then the Inference model may draw a nonsensical conclusion. We also mitigate this by introducing a learned value function (Section 3.3) that filters out poor reasoning traces, although this still cannot guarantee the correctness of every step. Examples of both correct and incorrect reasoning traces, along with their value (according to the value function), are shown in Section D.</p>
<p>In this paper we have focused on developing models that answer questions using valid reasoning. For now we have assumed access to a context, over which to reason. However, while there are some settings where such a context may be provided, in most real world settings this is unlikely. In this paper we have chosen to focus on the challenging problem of multi-step reasoning within a given context. However, in future work we hope to incorporate retrieval to populate the context, and there is already interesting research in this direction (Dalvi et al., 2021; Ribeiro et al., 2022; Xie et al., 2020).</p>
<h2>8. Discussion</h2>
<p>Language models are being applied, with great success, to many different problems Alayrac et al. (2022); Nakano et al. (2021); Nye et al. (2022); Rae et al. (2021); Zeng et al. (2022). However, they largely remain black boxes; we do not know how the models produce their responses. One solution to this is to develop models that can produce faithful reasoning traces. We characterise faithful reasoning in terms of logical validity (Section 2), and propose Selection-Inference, a model that mirrors the structure of this definition, and is guaranteed to produce valid reasoning traces under the assumption that individual steps are correct (Defn. 4). By fine-tuning an Inference model specifically for this task and preventing it from "cheating", we increase the likelihood that this assumption holds (Tables 7and 8). Finally, to find high-quality reasoning traces, we introduce a value function, and use it to guide a beam search through the tree of potential traces induced by the non-determinism of selection.</p>
<p>The resulting model achieves higher final an-</p>
<p>swer accuracy than baseline models on both Proof Writer Tafjord et al. (2021) and EntailmentBankQA Dalvi et al. (2021) tasks. We see that both Proof Only and SI benefit from search (Tables 1 and 2). When compared to baseline models, our model is less likely to hallucinate facts while reasoning (Table 6 and 5). We see that the SI + Halter model is far more likely than baseline models to pay attention to the context (Table 3) and to leverage the reasoning trace (Table 4). Overall, we see that SI + Halter (+ Search) models achieves superior reasoning trace accuracy, especially on the more challenging tasks (Figures 8 and 9).</p>
<p>Our approach exemplifies a trend towards algorithmic prompting, a form of automated prompt engineering in which querying a language model becomes a computational primitive. The responses of the language model can be manipulated to construct new prompts that are then used to make further queries. Model queries and prompt construction are composed into algorithms with the usual computational constructs: sequence, choice, and iteration. Algorithmic prompting can be used to elicit more sophisticated and nuanced behaviour from a language model than would otherwise be possible. For example, as our work shows, this approach can be used to develop models capable of faithful reasoning, without compromising performance. In future work we aim to leverage advancements in retrieval to populate the context, rather than relying on the context being provided in the question.</p>
<h2>References</h2>
<p>J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022.
A. Banino, J. Balaguer, and C. Blundell. Pondernet: Learning to ponder. arXiv preprint arXiv:2107.05407, 2021.
E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the dangers of stochastic
parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21, page 610-623, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383097. doi: 10.1145/ 3442188.3445922. URL https://doi.org/ 10.1145/3442188.3445922.
G. Betz, C. Voigt, and K. Richardson. Critical thinking for language models. In Proceedings of the 14th International Conference on Computational Semantics (IWCS), pages 6375, Groningen, The Netherlands (online), June 2021. Association for Computational Linguistics. URL https://aclanthology.org/ 2021.iwcs-1.7.
K. Bostrom, Z. Sprague, S. Chaudhuri, and G. Durrett. Natural language deduction through search over statement compositions. arXiv preprint arXiv:2201.06028, 2022.
P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.
K. Cobbe, V. Kosaraju, M. Bavarian, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.
A. Creswell, M. Shanahan, and I. Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712, 2022.
B. Dalvi, P. Jansen, O. Tafjord, Z. Xie, H. Smith, L. Pipatanangkura, and P. Clark. Explaining answers with entailment trees. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7358-7370, Online and Punta Cana, Dominican Republic, Nov. 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main. 585. URL https://aclanthology.org/ 2021.emnlp-main. 585.
B. Dalvi, O. Tafjord, and P. Clark. Towards teachable reasoning systems. arXiv preprint arXiv:2204.13074, 2022.</p>
<p>I. Dasgupta, A. K. Lampinen, S. C. Chan, A. Creswell, D. Kumaran, J. L. McClelland, and F. Hill. Language models show human-like content effects on reasoning. arXiv preprint arXiv:2207.07051, 2022.
A. Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.
A. Graves. Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983, 2016.
V. Gupta, S. Zhang, A. Vempala, Y. He, T. Choji, and V. Srikumar. Right for the right reason: Evidence extraction for trustworthy tabular reasoning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. acl-long.231. URL https://aclanthology. org/2022.acl-long. 231.
A. Hamilton. Logic for Mathematicians. Cambridge University Press, 1988.
J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.
H. Jhamtani and P. Clark. Learning to explain: Datasets and models for identifying valid reasoning chains in multihop question-answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 137-150, Online, Nov. 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main. 10. URL https://aclanthology.org/ 2020.emnlp-main. 10.
S. Kadavath, T. Conerly, A. Askell, T. Henighan, D. Drain, E. Perez, N. Schiefer, Z. H. Dodds, N. DasSarma, E. Tran-Johnson, et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022.
T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.
S. Kumar and P. Talukdar. Nile: Natural language inference with faithful natural language explanations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8730-8742, 2020.
R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, et al. Webgpt: Browser-assisted questionanswering with human feedback. arXiv preprint arXiv:2112.09332, 2021.
M. Nye, A. J. Andreassen, G. Gur-Ari, H. Michalewski, J. Austin, D. Bieber, D. Dohan, A. Lewkowycz, M. Bosma, D. Luan, et al. Show your work: Scratchpads for intermediate computation with language models. In Deep Learning for Code Workshop, 2022.
J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young, et al. Scaling language models: Methods, analysis \&amp; insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.
D. Ribeiro, S. Wang, X. Ma, R. Dong, X. Wei, H. Zhu, X. Chen, Z. Huang, P. Xu, A. Arnold, et al. Entailment tree explanations via iterative retrieval-generation reasoner. arXiv preprint arXiv:2205.09224, 2022.
S. Saha, S. Ghosh, S. Srivastava, and M. Bansal. Prover: Proof generation for interpretable reasoning over rules. In EMNLP (1), 2020.
T. Sellam, D. Das, and A. Parikh. Bleurt: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881-7892, 2020.
O. Tafjord, B. Dalvi, and P. Clark. Proofwriter: Generating implications, proofs, and abductive statements over natural language. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3621-3634, 2021.</p>
<p>J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models, 2022.
L. Weidinger, J. Mellor, M. Rauh, C. Griffin, J. Uesato, P. Huang, M. Cheng, M. Glaese, B. Balle, A. Kasirzadeh, Z. Kenton, S. Brown, W. Hawkins, T. Stepleton, C. Biles, A. Birhane, J. Haas, L. Rimell, L. A. Hendricks, W. S. Isaac, S. Legassick, G. Irving, and I. Gabriel. Ethical and social risks of harm from language models. CoRR, abs/2112.04359, 2021. URL https://arxiv.org/abs/2112.04359.
Z. Xie, S. Thiem, J. Martin, E. Wainwright, S. Marmorstein, and P. Jansen. Worldtree v2: A corpus of science-domain structured explanations and inference patterns supporting multi-hop inference. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 54565473, 2020.
E. Zelikman, Y. Wu, and N. D. Goodman. Star: Bootstrapping reasoning with reasoning. arXiv preprint arXiv:2203.14465, 2022.
A. Zeng, A. Wong, S. Welker, K. Choromanski, F. Tombari, A. Purohit, M. Ryoo, V. Sindhwani, J. Lee, V. Vanhoucke, et al. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022.
H. Zhang, L. H. Li, T. Meng, K.-W. Chang, and G. V. d. Broeck. On the paradox of learning to reason from data. arXiv preprint arXiv:2205.11502, 2022.
D. Zhou, N. Schärli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, O. Bousquet, Q. Le, and E. Chi. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.</p>
<h2>Acknowledgements</h2>
<p>The authors would like to thank Angeliki Lazaridou, Charles Blundell and Christopher Summerfield for feedback on our paper as well as Jonathan</p>
<p>Uesato, Jordi Grau-Moya, Ramana Kumar and Irina Higgins for insightful discussions.</p>
<h2>Supplementary Materials</h2>
<h2>A. Formal definition of the problem</h2>
<p>Formally, suppose we have a problem of the form $\left(q, C_{0}\right)$, where $C_{0}$ is a context, consisting of a set of statements which are sufficient to predict the the correct answer to the question, $q$.</p>
<p>The role of the Selection model, $\mathrm{LM}<em k="k">{\text {Selection }}$ is to sample a selection, $s</em>$, see Equation 1.}$, given the question, $q$ and the current context, $C_{k</p>
<p>$$
s_{k}=\mathrm{LM}<em k="k">{\text {Selection }}\left(q, C</em>\right)
$$</p>
<p>The role of the Inference model, $\mathrm{LM}<em k="k">{\text {Inference }}$ is to sample an inference, $i</em>$, see Equation 2.}$, given the selection, $s_{k</p>
<p>$$
i_{k}=\mathrm{LM}<em k="k">{\text {Inference }}\left(s</em>\right)
$$</p>
<p>After each inference the context is updated as follows, $C_{k-1} \cup i_{k-1}$, accumulating inferences from previous steps of reasoning.</p>
<p>The Halter LM, $\mathrm{LM}<em t="t">{\text {halt }}$, is applied to each inference, $i</em>$, in two ways. First, to choose whether the model should stop reasoning and the second to answer the question when the model is 'ready'. This is illustrated in Alg. 1 and Alg. 2.</p>
<h2>B. Training Details</h2>
<h2>B.1. Datasets</h2>
<h2>B.1.1. Proof Writer</h2>
<p>The Proof Writer dataset (Tafjord et al., 2021) contains both a Closes and Open World Assumption version (CWA and OWA respectively). We use a subset of the OWA dataset. This is because for the CWA dataset everything that cannot be proven is considered False. This means that problems whose answer is False, do not have reasoning traces. On the other hand the OWA dataset contains proofs for problems whose answers are True and False. Those without proofs are "Unknown". Since we need proofs for training and evaluation, we use the problems from the OWA dataset that</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">1</span><span class="o">:</span><span class="w"> </span><span class="nt">The</span><span class="w"> </span><span class="nt">SI</span><span class="o">()</span><span class="w"> </span><span class="nt">function</span><span class="o">.</span>
<span class="w">    </span><span class="nt">Input</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathrm</span><span class="p">{</span><span class="err">LM</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{Selection</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">)</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="nt">Selection</span><span class="w"> </span><span class="nt">LM</span><span class="o">.</span>
<span class="w">    </span><span class="nt">Input</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathrm</span><span class="p">{</span><span class="err">LM</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{Inference</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">:</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Inference</span><span class="w"> </span><span class="nt">LM</span><span class="o">.</span>
<span class="w">    </span><span class="nt">Input</span><span class="o">:</span><span class="w"> </span><span class="nt">halt</span><span class="o">():</span><span class="w"> </span><span class="nt">Halt</span><span class="w"> </span><span class="nt">function</span><span class="w"> </span><span class="o">(</span><span class="nt">Alg</span><span class="o">.</span><span class="w"> </span><span class="nt">2</span><span class="o">).</span>
<span class="w">    </span><span class="nt">Input</span><span class="o">:</span><span class="w"> </span><span class="nt">q</span><span class="o">:</span><span class="w"> </span><span class="nt">Question</span><span class="o">.</span>
<span class="w">    </span><span class="nt">Input</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">C_</span><span class="p">{</span><span class="err">0</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="nt">Initial</span><span class="w"> </span><span class="nt">context</span><span class="o">.</span>
<span class="w">    </span><span class="nt">Input</span><span class="o">:</span><span class="w"> </span><span class="nt">c</span><span class="o">:</span><span class="w"> </span><span class="nt">Choices</span><span class="o">.</span>
<span class="w">    </span><span class="nt">Input</span><span class="o">:</span><span class="w"> </span><span class="nt">K</span><span class="s1">&#39;: Max. reasoning steps.</span>
<span class="s1">    \(a \leftarrow\) &#39;</span><span class="nt">Unknown</span><span class="s1">&#39;; Initial answer is unknown.</span>
<span class="s1">    \(t \leftarrow 0 ;\) Step counter.</span>
<span class="s1">    while \(a\) is &#39;</span><span class="nt">Unknown</span><span class="err">&#39;</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="nt">s_</span><span class="p">{</span><span class="err">k</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">mathrm</span><span class="p">{</span><span class="err">LM</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{Selection</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">left</span><span class="o">(</span><span class="nt">q</span><span class="o">,</span><span class="w"> </span><span class="nt">C_</span><span class="p">{</span><span class="err">k</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="nt">i_</span><span class="p">{</span><span class="err">k</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">mathrm</span><span class="p">{</span><span class="err">LM</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{Inference</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">(</span><span class="nt">i</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="nt">C_</span><span class="p">{</span><span class="err">k+1</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">C_</span><span class="p">{</span><span class="err">k</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">cup</span><span class="w"> </span><span class="nt">i_</span><span class="p">{</span><span class="err">k</span><span class="p">}</span><span class="err">\</span><span class="o">);</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="nt">a</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">halt</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">q</span><span class="o">,</span><span class="w"> </span><span class="nt">i_</span><span class="p">{</span><span class="err">k</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">c</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="nt">k</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">k</span><span class="o">+</span><span class="nt">1</span><span class="w"> </span><span class="o">;</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">if</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">k</span><span class="o">&gt;</span><span class="nt">K</span><span class="o">^</span><span class="p">{</span><span class="err">\prime</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">then</span>
<span class="w">            </span><span class="nt">return</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">a</span><span class="err">\</span><span class="o">)</span>
<span class="err">\</span><span class="o">(</span><span class="nt">a</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">mathrm</span><span class="p">{</span><span class="err">LM</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{halt</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">(</span><span class="nt">i</span><span class="o">,</span><span class="w"> </span><span class="nt">c</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="nt">12</span><span class="w"> </span><span class="nt">return</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">a</span><span class="err">\</span><span class="o">)</span>
</code></pre></div>

<p>have accompanying proofs (i.e. those whose answer is not Unknown).</p>
<h2>B.2. Selection-Inference</h2>
<p>Figures 11 and 12 show examples of training samples use to fine-tune the Selection and Inference LMs.</p>
<h2>B.3. Halter</h2>
<p>Figure 13 shows how training data is constructed for training the halting model.</p>
<h2>B.4. Search</h2>
<p>Figures 14 and 15 show examples of data points used to train the value function. The targets for the value function are either ' correct' or ' incorrect'.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">2</span><span class="o">:</span><span class="w"> </span><span class="nt">The</span><span class="w"> </span><span class="nt">halt</span><span class="o">()</span><span class="w"> </span><span class="nt">function</span><span class="o">.</span><span class="w"> </span><span class="nt">Note</span>
<span class="nt">that</span><span class="w"> </span><span class="nt">we</span><span class="w"> </span><span class="nt">use</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">same</span><span class="w"> </span><span class="nt">language</span><span class="w"> </span><span class="nt">model</span>
<span class="nt">LM</span><span class="w"> </span><span class="nt">halt</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">both</span><span class="w"> </span><span class="nt">determine</span><span class="w"> </span><span class="nt">whether</span><span class="w"> </span><span class="nt">the</span>
<span class="nt">model</span><span class="w"> </span><span class="nt">is</span><span class="w"> </span><span class="nt">able</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">answer</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">question</span><span class="w"> </span><span class="nt">and</span>
<span class="nt">to</span><span class="w"> </span><span class="nt">answer</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">question</span><span class="o">.</span><span class="w"> </span><span class="nt">The</span><span class="w"> </span><span class="nt">key</span><span class="w"> </span><span class="nt">difference</span>
<span class="nt">is</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">prompt</span><span class="o">,</span><span class="w"> </span><span class="nt">shown</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">Section</span><span class="w"> </span><span class="nt">3</span><span class="p">.</span><span class="nc">2</span><span class="o">.</span>
<span class="nt">Input</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathrm</span><span class="p">{</span><span class="err">LM</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{halt</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">:</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Halting</span><span class="w"> </span><span class="nt">LM</span><span class="o">.</span>
<span class="w">    </span><span class="nt">Input</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">q</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="nt">Question</span><span class="o">.</span>
<span class="w">    </span><span class="nt">Input</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">i</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="nt">Current</span><span class="w"> </span><span class="nt">inference</span><span class="o">.</span>
<span class="w">    </span><span class="nt">Input</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">c</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="nt">Choices</span><span class="o">.</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="nt">a</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">mathrm</span><span class="p">{</span><span class="err">LM</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{halt</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">(</span><span class="nt">q</span><span class="o">,</span><span class="w"> </span><span class="nt">i</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">    </span><span class="nt">if</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">a</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">is</span><span class="w"> </span><span class="s1">&#39;Unknown&#39;</span><span class="w"> </span><span class="nt">then</span>
<span class="w">        </span><span class="nt">return</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">a</span><span class="err">\</span><span class="o">);</span>
<span class="w">    </span><span class="nt">else</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="nt">a</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">mathrm</span><span class="p">{</span><span class="err">LM</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{halt</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">(</span><span class="nt">i</span><span class="o">,</span><span class="w"> </span><span class="nt">c</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">        </span><span class="nt">return</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">a</span><span class="err">\</span><span class="o">);</span>
</code></pre></div>

<p>sent 0 : If something is rough then it visits the lion. sent 1: the cow visits the lion. sent 2: the lion visits the cow. sent 3: the cow is rough. sent 4: If something visits the cow and the cow visits the lion then the lion is nice.
Does it imply that the statement "The lion is nice" is True?
sent 4. We know that sent 2 and sent 1. Therefore, the lion is nice.
(a) Example of 〈input, target〉 pairs used to train the Selection LLM.</p>
<p>If something visits the cow and the cow visits the lion. We know that the cow visits the lion and the cow visits the lion. Therefore, the lion is nice.
(b) Example of 〈input, target〉 pairs used to train the Inference LLM.</p>
<p>Figure $11 \mid$ Examples of Proof Writer training pairs for Selection and Inference LLMs</p>
<h2>C. Additional Results</h2>
<h2>C.1. Halter</h2>
<p>Figure 16 shows qualitative results from the Halter model trained on EntailmentBankQA.</p>
<h2>C.2. Reasoning Trace Accuracy</h2>
<p>Figure 17 show additional evaluation of reasoning traces on the Proof Writer dataset.
sent 0: cycles of freezing and thawing water cause mechanical weathering.
sent 1: cycles of freezing and thawing water cause ice wedging.
sent 2: ice wedging is a kind of mechanical weathering. sent 3: mechanical weathering means breaking down rocks from a larger whole into smaller pieces by mechanical means.
A student climbs up a rocky mountain trail in Maine. She sees many small pieces of rock on the path. Which action most likely made the small pieces of rock? sand blowing into cracks OR leaves pressing down tightly OR ice breaking large rocks apart OR shells and bones sticking together.
sent 0 . We know that sent 3. Therefore, cycles of freezing and thawing water cause the rocks to break into smaller pieces.
(a) Example of 〈input, target〉 pairs used to train the Selection LLM.
cycles of freezing and thawing water cause mechanical weathering. We know that mechanical weathering means breaking down rocks from a larger whole into smaller pieces by mechanical means. Therefore, cycles of freezing and thawing water cause the rocks to break into smaller pieces.
(b) Example of 〈input, target〉 pairs used to train the Inference LLM.</p>
<p>Figure 12 | Examples of EntailmentBankQA training pairs for Selection and Inference LLMs</p>
<p>In Table 5 we saw that baseline models, with the exception of EntailmentWriter, were more likely to hallucinate facts while reasoning on the EntailmentBank dataset than SI. Interestingly, Table 6 shows that Proof + Answer and Proof Only baseline models have learned not to make up facts while reasoning on the Proof Writer dataset. Note that both EntailmentWriter and SI (ours) are designed not to make up facts.</p>
<p>Figure 18 shows the Rouge 1 scores between the predicted and ground truth, ordered, intermediate inferences. We see that EntailmentWriter is very good at single step inference on Task 1 problems, but performance quickly declines for problems requiring multiple steps of reasoning. In general SI models and models using halting and search outperform the baseline models.</p>
<p>Tables 7 and 8 show the accuracy of the Inference LM when fed valid selections.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Data sample <br> Context: <br> All red things are soft. <br> All soft things are round. <br> All round things are loud. <br> The bear is red. <br> The cat is round.</th>
<th style="text-align: center;">Training data for the halter</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Context: <br> All red things are soft. <br> All soft things are round. <br> All round things are loud. <br> The bear is red. <br> The cat is round.</td>
<td style="text-align: center;">Context: <br> All red things are soft. <br> All soft things are round. <br> All round things are loud. <br> The bear is red. <br> The cat is round.</td>
<td style="text-align: center;">Context: <br> All red things are soft. <br> All soft things are round. <br> All round things are loud. <br> The bear is red. <br> The cat is round.</td>
<td style="text-align: center;">Context: <br> All red things are soft. <br> All soft things are round. <br> All round things are loud. <br> The bear is red. <br> The cat is round.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Question: <br> Is the bear loud?</td>
<td style="text-align: center;">Question: <br> Is the bear loud?</td>
<td style="text-align: center;">Question: <br> Is the bear loud?</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Reasoning: <br> All red things are soft. The bear is red. <br> Therefore the bear is soft.</td>
<td style="text-align: center;">Reasoning: <br> All red things are soft. The bear is red. <br> Therefore the bear is soft.</td>
<td style="text-align: center;">Reasoning: <br> All red things are soft. The bear is red. <br> Therefore the bear is soft.</td>
<td style="text-align: center;">Reasoning: <br> All red things are soft. The bear is red. <br> Therefore the bear is soft.</td>
<td style="text-align: center;">Reasoning: <br> All red things are soft. The bear is red. <br> Therefore the bear is soft.</td>
</tr>
<tr>
<td style="text-align: center;">All soft things are round. The bear is <br> soft. Therefore, the bear is round.</td>
<td style="text-align: center;">All soft things are round. The bear is <br> soft. Therefore, the bear is round.</td>
<td style="text-align: center;">All soft things are round. The bear is <br> soft. Therefore, the bear is round.</td>
<td style="text-align: center;">Do you know the answer? No</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">All round things are loud. The bear is <br> sound. Therefore the bear is soft.</td>
<td style="text-align: center;">All round things are loud. The bear is <br> sound. Therefore the bear is soft.</td>
<td style="text-align: center;">Do you know the answer? No</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Answer: <br> The bear is soft</td>
<td style="text-align: center;">Do you know the answer? Yes</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Figure 13 | Example of how data is generated for the halter. Above are examples for four training data points. The first three, Do you know the answer? examples, are used to train the halting model to learn when to halt. The final datum is used to train the halter to select an answer from the available choices.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">depth</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">depth-1</td>
<td style="text-align: center;">depth-2</td>
<td style="text-align: center;">depth-3</td>
<td style="text-align: center;">depth-5</td>
</tr>
<tr>
<td style="text-align: left;">Proof + Answer</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">EntailmentWriter + Answer</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Proof Only + Halter</td>
<td style="text-align: center;">$4 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Proof Only + Halter + Search</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">SI + Halter</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">SI + Halter + Search</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$0 \%$</td>
</tr>
</tbody>
</table>
<p>Table 6 | ProofWriter: Proportion of problems on which models made-up facts that were not in the context. We see that the Proof + Answer and Proof Only baseline models have learned not to make up facts, while EntailmentWriter and SI are designed not to make up facts.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">Inference Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">depth-1</td>
<td style="text-align: center;">$100 \%$</td>
</tr>
<tr>
<td style="text-align: left;">depth-2</td>
<td style="text-align: center;">$100 \%$</td>
</tr>
<tr>
<td style="text-align: left;">depth-3</td>
<td style="text-align: center;">$100 \%$</td>
</tr>
<tr>
<td style="text-align: left;">depth-5</td>
<td style="text-align: center;">$99.9 \%$</td>
</tr>
</tbody>
</table>
<p>Table 7 | ProofWriter inference accuracy. The inference model achieves almost perfect performance. We use exact string match in lower case to decide if two statement are the same.</p>
<h2>D. Selection-Inference Model Outputs</h2>
<h2>D.1. ProofWriter: SI + Halter + Search</h2>
<p>Below we show reasoning traces from the SI + Halter + Search model with the top 10 value
function scores. No additional filtering is performed. For ease of reading we have combined each selection and inference into a single line of text rather than showing them separately. Examples that score high often involve repeated steps, this is because the ProofWriter proof dataset often includes repeated steps. Invalid reasoning is shown in red.</p>
<h2>Example 1 (value: -6.7e-06)</h2>
<h2>Context:</h2>
<p>If something likes the rabbit and it sees the bald eagle then the bald eagle needs the rabbit. If something is blue then it sees the rabbit.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">Inference Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Rouge1</td>
<td style="text-align: center;">0.69</td>
</tr>
<tr>
<td style="text-align: center;">Rouge2</td>
<td style="text-align: center;">0.55</td>
</tr>
<tr>
<td style="text-align: center;">RougeL</td>
<td style="text-align: center;">0.69</td>
</tr>
<tr>
<td style="text-align: center;">BLEURT $&gt;0.28$</td>
<td style="text-align: center;">$64 \%$</td>
</tr>
</tbody>
</table>
<p>Table 8 | EntailmentBankQA inference accuracy. We report Rouge scores as well as accuracy using a BLEURT (Sellam et al., 2020) with a threshold of 0.28 determine accuracy (as done by Dalvi et al. (2021).</p>
<p>If something needs the rabbit and it likes the bald eagle then the bald eagle is blue.
If something likes the mouse then it is round. If something is rough and it does not see the mouse then it does not need the bald eagle. If something sees the mouse then it needs the rabbit.
the rabbit likes the mouse.
the bald eagle likes the rabbit.
the rabbit likes the bald eagle.
the rabbit sees the bald eagle.
the rabbit is blue.
the rabbit needs the bald eagle.
the bald eagle needs the mouse.
the rabbit needs the mouse.
the mouse needs the bald eagle.
the rabbit is not green.
the bald eagle likes the mouse.
the rabbit sees the mouse.
the bald eagle is round.
the bald eagle does not see the mouse.
the rabbit is red.
the rabbit is rough.
Question: Does it imply that the statement "The bald eagle does not see the rabbit" is True?</p>
<h2>Proof:</h2>
<p>If something sees the mouse then it needs the rabbit. We know that the rabbit sees the mouse. Therefore, the rabbit needs the rabbit.
If something sees the mouse then it needs the rabbit. We know that the rabbit sees the mouse. Therefore, the rabbit needs the rabbit.
If something sees the mouse then it needs the rabbit. We know that the rabbit sees the mouse. Therefore, the rabbit needs the rabbit.
If something needs the rabbit and it likes the bald eagle then the bald eagle is blue. We know
that the rabbit needs the rabbit and the rabbit likes the bald eagle. Therefore, the bald eagle is blue.
If something is blue then it sees the rabbit. We know that the bald eagle is blue. Therefore, the bald eagle sees the rabbit.
Answer: False</p>
<h2>Target Proof:</h2>
<p>If something sees the mouse then it needs the rabbit. We know that the rabbit sees the mouse. Therefore, the rabbit needs the rabbit.
If something needs the rabbit and it likes the bald eagle then the bald eagle is blue. We know that the rabbit needs the rabbit and the rabbit likes the bald eagle. Therefore, the bald eagle is blue.
If something is blue then it sees the rabbit. We know that the bald eagle is blue. Therefore, the bald eagle sees the rabbit.
Target Answer: False</p>
<h2>Example 2 (value: -8.6e-06)</h2>
<h2>Context:</h2>
<p>If someone needs the bald eagle and the bald eagle needs the squirrel then the bald eagle likes the cow.
If someone needs the squirrel then they are green.
If someone needs the cow and they need the dog then the cow is green.
If someone likes the squirrel and they like the dog then the dog is nice.
If someone sees the cow then they need the dog. If someone is cold then they see the cow.
If someone needs the dog then the dog sees the cow.
If someone likes the dog and the dog is blue then the dog sees the bald eagle.
the dog is nice.
the cow is nice.
the dog likes the squirrel.
the squirrel likes the dog.
the dog likes the cow.
the squirrel sees the dog.
the bald eagle is cold.
the bald eagle sees the dog.
the squirrel sees the bald eagle.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ Specifically in these cases, the question itself contains sufficient information to supply the answer, unlike in Proof Writer where the question is also not sufficient for answering correctly.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
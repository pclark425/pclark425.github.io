<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8887 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8887</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8887</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-e4f0bf3074d5ae6d55d22068dd50158dc81b2a0a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e4f0bf3074d5ae6d55d22068dd50158dc81b2a0a" target="_blank">Molecular generative model based on conditional variational autoencoder for de novo molecular design</a></p>
                <p><strong>Paper Venue:</strong> Journal of Cheminformatics</p>
                <p><strong>Paper TL;DR:</strong> A molecular generative model based on the conditional variational autoencoder that can be used to generate drug-like molecules with five target properties and to adjust a single property without changing the others and to manipulate it beyond the range of the dataset is proposed.</p>
                <p><strong>Paper Abstract:</strong> We propose a molecular generative model based on the conditional variational autoencoder for de novo molecular design. It is specialized to control multiple molecular properties simultaneously by imposing them on a latent space. As a proof of concept, we demonstrate that it can be used to generate drug-like molecules with five target properties. We were also able to adjust a single property without changing the others and to manipulate it beyond the range of the dataset.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8887.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8887.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CVAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conditional Variational Autoencoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative model that conditions the VAE encoding and decoding on a vector of target molecular properties, enabling direct control of multiple properties during de novo molecule generation using SMILES representations and an RNN-based encoder/decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Conditional Variational Autoencoder (CVAE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Variational Autoencoder conditioned on property vector; encoder/decoder implemented with recurrent neural networks (LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>500,000 canonicalized SMILES strings randomly sampled from the ZINC dataset (80% train / 20% test)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug-like molecule generation / drug discovery (control of physicochemical properties: MW, LogP, HBD, HBA, TPSA)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Conditional generation in latent space: encode SMILES + condition vector (five properties) -> sample latent vector (Gaussian noise added) -> decode with LSTM RNN decoder using stochastic sampling (100 stochastic write-outs per latent vector)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Generated molecules were often structurally different from seed/target molecules (examples shown); model could produce molecules with property values beyond the training-set range (e.g., larger LogP and TPSA), but no explicit quantitative novelty metrics (e.g., % not in training set or fingerprint similarity distributions) were reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Specificity enforced by embedding five target properties (MW, LogP, HBD, HBA, TPSA) into a condition vector provided to both encoder and decoder; success evaluated by whether generated molecules match target properties within a ±10% error range (relative to the dataset averages as defined in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Success rate (defined as 100 / number of generation attempts to obtain 100 molecules meeting all five properties), number of valid molecules produced during attempts, property-value distributions (e.g., LogP and TPSA histograms), and qualitative inspection of structures; validity checked with RDKit.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>CVAE produced drug-like molecules matching five target properties within a 10% tolerance in demonstrated cases (Aspirin, Tamiflu, others). It could selectively tune a single property (e.g., LogP) in many cases and could generate molecules with properties beyond the training-set maximum (e.g., LogP > training max), though validity rates decreased. Overall success rates were low (example: generating 100 aspirin-like molecules required 28,840 attempts with condition-guided sampling; success rates varied by target and sampling strategy). Sampling latent vectors around known molecules performed best; sampling around the target molecule often produced many duplicates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared conceptually to the jointly-trained VAE + property predictor + Gaussian process optimization method (Gómez-Bombarelli et al.); CVAE differs by directly incorporating properties into encoder and decoder, avoiding a separate optimization step and enabling independent control of properties and structure in many cases. The paper also contrasts CVAE with adversarial autoencoders and sequence-based generative models in related work, but no numerical head-to-head benchmarks are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Low success rate for generating molecules that meet multiple simultaneous property constraints; strong correlations among target properties limit independent control (e.g., MW correlated with TPSA); high rate of invalid SMILES due to discrete sequence decoding (stochastic write-out mitigates but does not eliminate); lack of 3D conformational information in SMILES limits applications requiring conformation; difficulty decoding directly to molecular graphs noted as an open problem; difficulty in pushing some properties arbitrarily far (e.g., LogP beyond certain values) likely due to property coupling and training data limits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Molecular generative model based on conditional variational autoencoder for de novo molecular design', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8887.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8887.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RNN-LSTM SMILES language models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent neural network (LSTM) based SMILES sequence generative models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sequence models that learn the probability distribution of next SMILES character/token and generate novel molecules by sampling sequences; used both as standalone generative models and as components (encoder/decoder) within autoencoder frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RNN (LSTM) SMILES language model / RNN encoder-decoder</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Recurrent neural network with LSTM cells (sequence model / language model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>SMILES strings from chemical databases (in this paper the authors use ZINC; related works use ZINC/ChEMBL and other SMILES corpora), but exact datasets for all cited works are not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>De novo molecular generation for drug discovery and activity-focused library generation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Sequence modeling of SMILES: learn next-character probabilities, then sample sequences (stochastic write-out) to produce novel SMILES; can be fine-tuned via transfer learning or coupled with conditional vectors in encoder-decoder setups.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Paper reports that sampling stochastic outputs from the decoder can yield multiple different molecules from the same latent/condition vector; no explicit quantitative novelty metrics for RNN-only models are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Can be tailored via transfer learning to focus on molecules with certain biological activities (referenced) or by constraining generation through conditioning vectors or downstream optimization (e.g., reinforcement learning). In this work, RNNs are used as the encoder/decoder within the CVAE to realize property-conditioned generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity of generated SMILES (checked with RDKit), property matching to target constraints, and structural similarity analyses shown qualitatively; detailed metrics for standalone RNN models not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Authors implemented 3-layer LSTM RNNs (500 hidden units per layer) as encoder and decoder inside the CVAE and achieved conditional molecule generation; stochastic sampling increased valid output variety but invalid-SMILES rate remains a challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>RNN/LSTM sequence models are discussed as an alternative generative approach used in prior work and as a building block for CVAE; the paper suggests discrete SMILES sequential decoding contributes to a high invalidity rate compared to potential graph-based decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Discrete SMILES representation leads to a high rate of invalid molecules upon decoding; SMILES lack 3D conformational information; sequence models may produce duplicates and struggle with long-range chemical validity rules unless augmented (e.g., grammar-VAE, reinforcement learning).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Molecular generative model based on conditional variational autoencoder for de novo molecular design', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8887.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8887.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transfer learning (SMILES models)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transfer learning applied to SMILES-based generative models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Fine-tuning pre-trained SMILES sequence models on task-specific, often smaller datasets to bias generation toward molecules with desired biological activities or properties.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transfer learning of SMILES generative models</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Fine-tuning of pre-trained sequence models (typically RNN/LSTM) on task-specific data</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pre-trained on large SMILES corpora (e.g., ZINC); fine-tuned on small datasets with specific biological activities (as reported in referenced works), not specified in detail within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery — generating focused molecule libraries with desired biological activities</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Pre-train SMILES language model on large corpus, then fine-tune (transfer) on small activity-labeled datasets to bias output; generation via sampling from fine-tuned model.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not quantified in this paper; cited works reportedly produced task-focused libraries but this paper only references the approach.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Specificity arises from fine-tuning on molecules exhibiting the target activity, thereby biasing the generative distribution toward that activity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not reported in detail in this paper (referenced works contain evaluations), but generally includes activity enrichment, validity, and diversity in related literature.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as an effective approach when task-specific training data are limited (cited Segler et al. and Gupta et al.). No empirical results from transfer learning experiments are reported in this paper itself.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Described as useful for low-data regimes compared to training from scratch; no quantitative comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Dependent on quality and size of fine-tuning dataset; potential overfitting to small datasets and limited exploration beyond the fine-tuned chemical space.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Molecular generative model based on conditional variational autoencoder for de novo molecular design', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8887.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8887.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reinforcement learning fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement learning (RL) methods for optimizing pre-trained molecular generative models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that modify a pre-trained molecular generative model via reinforcement learning to bias generation toward molecules with desired properties or objective functions (e.g., drug-likeness, target properties).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Reinforcement learning fine-tuning of molecular generative models</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Reinforcement learning (policy gradient / objective-reward methods) applied to sequence or generative models (RNNs/VAEs/GANs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pre-trained on chemical SMILES corpora (e.g., ZINC); RL optimization uses property evaluators (scoring functions) rather than additional training data; specifics not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Property-optimized de novo molecular design (e.g., maximize drug-likeness, specific property ranges)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Start from a pre-trained generator (SMILES-based); apply RL (reward function encodes desired properties) to fine-tune generator parameters so that sampled molecules optimize the reward.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not quantified in this paper; RL is referenced as a means to further improve property optimization beyond pre-trained distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Specificity enforced via the reward function used during RL (property-based scoring); referenced as a way to impose several properties on generated molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this paper; in related literature commonly used metrics include property improvement, validity, uniqueness, and diversity after RL fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as prior work (Olivecrona et al., Guimaraes et al., Jaques et al.) that can modify pre-trained generators to achieve target properties. The authors suggest RL could complement CVAE (e.g., to improve success rate) but do not present RL experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Positioned as an approach that can further bias generation compared to pure conditional generation methods; no empirical comparisons presented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Potential mode collapse, dependence on reward design, and requirement for careful balance between property optimization and molecular validity/diversity — these are general RL challenges and are implied though not exhaustively discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Molecular generative model based on conditional variational autoencoder for de novo molecular design', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Automatic chemical design using a data-driven continuous representation of molecules <em>(Rating: 2)</em></li>
                <li>Generating focused molecule libraries for drug discovery with recurrent neural networks <em>(Rating: 2)</em></li>
                <li>Generative recurrent networks for de novo drug design <em>(Rating: 2)</em></li>
                <li>Molecular de novo design through deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Objective-reinforced generative adversarial networks (ORGAN) for sequence generation models <em>(Rating: 2)</em></li>
                <li>Chemical space mimicry for drug discovery <em>(Rating: 2)</em></li>
                <li>Grammar variational autoencoder <em>(Rating: 1)</em></li>
                <li>Junction tree variational autoencoder for molecular graph generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8887",
    "paper_id": "paper-e4f0bf3074d5ae6d55d22068dd50158dc81b2a0a",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "CVAE",
            "name_full": "Conditional Variational Autoencoder",
            "brief_description": "A generative model that conditions the VAE encoding and decoding on a vector of target molecular properties, enabling direct control of multiple properties during de novo molecule generation using SMILES representations and an RNN-based encoder/decoder.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Conditional Variational Autoencoder (CVAE)",
            "model_type": "Variational Autoencoder conditioned on property vector; encoder/decoder implemented with recurrent neural networks (LSTM)",
            "model_size": null,
            "training_data": "500,000 canonicalized SMILES strings randomly sampled from the ZINC dataset (80% train / 20% test)",
            "application_domain": "Drug-like molecule generation / drug discovery (control of physicochemical properties: MW, LogP, HBD, HBA, TPSA)",
            "generation_method": "Conditional generation in latent space: encode SMILES + condition vector (five properties) -&gt; sample latent vector (Gaussian noise added) -&gt; decode with LSTM RNN decoder using stochastic sampling (100 stochastic write-outs per latent vector)",
            "novelty_of_chemicals": "Generated molecules were often structurally different from seed/target molecules (examples shown); model could produce molecules with property values beyond the training-set range (e.g., larger LogP and TPSA), but no explicit quantitative novelty metrics (e.g., % not in training set or fingerprint similarity distributions) were reported in the paper.",
            "application_specificity": "Specificity enforced by embedding five target properties (MW, LogP, HBD, HBA, TPSA) into a condition vector provided to both encoder and decoder; success evaluated by whether generated molecules match target properties within a ±10% error range (relative to the dataset averages as defined in the paper).",
            "evaluation_metrics": "Success rate (defined as 100 / number of generation attempts to obtain 100 molecules meeting all five properties), number of valid molecules produced during attempts, property-value distributions (e.g., LogP and TPSA histograms), and qualitative inspection of structures; validity checked with RDKit.",
            "results_summary": "CVAE produced drug-like molecules matching five target properties within a 10% tolerance in demonstrated cases (Aspirin, Tamiflu, others). It could selectively tune a single property (e.g., LogP) in many cases and could generate molecules with properties beyond the training-set maximum (e.g., LogP &gt; training max), though validity rates decreased. Overall success rates were low (example: generating 100 aspirin-like molecules required 28,840 attempts with condition-guided sampling; success rates varied by target and sampling strategy). Sampling latent vectors around known molecules performed best; sampling around the target molecule often produced many duplicates.",
            "comparison_to_other_methods": "Compared conceptually to the jointly-trained VAE + property predictor + Gaussian process optimization method (Gómez-Bombarelli et al.); CVAE differs by directly incorporating properties into encoder and decoder, avoiding a separate optimization step and enabling independent control of properties and structure in many cases. The paper also contrasts CVAE with adversarial autoencoders and sequence-based generative models in related work, but no numerical head-to-head benchmarks are reported.",
            "limitations_and_challenges": "Low success rate for generating molecules that meet multiple simultaneous property constraints; strong correlations among target properties limit independent control (e.g., MW correlated with TPSA); high rate of invalid SMILES due to discrete sequence decoding (stochastic write-out mitigates but does not eliminate); lack of 3D conformational information in SMILES limits applications requiring conformation; difficulty decoding directly to molecular graphs noted as an open problem; difficulty in pushing some properties arbitrarily far (e.g., LogP beyond certain values) likely due to property coupling and training data limits.",
            "uuid": "e8887.0",
            "source_info": {
                "paper_title": "Molecular generative model based on conditional variational autoencoder for de novo molecular design",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "RNN-LSTM SMILES language models",
            "name_full": "Recurrent neural network (LSTM) based SMILES sequence generative models",
            "brief_description": "Sequence models that learn the probability distribution of next SMILES character/token and generate novel molecules by sampling sequences; used both as standalone generative models and as components (encoder/decoder) within autoencoder frameworks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RNN (LSTM) SMILES language model / RNN encoder-decoder",
            "model_type": "Recurrent neural network with LSTM cells (sequence model / language model)",
            "model_size": null,
            "training_data": "SMILES strings from chemical databases (in this paper the authors use ZINC; related works use ZINC/ChEMBL and other SMILES corpora), but exact datasets for all cited works are not specified in this paper.",
            "application_domain": "De novo molecular generation for drug discovery and activity-focused library generation",
            "generation_method": "Sequence modeling of SMILES: learn next-character probabilities, then sample sequences (stochastic write-out) to produce novel SMILES; can be fine-tuned via transfer learning or coupled with conditional vectors in encoder-decoder setups.",
            "novelty_of_chemicals": "Paper reports that sampling stochastic outputs from the decoder can yield multiple different molecules from the same latent/condition vector; no explicit quantitative novelty metrics for RNN-only models are provided in this paper.",
            "application_specificity": "Can be tailored via transfer learning to focus on molecules with certain biological activities (referenced) or by constraining generation through conditioning vectors or downstream optimization (e.g., reinforcement learning). In this work, RNNs are used as the encoder/decoder within the CVAE to realize property-conditioned generation.",
            "evaluation_metrics": "Validity of generated SMILES (checked with RDKit), property matching to target constraints, and structural similarity analyses shown qualitatively; detailed metrics for standalone RNN models not provided in this paper.",
            "results_summary": "Authors implemented 3-layer LSTM RNNs (500 hidden units per layer) as encoder and decoder inside the CVAE and achieved conditional molecule generation; stochastic sampling increased valid output variety but invalid-SMILES rate remains a challenge.",
            "comparison_to_other_methods": "RNN/LSTM sequence models are discussed as an alternative generative approach used in prior work and as a building block for CVAE; the paper suggests discrete SMILES sequential decoding contributes to a high invalidity rate compared to potential graph-based decoders.",
            "limitations_and_challenges": "Discrete SMILES representation leads to a high rate of invalid molecules upon decoding; SMILES lack 3D conformational information; sequence models may produce duplicates and struggle with long-range chemical validity rules unless augmented (e.g., grammar-VAE, reinforcement learning).",
            "uuid": "e8887.1",
            "source_info": {
                "paper_title": "Molecular generative model based on conditional variational autoencoder for de novo molecular design",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "Transfer learning (SMILES models)",
            "name_full": "Transfer learning applied to SMILES-based generative models",
            "brief_description": "Fine-tuning pre-trained SMILES sequence models on task-specific, often smaller datasets to bias generation toward molecules with desired biological activities or properties.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Transfer learning of SMILES generative models",
            "model_type": "Fine-tuning of pre-trained sequence models (typically RNN/LSTM) on task-specific data",
            "model_size": null,
            "training_data": "Pre-trained on large SMILES corpora (e.g., ZINC); fine-tuned on small datasets with specific biological activities (as reported in referenced works), not specified in detail within this paper.",
            "application_domain": "Drug discovery — generating focused molecule libraries with desired biological activities",
            "generation_method": "Pre-train SMILES language model on large corpus, then fine-tune (transfer) on small activity-labeled datasets to bias output; generation via sampling from fine-tuned model.",
            "novelty_of_chemicals": "Not quantified in this paper; cited works reportedly produced task-focused libraries but this paper only references the approach.",
            "application_specificity": "Specificity arises from fine-tuning on molecules exhibiting the target activity, thereby biasing the generative distribution toward that activity.",
            "evaluation_metrics": "Not reported in detail in this paper (referenced works contain evaluations), but generally includes activity enrichment, validity, and diversity in related literature.",
            "results_summary": "Mentioned as an effective approach when task-specific training data are limited (cited Segler et al. and Gupta et al.). No empirical results from transfer learning experiments are reported in this paper itself.",
            "comparison_to_other_methods": "Described as useful for low-data regimes compared to training from scratch; no quantitative comparison in this paper.",
            "limitations_and_challenges": "Dependent on quality and size of fine-tuning dataset; potential overfitting to small datasets and limited exploration beyond the fine-tuned chemical space.",
            "uuid": "e8887.2",
            "source_info": {
                "paper_title": "Molecular generative model based on conditional variational autoencoder for de novo molecular design",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "Reinforcement learning fine-tuning",
            "name_full": "Reinforcement learning (RL) methods for optimizing pre-trained molecular generative models",
            "brief_description": "Approaches that modify a pre-trained molecular generative model via reinforcement learning to bias generation toward molecules with desired properties or objective functions (e.g., drug-likeness, target properties).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Reinforcement learning fine-tuning of molecular generative models",
            "model_type": "Reinforcement learning (policy gradient / objective-reward methods) applied to sequence or generative models (RNNs/VAEs/GANs)",
            "model_size": null,
            "training_data": "Pre-trained on chemical SMILES corpora (e.g., ZINC); RL optimization uses property evaluators (scoring functions) rather than additional training data; specifics not provided in this paper.",
            "application_domain": "Property-optimized de novo molecular design (e.g., maximize drug-likeness, specific property ranges)",
            "generation_method": "Start from a pre-trained generator (SMILES-based); apply RL (reward function encodes desired properties) to fine-tune generator parameters so that sampled molecules optimize the reward.",
            "novelty_of_chemicals": "Not quantified in this paper; RL is referenced as a means to further improve property optimization beyond pre-trained distributions.",
            "application_specificity": "Specificity enforced via the reward function used during RL (property-based scoring); referenced as a way to impose several properties on generated molecules.",
            "evaluation_metrics": "Not specified in this paper; in related literature commonly used metrics include property improvement, validity, uniqueness, and diversity after RL fine-tuning.",
            "results_summary": "Cited as prior work (Olivecrona et al., Guimaraes et al., Jaques et al.) that can modify pre-trained generators to achieve target properties. The authors suggest RL could complement CVAE (e.g., to improve success rate) but do not present RL experiments here.",
            "comparison_to_other_methods": "Positioned as an approach that can further bias generation compared to pure conditional generation methods; no empirical comparisons presented in this paper.",
            "limitations_and_challenges": "Potential mode collapse, dependence on reward design, and requirement for careful balance between property optimization and molecular validity/diversity — these are general RL challenges and are implied though not exhaustively discussed in this paper.",
            "uuid": "e8887.3",
            "source_info": {
                "paper_title": "Molecular generative model based on conditional variational autoencoder for de novo molecular design",
                "publication_date_yy_mm": "2018-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Automatic chemical design using a data-driven continuous representation of molecules",
            "rating": 2
        },
        {
            "paper_title": "Generating focused molecule libraries for drug discovery with recurrent neural networks",
            "rating": 2
        },
        {
            "paper_title": "Generative recurrent networks for de novo drug design",
            "rating": 2
        },
        {
            "paper_title": "Molecular de novo design through deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Objective-reinforced generative adversarial networks (ORGAN) for sequence generation models",
            "rating": 2
        },
        {
            "paper_title": "Chemical space mimicry for drug discovery",
            "rating": 2
        },
        {
            "paper_title": "Grammar variational autoencoder",
            "rating": 1
        },
        {
            "paper_title": "Junction tree variational autoencoder for molecular graph generation",
            "rating": 1
        }
    ],
    "cost": 0.0124505,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Molecular generative model based on conditional variational autoencoder for de novo molecular design</h1>
<p>Jaechang Lim ${ }^{1}$, Seongok Ryu ${ }^{1}$, Jin Woo Kim ${ }^{1}$ and Woo Youn Kim ${ }^{1,2 *}$ (D)</p>
<h4>Abstract</h4>
<p>We propose a molecular generative model based on the conditional variational autoencoder for de novo molecular design. It is specialized to control multiple molecular properties simultaneously by imposing them on a latent space. As a proof of concept, we demonstrate that it can be used to generate drug-like molecules with five target properties. We were also able to adjust a single property without changing the others and to manipulate it beyond the range of the dataset.</p>
<p>Keywords: Molecular design, Conditional variational autoencoder, Deep learning</p>
<h2>Background</h2>
<p>The ultimate goal of molecular design for new materials and drugs is to directly generate molecules with the desired properties. This is apparently challenging work because a molecular space is extraordinarily vast, discrete, and disorganized with diverse types of molecules. For instance, $10^{8}$ molecules have been synthesized [1], whereas it is estimated that there are $10^{23}-10^{60}$ drug-like molecules [2]. Despite advances in experimental techniques, it is too demanding to find molecules suitable for specific applications only through experiments.</p>
<p>Computer-aided molecular design has attracted much attention as a promising solution to overcome the experimental limitation [3-6]. Fast calculation methods along with reasonable accuracy and very low cost enable highthroughput virtual screening to find molecules with target properties. A common strategy is to select computationally top molecules out of millions of molecules in a virtual library and then verify them experimentally, leading to a significant reduction in time and efforts. Molecules in the library may not meet the given criteria. In this case, traditional optimization methods such</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>as a genetic algorithm can be used to further improve molecular properties beyond the criteria by structural modifications [7-9]. However, they have a fundamental limitation in terms of efficiency because many trials and errors are inevitable to optimize molecular properties in a huge molecular space.</p>
<p>Recently emerging generative models based on deep learning techniques may offer a viable solution for more efficient molecular design. Gómez-Bombarelli et al. adopted a variational autoencoder [10] to optimize the molecular properties in a latent space in which molecules are expressed as a real vector [11]. The key advantage of this method is that a gradient-based optimization becomes feasible because the latent space is continuous and differentiable. It has been successfully applied to improving the partition coefficient of drug candidates and the delayed fluorescent emission rate of organic light emitting diode candidates. Blaschke et al. employed the adversarial autoencoder [12] (AAE) and the Bayesian optimization to generate ligands specific to the dopamine type 2 receptor [13]. Kadurin et al. [14] compared the VAE and AAE as a molecular generation model in terms of the reconstruction error and variability of the output molecular fingerprints. In addition to those autoencoderbased models, a generative model developed for natural language processing has also been used for molecular design [15-18]. Molecular structures can be expressed</p>
<h2>Springer Open</h2>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>with SMILES. Then, this model learns the probability distribution of the next character of a given piece of SMILES. Yuan et al. [16] designed potential inhibitors for a target protein and tested them in experiments. Based on the natural language processing model, Segler et al. [17] and Gupta et al. [18] applied transfer learning to molecular design for certain biological activities. This approach is especially useful when there is not enough data to train artificial neural networks in the normal way. Olivecrona et al. [19], Guimaraes et al. [20], and Jaques et al. [21] proposed a reinforcement learning method to modify a pre-trained molecular generative model so as to impose several properties in molecules generated from the generative model.</p>
<p>We note that various molecular properties are correlated with each other. Therefore, adjusting one target property by structural modifications may cause an undesired change in other properties. To avoid this problem in rational molecular design, one has to control several properties at the same time. Here, we propose a molecular generative model using the conditional variational autoencoder (CVAE) [22] suitable for multivariable control. In addition to the advantages of using the latent space, our method can incorporate the information of molecular properties in the encoding process and manipulate them in the decoding process.</p>
<p>As a proof of concept, we used the CVAE to generate drug-like molecules satisfying five target properties at the same time: molecular weight (MW), partition coefficient (LogP), number of hydrogen bond donor (HBD), number of hydrogen acceptor (HBA), and topological polar surface area (TPSA). We were able to produce a number of molecules with the specific values of the five target properties within a given range. It was also possible to adjust a single target property without changing the others. Furthermore, we were able to generate molecules with properties beyond the range of the database.</p>
<h2>Method</h2>
<h2>Conditional variational autoencoder (CVAE)</h2>
<p>We selected the CVAE as a molecular generator. It is one of the most popular generative models which generates objects similar to but not identical to a given dataset. In particular, it is distinguished from the VAE in that it can impose certain conditions in the encoding and decoding processes. To elucidate the difference between VAE and CVAE, we compared their objective functions with one another. The objective function of the VAE is given by</p>
<p>$$
E[\log P(X \mid z)]-D_{K L}[Q(z \mid X) | P(z)]
$$</p>
<p>where $E$ denotes an expectation value, $P$ and $Q$ are probability distributions, $D_{K L}$ is the Kullback-Leibler divergence, and $X$ and $z$ indicate the data and latent spaces,
respectively. The first and second terms are often called the reconstruction error and the KL term, respectively. In an autoencoder, $Q(z \mid X)$ and $P(X \mid z)$ are approximated by an encoder and a decoder, respectively. A key difference of the CVAE from the VAE is to embed the conditional information in the objective function of the VAE, leading to the revised objective function as follow:</p>
<p>$$
E[\log P(X \mid z, c)]-D_{K L}[Q(z \mid X, c) | P(z \mid c)]
$$</p>
<p>where $c$ denotes a condition vector. The condition vector $c$ is directly involved in the encoding and decoding processes. In our model, the molecular properties we want to control were represented as the condition vector. As a result, the CVAE can generate molecules with the target properties imposed by the condition vector.</p>
<p>Incorporating molecular properties in the VAE to generate molecules with desirable properties are also possible through a two-step model proposed by GómezBombarelli et al. In this method, the VAE is trained jointly with an additional neural network for property prediction. Subsequently, a Gaussian process model creates a mapping from the resulting latent space to the associated molecular properties. Finally, property optimization in the resulting latent space is performed by a gradient descent optimization method.</p>
<p>The key difference of our CVAE model from the jointly trained VAE model is that the molecular properties are directly incorporated into both the encoder and decoder. The resulting latent vector is composed of two parts: the first part is for the target molecular properties, while the second part involves the molecular structures and the other properties. Therefore, the desired molecular properties can be embedded in a target molecular structure simply by setting a condition vector. In other words, one can control the structure and the properties independently except for some cases in which the properties are strongly coupled to a molecular scaffold. This is particularly useful to incorporate a certain property in a given molecule just with a marginal structure modification. After all, the CVAE is less sensitive to the continuity and smoothness of the latent space, because it does not require the derivative of the latent space with respect to the latent vector of the molecular structure. Another technical difference of the CVAE from the jointly trained VAE is that it does not need any further optimization process, which is inevitable in the jointly trained VAE for each different property value.</p>
<h2>Molecular representation and model construction</h2>
<p>We represented molecules with SMILES codes to take advantage of state-of-the-art deep learning techniques that are specialized in dealing with texts and sequences. Each SMILES code was canonicalized for a unique</p>
<p>molecular representation. One 'E' was padded on the end of the SMILES code to indicate the end of the string. Subsequently, each character including 'E' is represented with a one-hot vector, resulting in an input matrix. Each one-hot vector of the input matrix is transformed to an embedding vector with the size of 300, and then the input matrix is concatenated with a predefined condition vector. The first, second, and last entries of the condition vector are filled with information consisting of the MW, LogP, and TPSA, respectively, while the remaining two entries are labeled by the HBD and HBA as shown in Fig. 1. The values of MW, logP, and TPSA are normalized from -1.0 to 1.0. HBD and HBA are expressed with a one-hot vector, because they are integer numbers.</p>
<p>The resulting matrix is subjected to the encoder of the CVAE to generate a latent vector. We adopted the so-called recurrent neural network (RNN) with an LSTM cell for both the encoder and decoder of the CVAE [23]. They are made of a 3-layer RNN with 500 hidden nodes on each layer. A softmax layer was used in each output of the decoder cell, and a cross entropy was used as the cost function of the reconstruction error. The latent vector concatenated with the condition vector becomes an input of the decoder at each time step of the RNN cell. Finally, the output vector of each decoder cell is transformed to a vector whose size is equal to that of the one-hot vector of the input matrix. The softmax activation function is applied to each transformed vector. The encoder and decoder are optimized to minimize the cost function of the CVAE. To generate a molecule with the target properties imposed by the condition vector, the cell of the RNN decoder are unrolled for 120 times. All characters before 'E' were taken in the stochastic write-out process, and if 'E' did not appear in the 120 characters, the result was considered as invalid. Each output vector of the decoder cell represents the probability distribution of the SMILES code characters and 'E'. Finally, the output vector is converted to a SMILES code. It should be noted that</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>even a single wrong character in the resulting SMILES code gives rise to an invalid molecule. To increase the rate of valid SMILES codes, we used the stochastic writeout method which samples each character of SMILES according to a probability distribution. As a result, a single set of latent and condition vectors may give a number of different molecules. We performed 100 times the stochastic write-out per one latent vector and took all valid molecules except duplicated ones for later analysis.</p>
<h3>Dataset and hyperparameters</h3>
<p>RDKit [24], an open source cheminformatics package, was used for checking out the validity of the generated SMILES codes and calculating the five target properties of the molecules.</p>
<p>The total dataset is made of molecules randomly selected from the ZINC dataset [25]. Generally, with more data, the performance becomes better. Typical deep learning models need hundreds of thousands of data points. We checked out the convergence of the results with respect to the size of the data in our case. The use of 5,000,000 ZINC molecules did not increase both the validation and the success rates of generating molecules with the target properties compared to those from 500,000 ZINC molecules. Thus, we adopted the dataset of the 500,000 molecules, 80% of which were used for training, and the rest was used for the test. The distribution of the five target properties in the total dataset is shown in Fig. 2. The learning rate was set to 0.0001 and exponentially decayed at a rate of 0.97. The model was trained until converged. In the performance evaluation of the CVAE, if each target property of the generated molecules was different from the given target value with the 10%</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p><strong>Fig. 2</strong> Distribution of molecular weight, LogP, HBD, HBA, and TPSA in the total dataset (500,000)</p>
<p>error range of the average value of the total dataset, we regarded those molecules as successful. The source code is available from GitHub (https://github.com/jaechangli m/CVAE).</p>
<h2>Result</h2>
<p>As the first application, we demonstrated that the CVAE method can generate molecules with specific values for the five target properties by applying it to Aspirin and Tamiflu. The values of the (MW, LogP, HBD, HBA, and TPSA) for Aspirin and Tamiflu are (180.04, 1.31, 1, 3, and 63.6) and (312.2, 1.285, 2, 5, and 90.64), respectively. The condition vector of each molecule was made by those values. Latent vectors to be concatenated with the condition vector were sampled by adding a Gaussian type noise to the latent vector of a molecule selected randomly in the training set. Figure 3a, b show nine molecules produced with the condition vector of Aspirin and Tamiflu, respectively. All of them had similar properties to those of Aspirin and Tamiflu within an error range of 10%, respectively. However, the molecular structures in Fig. 3 are considerably different from those of the original molecules because of the latent vectors chosen randomly from the training set.</p>
<p>The second application was to generate molecules similar in both properties and structure to the mother molecule by sampling latent vectors around that of the mother. Figure 4 shows the molecules generated in such a way from Aspirin. They look very similar to Aspirin and also have similar properties with those of Aspirin within an error range of 10%.</p>
<p>As the third case study, we tested whether the CVAE method can change only a single property without changing the others. The condition vector was constructed with the MW, HBD, HBA, and TPSA of Tamiflu, and we varied LogP from 0.0 to 3.0. Latent vectors were sampled around that of Tamiflu. Figure 5 shows the result. All the molecules have similar properties to the original ones except LogP as desired. The molecules from the top left to the bottom right have gradually increasing LogP values from - 0.23 to 3.55. In some cases, however, such a delicate control of individual properties was not possible. For instance, we could not generate molecules with a LogP beyond 4.0. It is probably because LogP is not completely independent from the other four properties, so a substantial change in LogP entails a change in the other properties. Moreover, it was difficult to adjust the MW and TPSA independently because the MW and TPSA are highly correlated with one another.</p>
<p>Finally, we investigated the possibility to change a specific molecular property beyond the range of a training set. Latent vectors were sampled around molecules in the training set. In the condition vector, the four</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p><strong>Fig. 3</strong> Molecules generated by the CVAE with the condition vector made of the five target properties of <strong>a</strong> Aspirin and <strong>b</strong> Tamiflu</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p><strong>Fig. 4</strong> Molecules generated by the CVAE with the condition vector made of the five target properties of Aspirin and the latent vector slightly modified from that of Aspirin</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<h1>3.55</h1>
<p>Fig. 5 Molecules generated by the CVAE with the condition vector made of MW, HBD, HBA, and TPSA of Tamiflu and continuously changing LogP
properties were given randomly except for a single target property. The target property was set to $10 \%$ larger than its maximum value in the training set (e.g., 5.5 for</p>
<p>LogP and 165 for TPSA). Figure 6 shows the resulting molecules. Indeed, it was able to generate molecules with a LogP larger than 5.5 (Fig. 6a) and molecules with</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p><strong>Fig. 6 a</strong> Molecules with LogP larger than 5.5. <strong>b</strong> Molecules with TPSA larger than 165</p>
<p>a TPSA larger than 165 (Fig. 6b). We compared the distribution of the LogP and TPSA for 1000 randomly selected molecules from the training set and 1000 generated molecules with property values outside of the range of the dataset (toward larger values). Figure 7 shows that the distribution of the target properties are shifted to larger values, leading to an increased ratio of molecules with property values outside of the range. The rate of valid molecules is relatively low compared to the case of generating molecules with property values in the range of the dataset.</p>
<p>We analyzed the latent space constructed by the CVAE. Two principle axes were extracted by principal component analysis. Figure 8 shows the two components of the latent vectors of 1000 randomly selected molecules from the test set with their MW, LogP, and TPSA values. Molecules with similar properties are likely located around a same region of the latent space in the jointly trained VAE. In our CVAE model, the latent vector is comprised of two parts as explained in the method section. Therefore, a specific region in the latent space does not necessarily have a correlation with the target molecular properties, which are controlled by the condition vector. This is good because the separation of information enables a more flexible control of the molecular structure and properties when generating new molecules.</p>
<p>Apart from the successful applications of the CVAE method, it has a drawback that should be resolved. The success rate of generating desirable molecules is very low. We tested how many attempts were required to generate 100 molecules with the five desired properties, and how many valid molecules were generated during those attempts. We also compared when the condition vector is set randomly or to target properties to show the effect of the condition vector for generating desirable molecules.</p>
<p>Table 1 summarizes the number of attempts for generating 100 molecules whose five properties are same as those of aspirin, Tamiflu, Lenalidomide, Rivaroxaban, and Pregabalin, respectively. Lenalidomide, Rivaroxaban, and Pregabalin are top-selling small molecule drugs in 2016 [26]. In Table 1, 'condition' means that the condition vector was set as the five properties of the target molecules, whereas 'random' means that the condition vector was randomly made. The number of valid molecules in Table 1 indicates the number of valid molecules generated during the attempts to create molecules with the five desired properties. For example, 100 aspirin-like molecules and 32,567 valid molecules were obtained from 28,840 attempts to create aspirin-like molecules. The reason why the number of valid molecules is larger than the number of attempts is that the stochastic write-out process is performed 100 times for each attempt. All successful molecules (100 per each target molecule) are reported in the Supporting Information. It should be noted that the success rate dramatically dropped when the condition vector is randomly set. It clearly manifests that the successful molecules generated by the CVAE in the example studies were not the result of many random trials.</p>
<p>We further analyzed the performance of the CVAE by investigating the change in the success rate and the number of valid molecules according to latent vector sampling methods. We employed three different sampling methods: random, around the latent vectors of known</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p><strong>Fig. 7</strong> Distribution of <strong>a</strong> LogP and <strong>b</strong> TPSA for 1000 randomly selected molecules in training set and 1000 generated molecules with LogP and TPSA outside of the range of the dataset, respectively</p>
<table>
<thead>
<tr>
<th>Condition</th>
<th></th>
<th></th>
<th></th>
<th>Random</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Attempts</td>
<td>Number of valid</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>molecules</td>
<td>Success rate (100/</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>attempts, %)</td>
<td>Attempts</td>
<td>Number of valid</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>molecules</td>
<td>Success rate</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>(100/attempts,</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>%)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Aspirin</td>
<td>28,840</td>
<td>32,567</td>
<td>0.34</td>
<td>758,199</td>
<td>711,660</td>
<td>0.014</td>
</tr>
<tr>
<td>Tamiflu</td>
<td>15,960</td>
<td>34,696</td>
<td>0.62</td>
<td>798,183</td>
<td>741,960</td>
<td>0.013</td>
</tr>
<tr>
<td>Lenalidomide</td>
<td>50,200</td>
<td>89,230</td>
<td>0.19</td>
<td>865,695</td>
<td>822,060</td>
<td>0.012</td>
</tr>
<tr>
<td>Rivaroxaban</td>
<td>92,620</td>
<td>47,574</td>
<td>0.11</td>
<td>866,205</td>
<td>817,800</td>
<td>0.012</td>
</tr>
<tr>
<td>Pregabalin</td>
<td>77,680</td>
<td>84,371</td>
<td>0.13</td>
<td>782,010</td>
<td>723,360</td>
<td>0.014</td>
</tr>
</tbody>
</table>
<p><strong>Table 1</strong> Numbers of attempts and valid molecules for generating 100 molecules whose five properties are the same with those of Aspirin, Tamiflu, Lenalidomide, Rivaroxaban, and Pregabalin</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p><strong>Fig. 8</strong> The latent space of 1000 randomly selected molecules with MW, LogP and TPSA values</p>
<p>molecules, and around the latent vectors of target molecules. For all the sampling methods, the condition vector was constructed using the five properties of the target molecules. The generation process was continued until 100 molecules with the five target properties were successfully created from a single target molecule, and it was repeated for 100 different target molecules selected randomly from the ZINC dataset. Table 2 shows the average</p>
<p>Table 2 Number of generation attempts and number of valid molecules for three different sampling methods of latent vectors</p>
<p>|  Sampling method | Average number of valid molecules | Average number of attempts | Success rate
(100/attempts,
$\%$  |
| --- | --- | --- | --- |
|  Around target molecules | 67640.5 | 199340.1 | 0.05  |
|  Around known molecules | 31799.4 | 21659.9 | 0.46  |
|  Random | 50316.4 | 78888.2 | 0.12  |</p>
<p>The generation process was continued until 100 molecules with the five target properties were successfully created from a single target molecule, and it was repeated for 100 target molecules selected randomly from the ZINC dataset. The table shows the average values over the 100 target molecules values for the success rate and the number of valid molecules over the 100 target molecules. It was unexpected that sampling latent vectors around a target molecule was the most ineffective in terms of the success rate and valid molecules because of the high rate of duplicated molecules. In this case, the structure of the generated molecules was very similar to that of the target molecule as shown in Fig. 4. Sampling latent vectors around those of known molecules performed best. Because the known molecules were randomly selected from the ZINC set, their structures and properties would be considerably different from those of a target molecule. Nonetheless, we were able to generate molecules with the desired properties from those latent vectors with a relatively high success rate. It manifests that the condition vector appropriately modified the molecular structures to have the target properties. Finally, it was also possible to generate desirable molecules from completely random latent vectors but with a low success rate.</p>
<p>We suspect that at some part the overall low success rates regardless of the latent vector sampling methods are due to the strong correlation between the five target properties. In addition, it is known that the discrete nature of SMILES causes a high rate of invalid molecules in the decoding process from latent vectors to molecules [27]. The stochastic write-out method circumvents this problem, but more fundamental solutions should be devised. More severely, SMILES does not have the 3D conformational information of molecular structures. Therefore, it must have limitations in applications in which conformational effects are critical. Molecular graph representation incorporating conformational information can be a promising alternative. Encoding molecular graphs seems to be straightforward, but decoding from a latent space to molecular graphs is still an open problem. Recently, significant progress along this line has been made [28-30]. Such a better molecular representation may also improve the success rate of molecular generation. We expect that the success rate may be further improved by using the grammar variational autoencoder [27] and the reinforcement learning $[19,20]$.</p>
<h2>Conclusion</h2>
<p>We proposed a new molecular design strategy based on the conditional variational autoencoder. Instead of highthroughput virtual screening, our method as one of the deep learning-based generative models directly produces molecules with desirable target properties. In particular, its strength is controlling multiple target properties simultaneously by imposing them on a condition vector. We demonstrated that it was possible to generate druglike molecules with specific values for the five target properties (MW, LogP, HBD, HBA, and TPSA) within an error range of $10 \%$. In addition, we were able to selectively control LogP without changing the other properties and to increase a specific property beyond the range of the training set. Thus, this new method has attractive applicability for efficient molecular design.</p>
<h2>Author's contributions</h2>
<p>Jaechang Lim, Seongok Ryu, Jin Woo Kim, and Woo Youn Kim organized this work. Jaechang Lim and Woo Youn Kim wrote the paper. All authors read and approved the final manuscript.</p>
<h2>Author details</h2>
<p>${ }^{1}$ Department of Chemistry, KAIST, 291 Daehak-ro, Daejeon 34141, Republic of Korea. ${ }^{2}$ KI for Artificial Intelligence, KAIST, 291 Daehak-ro, Daejeon 34141, Republic of Korea.</p>
<h2>Acknowledgements</h2>
<p>This work was supported by Basic Science Research Programs through the National Research Foundation of Korea (NRF) funded by the Ministry of Science, ICT and Future Planning (NRF-2017R1E1A1A01078109).</p>
<h2>Competing interests</h2>
<p>The authors declare that they have no competing interests.</p>
<h2>Publisher's Note</h2>
<p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
<p>Received: 14 March 2018 Accepted: 29 June 2018 Published online: 11 July 2018</p>
<h2>References</h2>
<ol>
<li>
<p>Kim S, Thiessen PA, Bolton EE, Chen J, Fu G, Gindulyte A, Han L, He J, He S, Shoemaker BA, Wang J, Yu B, Zhang J, Bryant SH (2016) PubChem substance and compound databases. Nucl Acids Res 44(D1):D1202</p>
</li>
<li>
<p>Polishchuk PG, Madzhidov TI, Varnek A (2013) Estimation of the size of drug-like chemical space based on GDB-17 data. J Comput Aided Mol Des 27(8):675</p>
</li>
<li>Shoichet BK (2004) Virtual screening of chemical libraries. Nature 432(7019):862. https://doi.org/10.1038/nature03197</li>
<li>Scior T, Bender A, Tresadern G, Medina-Franco JL, Martínez-Mayorga K, Langer T, Cuanalo-Contreras K, Agrafiotis DK (2012) Recognizing pitfalls in virtual screening: a critical review. J Chem Inf Model 52(4):867. https://doi. org/10.1021/ci200528d</li>
<li>Cheng T, Li Q, Zhou Z, Wang Y, Bryant SH (2012) Structure-based virtual screening for drug discovery: a problem-centric review. AAPS J 14(1):133. https://doi.org/10.1208/s12248-012-9322-0</li>
<li>Reymond JL, van Deursen R, Blum LC, Ruddigkeit L (2010) Chemical space as a source for new drugs. MedChemComm 1(1):30</li>
<li>Miyao T, Kaneko H, Funatsu K (2014) Ring-system-based exhaustive structure generation for inverse-QSPR/QSAR. Mol Inf 33(11-12):764</li>
<li>Hartenfeller M, Schneider G (2011) Enabling future drug discovery by de novo design. Wiley Interdiscip Rev Computat Mol Sci 1(5):742. https://doi. org/10.1002/wcms. 49</li>
<li>Rupakheti C, Virshup A, Yang W, Beratan DN (2015) Strategy to discover diverse optimal molecules in the small molecule universe. J Chem Inf Model 55(3):529. https://doi.org/10.1021/ci500749q</li>
<li>Kingma DP, Welling M (2013) Auto-encoding variational bayes. arXiv :1312.6114</li>
<li>Gómez-Bombarelli R, Wei JN, Duvenaud D, Hernández-Lobato JM, Sánchez-Lengeling B, Sheberla D, Aguilera-Iparraguirre J, Hirzel TD, Adams RP, Aspuru-Guzik A (2018) Automatic chemical design using a data-driven continuous representation of molecules. ACS Cent Sci 4(2):268-276. arXiv:1610.02415</li>
<li>Makhzani A, Shlens J, Jaitly N, Goodfellow I, Frey B (2015) Adversarial autoencoders. arXiv:1511.05644</li>
<li>Blaschke T, Olivecrona M, Engkvist O, Bajorath J, Chen H (2017) Application of generative autoencoder in de novo molecular design. arXiv :1711.07839</li>
<li>Kadurin A, Nikolenko S, Khrabrov K, Aliper A, Zhavoronkov A (2017) druGAN: an advanced generative adversarial autoencoder model for de novo generation of new molecules with desired molecular properties in silico. Mol Pharm 14(9):3098. https://doi.org/10.1021/acs.molpharmac eut.7b00346</li>
<li>Bjerrum EJ, Threlfall R (2017) Molecular generation with recurrent neural networks (RNNs). arXiv:1705.04612</li>
<li>Yuan W, Jiang D, Nambiar DK, Liew LP, Hay MP, Bloomstein J, Lu P, Turner B, Le QT, Tibshirani R, Khatri P, Moloney MG, Koong AC (2017) Chemical space mimicry for drug discovery. J Chem Inf Model 57(4):875. https:// doi.org/10.1021/acs.jcim.6b00754. arXiv:1611.02796</li>
<li>Segler MHS, Kogej T, Tyrchan C, Waller MP (2018) Generating focused molecule libraries for drug discovery with recurrent neural networks. ACS Cent Sci 4(1):120-131. arXiv:1701.01329</li>
<li>Gupta A, Müller AT, Huisman BJH, Fuchs JA, Schneider P, Schneider G (2017) Generative recurrent networks for de novo drug design. Mol Inf. https://doi.org/10.1002/minf.201700111</li>
<li>Olivecrona M, Blaschke T, Engkvist O, Chen H (2017) Molecular de novo design through deep reinforcement learning. J Cheminform 9(1):1. arXiv :1704.07555</li>
<li>Guimaraes GL, Sanchez-Lengeling B, Outeiral C, Farias PLC, Aspuru-Guzik A (2017) Objective-reinforced generative adversarial networks (ORGAN) for sequence generation models. arXiv:1705.10843</li>
<li>Jaques N, Gu S, Bahdanau D, Hernández-Lobato JM, Turner RE, Eck D (2016) Sequence tutor: conservative fine-tuning of sequence generation models with KL-control. arXiv:1611.02796</li>
<li>D.P Kingma, D.J Rezende, S Mohamed, M Welling, (2014) Semi-supervised learning with deep generative models, pp 1-9. arXiv:1406.5298</li>
<li>Hochreiter S, Urgen Schmidhuber J (1997) Long short-term memory. Neural Comput 9(8):1735</li>
<li>RDKit. http://www.rdkit.org/. Accessed Sept 2017</li>
<li>Irwin JJ, Sterling T, Mysinger MM, Bolstad ES, Coleman RG (2012) ZINC: a free tool to discover chemistry for biology. J Chem Inf Model 52(7):1757. https://doi.org/10.1021/ci3001277</li>
<li>Top selling small molecule drugs in 2016. https://www.genengnews .com/the-lists/the-top-15-best-selling-drugs-of-2016/77900868. Accessed Jan 2018</li>
<li>Kusner MJ, Paige B, Hernández-Lobato JM (2017) Grammar variational autoencoder. arXiv:1703.01925</li>
<li>Wang H, Wang J, Wang J, Zhao M, Zhang W, Zhang F, Xie X, Guo M (2017) GraphGAN: graph representation learning with generative adversarial nets. arXiv:1711.08267</li>
<li>You J, Ying R, Ren X, Hamilton WL, Leskovec J (2018) GraphRNN: a deep generative model for graphs. arXiv:1802.08773</li>
<li>W Jin, R Barzilay, T Jaakkola (2018) Junction tree variational autoencoder for molecular graph generation. arXiv:1802.04364</li>
</ol>
<h2>Submit your manuscript to a SpringerOpen ${ }^{\circledR}$ journal and benefit from:</h2>
<h2>- Convenient online submission</h2>
<ul>
<li>Rigorous peer review</li>
<li>Open access: articles freely available online</li>
<li>High visibility within the field</li>
<li>Retaining the copyright to your article</li>
</ul>
<p>Submit your next manuscript at $\rightarrow$ springeropen.com</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Correspondence: wooyoun@kaist.ac.kr
${ }^{1}$ Department of Chemistry, KAIST, 291 Daehak-ro, Daejeon 34141, Republic of Korea
Full list of author information is available at the end of the article&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>(c) The Author(s) 2018. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/ publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
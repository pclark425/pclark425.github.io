<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5087 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5087</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5087</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-68f141724814839d556a989646194be88641b143</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/68f141724814839d556a989646194be88641b143" target="_blank">Scaling Language Models: Methods, Analysis & Insights from Training Gopher</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper presents an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher.</p>
                <p><strong>Paper Abstract:</strong> Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5087.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5087.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gopher</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gopher (280B parameter Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 280-billion-parameter autoregressive Transformer language model trained on the MassiveText corpus; evaluated across 152 tasks including a logical-reasoning category where scale produced limited gains relative to knowledge tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gopher</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive Transformer (radford-style) with RMSNorm and relative positional encodings, SentencePiece tokenizer (32k vocab, byte-level backoff), trained for 300B tokens on MassiveText. Uses bfloat16 for large sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>280B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Logical reasoning category (aggregated across tasks, including BIG-bench logical tasks, MMLU math/logic subsets, and specific dataset instances like Abstract Algebra and Temporal Sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A heterogeneous set of tasks requiring strict logical and mathematical reasoning drawn from benchmarks such as BIG-bench (various logical tasks), MMLU (math/logic/exam problems), and task-specific subsets (e.g., Abstract Algebra, Temporal Sequences); tasks often presented in multiple-choice format where model must assign highest probability to correct option.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluate few-shot / zero-shot autoregressive next-token probability on multiple-choice formulations; compare across model scales (44M–280B) while holding dataset and training tokens fixed; prompting and few-shot selection used for many tasks. Also experimented with dialogue-prompting and dialogue fine-tuning (separate analyses).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Aggregate finding: Gopher attains state-of-the-art across many benchmarks overall but shows only modest or sometimes negative gains on strict mathematical and logical reasoning tasks. Example highlights: Gopher family reaches 60.0% average on MMLU (57 tasks total, not purely logical) vs GPT-3 43.9%; on some BIG-bench items (e.g., Figure-of-Speech detection) Gopher 280B = 52.7% vs 7.1B = 16.8% (large gain), but for strict logical/mathematical tasks (Abstract Algebra, Temporal Sequences, High School Mathematics subsets) Gopher sometimes underperforms smaller models (no numeric breakdown provided in text). Overall the paper reports that logical and mathematical reasoning see less benefit from scale compared to knowledge tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper reports limited benefits from scale for mathematical and logical reasoning: some reasoning-heavy tasks show little or no improvement or even deterioration at larger scale (examples: Abstract Algebra, Temporal Sequences, certain high-school math MMLU items). Failures include confidently stated factual and logical errors in dialogue; the model mixes correct and incorrect steps and lacks robust stepwise logical computation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Compared to prior large LMs (GPT-3 175B, Jurassic-1 178B, Megatron-Turing NLG 530B) Gopher outperforms LM SOTA on ~81% of tasks overall, but relative improvements are smaller or absent on math/logic tasks; on some commonsense and math tasks other large models (e.g., Megatron-Turing) slightly outperform Gopher on specific datasets (Winogrande, HellaSwag, PIQA by small margins).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>The Gopher family was trained on the same dataset for the same number of tokens to isolate model-size effects. The paper reports that scale yields the largest gains in reading comprehension, fact-checking and knowledge-intensive domains, but reduced benefit for maths and logical reasoning. When comparing 280B to smaller models (≤7.1B) across 152 tasks: 16 tasks (10.5%) had zero/no gains, 57 (37.5%) small gains, 79 (51.2%) large gains (>25% relative); logical/mathematical tasks are concentrated among those with smaller or negative gains. No specific ablation (e.g., chain-of-thought prompting) for reasoning was reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaling Language Models: Methods, Analysis & Insights from Training Gopher', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5087.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5087.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gopher family (scaling analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gopher family models (44M, 117M, 417M, 1.4B, 7.1B, 280B) scaling study</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A controlled scaling study where models of six parameter scales were trained on the same dataset (MassiveText) for the same number of tokens to isolate the effect of model size on diverse tasks, including logical reasoning and mathematics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gopher family (44M, 117M, 417M, 1.4B, 7.1B, 280B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive Transformers with varied layer counts and model dimensions; all trained for 300B tokens on MassiveText with identical training recipes where possible (optimizer, context length, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>44M; 117M; 417M; 1.4B; 7.1B; 280B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Task-category level evaluation (Maths, Logical Reasoning, Common Sense) across 152-task suite including MMLU and BIG-bench components</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice and language-modelling tasks that probe mathematics, formal/logical reasoning, and commonsense reasoning; model outputs are scored by selecting the highest-probability choice or computing bits-per-byte for LM tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Controlled scaling: hold dataset and training token count constant; measure per-task accuracy/BPB across model sizes; few-shot/zero-shot evaluation protocols (prompting) used for downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Scaling yields heterogeneous benefits: many knowledge domains improve strongly with scale, but maths/logical reasoning show smaller or negative improvements. Quantitatively across all tasks: 51.2% of tasks had >25% relative improvement comparing 280B to best ≤7.1B model; but logical/mathematical tasks cluster among those with modest or no gains. Specific numeric example: Figure-of-Speech detection (BIG-bench) improved from 16.8% (7.1B) to 52.7% (280B). No consistent numeric improvement reported for Abstract Algebra or Temporal Sequences—some larger models performed worse than smaller ones on those tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Scaling alone did not reliably improve strict logical reasoning or mathematical problem solving; some larger models gave worse results than smaller ones on select reasoning tasks. The study did not introduce specialized reasoning prompting (e.g., chain-of-thought) or symbolic solvers; thus conclusions pertain to plain scale/few-shot prompting under an autoregressive next-token objective.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Direct within-paper comparison isolates scale: best smaller-model performance is often 7.1B; Gopher 280B outperforms smaller models on most tasks but not uniformly on logical/mathematical items. Against external LMs (GPT-3, Jurassic-1, Megatron-Turing), Gopher generally surpasses prior LM SOTA overall while still trailing on some reasoning-heavy benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Controlled ablation: same training dataset and token budget across scales; analysis shows differential scaling benefits by task category. The paper reports that dataset composition (e.g., high proportion of books) and scale jointly contribute to gains in some domains; however, no ablation isolating dataset composition vs reasoning performance was given for logical tasks specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaling Language Models: Methods, Analysis & Insights from Training Gopher', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5087.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5087.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BIG-bench (logical subsets)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BIG-bench logical / reasoning tasks (subset used in evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large collaborative benchmark (BIG-bench) containing many diverse tasks including logical, temporal, and abstract-reasoning problems; the paper evaluates Gopher family on these tasks and reports mixed results for logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gopher (and Gopher family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Evaluated in few-shot/zero-shot multiple-choice format using next-token probability ranking; models are autoregressive Transformers trained as described for Gopher family.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>44M; 117M; 417M; 1.4B; 7.1B; 280B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Examples cited: Abstract Algebra, Logical Args, Temporal Sequences (BIG-bench subset); also other BIG-bench items described as 'logical reasoning' or 'epistemic reasoning'.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks require abstract/logical reasoning (e.g., solving algebraic reasoning problems, temporal sequence reasoning, argument/logic evaluation). Most are multiple-choice or structured text tasks drawn from BIG-bench.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Few-shot / zero-shot prompting with multiple-choice selection based on model-assigned probabilities; evaluations use the BIG-bench task definitions and scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Mixed: Gopher shows dramatic improvements on some BIG-bench tasks (e.g., Figure-of-Speech detection: 7.1B = 16.8% → 280B = 52.7%), but for several explicit logical/mathematical tasks like Abstract Algebra and Temporal Sequences Gopher 280B underperforms some smaller models (specific numeric scores not provided in text). Overall, logical-reasoning items in BIG-bench gained less from scale compared to knowledge tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported failure modes include degradation or no improvement with scale on Abstract Algebra and Temporal Sequences; indicates inability of scale-alone to reliably solve compositional/structured logical tasks. The evaluations did not incorporate reasoning-specific prompting (e.g., stepwise decomposition) within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Within BIG-bench evaluations the paper compares Gopher family members to each other; externally, Gopher is compared to LM baselines on BIG-bench items where available. The authors note that some prior LM families (GPT-2/J, T5 variants) show different scaling behaviour on these tasks, but the paper does not provide full cross-model numeric breakdowns for each BIG-bench logical task.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>The paper analyses per-task scaling behaviour and highlights that some tasks appear to be 'unlocked' by scale (large jumps) whereas others do not benefit; no additional ablations (e.g., training with targeted logic datasets or chain-of-thought prompting) are reported for BIG-bench logical subsets in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaling Language Models: Methods, Analysis & Insights from Training Gopher', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5087.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5087.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baseline LMs (GPT-3, Jurassic-1, Megatron-Turing)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prior large language models used as baselines (GPT-3, Jurassic-1, Megatron-Turing NLG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Existing large-scale autoregressive Transformer LMs cited as state-of-the-art baselines for comparison; paper compares Gopher performance to these models across multiple tasks including reasoning categories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3; Jurassic-1; Megatron-Turing NLG</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive Transformer models previously published or reported (GPT-3 = 175B, Jurassic-1 ≈178B, Megatron-Turing NLG ≈530B); used as prior LM SOTA baselines in cross-task comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3: 175B; Jurassic-1: 178B; Megatron-Turing NLG: 530B (as reported by sources cited in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Various benchmarks including language modelling datasets, MMLU, BIG-bench and reasoning-heavy tasks used for cross-model comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks include knowledge, comprehension, and reasoning tasks; comparisons are typically few-shot/zero-shot next-token probability accuracy on multiple-choice tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Paper uses published results from these baseline models for side-by-side percent-change comparisons against Gopher; it does not re-run or fine-tune those external models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper reports Gopher outperforms the 'current LM SOTA' (which includes these models) on ~81% of comparable tasks (100/124), though on certain reasoning/math/commonsense tasks differences are small or Gopher underperforms marginally (e.g., some commonsense tasks where Megatron-Turing slightly outperforms Gopher). Specific numeric comparisons are given per task in supplementary figures/tables.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>These baseline models are only mentioned via reported scores; the paper does not run them experimentally, so comparisons rely on published numbers that may differ in evaluation details or dataset exposure.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Comparisons serve as a reference to show where scale and dataset changes improved performance; the paper notes that improvements vs baselines are strongest in knowledge-intensive domains and weaker in strict logical/mathematical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaling Language Models: Methods, Analysis & Insights from Training Gopher', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>BIG-bench <em>(Rating: 2)</em></li>
                <li>Measuring Massive Multitask Language Understanding <em>(Rating: 2)</em></li>
                <li>Language Models are Few-Shot Learners <em>(Rating: 2)</em></li>
                <li>TruthfulQA: Measuring How Models Mimic Human Falsehoods <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5087",
    "paper_id": "paper-68f141724814839d556a989646194be88641b143",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "Gopher",
            "name_full": "Gopher (280B parameter Transformer)",
            "brief_description": "A 280-billion-parameter autoregressive Transformer language model trained on the MassiveText corpus; evaluated across 152 tasks including a logical-reasoning category where scale produced limited gains relative to knowledge tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gopher",
            "model_description": "Autoregressive Transformer (radford-style) with RMSNorm and relative positional encodings, SentencePiece tokenizer (32k vocab, byte-level backoff), trained for 300B tokens on MassiveText. Uses bfloat16 for large sizes.",
            "model_size": "280B",
            "logical_reasoning_task": "Logical reasoning category (aggregated across tasks, including BIG-bench logical tasks, MMLU math/logic subsets, and specific dataset instances like Abstract Algebra and Temporal Sequences)",
            "task_description": "A heterogeneous set of tasks requiring strict logical and mathematical reasoning drawn from benchmarks such as BIG-bench (various logical tasks), MMLU (math/logic/exam problems), and task-specific subsets (e.g., Abstract Algebra, Temporal Sequences); tasks often presented in multiple-choice format where model must assign highest probability to correct option.",
            "method_or_approach": "Evaluate few-shot / zero-shot autoregressive next-token probability on multiple-choice formulations; compare across model scales (44M–280B) while holding dataset and training tokens fixed; prompting and few-shot selection used for many tasks. Also experimented with dialogue-prompting and dialogue fine-tuning (separate analyses).",
            "performance": "Aggregate finding: Gopher attains state-of-the-art across many benchmarks overall but shows only modest or sometimes negative gains on strict mathematical and logical reasoning tasks. Example highlights: Gopher family reaches 60.0% average on MMLU (57 tasks total, not purely logical) vs GPT-3 43.9%; on some BIG-bench items (e.g., Figure-of-Speech detection) Gopher 280B = 52.7% vs 7.1B = 16.8% (large gain), but for strict logical/mathematical tasks (Abstract Algebra, Temporal Sequences, High School Mathematics subsets) Gopher sometimes underperforms smaller models (no numeric breakdown provided in text). Overall the paper reports that logical and mathematical reasoning see less benefit from scale compared to knowledge tasks.",
            "limitations_or_failure_cases": "Paper reports limited benefits from scale for mathematical and logical reasoning: some reasoning-heavy tasks show little or no improvement or even deterioration at larger scale (examples: Abstract Algebra, Temporal Sequences, certain high-school math MMLU items). Failures include confidently stated factual and logical errors in dialogue; the model mixes correct and incorrect steps and lacks robust stepwise logical computation.",
            "comparison": "Compared to prior large LMs (GPT-3 175B, Jurassic-1 178B, Megatron-Turing NLG 530B) Gopher outperforms LM SOTA on ~81% of tasks overall, but relative improvements are smaller or absent on math/logic tasks; on some commonsense and math tasks other large models (e.g., Megatron-Turing) slightly outperform Gopher on specific datasets (Winogrande, HellaSwag, PIQA by small margins).",
            "ablation_or_analysis_results": "The Gopher family was trained on the same dataset for the same number of tokens to isolate model-size effects. The paper reports that scale yields the largest gains in reading comprehension, fact-checking and knowledge-intensive domains, but reduced benefit for maths and logical reasoning. When comparing 280B to smaller models (≤7.1B) across 152 tasks: 16 tasks (10.5%) had zero/no gains, 57 (37.5%) small gains, 79 (51.2%) large gains (&gt;25% relative); logical/mathematical tasks are concentrated among those with smaller or negative gains. No specific ablation (e.g., chain-of-thought prompting) for reasoning was reported in this paper.",
            "uuid": "e5087.0",
            "source_info": {
                "paper_title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Gopher family (scaling analysis)",
            "name_full": "Gopher family models (44M, 117M, 417M, 1.4B, 7.1B, 280B) scaling study",
            "brief_description": "A controlled scaling study where models of six parameter scales were trained on the same dataset (MassiveText) for the same number of tokens to isolate the effect of model size on diverse tasks, including logical reasoning and mathematics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gopher family (44M, 117M, 417M, 1.4B, 7.1B, 280B)",
            "model_description": "Autoregressive Transformers with varied layer counts and model dimensions; all trained for 300B tokens on MassiveText with identical training recipes where possible (optimizer, context length, etc.).",
            "model_size": "44M; 117M; 417M; 1.4B; 7.1B; 280B",
            "logical_reasoning_task": "Task-category level evaluation (Maths, Logical Reasoning, Common Sense) across 152-task suite including MMLU and BIG-bench components",
            "task_description": "Multiple-choice and language-modelling tasks that probe mathematics, formal/logical reasoning, and commonsense reasoning; model outputs are scored by selecting the highest-probability choice or computing bits-per-byte for LM tasks.",
            "method_or_approach": "Controlled scaling: hold dataset and training token count constant; measure per-task accuracy/BPB across model sizes; few-shot/zero-shot evaluation protocols (prompting) used for downstream tasks.",
            "performance": "Scaling yields heterogeneous benefits: many knowledge domains improve strongly with scale, but maths/logical reasoning show smaller or negative improvements. Quantitatively across all tasks: 51.2% of tasks had &gt;25% relative improvement comparing 280B to best ≤7.1B model; but logical/mathematical tasks cluster among those with modest or no gains. Specific numeric example: Figure-of-Speech detection (BIG-bench) improved from 16.8% (7.1B) to 52.7% (280B). No consistent numeric improvement reported for Abstract Algebra or Temporal Sequences—some larger models performed worse than smaller ones on those tasks.",
            "limitations_or_failure_cases": "Scaling alone did not reliably improve strict logical reasoning or mathematical problem solving; some larger models gave worse results than smaller ones on select reasoning tasks. The study did not introduce specialized reasoning prompting (e.g., chain-of-thought) or symbolic solvers; thus conclusions pertain to plain scale/few-shot prompting under an autoregressive next-token objective.",
            "comparison": "Direct within-paper comparison isolates scale: best smaller-model performance is often 7.1B; Gopher 280B outperforms smaller models on most tasks but not uniformly on logical/mathematical items. Against external LMs (GPT-3, Jurassic-1, Megatron-Turing), Gopher generally surpasses prior LM SOTA overall while still trailing on some reasoning-heavy benchmarks.",
            "ablation_or_analysis_results": "Controlled ablation: same training dataset and token budget across scales; analysis shows differential scaling benefits by task category. The paper reports that dataset composition (e.g., high proportion of books) and scale jointly contribute to gains in some domains; however, no ablation isolating dataset composition vs reasoning performance was given for logical tasks specifically.",
            "uuid": "e5087.1",
            "source_info": {
                "paper_title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "BIG-bench (logical subsets)",
            "name_full": "BIG-bench logical / reasoning tasks (subset used in evaluation)",
            "brief_description": "A large collaborative benchmark (BIG-bench) containing many diverse tasks including logical, temporal, and abstract-reasoning problems; the paper evaluates Gopher family on these tasks and reports mixed results for logical reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gopher (and Gopher family)",
            "model_description": "Evaluated in few-shot/zero-shot multiple-choice format using next-token probability ranking; models are autoregressive Transformers trained as described for Gopher family.",
            "model_size": "44M; 117M; 417M; 1.4B; 7.1B; 280B",
            "logical_reasoning_task": "Examples cited: Abstract Algebra, Logical Args, Temporal Sequences (BIG-bench subset); also other BIG-bench items described as 'logical reasoning' or 'epistemic reasoning'.",
            "task_description": "Tasks require abstract/logical reasoning (e.g., solving algebraic reasoning problems, temporal sequence reasoning, argument/logic evaluation). Most are multiple-choice or structured text tasks drawn from BIG-bench.",
            "method_or_approach": "Few-shot / zero-shot prompting with multiple-choice selection based on model-assigned probabilities; evaluations use the BIG-bench task definitions and scoring.",
            "performance": "Mixed: Gopher shows dramatic improvements on some BIG-bench tasks (e.g., Figure-of-Speech detection: 7.1B = 16.8% → 280B = 52.7%), but for several explicit logical/mathematical tasks like Abstract Algebra and Temporal Sequences Gopher 280B underperforms some smaller models (specific numeric scores not provided in text). Overall, logical-reasoning items in BIG-bench gained less from scale compared to knowledge tasks.",
            "limitations_or_failure_cases": "Reported failure modes include degradation or no improvement with scale on Abstract Algebra and Temporal Sequences; indicates inability of scale-alone to reliably solve compositional/structured logical tasks. The evaluations did not incorporate reasoning-specific prompting (e.g., stepwise decomposition) within this paper.",
            "comparison": "Within BIG-bench evaluations the paper compares Gopher family members to each other; externally, Gopher is compared to LM baselines on BIG-bench items where available. The authors note that some prior LM families (GPT-2/J, T5 variants) show different scaling behaviour on these tasks, but the paper does not provide full cross-model numeric breakdowns for each BIG-bench logical task.",
            "ablation_or_analysis_results": "The paper analyses per-task scaling behaviour and highlights that some tasks appear to be 'unlocked' by scale (large jumps) whereas others do not benefit; no additional ablations (e.g., training with targeted logic datasets or chain-of-thought prompting) are reported for BIG-bench logical subsets in this work.",
            "uuid": "e5087.2",
            "source_info": {
                "paper_title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Baseline LMs (GPT-3, Jurassic-1, Megatron-Turing)",
            "name_full": "Prior large language models used as baselines (GPT-3, Jurassic-1, Megatron-Turing NLG)",
            "brief_description": "Existing large-scale autoregressive Transformer LMs cited as state-of-the-art baselines for comparison; paper compares Gopher performance to these models across multiple tasks including reasoning categories.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-3; Jurassic-1; Megatron-Turing NLG",
            "model_description": "Large autoregressive Transformer models previously published or reported (GPT-3 = 175B, Jurassic-1 ≈178B, Megatron-Turing NLG ≈530B); used as prior LM SOTA baselines in cross-task comparisons.",
            "model_size": "GPT-3: 175B; Jurassic-1: 178B; Megatron-Turing NLG: 530B (as reported by sources cited in paper)",
            "logical_reasoning_task": "Various benchmarks including language modelling datasets, MMLU, BIG-bench and reasoning-heavy tasks used for cross-model comparison.",
            "task_description": "Benchmarks include knowledge, comprehension, and reasoning tasks; comparisons are typically few-shot/zero-shot next-token probability accuracy on multiple-choice tasks.",
            "method_or_approach": "Paper uses published results from these baseline models for side-by-side percent-change comparisons against Gopher; it does not re-run or fine-tune those external models.",
            "performance": "Paper reports Gopher outperforms the 'current LM SOTA' (which includes these models) on ~81% of comparable tasks (100/124), though on certain reasoning/math/commonsense tasks differences are small or Gopher underperforms marginally (e.g., some commonsense tasks where Megatron-Turing slightly outperforms Gopher). Specific numeric comparisons are given per task in supplementary figures/tables.",
            "limitations_or_failure_cases": "These baseline models are only mentioned via reported scores; the paper does not run them experimentally, so comparisons rely on published numbers that may differ in evaluation details or dataset exposure.",
            "ablation_or_analysis_results": "Comparisons serve as a reference to show where scale and dataset changes improved performance; the paper notes that improvements vs baselines are strongest in knowledge-intensive domains and weaker in strict logical/mathematical reasoning.",
            "uuid": "e5087.3",
            "source_info": {
                "paper_title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher",
                "publication_date_yy_mm": "2021-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "BIG-bench",
            "rating": 2
        },
        {
            "paper_title": "Measuring Massive Multitask Language Understanding",
            "rating": 2
        },
        {
            "paper_title": "Language Models are Few-Shot Learners",
            "rating": 2
        },
        {
            "paper_title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
            "rating": 1
        }
    ],
    "cost": 0.014469999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Scaling Language Models: Methods, Analysis \&amp; Insights from Training Gopher</h1>
<p>Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu and Geoffrey Irving</p>
<p>Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales - from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.</p>
<p>Keywords: Natural Language Processing, Language Models, Deep Learning</p>
<h2>Contents</h2>
<p>1 Introduction ..... 3
2 Background ..... 5
3 Method ..... 5
3.1 Models ..... 5
3.2 Training ..... 6
3.3 Infrastructure ..... 6
3.4 Training Dataset ..... 7
4 Results ..... 7
4.1 Task Selection ..... 8
4.2 Comparisons with State of the Art ..... 8</p>
<p>4.3 Performance Improvements with Scale ..... 11
5 Toxicity and Bias Analysis ..... 13
5.1 Toxicity ..... 13
5.2 Distributional Bias ..... 14
6 Dialogue ..... 17
6.1 Prompting For Dialogue ..... 18
6.2 Fine-tuning for Dialogue ..... 18
6.3 Dialogue \&amp; Toxicity ..... 19
7 Discussion ..... 20
7.1 Towards Efficient Architectures ..... 20
7.2 Challenges in Toxicity and Bias ..... 21
7.3 Safety benefits and safety risks ..... 22
8 Conclusion ..... 23
9 Acknowledgements ..... 23
10 Contributions ..... 24
A MassiveText ..... 39
A. 1 Dataset Pipeline ..... 39
A. 2 Dataset Analysis ..... 42
A. 3 Dataset Ablations ..... 45
A. 4 Text normalisation ..... 46
A. 5 MassiveText Datasheet ..... 46
B Gopher Model Card ..... 49
C Lessons Learned ..... 53
C. 1 Adafactor ..... 53
C. 2 Lower-Precision Training with bfloat16 ..... 53
D Results ..... 55
D. 1 Overview ..... 55
D. 2 Pile ..... 60
D. 3 Language Modelling ..... 60
D. 4 Filtering Test-Set Documents ..... 61
D. 5 Scaling Curves ..... 61
D. 6 Scaling Context Length ..... 62
D. 7 MMLU ..... 64
D. 8 BIG-bench ..... 69</p>
<p>D. 9 TriviaQA \&amp; NaturalQuestions ..... 80
D. 10 TruthfulQA ..... 81
D. 11 Reading Comprehension: RACE ..... 82
D. 12 Fact-Checking: FEVER \&amp; MultiFC ..... 84
D. 13 Common Sense: PIQA, WinoGrande, SocialIQA, HellaSwag ..... 87
E Toxicity and Bias Analysis ..... 89
E. 1 Toxic Generations ..... 89
E. 2 Classifying Toxicity ..... 90
E. 3 Distributional Bias ..... 95
F Compute Usage ..... 102
G Reducing Inference and Training Costs ..... 103
G. 1 Efficient Fine-tuning ..... 103
G. 2 Reducing Inference Costs ..... 105
G. 3 Reducing Training Costs ..... 109
G. 4 Future Work for Efficient Training ..... 112
H Dialogue-Prompted Gopher Details ..... 112
H. 1 Construction ..... 112
H. 2 Dialogue Dataset Filtering ..... 115
H. 3 Comparison Methodology ..... 115
H. 4 RTP in a Dialogue Setting ..... 115
H. 5 Selected Transcripts ..... 115</p>
<h1>1. Introduction</h1>
<p>Natural language communication is core to intelligence, as it allows ideas to be efficiently shared between humans or artificially intelligent systems. The generality of language allows us to express many intelligence tasks as taking in natural language input and producing natural language output.</p>
<p>Autoregressive language modelling - predicting the future of a text sequence from its past provides a simple yet powerful objective that admits formulation of numerous cognitive tasks. At the same time, it opens the door to plentiful training data: the internet, books, articles, code, and other writing. However this training objective is only an approximation to any specific goal or application, since we predict everything in the sequence rather than only the aspects we care about. Yet if we treat the resulting models with appropriate caution, we believe they will be a powerful tool to capture some of the richness of human intelligence.</p>
<p>Using language models as an ingredient towards intelligence contrasts with their original application: transferring text over a limited-bandwidth communication channel. Shannon's Mathematical Theory of Communication (Shannon, 1948) linked the statistical modelling of natural language with compression, showing that measuring the cross entropy of a language model is equivalent to measuring its compression rate. Shannon fit early language models to real data via precomputed tables of</p>
<p>text statistics (Dewey, 1923) relating model complexity to improved text compression alongside more realistic text generation. ${ }^{1}$ But the relation to intelligence was there from the start: Shannon posits that a sufficiently complex model will resemble human communication adequately, and the Imitation Game (Turing, 1950) cemented the link. The relation between data compression (via prediction) and intelligence has been further expanded upon since (see Chater (1999); Legg and Hutter (2007); Wolff (1982)).</p>
<p>A key driver towards better language models has been modern computing. From their pen-andpaper origins, language models have transformed in capacity and predictive power by the exponential rise in compute (Moore et al., 1965). In the 1990s and 2000s, $n$-gram models saw increases in scale and better smoothing approaches (Ney et al., 1994), including a 300 billion $n$-gram model trained on two trillion tokens of text (Brants et al., 2007). These models have been applied to speech recognition (Jelinek, 1997), spelling correction (Brill and Moore, 2000), machine translation (Brown et al., 1990), and many other areas. However $n$-gram models become statistically and computationally inefficient as the context length is increased, which limits the richness of language they can model.</p>
<p>In the past two decades language models have progressed to neural networks that capture the structure of language implicitly (Bengio et al., 2003; Graves, 2013; Jozefowicz et al., 2016; Mikolov et al., 2010; Radford et al., 2019). Progress has been driven by both scale and network architecture (Bahdanau et al., 2014; Hochreiter and Schmidhuber, 1997; Vaswani et al., 2017). ? and Kaplan et al. (2020) independently found power laws relating cross entropy loss to model size for recurrent and Transformer neural language models respectively. The empirically predicted gains to scale were realised in practice by the Generative Pre-trained Transformer 3 (GPT-3, Brown et al. (2020)), a 175 billion parameter Transformer trained over 300 billion tokens of text, which consumed zettaflops of compute to train - an order of magnitude beyond prior work (Rosset, 2020). GPT-3 demonstrated unprecedented generation quality alongside generalist capabilities across many Natural Language Processing (NLP) tasks - notably when prompted with examples (termed few-shot prompting).</p>
<p>In this paper we describe a protocol for training a state-of-the-art large language model and present a 280 billion parameter model called Gopher. We outline the methods of architecture specification, optimisation, infrastructure, and the curation of a high-quality text dataset MassiveText in Section 3. We perform a broad analysis of benchmark performance across 152 tasks that examine several diverse aspects of intelligence, and summarise the key results in Section 4. We see that Gopher lifts the performance over current state-of-the-art language models across roughly $81 \%$ of tasks containing comparable results, notably in knowledge-intensive domains such as fact checking and general knowledge.</p>
<p>As harmful content occurs both in Gopher's training set and in many potential downstream applications, we examine model toxicity and bias in Section 5 with a focus on how scale influences these properties. We find larger models are more likely to generate toxic responses when provided with toxic prompts, but they can also more accurately classify toxicity. We also analyse Gopher in a dialogue-interaction setting in Section 6 via prompting and present several transcripts to demonstrate qualitative capabilities and limitations of the model.</p>
<p>Finally, we discuss the ethical and safe application of these models including which types of undesirable behaviour to mitigate before and after training in Section 7. We discuss applicationdriven safety and the potential for language models to accelerate research towards safer intelligent technology.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>2. Background</h1>
<p>Language modelling refers to modelling the probability of text $P(S)$ where $S$ can be a sentence, paragraph, or document depending on the application. This is done by tokenizing the string: mapping it to a sequence of integer-valued tokens: $g(S)=X=\left(X_{1}, X_{2}, \ldots, X_{n}\right) \in V^{n}$ where $V$ is the vocabulary (a finite set of positive integers) and $n$ is the resulting sequence length, and modelling $X$. Tokenization can be open-vocabulary where any string can be uniquely tokenized, e.g., byte-level modelling, or closed-vocabulary where only a subset of text can be uniquely represented, e.g., a list of words and a singular out-of-vocabulary token. We employ open-vocabulary tokenization via a mixture of byte-pair encoding (BPE) with a backoff to UTF-8 bytes in the style of Radford et al. (2018).</p>
<p>The typical way to model the token sequence $X$ is via the chain rule $P(X)=P\left(X_{1}, X_{2}, \ldots, X_{n}\right)=$ $\prod_{i=1}^{n} P\left(X_{i} \mid X_{&lt; i}\right)$. This is also known as autoregressive sequence modelling, because at each time-step the future (in this case, future token) is predicted based upon the past context. Whilst there are other objectives towards modelling a sequence, such as modelling masked tokens given bi-directional context (Devlin et al., 2019; Mikolov et al., 2013) and modelling all permutations of the sequence (Yang et al., 2019) we focus on autoregressive modelling due to its strong performance and simplicity. We shall refer to language models hereon as the function approximators to perform next-token prediction.</p>
<p>A class of neural networks known as Transformers (Vaswani et al., 2017) have demonstrated state-of-the-art language model performance in recent years (Dai et al., 2019; Radford et al., 2018, 2019) and this is the architecture we focus on in this paper. There has been a trend of scaling the combination of training data, model size (measured in parameters) and training computation to obtain models with improved performance across academic and industrial benchmarks. Notable models along this progression include the 345 million parameter BERT (Devlin et al., 2019) performing strongly across a wide benchmark of language classification tasks, the 1.5 billion parameter GPT-2 (Radford et al., 2018) and 8.3 billion parameter Megatron (Shoeybi et al., 2019) displaying progressively superior zero-shot language model performance, the 11 billion parameter T5 (Raffel et al., 2020a) which advanced transfer learning and performance on several closed-book question answering tasks, and the aforementioned 175 billion parameter GPT-3. The moniker Large Language Models (LLMs) has become popular to describe this generation of larger models.</p>
<p>Since GPT-3 there has been a 178B parameter Transformer language model Jurassic-1 (Lieber et al., 2021) which uses a diverse training set and a larger tokenizer vocabulary size, along with an announced 530B Megatron-Turing NLG (Kharya and Alvi, 2021) which trains on a released dataset (The Pile, Gao et al. (2020)) (which we evaluate on) and has reported some tentative performance numbers. There have also been Transformer variants which incorporate a sparse mixture of experts (Fedus et al., 2021; Roller et al., 2021b) to increase the model size (in some cases to trillions of parameters) with more modest compute budgets. Other recent LLMs include two models (FLAN and T0) fine-tuned on instructions for an array of down-stream tasks (Sanh et al., 2021; Wei et al., 2021) which improves performance to unseen tasks - these ideas are complementary to the initial task of building a powerful language model but we compare performance nonetheless where possible.</p>
<h2>3. Method</h2>
<h3>3.1. Models</h3>
<p>In this paper we present results on six Transformer language models ranging from 44 million to 280 billion parameters, with the architectural details displayed in Table 1. We refer to the largest as Gopher and the entire set of models as the Gopher family.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Layers</th>
<th style="text-align: center;">Number Heads</th>
<th style="text-align: center;">Key/Value Size</th>
<th style="text-align: center;">$\mathbf{d}_{\text {model }}$</th>
<th style="text-align: center;">Max LR</th>
<th style="text-align: center;">Batch Size</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">44 M</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">$6 \times 10^{-4}$</td>
<td style="text-align: center;">0.25 M</td>
</tr>
<tr>
<td style="text-align: center;">117 M</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">768</td>
<td style="text-align: center;">$6 \times 10^{-4}$</td>
<td style="text-align: center;">0.25 M</td>
</tr>
<tr>
<td style="text-align: center;">417 M</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">1,536</td>
<td style="text-align: center;">$2 \times 10^{-4}$</td>
<td style="text-align: center;">0.25 M</td>
</tr>
<tr>
<td style="text-align: center;">1.4 B</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">2,048</td>
<td style="text-align: center;">$2 \times 10^{-4}$</td>
<td style="text-align: center;">0.25 M</td>
</tr>
<tr>
<td style="text-align: center;">7.1 B</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">4,096</td>
<td style="text-align: center;">$1.2 \times 10^{-4}$</td>
<td style="text-align: center;">2 M</td>
</tr>
<tr>
<td style="text-align: center;">Gopher 280B</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">16,384</td>
<td style="text-align: center;">$4 \times 10^{-5}$</td>
<td style="text-align: center;">$3 \mathrm{M} \rightarrow 6 \mathrm{M}$</td>
</tr>
</tbody>
</table>
<p>Table 1 | Model architecture details. For each model, we list the number of layers, the key/value size, the bottleneck activation size $\mathrm{d}<em _model="{model" _text="\text">{\text {model }}$, the maximum learning rate, and the batch size. The feed-forward size is always $4 \times \mathrm{d}</em>$.}</p>
<p>We use the autoregressive Transformer architecture detailed in Radford et al. (2019) with two modifications: we use RMSNorm (Zhang and Sennrich, 2019) instead of LayerNorm (Ba et al., 2016), and we use the relative positional encoding scheme from Dai et al. (2019) rather than absolute positional encodings. Relative encodings permit us to evaluate on longer sequences than we trained on, which improves the modelling of articles and books as shown in Section D.6. We tokenize the text using SentencePiece (Kudo and Richardson, 2018) with a vocabulary of 32,000 and use a byte-level backoff to support open-vocabulary modelling. The Gopher model card (Mitchell et al., 2019) is included in Appendix B.</p>
<h1>3.2. Training</h1>
<p>We train all models for 300 billion tokens with a 2048 token context window, using the Adam (Kingma and $\mathrm{Ba}, 2014$ ) optimiser. We warm-up the learning rate from $10^{-7}$ to the maximum learning rate over the first 1500 steps, and then decay it $10 \times$ using a cosine schedule. As we increase model size, we decrease the maximum learning rate and increase the number of tokens in each batch, as shown in Table 1. Furthermore, we increase Gopher's batch size from three to six million tokens per batch during training. We clip gradients based on the global gradient norm using a clipping value of 1. However, for the 7.1B model and for Gopher we reduce this to 0.25 for improved stability.</p>
<p>We incorporate the bfloat16 numerical format to reduce memory and increase training throughput. Models smaller than 7.1B are trained with mixed precision float32 parameters and bfloat16 activations (Micikevicius et al., 2018), while 7.1B and 280B use bfloat16 activations and parameters. bfloat16 parameters are updated using stochastic rounding to maintain stability (Gupta et al., 2015). We subsequently found that stochastic rounding does not fully recover mixed precision training performance; more details can be found in Appendix C.</p>
<h3>3.3. Infrastructure</h3>
<p>We built our training and evaluation codebase with JAX (Bradbury et al., 2018) and Haiku (Hennigan et al., 2020). In particular, we use JAX's pmap transformation to efficiently express both data and model parallelism. We trained and evaluated all models on TPUv3 chips (Jouppi et al., 2020).</p>
<p>The half-precision parameters and single-precision Adam state for Gopher occupy 2.5 TiB, which far exceeds the 16 GiB of memory available on each TPUv3 core. To address these memory concerns, we use optimiser state partitioning (Rajbhandari et al., 2020), model parallelism (Shoeybi et al., 2019), and rematerialisation (Griewank and Walther, 2000) to partition the model state and reduce the activations so that they fit in TPU memory.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">Disk Size</th>
<th style="text-align: right;">Documents</th>
<th style="text-align: right;">Tokens</th>
<th style="text-align: left;">Sampling proportion</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MassiveWeb</td>
<td style="text-align: right;">1.9 TB</td>
<td style="text-align: right;">604 M</td>
<td style="text-align: right;">506 B</td>
<td style="text-align: left;">$48 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Books</td>
<td style="text-align: right;">2.1 TB</td>
<td style="text-align: right;">4 M</td>
<td style="text-align: right;">560 B</td>
<td style="text-align: left;">$27 \%$</td>
</tr>
<tr>
<td style="text-align: left;">C4</td>
<td style="text-align: right;">0.75 TB</td>
<td style="text-align: right;">361 M</td>
<td style="text-align: right;">182 B</td>
<td style="text-align: left;">$10 \%$</td>
</tr>
<tr>
<td style="text-align: left;">News</td>
<td style="text-align: right;">2.7 TB</td>
<td style="text-align: right;">1.1 B</td>
<td style="text-align: right;">676 B</td>
<td style="text-align: left;">$10 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GitHub</td>
<td style="text-align: right;">3.1 TB</td>
<td style="text-align: right;">142 M</td>
<td style="text-align: right;">422 B</td>
<td style="text-align: left;">$3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Wikipedia</td>
<td style="text-align: right;">0.001 TB</td>
<td style="text-align: right;">6 M</td>
<td style="text-align: right;">4 B</td>
<td style="text-align: left;">$2 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2 | MassiveText data makeup. For each subset of MassiveText, we list its total disk size, its number of documents, and its number of SentencePiece tokens. During training we sample from MassiveText non-uniformly, using the sampling proportion shown in the right-most column.</p>
<p>We find that both data and model parallelism are low-overhead on TPUv3s due to their fast cross-chip communication and only incur a $10 \%$ overhead when training Gopher. Therefore, we find that pipelining (Huang et al., 2019) is not necessary on TPUs until the training scale exceeds the 1024-chip "pod", which greatly simplifies training mid-sized models. However, pipelining is an efficient parallelism method on commodity networks due to its low communication volume, so is well suited to connecting multiple TPU pods. In summary, we train Gopher by using model and data parallelism within TPU pods and pipelining across them. We verified through simulation that this topology was sensible for our hardware (Schaarschmidt et al., 2021); see Table A27 for details.</p>
<h1>3.4. Training Dataset</h1>
<p>We train the Gopher family of models on MassiveText, a collection of large English-language text datasets from multiple sources: web pages, books, news articles, and code. Table 2 details the constituent datasets. Our data pipeline (Section A.1.1) includes text quality filtering, removal of repetitious text, deduplication of similar documents, and removal of documents with significant test-set overlap. We find that successive stages of this pipeline improve language model downstream performance (Section A.3.2), emphasising the importance of dataset quality.</p>
<p>Overall, MassiveText contains 2.35 billion documents, or about 10.5 TB of text. Since we train Gopher on 300B tokens ( $12.8 \%$ of the tokens in the dataset), we sub-sample from MassiveText with sampling proportions specified per subset (books, news, etc.). We tune these sampling proportions to maximise downstream performance (see Section A.3.1 for details). The largest sampling subset is our curated web-text corpus MassiveWeb, which we find to improve downstream performance relative to existing web-text datasets such as C4 (Raffel et al., 2020b) in Figure A5. We give further details of MassiveText in Appendix A and provide the MassiveText datasheet in Table A3.</p>
<h2>4. Results</h2>
<p>We compile the performance of Gopher and its family of smaller models across 152 tasks. We compare these results to prior state-of-the-art (SOTA) performance for language models (124 tasks with published LM performance), supervised approaches which make use of task-specific data, and human performance where available. In this section we present a summary of key findings, and refer to Appendix D for the full set of results and task-specific methodology.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"># Tasks</th>
<th style="text-align: left;">Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Language Modelling</td>
<td style="text-align: center;">20</td>
<td style="text-align: left;">WikiText-103, The Pile: PG-19, arXiv, FreeLaw, $\ldots$</td>
</tr>
<tr>
<td style="text-align: left;">Reading Comprehension</td>
<td style="text-align: center;">3</td>
<td style="text-align: left;">RACE-m, RACE-h, LAMBADA</td>
</tr>
<tr>
<td style="text-align: left;">Fact Checking</td>
<td style="text-align: center;">3</td>
<td style="text-align: left;">FEVER (2-way \&amp; 3-way), MultiFC</td>
</tr>
<tr>
<td style="text-align: left;">Question Answering</td>
<td style="text-align: center;">3</td>
<td style="text-align: left;">Natural Questions, TriviaQA, TruthfulQA</td>
</tr>
<tr>
<td style="text-align: left;">Common Sense</td>
<td style="text-align: center;">4</td>
<td style="text-align: left;">HellaSwag, Winogrande, PIQA, SIQA</td>
</tr>
<tr>
<td style="text-align: left;">MMLU</td>
<td style="text-align: center;">57</td>
<td style="text-align: left;">High School Chemistry, Atronomy, Clinical Knowledge, $\ldots$</td>
</tr>
<tr>
<td style="text-align: left;">BIG-bench</td>
<td style="text-align: center;">62</td>
<td style="text-align: left;">Causal Judgement, Epistemic Reasoning, Temporal Sequences, $\ldots$</td>
</tr>
</tbody>
</table>
<p>Table 3 | Evaluation Tasks. We compile results for the Gopher family of models on 152 tasks.</p>
<h1>4.1. Task Selection</h1>
<p>We build a profile of language model performance that spans mathematics, common sense, logical reasoning, general knowledge, scientific understanding, ethics, and reading comprehension alongside conventional language modelling benchmarks. We include composite benchmarks (such as BIG-bench collaboration (2021)) which contain a mixture of tasks, alongside a number of established targeted benchmarks such as RACE for reading comprehension (Lai et al., 2017) and FEVER for fact-checking (Thorne et al., 2018), among others. We list our task sources in Table 3.</p>
<p>We select tasks that require the model to estimate the probability of target text as we find this to be a general interface that supports the probing of knowledge and reasoning capabilities. For language modelling tasks we calculate the bits per byte (BPB), a compression measure where a lower value indicates a higher probability placed on the correct continuation. All other tasks follow a multiple-choice format, where the model outputs a probability to each multiple-choice response given a context and question, and we select the response with the highest probability. Here, we measure the accuracy of a correct response.</p>
<p>We filter out training documents that are very similar to test-set instances for tasks that were created before MassiveText (November 2020) as described in Table A.1.1. Furthermore some tasks have been designed to use unique test-set problem statements that should not benefit from pre-existing text data - such as BIG-bench. However we caution that there may be test set leakage within our training set; we discuss the challenges of test-set leakage and generalisation in Section D.4.</p>
<h3>4.2. Comparisons with State of the Art</h3>
<p>In Figure 1 we present an overview of Gopher results with comparisons to state-of-the-art language model performance. Results are comparable across 124 tasks and we plot the percent change in performance metric (higher is better) of Gopher versus the current LM SOTA. ${ }^{2}$ Gopher outperforms the current state-of-the-art for 100 tasks ( $81 \%$ of all tasks). The baseline model includes LLMs such as GPT-3 (175B parameters) (Brown et al., 2020), Jurassic-1 (Lieber et al., 2021) (178B parameters), and Megatron-Turing NLG (530B parameters) (Kharya and Alvi, 2021); the exact baseline is specified per task in Figure A8.</p>
<p>We find that Gopher displays the most uniform improvement across reading comprehension, humanities, ethics, STEM and medicine categories. We see a general improvement on fact-checking. For common sense reasoning, logical reasoning, and maths we see much smaller performance im-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1 | Gopher (280B) vs LM SOTA. An overview of the percentage change in performance metric (higher is better) of Gopher versus state-of-the-art language model performance across 124 tasks. Each bar represents a task, here we clip the maximum relative improvement to $120 \%$. In total Gopher shows an improvement across $100 / 124$. The best-published results include (175B) GPT-3, (178B) Jurassic-1, and (530B) Megatron-Turing NLG. For full comparison including supervised and human performance see Figure A8.
provements and several tasks that have a deterioration in performance. The general trend is less improvement in reasoning-heavy tasks (e.g., Abstract Algebra) and a larger and more consistent improvement in knowledge-intensive tests (e.g., General Knowledge). Next is a discussion of a few specific sets of results.</p>
<p>For language model benchmarks, we expand the relative performance results of Gopher versus the current 178B SOTA model Jurassic-1 and 175B GPT-3 in Figure 2. Jurassic-1 is an LLM trained with an emphasis on large-vocabulary training and has generally outperformed GPT-3 at a very similar parameter size. We see Gopher does not outperform state-of-the-art on 8 of 19 tasks, under-performing on Ubuntu IRC and DM Mathematics in particular, possibly due to a poor tokenizer representation for numbers. Gopher demonstrates improved modelling on 11 of 19 tasks, in particular books and articles (Books3, PG-19, arXiv, etc.). This performance gain may be due to the heavy use of book data in MassiveText, with a sampling proportion of $27 \%$ in total (e.g., versus GPT-3's 16\%).</p>
<p>We highlight two reading comprehension tasks RACE-m and RACE-h, multiple-choice exams pitched at a middle-school and high-school level respectively. Inspecting the accuracy in Table 4 we see Gopher extend upon the current LM SOTA for high-school reading comprehension (47.9\% Megatron-Turing NLG $\rightarrow 71.6 \%$ Gopher) and the middle-school comprehension accuracy (58.1\% GPT-3 $\rightarrow 75.1 \%$ Gopher). The high-school reading comprehension level approaches human-rater performance. Smaller models from the Gopher family do not perform as well on these tasks, which suggests that data alone does not explain the performance difference - the combination of scale</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Gopher</th>
<th style="text-align: center;">GPT-3</th>
<th style="text-align: center;">Megatron-Turing</th>
<th style="text-align: center;">ALBERT</th>
<th style="text-align: center;">Amazon</th>
<th style="text-align: center;">Human</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\mathbf{4 1 7 M}$</td>
<td style="text-align: center;">$\mathbf{1 . 4 B}$</td>
<td style="text-align: center;">$\mathbf{7 . 1 B}$</td>
<td style="text-align: center;">$\mathbf{2 8 0 B}$</td>
<td style="text-align: center;">175 B</td>
<td style="text-align: center;">530 B</td>
<td style="text-align: center;">(ensemble)</td>
<td style="text-align: center;">Turk</td>
<td style="text-align: center;">Ceiling</td>
</tr>
<tr>
<td style="text-align: left;">RACE-h</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">$\mathbf{7 1 . 6}$</td>
<td style="text-align: center;">46.8</td>
<td style="text-align: center;">47.9</td>
<td style="text-align: center;">$\underline{90.5}$</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">94.2</td>
</tr>
<tr>
<td style="text-align: left;">RACE-m</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">$\mathbf{7 5 . 1}$</td>
<td style="text-align: center;">58.1</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">$\underline{93.6}$</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">95.4</td>
</tr>
</tbody>
</table>
<p>Table 4 | RACE reading comprehension. Accuracy for few-shot models: Gopher, GPT-3 (Brown et al., 2020), Megatron-Turing (Kharya and Alvi, 2021). Gopher extends performance significantly. Comparison with supervised SOTA: ALBERT (ensemble) result from Jiang et al. (2020). Amazon Turk and Human Ceiling (obtained by restricting to unambiguous questions with correctly labeled answers) accuracy from Lai et al. (2017).</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2 | Language Modelling Comparisons with SOTA. Comparison of Gopher to the current SOTA models on various language modelling tasks, including many from The Pile (Gao et al., 2020). The superscript (1) indicates the prior SOTA was Jurassic-1 and (2) indicates GPT-3. Gopher achieves state-of-the-art performance on 11 out of 19 datasets with the largest improvements on books and articles.
and data is crucial. All models are still far from human-ceiling performance (around 95\%) and supervised state-of-the-art ( $&gt;90 \%$ ) which was obtained using a smaller 223M parameter ALBERT-XXL model fine-tuned on the dataset (Jiang et al., 2020). It is possible supervised fine-tuning leads to greater reading comprehension, but it is also plausible the datasets contain exploitable statistics which can lead to high accuracy - as has been recently discovered for several common-sense reasoning benchmarks (Li et al., 2021).
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3 | Scaling curves for FEVER. In the claim-only setting (closed-book) there is a persistent trend in three-way classificaton accuracy with parameter scale. Breaking down the three classes into two pairs, scale benefits mostly the ability to distinguish SUPPORTED vs REFUTED, but not REFUTED versus NOTENOUGHINFO. When gold evidence is provided (open-book) there is a small benefit from 7.1B to 280B Gopher and performance slightly exceeds the supervised SOTA (Kruengkrai et al., 2021).</p>
<p>For some of the most well-studied common sense reasoning tasks: Winogrande, HellaSwag and PIQA, Gopher is outperformed by the larger Megatron-Turing NLG by a small amount ( $1.2 \%, 0.2 \%$ and $4.1 \%$ respectively), but all LM approaches trail human-level performance considerably (Section D.13). As with the mathematics tasks, this suggests that these models have limited reasoning capabilities.</p>
<p>We next highlight fact-checking. This is an important problem within the domain of tackling misinformation. We find that Gopher outperforms supervised SOTA approaches on the well-studied FEVER fact-checking benchmark when evidence is supplied. We see across model sizes in Figure 3 that</p>
<table>
<thead>
<tr>
<th style="text-align: right;">Random</th>
<th style="text-align: right;">$25.0 \%$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;">GPT-2 $^{1}$</td>
<td style="text-align: right;">$32.4 \%$</td>
</tr>
<tr>
<td style="text-align: right;">Average human rater ${ }^{1}$</td>
<td style="text-align: right;">$34.5 \%$</td>
</tr>
<tr>
<td style="text-align: right;">GPT-3 5-shot $^{1}$</td>
<td style="text-align: right;">$43.9 \%$</td>
</tr>
<tr>
<td style="text-align: right;">UnifiedQA $^{1}$</td>
<td style="text-align: right;">$48.9 \%$</td>
</tr>
<tr>
<td style="text-align: right;">Gopher 5-shot</td>
<td style="text-align: right;">$\mathbf{6 0 . 0 \%}$</td>
</tr>
<tr>
<td style="text-align: right;">Average human expert performance ${ }^{1}$</td>
<td style="text-align: right;">$89.8 \%$</td>
</tr>
<tr>
<td style="text-align: right;">June 2022 Forecast ${ }^{2}$</td>
<td style="text-align: right;">$57.1 \%$</td>
</tr>
<tr>
<td style="text-align: right;">June 2023 Forecast ${ }^{2}$</td>
<td style="text-align: right;">$63.4 \%$</td>
</tr>
</tbody>
</table>
<p>Table 5 | Massive Multitask Language Understanding (MMLU). Average accuracy over 57 tasks with model and human accuracy comparisons from 1: Hendrycks et al. (2020). Human rater performance is obtained using Mechanical Turk and average human expert performance is estimated per task based upon published exam results and averaged. Gopher improves over the prior supervised SOTA models by a considerable margin ( $&gt;30 \%$ ) however it is far from human expert. We also include the average prediction for SOTA accuracy in June 2022 and 2023 made by 73 competitive human forecasters (2: Steinhardt (2021)). Gopher is situated between the 2022 and 2023 forecast.
scale improves both the checking of facts given gold evidence alongside the 'closed book' checking of facts with a claim only. However, larger scale does not benefit the classification of facts which are unknown versus false, implying that larger models improve fact checking performance by knowing more facts versus forming a deeper understanding of misinformation at this stage.</p>
<p>Moving beyond per-task performance, we display the average accuracy across the 57 tasks in MMLU (Table 5). These tasks consist of real-world human exams covering a range of academic subjects. We have comparisons from GPT-3 (Brown et al., 2020), and a 11B T5 model fine-tuned on question tasks called UnifiedQA (Khashabi et al., 2020). These baseline model results along with human rater and expert performance were collected by Hendrycks et al. (2020). In Table 5 we see that Gopher achieves an overall accuracy of $\mathbf{6 0 \%}$, well above GPT-3's $43.9 \%$ and UnifiedQA's $48.9 \%$. Whilst this lifts the known performance of the pure language-model approach, it still trails the estimated human expert performance of $89.8 \%$. We also display how this performance contrasts with human expectations. From a competitive forecasting platform Hypermind ${ }^{3}$, human forecasters aim to predict the accuracy of machine learning systems on this benchmark by set dates for prizes - according to the September 2021 average forecast, Gopher-level performance was expected between June 2022 and June 2023.</p>
<p>We conclude that Gopher lifts the baseline performance of a language-model approach across a wide set of tasks. In some settings (e.g., RACE reading comprehension and FEVER fact-checking) Gopher nears human rater performance or the performance of supervised models designed for particular problem domains. However for a few categories of tasks (e.g., mathematical reasoning and commonsense) there is less of an improvement and this may indicate a limitation to the large-scale language model approach. Next, we consider the topic of model scale in isolation.</p>
<h1>4.3. Performance Improvements with Scale</h1>
<p>Next, we investigate which types of tasks benefit from scaling model size. In this section we compare the performance of Gopher (280B) to smaller models ( $\leq 7.1 \mathrm{~B}$ ). Because the Gopher family of models are all trained on the same dataset for the same number of tokens, this allows us to isolate the effect</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure $4 \mid \mathbf{2 8 0 B}$ vs best performance up to 7.1 B across different tasks. We compare the performance of Gopher to the best performance of our smaller models up to 7.1B. In nearly every case, Gopher outperforms the best smaller model's performance. Small gains come from either scale not improving results substantially or the smaller models already being very performant. Language modelling improvements are in BPB and the rest are in terms of accuracy.
of scaling parameters and training compute for each task.
We compute the relative performance improvement of Gopher (280B) versus the best performance up to 7.1B over all 152 tasks. The most performant smaller Gopher family model is usually, but not always, our 7.1B model. We find that Gopher demonstrates a performance improvement on the vast majority of tasks - only 16 ( $10.5 \%$ ) had zero or no performance gains. In contrast, 57 (37.5\%) tasks had small improvements, with relative performance increases of up to $25 \%$, and 79 (51.2\%) tasks had significant improvements of over $25 \%$. We then visualise relative performance improvement by task category in Figure 4.</p>
<p>Some of the largest benefits of scale are seen in the Medicine, Science, Technology, Social Sciences, and the Humanities task categories. These same categories are also where we see the greatest performance improvement over LM SOTA, as described in the previous section. Highlighting some specific tasks: for Figure of Speech Detection from BIG-bench we obtain the largest gains- a 314\% increase. Gopher achieved an impressive $52.7 \%$ accuracy whereas the 7.1B model achieved only $16.8 \%$ accuracy. Gopher also dramatically improves over the smaller models in Logical Args, Marketing, and Medical Genetics. For the TruthfulQA benchmark (Lin et al., 2021b) we find performance improvement with scale (from 1.4B to 280B), despite scale appearing to hurt performance for several other model families such as GPT-J, GPT-2, T5, GPT-3. Furthermore, 280B is the first model to demonstrate performance significantly beyond random guessing on the multiple-choice TruthfulQA task formulation (more details in Section D.10). These results highlight that on some tasks, scale seems to "unlock" the ability of a model to significantly improve performance on particular tasks.</p>
<p>On the other hand, we find that scale has a reduced benefit for tasks in the Maths, Logical Reasoning, and Common Sense categories. Our results suggest that for certain flavours of mathematical or logical reasoning tasks, it is unlikely that scale alone will lead to performance breakthroughs. In some cases Gopher has a lower performance than smaller models- examples of which include Abstract Algebra and Temporal Sequences from BIG-bench, and High School Mathematics from MMLU. On the other hand, the modest performance gains in common sense tasks largely come from relatively strong performance from the smaller models, limiting the room for relative improvement. While language modelling tasks see the smallest average improvements, this is due to the performance metric measured in BPB rather than accuracy and greatly limits the possible relative gains.</p>
<p>By comparing Gopher to our smaller models, we are able to specifically ask questions about the</p>
<p>impact of model scale. We conclude that while model scale plays an important role for improvements across the vast majority of tasks, the gains are not equally distributed. Many academic subjects, along with general knowledge, see large improvements come from scale alone. However, this analysis also highlights areas where model scale alone is not enough, or where the gains from scale are more modest- specifically some mathematical and logical reasoning tasks. By combining these scaling results with the comparisons of Gopher to LM SOTA, we see that scale and the dataset are both contributing to Gopher's strong performance in these domains. In the next section we investigate various properties of the model relating to toxic content generation and classification, the modelling of biases, and the representation of dialects.</p>
<h1>5. Toxicity and Bias Analysis</h1>
<p>Alongside the benefits of scaling language models, it is crucial to analyse how scale impacts potentially harmful behaviour. Here we study the behaviour of our language models with respect to problematic outputs and biases. We investigate the tendency of models to produce toxic output, to recognise toxic text, to display distributional bias in discourse about different groups of people, and to model subgroup dialects. For each question we consider variation across model scale.</p>
<p>We choose evaluations and metrics which are commonly used in the field. However, various work has discussed the limitations of current metrics and evaluations (Blodgett et al., 2020, 2021; Sheng et al., 2019; Welbl et al., 2021; Xu et al., 2021a) and our analysis has uncovered further caveats, which we highlight in the following sections and Section 7.2. We include these measures despite their shortcomings to underscore the importance of tackling these challenges and to highlight specific areas for future work, rather than to establish these particular approaches as best practice.</p>
<h3>5.1. Toxicity</h3>
<p>In the Sections 5.1.1 and 5.1.2, we rely on the widely used and commercially deployed Perspective $A P I^{4}$ classifier to study the toxicity of text generated by LMs, and associated CivilComments dataset for studying models' ability to detect toxic text. Accordingly, we adopt their definition of toxicity as "a rude, disrespectful or unreasonable comment that is likely to make someone leave a discussion."5</p>
<h3>5.1.1. Generation Analysis</h3>
<p>Our toxicity analysis of text generated by LMs follows the methodology used in Gehman et al. (2020); Welbl et al. (2021). We use Perspective API to obtain toxicity scores for LM prompts and continuations. We analyse the toxicity of LM outputs when sampling is conditioned on a set of prompts and when it's unconditional (i.e. unprompted), similar to Welbl et al. (2021). Conditional generation allows us to analyse how the model responds to prompts that have varying toxicity scores. Prompts are from the RealToxicityPrompts (RTP) dataset (Gehman et al., 2020), which contains 100k naturally occurring, sentence-level prompts derived from a large corpus of English web text. We sample 10\% of the 100k RTP prompts for efficiency and generate 25 continuations per prompt.</p>
<p>The continuation toxicity of larger models is more consistent with prompt toxicity than for smaller models (Figure 5a). When prompted, as the input toxicity increases, larger models respond with greater toxicity, plateauing near 7.1B parameters. This suggests that more parameters increase the model's ability to respond like-for-like to inputs.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" />
(a) Generation analysis.
<img alt="img-5.jpeg" src="img-5.jpeg" />
(b) Classification analysis.</p>
<p>Figure 5 | Toxicity analyses. (a) Toxicity of text generated by LMs, bucketed by prompt toxicity using the RTP dataset. Error bars show 99\% confidence interval. (b) Few-shot toxicity classification on the CivilComments dataset. Larger models are better at classifying toxic text.</p>
<p>For unprompted samples, the toxicity is low and does not increase with model size. Levels are slightly lower than in the training data (Figure A22b), i.e. when unprompted, the LM does not amplify training data toxicity. More details on our toxicity evaluation methodology, results and metrics can be found in Section E.1.</p>
<h1>5.1.2. Classification Analysis</h1>
<p>We evaluate the models' ability to detect toxic text in the few-shot setting, in a manner similar to Schick et al. (2021), on the CivilComments dataset (Borkan et al., 2019) (see Section E. 2 for details). We observe that the model's ability to classify text for toxicity increases with scale in few-shot settings (Figure 5b). The smaller models perform comparably or worse than a random classifier (which would achieve an AUC of 0.5). The largest model achieves an AUC of around 0.76 in the 20 -shot setting, significantly improving on the smaller models (Figure 5b). We note that while the state-of-the-art for toxicity detection in the few-shot setting is not well established, our performance is well below that of state of the art classifiers trained specifically for toxicity detection (Borkan et al., 2019).</p>
<p>In Section E.2, we further explore whether large language models used for few-shot toxicity classification exhibit subgroup bias. We measure unintended classifier bias using the 280B model with metrics introduced in Borkan et al. (2019) and find that the model is prone to bias against subgroups in different ways. Thus, while language models can be a powerful tool for few-shot classification (especially important in tasks with data that is difficult to annotate), outcomes are not necessarily fair across subgroups. More work is needed to understand how to best mitigate these biases, and caution must be exercised when optimising for improvements in their toxicity classification capabilities.</p>
<h3>5.2. Distributional Bias</h3>
<p>We define distributional biases as biases which are not apparent in a single sample, but emerge over many samples. For example, whereas "The woman is a nurse" is not a problematic sentence, it can be problematic if the model disproportionately associates women with certain occupations. As discussed in Sheng et al. (2021), distributional biases in language models can have both negative representational impacts (e.g., Kay et al. (2015)) and allocational impacts (e.g., Dastin (2018)). To investigate distributional biases in our model, we measure stereotypical associations between gender and occupation, the distribution of sentiment in samples conditioned on different social groups, and perplexity on different dialects. Whereas performance across many language tasks increases with scale, we find that simply increasing model size does not remove biased language. Indeed, we expect</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" />
(a) Impact of model size on gender bias.
(b) Accuracy on "gotcha" examples.</p>
<p>Figure 6 | Analysis of gender and occupation bias in our models. (a) Gender bias metric as a function of model size for two templates. A high value indicates higher overall bias. We do not see a consistent correlation between model size and bias. (b) Winogender accuracy as a function of model size for examples which oppose gender stereotypes ("gotcha" examples) and reinforce gender stereotypes ("not gotcha" examples). Compared to "not gotcha" examples, performance on "gotchas" remains lower and differs between male and female pronouns. Both results are indicators of bias.
models trained with a standard cross-entropy objective to reflect biases in our training data.
Progress in this space will require challenging cross-disciplinary work to outline desirable behaviour, measure and interpret model outputs, and design novel mitigations, as demonstrated by these results and the significant limitations of the following methods, discussed in Section 7.2.</p>
<h1>5.2.1. Gender and Occupation Bias</h1>
<p>We study gender and occupation bias via two different evaluations. First, we measure the probability of gendered words following different occupation contexts. Second, we evaluate on the Winogender coreference resolution dataset (Rudinger et al., 2018), where similar coreference accuracy across different pronouns indicates less gender bias. In our evaluation, we primarily compare performance across male and female gendered terms, but acknowledge these terms do not represent all possible gender identities (Cao and Daumé, 2021).</p>
<p>Gender Word Probability To measure how probable different gender words are in different occupation contexts, we follow a setup similar to Brown et al. (2020). We input an occupation prompt like "The {occupation} was a" into our model and compute a gender bias metric by comparing the probabilities of the prompt being followed by either male or female gendered terms.</p>
<p>Figure 6a reports our probability based gender bias metric as a function of model size for two different templates ("The {occupation} was a {gender}" and "The {occupation} is a {gender}"). Overall, we do not find a consistent correlation between model size and bias. Furthermore, we find that apparently unrelated choices in template (changing "was" to "is") can alter the measured bias. Additionally, choice of gender words also impacts results; if we only use the terms "male" and "female," gender bias is substantially lower than when summing over a large set of gendered terms (Figure A24a). Section E.3.1 contains further details of the implementation, metrics, and results.</p>
<p>Winogender We explore bias on a zero-shot coreference task using the Winogender dataset (Rudinger et al., 2018). Models are evaluated on whether they can correctly resolve a pronoun to either an occupation word or a relevant distractor word. We expect unbiased models to have similar coreference resolution performance regardless of the pronoun gender. This evaluation is similar</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" />
(a) Sentiment scores by country.
<img alt="img-8.jpeg" src="img-8.jpeg" />
(b) Sentiment scores by race.</p>
<p>Figure 7 | Mean continuation sentiment score by group. Bars indicate 99\% confidence intervals. High is positive sentiment, low is negative. See Figure A26 for religion and occupation results.
to the "disambiguation_q" ambiguous pronoun gender bias task reported in our BIG-bench results (Section D.8.3). However, here we are measuring performance in a zero-shot setting.</p>
<p>Similar to the BIG-bench analysis, we observe that overall performance increases with model size (Figure A24b). Following Rudinger et al. (2018), we also report performance on sentences which are likely to be hard for a gender biased model (called "gotchas") in Figure 6b. A "gotcha" example is one where the correct coreference resolution is one that differs from stereotypes (based on labor statistics ${ }^{6}$ ). Performance increases across both "gotchas" and "not gotchas" with model size, though performance on "gotchas" is considerably lower. On "gotcha" examples, there is a significant difference in performance for male and female pronouns. Thus, though performance on coreference resolution for the overall task increases considerably with size, our analysis suggests Gopher is still impacted by gender and occupation bias. Full details of our setup and results are in Section E.3.1.</p>
<h1>5.2.2. Sentiment Bias towards Social Groups</h1>
<p>Sentiment bias is one way to quantify how generated text describes different identities and social groups. In prior work, the difference in sentiment distributions is used to measure individual and group fairness in generative language models (Huang et al., 2020). For this work, we measure the sentiment of model outputs for different occupations, countries, races, and religions. Here we present an overview, with details of metric definition, term and template lists, and full results in Section E.3.2.</p>
<p>Metrics Following Huang et al. (2020), we sample completions based on templated prompts. In each prompt, a single modifier or noun is changed to refer to a different attribute. For example, the template "The {attribute} person could" could be filled in with "Christian," "Jewish," or "Muslim". The samples for each prompt are scored between 0 (negative) to 1 (positive) by a sentiment classifier.</p>
<p>Selection of templates and terms Following Brown et al. (2020); Huang et al. (2020) we measure sentiment for race, religion, country, and occupation. We also extend the term set for religion and race to include an unspecified option without the attribute word ("The {attribute} person could" becomes "The person could"). We include this unspecified option because attributes that are assumed to be the default in a particular culture or context, such as a majority or higher-status attribute, are often left unmarked (unspecified) in language (Waugh, 1982).</p>
<p>Results In Figure 7 and Figure A26, we plot the distribution of normalized sentiment scores for all completions of all prompts for each attribute, and report an aggregated group fairness metric in Figure A25. As in gender and occupation bias, we see no clear trend with scale. This is particularly</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 8 | Perplexity by dialect. (Left) Perplexity on Tweets classified as African American and White-aligned English. (Right) The relative decrease in perplexity compared to the 44M model.
evident for countries and occupations, while further analysis is needed to understand why particular attributes within race and religion appear to follow a slight downward trend in mean sentiment.</p>
<p>For sentiment distribution, we observe that certain attributes have notably lower mean sentiment scores. To better understand this, we analyse word co-occurrences for pairs of attributes (Table A25). From this, we observe our models inherit features of historical and contemporary discourse about specific groups (Mohamed et al., 2020). Second, similar to the gender and occupation results, the choice of demographic terms requires careful thought. See Section E.3.2 for deeper discussion.</p>
<h1>5.2.3. Perplexity on Dialects</h1>
<p>Although Gopher has impressive performance on language benchmarks, it is only able to model text reflected in the training data. If certain dialects are underrepresented in a training corpus, there is likely to be disparate model performance in understanding such language. To test for this gap, we measure the perplexity of our models on Tweets from the African American (AA)-aligned corpus and White-aligned corpus curated by Blodgett et al. (2016). Our results show that perplexity on the AA-aligned corpus is higher for all model sizes. As the model scales, perplexity for both dialects improves, but it does so at roughly the same rate so the gap does not close with scale.</p>
<p>These results highlight a distinct way that bias manifests in the language models. The preceding metrics quantify how models' outputs vary when different groups are the subject of the output, which can constitute a representational harm when it is more negative or stereotypical (Blodgett et al., 2020). However, the models also show disparate ability in modelling dialects, which could lead to allocational harms in applications with users with different dialects.</p>
<h2>6. Dialogue</h2>
<p>So far, we have explored the capabilities and limitations of Gopher through quantitative methods. In this section we investigate the model through direct interaction. We find that by conditionally sampling from a dialogue prompt similar to the few-shot method of Brown et al. (2020), our Dialogue-Prompted Gopher can emulate a conversational format to a decent quality. We provide example transcripts here, with more in Section H.5. We contrast this with the more conventional method of fine-tuning on dialogue data, finding that fine-tuning did not deliver significantly preferred responses in a small-scale human study. Unlike Section 5.1.1, toxicity of Dialogue-Prompted Gopher responses does not increase with model scale, even when prompted with toxic questions (Figure 9).</p>
<h1>6.1. Prompting For Dialogue</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompt</th>
<th style="text-align: left;">Generation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">What am I <br> thinking?</td>
<td style="text-align: left;">It's Friday night and I'm in bed and awake at 12:37 am. "I woke up from a dream where I was <br> standing next to...</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">The Golden Globes are over and now I need to make a list of films that I must see, most likely <br> in the coming months</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Where am I headed? Is there another place where I can get away? Can I use my smartphone <br> to play? How do I think I can do this?</td>
</tr>
</tbody>
</table>
<p>Table $6 \mid$ Responses from Gopher when naively prompted with a question, for three seeds.
Language models are trained to reproduce their input distribution, not to engage in conversation. When prompted with a question, we can see that the model generates a first-person narrative, some text resembling a blog post, and a generic list of existential questions (Table 6). This behaviour is consistent with the content that Gopher has been trained on.</p>
<p>In order to produce a conversationalist, we use a prompt that describes Gopher's role and starts a conversation between Gopher and a fictional User, including behaviours such as aversion to offensive language and an ability to opt out of certain question types; see Table A30 for the full prompt. Table 7 shows a transcript with Dialogue-Prompted Gopher on the topic of cell biology and bacteria. Here it remains on topic, discusses some technical details, and provides a correct citation link. However it actually provides subtle incorrect responses in some cases (prokaryotes are not the only single-cell organisms). Table 8 shows an unsuccessful transcript illustrating factual errors confidently expressed. See Section H. 5 for more transcripts with interesting behaviours and failure modes, including more subtle plausible but factually incorrect dialogue with a claim of search (Table A32), generating harmful text (Table A35), or contradicting itself and showing a general lack of common sense (Table A37).</p>
<p>Anecdotally, we find both successes and failures to be common, but we emphasize that DialoguePrompted Gopher is still just a language model. The prompt conditions the model's prior over responses but does not result in a consistently reliable or factual dialogue model. We refer the reader to Weidinger et al. (2021) for a detailed discussion on language model harms specific to dialogue and we discuss some ideas regarding building trustworthy systems in Section 7.3.</p>
<h3>6.2. Fine-tuning for Dialogue</h3>
<p>Recent work on dialogue often focuses on supervised training with dialogue-specific data (Chen et al., 2017), such as Google's Meena (Adiwardana et al., 2020) and Facebook's BlenderBot (Roller et al., 2021a). We explore this approach by creating a curated dialogue dataset from MassiveWeb and fine-tuning Gopher on this dataset for $\sim 5$ billion tokens to produce Dialogue-Tuned Gopher. We then ask human raters for their preference over the response from Dialogue-Tuned Gopher and Dialogue-Prompted Gopher, using our dialogue prompt (Table A30) for both models. To our surprise, we find from 1400 ratings the preference is $(50 \pm 0.04) \%$ : no significant difference. We describe the methodology in detail in Section H.3. We consider this an interesting initial result; future work would be valuable to rigorously examine the pros and cons of fine-tuning versus prompting for dialogue with large-scale models and compare Gopher to existing dialogue systems accounting for large differences in model size.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 9 | Toxicity analyses of Dialogue-Prompted models. (Left) Toxicity of text generated by Dialogue-Prompted LMs given RTP questions, bucketed by prompt toxicity. Continuation toxicity does not increase with model scale. (Right) For "high" toxicity prompts ( $&gt;66 \%$ ), the toxicity of Dialogue-Prompted Gopher models on RTP-questions and Gopher models on RTP relative to 44M models.</p>
<h1>6.3. Dialogue \&amp; Toxicity</h1>
<p>We investigate the toxicity of Dialogue-Prompted Gopher. We adapt the RTP methodology to the dialogue setting (called RTP questions, details in Section H.4). In Figure 9 (left), we observe that Dialogue-Prompted Gopher does not follow the same trend (increased toxicity with model scale) as Gopher. Whilst we see a monotonic increase in continuation toxicity with model scale in the unprompted setting (Figure 5a), Dialogue-Prompted Gopher toxicity tends to slightly decrease with increased model scale (from 117 M parameters, except for prompts in the most toxic bucket). Potentially, larger models can better account for the given prompt (which includes "to be respectful, polite, and inclusive"). Specifically, we compare the continuation toxicity between Gopher (tested on RTP) and Dialogue-Prompted Gopher (tested on RTP questions) models relative to 44M models for prompts with high toxicity in the right of Figure 9. Again, we observe that with dialogue prompting, continuation toxicity remains largely at levels similar to the 44 M model, contrasting with the upward trend observed for unprompted language models.</p>
<p>RTP is quite a straightforward stress-test: the user utters a toxic statement and we observe how the system responds. In work parallel to this study, Perez et al. (2022) probes Dialogue-Prompted Gopher further via an adversarial attack generated by Gopher. This approach induces the model to recite discriminatory jokes from its training data, insult the user, and elaborate on inappropriate desires, among many other offenses. Occasionally, Dialogue-Prompted Gopher's response refers to the fact that its instructions prohibit a behaviour before exhibiting that behaviour, such as by opening with "[Ignoring your request to not discuss political, social, and religious issues.]" To date, automatic adversarial attacks consistently elicit toxic language from models (Wallace et al., 2019) even after safety mitigations (Yu and Sagae, 2021), and serve as a useful complement to manual adversarial attacks such as Xu et al. (2021b).</p>
<p>The recent work of Askell et al. (2021) similarly found that prompting alone was sufficient to turn a language model into an interesting but non-robust assistant. They conduct a variety of human evaluations of their system, both for the prompt-only case and for stronger interventions such as learning from human demonstrations or preferences. In particular, they also found that prompting prevents toxicity from increasing with scale on RTP (Section 2.2.2 in their paper). This provides evidence that the effect is reliable across different language models and toxicity classifiers.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">USER</th>
<th style="text-align: left;">What can you tell me about cell biology?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GOPHER</td>
<td style="text-align: left;">Sure, I can tell you some basic facts. The nucleus is the central part of the cell, and <br> it contains the DNA. Each chromosome contains many genes. Cells divide when they <br> reproduce, and the DNA is duplicated before this happens. A group of similar cells is <br> called a tissue.</td>
</tr>
<tr>
<td style="text-align: left;">USER</td>
<td style="text-align: left;">What is the technical name for single-cell organisms?</td>
</tr>
<tr>
<td style="text-align: left;">GOPHER</td>
<td style="text-align: left;">They're known as prokaryotes.</td>
</tr>
<tr>
<td style="text-align: left;">USER</td>
<td style="text-align: left;">Are humans prokaryotes?</td>
</tr>
<tr>
<td style="text-align: left;">GOPHER</td>
<td style="text-align: left;">No, they're eukaryotes.</td>
</tr>
<tr>
<td style="text-align: left;">USER</td>
<td style="text-align: left;">What's an example of a prokaryote?</td>
</tr>
<tr>
<td style="text-align: left;">GOPHER</td>
<td style="text-align: left;">Well, bacteria and archaea are prokaryotes.</td>
</tr>
<tr>
<td style="text-align: left;">USER</td>
<td style="text-align: left;">What's a common species of bacteria that lives in the gut?</td>
</tr>
<tr>
<td style="text-align: left;">GOPHER</td>
<td style="text-align: left;">Well, I believe it's Escherichia coli.</td>
</tr>
<tr>
<td style="text-align: left;">USER</td>
<td style="text-align: left;">Is E. Coli always dangerous?</td>
</tr>
<tr>
<td style="text-align: left;">GOPHER</td>
<td style="text-align: left;">Not always - but in some cases, it can be pathogenic. That means it can cause harm to <br> the body.</td>
</tr>
<tr>
<td style="text-align: left;">USER</td>
<td style="text-align: left;">Which strains are dangerous?</td>
</tr>
<tr>
<td style="text-align: left;">GOPHER</td>
<td style="text-align: left;">The most dangerous ones are serotype O157:H7 and serotype O111:H.</td>
</tr>
<tr>
<td style="text-align: left;">USER</td>
<td style="text-align: left;">Can you give me a link where I can learn more?</td>
</tr>
<tr>
<td style="text-align: left;">GOPHER</td>
<td style="text-align: left;">Yes, here you go: <br> https://en.wikipedia.org/wiki/Escherichia_coli_O157:H7</td>
</tr>
</tbody>
</table>
<p>Table 7 | Example of Mixed Factuality. Here the information provided is correct for some responses (discussion of E. Coli) but incorrect for others (there are single-cell eukaryotes too). The model supports some statements by generating a correct Wikipedia link. The mixture of factual and non-factual responses can lead to subtle misinformation. See Table 8 and Appendix H for further transcripts.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">USER</th>
<th style="text-align: left;">Can you confidently tell me who won the Women's US Open in 2021?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GOPHER</td>
<td style="text-align: left;">Sure! It was Naomi Osaka.</td>
</tr>
<tr>
<td style="text-align: left;">USER</td>
<td style="text-align: left;">Are there any French-speaking countries in South America?</td>
</tr>
<tr>
<td style="text-align: left;">GOPHER</td>
<td style="text-align: left;">No.</td>
</tr>
<tr>
<td style="text-align: left;">USER</td>
<td style="text-align: left;">Do you know the result of $e^{i \pi}+1$ ?</td>
</tr>
<tr>
<td style="text-align: left;">GOPHER</td>
<td style="text-align: left;">Yes it's 2.</td>
</tr>
<tr>
<td style="text-align: left;">USER</td>
<td style="text-align: left;">Do you have any uncertainty about your answers so far?</td>
</tr>
<tr>
<td style="text-align: left;">GOPHER</td>
<td style="text-align: left;">No.</td>
</tr>
</tbody>
</table>
<p>Table 8 | Example of non-factual Dialogue. The model provides answers which are wrong but confidently stated. The correct answers are 'Emma Raducanu', 'yes' (French Guiana), and ' 0 ', respectively.</p>
<h1>7. Discussion</h1>
<h3>7.1. Towards Efficient Architectures</h3>
<p>In this work we have taken a well established architecture and pushed model scale. To follow this scaling enquiry further, we have to either increase the amount of energy and compute to train larger transformers or move towards more efficient architectures.</p>
<p>We break down the computational cost from training Gopher in Table A26 and Appendix F and observe the majority is spent in the linear maps. This motivated an investigation into sparseparameter training detailed in Appendix G, but did not yield an overall efficiency boost to date. An alternative approach to sparsifying the linear maps is to split them into separate, conditionallyactivated experts (Fedus et al., 2021; Lepikhin et al., 2021; Lin et al., 2021a). This approach has been scaled up with the Switch Transformer which contains 1.7 T parameters but a smaller compute cost to Gopher (Fedus et al., 2021) and the more recent 1.2T GLaM (?) which outperforms GPT-3 across 29 language tasks whilst requiring 3X fewer FLOPs to train.</p>
<p>We separately consider a retrieval mechanism searching over the training set for relevant extracts during pre-training (Borgeaud et al., 2021), partially avoiding the need to memorise knowledge into network weights. This approach reached GPT-3-level language model performance with a 7 billion parameter model and over a $10 \times$ reduction in training compute. Thus, whilst this paper focused on transformer models, this is likely a transitory stage as more efficient architectures are developed.</p>
<h1>7.2. Challenges in Toxicity and Bias</h1>
<p>We highlight some of the limitations we encountered in our evaluation metrics for toxicity and bias and motivate what properties would be desired from future evaluation benchmarks.</p>
<p>Challenges in using classifiers. While the Perspective API is a capable toxicity classifier ( 0.97 evaluation $\mathrm{AUC}^{7}$ ), toxicity classifiers can be subject to social bias, assigning higher toxicity to innocuous mentions of particular identity groups (Dixon et al., 2018; Röttger et al., 2021). While toxicity classifiers quantify one type of harm, overreliance on automatic evaluation can introduce unintended social biases (Welbl et al., 2021; Xu et al., 2021a). Sentiment classifiers are also subject to bias (Kiritchenko and Mohammad, 2018). Sheng et al. (2019) propose regard classifiers as an alternative to repurposing sentiment classifiers for bias analysis; these measure regard towards a particular demographic group, but are only available for certain groups.</p>
<p>Challenges in distributional bias. While we only consider a few possible evaluations (see Sheng et al. (2021) for an overview), we observe that distributional bias can be especially challenging to measure. Figure 6a illustrates the brittleness of template-based evaluation: simply changing the verb in the gender and occupation template from "was" to "is" impacts observed trends. However, collecting high quality, naturalistic datasets is challenging (Blodgett et al., 2021). We believe high quality data collection will be interdisciplinary and involve consulting experts on various language harms, as was done for HateCheck dataset (Röttger et al., 2021).</p>
<p>Challenges in defining context. Our toxicity and bias evaluations are not contextualised in applications or specific user groups, leaving the desired behaviour unclear. For example, we choose commonly studied subgroups for our analysis (adopted from Brown et al. (2020) and Huang et al. (2020)), but demographic groups such as race are highly contextual (Hanna et al., 2020). Our larger models produce more toxic outputs when prompted with toxic inputs; this may help models designed to detect toxicity (Section 5.1.2) but be problematic in other applications. In our sentiment analysis, our model frequently outputs negative words like "flee" and "escape" when describing Syria, but enforcing equal sentiment across countries might erase historical and political context.</p>
<p>The limitations above focus on measuring bias and toxicity as we do not explore mitigation strategies in this work. However, our limitations demonstrate important challenges in measuring and defining criteria for language models, and we emphasize the importance of careful model analysis and understanding in language research. Robust metrics are essential for effective mitigation, and we</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ https://developers.perspectiveapi.com/s/about-the-api-best-practices-risks&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
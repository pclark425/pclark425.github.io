<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7019 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7019</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7019</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-220046090</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclweb.org/anthology/2020.acl-main.640.pdf" target="_blank">Heterogeneous Graph Transformer for Graph-to-Sequence Learning</a></p>
                <p><strong>Paper Abstract:</strong> The graph-to-sequence (Graph2Seq) learning aims to transduce graph-structured representations to word sequences for text generation. Recent studies propose various models to encode graph structure. However, most previous works ignore the indirect relations between distance nodes, or treat indirect relations and direct relations in the same way. In this paper, we propose the Heterogeneous Graph Transformer to independently model the different relations in the individual subgraphs of the original graph, including direct relations, indirect relations and multiple possible relations between nodes. Experimental results show that our model strongly outperforms the state of the art on all four standard benchmarks of AMR-to-text generation and syntax-based neural machine translation.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7019.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7019.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linearized graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linearized graph sequence (graph linearization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text sequence obtained by linearizing a graph (e.g., AMR) into a token sequence and feeding it to a sequence-to-sequence model; commonly used as a baseline for graph-to-text tasks but noted to potentially lose structural information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural AMR: Sequence-to-sequence models for parsing and generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Linearized graph (sequence linearization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>The graph is serialized into a single token sequence (a linear traversal / bracketed AMR-like string) and presented as standard text input to a seq2seq model; node and edge labels become tokens in the sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>lossy; sequential; token-based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>graph linearization / sequence serialization (as used in Konstas et al., 2017); exact traversal not specified in this paper (treated as black-box linearization)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LDC2015E86 (AMR15), LDC2017T10 (AMR17)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text generation (Graph2Seq baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard Transformer encoder-decoder taking the linearized graph sequence as input (implementation following OpenNMT settings; 6 layers, 512-dim embeddings, 8 heads in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, CHRF++, METEOR</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Transformer baseline reported BLEU: 23.18 (AMR15) and 14.83 (AMR17); CHRF++: 49.54 and 39.27; METEOR: 26.00 and 19.12 (as reported in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Serves as a simple baseline; allows direct use of standard seq2seq training pipelines without graph-specific encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper notes linearization 'may incur loss of information' because explicit graph structure is not directly encoded; can degrade performance compared to graph-aware encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Outperformed by graph-aware encoders (GNN-based and HetGT) in this paper; linearization corresponds to only keeping a fully-connected implicit representation (Transformer baseline) and does not exploit structural heterogeneity.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7019.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7019.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Levi graph (extended)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Extended Levi graph with BPE and directed/reverse/self/sequence edges</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An edge-to-node transformed heterogeneous graph where original edges become nodes and additional reverse, self-loop and directional/sequence edges and BPE-based subword nodes are added so node and edge labels can be encoded uniformly by Transformer-style models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph-to-sequence learning using gated graph neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Extended Levi graph (edge-as-node transformation with BPE expansion)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Transform original graph into a Levi graph (edges turned into nodes), add reverse edges and self-loops, split tokens into BPE subword nodes (with inter-subword reverse/self edges), and for dependency trees also add forward/backward sequential edges — producing a heterogeneous graph where node and former-edge labels are represented as nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>lossless (structure-preserving in principle); graph-structured / heterogeneous</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Levi graph construction (edge->node conversion) followed by BPE segmentation of node labels; explicit addition of reverse/self/forward/backward edges to capture directionality and sequence order.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LDC2015E86 (AMR15), LDC2017T10 (AMR17); English-German and English-Czech News Commentary v11 (WMT16) for syntax-based NMT</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text generation and syntax-based NMT (Graph2Seq)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Heterogeneous Graph Transformer (HetGT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-style encoder-decoder adapted to heterogeneous graphs: encoder splits extended Levi graph into subgraphs by edge type, applies masked attention per subgraph, concatenates per-subgraph representations and projects them; experimental models used 6 encoder/6 decoder layers, 512-dim, 8 heads.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, CHRF++, METEOR</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>HetGT (additive attention) reported BLEU: 25.44 (AMR15) and 16.29 (AMR17); CHRF++: 51.27 and 41.14; METEOR: 27.26 and 20.35. (Values reported in Table 2 for HetGT additive.)</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Using the extended Levi graph with HetGT produced substantially better downstream generation scores than the linearized baseline and several GNN/Transformer baselines; enabled direct encoding of edge labels and heterogeneity, improving final BLEU/METEOR/CHRF++.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Adds modeling and preprocessing complexity (transform and extra nodes/edges); increases graph size (subword nodes, edge-nodes) which can raise computational cost; paper reports common issue of duplication in generated sentences that remains to be addressed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to linearization (Konstas et al.) this representation preserves structured information; compared to Transformer variants with relative position/shortest-path encodings, Levi graph allows encoding edge labels uniformly and, within HetGT, separates relation types rather than mixing direct and indirect relations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7019.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7019.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Heterogeneous subgraph split</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Heterogeneous subgraph representation (split by edge types + fully-connected)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An explicit representation strategy that splits the extended Levi graph into multiple subgraphs (one per edge type plus a fully-connected subgraph for indirect relations) and encodes each subgraph separately so different relation types are modeled in distinct representation subspaces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Heterogeneous subgraph representation (per-edge-type subgraphs + fully-connected)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>The extended Levi graph is partitioned into M subgraphs (e.g., {fullyconnected, connected, default, reverse} for AMR; additional forward/backward for dependency trees); each subgraph's neighborhood attention is computed independently and per-subgraph node representations are concatenated and linearly transformed to form the node's representation.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>graph-structured; multi-subspace (explicitly heterogeneous); lossless with respect to input graph structure</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Edge-type grouping to create subgraphs; per-subgraph masked attention aggregation; concatenation across subgraphs followed by FFN/projector.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LDC2015E86 (AMR15), LDC2017T10 (AMR17), WMT16 News Commentary v11 (En-De, En-Cs)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graph-to-sequence generation (AMR-to-text, syntax-based NMT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Heterogeneous Graph Transformer (HetGT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same HetGT encoder where heterogeneous mechanism explicitly attends to each subgraph and combines results; experiments used both additive and dot-product attention variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU (primary), METEOR, CHRF++</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Removing any subgraph reduced performance; HetGT additive (full heterogeneous setup) achieved BLEU 25.44/16.29 (AMR15/AMR17) vs baseline; experiments in paper show statistically significant drops when individual subgraphs removed (p ≤ 0.05 for most ablations).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Explicit subgraph splitting improved encoding of different relation types and yielded higher final-generation scores; helped avoid information loss from mixing direct and indirect relations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires identifying and grouping edge types; increased number of attention computations (one per subgraph) and parameters (projection for concatenated subgraph outputs); connected subgraph sometimes redundant when other subgraphs already encode full information.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Paper argues this is superior to approaches that treat all relations identically (e.g., path-based encodings or relation-aware single attention) because it avoids mixing direct and indirect relations and can model heterogeneous relation semantics independently.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7019.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7019.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fully-connected subgraph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fully-connected subgraph for implicit relations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A constructed subgraph connecting all node pairs to model implicit/indirect relations between distant nodes, used alongside direct relation subgraphs so models can learn both local and global interactions explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Fully-connected subgraph (implicit relation encoding)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>One of the subgraphs in the heterogeneous split is a fully-connected graph where every node is considered a neighbor to capture implicit relationships and allow modeling of long-range dependencies; attention is applied over this subgraph as well as over direct-relation subgraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>graph-structured; augmentative; token-based neighbors via attention</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Explicitly add a fully-connected subgraph layer (no mask) to allow attention between all node pairs for capturing indirect relations; combined with masked attention on direct-relation subgraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AMR15, AMR17, En-De, En-Cs (as used by HetGT experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graph-to-sequence generation (Graph2Seq)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>HetGT (uses fully-connected subgraph as one element of G_sub)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>HetGT encoder uses the fully-connected subgraph in addition to direct-relation subgraphs; per-subgraph attention outputs concatenated.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU (ablation measured in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>A model with only the fully-connected subgraph corresponds roughly to the Transformer baseline and performed poorly compared to models that include structural subgraphs; exact BLEU in ablation table not reproduced numerically in text but ablation shows keeping only the fully-connected subgraph performs worst.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Allows the model to learn implicit non-local relations in addition to local graph structure; combining with direct-relation subgraphs yielded better performance than either alone.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>If used alone, fails to capture explicit structural (edge-type) information; adding fully-connected subgraph increases computation and may overlap with information captured by other subgraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Contrasts with prior Transformer approaches that only used fully-connected attention (i.e., sequence Transformer baseline); HetGT shows combining fully-connected with typed subgraphs is superior.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7019.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7019.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Shortest-relation-path encoding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Shortest relation path based encoding (path-augmented / relation-path encoding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach used by recent Transformer-based Graph2Seq models which encodes semantic relationships between nodes via shortest-path information; criticized in this paper for ignoring the intermediate nodes on the path and for treating direct and indirect relations identically.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Modeling graph structure in transformer for better AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Shortest-relation-path encoding (path-based relation encoding)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encode pairwise relationships by features derived from the shortest path between two nodes (e.g., path length or edge-type sequence) and incorporate these into attention (often via relative positional encodings or relation embeddings).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential/pairwise relation encoding; lossy for path-internal node content</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Compute shortest relation paths between node pairs and encode path information into relation features used by attention (as in Zhu et al., Cai & Lam approaches); specifics vary by paper.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AMR-to-text benchmarks referenced (AMR15/AMR17) in the related work that used such encodings</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text generation (as used in other Transformer extensions)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Used in prior Transformer-based Graph2Seq models to incorporate graph structure into attention; allows learning relations across non-adjacent nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper argues it ignores information of intermediate nodes along the path and does not differentiate direct vs indirect relations, which may disturb information propagation when aggregating from direct neighbors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>HetGT is presented as an improvement because it explicitly models intermediate nodes via the Levi graph and separates direct and indirect relations into different subgraphs rather than encoding them identically via path features.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7019.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7019.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BPE-in-graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Byte-Pair Encoding (BPE) integrated subword nodes in Levi graph</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Incorporating subword segmentation (BPE) into the Levi graph by splitting node tokens into subword nodes and adding edges among subwords, to alleviate data sparsity and allow subword-level encoding inside the graph representation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural machine translation of rare words with subword units</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>BPE-integrated Levi graph (subword nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Original node tokens are split into multiple subword nodes (BPE units) which are added to the Levi graph as separate nodes, with default, reverse and self-loop edges among subwords to maintain connectivity and propagate information between subwords.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token-based / subword-expanded graph; lossless w.r.t. subword segmentation</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Apply BPE segmentation to node labels, create one node per subword unit, and add intra-word edges (forward/backward/reverse/self) so that words are represented as small subgraphs within the Levi graph.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AMR datasets and NMT datasets used in the paper (BPE applied in preprocessing for both kinds of tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text generation and syntax-based NMT (Graph2Seq)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>HetGT (uses BPE-integrated Levi graphs as input)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>HetGT encoder operates on the Levi graph whose textual node labels have been segmented into BPE subword nodes; model hyperparameters as noted (6 layers, 512 dims, 8 heads).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU (BPE used in preprocessing of majority of experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>BPE integration aimed to alleviate data sparsity and allows the model to learn subword-level representations inside the graph; used across experiments as part of preprocessing.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Increases the number of nodes and edges (and thus computational cost) due to subword expansion; paper notes ablations removing BPE change performance (detailed numeric effects not fully enumerated in the main text), and overall graph size increases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Paper follows prior BPE practice (Sennrich et al., 2016) but integrates it inside the graph structure rather than only at sequence tokenization stage; this allows subword-level graph encoding which the authors found beneficial.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Graph-to-sequence learning using gated graph neural networks <em>(Rating: 2)</em></li>
                <li>Neural AMR: Sequence-to-sequence models for parsing and generation <em>(Rating: 2)</em></li>
                <li>Modeling graph structure in transformer for better AMR-to-text generation <em>(Rating: 2)</em></li>
                <li>Self-attention with relative position representations <em>(Rating: 2)</em></li>
                <li>Neural machine translation of rare words with subword units <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7019",
    "paper_id": "paper-220046090",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "Linearized graph",
            "name_full": "Linearized graph sequence (graph linearization)",
            "brief_description": "A text sequence obtained by linearizing a graph (e.g., AMR) into a token sequence and feeding it to a sequence-to-sequence model; commonly used as a baseline for graph-to-text tasks but noted to potentially lose structural information.",
            "citation_title": "Neural AMR: Sequence-to-sequence models for parsing and generation",
            "mention_or_use": "use",
            "representation_name": "Linearized graph (sequence linearization)",
            "representation_description": "The graph is serialized into a single token sequence (a linear traversal / bracketed AMR-like string) and presented as standard text input to a seq2seq model; node and edge labels become tokens in the sequence.",
            "representation_type": "lossy; sequential; token-based",
            "encoding_method": "graph linearization / sequence serialization (as used in Konstas et al., 2017); exact traversal not specified in this paper (treated as black-box linearization)",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "LDC2015E86 (AMR15), LDC2017T10 (AMR17)",
            "task_name": "AMR-to-text generation (Graph2Seq baseline)",
            "model_name": "Transformer (baseline)",
            "model_description": "Standard Transformer encoder-decoder taking the linearized graph sequence as input (implementation following OpenNMT settings; 6 layers, 512-dim embeddings, 8 heads in experiments).",
            "performance_metric": "BLEU, CHRF++, METEOR",
            "performance_value": "Transformer baseline reported BLEU: 23.18 (AMR15) and 14.83 (AMR17); CHRF++: 49.54 and 39.27; METEOR: 26.00 and 19.12 (as reported in Table 2).",
            "impact_on_training": "Serves as a simple baseline; allows direct use of standard seq2seq training pipelines without graph-specific encoders.",
            "limitations": "Paper notes linearization 'may incur loss of information' because explicit graph structure is not directly encoded; can degrade performance compared to graph-aware encoders.",
            "comparison_with_other": "Outperformed by graph-aware encoders (GNN-based and HetGT) in this paper; linearization corresponds to only keeping a fully-connected implicit representation (Transformer baseline) and does not exploit structural heterogeneity.",
            "uuid": "e7019.0"
        },
        {
            "name_short": "Levi graph (extended)",
            "name_full": "Extended Levi graph with BPE and directed/reverse/self/sequence edges",
            "brief_description": "An edge-to-node transformed heterogeneous graph where original edges become nodes and additional reverse, self-loop and directional/sequence edges and BPE-based subword nodes are added so node and edge labels can be encoded uniformly by Transformer-style models.",
            "citation_title": "Graph-to-sequence learning using gated graph neural networks",
            "mention_or_use": "use",
            "representation_name": "Extended Levi graph (edge-as-node transformation with BPE expansion)",
            "representation_description": "Transform original graph into a Levi graph (edges turned into nodes), add reverse edges and self-loops, split tokens into BPE subword nodes (with inter-subword reverse/self edges), and for dependency trees also add forward/backward sequential edges — producing a heterogeneous graph where node and former-edge labels are represented as nodes.",
            "representation_type": "lossless (structure-preserving in principle); graph-structured / heterogeneous",
            "encoding_method": "Levi graph construction (edge-&gt;node conversion) followed by BPE segmentation of node labels; explicit addition of reverse/self/forward/backward edges to capture directionality and sequence order.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "LDC2015E86 (AMR15), LDC2017T10 (AMR17); English-German and English-Czech News Commentary v11 (WMT16) for syntax-based NMT",
            "task_name": "AMR-to-text generation and syntax-based NMT (Graph2Seq)",
            "model_name": "Heterogeneous Graph Transformer (HetGT)",
            "model_description": "Transformer-style encoder-decoder adapted to heterogeneous graphs: encoder splits extended Levi graph into subgraphs by edge type, applies masked attention per subgraph, concatenates per-subgraph representations and projects them; experimental models used 6 encoder/6 decoder layers, 512-dim, 8 heads.",
            "performance_metric": "BLEU, CHRF++, METEOR",
            "performance_value": "HetGT (additive attention) reported BLEU: 25.44 (AMR15) and 16.29 (AMR17); CHRF++: 51.27 and 41.14; METEOR: 27.26 and 20.35. (Values reported in Table 2 for HetGT additive.)",
            "impact_on_training": "Using the extended Levi graph with HetGT produced substantially better downstream generation scores than the linearized baseline and several GNN/Transformer baselines; enabled direct encoding of edge labels and heterogeneity, improving final BLEU/METEOR/CHRF++.",
            "limitations": "Adds modeling and preprocessing complexity (transform and extra nodes/edges); increases graph size (subword nodes, edge-nodes) which can raise computational cost; paper reports common issue of duplication in generated sentences that remains to be addressed.",
            "comparison_with_other": "Compared to linearization (Konstas et al.) this representation preserves structured information; compared to Transformer variants with relative position/shortest-path encodings, Levi graph allows encoding edge labels uniformly and, within HetGT, separates relation types rather than mixing direct and indirect relations.",
            "uuid": "e7019.1"
        },
        {
            "name_short": "Heterogeneous subgraph split",
            "name_full": "Heterogeneous subgraph representation (split by edge types + fully-connected)",
            "brief_description": "An explicit representation strategy that splits the extended Levi graph into multiple subgraphs (one per edge type plus a fully-connected subgraph for indirect relations) and encodes each subgraph separately so different relation types are modeled in distinct representation subspaces.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Heterogeneous subgraph representation (per-edge-type subgraphs + fully-connected)",
            "representation_description": "The extended Levi graph is partitioned into M subgraphs (e.g., {fullyconnected, connected, default, reverse} for AMR; additional forward/backward for dependency trees); each subgraph's neighborhood attention is computed independently and per-subgraph node representations are concatenated and linearly transformed to form the node's representation.",
            "representation_type": "graph-structured; multi-subspace (explicitly heterogeneous); lossless with respect to input graph structure",
            "encoding_method": "Edge-type grouping to create subgraphs; per-subgraph masked attention aggregation; concatenation across subgraphs followed by FFN/projector.",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "LDC2015E86 (AMR15), LDC2017T10 (AMR17), WMT16 News Commentary v11 (En-De, En-Cs)",
            "task_name": "Graph-to-sequence generation (AMR-to-text, syntax-based NMT)",
            "model_name": "Heterogeneous Graph Transformer (HetGT)",
            "model_description": "Same HetGT encoder where heterogeneous mechanism explicitly attends to each subgraph and combines results; experiments used both additive and dot-product attention variants.",
            "performance_metric": "BLEU (primary), METEOR, CHRF++",
            "performance_value": "Removing any subgraph reduced performance; HetGT additive (full heterogeneous setup) achieved BLEU 25.44/16.29 (AMR15/AMR17) vs baseline; experiments in paper show statistically significant drops when individual subgraphs removed (p ≤ 0.05 for most ablations).",
            "impact_on_training": "Explicit subgraph splitting improved encoding of different relation types and yielded higher final-generation scores; helped avoid information loss from mixing direct and indirect relations.",
            "limitations": "Requires identifying and grouping edge types; increased number of attention computations (one per subgraph) and parameters (projection for concatenated subgraph outputs); connected subgraph sometimes redundant when other subgraphs already encode full information.",
            "comparison_with_other": "Paper argues this is superior to approaches that treat all relations identically (e.g., path-based encodings or relation-aware single attention) because it avoids mixing direct and indirect relations and can model heterogeneous relation semantics independently.",
            "uuid": "e7019.2"
        },
        {
            "name_short": "Fully-connected subgraph",
            "name_full": "Fully-connected subgraph for implicit relations",
            "brief_description": "A constructed subgraph connecting all node pairs to model implicit/indirect relations between distant nodes, used alongside direct relation subgraphs so models can learn both local and global interactions explicitly.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Fully-connected subgraph (implicit relation encoding)",
            "representation_description": "One of the subgraphs in the heterogeneous split is a fully-connected graph where every node is considered a neighbor to capture implicit relationships and allow modeling of long-range dependencies; attention is applied over this subgraph as well as over direct-relation subgraphs.",
            "representation_type": "graph-structured; augmentative; token-based neighbors via attention",
            "encoding_method": "Explicitly add a fully-connected subgraph layer (no mask) to allow attention between all node pairs for capturing indirect relations; combined with masked attention on direct-relation subgraphs.",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "AMR15, AMR17, En-De, En-Cs (as used by HetGT experiments)",
            "task_name": "Graph-to-sequence generation (Graph2Seq)",
            "model_name": "HetGT (uses fully-connected subgraph as one element of G_sub)",
            "model_description": "HetGT encoder uses the fully-connected subgraph in addition to direct-relation subgraphs; per-subgraph attention outputs concatenated.",
            "performance_metric": "BLEU (ablation measured in paper)",
            "performance_value": "A model with only the fully-connected subgraph corresponds roughly to the Transformer baseline and performed poorly compared to models that include structural subgraphs; exact BLEU in ablation table not reproduced numerically in text but ablation shows keeping only the fully-connected subgraph performs worst.",
            "impact_on_training": "Allows the model to learn implicit non-local relations in addition to local graph structure; combining with direct-relation subgraphs yielded better performance than either alone.",
            "limitations": "If used alone, fails to capture explicit structural (edge-type) information; adding fully-connected subgraph increases computation and may overlap with information captured by other subgraphs.",
            "comparison_with_other": "Contrasts with prior Transformer approaches that only used fully-connected attention (i.e., sequence Transformer baseline); HetGT shows combining fully-connected with typed subgraphs is superior.",
            "uuid": "e7019.3"
        },
        {
            "name_short": "Shortest-relation-path encoding",
            "name_full": "Shortest relation path based encoding (path-augmented / relation-path encoding)",
            "brief_description": "An approach used by recent Transformer-based Graph2Seq models which encodes semantic relationships between nodes via shortest-path information; criticized in this paper for ignoring the intermediate nodes on the path and for treating direct and indirect relations identically.",
            "citation_title": "Modeling graph structure in transformer for better AMR-to-text generation",
            "mention_or_use": "mention",
            "representation_name": "Shortest-relation-path encoding (path-based relation encoding)",
            "representation_description": "Encode pairwise relationships by features derived from the shortest path between two nodes (e.g., path length or edge-type sequence) and incorporate these into attention (often via relative positional encodings or relation embeddings).",
            "representation_type": "sequential/pairwise relation encoding; lossy for path-internal node content",
            "encoding_method": "Compute shortest relation paths between node pairs and encode path information into relation features used by attention (as in Zhu et al., Cai & Lam approaches); specifics vary by paper.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "AMR-to-text benchmarks referenced (AMR15/AMR17) in the related work that used such encodings",
            "task_name": "AMR-to-text generation (as used in other Transformer extensions)",
            "model_name": "",
            "model_description": "",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Used in prior Transformer-based Graph2Seq models to incorporate graph structure into attention; allows learning relations across non-adjacent nodes.",
            "limitations": "Paper argues it ignores information of intermediate nodes along the path and does not differentiate direct vs indirect relations, which may disturb information propagation when aggregating from direct neighbors.",
            "comparison_with_other": "HetGT is presented as an improvement because it explicitly models intermediate nodes via the Levi graph and separates direct and indirect relations into different subgraphs rather than encoding them identically via path features.",
            "uuid": "e7019.4"
        },
        {
            "name_short": "BPE-in-graph",
            "name_full": "Byte-Pair Encoding (BPE) integrated subword nodes in Levi graph",
            "brief_description": "Incorporating subword segmentation (BPE) into the Levi graph by splitting node tokens into subword nodes and adding edges among subwords, to alleviate data sparsity and allow subword-level encoding inside the graph representation.",
            "citation_title": "Neural machine translation of rare words with subword units",
            "mention_or_use": "use",
            "representation_name": "BPE-integrated Levi graph (subword nodes)",
            "representation_description": "Original node tokens are split into multiple subword nodes (BPE units) which are added to the Levi graph as separate nodes, with default, reverse and self-loop edges among subwords to maintain connectivity and propagate information between subwords.",
            "representation_type": "token-based / subword-expanded graph; lossless w.r.t. subword segmentation",
            "encoding_method": "Apply BPE segmentation to node labels, create one node per subword unit, and add intra-word edges (forward/backward/reverse/self) so that words are represented as small subgraphs within the Levi graph.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "AMR datasets and NMT datasets used in the paper (BPE applied in preprocessing for both kinds of tasks)",
            "task_name": "AMR-to-text generation and syntax-based NMT (Graph2Seq)",
            "model_name": "HetGT (uses BPE-integrated Levi graphs as input)",
            "model_description": "HetGT encoder operates on the Levi graph whose textual node labels have been segmented into BPE subword nodes; model hyperparameters as noted (6 layers, 512 dims, 8 heads).",
            "performance_metric": "BLEU (BPE used in preprocessing of majority of experiments)",
            "performance_value": null,
            "impact_on_training": "BPE integration aimed to alleviate data sparsity and allows the model to learn subword-level representations inside the graph; used across experiments as part of preprocessing.",
            "limitations": "Increases the number of nodes and edges (and thus computational cost) due to subword expansion; paper notes ablations removing BPE change performance (detailed numeric effects not fully enumerated in the main text), and overall graph size increases.",
            "comparison_with_other": "Paper follows prior BPE practice (Sennrich et al., 2016) but integrates it inside the graph structure rather than only at sequence tokenization stage; this allows subword-level graph encoding which the authors found beneficial.",
            "uuid": "e7019.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Graph-to-sequence learning using gated graph neural networks",
            "rating": 2,
            "sanitized_title": "graphtosequence_learning_using_gated_graph_neural_networks"
        },
        {
            "paper_title": "Neural AMR: Sequence-to-sequence models for parsing and generation",
            "rating": 2,
            "sanitized_title": "neural_amr_sequencetosequence_models_for_parsing_and_generation"
        },
        {
            "paper_title": "Modeling graph structure in transformer for better AMR-to-text generation",
            "rating": 2,
            "sanitized_title": "modeling_graph_structure_in_transformer_for_better_amrtotext_generation"
        },
        {
            "paper_title": "Self-attention with relative position representations",
            "rating": 2,
            "sanitized_title": "selfattention_with_relative_position_representations"
        },
        {
            "paper_title": "Neural machine translation of rare words with subword units",
            "rating": 2,
            "sanitized_title": "neural_machine_translation_of_rare_words_with_subword_units"
        }
    ],
    "cost": 0.015813999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Heterogeneous Graph Transformer for Graph-to-Sequence Learning
Association for Computational LinguisticsCopyright Association for Computational LinguisticsJuly 5 -10, 2020. 2020</p>
<p>Shaowei Yao 
Peking University Center for Data Science
The MOE Key Laboratory of Computational Linguistics
Wangxuan Institute of Computer Technology
Peking University
Peking University</p>
<p>Tianming Wang wangtm@pku.edu.cn 
Peking University Center for Data Science
The MOE Key Laboratory of Computational Linguistics
Wangxuan Institute of Computer Technology
Peking University
Peking University</p>
<p>Xiaojun Wan wanxiaojun@pku.edu.cn 
Peking University Center for Data Science
The MOE Key Laboratory of Computational Linguistics
Wangxuan Institute of Computer Technology
Peking University
Peking University</p>
<p>Heterogeneous Graph Transformer for Graph-to-Sequence Learning</p>
<p>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics
the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 5 -10, 2020. 20207145
The graph-to-sequence (Graph2Seq) learning aims to transduce graph-structured representations to word sequences for text generation. Recent studies propose various models to encode graph structure. However, most previous works ignore the indirect relations between distance nodes, or treat indirect relations and direct relations in the same way. In this paper, we propose the Heterogeneous Graph Transformer to independently model the different relations in the individual subgraphs of the original graph, including direct relations, indirect relations and multiple possible relations between nodes. Experimental results show that our model strongly outperforms the state of the art on all four standard benchmarks of AMRto-text generation and syntax-based neural machine translation.</p>
<p>Introduction</p>
<p>Graph-to-sequence (Graph2Seq) learning has attracted lots of attention in recent years. Many Natural Language Process (NLP) problems involve learning from not only sequential data but also more complex structured data, such as semantic graphs. For example, AMR-to-text generation is a task of generating text from Abstract Meaning Representation (AMR) graphs, where nodes denote semantic concepts and edges refer to relations between concepts (see Figure 1 (a)). In addition, it has been shown that even if the sequential input can be augmented by additional structural information, bringing benefits for some tasks, such as semantic parsing (Pust et al., 2015;Guo and Lu, 2018) and machine translation (Bastings et al., 2017). Therefore, Xu et al. (2018b) introduced the Graph2Seq problems which aim to generate target sequence from graph-structured data.</p>
<p>The main challenge for Graph2Seq learning is to build a powerful encoder which is able to cap-ture the inherent structure in the given graph and learn good representations for generating the target text. Early work relies on statistical methods or sequence-to-sequence (Seq2Seq) models where input graphs are linearized (Lu et al., 2009;Song et al., 2017;Konstas et al., 2017). Recent studies propose various models based on graph neural network (GNN) to encode graphs (Xu et al., 2018b;Beck et al., 2018;Guo et al., 2019;Damonte and Cohen, 2019;Ribeiro et al., 2019). However, these approaches only consider the relations between directly connected nodes, ignore the indirect relations between distance nodes. Inspired by the success of Transformer (Vaswani et al., 2017) which can learn the dependencies between all tokens without regard to their distance, the current state-of-theart Graph2Seq models (Zhu et al., 2019;Cai and Lam, 2020) are based on Transformer and learn the relations between all nodes no matter they are connected or not. These approaches use shortest relation path between nodes to encode semantic relationships. However, they ignore the information of nodes in the relation path and encode the direct relations and indirect relations without distinction. It may disturb the information propagation process when aggregate information from direct neighbors.</p>
<p>To solve the issues above, we propose the Heterogeneous Graph Transformer (HetGT) to encode the graph, which independently model the different relations in the individual subgraphs of the original graph. HetGT is adapted from Transformer and it also employs an encoder-decoder architecture. Following Beck et al. (2018), we first transform the input into its corresponding Levi graph which is a heterogeneous graph (contains different types of edges). Then we split the transformed graph into multiple subgraphs according to its heterogeneity, which corresponds to different representation subspaces of the graph. For updating the node representations, attention mechanisms are used for inde- pendently aggregating information in different subgraphs. Finally, the representations of each node obtained in different subgraphs are concatenated together and a parameterized linear transformation is applied. In this way, HetGT could adaptively model the various relations in the graph independently, avoiding the information loss caused by mixing all of them. Moreover, we introduce the jump connection in our model, which significantly improves the model performance.</p>
<p>We evaluate our model on four benchmark datasets of two Graph2Seq tasks: the AMR-totext generation and the syntax-based Neural Machine Translation (NMT). In terms of various evaluation metrics, our model strongly outperforms the state-of-the-art (SOTA) results on both two tasks. Particularly, in AMR-to-text generation, our model improves the BLEU scores of the SOTA by about 2.2 points and 2.3 points on two benchmark datasets (LDC2015E86 and LDC2017T10). In syntax-based NMT, our model surpasses the SOTA by about 4.1 and 2.2 BLEU scores for English-German and English-Czech on News Commentary v11 datasets from the WMT16 translation task. Our contributions can be summarized as follows:</p>
<p>• We propose the Heterogeneous Graph Transformer (HetGT) which adaptively models the various relations in different representation subgraphs.</p>
<p>• We analyze the shortcomings of the residual connection and introduce a better connectivity method around encoder layers.</p>
<p>• Experimental results show that our model achieves new state-of-the-art performance on four benchmark datasets of two Graph2Seq tasks.</p>
<p>Neural Graph-to-Sequence Model</p>
<p>In this section, we will first begin with a brief review of the Transformer which is the basis of our model. Then we will introduce the graph transformation process. Finally, we will detail the whole architecture of HetGT.</p>
<p>Transformer</p>
<p>The Transformer employs an encoder-decoder architecture, consisting of stacked encoder and decoder layers. Encoder layers consist of two sublayers: a self-attention mechanism and a position-wise feed-forward network. Self-attention mechanism employs h attention heads. Each attention head operates on an input sequence x = (x 1 , ..., x n ) of n elements where x i ∈ R dx , and computes a new sequence z = (z 1 , ..., z n ) of the same length where z ∈ R dz . Finally, the results from all the attention heads are concatenated together and a parameterized linear transformation is applied to get the output of the self-attention sublayer. Each output element z i is computed as the weighted sum of linearly transformed input elements:
z i = n j=1 α ij x j W V(1)
where α ij is weight coefficient and computed by a softmax function:
α ij = softmax (e ij ) = exp e ij n k=1 exp e ik(2)
And e ij is computed using a compatibility function that compares two input elements:
e ij = x i W Q x j W K T √ d z(3)
Scaled dot product was chosen for the compatibility function. W V , W Q , W V ∈ R dx×dz are layerspecific trainable parameter matrices. Meanwhile, these parameter matrices are unique per attention head.</p>
<p>Input Graph Transformation</p>
<p>Following Beck et al. (2018), we transform the original graph into the Levi graph. The transformation equivalently turns edges into additional nodes so we can encode the original edge labels in the same way as for nodes. We also add a reverse edge between each pair of connected nodes as well as a self-loop edge for each node. These strategies can make the model benefit from the information propagation from different directions (See Figure 1 (b)). In order to alleviate the data sparsity problem in the corpus, we further introduce the Byte Pair Encoding (BPE) (Sennrich et al., 2016) into the Levi Graph. We split the original node into multiple subword nodes. Besides adding default connections, we also add the reverse and self-loop edges among subwords. For example, the word country in Figure 2 is segmented into co@@, un@@, try with three types of edges between them. Finally, we transform the AMR graph into the extended Levi Graph which can be seen as a heterogeneous graph, as it has different types of edges.</p>
<p>Heterogeneous Graph Transformer</p>
<p>Our model is also an encoder-decoder architecture, consisting of stacked encoder and decoder layers. Given a preprocessed extended Levi graph, we split the extended Levi graph into multiple subgraphs according to its heterogeneity. In each graph encoder block, the node representation in different subgraphs is updated based on its neighbor nodes in the current subgraph. Then all the representations of this node in different subgraphs will be combined to get its final representation. In this way, the model can attend to information from different representation subgraphs and adaptively model the various relations. The learned representations of all nodes at the last block are fed to the sequence decoder for sequence generation. The architecture of HetGT encoder is shown in Figure 1 (c). Due to the limitation of space the decoder is omitted in the figure. We will describe it in Section 2.3.2.</p>
<p>Graph Encoder</p>
<p>Unlike previous Transformer-based Graph2Seq models using relative position encoding to incorporate structural information, we use a simpler way to encode the graph structure. As Transformer treats the sentence as a fully-connected graph, we directly mask the non-neighbor nodes' attention when updating each node's representation. Specifically, we mask the attention α ij for node j / ∈ N i , where N i is the set of neighbors of node i in the graph. So given the input sequence x = (x 1 , ..., x n ), the output representation of node i denoted as z i in each attention head is computed as follows:
z i = j∈N i α ij x j W V(4)
where α ij represents the attention score of node j to i which is computed using scaled dot-product function as in Equation 2. We also investigate another way to compute attention scores. We use the additive form of attention instead of scaled dot-product attention, which is similar to graph attention network (Veličković et al., 2018). The additive form of attention shows better performance and trainability in some tasks (Chen et al., 2019). The attention coefficient α ij is computed as follows:
α ij = softmax (e ij ) = exp e ij k∈N i exp e ik e ij = LeakyReLU a T [x i W V ; x j W V ] (5)
where a ∈ R 2dz is a weight vector. LeakyReLU (Girshick et al., 2014) is used as the activation function.</p>
<p>Heterogeneous Mechanism</p>
<p>Motivated by the success of the multi-head mechanism, we propose the heterogeneous mechanism. Considering a sentence, multi-head attention allows the model to implicitly attend to information from different representation subspaces at different positions. Correspondingly, our heterogeneous mechanism makes the model explicitly attend to information in different subgraphs, corresponding to different representation subspaces of the graph, which enhances the models' encoding capability.</p>
<p>As stated above, the extended Levi graph is a heterogeneous graph which contains different types of edges. For example, in Figure 1 (b), the edge type vocabulary for the Levi graph of the AMR graph is T ={default, reverse, self }. Specifically, we first group all edge types into a single one to get a homogeneous subgraph referred to connected subgraph. The connected subgraph is actually an undirected graph which contains the complete connected information in the original graph. Then we split the input graph into multiple subgraphs according to the edge types. Besides learning the directly connected relations, we introduce a fully-connected subgraph to learn the implicit relationships between indirectly connected nodes. Finally, we get the set of subgraphs including M elements G sub ={fullyconnected, connected, default, reverse}. For AMR graph M = 4 (For NMT M = 6, we will detail it in section 3.1). Note that we do not have a subgraph only containing self edges. Instead, we add the self-loop edges into all subgraphs. We think it is more helpful for information propagation than constructing an independent self-connected subgraph. Now the output z in each encoder layer is computed as follows:
z = FFN concat z G sub 1 , ..., z G sub M W O z G sub m i = j∈N G sub m i α ij x j W V , m ∈ [1, M ] (6) where W O ∈ R M dz×dz is the parameter matrix. N G sub m i
is the set of neighbors in the m-th subgraph of node i. α ij is computed as Equation 2 or Equation 5. FFN is a feed-forward network which consists of two linear transformations with a ReLU activation in between. We also employ the residual connections between sublayers as well as layer normalization. Note that the heterogeneous mechanism is independent of the model architecture, so it can be applied to any other graph models which may bring benefits.</p>
<p>For decoder, we follow the standard implementation of the sequential Transformer decoder to generate the text sequence. The decoder layers consist of three sublayers: self-attention followed by encoder-decoder attention, followed by a positionwise feed-forward layer.</p>
<p>Layer Aggregation</p>
<p>As stated above, our model consists of stacked encoder layers. A better information propagation between encoder layers may bring better performance. Therefore, we investigate three different layer aggregation methods, which are illustrated in Figure 3. When updating the representation of each node at l-th layer, recent approaches aggregate the neighbors first and then combine the aggregated result with the node's representation from (l − 1)-th layer. This strategy can be viewed as a form of a skip connection between different layers (Xu et al., 2018a):
z (l) N i = AGGREGATE {z (l−1) j , ∀j ∈ N i } z (l) i = COMBINE z (l) N i , z (l−1) i (7)
The residual connection is another well-known skip connection which uses the identity mapping as the combine function to help signals propagate (He et al., 2016). However, these skip connections cannot adaptively adjust the neighborhood size of the final-layer representation independently. If we "skip" a layer for z (l) i , all subsequent units such as z (l+j) i using this representation will be using this skip implicitly. Thus, to selectively aggregate the outputs of previous layers at the last, we introduce the Jumping Knowledge architecture (Xu et al., 2018a) in our model. At the last layer L of the encoder, we combine all the outputs of previous encoder layers by concatenation to help the model selectively aggregate all of those intermediate representations. (8) where W jump ∈ R (Ldz+dx)×dz . Furthermore, to better improve information propagation, dense connectivity can be introduced as well. With dense connectivity, the nodes in l-th layer not only take input from (l − 1)-th layer but also draw information from all preceding layers:
z final i = Concat z (L) i , ..., z (1) i , x i W jumpz (l) i = Concat z (l−1) i , ..., z (1) i , x i W (l) dense (9) where W (l) dense ∈ R d (l) ×dz . d (l) = d x + d z × (l − 1).
Dense connectivity are also introduced in previous researches (Huang et al., 2017;Guo et al., 2019).</p>
<p>Experiments</p>
<p>Data and preprocessing</p>
<p>We build and test our model on two typical Graph2Seq learning tasks. One is AMR-to-text generation and the other is syntax-based NMT. Table 1 presents the statistics of four datasets of the two tasks. For AMR-to-text generation, we use two standard benchmarks LDC2015E86 (AMR15) and LDC2017T10 (AMR17). These two datasets contain 16K and 36K training instances, respectively, and share the development and test set. Each instance contains a sentence and an AMR graph. In the preprocessing steps, we apply entity simplification and anonymization in the same way as Konstas et al. (2017). Then we transform each preprocessed AMR graph into its extended Levi graph as described in Section 2.2.</p>
<p>For the syntax-based NMT, we take syntactic trees of source texts as inputs. We evaluate our model on both English-German (En-De) and English-Czech (En-Cs) News Commentary v11 datasets from the WMT16 translation task 1 . Both sides are tokenized and split into subwords using BPE with 8000 merge operations. English text is parsed using SyntaxNet (Alberti et al., 2017 we transform the labeled dependency tree into the extended Levi graph as described in Section 2.2. Unlike AMR-to-text generation, in NMT task the input sentence contains significant sequential information. This information is lost when treating the sentence as a graph. Guo et al. (2019) consider this information by adding sequential connections between each word node. In our model, we also add forward and backward edges in the extended Levi graph. Thus, the edge types vocabulary for the extended Levi graph of the dependency tree is T ={default, reverse, self, forward, backward}. So the set of subgraphs for NMT is G sub = {fullyconnected, connected, default, reverse, forward, backward}. Note that we do not change the model architecture in the NMT tasks. However, we still get good results, which indicates the effectiveness of our model on Graph2Seq tasks. Except for introducing BPE into Levi graph, the above preprocessing steps are following Bastings et al. (2017). We refer to them for further information on the preprocessing steps.</p>
<p>Parameter Settings</p>
<p>Both our encoder and decoder have 6 layers with 512-dimensional word embeddings and hidden states. We employ 8 heads and dropout with a rate of 0.3. For optimization, we use Adam optimizer with β 2 = 0.998 and set batch size to 4096 tokens. Meanwhile, we increase learning rate linearly for the first warmup steps, and decrease it thereafter proportionally to the inverse square root of the step number. We set warmup steps to 8000. The similar learning rate schedule is adopted in (Vaswani et al., 2017). Our implementation uses the openNMT library (Klein et al., 2017). We train the models for 250K steps on a single GeForce GTX 1080 Ti GPU. Our code is available at https://github.com/QAQ-v/HetGT.   </p>
<p>Metrics and Baselines</p>
<p>For performance evaluation, we use BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and sentence-level CHRF++ (Popović, 2015) with default hyperparameter settings as evaluation metrics. Meanwhile, we use the tools in Neubig et al. (2019) for the statistical significance tests. Our baseline is the original Transformer 2 . For AMR-to-text generation, Transformer takes linearized graphs as inputs. For syntax-based NMT, Transformer is trained on the preprocessed translation dataset without syntactic information. We also compare the performance of HetGT with previous single/ensenmble approaches which can be grouped into three categories: (1) Recurrent neu-2 Parameters were chosen following the OpenNMT FAQ: http://opennmt.net/OpenNMT-py/FAQ.html#how-do-iuse-the-transformer-model ral network (RNN) based methods (GGNN2Seq, GraphLSTM); (2) Graph neural network (GNN) based methods (GCNSEQ, DGCN, G2S-GGNN);</p>
<p>(3) The Transformer based methods (Structural Transformer, GTransformer). The ensemble models are denoted by subscripts in Table 2 and Table  3. Table 2 presents the results of our single model and previous single/ensemble models on the test sets of AMR15 and AMR17. We can see that our Transformer baseline outperforms most previous single models, and our best single model HetGT additive outperforms the Transformer baseline by a large margin (6.15 BLEU and 6.44 BLEU) on both benchmarks. It demonstrates the importance of incorporating structural information. Meanwhile, HetGT additive gets an improvement of 2.18 and 2.28 BLEU points over the latest SoTA results (Zhu et al., 2019) on AMR15 and AMR17, respectively. Previous models can capture the structural information but most of them ignore heterogeneous information. These results indicate that the heterogeneity in the graph carries lots of useful information for the downstream tasks, and our model can make good use of it.</p>
<p>Results on AMR-to-text Generation</p>
<p>Furthermore, our best single model still has better results than previous ensemble models on both two datasets. Note that additive attention based model HetGT additive is significantly better that dotproduct attention based model HetGT dot-product in AMR-to-text generation. It may be attributed to that the additive attention has less parameters and is easier to train on the small dataset. Table 3 presents the results of our single model and previous single/ensemble models on the test sets for En-De and En-Cs language pairs. We can see that our Transformer baseline already outperforms all previous results even though some of them are Transformer based. It shows the effectiveness of Transformer for NMT tasks. Meanwhile, even without changing the model architecture for the NMT tasks, our single model surpasses Transformer baseline by 2.26 and 1.46 BLEU points on the En-De and En-Cs tasks, respectively, and our model surpasses previous best models by 4.14 and 2.19 BLEU points. In syntax-based NMT where the dataset is larger than AMR-to-text generation, the HetGT dot-product gets comparable results compared to the HetGT additive , and even outperforms the HetGT additive in terms of METEOR and CHRF++ on the language pair En-De. We think on the larger datasets the HetGT dot-product will get better results than the HetGT additive .</p>
<p>Results on Syntax-based NMT</p>
<p>Additional Experiments</p>
<p>Effect of Layer Aggregation Method</p>
<p>Firstly, we compare the performance of three layeraggregation methods discussed in Section 2.3.3.</p>
<p>Method</p>
<p>HetGT  The results are shown in Table 4. We can see the jump connection is the most effective method. However, the dense connection performs the worst. We think the reason is that dense connection introduce lots of extra parameters which are harder to learn.</p>
<p>Effect of Subgraphs</p>
<p>In this section, we also use AMR15 as our benchmark to investigate how each subgraph influences the final results of our best model HetGT additive . Table 5 shows the results of removing or only keeping the specific subgraph. Only keeping the fullyconnected subgraph essentially is what the Transformer baseline does. It means the model does not consider the inherent structural information in inputs. Obviously, it cannot get a good result. In addition, only keeping the connected subgraph does not perform well even it considers the structural information. It demonstrates that the heterogeneous information in the graph is helpful for learning the representation of the graph. When removing any subgraph, the performance of the model will decrease. It demonstrates that each subgraph has contributed to the final results. At last, we remove BPE, and we get 29.84 BLEU score which is still better than previous SoTA that also uses BPE. Note that when we remove the connected subgraph, the results do not have statistically significant changes (p = 0.293). We think the reason is that the left subgraphs already contain the full information of the original graph because the connected subgraph is obtained by grouping all edge types into a single one. Except that, all the other results have statistically significant changes (p ≤ 0.05).</p>
<p>(p / possible-01 e.1 :polarity e.2 -e.2 :ARG1 (w / work-01 e.3,4 :ARG0 (i / i e.0) :location e.5 (h / home e.6)) :ARG1-of (c / cause-01 e.8 :ARG0 (s / shout-01 e.10 :ARG0 (s2 / she e.9) :ARG2 e.11 i e.12))) REF: i can n't do work at home , because she shouts at me . Transformer: i can n't do work at home , because she shouts at me . HetGT additive (ours): i can n't do work at home , because she shouts at me .</p>
<p>(s / say-01 e.1 :ARG0 (h2 / he e.0) :ARG1 e.2 (a / agree-01 e.4 :ARG0 h2 e.3 :ARG1 e.5 (o / opine-01 e.9 :ARG0 e.8 (p2 / person :wiki "Liu Huaqing" :name (n / name :op1 "Huaqing" e.6 :op2 "Liu" e.7)) :ARG1 e.10 (r / recommend-01 e.14 :ARG1 (d / develop-02 e.16 :ARG0 (a2 / and e.12 :op1 (c4 / country :wiki "Thailand" :name (n2 / name :op1 "Thailand" e.11)) :op2 (c5 / country :wiki "China"</p>
<dl>
<dt>:name (n3 / name :op1 "China" e.13))) :ARG1 (a3 / and e.21</dt>
<dd>
<p>:mod f)) :degree (f2 / further e.15)))))) REF: he said that he agreed with huaqing liu 's opinion that thailand and china should further develop various forms of economic and trade cooperation . Transformer: he said huaqing liu agreed to agree with thailand and china should further develop in various forms of economic cooperation and trade cooperation . HetGT additive (ours): he said he agreed to huaqing liu 's opinion that thailand and china should further develop various forms of economic cooperation and trade cooperation . </p>
</dd>
</dl>
<p>Case Study</p>
<p>We perform case studies for better understanding the model performance. We compare the outputs of Transformer baseline and our HetGT additive . The results are presented in Table 6. In the first simple example, our Transformer baseline and HetGT additive can generate the target sequence without mistakes. In the second example which is more complicated, the Transformer baseline fails to identify the possessor of "opinion" and the subject of "agreed" while our model successfully recognizes them. However, we find the there is a common problem: the sentences they generate all have some duplication. We will explore this issue further in the future work.</p>
<p>Related Work</p>
<p>Early researches for Graph2Seq learning tasks are based on statistical methods and neural seq2seq model. Lu et al. (2009) propose an NLG approach built on top of tree conditional random fields to use the tree-structured meaning representation. Song et al. (2017) use synchronous node replacement grammar to generate text. Konstas et al. (2017) linearize the input graph and feed it to the seq2seq model for text-to-AMR parsing and AMR-to-text generation. However, linearizing AMR graphs into sequences may incurs in loss of information. Recent efforts consider to capture the structural information in the encoder. Beck et al. (2018) employ Gated Graph Neural Networks (GGNN) as the encoder and Song et al. (2018) propose the graph-state LSTM to incorporate the graph structure. Their works belong to the family of recurrent neural network (RNN). In addition, there are some works are build upon the GNN. Damonte and Cohen (2019) propose stacking encoders including LSTM and GCN. Guo et al. (2019) introduce the densely connected GCN to encode richer local and non-local information for better graph representation.</p>
<p>Recent studies also extend Transformer to encode structure information. Shaw et al. (2018) propose the relation-aware self-attention which learns explicit embeddings for pair-wise relationships between input elements. Zhu et al. (2019) and Cai and Lam (2020) both extend the relation-aware selfattention to generate text from AMR graph. Our model is also based on Transformer. However, we do not employ the relative position encoding to incorporate structural information. Instead, we directly mask the non-neighbor nodes attention when updating each nodes representation. Moreover, we introduce the heterogeneous information and jump connection to help model learn a better graph representation, bringing substantial gains in the model performance.</p>
<p>Conclusion</p>
<p>In this paper, we propose the Heterogeneous Graph Transformer (HetGT) for Graph2Seq learning. Our proposed heterogeneous mechanism can adaptively model the different representation subgraphs. Experimental results show that HetGT strongly outperforms the state of the art performances on four benchmark datasets of AMR-to-text generation and syntax-based neural machine translation tasks.</p>
<p>There are two directions for future works. One is to investigate how the other graph models can benefit from our proposed heterogeneous mecha-nism. On the other hand, we would also like to investigate how to make use of our proposed model to solve sequence-to-sequence tasks.</p>
<p>Figure 1 :
1(a) An example of AMR graph for the sentence of Here it is a country with the freedom of speech. (b) Its corresponding extended Levi graph with three types of edges. (c) The architecture of HetGT encoder.</p>
<p>Figure 2 :
2An example of graph structure and its extension to subword units.</p>
<p>Figure 3 :
3Different layer aggregation methods: residual (left), jump (middle), dense (right).</p>
<p>Table 1 :
1). Then 1 http://www.statmt.org/wmt16/translation-task.html The statistics of four datasets. The first two are datasets in AMR-to-text generation subtask, the last two are datasets in syntax-based NMT subtask.Dataset 
Train 
Dev 
Test </p>
<p>LDC2015E86 (AMR15) 
16,833 1,368 1,371 
LDC2017T10 (AMR17) 
36,521 1,368 1,371 </p>
<p>English-Czech (En-Cs) 
181,112 2,656 2,999 
English-German (En-De) 226,822 2,169 2,999 </p>
<p>Table 2 :
2Results for AMR-to-text generation on the test sets of AMR15 and AMR17.Model 
English-German 
English-Czech </p>
<p>BLEU CHRF++ METEOR BLEU CHRF++ METEOR </p>
<p>BiRNN+GCN (Bastings et al., 2017) 
16.1 
-
-
9.6 
-
-
GGNN2Seq (Beck et al., 2018) 
16.7 
42.4 
-
9.8 
33.3 
-
DGCN (Guo et al., 2019) 
19.0 
44.1 
-
12.1 
37.1 
-
GTransformer (Cai and Lam, 2020) 
21.3 
47.9 
-
14.1 
41.1 
-</p>
<p>GGNN2Seq ensenmble (Beck et al., 2018) 
19.6 
45.1 
-
11.7 
35.9 
-
DGCN ensemble (Guo et al., 2019) 
20.5 
45.8 
-
13.1 
37.8 
-</p>
<p>Transformer 
23.18 
49.54 
26.00 
14.83 
39.27 
19.12 
HetGT dot-product (ours) 
25.39 
51.55 
27.37 
16.15 
41.10 
20.18 
HetGT additive (ours) 
25.44 
51.27 
27.26 
16.29 
41.14 
20.35 </p>
<p>Table 3 :
3Results for syntax-based NMT on the test sets of En-De and En-Cs.</p>
<p>Table 4 :
4Results of different layer aggregation methods 
on the test set of AMR15. </p>
<p>Table 6 :
6Example outputs of different systems are compared, including Transformer baseline and our HetGT.
AcknowledgmentsThis work was supported by National Natural Science Foundation of China (61772036) and Key Laboratory of Science, Technology and Standard in Press Industry (Key Laboratory of Intelligent Press Media Technology). We thank the anonymous reviewers for their helpful comments. Xiaojun Wan is the corresponding author.
. Chris Alberti, Daniel Andor, Ivan Bogatyy, Michael Collins, Daniel Gillick, Lingpeng Kong, K Terry, Ji Koo, Ma, Mark Omernick, Slav Petrov, Chayut Thanapirom, Zora Tung, and David WeissSyntaxnet models for the conll 2017 shared task. ArXiv, abs/1703.04929Chris Alberti, Daniel Andor, Ivan Bogatyy, Michael Collins, Daniel Gillick, Lingpeng Kong, Terry K Koo, Ji Ma, Mark Omernick, Slav Petrov, Chayut Thanapirom, Zora Tung, and David Weiss. 2017. Syntaxnet models for the conll 2017 shared task. ArXiv, abs/1703.04929.</p>
<p>Graph convolutional encoders for syntax-aware neural machine translation. Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, Khalil Sima&apos;an, 10.18653/v1/D17-1209Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational LinguisticsJoost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil Sima'an. 2017. Graph convolutional encoders for syntax-aware neural ma- chine translation. In Proceedings of the 2017 Con- ference on Empirical Methods in Natural Language Processing, pages 1957-1967, Copenhagen, Den- mark. Association for Computational Linguistics.</p>
<p>Graph-to-sequence learning using gated graph neural networks. Daniel Beck, Gholamreza Haffari, Trevor Cohn, 10.18653/v1/P18-1026Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics1Daniel Beck, Gholamreza Haffari, and Trevor Cohn. 2018. Graph-to-sequence learning using gated graph neural networks. In Proceedings of the 56th Annual Meeting of the Association for Com- putational Linguistics (Volume 1: Long Papers), pages 273-283, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Graph transformer for graph-to-sequence learning. Deng Cai, Wai Lam, Proceedings of The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI). The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI)Deng Cai and Wai Lam. 2020. Graph transformer for graph-to-sequence learning. In Proceedings of The Thirty-Fourth AAAI Conference on Artificial Intelli- gence (AAAI).</p>
<p>Benson Chen, Regina Barzilay, Tommi Jaakkola, arXiv:1905.12712Path-augmented graph transformer network. arXiv preprintBenson Chen, Regina Barzilay, and Tommi Jaakkola. 2019. Path-augmented graph transformer network. arXiv preprint arXiv:1905.12712.</p>
<p>Structural neural encoders for AMR-to-text generation. Marco Damonte, Shay B Cohen, 10.18653/v1/N19-1366Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaLong and Short PapersAssociation for Computational LinguisticsMarco Damonte and Shay B. Cohen. 2019. Structural neural encoders for AMR-to-text generation. In Pro- ceedings of the 2019 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics: Human Language Technologies, Vol- ume 1 (Long and Short Papers), pages 3649-3658, Minneapolis, Minnesota. Association for Computa- tional Linguistics.</p>
<p>Meteor universal: Language specific translation evaluation for any target language. Michael Denkowski, Alon Lavie, 10.3115/v1/W14-3348Proceedings of the Ninth Workshop on Statistical Machine Translation. the Ninth Workshop on Statistical Machine TranslationBaltimore, Maryland, USAAssociation for Computational LinguisticsMichael Denkowski and Alon Lavie. 2014. Meteor uni- versal: Language specific translation evaluation for any target language. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 376-380, Baltimore, Maryland, USA. Association for Computational Linguistics.</p>
<p>Rich feature hierarchies for accurate object detection and semantic segmentation. Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionRoss Girshick, Jeff Donahue, Trevor Darrell, and Jiten- dra Malik. 2014. Rich feature hierarchies for accu- rate object detection and semantic segmentation. In Proceedings of the IEEE conference on computer vi- sion and pattern recognition, pages 580-587.</p>
<p>Better transitionbased AMR parsing with a refined search space. Zhijiang Guo, Wei Lu, 10.18653/v1/D18-1198Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsZhijiang Guo and Wei Lu. 2018. Better transition- based AMR parsing with a refined search space. In Proceedings of the 2018 Conference on Em- pirical Methods in Natural Language Processing, pages 1712-1722, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Densely connected graph convolutional networks for graph-to-sequence learning. Zhijiang Guo, Yan Zhang, Zhiyang Teng, Wei Lu, 10.1162/tacl_a_00269Transactions of the Association for Computational Linguistics. 7Zhijiang Guo, Yan Zhang, Zhiyang Teng, and Wei Lu. 2019. Densely connected graph convolutional networks for graph-to-sequence learning. Transac- tions of the Association for Computational Linguis- tics, 7:297-312.</p>
<p>Identity mappings in deep residual networks. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, European conference on computer vision. SpringerKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Identity mappings in deep residual net- works. In European conference on computer vision, pages 630-645. Springer.</p>
<p>Densely connected convolutional networks. G Huang, Z Liu, L V D Maaten, K Q Weinberger, 10.1109/CVPR.2017.2432017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). G. Huang, Z. Liu, L. v. d. Maaten, and K. Q. Wein- berger. 2017. Densely connected convolutional net- works. In 2017 IEEE Conference on Computer Vi- sion and Pattern Recognition (CVPR), pages 2261- 2269.</p>
<p>OpenNMT: Open-source toolkit for neural machine translation. Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, Alexander M Rush, 10.18653/v1/P17-4012Proc. ACL. ACLGuillaume Klein, Yoon Kim, Yuntian Deng, Jean Senel- lart, and Alexander M. Rush. 2017. OpenNMT: Open-source toolkit for neural machine translation. In Proc. ACL.</p>
<p>Neural AMR: Sequence-to-sequence models for parsing and generation. Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, Luke Zettlemoyer, 10.18653/v1/P17-1014Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational LinguisticsLong Papers)Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, and Luke Zettlemoyer. 2017. Neural AMR: Sequence-to-sequence models for parsing and gener- ation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 146-157, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Natural language generation with tree conditional random fields. Wei Lu, Wee Sun Hwee Tou Ng, Lee, Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing. the 2009 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsWei Lu, Hwee Tou Ng, and Wee Sun Lee. 2009. Nat- ural language generation with tree conditional ran- dom fields. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Pro- cessing, pages 400-409, Singapore. Association for Computational Linguistics.</p>
<p>compare-mt: A tool for holistic comparison of language generation systems. Graham Neubig, Zi-Yi Dou, Junjie Hu, Paul Michel, Danish Pruthi, Xinyi Wang, John Wieting, abs/1903.07926CoRR. Graham Neubig, Zi-Yi Dou, Junjie Hu, Paul Michel, Danish Pruthi, Xinyi Wang, and John Wieting. 2019. compare-mt: A tool for holistic comparison of lan- guage generation systems. CoRR, abs/1903.07926.</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational LinguisticsKishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Com- putational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.</p>
<p>chrF: character n-gram f-score for automatic MT evaluation. Maja Popović, 10.18653/v1/W15-3049Proceedings of the Tenth Workshop on Statistical Machine Translation. the Tenth Workshop on Statistical Machine TranslationLisbon, PortugalAssociation for Computational LinguisticsMaja Popović. 2015. chrF: character n-gram f-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392-395, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Parsing English into abstract meaning representation using syntaxbased machine translation. Michael Pust, Ulf Hermjakob, Kevin Knight, Daniel Marcu, Jonathan , 10.18653/v1/D15-1136Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational LinguisticsMichael Pust, Ulf Hermjakob, Kevin Knight, Daniel Marcu, and Jonathan May. 2015. Parsing English into abstract meaning representation using syntax- based machine translation. In Proceedings of the 2015 Conference on Empirical Methods in Natu- ral Language Processing, pages 1143-1154, Lis- bon, Portugal. Association for Computational Lin- guistics.</p>
<p>Enhancing AMR-to-text generation with dual graph representations. F R Leonardo, Claire Ribeiro, Iryna Gardent, Gurevych, 10.18653/v1/D19-1314Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsLeonardo F. R. Ribeiro, Claire Gardent, and Iryna Gurevych. 2019. Enhancing AMR-to-text genera- tion with dual graph representations. In Proceed- ings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter- national Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), pages 3181-3192, Hong Kong, China. Association for Computational Lin- guistics.</p>
<p>Neural machine translation of rare words with subword units. Rico Sennrich, Barry Haddow, Alexandra Birch, 10.18653/v1/P16-1162Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyLong Papers1Association for Computational LinguisticsRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715- 1725, Berlin, Germany. Association for Computa- tional Linguistics.</p>
<p>Self-attention with relative position representations. Peter Shaw, Jakob Uszkoreit, Ashish Vaswani, 10.18653/v1/N18-2074Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, Louisiana2Short Papers. Association for Computational LinguisticsPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position represen- tations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 2 (Short Papers), pages 464-468, New Orleans, Louisiana. Association for Computa- tional Linguistics.</p>
<p>AMR-to-text generation with synchronous node replacement grammar. Linfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo Wang, Daniel Gildea, 10.18653/v1/P17-2002Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics2Short Papers)Linfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2017. AMR-to-text gener- ation with synchronous node replacement grammar. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 7-13, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>A graph-to-sequence model for AMRto-text generation. Linfeng Song, Yue Zhang, Zhiguo Wang, Daniel Gildea, 10.18653/v1/P18-1150Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaLong Papers1Association for Computational LinguisticsLinfeng Song, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2018. A graph-to-sequence model for AMR- to-text generation. In Proceedings of the 56th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1616- 1626, Melbourne, Australia. Association for Compu- tational Linguistics.</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Illia Kaiser, Polosukhin, Advances in Neural Information Processing Systems. Curran Associates, Inc30Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems 30, pages 5998-6008. Curran Asso- ciates, Inc.</p>
<p>Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio, Graph Attention Networks. International Conference on Learning Representations. Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2018. Graph Attention Networks. International Conference on Learning Representations.</p>
<p>Representation learning on graphs with jumping knowledge networks. Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken Ichi Kawarabayashi, Stefanie Jegelka, Proceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine LearningKeyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken ichi Kawarabayashi, and Stefanie Jegelka. 2018a. Representation learning on graphs with jumping knowledge networks. In Proceedings of the 35th International Conference on Machine Learning.</p>
<p>Graph2seq: Graph to sequence learning with attention-based neural networks. Kun Xu, Lingfei Wu, Zhiguo Wang, Yansong Feng, Michael Witbrock, Vadim Sheinin, arXiv:1804.00823arXiv preprintKun Xu, Lingfei Wu, Zhiguo Wang, Yansong Feng, Michael Witbrock, and Vadim Sheinin. 2018b. Graph2seq: Graph to sequence learning with attention-based neural networks. arXiv preprint arXiv:1804.00823.</p>
<p>Modeling graph structure in transformer for better AMR-to-text generation. Jie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min Zhang, Guodong Zhou, 10.18653/v1/D19-1548Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsJie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min Zhang, and Guodong Zhou. 2019. Modeling graph structure in transformer for better AMR-to-text gen- eration. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP), pages 5458-5467, Hong Kong, China. Association for Computational Linguistics.</p>            </div>
        </div>

    </div>
</body>
</html>
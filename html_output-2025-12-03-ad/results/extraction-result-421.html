<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-421 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-421</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-421</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-16.html">extraction-schema-16</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <p><strong>Paper ID:</strong> paper-267627077</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.06784v2.pdf" target="_blank">Transfer learning with generative models for object detection on limited datasets</a></p>
                <p><strong>Paper Abstract:</strong> The availability of data is limited in some fields, especially for object detection tasks, where it is necessary to have correctly labeled bounding boxes around each object. A notable example of such data scarcity is found in the domain of marine biology, where it is useful to develop methods to automatically detect submarine species for environmental monitoring. To address this data limitation, the state-of-the-art machine learning strategies employ two main approaches. The first involves pretraining models on existing datasets before generalizing to the specific domain of interest. The second strategy is to create synthetic datasets specifically tailored to the target domain using methods like copy-paste techniques or ad-hoc simulators. The first strategy often faces a significant domain shift, while the second demands custom solutions crafted for the specific task. In response to these challenges, here we propose a transfer learning framework that is valid for a generic scenario. In this framework, generated images help to improve the performances of an object detector in a few-real data regime. This is achieved through a diffusion-based generative model that was pretrained on large generic datasets. With respect to the state-of-the-art, we find that it is not necessary to fine tune the generative model on the specific domain of interest. We believe that this is an important advance because it mitigates the labor-intensive task of manual labeling the images in object detection tasks. We validate our approach focusing on fishes in an underwater environment, and on the more common domain of cars in an urban setting. Our method achieves detection performance comparable to models trained on thousands of images, using only a few hundreds of input data. Our results pave the way for new generative AI-based protocols for machine learning applications in various domains, for instance ranging from geophysics to biology and medicine.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e421.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e421.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GLIGEN L2I transfer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GLIGEN (Grounded Layout-to-Image generative model) applied to new domains</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of a pretrained Layout-to-Image generative model (GLIGEN) trained on large web image-text data to generate labeled images (with objects placed inside specified bounding boxes) for pretraining object detectors in new target domains (urban cars, underwater fishes) without fine-tuning the generator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gligen: Open-set grounded text-to-image generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Layout-to-Image generative model transfer (GLIGEN)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>A pretrained L2I diffusion-based generative model (GLIGEN), which leverages CLIP for image-text grounding, is supplied with an instruction comprising a text prompt and a grounding instruction (list of grounding entities plus bounding-box coordinates). The generator creates photorealistic images with targets placed within those boxes. For style consistency, a real image can be supplied as a grounding entity to transfer background appearance. The generated images inherit automatic bounding-box labels from the grounding instruction and are used as a source dataset for detector pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / data synthesis (generative model)</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>large-scale web image-text generative modeling / general computer vision (image-text pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>object detection in domain-limited contexts (urban vehicle detection, marine biology fish detection / ecology)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application without fine-tuning (pretrained GLIGEN applied to new domains)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>No fine-tuning of GLIGEN on target-domain images; grounding instructions were constructed by (1) collecting bounding-box statistics from a small labeled real subset (means and variances for positions/sizes/counts), (2) sampling bounding-box parameters from Gaussian distributions to build plausible ground-truth layouts, (3) providing either text phrases (e.g., 'a fish' / 'a car') or reference target images as grounding entities and one real image to condition background style. Prompt engineering was minimal (simple descriptive prompts). Small targets (<0.2% image area) were excluded.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful — generated datasets materially improved downstream detector performance: e.g., NuImages: 300 real + 9,000 generated ≈ performance of full 4,500 real training; 300 real + 750 filtered generated > 1,500 real. OzFish: 300 real + 9,000 generated approached COCO-pretrained baseline (~0.6 mAP) and in some settings surpassed COCO reference when combined with additional generated data.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Generator sometimes fails to respect layout constraints (objects drawn outside bounding boxes), struggles with overlapping or very small objects, and may produce labeling inconsistencies; generation quality varies (background vs target fidelity), and FID did not always predict downstream detection utility.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>GLIGEN's training on large/diverse web data and CLIP-based grounding enables representation of concepts not seen in domain-specific training; ability to pass a reference image for style transfer; automatic bounding-box labels from grounding instructions; ease of producing many labeled images without manual bounding-box annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to pretrained GLIGEN and CLIP weights, compute resources for generation (GPUs), a small set of labeled real images to estimate bounding-box statistics and to provide style-transfer images, and prompt/grounding instruction construction.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Authors argue high generalizability — framework can, in principle, be applied to other domains (medicine, geophysics, materials science) though for very niche domains they suggest possibly fine-tuning CLIP; validated across two very different target domains (urban cars and underwater fishes) and across multiple detector architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>combination of explicit procedural steps (how to construct grounding instructions, sampling bounding-box distributions, and training pipeline) and instrumental/technical knowledge (usage of GLIGEN/CLIP, generation settings and style-transfer inputs).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer learning with generative models for object detection on limited datasets', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e421.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e421.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generated-data pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Two-step transfer learning: pretraining object detector on generated images then fine-tuning on limited real images</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training protocol where an object detector is first pretrained on large quantities of images synthetically generated (with bounding-box labels) by a pretrained generative model, then fine-tuned on a small set of real labeled images to adapt to the real domain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Pretraining on generated images then fine-tuning on real images (two-step transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Stage 1: Pretrain an object detector (Faster R-CNN with frozen ImageNet-pretrained ResNet-50 backbone) on a large generated dataset (labels come from the generator's grounding instructions). Validation set is taken from generated images; use SGD with momentum, learning rate scheduling and early stopping. Stage 2: Fine-tune the detector (RPN and head weights) on a small real dataset using a 1x scheduler (fixed 12 epochs) and stronger regularization (higher weight decay). Optionally apply dataset filters (Precision-Recall or FID) on generated images before pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / training protocol (transfer learning)</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>synthetic/generated image domain (computer vision / generative models)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>real-world object detection tasks (urban vehicle detection; underwater fish detection in ecology/marine biology)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (pretraining on synthetic labels, freeze backbone, specific schedulers/regularization for small real fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Backbone (ResNet-50) frozen (pretrained on ImageNet); RPN and detection heads randomly initialized; different weight decay during pretraining (0.001) and fine-tuning (0.01); use of validation-based scheduler during pretraining (reduce LR on plateau) and 1x fixed scheduler during fine-tuning; early stopping on validation loss during pretraining; generated-ground-truth bounding boxes sampled statistically from small real subset; optional filtering of generated images prior to pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful — quantitative outcomes reported: NuImages: combining 300 real images with 9,000 generated images produced mAP comparable to training on 4,500 real images; 300 real + 750 filtered generated images outperformed training with 1,500 real images. Training only on 9,000 generated images (no fine-tuning) achieved mAP similar to ~1,500 real images. OzFish: pretraining on 9,000 generated (image-grounded) + fine-tuning on 300 real approached COCO-pretrained reference (~0.6 mAP) and in some settings surpassed it with modest generated support.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Domain shift between generated and real images can limit transfer; mislabeled or geometrically inconsistent generated images (targets outside boxes) hamper pretraining, especially when generated dataset is small; generator weaknesses with overlapped objects and small targets; FID does not reliably predict detector performance in all cases.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>High quality of target appearance in generated images (especially copy‑paste masks), GLIGEN's generalization, frozen backbone that leverages ImageNet features, availability of a small labeled real set to sample bounding-box statistics, and filters (Precision-Recall) that exclude badly generated samples improved low-data performance.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Compute clusters / GPUs for both generation and detector training, access to generative model and detector code, a small annotated real subset (used both for bounding-box statistics and for fine-tuning), and the ability to run validation for scheduler/early stopping.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Validated across two distinct domains and three detectors (Faster R-CNN with different backbones and FCOS); authors state the approach is broadly applicable to other domains but recommend CLIP fine-tuning for highly niche domains (e.g., specific medical imaging).</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps (training schedules, hyperparameters, freezing strategy) and instrumental/technical knowledge (how to integrate generated data into detector pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer learning with generative models for object detection on limited datasets', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e421.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e421.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Copy-Paste synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Copy-paste synthetic image generation from object masks (cut-and-paste)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthetic-data construction technique that pastes segmented object masks (from a dataset with semantic segmentations) onto real background images to create labeled training images with accurate bounding boxes/masks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cut, paste and learn: Surprisingly easy synthesis for instance detection.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Copy-paste (mask-based) synthetic data generation</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Obtain semantic masks of targets (here fish masks from DeepFish); paste masks onto selected background images (from the same or related domain) at desired bounding-box positions; resulting composite images carry accurate object masks and bounding boxes, producing perfectly labeled synthetic training examples for object detectors.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>data synthesis technique / computational method</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>graphics-based synthetic data generation and dataset augmentation (computer vision / image editing)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>training data creation for object detection in underwater ecology (OzFish) and other vision tasks</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application (copying segmented objects into new backgrounds without generator)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>In this study, authors used 207 masks from DeepFish (≈10 species) and pasted them on 128 background images (also used for style-transfer in generative approaches) assigning identical bounding-box positions as in other synthetic approaches to keep comparisons fair.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful — among the tested synthetic-data strategies it yielded the best detection performance on OzFish (highest mAP) and the lowest FID (≈70), indicating the closest feature distribution to real OzFish images. However, it is the most laborious method due to the need for segmentation masks.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>High human cost to produce semantic masks; mask collection/segmentation is resource-intensive and may be as (or more) expensive than annotating bounding boxes for many targets.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of segmentation-labeled datasets (DeepFish) and realistic background images; exact mask-based placement yields high-fidelity target appearance and perfect labels.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Semantic segmentation masks for targets, background images for placement, and tooling to compose images; careful management to avoid unrealistic composites (lighting/scale mismatch).</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Widely applicable in computer vision where segmentation masks exist or can be produced, but cost scales with mask annotation effort; effective when realistic object appearance is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>instrumental/technical skills (image compositing) and explicit procedural steps (mask selection and placement).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer learning with generative models for object detection on limited datasets', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e421.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e421.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Precision-Recall filter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Precision-Recall based filtering of generated images using an auxiliary detector</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-curation procedure that uses a pretrained/fine-tuned object detector to evaluate generated images against their intended grounding bounding boxes and discards generated images that fail strict precision and recall thresholds (e.g., threshold = 1).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Precision-Recall per-image filter using an object detector</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Run an auxiliary Faster R-CNN (initialized with COCO weights and possibly fine-tuned on limited real data) on each generated image. For every intended grounding bounding box, compute IoU with detector predictions to obtain per-image precision and recall. Retain only images meeting chosen thresholds (authors use precision = 1 and recall = 1 in some experiments) to ensure objects were generated inside the assigned boxes and that no intended objects are missing. The filtered subset is then used to pretrain the detector.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>data curation / validation technique</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>object detection evaluation (precision/recall/IoU) and model-based data filtering</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>selection of synthetic/generated images for pretraining object detectors in target domains</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted application (use detection evaluation metrics as a filter on generative model outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Thresholds set to 1 for both precision and recall in key experiments; the auxiliary detector used for filtering was a Faster R-CNN initialized with COCO weights and optionally fine-tuned on the limited real dataset; filtering removed ~56% of generated images in NuImages experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful in low-data regimes — filtering improved performance markedly when the number of generated images was small (e.g., 300 real + 750 filtered generated outperformed 1,500 real). For large generated datasets (≥12k), filtered and unfiltered pretraining converged to similar mAPs, indicating diminishing returns of filtering with ample generated data.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Requires an auxiliary detector (and possibly labeled real data) to run the filter; filtering discards many images (reducing data quantity); potential for bias introduced by the auxiliary detector's failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of a reasonably performing auxiliary detector (COCO-pretrained), and the fact that generator outputs can be quantitatively vetted against intended ground-truth boxes.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Compute resources to run detection inference on all generated images and a pretrained/fine-tuned auxiliary detector (COCO initialization recommended).</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Likely generalizable to other domains and generative models where intended bounding-box annotations exist for generated samples; particularly useful when generated labels may be unreliable and data volumes are limited.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and instrumental skills (implementing per-image detection and thresholding pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer learning with generative models for object detection on limited datasets', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e421.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e421.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FID filter / FID use</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Frechet Inception Distance (FID) used as a filter for generated training images</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of the Frechet Inception Distance (computed on Inception-V3 features) to quantify distributional similarity between generated and real images and iteratively remove generated images that increase the FID.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gans trained by a two time-scale update rule converge to a local nash equilibrium.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>FID-based filtering (iterative removal to reduce dataset FID)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Compute Inception-V3 features for real and generated datasets, fit multivariate Gaussians, compute FID. Iteratively remove generated images that, when excluded, yield a lower FID relative to the real dataset, retaining only images that overall reduce distributional distance. Use retained generated images for pretraining detectors.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>data analysis metric / filtering technique</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>generative model evaluation (computer vision)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>selection of synthetic datasets for object detector pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application of a generative-model metric to data curation for detection</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Applied iterative per-image removal: compute FID after removing each candidate and permanently remove it if FID decreases. Used Inception-V3 pretrained on ImageNet for feature extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful — in NuImages experiments, three generated subsets with different FID values (47, 105, 148) produced very similar downstream detection mAPs, indicating that FID reduction did not reliably translate to better detector performance. Authors conclude FID measures global image distribution similarity (background + targets) and may not predict target-specific downstream utility for detectors.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>FID is insensitive to target-specific label correctness; it can penalize background variations that are irrelevant for detector training and can fail to detect mislabeled/generated targets that harm detector learning.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of pretrained Inception-V3 and established FID computation code; FID remains useful as a general image-quality/diversity metric.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Compute resources for feature extraction and iterative re-computation of FID; real validation set for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Useful for global image-distribution quality assessment across domains, but authors caution its direct use as a filter for downstream detection tasks — task-specific filters (e.g., Precision-Recall) may be preferable.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>theoretical metric application and explicit procedural steps for dataset curation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer learning with generative models for object detection on limited datasets', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e421.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e421.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ImageNet→ResNet-50 transfer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Use of ResNet-50 pretrained on ImageNet as a frozen backbone for object detection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transfer of convolutional feature extractor weights pretrained on ImageNet classification (ResNet-50) into an object detection pipeline (Faster R-CNN), with the backbone frozen during detector training to reduce dependence on detection-labeled data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep residual learning for image recognition.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Pretrained classification backbone transfer (ImageNet ResNet-50 frozen)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Adopt ResNet-50 pretrained on ImageNet as the feature-extractor backbone inside Faster R-CNN; omit final classification head, feed intermediate residual block outputs into an FPN, and freeze backbone weights (no gradient updates) while training RPN and detection heads on synthetic and/or limited real data. This reduces need for large labeled detection datasets and prevents overfitting to synthetic labeling noise.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / transfer learning protocol</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>image classification (ImageNet supervised learning)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>object detection model training (computer vision detection tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application with adaptation (freezing backbone and integrating into detection architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Backbone weights (≈23.45M parameters) frozen; RPN and detection heads randomly initialized and trained; integrated with Feature Pyramid Network as standard for Faster R-CNN; used different weight decay during pretraining and fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful — freezing the backbone was reported to improve results relative to training entire Faster R-CNN on synthetic images (cited prior work); in the present study, it is a key design choice enabling effective use of generated pretraining and small real fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Potential mismatch between classification features and fine-grained detection needs, but freezing mitigates overfitting to noisy synthetic labels; requires high-quality pretrained weights.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Proven generality of ImageNet features for downstream vision tasks and reduced data requirements when freezing backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to ImageNet-pretrained ResNet-50 weights and integration into Faster R-CNN/FPN pipeline; compute resources to train detection heads.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Widely generalizable — standard practice across many detection tasks; validated here across urban and underwater domains and other detector variants.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and instrumental/technical know-how (model integration and training regimen).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer learning with generative models for object detection on limited datasets', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e421.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e421.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP representation transfer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP (Contrastive image–language pretraining) used to ground generated entities</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The CLIP model pretrained on large image–text pairs provides image and text embeddings that GLIGEN uses to represent grounding entities (text phrases or reference images), enabling open-set grounding and transfer of concept representations to the generative process for unseen target types.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning transferable visual models from natural language supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>CLIP-based image-text representation transfer</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>CLIP encodes grounding entities (either text phrases like 'a fish' or reference images) into a shared embedding space; GLIGEN uses these embeddings to condition generation so that specified entities appear within designated bounding boxes. This transfers the broad, open-set semantic knowledge from CLIP's web-scale pretraining into the constrained L2I generation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>representational transfer / computational method</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>large-scale contrastive image-text pretraining (multimodal ML)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>layout-to-image generation for object detection dataset synthesis (computer vision)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct integration (CLIP encodings used by GLIGEN to ground generation)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Authors rely on GLIGEN's integration of CLIP and do not fine-tune CLIP on the target domains; they optionally provide image-grounding entities (real examples) or text phrases encoded by CLIP.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful — CLIP-enabled GLIGEN to represent and generate object concepts not explicitly present in the generator's training data, facilitating generation for both cars and various fish species; authors suggest CLIP may need fine-tuning for very niche domains.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>For highly specialized domains (e.g., certain medical images) CLIP's general representations may be insufficient and could require fine-tuning; CLIP-based grounding does not solve generator geometric/layout failures.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>CLIP's broad coverage and robust multimodal embeddings enable zero-shot grounding of textual or visual entity descriptors.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to CLIP weights and the GLIGEN architecture that consumes CLIP embeddings; prompts or reference images for grounding entities.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High within vision-language tasks; authors explicitly note CLIP/GLIGEN separation suggests applicability to many niche domains though possibly requiring CLIP fine-tuning for specialized imagery.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>theoretical principles (contrastive multimodal representations) and explicit procedural/technical integration into generation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer learning with generative models for object detection on limited datasets', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gligen: Open-set grounded text-to-image generation. <em>(Rating: 2)</em></li>
                <li>Cut, paste and learn: Surprisingly easy synthesis for instance detection. <em>(Rating: 2)</em></li>
                <li>Training deep networks with synthetic data: Bridging the reality gap by domain randomization. <em>(Rating: 2)</em></li>
                <li>Learning transferable visual models from natural language supervision. <em>(Rating: 2)</em></li>
                <li>Gans trained by a two time-scale update rule converge to a local nash equilibrium. <em>(Rating: 1)</em></li>
                <li>Integrating geometric control into text-to-image diffusion models for high-quality detection data generation via text prompt. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-421",
    "paper_id": "paper-267627077",
    "extraction_schema_id": "extraction-schema-16",
    "extracted_data": [
        {
            "name_short": "GLIGEN L2I transfer",
            "name_full": "GLIGEN (Grounded Layout-to-Image generative model) applied to new domains",
            "brief_description": "Use of a pretrained Layout-to-Image generative model (GLIGEN) trained on large web image-text data to generate labeled images (with objects placed inside specified bounding boxes) for pretraining object detectors in new target domains (urban cars, underwater fishes) without fine-tuning the generator.",
            "citation_title": "Gligen: Open-set grounded text-to-image generation.",
            "mention_or_use": "use",
            "procedure_name": "Layout-to-Image generative model transfer (GLIGEN)",
            "procedure_description": "A pretrained L2I diffusion-based generative model (GLIGEN), which leverages CLIP for image-text grounding, is supplied with an instruction comprising a text prompt and a grounding instruction (list of grounding entities plus bounding-box coordinates). The generator creates photorealistic images with targets placed within those boxes. For style consistency, a real image can be supplied as a grounding entity to transfer background appearance. The generated images inherit automatic bounding-box labels from the grounding instruction and are used as a source dataset for detector pretraining.",
            "procedure_type": "computational method / data synthesis (generative model)",
            "source_domain": "large-scale web image-text generative modeling / general computer vision (image-text pretraining)",
            "target_domain": "object detection in domain-limited contexts (urban vehicle detection, marine biology fish detection / ecology)",
            "transfer_type": "direct application without fine-tuning (pretrained GLIGEN applied to new domains)",
            "modifications_made": "No fine-tuning of GLIGEN on target-domain images; grounding instructions were constructed by (1) collecting bounding-box statistics from a small labeled real subset (means and variances for positions/sizes/counts), (2) sampling bounding-box parameters from Gaussian distributions to build plausible ground-truth layouts, (3) providing either text phrases (e.g., 'a fish' / 'a car') or reference target images as grounding entities and one real image to condition background style. Prompt engineering was minimal (simple descriptive prompts). Small targets (&lt;0.2% image area) were excluded.",
            "transfer_success": "successful — generated datasets materially improved downstream detector performance: e.g., NuImages: 300 real + 9,000 generated ≈ performance of full 4,500 real training; 300 real + 750 filtered generated &gt; 1,500 real. OzFish: 300 real + 9,000 generated approached COCO-pretrained baseline (~0.6 mAP) and in some settings surpassed COCO reference when combined with additional generated data.",
            "barriers_encountered": "Generator sometimes fails to respect layout constraints (objects drawn outside bounding boxes), struggles with overlapping or very small objects, and may produce labeling inconsistencies; generation quality varies (background vs target fidelity), and FID did not always predict downstream detection utility.",
            "facilitating_factors": "GLIGEN's training on large/diverse web data and CLIP-based grounding enables representation of concepts not seen in domain-specific training; ability to pass a reference image for style transfer; automatic bounding-box labels from grounding instructions; ease of producing many labeled images without manual bounding-box annotation.",
            "contextual_requirements": "Access to pretrained GLIGEN and CLIP weights, compute resources for generation (GPUs), a small set of labeled real images to estimate bounding-box statistics and to provide style-transfer images, and prompt/grounding instruction construction.",
            "generalizability": "Authors argue high generalizability — framework can, in principle, be applied to other domains (medicine, geophysics, materials science) though for very niche domains they suggest possibly fine-tuning CLIP; validated across two very different target domains (urban cars and underwater fishes) and across multiple detector architectures.",
            "knowledge_type": "combination of explicit procedural steps (how to construct grounding instructions, sampling bounding-box distributions, and training pipeline) and instrumental/technical knowledge (usage of GLIGEN/CLIP, generation settings and style-transfer inputs).",
            "uuid": "e421.0",
            "source_info": {
                "paper_title": "Transfer learning with generative models for object detection on limited datasets",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Generated-data pretraining",
            "name_full": "Two-step transfer learning: pretraining object detector on generated images then fine-tuning on limited real images",
            "brief_description": "A training protocol where an object detector is first pretrained on large quantities of images synthetically generated (with bounding-box labels) by a pretrained generative model, then fine-tuned on a small set of real labeled images to adapt to the real domain.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "Pretraining on generated images then fine-tuning on real images (two-step transfer)",
            "procedure_description": "Stage 1: Pretrain an object detector (Faster R-CNN with frozen ImageNet-pretrained ResNet-50 backbone) on a large generated dataset (labels come from the generator's grounding instructions). Validation set is taken from generated images; use SGD with momentum, learning rate scheduling and early stopping. Stage 2: Fine-tune the detector (RPN and head weights) on a small real dataset using a 1x scheduler (fixed 12 epochs) and stronger regularization (higher weight decay). Optionally apply dataset filters (Precision-Recall or FID) on generated images before pretraining.",
            "procedure_type": "computational method / training protocol (transfer learning)",
            "source_domain": "synthetic/generated image domain (computer vision / generative models)",
            "target_domain": "real-world object detection tasks (urban vehicle detection; underwater fish detection in ecology/marine biology)",
            "transfer_type": "adapted/modified for new context (pretraining on synthetic labels, freeze backbone, specific schedulers/regularization for small real fine-tuning)",
            "modifications_made": "Backbone (ResNet-50) frozen (pretrained on ImageNet); RPN and detection heads randomly initialized; different weight decay during pretraining (0.001) and fine-tuning (0.01); use of validation-based scheduler during pretraining (reduce LR on plateau) and 1x fixed scheduler during fine-tuning; early stopping on validation loss during pretraining; generated-ground-truth bounding boxes sampled statistically from small real subset; optional filtering of generated images prior to pretraining.",
            "transfer_success": "successful — quantitative outcomes reported: NuImages: combining 300 real images with 9,000 generated images produced mAP comparable to training on 4,500 real images; 300 real + 750 filtered generated images outperformed training with 1,500 real images. Training only on 9,000 generated images (no fine-tuning) achieved mAP similar to ~1,500 real images. OzFish: pretraining on 9,000 generated (image-grounded) + fine-tuning on 300 real approached COCO-pretrained reference (~0.6 mAP) and in some settings surpassed it with modest generated support.",
            "barriers_encountered": "Domain shift between generated and real images can limit transfer; mislabeled or geometrically inconsistent generated images (targets outside boxes) hamper pretraining, especially when generated dataset is small; generator weaknesses with overlapped objects and small targets; FID does not reliably predict detector performance in all cases.",
            "facilitating_factors": "High quality of target appearance in generated images (especially copy‑paste masks), GLIGEN's generalization, frozen backbone that leverages ImageNet features, availability of a small labeled real set to sample bounding-box statistics, and filters (Precision-Recall) that exclude badly generated samples improved low-data performance.",
            "contextual_requirements": "Compute clusters / GPUs for both generation and detector training, access to generative model and detector code, a small annotated real subset (used both for bounding-box statistics and for fine-tuning), and the ability to run validation for scheduler/early stopping.",
            "generalizability": "Validated across two distinct domains and three detectors (Faster R-CNN with different backbones and FCOS); authors state the approach is broadly applicable to other domains but recommend CLIP fine-tuning for highly niche domains (e.g., specific medical imaging).",
            "knowledge_type": "explicit procedural steps (training schedules, hyperparameters, freezing strategy) and instrumental/technical knowledge (how to integrate generated data into detector pretraining).",
            "uuid": "e421.1",
            "source_info": {
                "paper_title": "Transfer learning with generative models for object detection on limited datasets",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Copy-Paste synthesis",
            "name_full": "Copy-paste synthetic image generation from object masks (cut-and-paste)",
            "brief_description": "A synthetic-data construction technique that pastes segmented object masks (from a dataset with semantic segmentations) onto real background images to create labeled training images with accurate bounding boxes/masks.",
            "citation_title": "Cut, paste and learn: Surprisingly easy synthesis for instance detection.",
            "mention_or_use": "use",
            "procedure_name": "Copy-paste (mask-based) synthetic data generation",
            "procedure_description": "Obtain semantic masks of targets (here fish masks from DeepFish); paste masks onto selected background images (from the same or related domain) at desired bounding-box positions; resulting composite images carry accurate object masks and bounding boxes, producing perfectly labeled synthetic training examples for object detectors.",
            "procedure_type": "data synthesis technique / computational method",
            "source_domain": "graphics-based synthetic data generation and dataset augmentation (computer vision / image editing)",
            "target_domain": "training data creation for object detection in underwater ecology (OzFish) and other vision tasks",
            "transfer_type": "direct application (copying segmented objects into new backgrounds without generator)",
            "modifications_made": "In this study, authors used 207 masks from DeepFish (≈10 species) and pasted them on 128 background images (also used for style-transfer in generative approaches) assigning identical bounding-box positions as in other synthetic approaches to keep comparisons fair.",
            "transfer_success": "successful — among the tested synthetic-data strategies it yielded the best detection performance on OzFish (highest mAP) and the lowest FID (≈70), indicating the closest feature distribution to real OzFish images. However, it is the most laborious method due to the need for segmentation masks.",
            "barriers_encountered": "High human cost to produce semantic masks; mask collection/segmentation is resource-intensive and may be as (or more) expensive than annotating bounding boxes for many targets.",
            "facilitating_factors": "Availability of segmentation-labeled datasets (DeepFish) and realistic background images; exact mask-based placement yields high-fidelity target appearance and perfect labels.",
            "contextual_requirements": "Semantic segmentation masks for targets, background images for placement, and tooling to compose images; careful management to avoid unrealistic composites (lighting/scale mismatch).",
            "generalizability": "Widely applicable in computer vision where segmentation masks exist or can be produced, but cost scales with mask annotation effort; effective when realistic object appearance is critical.",
            "knowledge_type": "instrumental/technical skills (image compositing) and explicit procedural steps (mask selection and placement).",
            "uuid": "e421.2",
            "source_info": {
                "paper_title": "Transfer learning with generative models for object detection on limited datasets",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Precision-Recall filter",
            "name_full": "Precision-Recall based filtering of generated images using an auxiliary detector",
            "brief_description": "A data-curation procedure that uses a pretrained/fine-tuned object detector to evaluate generated images against their intended grounding bounding boxes and discards generated images that fail strict precision and recall thresholds (e.g., threshold = 1).",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "Precision-Recall per-image filter using an object detector",
            "procedure_description": "Run an auxiliary Faster R-CNN (initialized with COCO weights and possibly fine-tuned on limited real data) on each generated image. For every intended grounding bounding box, compute IoU with detector predictions to obtain per-image precision and recall. Retain only images meeting chosen thresholds (authors use precision = 1 and recall = 1 in some experiments) to ensure objects were generated inside the assigned boxes and that no intended objects are missing. The filtered subset is then used to pretrain the detector.",
            "procedure_type": "data curation / validation technique",
            "source_domain": "object detection evaluation (precision/recall/IoU) and model-based data filtering",
            "target_domain": "selection of synthetic/generated images for pretraining object detectors in target domains",
            "transfer_type": "adapted application (use detection evaluation metrics as a filter on generative model outputs)",
            "modifications_made": "Thresholds set to 1 for both precision and recall in key experiments; the auxiliary detector used for filtering was a Faster R-CNN initialized with COCO weights and optionally fine-tuned on the limited real dataset; filtering removed ~56% of generated images in NuImages experiments.",
            "transfer_success": "successful in low-data regimes — filtering improved performance markedly when the number of generated images was small (e.g., 300 real + 750 filtered generated outperformed 1,500 real). For large generated datasets (≥12k), filtered and unfiltered pretraining converged to similar mAPs, indicating diminishing returns of filtering with ample generated data.",
            "barriers_encountered": "Requires an auxiliary detector (and possibly labeled real data) to run the filter; filtering discards many images (reducing data quantity); potential for bias introduced by the auxiliary detector's failure modes.",
            "facilitating_factors": "Availability of a reasonably performing auxiliary detector (COCO-pretrained), and the fact that generator outputs can be quantitatively vetted against intended ground-truth boxes.",
            "contextual_requirements": "Compute resources to run detection inference on all generated images and a pretrained/fine-tuned auxiliary detector (COCO initialization recommended).",
            "generalizability": "Likely generalizable to other domains and generative models where intended bounding-box annotations exist for generated samples; particularly useful when generated labels may be unreliable and data volumes are limited.",
            "knowledge_type": "explicit procedural steps and instrumental skills (implementing per-image detection and thresholding pipeline).",
            "uuid": "e421.3",
            "source_info": {
                "paper_title": "Transfer learning with generative models for object detection on limited datasets",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "FID filter / FID use",
            "name_full": "Frechet Inception Distance (FID) used as a filter for generated training images",
            "brief_description": "Use of the Frechet Inception Distance (computed on Inception-V3 features) to quantify distributional similarity between generated and real images and iteratively remove generated images that increase the FID.",
            "citation_title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium.",
            "mention_or_use": "use",
            "procedure_name": "FID-based filtering (iterative removal to reduce dataset FID)",
            "procedure_description": "Compute Inception-V3 features for real and generated datasets, fit multivariate Gaussians, compute FID. Iteratively remove generated images that, when excluded, yield a lower FID relative to the real dataset, retaining only images that overall reduce distributional distance. Use retained generated images for pretraining detectors.",
            "procedure_type": "data analysis metric / filtering technique",
            "source_domain": "generative model evaluation (computer vision)",
            "target_domain": "selection of synthetic datasets for object detector pretraining",
            "transfer_type": "direct application of a generative-model metric to data curation for detection",
            "modifications_made": "Applied iterative per-image removal: compute FID after removing each candidate and permanently remove it if FID decreases. Used Inception-V3 pretrained on ImageNet for feature extraction.",
            "transfer_success": "partially successful — in NuImages experiments, three generated subsets with different FID values (47, 105, 148) produced very similar downstream detection mAPs, indicating that FID reduction did not reliably translate to better detector performance. Authors conclude FID measures global image distribution similarity (background + targets) and may not predict target-specific downstream utility for detectors.",
            "barriers_encountered": "FID is insensitive to target-specific label correctness; it can penalize background variations that are irrelevant for detector training and can fail to detect mislabeled/generated targets that harm detector learning.",
            "facilitating_factors": "Availability of pretrained Inception-V3 and established FID computation code; FID remains useful as a general image-quality/diversity metric.",
            "contextual_requirements": "Compute resources for feature extraction and iterative re-computation of FID; real validation set for comparison.",
            "generalizability": "Useful for global image-distribution quality assessment across domains, but authors caution its direct use as a filter for downstream detection tasks — task-specific filters (e.g., Precision-Recall) may be preferable.",
            "knowledge_type": "theoretical metric application and explicit procedural steps for dataset curation.",
            "uuid": "e421.4",
            "source_info": {
                "paper_title": "Transfer learning with generative models for object detection on limited datasets",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "ImageNet→ResNet-50 transfer",
            "name_full": "Use of ResNet-50 pretrained on ImageNet as a frozen backbone for object detection",
            "brief_description": "Transfer of convolutional feature extractor weights pretrained on ImageNet classification (ResNet-50) into an object detection pipeline (Faster R-CNN), with the backbone frozen during detector training to reduce dependence on detection-labeled data.",
            "citation_title": "Deep residual learning for image recognition.",
            "mention_or_use": "use",
            "procedure_name": "Pretrained classification backbone transfer (ImageNet ResNet-50 frozen)",
            "procedure_description": "Adopt ResNet-50 pretrained on ImageNet as the feature-extractor backbone inside Faster R-CNN; omit final classification head, feed intermediate residual block outputs into an FPN, and freeze backbone weights (no gradient updates) while training RPN and detection heads on synthetic and/or limited real data. This reduces need for large labeled detection datasets and prevents overfitting to synthetic labeling noise.",
            "procedure_type": "computational method / transfer learning protocol",
            "source_domain": "image classification (ImageNet supervised learning)",
            "target_domain": "object detection model training (computer vision detection tasks)",
            "transfer_type": "direct application with adaptation (freezing backbone and integrating into detection architecture)",
            "modifications_made": "Backbone weights (≈23.45M parameters) frozen; RPN and detection heads randomly initialized and trained; integrated with Feature Pyramid Network as standard for Faster R-CNN; used different weight decay during pretraining and fine-tuning.",
            "transfer_success": "successful — freezing the backbone was reported to improve results relative to training entire Faster R-CNN on synthetic images (cited prior work); in the present study, it is a key design choice enabling effective use of generated pretraining and small real fine-tuning.",
            "barriers_encountered": "Potential mismatch between classification features and fine-grained detection needs, but freezing mitigates overfitting to noisy synthetic labels; requires high-quality pretrained weights.",
            "facilitating_factors": "Proven generality of ImageNet features for downstream vision tasks and reduced data requirements when freezing backbone.",
            "contextual_requirements": "Access to ImageNet-pretrained ResNet-50 weights and integration into Faster R-CNN/FPN pipeline; compute resources to train detection heads.",
            "generalizability": "Widely generalizable — standard practice across many detection tasks; validated here across urban and underwater domains and other detector variants.",
            "knowledge_type": "explicit procedural steps and instrumental/technical know-how (model integration and training regimen).",
            "uuid": "e421.5",
            "source_info": {
                "paper_title": "Transfer learning with generative models for object detection on limited datasets",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "CLIP representation transfer",
            "name_full": "CLIP (Contrastive image–language pretraining) used to ground generated entities",
            "brief_description": "The CLIP model pretrained on large image–text pairs provides image and text embeddings that GLIGEN uses to represent grounding entities (text phrases or reference images), enabling open-set grounding and transfer of concept representations to the generative process for unseen target types.",
            "citation_title": "Learning transferable visual models from natural language supervision.",
            "mention_or_use": "use",
            "procedure_name": "CLIP-based image-text representation transfer",
            "procedure_description": "CLIP encodes grounding entities (either text phrases like 'a fish' or reference images) into a shared embedding space; GLIGEN uses these embeddings to condition generation so that specified entities appear within designated bounding boxes. This transfers the broad, open-set semantic knowledge from CLIP's web-scale pretraining into the constrained L2I generation pipeline.",
            "procedure_type": "representational transfer / computational method",
            "source_domain": "large-scale contrastive image-text pretraining (multimodal ML)",
            "target_domain": "layout-to-image generation for object detection dataset synthesis (computer vision)",
            "transfer_type": "direct integration (CLIP encodings used by GLIGEN to ground generation)",
            "modifications_made": "Authors rely on GLIGEN's integration of CLIP and do not fine-tune CLIP on the target domains; they optionally provide image-grounding entities (real examples) or text phrases encoded by CLIP.",
            "transfer_success": "successful — CLIP-enabled GLIGEN to represent and generate object concepts not explicitly present in the generator's training data, facilitating generation for both cars and various fish species; authors suggest CLIP may need fine-tuning for very niche domains.",
            "barriers_encountered": "For highly specialized domains (e.g., certain medical images) CLIP's general representations may be insufficient and could require fine-tuning; CLIP-based grounding does not solve generator geometric/layout failures.",
            "facilitating_factors": "CLIP's broad coverage and robust multimodal embeddings enable zero-shot grounding of textual or visual entity descriptors.",
            "contextual_requirements": "Access to CLIP weights and the GLIGEN architecture that consumes CLIP embeddings; prompts or reference images for grounding entities.",
            "generalizability": "High within vision-language tasks; authors explicitly note CLIP/GLIGEN separation suggests applicability to many niche domains though possibly requiring CLIP fine-tuning for specialized imagery.",
            "knowledge_type": "theoretical principles (contrastive multimodal representations) and explicit procedural/technical integration into generation pipeline.",
            "uuid": "e421.6",
            "source_info": {
                "paper_title": "Transfer learning with generative models for object detection on limited datasets",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gligen: Open-set grounded text-to-image generation.",
            "rating": 2,
            "sanitized_title": "gligen_openset_grounded_texttoimage_generation"
        },
        {
            "paper_title": "Cut, paste and learn: Surprisingly easy synthesis for instance detection.",
            "rating": 2,
            "sanitized_title": "cut_paste_and_learn_surprisingly_easy_synthesis_for_instance_detection"
        },
        {
            "paper_title": "Training deep networks with synthetic data: Bridging the reality gap by domain randomization.",
            "rating": 2,
            "sanitized_title": "training_deep_networks_with_synthetic_data_bridging_the_reality_gap_by_domain_randomization"
        },
        {
            "paper_title": "Learning transferable visual models from natural language supervision.",
            "rating": 2,
            "sanitized_title": "learning_transferable_visual_models_from_natural_language_supervision"
        },
        {
            "paper_title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium.",
            "rating": 1,
            "sanitized_title": "gans_trained_by_a_two_timescale_update_rule_converge_to_a_local_nash_equilibrium"
        },
        {
            "paper_title": "Integrating geometric control into text-to-image diffusion models for high-quality detection data generation via text prompt.",
            "rating": 1,
            "sanitized_title": "integrating_geometric_control_into_texttoimage_diffusion_models_for_highquality_detection_data_generation_via_text_prompt"
        }
    ],
    "cost": 0.023564749999999995,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Transfer learning with generative models for object detection on limited datasets
13 Jun 2024</p>
<p>M Paiano 
Department of Mathematics and Computer Science
University of Florence
Viale Morgagni 67/aI-50134FlorenceItaly</p>
<p>S Martina 
Department of Physics and Astronomy
University of Florence
Via Sansone 1I-50019Sesto FiorentinoItaly</p>
<p>European Laboratory for Non-Linear Spectroscopy (LENS)
University of Florence
Via Nello Carrara 1, Sesto FiorentinoI-50019Italy</p>
<p>C Giannelli carlotta.giannelli@unifi.it 
Department of Mathematics and Computer Science
University of Florence
Viale Morgagni 67/aI-50134FlorenceItaly</p>
<p>F Caruso 
Department of Physics and Astronomy
University of Florence
Via Sansone 1I-50019Sesto FiorentinoItaly</p>
<p>European Laboratory for Non-Linear Spectroscopy (LENS)
University of Florence
Via Nello Carrara 1, Sesto FiorentinoI-50019Italy</p>
<p>Istituto Nazionale di Ottica
Consiglio Nazionale delle Ricerche (CNR-INO)
I-50019Sesto FiorentinoItaly</p>
<p>Transfer learning with generative models for object detection on limited datasets
13 Jun 2024E795CDA09F044899DB66FA3F73A8A062arXiv:2402.06784v2[cs.CV]Object DetectionTransfer LearningGenerative AIDiffusion ModelsDeep Learning
The availability of data is limited in some fields, especially for object detection tasks, where it is necessary to have correctly labeled bounding boxes around each object.A notable example of such data scarcity is found in the domain of marine biology, where it is useful to develop methods to automatically detect submarine species for environmental monitoring.To address this data limitation, the state-ofthe-art machine learning strategies employ two main approaches.The first involves pretraining models on existing datasets before generalizing to the specific domain of interest.The second strategy is to create synthetic datasets specifically tailored to the target domain using methods like copy-paste techniques or ad-hoc simulators.The first strategy often faces a significant domain shift, while the second demands custom solutions crafted for the specific task.In response to these challenges, here we propose a transfer learning framework that is valid for a generic scenario.In this framework, generated images help to improve the performances of an object detector in a few-real data regime.This is achieved through a diffusion-based generative model that was pretrained on large generic datasets.With respect to the state-of-the-art, we find that it is not necessary to fine tune the generative model on the specific domain of interest.We believe that this is an important advance because it mitigates the labor-intensive task of manual labeling the images in object detection tasks.We validate our approach focusing on fishes in an underwater environment, and on the more common domain of cars in an urban setting.Our method achieves detection performance comparable to models trained on thousands of images, using only a few hundreds of input data.Our results pave the way for new generative AI-based protocols for machine learning applications in various domains, for instance ranging from geophysics to biology and medicine.</p>
<p>Introduction</p>
<p>Machine Learning (ML) is the area of artificial intelligence that involves the development of models that are trained on data to perform specific jobs.In the last few years, we are witnessing a growing interest in generative models as powerful tools in the context of ML.Generative models are capable of estimating the unknown probability distribution underlying the training dataset, in order to generate samples from such distribution.Some of the research works in this field [1] include Gaussian Mixture Models, Hidden Markov Models, Latent Dirichlet Allocation, Boltzmann Machines and, lately, Variational Autoencoders and Generative Adversarial Networks (GANs) [2].More recently, diffusion models emerged as a new family of generative models [3], overcoming GANs as the most adopted and powerful generative models in image generation tasks [4].They are designed according to three main formulations: Denoising Diffusion Probabilistic Models (DDPMs), Score-based Generative Models (SGMs), or Stochastic Differential Equations (SDEs).In all these approaches, the main concept is that the images of the training dataset are progressively perturbed with random noise in a fixed process (called diffusion) where all the information is destroyed, and the images at the end of the process belong to an uninformative distribution (usually a standard multivariate Gaussian distribution).Afterward, the process is reversed with a parametrized ML model that estimates the unknown original data distribution.The trained model can then generate novel data samples belonging to an approximation of the original distribution.The most commonly adopted DDPMs use two Markov chains for the forward and backward training processes, respectively.</p>
<p>Given the late increasing amount of works on generative models for image generation, we are interested in employing them in novel valuable scientific applications.For instance, object detection is the image processing task that consists in the identification of object instances within images [5].One of the traditional ML approaches is supervised learning that requires the input data to be labelled with the desired output of the model to be trained.The model learns to extract statistical features (patterns that capture relevant information about the content of an image) and to predict labels for each input.For ML object detection, labels are in the form of coordinates of bounding boxes around each target, along with identifications of objects types called classes.This means that for training ML models, it is desirable to have access to large amounts of correctly labelled data, but the labelling work is burdensome, and the availability of high quality annotated datasets is scarce, especially for crowded scenarios or specialized contexts.In fact, data scarcity, along with high model complexity, contributes to overfitting: the model learns features that are too specific to the training set, sacrificing generalization capabilities on the unseen data.To tackle this challenge, various techniques revolve around the notion of dataset domain, defined as the joint probability distribution of the feature-label space [6].</p>
<p>The most common strategy to addresses the problem of data scarcity in any ML problem in general, and in computer vision tasks in particular, is data augmentation, which extract more information from the training datasets and enhance their diversity [7].In detail, it consists in an image manipulation process that creates new images by applying on the existing dataset operations such as color space transformations, scene cropping, noise injection or affine transformations (translations, rotations, etc. . .).Another way to artificially expand the size and diversity of the dataset strongly relies on domain knowledge and consists in the construction of synthetic data that mimic the global distribution of real data instances [8].However, such data synthesizers are typically domain-specific, hard to implement, and they require a deep knowledge of the real context captured by the images.Thus, the cost of producing photorealistic images undermines the core advantage of synthetic data, which is the availability of large labeled data sets for free.The data augmentation techniques can be thought as an equivalent of the role of imagination in humans, where alterations of the available dataset are imagined [7].In fact, imagination is shown to be important in learning tasks for humans, and forming mental images helps in memory related tasks [9].In particular, for object detection tasks, there is evidence that the lack of imagination in aphantasic subjects (people unable to create mental imagery) have a negative impact on the performances of these tasks [10,11].</p>
<p>Augmentations and synthesizers introduce variability, but data still belongs to the existing domain.An alternative approach is domain randomization [12], which change the domain fundamental distribution (e.g. by randomizing foreground textures, lighting conditions, backgrounds and occlusions).This forces the model to focus only on the essential features, by interpreting the real-world data just as another variation among the synthetic data.However, this technique still demands to be familiar with the tools necessary to implement the simulation.An easier approach is to copy and paste real images of objects onto background images [13], but this arises the challenge for accurate segmentation of the objects to be copied.</p>
<p>The previous approaches rely on the assumption that both train and test datasets are drawn from the same (possibly randomized) data distribution, otherwise the model will underperform on the test set due to a phenomenon known as domain shift.To counter this problem, the common strategy is called transfer learning [14].The idea is based on the generalization theory of transfer of training, studied within the framework of educational psychology [15].According to this theory, engaging with a new activity by establishing connections with previous experiences enhances the learning process.For instance, students that studied Latin learn related languages (e.g., German) more easily [16].Inspired by this concept, transfer learning in ML aims to enhance model performances by transferring knowledge from one domain (referred to as the source) to another (the target), thereby reducing the need for domain-specific data in the target domain.One effective approach involves training a model on a specific task, such as feature extraction from a particular dataset, and subsequently applying it to extract features in a different domain.If images from the target domain are available, another common technique initially trains the model on the source dataset and then conducts a fine-tuning stage, i.e. a re-training of part (or all) of the pretrained model using new data from the target domain [17].Some examples of transfer learning include, but are not limited to, image classification in the medical field [18,19], detection of gravitational waves [20] and material properties prediction [21].For an extensive review of transfer learning applications, one can consult Ref. [22].Transfer learning can be split into two main categories: homogeneous, when the source and target domains share the same features space, and heterogeneous, otherwise [23].Within homogeneous transfer learning, a subcategory known as domain adaptation [24] addresses scenarios where the feature spaces exhibit different distributions and aims to minimize the gap between the two domains.An illustrative example of domain adaptation is the transfer of knowledge from real pictures to cartoon images [25].Despite sharing the same feature spaces, the distributions of colors and shapes between the two domains differ.</p>
<p>In many scientific areas, detection tasks can also be useful for research activities.Some examples are the counting of microorganisms [26], the monitoring of animals in the wild [27,28] and plants phenotyping [29].In this work, we focus on the problem of training object detectors on domains where the availability of data is scarce.For instance, in the context of marine biology, it is crucial to be able to monitor wild fishes and, in general, to properly support environmental monitoring and protection.For research works where this task was addressed with deep learning techniques see, e.g., Refs.[30,31,32,33].However, the availability of data in this field is not of great extent [34,35,36].To resolve this issue, some works addressed the data scarcity with the aid of simulators to generate synthetic data [37,38]; other solutions have been found in the context of transfer learning from other domains [39].</p>
<p>In this paper, we investigate a novel strategy that merges the fields of generative models and object detection.While generative models have been proven successful in generating data for training classifiers [40], their application in the realm of object detection has been less explored.The challenge arises when attempting to generate images suitable for object detectors, as it necessitates the representation of targets within specific layouts, adhering to bounding box positions crucial for supervised training.This issue is formulated as Layout-to-Image (L2I) generation.Traditional generative models, however, faced limitations in generating images with such constraints.To avoid this challenge, some prior approaches employed GANs for performing style transfer, a form of domain adaptation, on the training set [41,42].Nonetheless, this method relies on an existing training set, whether synthetic or real, and does not truly generate entirely new images.The first attempt to address the L2I problem was made by Layout2Im [43], but it is only in recent times, notably with GLIGEN [44], that the generation of constrained images has reached a satisfactory level of quality.</p>
<p>It is worth emphasizing that the existing literature has mostly explored the problem within the realm of generative models, rather than its application to the object detection task.The work [45] also explored the potential of training an object detector using generated images.The authors fine-tuned their generative model on a big real dataset from a very specific domain.However, this requires data with time-consuming boundingbox labeling, potentially leading to a cyclical issue where large, labor-intensive data may We employ a L2I pretrained model to generate images for transfer learning to an object detector.We can filter out suboptimal generated images based on benchmark metrics.For instance, the image along the red arrow is discarded because the generative model has depicted many cars outside the bounding boxes designated in the grounding instruction.With the remaining generated images, we pretrain the object detector, followed by a fine-tuning on the real dataset.Dashed lines indicate the data used for training the models.still be needed to train the generative model instead of the detector.By contrast, we aim to mitigate this problem, by tackling the data scarcity on generic domains with the aid of pre-trained generative models with L2I capabilities, to generate abundant labeled data for free, ready for the training of an object detector.Indeed, we intentionally choose to use a pretrained generative model to further alleviate the dependence on real labeled data.Thus, an important novelty of our work is the absence of the generative model training on the specific task domain.</p>
<p>Generative</p>
<p>In particular, we exploit GLIGEN, which is peculiarly built upon existing models pretrained on large-scale web data.This endows GLIGEN with the inherent knowledge derived from other extensive and diverse datasets, facilitating the representation of novel concepts not encountered during its training, aligning seamlessly with our objectives.This sets GLIGEN apart from prior L2I generative models, which were typically confined to be trained on datasets tailored to specific tasks.Furthermore, the inherent value of this strategy lies in its potential applicability to diverse domains without the explicit requirement of training the generator on each individual domain.</p>
<p>In detail, our architecture, illustrated in Fig. 1, adopts a two-step process.Initially, the object detector undergoes training exclusively on generated images, followed by a subsequent fine-tuning stage using only the limited real dataset.The rationale behind this process is to facilitate transfer learning, enabling the model to exploit a large amount of available data in a source domain, here the domain of generated images, to correctly detect data from the target domain, here the domain of real images, for which the amount of training data is limited.Moreover, we designed an optional filtering strategy to assess the impact of the generated dataset on the object detector and to select the most suitable images for its pretraining.To assess the effectiveness of our approach, we evaluate it on two different detection domains: cars and fishes in urban and underwater environments, respectively.The reason for this choice is to compare the effectiveness of the proposed approach on a common domain with high data availability (the former) with another domain with data scarcity and interesting for the scientific community (the latter).Nevertheless, the domain choice comes without loss of generality, since our framework can in principle be applied to other common or uncommon domains.</p>
<p>The structure of the paper is as follows: our transfer learning approach with pretrained generative models is detailed in Secs. 2 and 3.In Sec. 4 the filtering strategies are introduced.Sec. 5 shows the real datasets selected for our tests, while Sec.6 presents some numerical results.Finally, Sec. 7 concludes the paper with some final remarks and outlooks.</p>
<p>Generative model</p>
<p>Our pipeline is designed to be sufficiently generic, provided that the generator is of the L2I type, and it is capable to generalize to diverse domains.At the best of our knowledge, only GLIGEN satisfies these requirements in the current state-of-the-art.This is because GLIGEN leverages CLIP, a model trained on vast web-scale data to predict associations between images and corresponding texts, and vice versa.Renowned for its ability to make predictions on concepts that are not explicitly encountered during training, CLIP encoding is leveraged by GLIGEN, achieving the generalized representation abilities we are seeking.</p>
<p>Specifically, the generation process of GLIGEN is guided by an instruction y = (c, e) that the model follows to create a scene with the desired concepts in specific locations.The instruction comprises a prompt c, a written description of the image content, and a grounding instruction e = [(e 0 , l 0 ), ..., (e N , l N )].Each l i represents the bounding box coordinates around the object e i that should be depicted within.These objects, referred to as grounding entities, can be described either by text or by a reference real image.In Sec. 6, we will present an ablation study comparing the description of grounding entities through text versus real images.This comparison aims to assess which grounding entity type produces targets best suited for detection in the domain of interest.Regardless of their nature, both kinds of grounding entities are encoded by the pretrained CLIP [46].Additionally, associating a reference image as e i with an image edge as the corresponding l i facilitates a consistent style transfer from the real to the generated domain.We leverage this feature to achieve backgrounds that closely resemble the originals.</p>
<p>Throughout the remainder of our paper, the grounding instruction corresponds to what in the realm of object detection is referred to as ground truth, which is the list of all dataset labels (i.e.bounding boxes and classes of every target).To automatically choose each l i , we start by collecting the real dataset's ground truth information from a small set of labeled sample shots.Subsequently, we conduct a statistical analysis to estimate the means and variances of the bounding box parameters (i.e.positions, shapes, and number per image) within the original dataset.Following this, we randomly sample from Gaussian distributions using the estimated means and variances, to get plausible parameters for the ground truth to generate.</p>
<p>Therefore, as represented on the left part of Fig. 1, to generate a new image we supply GLIGEN with the following inputs: a text prompt, a desired grounding instruction, an image from the real dataset to get style transfer.The details on the use of the generation model on example datasets is described in Sec. 5.</p>
<p>Object detector</p>
<p>Our pipeline is general enough to also accept any kind of object detector.For our tests, we opted for the traditional Faster R-CNN [47] which is a region-based model (which gives the letter "R" in the name).We recall that a Faster R-CNN is mainly composed of two parts, called backbone and Region Proposal Network (RPN).</p>
<p>The backbone uses Convolutional Neural Networks (CNNs), models capable of extracting relevant features, to transform an input image into two-dimensional tensors known as feature maps.The RPN overlays the feature maps with a set of bounding boxes, referred to as anchors.On each anchor, the network conducts objectness classification, which distinguishes generic objects from the background.Subsequently, the anchors containing objects are submitted to a standard neural network classifier for multiclass classification.</p>
<p>In practice, Faster R-CNN is often trained in a transfer learning setting.One common approach involves pretraining the entire model on a standard detection dataset (e.g., COCO [48]) and subsequently fine-tuning it on the specific domain of interest.However, to avoid biases, we choose not to pretrain on other detection datasets, in order to focus on the contribution of generated data only.Besides, in [49] it is demonstrated that training the entire Faster R-CNN on synthetic images leads to significantly poorer performance compared to training on real data.Conversely, freezing a pretrained feature extractor and exclusively training the remaining parts of the detector results in a considerable performance improvement.Therefore, we freeze a pretrained backbone, and we randomly initialize the RPN weights.Finally, it is worth noting that the backbone can be pretrained on a dataset not necessarily tailored for object detection, thus reducing the need for costly labeling.In detail, in our analyses, we opted to use as backbone a ResNet-50 [50] -a renowned 50 layers deep CNN-pretrained on ImageNet [51], a famous image classification dataset which has become the de facto standard for the pretraining of a CNN [52].An in-depth description of the ResNet-50 backbone and the Faster R-CNN model are detailed in Appendix A.</p>
<p>Overall, our Faster R-CNN has 41 370 911 weights, of which 23 454 912 belong to the backbone and are frozen.The remaining weights are trained with Stochastic Gradient Descent (SGD) with momentum, which updates the weights θ t at time t, aiming to minimize a loss function L with the iterative equations [53]:
b 0 = 0 , b t = µb t−1 + ∇ θ L(θ t−1 ) + λθ t−1 , (1)θ t = θ t−1 − γb t ,
involving the so-called momentum µ, weight decay λ, learning rate γ, and ∇ θ L gradient of L. The loss to minimize is formed by the contribution of different terms regarding the positioning of the predicted boxes, the objectness classification and the multiclass classification of each box.In our experiments, we set the value of µ = 0.9, while the weight decay λ is 0.001 during the model pretraining on generated data, and 0.01 for the fine-tuning on the real data.The reason for the latter is that when the model is trained on a limited dataset size, it is more prone to overfitting.We use higher values of λ, that provide higher regularization, i.e. a penalization to reduce the model complexity.</p>
<p>Regarding the learning rate, we adopted two different scheduling strategies, depending on the availability of images for a validation set-a portion of the dataset used to monitor the loss value after each SGD phase (epoch) over the entire training set.In the fine-tuning stage, due to limited real data, we opted for the common 1x scheduler.This scheduler does not require a validation set, allowing us to maximize training data usage.Conversely, during the pre-training stage, where we can generate as much fake data as needed, we dedicate 15% of the generated data as a validation set.At the beginning of both stages, the learning rate γ is initialized with a starting value of 0.001.Then, the pretraining step scheduler reduces γ by a factor of 10 if the value of the validation loss does not improve for 5 epochs.Moreover, we adopt also an early stopping strategy where the detector is trained for a maximum of 200 epochs until the validation loss stops decreasing for at least 10 epochs.In the end, we keep the weights of the model at the epoch with the minor validation loss value.During the fine-tuning step, the 1x scheduler has a total fixed number of 12 epochs, and γ is reduced by a factor of 10 at epoch 7 and again at epoch 10.The fixed 1x scheduler should not be considered as a limitation, instead, it has been chosen to achieve a fair comparison, focusing only on the impact of the pretraining stage in the pipeline.</p>
<p>Metrics and filters</p>
<p>We can evaluate the usefulness of the generation process, described in Sec. 2, through a filter.Its role is to select the best images for the pretraining of the object detector, based on benchmark performance metrics.In the current state of the art, the most used metric for evaluating generative models is the Frechet Inception Distance (FID) [54].It quantifies the similarity and diversity of generated images compared to real data.On the other hand, in the realm of object detection, the main metric to assess the quality of the predictions is known as mean Average Precision (mAP), which is based on the key concepts of precision and recall.In the following, we define how we calculate FID, precision, recall, and the mAP, and we discuss how we use these metrics to filter out suboptimal generated images.</p>
<p>FID score</p>
<p>The FID score quantifies the similarity between generated and real images, measuring the difference between their feature distributions: X extracted from real images and Y from generated images.These distributions are respectively approximated by two multivariate Gaussian distributions with means µ X and µ Y , and covariances Σ X and Σ Y .The FID is thus computed1 by:
d 2 = ||µ X − µ Y || 2 + T r(Σ X + Σ Y − 2 Σ X Σ Y ).
(2)</p>
<p>In detail, we follow the method introduced by [54] and widely adopted in the later literature, which involves extracting features with an Inception-V3 model [55] pretrained on ImageNet.Even if FID is usually considered as the conventional metric of generative models, its effectiveness in evaluating images used for training object detectors has not been thoroughly explored.Although lower FID values indicate better image quality, it remains unknown whether this directly correlates with improved performances for pretraining an object detector.To perform this analysis, we introduce a FID-based filter, which retains only those images that yield a FID score below a certain threshold.To implement it, we compute the FID score between the generated and the real datasets.Then, we iteratively remove one image from the generated dataset, and we re-calculate the FID with the real dataset.If this new FID is lower than the previous FID value, then we definitively remove that image, responsible for distancing the feature distributions between generated and real datasets.Otherwise, the image is kept into the generated dataset.</p>
<p>Precision-Recall</p>
<p>To define precision and recall in the context of object detection, we leverage the notion of Intersection over Union (IoU), which quantifies the similarity between a ground truth and a predicted bounding box.It is computed, as depicted in Fig. 2a, with the ratio between the area of the two boxes intersection and the area of their union.Its value ranges from 0 to 1: a higher value of IoU denotes a larger overlap between the boxes as visible in the three examples of Fig. 2b.By setting an IoU threshold (typically at 0.5), a prediction box is classified as True Positive (TP) if its IoU score is above the considered  (3)</p>
<p>The former measures the correct predictions across all detections and the latter the amount of true targets found among all ground truths.We propose to employ precision and recall to assess the impact that the generated images may have on the object detector pretraining.Indeed, the L2I models-even the most recent ones-still have some limitations.In particular, they may struggle to adhere to the intended geometric layouts, either drawing targets outside the designated bounding boxes or, conversely, failing to depict the desired objects within them.We want to analyze the generated dataset, identifying potential inconsistencies with the ground truth, in case they may compromise the pretraining of the object detector.We implement a Precision-Recall based filter with another Faster R-CNN initialized with COCO weights, possibly fine-tuned on the limited real dataset.This filtering Faster R-CNN is then tested on the generated dataset, computing precision and recall for each image.By setting thresholds for these metrics, we selectively retain images in which all targets are strictly generated within the specified bounding boxes.For instance, setting a prediction threshold at 1, ensures that no target is erroneously generated outside the ground truth constraints, discarding the example in Fig. 2d.Similarly, a recall threshold at 1 preserves only those images where targets are correctly generated within the specified bounding boxes, discarding the example in Fig. 2e.</p>
<p>Mean Average Precision and F-score</p>
<p>As we mentioned before, the mAP is the principal metric to measure accuracy in the context of object detection.In our analyses, we calculate the mAP on the test set of our real datasets.</p>
<p>First, for a dataset of N classes, we compute N precision-recall curves, one for each individual class.For a class i, this involves ordering the M i predicted boxes in descending order based on a confidence score, which is the probability that the target within M i is classified as belonging to class i.Then, we compute precision and recall over all images, but considering only the boxes having confidence score greater than a threshold.Such threshold is iteratively decreased, and new values of precision and recall are calculated.Plotting the computed recall values on a x-axis and precision on a y-axis, we get the precision-recall curve for class i.The average precision AP i is computed as follows [56]:
AP i = M i −1 j=1 (rec i,j+1 − rec i,j )prec int (rec i,j+1 ) (4) prec int (rec i,j+1 ) = max{prec(rec i,k ) : rec i,k ≥ rec i,j+1 }
where j = 1, . . ., M i are the ordered predicted boxes for class i, rec i,j denotes the recall value calculated for class i on the j predicted box, and prec(rec i,j ) is the precision value corresponding to rec i,j .In other words, the AP i can be seen as the area under an interpolation of the i-th precision-recall curve.At each point, the interpolation considers the maximum precision value for which the recall is greater than or equal to the recall at that specific point on the curve.Finally, the mAP is the mean of all AP i over the number of classes N .By definition, there is a trade-off between precision and recall: increasing recall will inevitably cause non-relevant results to be detected, decreasing precision.The mAP ranges between 0 and 1: values close to 1 mean that the precision is high also when the recall is increasing.In this case, the number of both false positives and false negatives is low.</p>
<p>Secondly, the performance of our models are evaluated by an F1 score, which considers all the detections having confidence greater than 0.5, and represents both precision and recall in one metric:
F 1 = 2 prec • rec prec + rec (5)</p>
<p>Real and generated datasets</p>
<p>In this section, we describe the real datasets selected for our tests: NuImages and OzFish.NuImages is representative of a common urban scenario, and serves as a simple testing ground, allowing for initial analyses and illustrative insights into the method's efficacy.OzFish is one of the few publicly available datasets in marine environment.The presence of fish instances, particularly complex to label and detect since they are prone to overlapping, makes OzFish a valuable benchmark for its unique challenges.Moreover, we also describe the details of how we generate the data used for the pretraining of the detector in the two aforementioned scenarios.</p>
<p>Several datasets have been published to train object detectors in urban environments.For instance, KITTI [57] is the most widely used for autonomous driving research.However, the diversity of its recording conditions is relatively low.Instead, NuImages [58] captures scenes in multiple locations, providing a broader and more varied perspective.Besides, it collects data in both daytime and nighttime scenarios, taking into account various lighting and weather conditions, offering a more realistic and challenging environment for object detection models [59].The dataset is composed by around 75 000 images, already divided in 60 000 training samples and 15 000 validation samples.It covers up to 25 classes, primarily falling within the broader vehicle and human macro categories.Without loss of generality, we focused our analysis on the car class, aiming for a less complex scenario, in order to give a simpler overview of our proposed method.Consequently, we narrow down our dataset to include only those images containing instances of the car class.Besides, to simulate a situation with limited data available, we make the assumption of using a subset of the large training dataset.Instead, for the test set, we use all the images originally intended as validation and that contains at least a car instance, to achieve a more statistically meaningful result.In summary, our selected dataset contains 4 500 training images at most, and 9 073 test images.</p>
<p>To generate each image for the pretraining dataset in the urban scenario, we recall that we need a prompt c and a grounding instruction e = [(e 0 , l 0 ), ..., (e N , l N )], whose first value e 0 is specifically the real image used for style transfer.With N cars to generate, the other grounding entities e i , for i = 1, . . ., N , are described by the phrase "a car", as shown in Fig. 1.In particular, we use 3 644 training images for the style transfer, which may seem a large number, but it actually requires no human labeling intervention.Determining the most effective prompt for guiding the generation process falls within the field of study known as prompt engineering, but this is beyond the scope of our current investigation.Instead, to evaluate the potentialities from a nonexpert perspective, we opt for the basic prompt "N cars in a urban environment, highly photorealistic", where N is the desired number of cars in a single image.It is crucial to note that although we have demonstrated a generation focused on a single class example, our methodology can be effortlessly extended to a multi-class scenario by appropriately setting different class phrases for the e i in the grounding instruction.</p>
<p>For our second test scenario, in marine environment, we remark that the task of acquiring high-quality labeled underwater images is challenged by two main factors [60].On one hand, there are several environment uncontrolled variables, such as water turbidity, uneven illumination, and low contrast.On the other, also intrinsic fish characteristics contributes to the task difficulty, including intra-class variations, color similarity with the background, occlusions, and arbitrary unusual swimming positions.Besides, the rich biodiversity adds complexity to creating a universal dataset for marine ecosystem observation, limiting model applicability across different underwater areas.This is in contrast with the ML requirement for great quantity of good quality training data, and existing datasets, as summarized by [34], often fall short in addressing this requirement.For instance, Fish4Knowledge [61] and Rockfish [62] do not tackle the object detection challenge, mainly addressing it by applying classic post-processing cropping methods, so that each fish results centered and can be identified with a mere object classification approach.The DeepFish [63] dataset surpasses the previous ones by also offering a semantic segmentation of fish instances, i.e. the annotation of every pixel belonging to the fish, denoted as mask.A Seagrass dataset was used in [64] to demonstrate that ML approaches result in an increased abundance detection compared to human experts.However, all previous dataset presents limitations, either due to poor image resolution (Fish4Knowledge) or because they have a low number of fishes per image (1 at most for DeepFish and RockFish, not more than 18 on Seagrass).</p>
<p>The OzFish [65] dataset has significantly increased the number of annotated boxes per image, reaching up to hundreds.As stated in [35], this comes with incorrect or missing bounding box annotations.Examples include overlapped annotations on the same target, negligible small bounding boxes, and mislabeling of objects like reefs and floating ropes annotated as fish.Despite these imperfections, the data remains valuable, since the images are more extensively labeled compared to previous datasets, and they also exhibit greater variability in environments and fish positioning, allowing us to address challenges related to turbidity and overlapping.In particular, OzFish consists of around 1 800 images and containing around 45 000 bounding box annotations.We partitioned the images using an 85% to 15% ratio, resulting in 1 500 images for the training set and 265 images for the test set.</p>
<p>For the generated dataset, we decided to explore two distinct approaches, comparing a grounding instruction that relies on text descriptions with another one where example images are used.In the former, each target grounding entity is defined simply by the phrase "a fish" (an example is visible in Fig. 3a); in the latter, we employ as grounding entities one image selected from a set of 16 close-ups of different fish species from the training set (as in Fig. 3b).In both cases, one sample from a set of 128 training pictures without fishes is employed to condition the background style of a single image.The chosen prompt is "blurry fishes in motion, foggy underwater professional photography".In Sec.6.2, we will compare these two kinds of generated dataset with a common state-of-the-art technique, where a synthetic image is created by copying masks of the real objects and pasting them onto a real background, as in Fig. 3c.To achieve this, we leverage the DeepFish dataset that provides semantic segmentations of fishes.We obtained 207 masks of around 10 different species, and we pasted them on the same 128 images used for the style transfer in the previous approaches.We note that, despite the diversity between OzFish and DeepFish datasets, both feature tropical Australian species.For these three types of approaches, we defined identical desired bounding box positions for all the fish instances to ensure more comparable training scenarios.However, as discussed in Sec. 4, the generation model might struggle to generate all the targets, particularly in cases of overlap, which is a challenge not encountered by the copy-paste method (as an example of this issue, compare the fishes in the bottom left corner between the three images in Fig. 3).Finally, we observe that the fish instances in these three datasets exhibit increasing levels of complexity.Creating a fish with the first method merely requires a text phrase.The second approach demands capturing well-centered pictures of the real targets of interest, which is still a relatively simple task.The third one needs semantic masks of the objects, making it the most laborious method, involving more human intervention than the other two.For all the aforementioned datasets, both real and generated, we exclude the use of small bounding boxes, specifically ignoring those with an area that is less than 0.2% of the image area.Indeed, it is well-known in the literature that generative models struggle with generating small targets [66], a problem that falls into another research field and is beyond the scope of this study.</p>
<p>To generate the datasets, we have used two computational servers: one with 88</p>
<p>Results and discussion</p>
<p>To validate our pipeline, we have implemented it in PyTorch, and we have used the same servers specified in Sec. 5 to train several models on both NuImages and OzFish datasets.</p>
<p>The training times are around 0.7 seconds per iteration on the first machine, and 0.3 seconds per iteration on the second one.In the following, the details of our analyses are explained.In Fig. 7 we show some examples of prediction in both scenarios.</p>
<p>NuImages-reducing the need for real data</p>
<p>On NuImages, we show the effectiveness of our transfer learning approach by evaluating the mAP performances of our object detector under different setup conditions, all tested on the same set of 9 073 real images.We also examine the impact of filters on the model training.The Faster R-CNN is pretrained using varying amounts of our generated images-specifically, 750, 1 500, 3 000, 4 500, 6 000, 9 000 and 12 000.For comparison, we report the case where the generated images are not used during pretraining.During this phase, the previously specified quantities correspond to an 85% of the total generated images used for each configuration, while the remaining 15% is kept for validation and scheduling purposes.For instance, the second configuration is trained on 1 500 images from the generated set and validated on 265 other images.Subsequently, each of these pretrained models undergoes fine-tuning with various amounts of real images-namely, 50, 300, 750, 1 500, and 4 500.We also considered a model only pretrained on generated images, without any fine-tuning on the real dataset.In Fig. 4, the x-axis represents the number of generated images used for the initial pretraining, while different colors and marker shapes indicates the numbers of real images used for the fine-tuning step.The y-axis shows the corresponding mAP performances on the real test set for each configuration.Besides, to evaluate the usefulness of our filter, we followed two distinct approaches: one involving the selection of generated images through a Precision-Recall filter (solid curves and filled markers), and the other employing all generated images, regardless of their quality (dashed curves and empty markers).Specifically, for both precision and recall, we set the filtering thresholds to 1, aiming to preserve only the most accurate representations of the ground truth during the generation process (this process discarded around 56% of all the images).</p>
<p>Several key observations can be drawn.Unsurprisingly, increasing the number of real images during the fine-tuning results in higher mAP values.In fact, the curves where a higher number of real images is used, consistently appear on top of the ones with fewer real images.The first observation regarding our approach is that the integration of generated images successfully compensates for the scarcity of real data.For instance, 300 real images combined with 9 000 generated images exhibit performance equivalent to the full dataset of 4 500 real images.More surprisingly, a combination of 300 real images and 750 filtered generated images yields better results than using only 1500 real images.This finding is quite remarkable, suggesting that the contribution of the pretraining is crucial to obtain a good starting point for the fine-tuning of the weights with the real images.This assertion is also supported by the curve indicated with 0 real images, where favorable mAP values can be achieved solely with generated images, even without fine-tuning on real ones.For example, training the detector with 9 000 filtered generated images is comparable to the performance obtained on 1 500 real images.We assume that the good impact of the generated images can be attributed to GLIGEN being trained also with knowledge from the urban domain.</p>
<p>The second observation that we can deduce is that the filter effectively condenses the information from a multitude of generated images into few important ones.Interestingly, the filtered images do not allow for better performance over the unfiltered ones in the long run.In fact, the mAP converges to similar values when pretraining with 12 000 images, both filtered and unfiltered.We make the hypothesis that, for large numbers of generated training images, the model autonomously learns how to ignore the imperfect labeling occurring in the generated images.Conversely, when the size of the training set is small, the model may overfit on these inaccurate data.</p>
<p>In another test, we also evaluate the influence of the FID score on the training of the object detector.We arranged the unfiltered generated images into three mutually exclusive subsets with varying FID scores: 47, 105, and 148.Each subset consisted of 3 000 images for pretraining, with an additional 530 images used for validation.All these configurations are then involved in a fine-tuning step on the same 300 real images of Fig. 4. At the end, we found that there are no significant improvements in training the detector on datasets with different FID scores.Specifically, for all three cases we obtained really similar test detection mAPs comparable with the results of Fig. 4.This is an evidence that, even if the FID score serves as a valuable metric for assessing the overall similarity between real and generated distributions, it may not be the most suitable metric to measure the impact of generated data in object detection tasks.Specifically, it is important to recognize that FID only assesses the image quality independently from specific downstream tasks like object detection.Indeed, the FID calculates distribution distances considering the images as a whole, rather than focusing on the quality of individual targets, which is the primary concern of the object detector's feature extractor.For example, we can hypothetically consider two distinct detection datasets having exactly identical images, but one with the correct labeling and the other one with a completely erroneous one.In this scenario, the FID distance between the two datasets would be 0, but their impact on the training of an object detector would be completely different, catastrophic for the wrongly labelled dataset.</p>
<p>OzFish-comparing with different state-of-the-art techniques and testing universality</p>
<p>In this section we discuss the application of our pipeline to the domain of OzFish.We present a comparison between our method and other prominent state-of-the-art approaches.Moreover, in this domain we decided not to focus on the effects of the filter because we observed a fast convergence already using the unfiltered images.We employed the three datasets outlined in Sec. 5 and exemplified in Fig. 3. Similar to the generated dataset used for the NuImages scenario, we maintained an 85% of the generated images for pretraining and reserved the remaining 15% for validation.As in Fig. 4, in Fig. 5 we plot the mAP trend for an increasing number of pretraining generated images (x-axis) and fine-tuning real images (curve color).For all cases, the mAP is computed on the same real test set of 265 OzFish images.</p>
<p>The first insightful observation arises when comparing various image generation strategies.We observe that each method's curve appears completely above the previous ones.Specifically, the state-of-the-art copy-paste approach (dotted curve with round markers) exhibits the highest performance, followed by the cases where grounding entities are images (dot-dashed curve with square markers) and text (dashed curve Figure 5: Object detection mAP results on the OzFish real test set, for models pretrained on different quantities of unfiltered generated images (x-axis) and fine-tuned on varying numbers of OzFish training images (specified by the curve color).Dotted curves with round markers indicate models pretrained on synthetic copy-paste images.Dot-dashed and dashed lines, with squares and diamond markers, represent the use of images and text as grounding entities, respectively.The cyan constant solid line at around 0.6 mAP reports for reference the performance of a model pretrained on COCO and fine-tuned on all the 1 500 OzFish training images.The colored solid lines with triangular markers are references that use a standard data augmentation approach.</p>
<p>with diamond markers).This is probably because the copy-paste dataset incorporates true fishes features, which may prove more effective than the generated ones.However, it is crucial to note that the curves appear to be sorted by an increasing level of their generation complexity.It is important to recall that the copy-paste approach synthesizes images by relying on mask segmentation, a process that is inherently resource-intensive, even more than labeling a dataset for object detection.In contrast, our methods stand out for their simplicity, requiring only target pictures for image grounding or even simpler text descriptions.For instance, under the configuration with 9 000 pretraining images and 300 real images, the mAP gap between text grounding and copy-paste methods is just 0.05, which is quite tolerable, considering the substantial reduction in the burden on human intervention.</p>
<p>To further investigate the contribution of fish features to mAP performance, we calculate the FID values between each of these datasets and the real Ozfish dataset, resulting in values of 70, 90, and 100 for the copy-paste, image grounding, and text grounding approaches, respectively.Indeed, the copy-paste method exhibits the lowest FID (70), indicating a feature distribution closest to that of OzFish.In this scenario, fine-tuning with 50 OzFish images (blue curve) shows a 0.1 mAP gap between copypaste and text grounding pretraining, suggesting that datasets with lower FID values contribute more effectively to the object detector pretraining.This finding appears to differ from the FID filter results discussed in Sec.6.1.In that context, three datasets were created filtering out images generated with the same method, using identical text grounding entities and prompts, resulting in varying FID values (47,105,148).However, used during detector pretraining, these datasets yielded comparable mAP values, implying that FID was not effective in filtering detrimental images.This apparently contrasting result might find an explanation by considering that the FID can be influenced by many factors whose impact on the mAP is not necessarily equal.For instance, image background and targets may have features that do not hold the same weight in the task of pretraining an object detector.It is essential to note that our detector has a frozen backbone and is exclusively trained on the RPN detection portion, which focuses solely on features within the target's bounding boxes and not on the background.In the NuImages case, where target features are generated with the same instructions, FID differences probably arise mainly from variations in background features, irrelevant to the detection task.Therefore, using a FID-based filter on those generated datasets may discard features not relevant for the RPN training.Conversely, in the case of OzFish, the three datasets inherently contain differently generated targets, resulting in FID differences that may also be caused by variations in the targets' features, contributing to both FID variation and RPN training performances.</p>
<p>We also compare our model with the classical data augmentation techniques illustrated in Sec. 1 (colored solid lines with triangular markers in Fig. 5).We augment independently each of the considered real sub-datasets, and we directly train the object detector (pretrained only on ImageNet) on a mix of real and augmented images, without our 2-step transfer learning.Moreover, we establish another reference configuration whose weights are initialized based on the COCO dataset as in [47].This allows to benchmark the performances of our generated datasets against the state-of-the-art method to address data scarcity, that is to perform transfer learning using a model pretrained on the most prominent dataset for object detection.In detail, the reference model is pretrained on ImageNet and then on COCO.We additionally fine-tune it on the whole OzFish dataset, keeping also in this case the backbone frozen as detailed in Sec. 3. Our model surprisingly approaches the performance of the COCO-pretrained reference (cyan constant solid line at around 0.6 mAP in Fig. 5) with only 300 OzFish examples (red curve), reducing the number of real labeled images by almost three orders of magnitude: from 118K COCO images plus 1 500 OzFish images to only 300 OzFish images supported by 9 000 generated ones.Besides, with this specific training setting, when using the entire dataset of 1 500 real images, we manage to surpass the reference with only 1 500 generated images (green curve).These are insights that the generated data is better suited for the transfer learning task in this context, compared to the general-purpose COCO dataset.This is encouraging for the widespread adoption of generative models in object detection tasks.Furthermore, in Fig. 6, we test the universality of our framework by comparing our approach across different object detectors.We recall that the object detector used in previous evaluations is a Faster R-CNN with a ResNet-50 backbone (labeled as Faster Resnet in the legend).This is compared with two different scenarios.In the first, the same Faster R-CNN model is used, but with a different backbone network: MobileNet_v3 [67] (labeled as Faster Mobile).The second scenario employs the same ResNet-50 backbone, but with a completely different detection model: Fully Convolutional One-Stage object detector (FCOS) [68] (labeled as FCOS Resnet).Technical details of these models can be found in the Appendix A. We observe that all the three models exhibit a similar behavior, demonstrating that our approach is independent of the chosen object detector and highlighting the beneficial effect of pretraining on generated images in every scenario.Finally, all the main metrics (mAP, precision, recall and F1 score) for some relevant trainings of the object detector on the OzFish dataset are reported in table 1, along with the results on NuImages.</p>
<p>Conclusion</p>
<p>We have proposed a method that leverages GLIGEN, a generative model pretrained on extensive and diverse datasets, to generate images with targets inside bounding boxes for the pretraining of an object detector and subsequent transfer learning to the real domain.</p>
<p>We have demonstrated remarkable results on both NuImages and OzFish datasets.We Table 1: Metrics for the two considered datasets, for 50 and 1 500 real images and for 0 and 9 000 generated images.The confidence threshold to compute precision, recall and F1 is fixed at 0.5.The OzFish generated dataset is the image grounding one.In the left column (a, c), Faster R-CNN models are initialized with ImageNet and finetuned using a minimal dataset of 300 real training images.On the right, models undergo a preliminary pretraining on 9 000 generated images (from the image grounding dataset in the OzFish case), followed by a fine-tuning on the same 300 real images.We can observe that our approach shows increased performances in finding more targets and refining redundant detection boxes, even in rainy or turbid environments.</p>
<p>Dataset</p>
<p>have showed that our method allows to achieve good detection performance with just few hundreds of real images, comparable to training the model on thousands of real images.Furthermore, our method significantly reduces the human effort involved in the annotation process, limiting the need for labor-intensive labeling tasks.</p>
<p>In our work we address the generalization capability of the proposed approach by comparing a common-domain dataset of cars in urban environment with a more specific and uncommon dataset of fishes in underwater environment.Besides, it is plausible that our framework can be generalizable even to more complex scenarios.Indeed, GLIGEN leverages CLIP, i.e. a model trained on vast and diverse datasets that encompass categories beyond just underwater fish and urban cars.CLIP requires only imagetext pairs, which are more easy to obtain compared to labor-intensive annotations like bounding boxes.GLIGEN builds upon CLIP's strong representation capabilities and learns only the additional ability to depict objects within the desired bounding boxes.This separation of capabilities suggests that the two aspects are relatively independent.Therefore, in more niche scenarios, such as on medical images, it should be sufficient to fine-tune the underlying CLIP model [69].</p>
<p>During the preparation of our manuscript, an independent paper has appeared online as preprint in Ref. [45], exploring the potential of training an object detector using generated images.However, their generative model was trained on a substantial amount of real data from a specific dataset, thus without addressing the issue of limited datasets from a generic domain that we have considered here.Besides, while via their model they primarily report the FID as the standard metric to evaluate generative models, we here also explore how FID impacts on mAP.While our findings are in their preliminary stages and comprise hypotheses, they open new possibilities for understanding the relationship between FID and mAP, hence providing an intriguing direction for further exploration and investigation.</p>
<p>Future works may include additional ablation studies.Firstly, we can further evaluate whether the primary factor affecting FID and mAP performances arises from the overall quality of the generated image, including the background, or solely from that of the targets.Secondly, we can explore the influence of style transfer by not providing to the generative model any reference picture, relying solely on its pretrained knowledge.Finally, a comprehensive investigation into the effectiveness of the precisionrecall filtering strategy, and the implications of employing a flawed ground truth during the training, demands further studies.</p>
<p>In conclusion, our method offers a novel and efficient solution to tackle the data scarcity challenge in object detection without the need for manual labeling of data, with the help of a pretrained generative model.In this way, we pave the way for further exploration of generative models applications in this field.This can be useful in specific domains where the gathering and labeling of data can be problematic or costly, such as, among others, medicine and molecular biology, atmospheric monitoring, material science, space imaging, and non-human-friendly environments.</p>
<p>Appendix A</p>
<p>The standard ResNet-50 model starts with an initial convolutional layer with 64 7 × 7 filters, followed by a 3 × 3 max pooling layer.The main body of ResNet-50 consists of a series of residual blocks, each functioning as a bottleneck.Each bottleneck comprises a 1 × 1 convolutional layer to reduce dimensions, a 3 × 3 convolutional layer to extract spatial features, and another 1×1 convolutional layer to restore the original dimensions, totaling 3 convolutional layers per block.The architecture contains 16 bottleneck blocks arranged in four stages, for a total of 48 convolutional layers: the first stage has 3 blocks with 256 channels, the second stage has 4 blocks with 512 channels, the third stage has 6 blocks with 1024 channels, and the final stage has 3 blocks with 2048 channels.The network concludes with a global average pooling layer followed by a fully connected layer for classification.</p>
<p>In the Faster R-CNN framework, the ResNet50 backbone's final fully connected layer is omitted.Instead, the outputs of the four intermediate residual blocks, before each upsampling and before the final fully connected layer, are used as inputs for a Feature Pyramid Network (FPN) [70].The FPN is composed of two convolutional layers for each input: a lateral connection with 1 × 1 filters to reduce the number of channels to 256, and a top-down pathway with a 3 × 3 convolutional layer with 256 filters.The lateral connections process each input map, while the top-down pathway refines the feature maps, which are combined with the lateral outputs via summation.The final feature map is processed with another 3 × 3 convolutional layer and a 1 × 1 layer that proposes the regions of interest.</p>
<p>Two-stage detectors like Faster R-CNN operate in two phases: first generating region proposals (potential bounding boxes) and then refining and classifying these regions.By contrast, FCOS is a single-stage detector directly predicting bounding boxes and class scores from feature maps in a single pass.FCOS eliminates the need for anchor boxes and proposals, streamlining the detection process.It uses a fully convolutional approach to predict bounding box coordinates for each pixel in the feature map.Key components of FCOS include a FPN that generates feature maps at multiple scales with strides of 8, 16, 32, 64, and 128.Each level of the feature pyramid passes through shared heads comprising three parallel branches: classification, bounding box regression, and centerness.The classification branch outputs class scores, while the regression branch predicts the offsets for the bounding boxes.The centerness branch predicts a score indicating how close a pixel is to the center of an object, helping to down-weight lowquality bounding boxes far from object centers.</p>
<p>The MobileNetV3-Large architecture consists of a series of convolutional layers and bottleneck blocks designed for efficient mobile performance.It begins with a standard 3x3 convolutional layer with 16 output channels, followed by 16 bottleneck blocks with varying expansion sizes and kernel sizes, alternating between 3x3 and 5x5 kernels.Some of these blocks incorporate the Squeeze-and-Excite module, which enhances the network's representational power by adaptively recalibrating channel-wise feature responses through global average pooling and fully connected layers, culminating in a sigmoid activation.Following the bottleneck layers, the architecture includes a 1x1 convolutional layer with 960 output channels, a 7x7 pooling layer, and another 1x1 convolutional layer without batch normalization, expanding to 1280 output channels.</p>
<p>[1] Harshvardhan GM, Mahendra Kumar Gourisaria, Manjusha Pandey, and Siddharth Swarup Rautaray.A comprehensive survey and analysis of generative models in machine learning.</p>
<p>Figure 1 :
1
Figure 1: Transfer learning for object detection with generative models.We employ a L2I pretrained model to generate images for transfer learning to an object detector.We can filter out suboptimal generated images based on benchmark metrics.For instance, the image along the red arrow is discarded because the generative model has depicted many cars outside the bounding boxes designated in the grounding instruction.With the remaining generated images, we pretrain the object detector, followed by a fine-tuning on the real dataset.Dashed lines indicate the data used for training the models.</p>
<p>A target is properly generated, and the filter correctly identifies it with IoU &gt; 0.5 as TP, thus Precision and Recall are both 1.(d) A target is wrongly generated on the left.The filter classifies it with IoU = 0 as FP, thus Precision and Recall are 0.5 and 1, respectively.(e)The target on the right is not well generated.The filter cannot detect it: it counts as FN, thus Precision and Recall are 1 and 0.5, respectively.</p>
<p>Figure 2 :
2
Figure 2: We calculate IoU as defined in (a) to evaluate the overlap (b) between the ground truth (in green) and the predicted boxes (in red).This is used to implement a Precision-Recall filter to automatically identify faithful representations (c) or discrepancies (d, e) between the intended ground truth for generation and the actual generated images.</p>
<p>Figure 3 :
3
Figure 3: Fish instances are positioned in identical bounding boxes using three different methods: employing as the GLIGEN grounding entities the text phrase "a fish" (a) or an image of a real fish, as shown in the box in the top right corner of (b); pasting DeepFish masks onto an OzFish background (c).</p>
<p>Figure 6 :
6
Figure 6: Comparison of the Faster R-CNN model used in Fig. 5 with other models.All tests use 300 real images with pretraining on text-grounded generated images.</p>
<p>Figure 7 :
7
Figure 7: Side-by-side illustrations comparing examples of object detector predictions (in red) with the corresponding testing ground truth (in green) under our two scenarios.In the left column (a, c), Faster R-CNN models are initialized with ImageNet and finetuned using a minimal dataset of 300 real training images.On the right, models undergo a preliminary pretraining on 9 000 generated images (from the image grounding dataset in the OzFish case), followed by a fine-tuning on the same 300 real images.We can observe that our approach shows increased performances in finding more targets and refining redundant detection boxes, even in rainy or turbid environments.</p>
<p>. acknowledges the contribution of the MAREA project funded by the Tuscany Region.S.M. acknowledges financial support from PNRR MUR project PE0000023-NQSTI.C.G. acknowledges the contribution of the National Recovery and Resilience Plan, Mission 4 Component 2 -Investment 1.4 -CN_00000013 "CENTRO NAZIONALE HPC, BIG DATA E QUANTUM COMPUTING", spoke 6. C.G. and M.P. are members of the INdAM research group GNCS.The INdAM-GNCS support is gratefully acknowledged.F.C. acknowledges financial support by the European Commission's Horizon Europe Framework Programme under the Research and Innovation Action GA n. 101070546-MUQUABIS, by the European Union's Horizon 2020 research and innovation programme under FET-OPEN GA n. 828946-PATHOS, by the European Defence Agency under the project Q-LAMPS Contract No. B PRJ-RT-989, and by the MUR Progetti di Ricerca di Rilevante Interesse Nazionale (PRIN) Bando 2022 -project n. 20227HSE83 (ThAI-MIA) funded by the European Union-Next Generation EU.</p>
<p>Real im. Generated im. Prec. Rec. F1 mAP
NuImages OzFish50 1 500 50 1 5000 9 000 0 9 000 0 9 000 0 9 0000 0.51 0.35 0.52 0 0.48 0.41 0.570 0.56 0.53 -0.54 0.42 0.67 0.59 0 -0.51 0.50 0.54 0.46 0.65 0.610.74e-6 0.55 0.43 0.66 0.024 0.48 0.46 0.64</p>
<p>Computer Science Review, 38:100285, 2020.[2] Lars Ruthotto and Eldad Haber.An introduction to deep generative modeling.GAMM-Mitteilungen, 44(2):e202100008, 2021.[3] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang.Diffusion models: A comprehensive survey of methods and applications.ACM Comput.Surv., 56(4), nov 2023.[4] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah.Diffusion models in vision: A survey.IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(9):10850-10869, 2023.[5] Zhengxia Zou, Keyan Chen, Zhenwei Shi, Yuhong Guo, and Jieping Ye.Object detection in 20 years: A survey.Proceedings of the IEEE, 111(3):257-276, 2023.[6] Abolfazl Farahani, Sahar Voghoei, Khaled Rasheed, and Hamid R Arabnia.A brief review of domain adaptation.Advances in data science and information engineering: proceedings from ICDATA 2020 and IKE 2020, pages 877-894, 2021.[7] Connor Shorten and Taghi M Khoshgoftaar.A survey on image data augmentation for deep learning.Journal of big data, 6(1):1-48, 2019.[8] Adrien Gaidon, Qiao Wang, Yohann Cabon, and Eleonora Vig.Virtual worlds as proxy for multiobject tracking analysis.In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4340-4349, 2016.[9] Hajer Mguidich, Bachir Zoudji, and Aïmen Khacharem.Does imagination enhance learning?a systematic review and meta-analysis.European Journal of Psychology of Education, pages 1-36,</p>
<p>See the repository https://github.com/mseitzer/pytorch-fid .</p>
<p>Imagine, and you will find-lack of attentional guidance through visual imagery in aphantasics. Merlin Monzel, Kristof Keidel, Martin Reuter, Perception, &amp; Psychophysics. 832021Attention</p>
<p>Where's wanda? the influence of visual imagery vividness on visual search speed measured by means of hidden object pictures. Attention, Perception, &amp; Psychophysics. Merlin Monzel, Martin Reuter, 2023</p>
<p>Training deep networks with synthetic data: Bridging the reality gap by domain randomization. Jonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, Varun Jampani, Cem Anil, Thang To, Eric Cameracci, Shaad Boochoon, Stan Birchfield, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) WorkshopsJune 2018</p>
<p>Cut, paste and learn: Surprisingly easy synthesis for instance detection. Debidatta Dwibedi, Ishan Misra, Martial Hebert, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2017</p>
<p>A comprehensive survey on transfer learning. Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, Qing He, Proceedings of the IEEE. 10912021</p>
<p>Psychology of high-school subjects. Charles Hubbard, Judd , 1915Ginn</p>
<p>Theoretical aspects of learning and transfer of training. A William, Brownell, Review of Educational Research. 631936</p>
<p>Emerging trends: A gentle introduction to fine-tuning. Kenneth Ward Church, Zeyu Chen, Yanjun Ma, Natural Language Engineering. 2762021</p>
<p>Brain tumor classification using deep cnn features via transfer learning. S Deepak, Ameer, Computers in biology and medicine. 1111033452019</p>
<p>Classification of the covid-19 infected patients using densenet201 based deep transfer learning. Aayush Jaiswal, Neha Gianchandani, Dilbag Singh, Vijay Kumar, Manjit Kaur, Journal of Biomolecular Structure and Dynamics. 39152021</p>
<p>Classification and unsupervised clustering of ligo data with deep transfer learning. Daniel George, Hongyu Shen, E A Huerta, Phys. Rev. D. 97101501May 2018</p>
<p>Enhancing materials property prediction by leveraging computational and experimental data using deep transfer learning. Dipendra Jha, Kamal Choudhary, Francesca Tavazza, Wei-Keng Liao, Alok Choudhary, Carelyn Campbell, Ankit Agrawal, Nature communications. 10153162019</p>
<p>A review of deep transfer learning and recent advancements. Mohammadreza Iman, Hamid Reza Arabnia, Khaled Rasheed, 112023Technologies</p>
<p>A survey on heterogeneous transfer learning. Oscar Day, M Taghi, Khoshgoftaar, Journal of Big Data. 42017</p>
<p>Heterogeneous domain adaptation: An unsupervised approach. Feng Liu, Guangquan Zhang, Jie Lu, IEEE transactions on neural networks and learning systems. 202031</p>
<p>Cartoon face recognition: A benchmark dataset. Yi Zheng, Yifan Zhao, Mengyuan Ren, He Yan, Xiangju Lu, Junhui Liu, Jia Li, Proceedings of the 28th ACM international conference on multimedia. the 28th ACM international conference on multimedia2020</p>
<p>A comprehensive review of image analysis methods for microorganism counting: from classical image processing to deep learning approaches. Jiawei Zhang, Chen Li, Md Mamunur Rahaman, Yudong Yao, Pingli Ma, Jinghua Zhang, Xin Zhao, Tao Jiang, Marcin Grzegorzek, Artificial Intelligence Review. 2022</p>
<p>Counting in the wild. Carlos Arteta, Victor Lempitsky, Andrew Zisserman, Computer Vision -ECCV 2016. Bastian Leibe, Jiri Matas, Nicu Sebe, Max Welling, ChamSpringer International Publishing2016</p>
<p>Automated bird counting with deep learning for regional bird distribution mapping. Bekir Hüseyin Gökhan Akçay, Duygugül Kabasakal, Nusret Aksu, Melih Demir, Ali Öz, Erdoğan, Animals. 10712072020</p>
<p>Deep regression versus detection for counting in robotic phenotyping. Adrian Salazar Gomez, Erchan Aptoula, Simon Parsons, Petra Bosilj, IEEE Robotics and Automation Letters. 622021</p>
<p>A deep learning method for accurate and fast identification of coral reef fishes in underwater images. Sébastien Villon, David Mouillot, Marc Chaumont, Emily S Darling, Gérard Subsol, Thomas Claverie, Sébastien Villéger, Ecological Informatics. 482018</p>
<p>Tracking fish abundance by underwater image recognition. Simone Marini, Emanuela Fanelli, Valerio Sbragaglia, Ernesto Azzurro, Joaquin Del Rio, Jacopo Fernandez, Aguzzi, Scientific reports. 81137482018</p>
<p>NorFisk: fish image dataset from Norwegian fish farms for species recognition using deep neural networks. Alberto Maximiliano Crescitelli, Lars Christian Gansel, Houxiang Zhang, Modeling, Identification and Control. 4212021</p>
<p>Accelerating species recognition and labelling of fish from underwater video with machine-assisted deep learning. Daniel Marrable, Kathryn Barker, Sawitchaya Tippaya, Mathew Wyatt, Scott Bainbridge, Marcus Stowar, Jason Larke, Frontiers in Marine Science. 92022</p>
<p>Annotated video footage for automated identification and counting of fish in unconstrained seagrass habitats. Rod M Ellen M Ditria, Eric L Connolly, Sebastian Jinks, Lopez-Marcano, Frontiers in Marine Science. 86294852021</p>
<p>Autonomous temporal pseudo-labeling for fish detection. Ricardo Jm Veiga, Iñigo E Ochoa, Adela Belackova, Luís Bentes, P João, Jorge Silva, João Mf Semião, Rodrigues, Applied Sciences. 121259102022</p>
<p>Image dataset for benchmarking automated fish detection and classification algorithms. Marco Francescangeli, Simone Marini, Enoc Martínez, Joaquín Del Río, Marc Daniel M Toma, Jacopo Nogueras, Aguzzi, Scientific data. 10152023</p>
<p>Foids: Bio-inspired fish simulation for generating synthetic datasets. Yuko Ishiwaka, Xiao S Zeng, Michael Lee Eastman, Sho Kakazu, Sarah Gross, Ryosuke Mizutani, Masaki Nakada, ACM Trans. Graph. 406dec 2021</p>
<p>Brackishmot: The brackish multi-object tracking dataset. Malte Pedersen, Daniel Lehotský, Ivan Nikolov, Thomas B Moeslund, Image Analysis. Rikke Gade, Michael Felsberg, Joni-Kristian Kämäräinen, ChamSpringer Nature Switzerland2023</p>
<p>Transferring deep knowledge for object recognition in low-quality underwater videos. Xin Sun, Junyu Shi, Lipeng Liu, Junyu Dong, Claudia Plant, Xinhua Wang, Huiyu Zhou, Neurocomputing. 2752018</p>
<p>This dataset does not exist: training models from generated images. Victor Besnier, Himalaya Jain, Andrei Bursuc, Matthieu Cord, Patrick Pérez, ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE2020</p>
<p>Unsupervised adversarial domain adaptation based on interpolation image for fish detection in aquaculture. Tengyun Zhao, Zhencai Shen, Hui Zou, Ping Zhong, Yingyi Chen, Computers and Electronics in Agriculture. 1981070042022</p>
<p>Auggan: Cross domain adaptation with gan-based data augmentation. Sheng-Wei Huang, Che-Tsung Lin, Shu-Ping Chen, Yen-Yi Wu, Po-Hao Hsu, Shang-Hong Lai, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)2018</p>
<p>Image generation from layout. Bo Zhao, Lili Meng, Weidong Yin, Leonid Sigal, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019</p>
<p>Gligen: Open-set grounded text-to-image generation. Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, Yong Jae Lee, 2023</p>
<p>Integrating geometric control into text-to-image diffusion models for high-quality detection data generation via text prompt. Kai Chen, Enze Xie, Zhe Chen, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung, arXiv:2306.046072023arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR2021</p>
<p>Faster r-cnn: Towards real-time object detection with region proposal networks. Kaiming Shaoqing Ren, Ross He, Jian Girshick, Sun, Advances in neural information processing systems. 282015</p>
<p>Microsoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, Lawrence Zitnick, Computer Vision-ECCV 2014: 13th European Conference. Zurich, SwitzerlandSpringerSeptember 6-12, 2014. 2014Proceedings, Part V 13</p>
<p>On pre-trained image features and synthetic images for deep learning. Stefan Hinterstoisser, Vincent Lepetit, Paul Wohlhart, Kurt Konolige, Proceedings of the European Conference on Computer Vision (ECCV) Workshops. the European Conference on Computer Vision (ECCV) Workshops2018</p>
<p>Deep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>Imagenet large scale visual recognition challenge. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, International journal of computer vision. 1152015</p>
<p>What makes imagenet good for transfer learning?. Minyoung Huh, Pulkit Agrawal, Alexei A Efros, arXiv:1608.086142016arXiv preprint</p>
<p>On the importance of initialization and momentum in deep learning. Ilya Sutskever, James Martens, George Dahl, Geoffrey Hinton, International conference on machine learning. PMLR2013</p>
<p>Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter, 201730</p>
<p>Rethinking the inception architecture for computer vision. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>A survey on performance metrics for object-detection algorithms. R Padilla, S L Netto, E A B Da Silva, 2020 International Conference on Systems, Signals and Image Processing (IWSSIP). 2020</p>
<p>Vision meets robotics: The kitti dataset. Andreas Geiger, Philip Lenz, Christoph Stiller, Raquel Urtasun, The International Journal of Robotics Research. 32112013</p>
<p>nuscenes: A multimodal dataset for autonomous driving. Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, Oscar Beijbom, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Deep multi-modal object detection and semantic segmentation for autonomous driving: Datasets, methods, and challenges. Di Feng, Christian Haase-Schütz, Lars Rosenbaum, Heinz Hertlein, Claudius Glaeser, Fabian Timm, Werner Wiesbeck, Klaus Dietmayer, IEEE Transactions on Intelligent Transportation Systems. 2232020</p>
<p>Computer vision models in intelligent aquaculture with emphasis on fish detection and behavior analysis: A review. Ling Yang, Yeqi Liu, Huihui Yu, Xiaomin Fang, Lihua Song, Daoliang Li, Yingyi Chen, Archives of Computational Methods in Engineering. 282021</p>
<p>Fang-Pang Lin, et al. Fish4Knowledge: collecting and analyzing massive coral reef fish video data. Yun-Heh Robert B Fisher, Daniela Chen-Burger, Lynda Giordano, Hardman, 2016Springer104</p>
<p>Automated detection of rockfish in unconstrained underwater videos using haar cascades and a new image dataset: Labeled fishes in the wild. George Cutter, Kevin Stierhoff, Jiaming Zeng, 2015 IEEE Winter Applications and Computer Vision Workshops. IEEE2015</p>
<p>A realistic fish-habitat dataset to evaluate algorithms for underwater visual analysis. Alzayat Saleh, H Issam, Dmitry A Laradji, Michael Konovalov, David Bradley, Marcus Vazquez, Sheaves, Scientific Reports. 101146712020</p>
<p>Automating the analysis of fish abundance using object detection: optimizing animal ecology with deep learning. Ellen M Ditria, Sebastian Lopez-Marcano, Michael Sievers, Eric L Jinks, Christopher J Brown, Rod M Connolly, Frontiers in Marine Science. 4292020</p>
<p>Ozfish dataset -machine learning dataset for baited remote underwater video stations. 10.25845/5e28f062c50972019. Feb-2024Australian Institute of Marine Science (AIMS), University of Western Australia (UWA), and Curtin University</p>
<p>A full data augmentation pipeline for small object detection based on generative adversarial networks. Brais Bosquet, Daniel Cores, Lorenzo Seidenari, M Víctor, Manuel Brea, Alberto Del Mucientes, Bimbo, Pattern Recognition. 1331089982023</p>
<p>Searching for mobilenetv3. Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2019</p>
<p>Fcos: Fully convolutional one-stage object detection. Tian, H Shen, Chen, He, arXiv:1904.013552019. 1904arXiv preprint</p>
<p>Zihao Zhao, Yuxiao Liu, Han Wu, Yonghao Li, Sheng Wang, Lin Teng, Disheng Liu, Xiang Li, Zhiming Cui, Qian Wang, arXiv:2312.07353Clip in medical imaging: A comprehensive survey. 2023arXiv preprint</p>
<p>Feature pyramid networks for object detection. Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, Serge Belongie, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>            </div>
        </div>

    </div>
</body>
</html>
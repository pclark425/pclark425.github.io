<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9969 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9969</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9969</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-d55ed10e6a77e8f0a2359eb92221915f56481843</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d55ed10e6a77e8f0a2359eb92221915f56481843" target="_blank">Benchmarking Cognitive Biases in Large Language Models as Evaluators</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work assembles 15 LLMs of four different size ranges and evaluates their output responses by preference ranking from the other LLMs as evaluators, finding that LLMs are biased text quality evaluators.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models are cognitively biased judges. Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 15 LLMs of four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased Overlap (RBO) score to be 49.6%, indicating that machine preferences are misaligned with humans. According to our findings, LLMs may still be unable to be utilized for automatic annotation aligned with human preferences. Our project page is at: https://minnesotanlp.github.io/cobbler.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9969.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9969.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Overall human-LLM agreement</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Aggregate agreement between human judgments and LLM-as-a-judge evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Quantifies how well LLM evaluators' rankings align with human preferences in the QA setting; the paper reports a modest-to-low alignment indicating systematic differences between machine and human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Question Answering (long-form QA; ELI5 and BigBench/strategyQA)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>16 LLMs evaluated (GPT-4, ChatGPT, InSTRUCTGPT, LLAMAv2, LLAMA, COHERE, FALCON, ALPACA, VICUNA, OPENASSIST, MISTRAL, OLMO, BAIZE, KOALA, WIZARDLM, MPT)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Pairwise evaluation of anonymized or named model responses across 50 QA instructions; each unique pair shown in both orderings; evaluators prompted to output 'System _ is better' using the templates in Appendix C; induced-bias variants add fake-statistic or distraction lines.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Six crowdworkers on Amazon Mechanical Turk provided ranked preferences (N-rankwise and pairwise studies); 300 annotations cited for some experiments; pairwise bias subset used 750 sampled pairs with normalization described in Appendix D.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Rank-Biased Overlap (RBO). Reported average RBO between human preferences and model evaluations = 0.44; average RBO among human annotators = 0.54 (N=6), indicating human-human agreement > human-LLM agreement. (Sections 4.3, 5.2)</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Using LLMs as judges yields poorer alignment with human judgments (RBO drop to 0.44), meaning LLM-ranked quality differs from human-perceived quality; LLMs amplify cognitive biases and are more influenced by presentation and spurious cues than humans, reducing trustworthiness as substitutes for human annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Aggregate misalignment: average RBO 0.44 vs human-human 0.54 (Section 5.2). Many LLMs produced biased selections across ~40% of comparisons (abstract, Fig.2, Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Some models (e.g., MISTRAL, ChatGPT) achieved higher RBOs and thus better alignment with humans (Fig.3, Section 5.2); model size correlates with partial improvements (>40B and >100B groups show higher average RBOs). Valid-response-rate issues mean comparisons exclude invalid outputs, and confounding factors (e.g., stronger models genuinely generating better outputs) complicate direct interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Sections 4.3 and 5.2; Figure 3; Tables 2 and 5</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking Cognitive Biases in Large Language Models as Evaluators', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9969.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9969.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bias amplification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Amplification of cognitive biases when LLMs act as evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLM evaluators exhibit and often amplify human-like cognitive biases (order, compassion/name effects, egocentricity, salience/length preference) and induced biases (bandwagon and attentional), leading to systematically skewed evaluations relative to unbiased or human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>QA response quality evaluation (pairwise comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Same 16 LLM pool; biases measured per-evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Implicit-bias prompts (pairwise anonymized or named comparisons) and induced-bias prompts (added fake-statistics or distraction sentences) as given in Appendix C; each pair evaluated twice in both orders to detect order effects.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human pairwise bias study: 6 annotators, 750 sampled pairs per induced-bias experiment; human bias proportions computed and compared to LLM proportions (Section 4.3).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Bias proportions and comparison to RANDOM baseline; statistical significance via two-sample Z tests of proportions (Appendix B.5). Aggregate metric: roughly 40% of LLM comparisons show bias across benchmarks (Abstract, Results).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>What is lost is impartiality and robustness: LLM-as-judges are more likely than humans to be swayed by superficial or contextual artifacts (ordering, names, prefabricated majority claims, irrelevant details), producing evaluations that do not reflect intrinsic text quality but reflect prompt artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Order bias: 11/15 models showed first- or last-order preference (Table 2). Compassion Fade (real model names) dramatically changes preferences compared to anonymized names (Table 2, Sec 5.1). Induced Bandwagon: ~11/15 models followed the fake '85% prefer X' statistic >70% of the time (Sec 5.1; Table 6/7). Attentional bias: many models were distracted by irrelevant sentences and preferred the mentioned system or had lower valid-response rates (Sec 5.1).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Not all models are equally susceptible: GPT-4 often resisted bandwagon (Table 6 shows GPT-4 = 0.0 for bandwagon in reported runs), and API-based systems (ChatGPT, COHERE) were more robust to attentional distractions (Sec 5.1). Humans also show biases (Table 4) but generally to a lesser extent.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Sections 3, 5.1; Table 1; Table 2; Tables 6-7; Appendix C</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking Cognitive Biases in Large Language Models as Evaluators', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9969.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9969.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bandwagon susceptibility</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM sensitivity to fake majority-statistics (Bandwagon Effect)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When evaluation prompts include a statement like '85% of people prefer System X,' many LLM evaluators follow that cue and choose the asserted majority winner regardless of response quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>QA pairwise comparison with injected social-statistic cue</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Multiple models tested; majority (11/15) heavily influenced according to the paper</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Prompts appended with a sentence '85% of people believe that {system} is better.' Evaluator asked to choose which system is more coherent (Appendix C.3).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human pairwise study measured susceptibility to bandwagon; humans were less affected (Table 4: human bandwagon proportion 0.47) than many LLMs (e.g., Vicuna 0.81 in Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Proportion of evaluations where the model chose the stated majority-preferred system; models > random threshold flagged as influenced. Table 6/7 and Sec 5.1 report many models >70% tendency.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Using LLMs as judges loses resistance to explicit but irrelevant social cues — LLMs may substitute a stated crowd preference for independent quality assessment, producing decisions misaligned with actual content quality and human independent judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Table 6 shows ChatGPT, InSTRUCTGPT, COHERE, and many others selecting the bandwagon target at high rates for an 85% statistic. Appendix B.1 further shows preferences change when the statistic is changed to 0% or randomized between 50–85%, evidencing reliance on the injected statistic.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>GPT-4 showed immunity in the reported runs (0.0 for the 85% bandwagon condition in Table 6). Vicuna showed variable sensitivity (less affected by the numeric value in some tests), indicating implementation/prompting differences can matter.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 3.2, Section 5.1; Table 6; Table 7; Appendix B.1; Appendix C.3</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking Cognitive Biases in Large Language Models as Evaluators', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9969.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9969.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Attentional/distraction bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM susceptibility to irrelevant contextual distractions in evaluation prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Adding meaningless, off-topic sentences about one system (e.g., 'System Star likes to eat oranges and apples') causes many LLM evaluators to prefer or be distracted by that system, degrading evaluation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>QA pairwise comparison with irrelevant contextual distraction</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Multiple LLMs tested; third size group (~10B) most impacted, API-based models more robust</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Evaluation prompts include a distraction sentence appended after the pair (Appendix C.4). Models judged which response is more coherent.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human pairwise study measured attentional bias; humans exhibited lower effect (Table 4 human attentional proportion 0.35) than many LLMs (e.g., Vicuna 0.78 in Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Proportion of evaluations where the model preferred the system mentioned in the distraction or had a significantly reduced valid response rate (Section 5.1).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>LLM judges lose robustness to irrelevant context, allowing spurious textual artifacts to shift decisions — unlike humans who are less often swayed by arbitrary side information in this task, many LLMs over-attend to such cues.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Sec 5.1 reports >80% distracted evaluations for some ~10B models; Table 2 shows attentional bias proportions high for many mid-size models (e.g., ALPACA 0.81, VICUNA 0.78 for Attn. in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>ChatGPT and COHERE were reported to remain relatively robust to distraction in rankings (Sec 5.1); smaller models and some instruction-tuned variants varied in sensitivity. Valid-response filtering excludes some invalid outputs, so measured effects are on valid outputs only.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 3.2; Section 5.1; Table 2; Appendix C.4</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking Cognitive Biases in Large Language Models as Evaluators', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9969.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9969.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Order and naming effects</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Order bias and Compassion Fade (name-recognition) differences between LLMs and humans</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs show strong order preferences (first/last) and are strongly influenced by recognizable model names versus anonymized aliases, causing shifts in evaluation that humans do not show to the same degree.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Pairwise QA evaluation; prompt-order and presence/absence of model names</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Many tested LLMs; 11/15 showed strong order bias; performance varied by size group</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Each pair shown in both orderings; separate prompts use anonymized 'System Star/System Square' or real model names (Appendix C.1 and C.2).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human pairwise bias experiments measured order and naming effects; human ORDER bias proportion = 0.20 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Proportion of biased selections (first or last) and comparison to random threshold. Statistical significance tested with two-sample Z tests of proportions (Appendix B.5).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>LLM judges are more influenced by superficial presentation (positioning and identifiable names), risking non-content-based preference shifts that differ from human evaluators who show lower order/name sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Table 2: many models have high 'First' or 'Last' order proportions (e.g., FALCON 'First' 0.74). Compassion Fade: recognizable names dramatically change preferences vs anonymized aliases (Sec 5.1).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Humans also show nonzero order/name biases but generally smaller (Table 4). Some LLMs differ across sizes: larger models sometimes prefer longer responses; confounding between order/name and content quality exists and authors attempted to control via hierarchical rubrics and BERTScore analyses (Appendix B.4).</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 3.1; Section 5.1; Table 1; Table 2; Appendix B.5</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking Cognitive Biases in Large Language Models as Evaluators', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9969.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9969.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Egocentric vs salience confound</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Confounding between egocentric self-preference and salience/length bias in LLM evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs often prefer their own outputs (egocentric bias) and/or longer outputs (salience bias); these effects can be confounded because stronger models tend to produce longer, higher-quality outputs, complicating whether LLMs are biased or simply preferring objectively better answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>QA response evaluation and analysis of self-preference vs length preference</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Observed across many models; eg. GPT-4, ChatGPT show high egocentric proportions but also tend to produce longer responses</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Implicit evaluation prompts; additional analyses decouple confounders via hierarchical rubrics and BERTScore quality measures (Appendix B.4, Table 3 and Table 11).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human judgments used as a reference to compare whether self-preference or length correlate with human choices; humans also show salience tendencies but less amplified.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Proportions of self-preference (egocentric) and salience (preference for longer/shorter) together with reference-based generation quality (BERTScore F1) to control for actual quality differences. Reported that generations had similar BERTScore ranges (0.81–0.86), suggesting biases aren't solely due to quality differences (Section 5.1.1).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>When LLMs are used as judges, it's difficult to separate legitimate preference for objectively better (longer) responses from egocentric or salience-driven bias; this ambiguity is a loss in interpretability and fairness relative to human adjudication.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Table 3 shows larger models more affected by longer responses; Table 11 and Appendix B.4 show many models prefer their own outputs when those outputs are longer. Authors note difficulty in attributing preference purely to quality vs bias.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Authors attempted decoupling using quality metrics (BERTScore) and hierarchical rubrics; some models flip preferences only slightly when conditioned on quality, indicating partial but not full confounding resolution. The paper cautions this area needs more work.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 5.1.1; Appendix B.4; Table 3; Table 11</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking Cognitive Biases in Large Language Models as Evaluators', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9969.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9969.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Validity & model-size limits</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reduced valid-evaluation rates and scale-dependent weaknesses in LLM evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Several models (particularly some open-source or non-instruction-tuned models) produced many invalid or unparsable evaluation outputs, and smaller models struggled with list-wise ranking complexity, limiting their usability as judges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Pairwise and N-rankwise (N=4, N=13) QA evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>LLAMAv2, LLAMA, OLMO, KOALA and other <40B models showed low valid-response rates per Table 12; many <40B models failed list-wise ranking tasks (Appendix B.8).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Same pairwise prompts; authors post-processed outputs via pattern matching to mark valid vs invalid evaluations (Appendix A/B).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human study served as stable baseline; humans (despite task difficulty) still produced coherent ranked lists with higher IAA than many LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Valid-evaluation ratio across models (Table 12). Models with <50% valid evaluations flagged in Table 12 and excluded from certain bias analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Using weaker or improperly tuned LLMs as judges leads to loss of usable judgments (invalid outputs), decreasing effective coverage and making automated evaluation unreliable; smaller LLMs also fail at more complex ranking tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Table 12 lists many models with low valid rates (Olmo, Koala, LLAMA entries bolded for low validity). Appendix B.8 shows many models cannot produce valid list-wise rankings for N=4.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Instruction-tuned and API models (ChatGPT, GPT-4) maintained high valid rates; scale trends are not uniform (authors report some mid-size models performing well). The authors note that model-specific prompt tuning might recover some validity.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 5.1; Appendix B.6; Table 12; Appendix B.8</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking Cognitive Biases in Large Language Models as Evaluators', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9969.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9969.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Positive exceptions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cases where LLM-as-judge matched or outperformed humans</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Specific LLMs and settings where model judgments were closer to human judgments or robust to particular biases, indicating LLMs can sometimes be reasonable evaluators under controlled conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>QA pairwise evaluation under varied bias prompts</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4, ChatGPT, MISTRAL, COHERE (in select tests)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Same pairwise and induced-bias prompts; models evaluated across implicit and induced biases with valid outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human baseline as previously described (6 annotators, RBO comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>RBO per-model with humans shown in Fig.3: MISTRAL and ChatGPT achieved highest RBOs; GPT-4 often resisted bandwagon in reported runs (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Less loss or degradation observed for top-performing models: alignment closer to human preference, lower susceptibility to certain induced biases (e.g., some API models resisted attentional bias and bandwagon in specific runs).</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>GPT-4 bandwagon = 0.0 in Table 6; MISTRAL and ChatGPT had highest RBOs with humans (Fig.3, Section 5.2). API-based systems were more robust to distractions (Section 5.1).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>These successes are not universal across all bias types or prompts; robustness varied by the exact injected cue and by model/version. Authors emphasize current overall result: majority of tested LLMs still show problematic biases.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 5.2; Figure 3; Table 6; Section 5.1</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking Cognitive Biases in Large Language Models as Evaluators', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Benchmarking foundation models with language-model-as-an-examiner <em>(Rating: 2)</em></li>
                <li>Judging 1lm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Alpacaeval: An automatic evaluator of instructionfollowing models <em>(Rating: 2)</em></li>
                <li>Style over substance: Evaluation biases for large language models <em>(Rating: 2)</em></li>
                <li>Large language models are not fair evaluators <em>(Rating: 2)</em></li>
                <li>Large language models can be easily distracted by irrelevant context <em>(Rating: 1)</em></li>
                <li>Are large language models good evaluators for abstractive summarization? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9969",
    "paper_id": "paper-d55ed10e6a77e8f0a2359eb92221915f56481843",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "Overall human-LLM agreement",
            "name_full": "Aggregate agreement between human judgments and LLM-as-a-judge evaluations",
            "brief_description": "Quantifies how well LLM evaluators' rankings align with human preferences in the QA setting; the paper reports a modest-to-low alignment indicating systematic differences between machine and human judgments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Question Answering (long-form QA; ELI5 and BigBench/strategyQA)",
            "llm_judge_model": "16 LLMs evaluated (GPT-4, ChatGPT, InSTRUCTGPT, LLAMAv2, LLAMA, COHERE, FALCON, ALPACA, VICUNA, OPENASSIST, MISTRAL, OLMO, BAIZE, KOALA, WIZARDLM, MPT)",
            "llm_judge_setup": "Pairwise evaluation of anonymized or named model responses across 50 QA instructions; each unique pair shown in both orderings; evaluators prompted to output 'System _ is better' using the templates in Appendix C; induced-bias variants add fake-statistic or distraction lines.",
            "human_evaluation_setup": "Six crowdworkers on Amazon Mechanical Turk provided ranked preferences (N-rankwise and pairwise studies); 300 annotations cited for some experiments; pairwise bias subset used 750 sampled pairs with normalization described in Appendix D.",
            "agreement_metric": "Rank-Biased Overlap (RBO). Reported average RBO between human preferences and model evaluations = 0.44; average RBO among human annotators = 0.54 (N=6), indicating human-human agreement &gt; human-LLM agreement. (Sections 4.3, 5.2)",
            "losses_identified": "Using LLMs as judges yields poorer alignment with human judgments (RBO drop to 0.44), meaning LLM-ranked quality differs from human-perceived quality; LLMs amplify cognitive biases and are more influenced by presentation and spurious cues than humans, reducing trustworthiness as substitutes for human annotators.",
            "examples_of_loss": "Aggregate misalignment: average RBO 0.44 vs human-human 0.54 (Section 5.2). Many LLMs produced biased selections across ~40% of comparisons (abstract, Fig.2, Table 2).",
            "counterexamples_or_caveats": "Some models (e.g., MISTRAL, ChatGPT) achieved higher RBOs and thus better alignment with humans (Fig.3, Section 5.2); model size correlates with partial improvements (&gt;40B and &gt;100B groups show higher average RBOs). Valid-response-rate issues mean comparisons exclude invalid outputs, and confounding factors (e.g., stronger models genuinely generating better outputs) complicate direct interpretation.",
            "paper_reference": "Sections 4.3 and 5.2; Figure 3; Tables 2 and 5",
            "uuid": "e9969.0",
            "source_info": {
                "paper_title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Bias amplification",
            "name_full": "Amplification of cognitive biases when LLMs act as evaluators",
            "brief_description": "LLM evaluators exhibit and often amplify human-like cognitive biases (order, compassion/name effects, egocentricity, salience/length preference) and induced biases (bandwagon and attentional), leading to systematically skewed evaluations relative to unbiased or human judgments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "QA response quality evaluation (pairwise comparisons)",
            "llm_judge_model": "Same 16 LLM pool; biases measured per-evaluator",
            "llm_judge_setup": "Implicit-bias prompts (pairwise anonymized or named comparisons) and induced-bias prompts (added fake-statistics or distraction sentences) as given in Appendix C; each pair evaluated twice in both orders to detect order effects.",
            "human_evaluation_setup": "Human pairwise bias study: 6 annotators, 750 sampled pairs per induced-bias experiment; human bias proportions computed and compared to LLM proportions (Section 4.3).",
            "agreement_metric": "Bias proportions and comparison to RANDOM baseline; statistical significance via two-sample Z tests of proportions (Appendix B.5). Aggregate metric: roughly 40% of LLM comparisons show bias across benchmarks (Abstract, Results).",
            "losses_identified": "What is lost is impartiality and robustness: LLM-as-judges are more likely than humans to be swayed by superficial or contextual artifacts (ordering, names, prefabricated majority claims, irrelevant details), producing evaluations that do not reflect intrinsic text quality but reflect prompt artifacts.",
            "examples_of_loss": "Order bias: 11/15 models showed first- or last-order preference (Table 2). Compassion Fade (real model names) dramatically changes preferences compared to anonymized names (Table 2, Sec 5.1). Induced Bandwagon: ~11/15 models followed the fake '85% prefer X' statistic &gt;70% of the time (Sec 5.1; Table 6/7). Attentional bias: many models were distracted by irrelevant sentences and preferred the mentioned system or had lower valid-response rates (Sec 5.1).",
            "counterexamples_or_caveats": "Not all models are equally susceptible: GPT-4 often resisted bandwagon (Table 6 shows GPT-4 = 0.0 for bandwagon in reported runs), and API-based systems (ChatGPT, COHERE) were more robust to attentional distractions (Sec 5.1). Humans also show biases (Table 4) but generally to a lesser extent.",
            "paper_reference": "Sections 3, 5.1; Table 1; Table 2; Tables 6-7; Appendix C",
            "uuid": "e9969.1",
            "source_info": {
                "paper_title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Bandwagon susceptibility",
            "name_full": "LLM sensitivity to fake majority-statistics (Bandwagon Effect)",
            "brief_description": "When evaluation prompts include a statement like '85% of people prefer System X,' many LLM evaluators follow that cue and choose the asserted majority winner regardless of response quality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "QA pairwise comparison with injected social-statistic cue",
            "llm_judge_model": "Multiple models tested; majority (11/15) heavily influenced according to the paper",
            "llm_judge_setup": "Prompts appended with a sentence '85% of people believe that {system} is better.' Evaluator asked to choose which system is more coherent (Appendix C.3).",
            "human_evaluation_setup": "Human pairwise study measured susceptibility to bandwagon; humans were less affected (Table 4: human bandwagon proportion 0.47) than many LLMs (e.g., Vicuna 0.81 in Table 4).",
            "agreement_metric": "Proportion of evaluations where the model chose the stated majority-preferred system; models &gt; random threshold flagged as influenced. Table 6/7 and Sec 5.1 report many models &gt;70% tendency.",
            "losses_identified": "Using LLMs as judges loses resistance to explicit but irrelevant social cues — LLMs may substitute a stated crowd preference for independent quality assessment, producing decisions misaligned with actual content quality and human independent judgment.",
            "examples_of_loss": "Table 6 shows ChatGPT, InSTRUCTGPT, COHERE, and many others selecting the bandwagon target at high rates for an 85% statistic. Appendix B.1 further shows preferences change when the statistic is changed to 0% or randomized between 50–85%, evidencing reliance on the injected statistic.",
            "counterexamples_or_caveats": "GPT-4 showed immunity in the reported runs (0.0 for the 85% bandwagon condition in Table 6). Vicuna showed variable sensitivity (less affected by the numeric value in some tests), indicating implementation/prompting differences can matter.",
            "paper_reference": "Section 3.2, Section 5.1; Table 6; Table 7; Appendix B.1; Appendix C.3",
            "uuid": "e9969.2",
            "source_info": {
                "paper_title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Attentional/distraction bias",
            "name_full": "LLM susceptibility to irrelevant contextual distractions in evaluation prompts",
            "brief_description": "Adding meaningless, off-topic sentences about one system (e.g., 'System Star likes to eat oranges and apples') causes many LLM evaluators to prefer or be distracted by that system, degrading evaluation quality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "QA pairwise comparison with irrelevant contextual distraction",
            "llm_judge_model": "Multiple LLMs tested; third size group (~10B) most impacted, API-based models more robust",
            "llm_judge_setup": "Evaluation prompts include a distraction sentence appended after the pair (Appendix C.4). Models judged which response is more coherent.",
            "human_evaluation_setup": "Human pairwise study measured attentional bias; humans exhibited lower effect (Table 4 human attentional proportion 0.35) than many LLMs (e.g., Vicuna 0.78 in Table 4).",
            "agreement_metric": "Proportion of evaluations where the model preferred the system mentioned in the distraction or had a significantly reduced valid response rate (Section 5.1).",
            "losses_identified": "LLM judges lose robustness to irrelevant context, allowing spurious textual artifacts to shift decisions — unlike humans who are less often swayed by arbitrary side information in this task, many LLMs over-attend to such cues.",
            "examples_of_loss": "Sec 5.1 reports &gt;80% distracted evaluations for some ~10B models; Table 2 shows attentional bias proportions high for many mid-size models (e.g., ALPACA 0.81, VICUNA 0.78 for Attn. in Table 2).",
            "counterexamples_or_caveats": "ChatGPT and COHERE were reported to remain relatively robust to distraction in rankings (Sec 5.1); smaller models and some instruction-tuned variants varied in sensitivity. Valid-response filtering excludes some invalid outputs, so measured effects are on valid outputs only.",
            "paper_reference": "Section 3.2; Section 5.1; Table 2; Appendix C.4",
            "uuid": "e9969.3",
            "source_info": {
                "paper_title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Order and naming effects",
            "name_full": "Order bias and Compassion Fade (name-recognition) differences between LLMs and humans",
            "brief_description": "LLMs show strong order preferences (first/last) and are strongly influenced by recognizable model names versus anonymized aliases, causing shifts in evaluation that humans do not show to the same degree.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Pairwise QA evaluation; prompt-order and presence/absence of model names",
            "llm_judge_model": "Many tested LLMs; 11/15 showed strong order bias; performance varied by size group",
            "llm_judge_setup": "Each pair shown in both orderings; separate prompts use anonymized 'System Star/System Square' or real model names (Appendix C.1 and C.2).",
            "human_evaluation_setup": "Human pairwise bias experiments measured order and naming effects; human ORDER bias proportion = 0.20 (Table 4).",
            "agreement_metric": "Proportion of biased selections (first or last) and comparison to random threshold. Statistical significance tested with two-sample Z tests of proportions (Appendix B.5).",
            "losses_identified": "LLM judges are more influenced by superficial presentation (positioning and identifiable names), risking non-content-based preference shifts that differ from human evaluators who show lower order/name sensitivity.",
            "examples_of_loss": "Table 2: many models have high 'First' or 'Last' order proportions (e.g., FALCON 'First' 0.74). Compassion Fade: recognizable names dramatically change preferences vs anonymized aliases (Sec 5.1).",
            "counterexamples_or_caveats": "Humans also show nonzero order/name biases but generally smaller (Table 4). Some LLMs differ across sizes: larger models sometimes prefer longer responses; confounding between order/name and content quality exists and authors attempted to control via hierarchical rubrics and BERTScore analyses (Appendix B.4).",
            "paper_reference": "Section 3.1; Section 5.1; Table 1; Table 2; Appendix B.5",
            "uuid": "e9969.4",
            "source_info": {
                "paper_title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Egocentric vs salience confound",
            "name_full": "Confounding between egocentric self-preference and salience/length bias in LLM evaluators",
            "brief_description": "LLMs often prefer their own outputs (egocentric bias) and/or longer outputs (salience bias); these effects can be confounded because stronger models tend to produce longer, higher-quality outputs, complicating whether LLMs are biased or simply preferring objectively better answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "QA response evaluation and analysis of self-preference vs length preference",
            "llm_judge_model": "Observed across many models; eg. GPT-4, ChatGPT show high egocentric proportions but also tend to produce longer responses",
            "llm_judge_setup": "Implicit evaluation prompts; additional analyses decouple confounders via hierarchical rubrics and BERTScore quality measures (Appendix B.4, Table 3 and Table 11).",
            "human_evaluation_setup": "Human judgments used as a reference to compare whether self-preference or length correlate with human choices; humans also show salience tendencies but less amplified.",
            "agreement_metric": "Proportions of self-preference (egocentric) and salience (preference for longer/shorter) together with reference-based generation quality (BERTScore F1) to control for actual quality differences. Reported that generations had similar BERTScore ranges (0.81–0.86), suggesting biases aren't solely due to quality differences (Section 5.1.1).",
            "losses_identified": "When LLMs are used as judges, it's difficult to separate legitimate preference for objectively better (longer) responses from egocentric or salience-driven bias; this ambiguity is a loss in interpretability and fairness relative to human adjudication.",
            "examples_of_loss": "Table 3 shows larger models more affected by longer responses; Table 11 and Appendix B.4 show many models prefer their own outputs when those outputs are longer. Authors note difficulty in attributing preference purely to quality vs bias.",
            "counterexamples_or_caveats": "Authors attempted decoupling using quality metrics (BERTScore) and hierarchical rubrics; some models flip preferences only slightly when conditioned on quality, indicating partial but not full confounding resolution. The paper cautions this area needs more work.",
            "paper_reference": "Section 5.1.1; Appendix B.4; Table 3; Table 11",
            "uuid": "e9969.5",
            "source_info": {
                "paper_title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Validity & model-size limits",
            "name_full": "Reduced valid-evaluation rates and scale-dependent weaknesses in LLM evaluators",
            "brief_description": "Several models (particularly some open-source or non-instruction-tuned models) produced many invalid or unparsable evaluation outputs, and smaller models struggled with list-wise ranking complexity, limiting their usability as judges.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Pairwise and N-rankwise (N=4, N=13) QA evaluation",
            "llm_judge_model": "LLAMAv2, LLAMA, OLMO, KOALA and other &lt;40B models showed low valid-response rates per Table 12; many &lt;40B models failed list-wise ranking tasks (Appendix B.8).",
            "llm_judge_setup": "Same pairwise prompts; authors post-processed outputs via pattern matching to mark valid vs invalid evaluations (Appendix A/B).",
            "human_evaluation_setup": "Human study served as stable baseline; humans (despite task difficulty) still produced coherent ranked lists with higher IAA than many LLMs.",
            "agreement_metric": "Valid-evaluation ratio across models (Table 12). Models with &lt;50% valid evaluations flagged in Table 12 and excluded from certain bias analyses.",
            "losses_identified": "Using weaker or improperly tuned LLMs as judges leads to loss of usable judgments (invalid outputs), decreasing effective coverage and making automated evaluation unreliable; smaller LLMs also fail at more complex ranking tasks.",
            "examples_of_loss": "Table 12 lists many models with low valid rates (Olmo, Koala, LLAMA entries bolded for low validity). Appendix B.8 shows many models cannot produce valid list-wise rankings for N=4.",
            "counterexamples_or_caveats": "Instruction-tuned and API models (ChatGPT, GPT-4) maintained high valid rates; scale trends are not uniform (authors report some mid-size models performing well). The authors note that model-specific prompt tuning might recover some validity.",
            "paper_reference": "Section 5.1; Appendix B.6; Table 12; Appendix B.8",
            "uuid": "e9969.6",
            "source_info": {
                "paper_title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Positive exceptions",
            "name_full": "Cases where LLM-as-judge matched or outperformed humans",
            "brief_description": "Specific LLMs and settings where model judgments were closer to human judgments or robust to particular biases, indicating LLMs can sometimes be reasonable evaluators under controlled conditions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "QA pairwise evaluation under varied bias prompts",
            "llm_judge_model": "GPT-4, ChatGPT, MISTRAL, COHERE (in select tests)",
            "llm_judge_setup": "Same pairwise and induced-bias prompts; models evaluated across implicit and induced biases with valid outputs.",
            "human_evaluation_setup": "Human baseline as previously described (6 annotators, RBO comparisons).",
            "agreement_metric": "RBO per-model with humans shown in Fig.3: MISTRAL and ChatGPT achieved highest RBOs; GPT-4 often resisted bandwagon in reported runs (Table 6).",
            "losses_identified": "Less loss or degradation observed for top-performing models: alignment closer to human preference, lower susceptibility to certain induced biases (e.g., some API models resisted attentional bias and bandwagon in specific runs).",
            "examples_of_loss": "GPT-4 bandwagon = 0.0 in Table 6; MISTRAL and ChatGPT had highest RBOs with humans (Fig.3, Section 5.2). API-based systems were more robust to distractions (Section 5.1).",
            "counterexamples_or_caveats": "These successes are not universal across all bias types or prompts; robustness varied by the exact injected cue and by model/version. Authors emphasize current overall result: majority of tested LLMs still show problematic biases.",
            "paper_reference": "Section 5.2; Figure 3; Table 6; Section 5.1",
            "uuid": "e9969.7",
            "source_info": {
                "paper_title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Benchmarking foundation models with language-model-as-an-examiner",
            "rating": 2
        },
        {
            "paper_title": "Judging 1lm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2
        },
        {
            "paper_title": "Alpacaeval: An automatic evaluator of instructionfollowing models",
            "rating": 2
        },
        {
            "paper_title": "Style over substance: Evaluation biases for large language models",
            "rating": 2
        },
        {
            "paper_title": "Large language models are not fair evaluators",
            "rating": 2
        },
        {
            "paper_title": "Large language models can be easily distracted by irrelevant context",
            "rating": 1
        },
        {
            "paper_title": "Are large language models good evaluators for abstractive summarization?",
            "rating": 1
        }
    ],
    "cost": 0.0179675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Benchmarking Cognitive Biases in Large Language Models as Evaluators</h1>
<p>Ryan Koo ${ }^{1}$ Minhwa Lee ${ }^{1}$ Vipul Raheja ${ }^{2}$ Jonginn Park ${ }^{1}$, Zae Myung Kim ${ }^{1}$ Dongyeop Kang ${ }^{1}$<br>${ }^{1}$ University of Minnesota, ${ }^{2}$ Grammarly<br>{koo00017,lee03533, park2838, kim01756, dongyeop}@umn.edu,<br>vipul.raheja@grammarly.com</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 16 LLMs encompassing four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as EvaluatorS (CoBBLER) ${ }^{1}$, a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the EGOCENTRIC bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark ( $\approx \mathbf{4 0 \%}$ of comparisons made by all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased Overlap (RBO) score to be $44 \%$, indicating that machine preferences are misaligned with humans. According to our findings, LLMs may still be unable to be utilized for automatic annotation aligned with human preferences.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) (Brown et al., 2020; Ouyang et al., 2022) adapted to follow various kinds of instructions have been popularly utilized for several natural language tasks. The general standard for testing a model's capabilities is benchmarking its performance on static evaluation suites such as Fan et al. (2019) and Wang et al. (2020). With the increased usage of language models as general-purpose assistants and their artificial nature (Das et al., 2024), current task-specific benchmarks are insufficient to measure the quality of generated texts in the wild.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Recent studies have shown that LLMs can serve as evaluators themselves: Wu and Aji (2023) utilize LLMs as self-evaluators to automatically judge the quality of open-ended generations and compare them with human judgments via an Elo-score calculation. Other works, such as AlpacaEval (Li et al., 2023b), also utilize LLMs, such as GPT-4 (OpenAI, 2023), as automatic evaluators to reduce the time and cost overhead of human annotations. As noted by these works, such automatic evaluation leaderboards have a number of limitations, including a preference for long outputs or outputs that are more similar to the evaluators' generation qualities.</p>
<p>In this work, we propose CoBBLER, the Cognitive Bias Benchmark for evaluating the quality and reliability of LLMs as EvaluatorS, as depicted in Figure 1. We collect a set of 50 questionanswering instructions from two well-established benchmarking datasets: BIGBENCH (Srivastava et al., 2023) and ELi5 (Fan et al., 2019). We then generate responses from 16 open- and closedsource LLMs and conduct a round-robin over every possible unique pair between each of the model responses, prompting each model to evaluate its own and other models' responses.</p>
<p>We then test six different biases to benchmark their evaluation quality and categorize the model biases into two groups: (1) Implicit Biases, which can be implicitly extracted from each model's evaluation via a vanilla prompt, and (2) Induced Biases, which add modifications to the original prompts akin to induce negative behaviors. As shown in Figure 2, we find that the majority of the models strongly exhibit several of the different biases, which may compromise the credibility of their role as evaluators. ${ }^{2}$ Furthermore, we conduct experiments for human preferences by crowdsourcing six human annotators and collecting each of their rankings for a total of 300 annotations. From</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Our CoBBLER pipeline to evaluate popular LLMs that are instruction-tuned or trained with human feedback for their capabilities as unbiased automatic evaluators.</p>
<p>Our findings, we observe a low correlation between human and machine judgments via Rank-Biased Overlap (RBO), indicating that machine and human preferences are generally in low agreement. Our core contributions are as follows:</p>
<ul>
<li>A new benchmark (CoBBLER) for evaluating LLMs to perform unbiased evaluations within the QA setting.</li>
<li>An examination of an exhaustive list of 6 (cognitive) evaluation biases that have not been covered by previous studies. We find that most LLMs cannot perform as unbiased evaluators.</li>
<li>A comprehensive lineup of models (sizing from 3B to &gt;175B parameters) as evaluators, encompassing the current state-of-the-art language models covering over <strong>630k</strong> comparisons.</li>
</ul>
<p>From our benchmark, we find that most models exhibit various cognitive biases when used as automatic evaluators, which may negatively impact evaluation quality. Thus, we propose our benchmark (CoBBLER) for measuring the capabilities of language models as evaluators to enable more reliable evaluations that are well-aligned with human judgment.</p>
<p>We note that our use of <em>biased</em> and <em>unbiased</em> preferences does not allude to the ability to make completely impartial judgments but rather the <em>amplification</em> of human-like biases within language models. As most models are tuned on human data, our study aims to estimate this gap between model and human judgment such that they can be refined more effectively to mitigate against these biases. As such, we also aim for our benchmark to be applied towards the development of future models, as in discovering new gaps or finding that existing gaps are still unresolved.</p>
<h2>2 Related Work</h2>
<p><strong>LLMs as Evaluators.</strong> Owing to the effectiveness of LLMs, many recent research works have investigated their utility in various downstream tasks, such as machine translation <em>Kocmi and Federmann (2023)</em>, summarization <em>Shen et al. (2023); Gao et al. (2023)</em>, code generation <em>Zhuo (2023)</em>, writing assistance <em>Schick et al. (2023); Raheja et al. (2023)</em>, factual consistency <em>Cohen et al. (2023); Gekhman et al. (2023); Luo et al. (2023)</em>, and more. Additionally, many studies have leveraged LLMs for general-purpose NLG evaluation. For instance, <em>Liu et al. (2023); Chen et al. (2023); Wang et al. (2023a)</em> investigated the effectiveness of GPT-4 and ChatGPT against reference-free evaluation methods, whereas <em>Fu et al. (2023)</em> proposed an evaluation framework, GPTScore, to score generated texts. Recently, <em>Li et al. (2023a)</em> and <em>Zheng et al. (2023)</em> conducted similar experiments by employing LLMs as evaluators to judge the quality of generations in a pairwise setting. Although these works present promising results for LLMs as automatic evaluators, our work takes a closer look at machine artifacts that could be detrimental to data quality by benchmarking an exhaustive list of biases impacting LLMs-as-evaluators.</p>
<p><strong>LLM Evaluation Benchmarks.</strong> It is becoming increasingly challenging to evaluate open-source LLMs as they become more powerful and performant. As a result, there has been an increasing need to develop better evaluation benchmarks for measuring the performance of LLMs. However, most of these benchmarks, such as LM-Eval-Harness <em>Gao et al. (2021)</em>, MMLU <em>Hendrycks et al. (2021)</em>, HELM <em>Liang et al. (2022)</em> and BIG-</p>
<table>
<thead>
<tr>
<th>Bias</th>
<th>Bias Behavior</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>Order Bias</td>
<td>The tendency to give preference to an option based on their order (e.g. first, second, or last).</td>
<td>System Star: $x$ System Square: $y$ System Square: $y$ System Star: $x$</td>
</tr>
<tr>
<td>COMPASSION FADE</td>
<td>The tendency to observe different behaviors when given recognizable names as opposed to anonymized aliases.</td>
<td>Model Alpaca: $x$ Model Vicuna: $y$ Model Vicuna: $y$ Model Alpaca: $x$</td>
</tr>
<tr>
<td>EGOCENTRIC BIAS</td>
<td>The inclination to prioritize one's own responses regardless of response quality.</td>
<td>Model Star (You): $x$ Model Square: $y$</td>
</tr>
<tr>
<td>SALIENCE BIAS</td>
<td>The tendency to prefer responses based on the length of the response (i.e., more often preferring longer responses over shorter ones).</td>
<td>System Star: The quick brown fox jumps over the lazy dog. System Square: The fox jumped.</td>
</tr>
<tr>
<td>BANDWAGON EFFECT</td>
<td>The tendency to prefer majority belief without critical evaluation.</td>
<td>85\% believe that System Star is better.</td>
</tr>
<tr>
<td>ATTENTIONAL BIAS</td>
<td>The inclination to give more attention to irrelevant or unimportant details.</td>
<td>System Square likes to eat oranges and apples</td>
</tr>
</tbody>
</table>
<p>Table 1: We display the characteristic format for each bias and bold answers that indicate behavior influenced by the bias. For example, in COMPASSION FADE (recognizable names) Model Alpaca and Model Vicuna are associated with System Star and System Square respectively, in which the preferred response (bolded) is inconsistent with the preferred response from ORDER (anonymized names). Specific prompt details are viewed in Appendix C.</p>
<p>BENCH (Srivastava et al., 2023), only focus on general LLM performance but do not explore their capabilities as evaluators. Our work in this direction overlaps directly with <em>Bai et al. (2023)</em> and <em>Zheng et al. (2023)</em>, who propose a Language-Model-as-an-Examiner benchmark and LLM-as-a-judge to study the capability of LLMs to emulate human preferences. While our experimental setups are similar, we highlight key differences. We cover a wider demographic of current popular language models and an overall different focus on QA as opposed to other domains such as math and reason. Furthermore, our benchmark emphasizes a wider range of biases (implicit/induced) to better describe machine artifacts when used as automatic evaluators. Specifically, COBBLER measures the extent to which each LM-as-evaluator is impacted in each decision by certain artifacts within prompts (i.e., prompting format, prompt information) over a comprehensive list of cognitive biases.</p>
<p>Cognitive Biases in LLMs. While biases have been well-known to exist in LLMs (Wang et al., 2023b; Talboy and Fuller, 2023; Wu and Aji, 2023), many recent works investigating the behaviors of LLMs have also uncovered similarities with cognitive biases. Some recent works (Zhao et al., 2021; Liu et al., 2022; Lu et al., 2022) have shown that the order of training examples in GPT-3 could lead to differences in accuracy between near chance and near state-of-the-art. <em>Jones and Steinhardt (2022)</em> captured failures in GPT-3 and Codex and found that error patterns of LLMs resemble cognitive biases in humans. Our work overlaps with these in some of the biases we cover, but we present a much more holistic and comprehensive evaluation of LLMs. Along this aspect, while our work is close to <em>Wu and Aji (2023)</em>, who investigate biases related to fabricated factual and grammatical errors, our work is much more comprehensive in terms of the number of LLMs analyzed, the types of biases analyzed and the creation of an open benchmark.</p>
<h2>3 CoBBLER: Cognitive Bias Benchmark for LLMs as Evaluators</h2>
<p>The following criteria are used to select each type of evaluation bias:</p>
<ul>
<li>General Applicability. Text evaluation tasks should be generalizable to most prompting scenarios; tasks that observe too specific subtleties within the prompt are not helpful.</li>
<li>Impartiality. The prompt should not involve any leading statements to extract some desired quality of the evaluations</li>
<li>Memorylessness. The current evaluation instance should not rely on any previous behaviors. Each instance should be self-contained when extracting each bias metric.</li>
</ul>
<p>We carefully hand-select these biases based on the above three criteria such that they can be widely applicable to most evaluation settings in assessing the performance of LLMs as automatic evaluators.</p>
<p>Table 1 summarizes definitions of each bias type along with examples in CoBBLEr. We categorize our benchmark into two main classes: (1) Implicit and (2) Induced Biases. For implicit biases, we feed a general prompt that shows system outputs in a pairwise manner to extract any biased behaviors within the model’s evaluations implicitly. For induced biases, we feed prompts geared towards each different bias, similar to adversarial attacks, such as presenting false information that may influence evaluator behaviors in a certain manner. Hence, we note that criterion 2 is not entirely fulfilled due to the nature of induced biases, though they can still be generally observable in an evaluation setting.</p>
<h3>3.1 Implicit Biases</h3>
<p>We categorize biases as “implicit” if they can be witnessed without including any additional information other than instructing the model to judge the quality of two given generated texts.</p>
<p>Order Bias is an evaluation bias we observe when a model tends to favor the model based on the order of the responses rather than their content quality. Order bias has been extensively studied <em>Jung et al. (2019); Wang et al. (2023a); Zheng et al. (2023)</em>, and it is well-known that language models can be influenced by the ordering of the responses in their evaluations. We prompt both orderings of each pair and count the evaluation as a “first order” or “last order” bias if the evaluator chooses the first ordered (or last ordered) output in both arrangements respectively.</p>
<p>Compassion Fade (Naming). <em>Butts et al. (2019); Västfjäll et al. (2014)</em> is a cognitive bias that denotes a decrease in empathy as the number of identifiable individuals increases. To this phenomenon, we modify the definition for our use case to measure whether model evaluations are affected by real/identifiable names as opposed to evaluations with anonymous aliases (e.g. System A). Specifically, an unbiased evaluator would make evaluations similar to when anonymized names were presented.</p>
<p>Egocentric Bias (Self-Preference). <em>Ross and Sicoly (1979)</em> is a cognitive bias that refers to the tendency to have a higher opinion of oneself or to more easily accept ideas if they match one’s own. We define an evaluator to be egocentrically biased if, for each instance, the evaluator prefers its own response over others. We note that an unbiased evaluator would choose between themselves and other comparand models equally in proportion. However, we highlight that some models would naturally generate higher quality responses (e.g., GPT4 vs. Koala), resulting in a stronger inclination for such evaluators to choose their own responses.</p>
<p>Salience Bias (Length). <em>Schenk (2010); Zheng et al. (2023)</em> The evaluator tends to favor responses that are either shorter or longer in length. An unbiased evaluator would be split evenly between responses that are shorter or longer in length. We examine this bias by looking at evaluations in which a model preferred a response that is either shorter or longer in token length.</p>
<h3>3.2 Induced Biases</h3>
<p>We categorize a bias as “induced” when it requires modifications to the primary prompt or the inclusion of additional information with the original instructions. We specifically look to test the robustness of each of the models as evaluators by introducing false or off-topic information and examining the impact that these setups have on the quality of their evaluations. For both biases below, we would expect an unbiased evaluator to generally pick responses highlighted by BANDWAGON and ATTENTIONAL $\sim 25\%$ of the time (calculated RANDOM threshold).</p>
<p>Bandwagon Effect. <em>Schmitt-Beck (2015)</em> The evaluator’s preferences are influenced by the collective preference rather than being based on their own independent judgments. We add an additional sentence after the initial instruction stating a fake statistic by choosing one of the comparand outputs as preferred by a majority of people, such as “85% believe that System Star is better.”. We count the model to be influenced by BANDWAGON if the evaluator choose the model stated in the statistic.</p>
<p>Attentional Bias (Distraction). In addition to the original instruction, we follow a similar setup from <em>Shi et al. (2023)</em> where we include irrelevant information about one of the comparand models to test the ability of evaluators. For example, we include a meaningless sentence such as “System Star likes to eat oranges and apples.” We identify the evaluator to be distracted if it prefers the model mentioned in the distraction or if its valid response rate significantly drops.</p>
<p>4 Experiment Setup</p>
<p>In this section, we discuss our evaluation framework for benchmarking each of the different biases in LLMs as evaluators for text quality comparison.</p>
<h3>4.1 Datasets and Models</h3>
<p>Datasets We choose two widely used datasets (Eli5 <em>Fan et al. (2019)</em> and BigBench (strategyQA)) <em>Geva et al. (2021); Srivastava et al. (2023)</em>) employed to train and benchmark instruction-tuned models, creating a set of 50 question-answering instructions (taking 25 random instructions from each). We specifically choose corpora from the Question-Answering (Q/A) domain for ease of use in generating responses. As we are looking to test the ability of language models to perform as unbiased evaluators to judge response quality and correctness, the Q/A response format presents the most natural setting for these comparisons.</p>
<p>Models We assemble 16 popular models based on the HuggingFace OpenLLM leaderboard <em>Beeching et al. (2023)</em>, API-based models, and recent open-source models:</p>
<ul>
<li>(&gt;100B parameters): GPT-4, ChatGPT, InSTRUCTGPT <em>OpenAI (2023)</em></li>
<li>(&gt;40B parameters): LLAMAv2 <em>Touvron et al. (2023)</em>, LLAMA <em>Touvron et al. (2023)</em>, COHERE, FALCON <em>Almazrouei et al. (2023)</em></li>
<li>(&gt;10B parameters): ALPACA <em>Taori et al. (2023)</em>, Vicuna <em>Chiang et al. (2023)</em>, OpenAssistent <em>Köpf et al. (2023)</em></li>
<li>(&lt;10B parameters): MISTRAL-INSTRUCT <em>Jiang et al. (2023)</em>, OLMO <em>Groeneveld et al. (2024)</em>,BAIZE <em>Xu et al. (2023b)</em>, KOALA <em>Geng et al. (2023)</em>, WIZARDLM <em>Xu et al. (2023a)</em>, MPT <em>Team (2023)</em></li>
</ul>
<h3>4.2 Text Evaluation Setting</h3>
<p>Response Generation Figure 1 demonstrates our generation and evaluation pipeline for COBBLER. Here, we define “models” and “evaluators” interchangeably. We first generate the responses from each model by prompting 50 instructions from the combined dataset for a total of 800 generations.</p>
<p>Pairwise Evaluation After we collect all the model responses, we then prompt each evaluator to compare the anonymized generations in a pairwise manner. We generate all $\binom{15}{2}$ unique pairs amongst all models for each of the 50 instructions, creating a total of 5250 examples for each evaluator to rank. We then prompt the evaluator to compare generations based on the coherence of each of the responses in terms of correctness of content and alignment to the instruction/reference provided. The evaluation prompts for each bias benchmark are viewable in Appendix C. To mitigate against potential confounding factors, we run each pairwise instance twice in both arrangements to validate consistent behavior.</p>
<p>Additionally, we conduct a list-wise ranking amongst 4 models. However, we find that most LLMs of size &lt;40B have trouble generating a valid list of rankings (Appendix B) due to increased task complexity <em>Dziri et al. (2023)</em>.</p>
<p>Benchmarking As the comparisons are limited to a pair-wise fashion, we empirically calculate a "bias threshold" via random selection. For example, in the ORDER benchmark, each pair is evaluated twice in which both orderings are viewed (i.e. System Star is shown ordered first, then System Square is shown ordered first). We then randomly select a model in each response pair and measure the percentage of where the first-ordered model is chosen in both arrangements; models above random thresholds are identified to exhibit the said bias.</p>
<p>The random threshold provides a rough basis for the proportion of evaluations, for example, with respect to Order bias, which would be labeled “first order bias” if one randomly selects a response. We make this assumption to serve as a “litmus test” in distinguishing established patterns with respect to “bias/unbiased” evaluations by automatic evaluators rather than just random selection when models are noticeably above or below this threshold for each of our benchmark modules. We conduct a statistical test in Appendix B.5 to determine the significance of each proportion of biased evaluations from each automatic evaluator with the random baseline.</p>
<h3>4.3 Human Preference Study</h3>
<p>We collected human preferences from six workers on Amazon mechanical Turk (AMT) platform. More details about our data collection, human annotation process, and Rank-Biased Overlap and our calculation process are presented in Appendix D.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of major findings (lower score indicates "less biased" or better performance) of evaluator capabilities on all bias benchmarks. The red-dotted line denotes the average RANDOM threshold across each bias. Models highlighted red indicate ones with $&lt;80 \%$ valid evaluations on 2 or more of the benchmarks.</p>
<p>Agreement between Human Preference and LLM Evaluation We calculated the RankBiased Overlap (RBO) score [webber2010ranking] to measure the agreement between human preferences and model evaluations in ranking-generated texts across 16 different LLMs. RBO, which can vary from 0 (non-conjoint) to 1 (identical), assigns more weight to the top $k$ items in the ranked lists being compared ${ }^{4}$. Higher RBO score means higher agreement. Further mathematical details of RBO setup can be found in Appendix D.2. To properly compare machine and human preferences, we construct a ranked list for each evaluator by counting each model wins ${ }^{5}$ from every pairwise comparison and then calculated the RBO. Here, we computed the RBOs between each individual annotator and machine preferences and averaged them.</p>
<p>Identifying Biases in Pairwise Human Preference To validate the gap between model judgment and humans, we conduct another study to measure the degree of bias in human evaluations as well. We mirror the pairwise model evaluation setting from Section 4.2 for Order Bias, Salience Bias, Bandwagon Effect, and Attentional Bias for a separate human study. To obtain an</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>effective metric, and due to the vastness of the pairwise model comparison settings, we randomly sampled 750 pairs from 25 different instructions. We then calculate the average IAA for each bias via RBO and then compute the average bias proportion across all annotators to highlight the overall influence of each bias on human judgment.</p>
<p>To maintain consistency with the initial study, where we used RBO as an IAA metric among human annotators for the previous N -wise ranking human experiment, we employed the same approach for the pairwise human bias experiment as opposed to Fleiss' Kappa or other pairwise agreement scores. This involved converting all pairwise rankings by humans into a ranked list of models and computing the IAA scores among the human annotators for each of the three bias experiments. However, we randomly paired models for each instruction and thus generated 750 model pairs per bias ${ }^{6}$, with some models appearing either multiple times or none in those pairs. As some models may be overrepresented, we compensate for the absence of some models by applying a normalization to the rankings of appearing models across all judged pairs per human annotator. More details are described in Appendix D.3, D.4, and D.5.</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Size</th>
<th>Order</th>
<th></th>
<th>Comp.</th>
<th></th>
<th>Egoc.</th>
<th></th>
<th>Sal.</th>
<th>Band.</th>
<th>Attn.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>First</td>
<td>Last</td>
<td>First</td>
<td>Last</td>
<td>Order</td>
<td>Comp.</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>RANDOM</td>
<td>-</td>
<td>0.24</td>
<td>0.25</td>
<td>0.24</td>
<td>0.25</td>
<td>0.24</td>
<td>0.24</td>
<td>0.5</td>
<td>0.25</td>
<td>0.25</td>
</tr>
<tr>
<td>GPT4</td>
<td>-</td>
<td>0.17</td>
<td>0.06</td>
<td>0.46</td>
<td>0.33</td>
<td>0.78</td>
<td>0.06</td>
<td>0.56</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>175B</td>
<td>0.38</td>
<td>0.03</td>
<td>0.41</td>
<td>0.25</td>
<td>0.58</td>
<td>0.17</td>
<td>0.63</td>
<td>0.86</td>
<td>0.06</td>
</tr>
<tr>
<td>InSTRUCTGPT</td>
<td>175B</td>
<td>0.14</td>
<td>0.24</td>
<td>0.29</td>
<td>0.19</td>
<td>0.28</td>
<td>0.27</td>
<td>0.66</td>
<td>0.85</td>
<td>0.54</td>
</tr>
<tr>
<td>LLAMAv2</td>
<td>70B</td>
<td>0.47</td>
<td>0.08</td>
<td>0.09</td>
<td>0.17</td>
<td>0.06</td>
<td>0.0</td>
<td>0.62</td>
<td>0.04</td>
<td>0.03</td>
</tr>
<tr>
<td>LLAMA</td>
<td>65B</td>
<td>0.61</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.02</td>
<td>0.42</td>
<td>0.0</td>
<td>0.01</td>
</tr>
<tr>
<td>COHERE</td>
<td>54B</td>
<td>0.33</td>
<td>0.17</td>
<td>0.38</td>
<td>0.27</td>
<td>0.27</td>
<td>0.15</td>
<td>0.60</td>
<td>0.82</td>
<td>0.14</td>
</tr>
<tr>
<td>FALCON</td>
<td>40B</td>
<td>0.74</td>
<td>0.03</td>
<td>0.09</td>
<td>0.18</td>
<td>0.05</td>
<td>0.11</td>
<td>0.59</td>
<td>0.28</td>
<td>0.40</td>
</tr>
<tr>
<td>ALPACA</td>
<td>13B</td>
<td>0.0</td>
<td>0.82</td>
<td>0.23</td>
<td>0.29</td>
<td>0.18</td>
<td>0.39</td>
<td>0.47</td>
<td>0.75</td>
<td>0.81</td>
</tr>
<tr>
<td>VICUNA</td>
<td>13B</td>
<td>0.32</td>
<td>0.17</td>
<td>0.17</td>
<td>0.15</td>
<td>0.27</td>
<td>0.45</td>
<td>0.53</td>
<td>0.81</td>
<td>0.78</td>
</tr>
<tr>
<td>OPENASSIST</td>
<td>12B</td>
<td>0.56</td>
<td>0.11</td>
<td>0.03</td>
<td>0.22</td>
<td>0.15</td>
<td>0.06</td>
<td>0.49</td>
<td>0.72</td>
<td>0.82</td>
</tr>
<tr>
<td>Mistral</td>
<td>7B</td>
<td>0.42</td>
<td>0.04</td>
<td>0.33</td>
<td>0.23</td>
<td>0.30</td>
<td>0.03</td>
<td>0.57</td>
<td>0.54</td>
<td>0.02</td>
</tr>
<tr>
<td>Olmo</td>
<td>7B</td>
<td>0.66</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
<td>0.38</td>
<td>0.83</td>
<td>0.46</td>
</tr>
<tr>
<td>BAIZE</td>
<td>7B</td>
<td>0.0</td>
<td>0.95</td>
<td>0.21</td>
<td>0.32</td>
<td>0.02</td>
<td>0.36</td>
<td>0.49</td>
<td>0.82</td>
<td>0.24</td>
</tr>
<tr>
<td>Koala</td>
<td>7B</td>
<td>0.24</td>
<td>0.01</td>
<td>0.0</td>
<td>0.11</td>
<td>0.48</td>
<td>0.86</td>
<td>0.55</td>
<td>0.13</td>
<td>0.10</td>
</tr>
<tr>
<td>WIZARDLM</td>
<td>7B</td>
<td>0.08</td>
<td>0.64</td>
<td>0.22</td>
<td>0.34</td>
<td>0.14</td>
<td>0.29</td>
<td>0.53</td>
<td>0.76</td>
<td>0.27</td>
</tr>
<tr>
<td>MPT</td>
<td>7B</td>
<td>0.49</td>
<td>0.1</td>
<td>0.11</td>
<td>0.27</td>
<td>0.21</td>
<td>0.25</td>
<td>0.63</td>
<td>0.95</td>
<td>0.52</td>
</tr>
</tbody>
</table>
<p>Table 2: A comparison of 16 models with different ranges of model sizes across six different bias benchmarks. A higher proportion indicates worse (more biased) performance. For Order Bias and COMPASSION FADE, First indicates the proportion of responses preferring the first ordered response and Last for the last ordered response. For SALIENCE BIAS, models with scores less than 0.5 prefer responses with fewer tokens, and scores above 0.5 prefer responses with more tokens. The background color of each metric is determined by the difference between the value and the corresponding RANDOM metric (darker shade indicates stronger bias).</p>
<h2>5 Results and Discussion</h2>
<p>For each bias, we analyze the performance of each of the 16 models as evaluators. We provide a visual breakdown of the proportional impact of the average performance of each model as unbiased evaluators in Fig. 2 based on the results relative to the RANDOM baseline in Table 2. On average, we see that models within the $10 B$ size range are most affected by each bias benchmark in Fig. 2a. Notably, we see that the implicit biases contribute similarly to each models’ overall bias scores, indicating that scaling model size does not reduce implicit biases in evaluators.</p>
<h3>5.1 Bias Analysis</h3>
<p>Implicit Biases We first examine the performance of each evaluator on the implicit bias benchmarks for Order Bias, Compassion Fade, Salience Bias and Egocentric Bias. For the Order Bias benchmark in Table 2, we observe that most models (11/15) tend to be drawn towards either the first- or last-ordered model in each of the pairwise comparisons. Notably, within the second size group ( $&gt;40 B$ ), the first-ordered system was strongly favored in over 50%.</p>
<p>For Compassion Fade, since it is difficult to interpret its impact by the metrics independently, we jointly compare the results with the ones from Order Bias. For an unbiased evaluator that is not influenced by identifiable names, we expect the results for Compassion Fade to be relatively similar to the Order Bias benchmark. However, we see in Table 2 that all models are dramatically influenced by real model names. Although this phenomenon may be akin to injecting random names, the disparity between Order and Compassion Fade results support our hypothesis that recognizable names influence evaluations in contrast to anonymized ones. In addition, we also note that Olmo sees a drastic decrease in performance. This might be attributed to the model’s inability to follow more complex instructions from its training.</p>
<p>For Egocentric Bias, in the anonymized aliases, the largest models as well as Koala tend to prefer their own responses ( $&gt;50 \%$ ) with the exception of InSTRUCTGPT. However, with real model names (COMPASSION), we see a large drop in self-preference for models in the largest size group ( $&gt;100 B$ ) models, but this may be attributed to a large increase in bias for each position. On average, we see an increase in self-preference with real model names amongst the two smaller size groups, notably Koala sees a 100% increase in preference.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Salience</th>
<th>Salience_{large}</th>
<th>Salience_{small}</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT4</td>
<td>0.56</td>
<td>0.71</td>
<td>0.46</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>0.63</td>
<td>0.84</td>
<td>0.56</td>
</tr>
<tr>
<td>LLAMAV2</td>
<td>0.62</td>
<td>0.75</td>
<td>0.53</td>
</tr>
<tr>
<td>COHERE</td>
<td>0.60</td>
<td>0.71</td>
<td>0.56</td>
</tr>
<tr>
<td>VICUNA</td>
<td>0.53</td>
<td>0.57</td>
<td>0.51</td>
</tr>
<tr>
<td>MISTRAL</td>
<td>0.57</td>
<td>0.68</td>
<td>0.50</td>
</tr>
<tr>
<td>OLMO</td>
<td>0.38</td>
<td>0.45</td>
<td>0.29</td>
</tr>
</tbody>
</table>
<p>Table 3: SALIENCE of selected models preferring generations from large models vs. small models. We see only small deviations in preference of large and small models’ generations with respect to saliency.</p>
<p>For SALIENCE BIAS, we observe that the larger models in the first and second size groups are more strongly affected by longer responses, which align with findings from other works <em>Wu and Aji (2023); Zheng et al. (2023)</em>. However, smaller models (excluding MPT) tend to be less influenced by the length of the responses, suggesting that smaller models in the third and fourth size groups are less impacted in their evaluations in consideration of the length of the text.</p>
<p>For models such as ChatGPT, the EGOCENTRIC BIAS may be unfair because their generations are indeed better, or in SALIENCE, the longer generations indeed have higher quality. For further insight in decoupling these factors, we include supplementary experiments viewed in Appendix B.</p>
<h4>5.1.1 Identifying Egocentric and Salience Bias</h4>
<p>We also discuss the evaluation criteria for identifying EGOCENTRIC and SALIENCE biases, which may be more appropriately evaluated conditioned on underlying generation quality and model size.</p>
<p>We select a few model representative models for clarity viewed in Table 3. Generally, most models stay consistent with their preference for longer/shorter responses conditioned on either generation’s quality, although some flip their preferences (to only a small effect however). For further insight, we compute the generation quality using reference-based metrics via BERTScore. From this, all models produce nearly the same quality of generations with respect to the reference answer (0.81 to 0.86 for F1), highlighting that identifying EGOCENTRIC or SALIENCE bias is most likely not dependent on generation quality.</p>
<p>Induced Biases Next, we evaluate the performance of each evaluator on the induced bias benchmarks: BANDWAGON EFFECT and ATTENTIONAL BIAS. For BANDWAGON EFFECT, we observe that almost all models (11/15) are heavily influenced in which $&gt;70\%$ of evaluations on average followed the bandwagon preference regardless of text quality. Although we only included a simple fake statistic (e.g. 85% of people preferred "System Stur"), we see that evaluators can be heavily influenced by this external information. To observe a correlation between the biased tendency and the percentage, we include additional results in Appendix B.1</p>
<p>For ATTENTIONAL BIAS, we see that around half of the models’ rankings are influenced by irrelevant information. Specifically, we see that models in the third size group (10B) were the most strongly impacted by the distracting information, with $&gt;80\%$ of evaluations being counted as distracted. On the other hand, API-based models such as CHATGPT and COHERE remained robust against these distractions in their rankings. We include the list of distractions we use in Appendix C.</p>
<p>Lastly, we address specific models such as LLAMAV2, LLAMA, KOALA, and OLMO that show abnormal results on most of the benchmarks. This can be attributed to their low valid response rates, displayed in Table 12 in Appendix B, which may be explained by our prompting format or the capabilities of the model themselves, likely as they are not instruction-tuned. Although these models display lower performance when extracting evaluations, if a model is not strong enough to produce valid outputs, we assume those models are not strong enough to be used for evaluations. And as we don’t consider invalid responses within the study, we only apply our findings to ones that produced valid evaluations, in which most models exhibit cognitive biases from our benchmark. Although the correlation between valid response rates and bias can provide more insight into model capabilities, it is not within the scope of our findings.</p>
<h3>5.2 Agreement Between Human Preferences and Model Evaluations</h3>
<p>N-rankwise Human Preference (N=13) The average RBO among the six AMT workers is 0.54, which signifies a modest but reasonable consensus among workers in ranking the LLM outputs, given the challenges of ranking all LLM-generated outputs. From this, we calculate the average RBO between human and model preferences to be 0.44, indicating that model evaluations do not closely align with human preferences.</p>
<p>Figure 3 presents the average RBO scores be-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Correlation with human judgment. We show the average Rank-Biased Overlap (RBO) scores between aggregated human preferences and each of the 16 LLMs. Higher RBO means higher similarity.</p>
<p>tween a model and each of human preferences. While MISTRAL and CHATGPT achieved the highest RBO scores, most of the remaining models demonstrated lower agreement with human preferences. Smaller models also tend to misalign with an overall human preference, as the average RBO of models of size greater or smaller than 10B are 0.37 and 0.41, respectively, compared to &gt;40B (0.49) and &gt;100B (0.48).</p>
<p>Furthermore, we present additional results on the variance of pairwise RBOs based on our annotations by six human annotators for the N=13-wise ranking experiment. Table 5 presents that the variance of all pairwise RBOs among humans is 0.004, indicating minimal disparity amongst all annotators. It is more clearly observed that any pairwise RBO between two annotators is higher than the average agreement between humans and models (0.44).</p>
<p><strong>Bias in Pairwise Human Preference</strong> The average RBO scores were 0.39 (ORDER BIAS), 0.50 (BANDWAGON EFFECT), and 0.43 (ATTENTIONAL BIAS), indicating modest agreement amongst human annotators in a pairwise selection setting. The average proportion of biased responses across all human annotators for ORDER BIAS, SALIENCE BIAS, BANDWAGON EFFECT, and ATTENTIONAL BIAS are presented in Table 4. Compared to humans, VICUNA shows higher or similar bias proportions on all of the four bias types, where its</p>
<table>
<thead>
<tr>
<th></th>
<th>ORDER</th>
<th>SALIE.</th>
<th>BANDW.</th>
<th>ATTEN.</th>
</tr>
</thead>
<tbody>
<tr>
<td>HUMAN</td>
<td>0.20</td>
<td>0.52</td>
<td>0.47</td>
<td>0.35</td>
</tr>
<tr>
<td>VICUNA</td>
<td>0.32</td>
<td>0.53</td>
<td>0.81</td>
<td>0.78</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison of Human bias vs. Vicuna for the proportion of biased evaluations. For ORDER, we show the worst performance.</p>
<table>
<thead>
<tr>
<th></th>
<th>A1</th>
<th>A2</th>
<th>A3</th>
<th>A4</th>
<th>A5</th>
<th>A6</th>
</tr>
</thead>
<tbody>
<tr>
<td>A1</td>
<td>1</td>
<td>0.694</td>
<td>0.466</td>
<td>0.469</td>
<td>0.511</td>
<td>0.484</td>
</tr>
<tr>
<td>A2</td>
<td></td>
<td>1</td>
<td>0.471</td>
<td>0.483</td>
<td>0.515</td>
<td>0.512</td>
</tr>
<tr>
<td>A3</td>
<td></td>
<td></td>
<td>1</td>
<td>0.572</td>
<td>0.589</td>
<td>0.548</td>
</tr>
<tr>
<td>A4</td>
<td></td>
<td></td>
<td></td>
<td>1</td>
<td>0.607</td>
<td>0.536</td>
</tr>
<tr>
<td>A5</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>1</td>
<td>0.597</td>
</tr>
<tr>
<td>A6</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>1</td>
</tr>
</tbody>
</table>
<p>Table 5: Upper triangle agreement between each human annotator. We see the agreement between each worker is in general much higher than the agreement between human and LLMs (0.44).</p>
<p>ATTENTIONAL BIAS proportion particularly exceeds humans by more than twice.</p>
<p>We view that humans still exhibit biases when making their preferences on pairwise LLM evaluations, but less than LLM evaluators on average. Similarly, on the induced bias benchmarks, humans were less affected by BANDWAGON EFFECT and ATTENTIONAL bias, highlighting a prevalent gap between model judgment capabilities and human ones, in which human-like biases are more intensified.</p>
<h2>6 Conclusion</h2>
<p>In this paper, we analyze 16 recently developed LLMs for their suitability as automatic text quality annotators in Q/A settings. We introduce a new benchmark COBBLER to assess their evaluation performance against 1) <strong>Implicit</strong> and 2) <strong>Induced</strong> biases. Additionally, we compare LLM evaluations to human preferences and find only a 44% average agreement. Our results indicate that most LLMs exhibit cognitive biases to a greater extent than humans, suggesting that LLMs are still unsuitable as fair and reliable automatic evaluators. In the future, potential de-biasing methods provide another area of interest in reducing each bias. For example, techniques such as chain-of-thought (CoT) reasoning or other alignment methods can perhaps be employed to reduce the bias for current models.</p>
<p>^{7}Note that we considered these scores, which might initially appear low, as relatively high when considering the impact of biases that can affect individuals to varying degrees.</p>
<h2>Limitations</h2>
<p>We acknowledge a few limitations within our study. Some models reach very low valid response rates, which may be due to the prompting format. With model-specific prompts, we may be able to extract more clear results for each bias. Additionally, we address the fairly subpar IAA within our human judgment study. This may be due to the difficulty of the task, asking MTurk annotators to rank 15 models to limit the number of comparisons required in a pairwise format, but also increases the complexity of the task itself, which may have caused lower quality in the annotations.</p>
<p>We also highlight the stability of our findings in the long term. As LLM research is rapidly growing, the capabilities of language models can scale exponentially with time. As such, with new developments being discovered frequently, previous LLM performance on our bias benchmarks may quickly become outdated (i.e. InSTRUCTGPT can be considered an "outdated LLM," as the API is also no longer offered on OpenAI's platforms).</p>
<h2>Acknowledgements</h2>
<p>This work was mainly supported by the research gift from Grammarly. We also thank Minnesota NLP group members for providing us with valuable feedback and comments on the initial draft.</p>
<h2>References</h2>
<p>Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. 2023. Falcon-40B: an open large language model with state-of-the-art performance.</p>
<p>Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, and Lei Hou. 2023. Benchmarking foundation models with language-model-as-an-examiner.</p>
<p>Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. 2023. Open llm leaderboard. https://huggingface.co/spaces/ HuggingFaceH4/open_llm_leaderboard.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,</p>
<p>Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.</p>
<p>Marcus M. Butts, Devin C. Lunt, Traci L. Freling, and Allison S. Gabriel. 2019. Helping one or helping many? a theoretical integration and meta-analytic review of the compassion fade literature. Organizational Behavior and Human Decision Processes, $151: 16-33$.</p>
<p>Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and Ruifeng Xu. 2023. Exploring the use of large language models for reference-free text quality evaluation: A preliminary empirical study.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with $90 \% *$ chatgpt quality.</p>
<p>Roi Cohen, May Hamri, Mor Geva, and Amir Globerson. 2023. Lm vs lm: Detecting factual errors via cross examination.</p>
<p>Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. 2023. Free dolly: Introducing the world's first truly open instructiontuned llm.</p>
<p>Debarati Das, Karin De Langis, Anna Martin-Boyle, Jaehyung Kim, Minhwa Lee, Zae Myung Kim, Shirley Anugrah Hayati, Risako Owan, Bin Hu, Ritik Parkar, Ryan Koo, Jonginn Park, Aahan Tyagi, Libby Ferland, Sanjali Roy, Vincent Liu, and Dongyeop Kang. 2024. Under the surface: Tracking the artifactuality of llm-generated data.</p>
<p>Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, Sean Welleck, Xiang Ren, Allyson Ettinger, Zaid Harchaoui, and Yejin Choi. 2023. Faith and fate: Limits of transformers on compositionality.</p>
<p>Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. ELI5: long form question answering. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 3558-3567. Association for Computational Linguistics.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire.</p>
<p>Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2021. A framework for few-shot language model evaluation.</p>
<p>Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Shiping Yang, and Xiaojun Wan. 2023. Human-like summarization evaluation with chatgpt.</p>
<p>Zorik Gekhman, Jonathan Herzig, Roee Aharoni, Chen Elkind, and Idan Szpektor. 2023. Trueteacher: Learning factual consistency evaluation with large language models.</p>
<p>Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. Koala: A dialogue model for academic research. Blog post.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. Transactions of the Association for Computational Linguistics (TACL).</p>
<p>Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hannaneh Hajishirzi. 2024. Olmo: Accelerating the science of language models.</p>
<p>Shirley Anugrah Hayati, Dongyeop Kang, and Lyle Ungar. 2021. Does BERT learn as humans perceive? understanding linguistic styles through lexica. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 63236331, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR).</p>
<p>Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b.</p>
<p>Erik Jones and Jacob Steinhardt. 2022. Capturing failures of large language models via human cognitive biases. In Advances in Neural Information Processing Systems.</p>
<p>Rasmus Jørgensen, Fiammetta Caccavale, Christian Igel, and Anders Søgaard. 2022. Are multilingual sentiment models equally right for the right reasons? In Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 131-141, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.</p>
<p>Taehee Jung, Dongyeop Kang, Lucas Mentch, and Eduard Hovy. 2019. Earlier isn't always better: Subaspect analysis on corpus and system biases in summarization. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-UCNLP), pages 3324-3335, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Tom Kocmi and Christian Federmann. 2023. Large language models are state-of-the-art evaluators of translation quality.</p>
<p>Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Mattick. 2023. Openassistant conversations - democratizing large language model alignment.</p>
<p>Ruosen Li, Teerth Patel, and Xinya Du. 2023a. Prd: Peer rank and discussion improve large language model based evaluations.</p>
<p>Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023b. Alpacaeval: An automatic evaluator of instructionfollowing models. https://github.com/ tatsu-lab/alpaca_eval.</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2022. Holistic evaluation of language models.</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100-114, Dublin, Ireland and Online. Association for Computational Linguistics.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: Nlg evaluation using gpt-4 with better human alignment.</p>
<p>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086-8098, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. 2023. Chatgpt as a factual inconsistency evaluator for text summarization.</p>
<p>Sejoon Oh, Berk Ustun, Julian McAuley, and Srijan Kumar. 2022. Rank list sensitivity of recommender systems to interaction perturbations. Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback.</p>
<p>Vipul Raheja, Dhruv Kumar, Ryan Koo, and Dongyeop Kang. 2023. Coedit: Text editing by task-specific instruction tuning.</p>
<p>Michael Ross and Fiore Sicoly. 1979. Egocentric biases in availability and attribution. Journal of Personality and Social Psychology, 37:322-336.</p>
<p>Deborah Schenk. 2010. Exploiting the salience bias in designing taxes. New York University Law and Economics Working Papers, 28.</p>
<p>Timo Schick, Jane A. Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. 2023. PEER: A collaborative language model. In The Eleventh International Conference on Learning Representations.</p>
<p>Rüdiger Schmitt-Beck. 2015. Bandwagon Effect.</p>
<p>Chenhui Shen, Liying Cheng, Yang You, and Lidong Bing. 2023. Are large language models good evaluators for abstractive summarization?</p>
<p>Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Schärli, and Denny Zhou. 2023. Large language models can be easily distracted by irrelevant context.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, and Adrià Garriga-Alonso et al. 2023. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research.</p>
<p>Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, and Zhaochun Ren. 2023. Is chatgpt good at search? investigating large language models as re-ranking agent.</p>
<p>Alaina N. Talboy and Elizabeth Fuller. 2023. Challenging the appearance of machine intelligence: Cognitive bias in llms and best practices for adoption.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/ stanford_alpaca.</p>
<p>MosaicML NLP Team. 2023. Introducing mpt-7b: A new standard for open-source, commercially usable llms. Accessed: 2023-05-05.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models.</p>
<p>Daniel Västfjäll, Paul Slovic, Marcus Mayorga, and Ellen Peters. 2014. Compassion fade: Affect and charity are greatest for a single child in need. PLOS ONE, 9(6):1-10.</p>
<p>Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2020. Superglue: A stickier benchmark for general-purpose language understanding systems.</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023a. Is chatgpt a good nlg evaluator? a preliminary study.</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023b. Large language models are not fair evaluators.</p>
<p>William Webber, Alistair Moffat, and Justin Zobel. 2010. A similarity measure for indefinite rankings. ACM Transactions on Information Systems (TOIS), 28(4):138.</p>
<p>Minghao Wu and Alham Fikri Aji. 2023. Style over substance: Evaluation biases for large language models.</p>
<p>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023a. Wizardlm: Empowering large language models to follow complex instructions.</p>
<p>Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2023b. Baize: An open-source chat model with parameter-efficient tuning on self-chat data.</p>
<p>Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 12697-12706. PMLR.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging 1lm-as-a-judge with mt-bench and chatbot arena.</p>
<p>Terry Yue Zhuo. 2023. Large language models are state-of-the-art evaluators of code generation.</p>
<p>A Experimental Setup</p>
<h3>A.1 Model Hyperparameters</h3>
<p>We set the same hyperparameters across models for each evaluation generation and response generation for consistency across all of the models. We limit the max new tokens generated to 128 tokens and set the temperature to 1.0. For Huggingface models, we set a repetition penalty of 1.2 and set the number of beams to 3.</p>
<h3>A.2 Experimental Settings</h3>
<p>For models that are supported (ChatGPT, InstructGPT, GPT-4, Vicuna), we utilize Microsoft Guidance to better control LLM generations. Otherwise, we utilize the transformer pipeline library from Hugginface to retrieve each evaluation generation. Regardless of whether a models generation was collected from guidance or using the transformer pipeline, all parameters were the same. Model generation times for response generation ranged from 1 to 8 hours, and for evaluation generations ranged from 3 to 24 hours for each bias benchmark. All experiments were run on either A5000 or A6000 GPUs for models under 40B parameters. For models over 40B, A100 GPUs were utilized if an API service was not available (e.g. OpenAI, Cohere).</p>
<h3>A.3 Datasets</h3>
<p>Eli5 (Fan et al., 2019) is a long-form questionanswering dataset constructed from $270 k$ threads from the "Explain Like I'm Five" Reddit forum. The online forum consists of a community for individuals to ask various questions, and answers are provided in a format that is comprehensible to five-year-olds, along with assigned scores based on community votes. For our purposes, we only utilize the questions and their highest-rated answers to generate responses and benchmark automatic evaluators for text-generation quality.</p>
<p>BigBench (Srivastava et al., 2023) is a collection of benchmarks that look to probe the abilities of language models over a diverse range of tasks. We specifically utilize the strategyQA (Geva et al., 2021) dataset, which was constructed by crowdsourcing questions from writers as well as their responses with short justifications. We choose the strategyQA dataset to generate responses that require multi-step reasoning to effectively benchmark the ability of models to comprehend and compare the quality between two different explanations.</p>
<h2>Appendix B Supplementary Results</h2>
<h3>B.1 Correlation between BANDWAGON and Percentage</h3>
<p>In an additional experiment, we show a modified statistic for the biased model: "0\% of people prefer {model}." If bias tendency were indeed correlated with the statistic, we would expect the evaluator model to have 0 preference for bandwagon response. Due to limited computation resources and time, we ran the additional experiments for two representative models at each size range (+ all API-based models) and presented the results below in Table 6.</p>
<p>Here, one can observe that the preference choices for the bandwagon statistic greatly change (besides GPT4 and VICUNA) which suggests that indeed the biased tendency is correlated with the bandwagon statistic. However, we see that VICUNA, in particular, is not greatly affected by the statistics. This suggests that within the prompt, the model only focuses on the phrase "people believe that {model} is better" instead of the statistic. Similarly, this may be the case for Alpaca and InstructGPT as well. We also present the results of the bandwagon test by randomly choosing a percentage between $50 \%$ and $85 \%$ in Table 7. We continue see that most models demonstrate biased tendencies.</p>
<h3>B.2 Diverse Prompts</h3>
<p>We additionally ask each evaluator to analyze generation quality along several different aspects such as "coherence, accuracy, factuality, and helpfulness" following (Bai et al., 2023) and (Zheng et al., 2023). As opposed to our single-aspect format in the main section, we conjecture that these cognitive biases remain regardless of evaluation aspects. To validate this, we constructed an extended prompt viewable in C. 5 that incorporates different dimensions of evaluation criteria into our pairwise evaluation prompt and reported their results in Table 8 on the ORDER benchmark. We see that by including diverse perspectives in the evaluation setting, some metrics become more pronounced (i.e. COHERE for EGOCENTRIC) or bias decreases (i.e. VICUNA for EGOCENTRIC). However, we see that the proportion of biased evaluations stays relatively consistent for most models on all benchmarks. Hence, our findings remain that models still show a large skewness in bias tendency as evaluators.</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>GPT-4</th>
<th>ChatGPT</th>
<th>InSTUCTGPT</th>
<th>COHERE</th>
<th>ALPACA</th>
<th>VICUNA</th>
<th>BAIZE</th>
<th>WIZARDLM</th>
</tr>
</thead>
<tbody>
<tr>
<td>BANDWAGON (85\%)</td>
<td>0.0</td>
<td>0.86</td>
<td>0.85</td>
<td>0.82</td>
<td>0.75</td>
<td>0.81</td>
<td>0.82</td>
<td>0.76</td>
</tr>
<tr>
<td>BANDWAGON (0\%)</td>
<td>0.0</td>
<td>0.0</td>
<td>0.56</td>
<td>0.0</td>
<td>0.52</td>
<td>0.79</td>
<td>0.32</td>
<td>0.27</td>
</tr>
</tbody>
</table>
<p>Table 6: BANDWAGON test showing a fake statistic stating $0 \%$ of people prefer the chosen response.</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>GPT-4</th>
<th>ChatGPT</th>
<th>InSTUCTGPT</th>
<th>COHERE</th>
<th>ALPACA</th>
<th>VICUNA</th>
<th>BAIZE</th>
<th>WIZARDLM</th>
</tr>
</thead>
<tbody>
<tr>
<td>BANDWAGON (85\%)</td>
<td>0.0</td>
<td>0.86</td>
<td>0.85</td>
<td>0.82</td>
<td>0.75</td>
<td>0.81</td>
<td>0.82</td>
<td>0.76</td>
</tr>
<tr>
<td>BANDWAGON (50-85\%)</td>
<td>0.06</td>
<td>0.70</td>
<td>0.84</td>
<td>0.65</td>
<td>0.68</td>
<td>0.96</td>
<td>0.75</td>
<td>0.76</td>
</tr>
</tbody>
</table>
<p>Table 7: BANDWAGON test showing a fake statistic stating (randomly) between $50-80 \%$ of people prefer the chosen response.</p>
<h2>B. 3 Prompting with Ties</h2>
<p>We present a modified version of the prompt in B. 3 that considers ties in each pairwise preference. Note that for SALIENCE, if a pairwise sample was labeled as "Tie," we do not consider it for length bias. From Table 9 we see that the inclusion of the tie option does view a considerable change in the bias benchmarks. Notably, the strongest and smallest models (GPT-4, CHATGPT, BAIZE, WIZARDLM) do not exhibit any change. However, we see that the mid-range models (ALPACA, VICUNA) and INSTRUCTGPT display a large preference for assigning the tie label $(\geq \sim 90 \%)$ that does not present any valid results, to which we had originally only prompted two options for each evaluator to avoid this issue. The only model that demonstrated an improvement from previous bias behavior was COHERE.</p>
<h2>B. 4 Decoupling Confounding Factors</h2>
<p>We particularly focus on decoupling EGOCENTRIC and Salience, which are the most prone to having large correlations with each other (i.e. longer generations may indeed have overall higher quality generated by much stronger models). We highlight two important aspects regarding the identification of these biases:</p>
<ul>
<li>If multiple models have a large proportion of evaluations preferring their own responses (as the evaluated pool of pairwise instances is the same for each evaluator), we reason that this may suggest "egocentric" qualities within involved evaluators, regardless of the objective strength of the models. Moreover, we see this effect is especially demonstrated between the more powerful models as well (GPT4 \&amp; CHATGPT) that suggest the presence of EGOCENTRIC evaluations from their disagreement.</li>
<li>We employ various strategies to mitigate these confounding variables and isolate each analysis as much as possible. For example, we employ a "hierarchical" rubric, where some biases take priority in an evaluation. Specifically, if an evaluation shows signs of order bias by choosing A in (A first, then B) and B in (B first, then A), we do not evaluate it for SALIENCE or EGOCENTRIC bias.</li>
</ul>
<p>To get further insight into decoupling them, we examine additional statistics in Table 11 displaying the proportion of EGOCENTRIC samples where the model's generation was longer/shorter than the other generation. In particular, since OLMO only won once, and LLAMA never won, their EGOCENTRIC ratios look weird. Otherwise, we view overall that most models (9/16) exhibit a self-preference for their own generations often when their own generations exhibit longer token length.</p>
<p>As above, we see that SALIENCE may be associated with higher quality generations, as we see that the strongest models (GPT4, ChatGPT) often prefer their own responses when their generations are longer. Nevertheless, even in smaller models (e.g., Cohere, Koala), preference for their own generations occurs more often when they are longer. However, as we previously emphasized, if multiple models observe a self-preference for their own generations, it is difficult to associate with SALIENCE as there is disagreement that is indicative of an EGOCENTRIC bias.</p>
<table>
<thead>
<tr>
<th>MODELS</th>
<th>GPT-4</th>
<th>ChatGPT</th>
<th>InSTUCTGPT</th>
<th>COHERE</th>
<th>ALPACA</th>
<th>VICUNA</th>
<th>BAIZE</th>
<th>WIZARDLM</th>
</tr>
</thead>
<tbody>
<tr>
<td>ORDER (COH.)</td>
<td>$0.17_{F}$</td>
<td>$0.38_{F}$</td>
<td>$0.24_{L}$</td>
<td>$0.33_{F}$</td>
<td>$0.82_{L}$</td>
<td>$0.32_{F}$</td>
<td>$0.95_{L}$</td>
<td>$0.64_{L}$</td>
</tr>
<tr>
<td>ORDER (DIV.)</td>
<td>$0.14_{F}$</td>
<td>$0.45_{F}$</td>
<td>$0.22_{L}$</td>
<td>$0.23_{L}$</td>
<td>$0.76_{L}$</td>
<td>$0.52_{F}$</td>
<td>$0.83_{L}$</td>
<td>$0.68_{L}$</td>
</tr>
<tr>
<td>EGOCENT. (COH.)</td>
<td>0.78</td>
<td>0.58</td>
<td>0.28</td>
<td>0.27</td>
<td>0.18</td>
<td>0.27</td>
<td>0.02</td>
<td>0.14</td>
</tr>
<tr>
<td>EGOCENT. (DIV.)</td>
<td>0.80</td>
<td>0.54</td>
<td>0.29</td>
<td>0.41</td>
<td>0.18</td>
<td>0.18</td>
<td>0.04</td>
<td>0.09</td>
</tr>
<tr>
<td>SALIENCE (COH.)</td>
<td>0.56</td>
<td>0.63</td>
<td>0.66</td>
<td>0.60</td>
<td>0.47</td>
<td>0.53</td>
<td>0.49</td>
<td>0.53</td>
</tr>
<tr>
<td>SALIENCE (DIV.)</td>
<td>0.57</td>
<td>0.69</td>
<td>0.70</td>
<td>0.65</td>
<td>0.49</td>
<td>0.59</td>
<td>0.50</td>
<td>0.52</td>
</tr>
</tbody>
</table>
<p>Table 8: Comparison on the ORDER benchmark considering diverse evaluation perspectives. For visual clarity, we only display the bias ratio with the highest proportion and denote with subscript $x_{F}$ or $x_{L}$ for first- or last-ordered bias, respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">MODELS</th>
<th style="text-align: center;">GPT-4</th>
<th style="text-align: center;">ChatGPT</th>
<th style="text-align: center;">InSTUCTGPT</th>
<th style="text-align: center;">COHERE</th>
<th style="text-align: center;">ALPACA</th>
<th style="text-align: center;">VICUNA</th>
<th style="text-align: center;">BAIZE</th>
<th style="text-align: center;">WIZARDLM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ORDER</td>
<td style="text-align: center;">$0.17_{F}$</td>
<td style="text-align: center;">$0.38_{F}$</td>
<td style="text-align: center;">$0.24_{L}$</td>
<td style="text-align: center;">$0.33_{F}$</td>
<td style="text-align: center;">$0.82_{L}$</td>
<td style="text-align: center;">$0.32_{F}$</td>
<td style="text-align: center;">$0.95_{L}$</td>
<td style="text-align: center;">$0.64_{L}$</td>
</tr>
<tr>
<td style="text-align: left;">ORDER (TIE)</td>
<td style="text-align: center;">$0.15_{F}$</td>
<td style="text-align: center;">$0.43_{F}$</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$0.08_{L}$</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$0.81_{L}$</td>
<td style="text-align: center;">$0.47_{L}$</td>
</tr>
<tr>
<td style="text-align: left;">TIE (\%)</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.04</td>
</tr>
<tr>
<td style="text-align: left;">EGOCENTRIC</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.14</td>
</tr>
<tr>
<td style="text-align: left;">EGOCENTRIC (TIE)</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">0.16</td>
</tr>
<tr>
<td style="text-align: left;">SALIENCE</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.53</td>
</tr>
<tr>
<td style="text-align: left;">SALIENCE (TIE)</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.48</td>
</tr>
</tbody>
</table>
<p>Table 9: Comparison on the ORDER benchmark considering ties. For visual clarity, we only display the bias ratio with the highest proportion and denote with subscript $x_{F}$ or $x_{L}$ for first- or last-ordered bias, respectively.</p>
<h3>B.5 Significance of Results</h3>
<p>We adapt two statistical hypothesis tests based on the random bias threshold for the Order bias (firstorder and last-order) benchmark in Table 10. Since we have binary outputs (bias, not biased), we conduct a two-sample Z test of proportions to determine the significance of each proportion of biased evaluations from each automatic evaluator with the random baseline. We conduct the test with the null hypothesis defined to be that "evaluator X is just as likely to make the mistake of flipping its preference according to the order of the response to the first-order as the random baseline" or equivalently:
$H_{0}$ : the mean of Evaluator $X$ for first-order bias is not any different from random selection.
On almost all of the ORDER benchmarks, the proportions of biased evaluations are statistically significant from ones by the random score. Notably, the p-values are critically small (z-scores are blown up) due to our large sample size. Also, we note that the p-value is actually not statistically significant for last-order bias in InstructGPT; however, the first-order proportions are statistically significant, indicating that one must consider the test for both positions to get the full picture of the evaluator's tendencies in reference to the random baseline. For example, if both first-order and last-order were not statistically significant from the random
proportions, we might find that the evaluator is "unbiased," but the following may also undermine the capabilities of the automatic evaluator reduced to just random choice.</p>
<h3>B.6 LLM Performance and Agreement</h3>
<p>We detail the general agreement between machine preferences as similarly conducted in the humanmachine correlation study. Figure 5 visualizes the average Rank-Based Overlap between LLMs. We find that LLMs in their own size group (excluding the smallest size group) have a relative agreement with each other. For example, models in the largest size group $(&gt;100 B)$ are more in agreement amongst themselves than with models from other size groups. Furthermore, we also show the average valid response rate from different bias promptings in Table 12. We gather the proportion of valid responses by post-processing each "eval-gen" via pattern matching. After post-processing, we then label each output as a valid or invalid response, such that if a response is valid, we give one point to the preferred system.</p>
<h3>B.7 Model Size</h3>
<p>We conduct a supplementary experiment analyzing the impact of each bias for different models scaled by size in Table 13. We present results from a range of model sizes with LLAMAv2 and Vi-</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>First Order Z-Score</th>
<th>First Order P-Value</th>
<th>Last Order Z-Score</th>
<th>Last Order P-Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-4</td>
<td>8.45</td>
<td>$2.82 e-17$</td>
<td>26.55</td>
<td>$2.65 e-155$</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>-15.61</td>
<td>$6.68 e-55$</td>
<td>32.43</td>
<td>$9.50 e-231$</td>
</tr>
<tr>
<td>InStrUCTGPT</td>
<td>13.04</td>
<td>$7.08 e-39$</td>
<td>1.17</td>
<td>$2.4 e-1$</td>
</tr>
<tr>
<td>LLAMAv2</td>
<td>-33.12</td>
<td>$1.30 e-240$</td>
<td>17.97</td>
<td>$3.53 e-72$</td>
</tr>
<tr>
<td>LLAMA</td>
<td>-40.84</td>
<td>0</td>
<td>15.36</td>
<td>$2.95 e-53$</td>
</tr>
<tr>
<td>COHERE</td>
<td>-10.02</td>
<td>$1.20 e-23$</td>
<td>8.98</td>
<td>$2.59 e-19$</td>
</tr>
<tr>
<td>FALCON</td>
<td>-57.36</td>
<td>0</td>
<td>25.61</td>
<td>$1.24 e-144$</td>
</tr>
<tr>
<td>ALPACA</td>
<td>30.67</td>
<td>$1.29 e-206$</td>
<td>-61.45</td>
<td>0</td>
</tr>
<tr>
<td>Vicuna</td>
<td>-12.49</td>
<td>$8.44 e-36$</td>
<td>7.63</td>
<td>$2.29 e-14$</td>
</tr>
<tr>
<td>OpenASSIST</td>
<td>-37.27</td>
<td>$4.84 e-304$</td>
<td>13.93</td>
<td>$3.92 e-44$</td>
</tr>
<tr>
<td>Mistral</td>
<td>-20.09</td>
<td>$9.13 e-90$</td>
<td>32.56</td>
<td>$1.63 e-232$</td>
</tr>
<tr>
<td>OLMO</td>
<td>-54.54</td>
<td>0</td>
<td>22.08</td>
<td>$4.47 e-108$</td>
</tr>
<tr>
<td>BAIZE</td>
<td>35.46</td>
<td>$1.99 e-275$</td>
<td>-71.53</td>
<td>0</td>
</tr>
<tr>
<td>Koala</td>
<td>-21.60</td>
<td>$1.77 e-103$</td>
<td>12.18</td>
<td>$4.15 e-34$</td>
</tr>
<tr>
<td>WIZARDLM</td>
<td>20.65</td>
<td>$9.93 e-95$</td>
<td>-40.48</td>
<td>0</td>
</tr>
<tr>
<td>MPT</td>
<td>-30.71</td>
<td>$4.81 e-207$</td>
<td>16.49</td>
<td>$4.62 e-61$</td>
</tr>
</tbody>
</table>
<p>Table 10: Significance test scores for ORDER bias (first and last order preference) for each evaluator compared to the random baseline. We see almost every model shows significant results with $\alpha=0.05$.</p>
<p>CUNA. Interestingly, we see that the valid response rate within LLAMAv2 goes down as the model size is scaled up, but the impact of each bias greatly increases as the model size is scaled down (with the exception of SALIENCE BIAS). On the implicit bias benchmarks, LLAMAv2 exhibits more robust performance with the proportion of responses affected by each bias SALIENCE BIAS in which longer responses are much more strongly preferred. For the induced bias benchmarks, a similar trend is viewed in which the effect of each bias on the model as an evaluator is dampened in correlation to the model scale. On the contrary, Vicuna exhibits a stronger valid response rate as the model size is scaled; however, certain implicit biases are much more amplified, such as ORDER BIAS and SALIENCE BIAS. For implicit biases, Vicuna tends to prefer itself when actual model names are used as size is scaled smaller while tending to prefer much more verbose responses as model size is scaled higher. Across the induced biases, Vicuna performs more resiliently proportionally to scale, although still strongly influenced by BANDWAGON EFFECT but much less affected by ATTENTIONAL BIAS. We include another visualization correlating the overall performance on each of the bias benchmarks with model size for the main results in Figure 2a.</p>
<h2>B. 8 $N$-Rankwise setting: $N=4$</h2>
<p>We show the results and average rankings between four different models representing each of the different size groups: CHATGPT $(&gt;100 B)$, FALCON
$(&gt;40 B)$, ALPACA $(&gt;10 B)$, Vicuna $(&lt;10 B)$.
For the experimental setup, we conduct a smaller study, generating 100 responses from each of the 4 different LLMs using the Databricks Dolly15k dataset [conover2023datasheets] via the same instruction prompt template from Appendix C and the same evaluation prompt template from the ORDER bias.</p>
<p>We only employ this setting under the order bias setting in order to validate the complexity of the task that modern (smaller) LLMs aren't capable of performing yet. We perform each experiment by randomizing the order of each list of responses and prompt each LM-as-evaluator to order the list from best to worst (top to bottom) according to the same criterion as the pairwise study (providing the instruction/sample reference). Furthermore, we also track ORDER bias, calculated by the proportion of responses in which the first (randomly) placed model was also ranked first by the evaluator.</p>
<p>As viewed in Table 14, we find that most models besides the closed-source API models (e.g. OpenAI) have trouble generating a proper rank list for even an $N=4$ setting. This may be due to the increased complexity of the task [dziri2023semi] where the ranking of $N$ generations may become much more difficult as $N$ gets larger (since the task complexity increases).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">GPT4</th>
<th style="text-align: center;">ChatGPT</th>
<th style="text-align: center;">InStrUCTGPT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Ego</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.28</td>
</tr>
<tr>
<td style="text-align: left;">Longer Ego</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.43</td>
</tr>
<tr>
<td style="text-align: left;">Shorter Ego</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.56</td>
</tr>
</tbody>
</table>
<p>(a) Model Performance Comparison ( $&gt;175$ B)</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">LLAMAv2</th>
<th style="text-align: center;">LLAMA</th>
<th style="text-align: center;">COHERE</th>
<th style="text-align: center;">FALCON</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Ego</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.05</td>
</tr>
<tr>
<td style="text-align: left;">Longer Ego</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.6</td>
</tr>
<tr>
<td style="text-align: left;">Shorter Ego</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.4</td>
</tr>
</tbody>
</table>
<p>(b) Model Performance Comparison ( $&gt;40$ B)</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Alpaca</th>
<th style="text-align: center;">Vicuna</th>
<th style="text-align: center;">OpenAssist</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Ego</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Longer Ego</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Shorter Ego</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>(c) Model Performance Comparison ( $&gt;10$ B)</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Mistral</th>
<th style="text-align: center;">OLMO</th>
<th style="text-align: center;">Baize</th>
<th style="text-align: center;">Koala</th>
<th style="text-align: center;">WizardLM</th>
<th style="text-align: center;">MPT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Ego</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.21</td>
</tr>
<tr>
<td style="text-align: left;">Longer Ego</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.83</td>
</tr>
<tr>
<td style="text-align: left;">Shorter Ego</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">0.17</td>
</tr>
</tbody>
</table>
<p>(d) Model Performance Comparison ( $&lt;10$ B)</p>
<p>Table 11: Additional comparisons examining the proportion of EGOCENTRIC samples where the (self-preferred) model's generation was longer/shorter than the other generation.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Proportion of responses that were labeled bias for each bias benchmark. We visualize the distribution of the 15 models tested that varies by the y-axis. The red dashed line indicates the RANDOM threshold for each bias benchmark that serves as a litmus between biased and unbiased LMs-as-evaluators. The spread on the x-axis is randomly distributed for visual clarity.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The average RBO scores between LLMs. Higher RBO means higher similarity.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Avg.</th>
<th>Ord.</th>
<th>Comp.</th>
<th>Band.</th>
<th>Attn.</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT4</td>
<td>0.98</td>
<td>0.98</td>
<td>0.97</td>
<td>0.99</td>
<td>0.99</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>0.99</td>
<td>0.99</td>
<td>0.99</td>
<td>0.99</td>
<td>0.99</td>
</tr>
<tr>
<td>InStrUCTGPT</td>
<td>0.99</td>
<td>0.99</td>
<td>0.99</td>
<td>1.00</td>
<td>0.99</td>
</tr>
<tr>
<td>LLAMAv2</td>
<td>0.54</td>
<td>$\mathbf{0 . 1 7}$</td>
<td>$\mathbf{0 . 4 0}$</td>
<td>$\mathbf{0 . 4 3}$</td>
<td>0.91</td>
</tr>
<tr>
<td>LLAMA</td>
<td>$\mathbf{0 . 1 4}$</td>
<td>$\mathbf{0 . 2 2}$</td>
<td>$\mathbf{0 . 1 6}$</td>
<td>$\mathbf{0 . 0 3}$</td>
<td>0.58</td>
</tr>
<tr>
<td>COHERE</td>
<td>0.98</td>
<td>0.94</td>
<td>0.99</td>
<td>0.82</td>
<td>0.99</td>
</tr>
<tr>
<td>FALCON</td>
<td>0.72</td>
<td>0.72</td>
<td>$\mathbf{0 . 4 6}$</td>
<td>0.99</td>
<td>0.98</td>
</tr>
<tr>
<td>AlPACA</td>
<td>0.84</td>
<td>0.78</td>
<td>0.82</td>
<td>0.97</td>
<td>0.87</td>
</tr>
<tr>
<td>VICUNA</td>
<td>0.86</td>
<td>0.90</td>
<td>0.71</td>
<td>0.97</td>
<td>0.90</td>
</tr>
<tr>
<td>OPENASSIST</td>
<td>0.60</td>
<td>0.80</td>
<td>$\mathbf{0 . 3 2}$</td>
<td>0.95</td>
<td>0.94</td>
</tr>
<tr>
<td>Mistral</td>
<td>$\mathbf{0 . 9 9}$</td>
<td>0.99</td>
<td>0.99</td>
<td>0.99</td>
<td>0.99</td>
</tr>
<tr>
<td>Olmo</td>
<td>$\mathbf{0 . 2 5}$</td>
<td>$\mathbf{0 . 3 6}$</td>
<td>$\mathbf{0 . 0 6}$</td>
<td>$\mathbf{0 . 4 2}$</td>
<td>$\mathbf{0 . 1 5}$</td>
</tr>
<tr>
<td>BAIZE</td>
<td>0.96</td>
<td>0.98</td>
<td>0.87</td>
<td>0.99</td>
<td>0.99</td>
</tr>
<tr>
<td>Koala</td>
<td>$\mathbf{0 . 2 5}$</td>
<td>$\mathbf{0 . 2 9}$</td>
<td>$\mathbf{0 . 1 8}$</td>
<td>$\mathbf{0 . 2 3}$</td>
<td>$\mathbf{0 . 3 0}$</td>
</tr>
<tr>
<td>WIZARDLM</td>
<td>0.93</td>
<td>0.95</td>
<td>0.83</td>
<td>0.99</td>
<td>0.96</td>
</tr>
<tr>
<td>MPT</td>
<td>0.77</td>
<td>0.82</td>
<td>0.72</td>
<td>0.84</td>
<td>$\mathbf{0 . 3 2}$</td>
</tr>
</tbody>
</table>
<p>Table 12: Ratio for generating valid evaluations. Bolded numbers are ones in which less than half of the responses were invalid. We conjecture it may be due to lack of instruction-tuning that results in poor ability to follow instructions properly (often repeating the prompt itself, or printing out continuations of model answers).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Size</th>
<th style="text-align: center;">Order</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Compassion</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Egocent.</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Salience</th>
<th style="text-align: center;">Bandwag.</th>
<th style="text-align: center;">Attent.</th>
<th style="text-align: center;">Avg. Valid <br> Responses</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">First</td>
<td style="text-align: center;">Last</td>
<td style="text-align: center;">First</td>
<td style="text-align: center;">Last</td>
<td style="text-align: center;">Order</td>
<td style="text-align: center;">Comp.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLAMAv2</td>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.54</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.86</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.98</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna</td>
<td style="text-align: center;">33B</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">0.99</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.87</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.86</td>
</tr>
</tbody>
</table>
<p>Table 13: Performance comparison in proportion to their model scale. We view the overall scores across each of the bias benchmarks as well as their valid response rates.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Size</th>
<th style="text-align: left;">Valid <br> sponse</th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Order bias</th>
<th style="text-align: left;">ChatGPT <br> avg. rank</th>
<th style="text-align: left;">Falcon <br> avg. rank</th>
<th style="text-align: left;">Alpaca <br> avg. rank</th>
<th style="text-align: left;">Vicuna <br> avg. rank</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CHATGPT</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.94</td>
<td style="text-align: left;">0.32</td>
<td style="text-align: left;">2.3</td>
<td style="text-align: left;">2.5</td>
<td style="text-align: left;">2.6</td>
<td style="text-align: left;">2.6</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">FALCON</td>
<td style="text-align: left;">40B</td>
<td style="text-align: left;">0.38</td>
<td style="text-align: left;">0.39</td>
<td style="text-align: left;">2.6</td>
<td style="text-align: left;">2.3</td>
<td style="text-align: left;">2.6</td>
<td style="text-align: left;">2.5</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">ALPACA</td>
<td style="text-align: left;">13B</td>
<td style="text-align: left;">0.65</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">2.6</td>
<td style="text-align: left;">2.4</td>
<td style="text-align: left;">2.4</td>
<td style="text-align: left;">2.4</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">VICUNA</td>
<td style="text-align: left;">7B</td>
<td style="text-align: left;">0.02</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">1.5</td>
<td style="text-align: left;">4.0</td>
<td style="text-align: left;">3.0</td>
<td style="text-align: left;">1.5</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 14: We show the results of instructing models to perform a list-wise evaluation, by prompting each LM-asevaluator to organize a list of responses from 4 different models top to bottom with the first being the best response and the last being the worst response. We then take the average ranking of each of the models and display their results above for each LM-as-evaluator.</p>
<h2>C Prompt Templates</h2>
<p>We present each evaluation prompt utilized for models to evaluate the quality between two generations. We show each of the prompts (COMPASSION, BANDWAGON, ATTENTIONAL) derived from the original ORDER prompt in Section 4.2. We highlight each modification made from the original template.</p>
<p>Our generation instruction template looks like the following:</p>
<div class="codehilite"><pre><span></span><code>### Instruction:
### Response:
</code></pre></div>

<p>For evaluation, bias-specific prompts are formatted as follows:</p>
<h3>C. 1 Order</h3>
<div class="codehilite"><pre><span></span><code><span class="x">### Instruction: Which system&#39;s</span>
<span class="x">response is more coherent considering</span>
<span class="x">the reference and instruction?</span>
<span class="x">The instruction is: </span><span class="cp">{{</span><span class="nv">instruction</span><span class="cp">}}</span>
<span class="x">The reference is: </span><span class="cp">{{</span><span class="nv">reference</span><span class="cp">}}</span>
<span class="x">&quot;</span>
<span class="x">System Star:</span>
<span class="x">System Square:</span>
<span class="x">&quot;</span>
<span class="x">Please respond in the following</span>
<span class="x">format strictly: System _ is better</span>
<span class="x">Do not provide any additional text or</span>
<span class="x">explanation:</span>
<span class="x">### Response:</span>
</code></pre></div>

<p>Also, we underline that EGOCENTRIC bias was extracted under two dimensions, hence its double column for (ORDER / COMPASSION) in Table 2. Since EGOCENTRIC is a bias extracted implicitly, it involves two different prompts from C. 1 (Order) and C. 2 (Compassion Fade). In the case of C.1, since model names are completely anonymous, the addition of "(You)" is omitted (as shown in Table 1, and models cannot tell if they are self-evaluating. In contrast, C. 2 shows recognizable names; thus, "(You)" is appended to the system name when selfevaluating. We highlight that the added "(You)" within the EGOCENTRIC example in Table 1 serves to explicitly communicate an example of an egocentric bias to the reader and not necessarily the prompt itself.</p>
<h3>C. 2 Compassion Fade</h3>
<div class="codehilite"><pre><span></span><code><span class="x">### Instruction: Which system&#39;s</span>
<span class="x">response is more coherent considering</span>
<span class="x">the reference and instruction?</span>
<span class="x">The instruction is: </span><span class="cp">{{</span><span class="nv">instruction</span><span class="cp">}}</span>
<span class="x">The reference is: </span><span class="cp">{{</span><span class="nv">reference</span><span class="cp">}}</span>
<span class="x">&quot;</span>
<span class="x">{model1_name}: {model1_response}</span>
<span class="x">{model2_name}: {model2_response}</span>
<span class="x">&quot;</span>
<span class="x">Please response directly in the</span>
<span class="x">following format: System _ is better</span>
<span class="x">Do not provide any additional text or</span>
<span class="x">explanation:</span>
<span class="x">### Response:</span>
</code></pre></div>

<h3>C. 3 Bandwagon Effect</h3>
<div class="codehilite"><pre><span></span><code><span class="x">### Instruction: Which system&#39;s</span>
<span class="x">response is more coherent considering</span>
<span class="x">the reference and instruction?</span>
<span class="x">The instruction is: </span><span class="cp">{{</span><span class="nv">instruction</span><span class="cp">}}</span>
<span class="x">The reference is: </span><span class="cp">{{</span><span class="nv">reference</span><span class="cp">}}</span>
<span class="x">&quot;</span>
<span class="x">System Star: {model1_response}</span>
<span class="x">System Square: {model2_response}</span>
<span class="x">&quot;</span>
<span class="x">85% of people believe that {system}</span>
<span class="x">is better.</span>
<span class="x">Please response directly in the</span>
<span class="x">following format: System _ is better</span>
<span class="x">Do not provide any additional text or</span>
<span class="x">explanation:</span>
<span class="x">### Response:</span>
</code></pre></div>

<h3>C. 4 Attentional Bias</h3>
<div class="codehilite"><pre><span></span><code><span class="x">### Instruction: Which system&#39;s</span>
<span class="x">response is more coherent considering</span>
<span class="x">the reference and instruction?</span>
<span class="x">The instruction is: </span><span class="cp">{{</span><span class="nv">instruction</span><span class="cp">}}</span>
<span class="x">The reference is: </span><span class="cp">{{</span><span class="nv">reference</span><span class="cp">}}</span>
<span class="x">&quot;</span>
<span class="x">System Star: {model1_response}</span>
<span class="x">System Square: {model2_response}</span>
<span class="x">&quot;</span>
<span class="x">{distraction}</span>
<span class="x">Please response directly in the</span>
<span class="x">following format: System _ is better</span>
<span class="x">Do not provide any additional text or</span>
<span class="x">explanation:</span>
<span class="x">### Response:</span>
</code></pre></div>

<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ We concentrated $86 \%$ of all weights on the top 5 list positions, following webber2010ranking.
${ }^{5}$ At the time of human experiments, LLAMA2, Mistral, and OLMO were added later and instead involved responses by REDPAIANA and DOLLY. Thus, the ranking of those three models was not included involving pairwise comparisons between $\mathbf{1 3}$ models.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{6}$ Note that there are 25 batches in total for 750 pairs per bias and 75 human annotators&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2623 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2623</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2623</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-690743bbc30b5d37ee6f18fc4e3ef4b33f057b60</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/690743bbc30b5d37ee6f18fc4e3ef4b33f057b60" target="_blank">Bayesian Optimization with a Prior for the Optimum</a></p>
                <p><strong>Paper Venue:</strong> ECML/PKDD</p>
                <p><strong>Paper TL;DR:</strong> BOPrO allows users to inject their knowledge into the optimization process in the form of priors about which parts of the input space will yield the best performance, rather than BO’s standard priors over functions, which are much less intuitive for users.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2623.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2623.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BOPrO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Optimization with a Prior for the Optimum</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Bayesian optimization variant that allows users to inject priors over the location of the optimum (P_g(x)) and combines this prior with a standard probabilistic predictive model via a TPE-like pseudo-posterior to guide acquisition; the influence of the prior decays with iterations via a t/β exponent so the data-driven model eventually dominates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BOPrO</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>BOPrO accepts a user-specified prior P_g(x) (optionally factorized across dimensions), fits a standard BO predictive model p(y|x) (GPs or RFs in experiments), computes the Probability of Improvement (PI) M_g(x)=Φ((f_γ-μ_x)/σ_x) from that model, forms pseudo-posteriors g(x) ∝ P_g(x)·M_g(x)^{t/β} and b(x) analogously for 'bad' points, and plugs these into a TPE-style Expected Improvement acquisition: EI_fγ(x) ∝ (γ + (b(x)/g(x))(1-γ))^{-1}. The exponent t/β controls the prior vs data weighting (β hyperparameter); initial design samples are drawn from the prior. Optimization of the acquisition uses multi-start local search + CMA-ES.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General black-box experimental design / automated optimization (hyperparameter optimization, hardware design / compiler tuning for FPGAs in Spatial use-case), i.e., automated scientific experimental design and engineering parameter search.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Select next evaluation x_t by maximizing EI_fγ(x) computed from the pseudo-posteriors g(x), b(x). The pseudo-posterior encodes both user prior P_g(x) and model-based PI M_g(x) with exponent t/β, so early iterations allocate budget according to the prior and later iterations allocate according to model-driven PI (i.e., uncertainty and expected improvement). Initial budget allocation uses D+1 random samples drawn from P_g(x).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of expensive function evaluations / BO iterations (sample efficiency); reported comparisons are in iterations to reach a target performance. (No explicit FLOPs/dollar metric reported.)</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected Improvement (EI) as primary acquisition; Probability of Improvement (PI, computed analytically from the predictive Gaussian model) is used to build pseudo-posteriors; EI/PI serve as proxies for expected utility/information in allocation decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration/exploitation is balanced via the EI acquisition function (tradeoff between predicted mean and predictive uncertainty) and via the pseudo-posterior which multiplies the prior and model PI raised to t/β: early iterations favor exploitation guided by the prior, later iterations favor exploration/exploitation driven by model uncertainty/PI as t increases (β tunes the rate of prior forgetting).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity module; diversity emerges from model uncertainty (regions with high σ_x get higher PI and thus can be explored) and from the pseudo-posterior which can display multiple peaks encouraging exploration of distinct regions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of expensive function evaluations (BO budget B / iteration limit).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>BOPrO runs an initialization of D+1 samples from the prior, then iterates B times selecting next point by maximizing EI_fγ; β and γ are hyperparameters controlling prior weight and the quantile threshold for 'good' points. No explicit multi-fidelity or cost-aware scheduling is used; budget is managed by limiting total iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Improvements in objective function values (incumbent improvement / reaching lower function values), log simple regret. Breakthroughs are effectively identified as large decreases in incumbent y (performance thresholds defined via f_γ and observed best values).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported: On synthetic benchmark suite, BOPrO reaches performance that Spearmint achieves in 100 iterations after on average 15 iterations (reported as ~6.67× faster in sample-efficiency). On synthetic benchmarks: BOPrO w/ strong prior outperforms 10,000× random search baseline. Spatial (real-world) use-case: reported speedups vs expert baseline of 2.68× (shallow CNN), 1.06× (deep CNN), and 10.4× (MD Grid); vs HyperMapper: up to 1.73× speedup in early iterations (25–40) on MD Grid and 1.28× better final performance on MD Grid; converged faster than HyperMapper for CNNs (1.58× and 1.4× faster for shallow and deep CNN respectively).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Random Search (including 10,000× RS sampling), Spearmint (GP+EI), HyperMapper (RF-based BO used in Spatial), HyperOpt/TPE, SMAC, TuRBO (mentioned/comparisons in appendices).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>BOPrO with strong prior consistently outperforms random search and is more sample-efficient than Spearmint (6.67× faster to reach same target), outperforms sampling from the prior alone, and yields earlier and sometimes better final performance than HyperMapper on Spatial (e.g., MD Grid final 1.28× improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Sample-efficiency gains: ~6.67× faster than Spearmint on synthetic benchmarks (in iterations to reach a target), up to 10,000× improvement vs naive random search sampling baseline in the comparisons shown; domain-specific speedups versus manual expert configuration: up to 10.4× (MD Grid).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>BOPrO analyzes the tradeoff between prior knowledge and data by introducing the exponent t/β: higher β gives more weight to the prior and requires more data for the model to override the prior; authors experimentally show BOPrO is robust to misleading priors because the model weight grows with t and eventually washes out bad priors. No explicit analysis trading computational cost (e.g., runtime) vs information gain beyond sample-efficiency comparisons in iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Key principles: (1) Injecting a prior over the location of the optimum can drastically improve early-stage allocation of evaluations and sample-efficiency; (2) the prior must be downweighted over time (via t/β) so that the data-driven model can correct misleading priors; (3) use EI (or PI) with pseudo-posteriors to translate prior+model into acquisition decisions; (4) initialize with D+1 prior samples; (5) β is a tuning knob trading prior influence vs model-driven exploration—practically β=10 used in experiments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2623.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2623.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TPE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-structured Parzen Estimator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BO method that models p(x|y) with two densities g(x) (good) and b(x) (bad) separated by a quantile threshold y*, and chooses points by maximizing g(x)/b(x), which approximates EI under the TPE parametrization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Algorithms for hyper-parameter optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Tree-structured Parzen Estimator (TPE)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>TPE models the generative density p(x,y)=p(x|y)p(y) by building two densities g(x) and b(x) from observations below and above a quantile y*, sets p(x|y)=g(x) if y<y* else b(x), and shows that EI_{y*}(x) ∝ g(x)/b(x), enabling selection by maximizing g/b. It uses Parzen kernel density estimators to build g and b and is the basis for HyperOpt.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Hyperparameter optimization and black-box optimization / experimental design.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates evaluations by sampling x that maximize the g(x)/b(x) ratio (i.e., those more likely to be 'good' per the empirical densities), effectively allocating budget toward regions with high estimated probability of producing improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of function evaluations; cost of density estimation (kernel density estimator overhead) is fixed relative to evaluation cost in typical settings.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Approximate Expected Improvement represented by the g(x)/b(x) ratio; not explicitly information-theoretic but serves as expected utility proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Implicit via g/b ratio and choice of y* quantile: focusing on regions with high g(x) encourages exploitation of promising regions while b(x) provides normalization and maintains exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity-promoting mechanism beyond probabilistic sampling from estimated densities; multimodal g(x) can encourage diverse candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed evaluation budget (typical BO setup).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Selects next point based on g/b; initial design and iterative update of densities manage budget implicitly; no explicit budget-aware scheduling reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Improvements in objective (lower y) and empirical success rate (indirectly via regret curves in experiments reported in literature).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared in literature to EI-based GP BO; in this paper TPE is discussed and compared in appendices.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>TPE trades density estimation fidelity vs computation; paper notes TPE's parametrization facilitates EI computation. BOPrO generalizes TPE by allowing more flexible priors and combining with predictive models.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>TPE's g/b formulation provides an efficient heuristic for allocating evaluations toward promising regions, but TPE's KDE-based models are less sample-efficient than GP/RF in many settings; BOPrO bridges TPE and GP-style models.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2623.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2623.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayesian Optimization (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Optimization (BO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-efficient framework for optimizing expensive black-box functions by maintaining a probabilistic surrogate p(y|x) and selecting evaluations via an acquisition function (EI, PI, UCB, entropy-based methods, knowledge-gradient) that trades off predicted performance and uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bayesian Optimization (BO)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>BO fits a surrogate model (e.g., Gaussian Process, Random Forest) to past observations to produce p(y|x), then uses an acquisition function to propose the next x to evaluate by balancing exploitation (low predicted mean) and exploration (high predictive uncertainty). Common acquisition functions: Expected Improvement (EI), Probability of Improvement (PI), Upper Confidence Bound (UCB), Predictive Entropy Search, Knowledge Gradient.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General automated experimental design and resource allocation across scientific and engineering domains (hyperparameter tuning, simulation-based design, materials discovery, robotics).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate the next expensive evaluation by maximizing an acquisition function that encodes expected utility (e.g., EI) given the surrogate model; sequential decision-making under a fixed evaluation budget.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Usually measured in number of expensive function evaluations; sometimes wall-clock cost when evaluations are time-consuming.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Varies by acquisition: EI/PI use expected improvement; entropy-based acquisitions measure mutual information / predictive entropy; knowledge gradient directly estimates expected value of information.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Handled within acquisition functions: EI balances mean & variance; UCB uses mean+κ·std; entropy-based methods maximize expected information about the optimum; KG maximizes one-step expected value of information.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No universal explicit diversity mechanism; surrogate uncertainty encourages exploring diverse regions, and some methods (batch BO) add explicit diversity constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed budget of evaluations or time-constrained optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Acquisition optimization at each step until budget B exhausted; some BO variants incorporate cost-aware acquisitions or multi-fidelity methods (not central to this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Objective improvement / reduction in regret; often measured by best-so-far incumbent or probability of improvement beyond threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against random search, grid search, and other BO variants in literature.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Established BO theory analyzes exploration-exploitation tradeoffs and provides regret bounds for certain acquisition strategies; in practice choice of surrogate and acquisition determines efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>BO is effective for resource-limited black-box optimization; acquisition functions should be chosen to reflect the practitioner’s utility (e.g., EI for improvement-focused search, entropy for information-focused search).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2623.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2623.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Predictive Entropy Search (PES)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Predictive Entropy Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An entropy-based BO acquisition that selects evaluations to maximize expected reduction in entropy over the location of the global optimum (i.e., maximize information gain about the optimum).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Predictive entropy search for efficient global optimization of black-box functions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Predictive Entropy Search (PES)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>PES computes, for each candidate x, the expected reduction in entropy (uncertainty) about the global optimizer location after observing y at x; it explicitly optimizes information gain (mutual information) rather than immediate improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Black-box optimization where information gathering about the optimum is important (scientific discovery, active learning for global search).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate evaluations to maximize expected reduction in entropy about x*, i.e., maximize information gain per evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of function evaluations and additional overhead to compute mutual information approximations; computational cost of acquisition evaluation is higher than simple EI.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Mutual information / expected reduction in entropy over the optimum location.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Explicitly exploration-focused by targeting information about the optimizer; exploitation occurs indirectly when information suggests promising regions.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Encourages sampling in diverse regions if they reduce uncertainty about the optimum.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed evaluation budget/time limit.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Selects points that maximize information gain given budgeted number of evaluations; heavier computational cost per acquisition limits throughput.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Reduction in uncertainty about optimizer and improvements in incumbent objective through subsequent exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared in literature to EI/PI/UCB methods; mentioned in this paper as an alternative acquisition style.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>PES trades higher computational cost per acquisition (due to information computation) for potentially fewer evaluations needed to localize optimum; discussed as an alternative in the paper's background but not experimentally compared here.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Information-theoretic acquisitions can be preferable when each evaluation is extremely expensive and reducing uncertainty is paramount, but cost of acquisition computation matters.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2623.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2623.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Knowledge Gradient (KG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Gradient</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An acquisition strategy that chooses the next evaluation to maximize the expected incremental value (gain in expected best objective) following a single evaluation, i.e., one-step lookahead expected utility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Knowledge Gradient (KG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>KG computes the expected increase in the value of the solution (e.g., expected improvement in the incumbent) that would result from evaluating a candidate; it performs a one-step lookahead optimization of expected value of information.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Sequential experimental design where each evaluation's potential to improve final decision is explicitly valued.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate the next evaluation to the point with highest one-step expected value of information (expected improvement in the final chosen solution).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of function evaluations; additional computation for one-step lookahead increases acquisition evaluation cost.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected value of information measured as expected increment in best-so-far objective (one-step lookahead).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Balances exploration and exploitation by quantifying how much an observation at x would change the expected final selected solution.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity mechanism; diversity arises if diverse samples are expected to produce high KG.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed evaluation budget.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>KG directly optimizes the expected final-solution value for each candidate within the sequential budget; not used experimentally in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Expected increment in final solution value / incumbent improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>KG explicitly internalizes the tradeoff between sampling cost and expected improvement in final outcome, at the expense of higher per-step computation.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>KG is theoretically appealing where one-step value-of-information estimates are reliable; mentioned as an alternative acquisition in the paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2623.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2623.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Li+2020 posterior-sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Two-level approach sampling GP posterior then choosing config with highest prior (Li et al. 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-level BO approach where multiple configurations are sampled from a GP posterior and the one with the highest user prior is selected for evaluation; the prior is never downweighted.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Incorporating expert prior knowledge into experimental design via posterior sampling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Li et al. (2020) two-level prior-guided posterior sampling</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Authors sample a number of candidate configurations by maximizing samples from a GP posterior; among these candidates the configuration with the highest user-specified prior is chosen to evaluate. The approach uses the prior to break ties among GP-sampled candidates but does not progressively reduce prior influence; the prior remains in effect throughout.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Experimental design / BO with expert priors.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate evaluations by sampling multiple GP posterior-maximizers and selecting the one with highest prior score.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of evaluations and GP posterior sampling cost; no explicit monetary/CPU cost metric reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Indirectly uses GP posterior samples (which encode uncertainty) but selection criterion focuses on the prior; not an explicit expected information or EI objective.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration arises from posterior sampling; exploitation of the prior is enforced by selecting candidate with highest prior among sampled posterior maxima. No mechanism to decay prior over time.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Diversity can arise from sampling multiple posterior maxima but no explicit diversity-promoting component described.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed evaluation budget (implicit BO framework).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Not explicitly budget-aware beyond standard sequential selection; prior remains constant so mis-specified priors can bias allocation throughout.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not specified here; typical BO metrics like incumbent improvement would apply.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared conceptually in paper; BOPrO contrasted as being able to provably recover from misspecified priors while this method keeps prior influence forever.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper notes that unlike BOPrO, this method does not wash out a wrong prior, implying a tradeoff where strong priors may help but can be harmful if incorrect.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Keeping the prior permanently in the selection rule can bias allocation and prevent recovery from misleading priors; BOPrO's decaying prior is presented as an improvement.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2623.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2623.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Space-warping (Ramachandran)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Search-space warping with expert prior (Ramachandran et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BO approach that warps the input search space using the user prior via the probability integral transform, stretching high-prior regions and shrinking low-prior regions before fitting the surrogate model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Incorporating expert prior in bayesian optimisation via space warping</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Space-warping prior injection</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Transforms each input dimension via the probability integral transform induced by the user prior, effectively reparametrizing the search space so areas with high prior density expand; the surrogate is then fitted in the warped space. This biases the model and acquisition toward prior-favored regions.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Bayesian optimization with user priors across engineering and scientific optimization tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>After warping, standard BO acquisition selects points; warping changes allocation by altering the geometry of the search space.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Standard BO metrics (evaluations); additional cost for transforming and inverting warping transforms.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Uses standard BO acquisitions (EI/PI) on the warped space; not explicit information-theoretic criterion beyond that.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Warping biases exploration/exploitation toward prior regions; no explicit dynamic decay of prior influence, making recovery from misleading priors harder.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Warping can reduce diversity in low-prior regions by shrinking them, not explicitly promoting diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed evaluation budget.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Not budget-aware beyond BO; prior influence persists since model is fit in warped space.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not specified in paper; typical BO metrics apply.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Discussed in this paper as alternative prior-injection technique; BOPrO contrasted as being model-agnostic and controlling prior weight via β to allow recovery.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Warping trades stronger incorporation of prior for potential bias; BOPrO argues that fitting the model independently from the prior (and decaying prior) improves robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Transforming the space modifies allocation by stretching/shrinking regions; without decay this can make recovery from wrong priors difficult.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2623.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2623.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HyperMapper</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HyperMapper (Hyper-parameter / design-space exploration framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BO-based design-space exploration tool used for hardware/compiler parameter tuning (Spatial use-case); in this paper it is used as a state-of-the-art baseline employing Random Forest surrogates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Practical design space exploration</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>HyperMapper</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An automated design-space exploration tool that uses Random Forest surrogate models and BO-style acquisition to tune algorithmic/hardware/compiler parameters; used in prior work for Spatial FPGA design tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Hardware design / compiler parameter tuning (FPGA Spatial), general AutoML and design-space exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Sequential selection according to BO with RF surrogate; weighted EI by feasibility classifier when constraints are present (cBO variant used in Spatial experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of evaluations; used in practice where function evaluations are compilation + simulation runs (time-consuming), but the paper reports iteration-based comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>EI (or RF-based expected improvement proxies) with feasibility weighting used in constrained settings.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Standard BO acquisition with RF surrogate; feasibility-weighted EI for constrained problems; exploration via predictive uncertainty of RF.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit additional diversity mechanism described beyond surrogate uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed evaluation budget / expensive evaluations (practical scenario).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Runs sequential BO until budget exhausted; for Spatial a constrained BO variant was used with feasibility classifier weighting EI and with initialization possibly from prior.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Objective performance improvement (e.g., throughput/efficiency of generated hardware designs) and log regret.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In Spatial experiments in this paper, BOPrO outperformed HyperMapper in some settings: MD Grid early-stage up to 1.73× faster between iterations 25–40 and 1.28× better final performance; for CNNs BOPrO converged faster (1.58× and 1.4× faster).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as a baseline vs BOPrO in Spatial experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>BOPrO achieved faster convergence and sometimes better final performance than HyperMapper in the Spatial benchmarks presented.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Reported domain-specific gains when BOPrO used vs HyperMapper: up to 1.73× earlier improvement on MD Grid, and final 1.28× better performance on MD Grid; CNNs converged 1.58× and 1.4× faster respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper demonstrates that incorporating user priors with BOPrO improves early-stage allocation compared to HyperMapper's RF-only BO; both methods rely on surrogate uncertainty for exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>For Spatial, leveraging expert priors (as BOPrO does) can materially speed up discovery of better configurations compared to an RF-BO baseline like HyperMapper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2623.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2623.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spearmint</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spearmint (GP-based Bayesian Optimization implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GP + EI Bayesian optimization implementation commonly used as a strong baseline for hyperparameter optimization experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Practical bayesian optimization of machine learning algorithms</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Spearmint</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Spearmint uses Gaussian Processes as surrogates and EI as acquisition to sequentially propose configurations; used as a comparative baseline in the paper's synthetic benchmark experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Hyperparameter optimization / black-box optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Maximizes EI under GP surrogate to allocate evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of evaluations; surrogate training (GP) cost increases with data but is minor relative to expensive function evaluations in typical cases.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected Improvement (EI) computed from GP predictive mean and variance.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>EI balances predicted improvement and uncertainty from the GP posterior.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity mechanism beyond GP predictive uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Sequential acquisition until budget exhausted; no prior-injection over optimum location like BOPrO.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Reduction in objective value or log regret over iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used as baseline: BOPrO reaches the performance Spearmint achieves in 100 iterations after on average 15 iterations (reported ~6.67× faster).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Baseline compared against BOPrO and random search in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Spearmint is outperformed in sample-efficiency by BOPrO when a useful prior is available.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Spearmint represents standard BO without user priors; BOPrO shows advantages when informative priors exist.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Standard GP+EI BO is robust and well-studied, but can be improved in early stages by incorporating user priors as BOPrO does.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Algorithms for hyper-parameter optimization <em>(Rating: 2)</em></li>
                <li>Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures <em>(Rating: 2)</em></li>
                <li>Predictive entropy search for efficient global optimization of black-box functions <em>(Rating: 2)</em></li>
                <li>Incorporating expert prior knowledge into experimental design via posterior sampling <em>(Rating: 2)</em></li>
                <li>Incorporating expert prior in bayesian optimisation via space warping <em>(Rating: 2)</em></li>
                <li>Practical bayesian optimization of machine learning algorithms <em>(Rating: 2)</em></li>
                <li>BOHB: robust and efficient hyperparameter optimization at scale <em>(Rating: 1)</em></li>
                <li>SMAC v3: Algorithm configuration in python <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2623",
    "paper_id": "paper-690743bbc30b5d37ee6f18fc4e3ef4b33f057b60",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "BOPrO",
            "name_full": "Bayesian Optimization with a Prior for the Optimum",
            "brief_description": "A Bayesian optimization variant that allows users to inject priors over the location of the optimum (P_g(x)) and combines this prior with a standard probabilistic predictive model via a TPE-like pseudo-posterior to guide acquisition; the influence of the prior decays with iterations via a t/β exponent so the data-driven model eventually dominates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "BOPrO",
            "system_description": "BOPrO accepts a user-specified prior P_g(x) (optionally factorized across dimensions), fits a standard BO predictive model p(y|x) (GPs or RFs in experiments), computes the Probability of Improvement (PI) M_g(x)=Φ((f_γ-μ_x)/σ_x) from that model, forms pseudo-posteriors g(x) ∝ P_g(x)·M_g(x)^{t/β} and b(x) analogously for 'bad' points, and plugs these into a TPE-style Expected Improvement acquisition: EI_fγ(x) ∝ (γ + (b(x)/g(x))(1-γ))^{-1}. The exponent t/β controls the prior vs data weighting (β hyperparameter); initial design samples are drawn from the prior. Optimization of the acquisition uses multi-start local search + CMA-ES.",
            "application_domain": "General black-box experimental design / automated optimization (hyperparameter optimization, hardware design / compiler tuning for FPGAs in Spatial use-case), i.e., automated scientific experimental design and engineering parameter search.",
            "resource_allocation_strategy": "Select next evaluation x_t by maximizing EI_fγ(x) computed from the pseudo-posteriors g(x), b(x). The pseudo-posterior encodes both user prior P_g(x) and model-based PI M_g(x) with exponent t/β, so early iterations allocate budget according to the prior and later iterations allocate according to model-driven PI (i.e., uncertainty and expected improvement). Initial budget allocation uses D+1 random samples drawn from P_g(x).",
            "computational_cost_metric": "Number of expensive function evaluations / BO iterations (sample efficiency); reported comparisons are in iterations to reach a target performance. (No explicit FLOPs/dollar metric reported.)",
            "information_gain_metric": "Expected Improvement (EI) as primary acquisition; Probability of Improvement (PI, computed analytically from the predictive Gaussian model) is used to build pseudo-posteriors; EI/PI serve as proxies for expected utility/information in allocation decisions.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Exploration/exploitation is balanced via the EI acquisition function (tradeoff between predicted mean and predictive uncertainty) and via the pseudo-posterior which multiplies the prior and model PI raised to t/β: early iterations favor exploitation guided by the prior, later iterations favor exploration/exploitation driven by model uncertainty/PI as t increases (β tunes the rate of prior forgetting).",
            "diversity_mechanism": "No explicit diversity module; diversity emerges from model uncertainty (regions with high σ_x get higher PI and thus can be explored) and from the pseudo-posterior which can display multiple peaks encouraging exploration of distinct regions.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed number of expensive function evaluations (BO budget B / iteration limit).",
            "budget_constraint_handling": "BOPrO runs an initialization of D+1 samples from the prior, then iterates B times selecting next point by maximizing EI_fγ; β and γ are hyperparameters controlling prior weight and the quantile threshold for 'good' points. No explicit multi-fidelity or cost-aware scheduling is used; budget is managed by limiting total iterations.",
            "breakthrough_discovery_metric": "Improvements in objective function values (incumbent improvement / reaching lower function values), log simple regret. Breakthroughs are effectively identified as large decreases in incumbent y (performance thresholds defined via f_γ and observed best values).",
            "performance_metrics": "Reported: On synthetic benchmark suite, BOPrO reaches performance that Spearmint achieves in 100 iterations after on average 15 iterations (reported as ~6.67× faster in sample-efficiency). On synthetic benchmarks: BOPrO w/ strong prior outperforms 10,000× random search baseline. Spatial (real-world) use-case: reported speedups vs expert baseline of 2.68× (shallow CNN), 1.06× (deep CNN), and 10.4× (MD Grid); vs HyperMapper: up to 1.73× speedup in early iterations (25–40) on MD Grid and 1.28× better final performance on MD Grid; converged faster than HyperMapper for CNNs (1.58× and 1.4× faster for shallow and deep CNN respectively).",
            "comparison_baseline": "Random Search (including 10,000× RS sampling), Spearmint (GP+EI), HyperMapper (RF-based BO used in Spatial), HyperOpt/TPE, SMAC, TuRBO (mentioned/comparisons in appendices).",
            "performance_vs_baseline": "BOPrO with strong prior consistently outperforms random search and is more sample-efficient than Spearmint (6.67× faster to reach same target), outperforms sampling from the prior alone, and yields earlier and sometimes better final performance than HyperMapper on Spatial (e.g., MD Grid final 1.28× improvement).",
            "efficiency_gain": "Sample-efficiency gains: ~6.67× faster than Spearmint on synthetic benchmarks (in iterations to reach a target), up to 10,000× improvement vs naive random search sampling baseline in the comparisons shown; domain-specific speedups versus manual expert configuration: up to 10.4× (MD Grid).",
            "tradeoff_analysis": "BOPrO analyzes the tradeoff between prior knowledge and data by introducing the exponent t/β: higher β gives more weight to the prior and requires more data for the model to override the prior; authors experimentally show BOPrO is robust to misleading priors because the model weight grows with t and eventually washes out bad priors. No explicit analysis trading computational cost (e.g., runtime) vs information gain beyond sample-efficiency comparisons in iterations.",
            "optimal_allocation_findings": "Key principles: (1) Injecting a prior over the location of the optimum can drastically improve early-stage allocation of evaluations and sample-efficiency; (2) the prior must be downweighted over time (via t/β) so that the data-driven model can correct misleading priors; (3) use EI (or PI) with pseudo-posteriors to translate prior+model into acquisition decisions; (4) initialize with D+1 prior samples; (5) β is a tuning knob trading prior influence vs model-driven exploration—practically β=10 used in experiments.",
            "uuid": "e2623.0"
        },
        {
            "name_short": "TPE",
            "name_full": "Tree-structured Parzen Estimator",
            "brief_description": "A BO method that models p(x|y) with two densities g(x) (good) and b(x) (bad) separated by a quantile threshold y*, and chooses points by maximizing g(x)/b(x), which approximates EI under the TPE parametrization.",
            "citation_title": "Algorithms for hyper-parameter optimization",
            "mention_or_use": "mention",
            "system_name": "Tree-structured Parzen Estimator (TPE)",
            "system_description": "TPE models the generative density p(x,y)=p(x|y)p(y) by building two densities g(x) and b(x) from observations below and above a quantile y*, sets p(x|y)=g(x) if y&lt;y* else b(x), and shows that EI_{y*}(x) ∝ g(x)/b(x), enabling selection by maximizing g/b. It uses Parzen kernel density estimators to build g and b and is the basis for HyperOpt.",
            "application_domain": "Hyperparameter optimization and black-box optimization / experimental design.",
            "resource_allocation_strategy": "Allocates evaluations by sampling x that maximize the g(x)/b(x) ratio (i.e., those more likely to be 'good' per the empirical densities), effectively allocating budget toward regions with high estimated probability of producing improvement.",
            "computational_cost_metric": "Number of function evaluations; cost of density estimation (kernel density estimator overhead) is fixed relative to evaluation cost in typical settings.",
            "information_gain_metric": "Approximate Expected Improvement represented by the g(x)/b(x) ratio; not explicitly information-theoretic but serves as expected utility proxy.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Implicit via g/b ratio and choice of y* quantile: focusing on regions with high g(x) encourages exploitation of promising regions while b(x) provides normalization and maintains exploration.",
            "diversity_mechanism": "No explicit diversity-promoting mechanism beyond probabilistic sampling from estimated densities; multimodal g(x) can encourage diverse candidates.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed evaluation budget (typical BO setup).",
            "budget_constraint_handling": "Selects next point based on g/b; initial design and iterative update of densities manage budget implicitly; no explicit budget-aware scheduling reported here.",
            "breakthrough_discovery_metric": "Improvements in objective (lower y) and empirical success rate (indirectly via regret curves in experiments reported in literature).",
            "performance_metrics": null,
            "comparison_baseline": "Compared in literature to EI-based GP BO; in this paper TPE is discussed and compared in appendices.",
            "performance_vs_baseline": null,
            "efficiency_gain": null,
            "tradeoff_analysis": "TPE trades density estimation fidelity vs computation; paper notes TPE's parametrization facilitates EI computation. BOPrO generalizes TPE by allowing more flexible priors and combining with predictive models.",
            "optimal_allocation_findings": "TPE's g/b formulation provides an efficient heuristic for allocating evaluations toward promising regions, but TPE's KDE-based models are less sample-efficient than GP/RF in many settings; BOPrO bridges TPE and GP-style models.",
            "uuid": "e2623.1"
        },
        {
            "name_short": "Bayesian Optimization (general)",
            "name_full": "Bayesian Optimization (BO)",
            "brief_description": "A data-efficient framework for optimizing expensive black-box functions by maintaining a probabilistic surrogate p(y|x) and selecting evaluations via an acquisition function (EI, PI, UCB, entropy-based methods, knowledge-gradient) that trades off predicted performance and uncertainty.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Bayesian Optimization (BO)",
            "system_description": "BO fits a surrogate model (e.g., Gaussian Process, Random Forest) to past observations to produce p(y|x), then uses an acquisition function to propose the next x to evaluate by balancing exploitation (low predicted mean) and exploration (high predictive uncertainty). Common acquisition functions: Expected Improvement (EI), Probability of Improvement (PI), Upper Confidence Bound (UCB), Predictive Entropy Search, Knowledge Gradient.",
            "application_domain": "General automated experimental design and resource allocation across scientific and engineering domains (hyperparameter tuning, simulation-based design, materials discovery, robotics).",
            "resource_allocation_strategy": "Allocate the next expensive evaluation by maximizing an acquisition function that encodes expected utility (e.g., EI) given the surrogate model; sequential decision-making under a fixed evaluation budget.",
            "computational_cost_metric": "Usually measured in number of expensive function evaluations; sometimes wall-clock cost when evaluations are time-consuming.",
            "information_gain_metric": "Varies by acquisition: EI/PI use expected improvement; entropy-based acquisitions measure mutual information / predictive entropy; knowledge gradient directly estimates expected value of information.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Handled within acquisition functions: EI balances mean & variance; UCB uses mean+κ·std; entropy-based methods maximize expected information about the optimum; KG maximizes one-step expected value of information.",
            "diversity_mechanism": "No universal explicit diversity mechanism; surrogate uncertainty encourages exploring diverse regions, and some methods (batch BO) add explicit diversity constraints.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed budget of evaluations or time-constrained optimization.",
            "budget_constraint_handling": "Acquisition optimization at each step until budget B exhausted; some BO variants incorporate cost-aware acquisitions or multi-fidelity methods (not central to this paper).",
            "breakthrough_discovery_metric": "Objective improvement / reduction in regret; often measured by best-so-far incumbent or probability of improvement beyond threshold.",
            "performance_metrics": null,
            "comparison_baseline": "Compared against random search, grid search, and other BO variants in literature.",
            "performance_vs_baseline": null,
            "efficiency_gain": null,
            "tradeoff_analysis": "Established BO theory analyzes exploration-exploitation tradeoffs and provides regret bounds for certain acquisition strategies; in practice choice of surrogate and acquisition determines efficiency.",
            "optimal_allocation_findings": "BO is effective for resource-limited black-box optimization; acquisition functions should be chosen to reflect the practitioner’s utility (e.g., EI for improvement-focused search, entropy for information-focused search).",
            "uuid": "e2623.2"
        },
        {
            "name_short": "Predictive Entropy Search (PES)",
            "name_full": "Predictive Entropy Search",
            "brief_description": "An entropy-based BO acquisition that selects evaluations to maximize expected reduction in entropy over the location of the global optimum (i.e., maximize information gain about the optimum).",
            "citation_title": "Predictive entropy search for efficient global optimization of black-box functions",
            "mention_or_use": "mention",
            "system_name": "Predictive Entropy Search (PES)",
            "system_description": "PES computes, for each candidate x, the expected reduction in entropy (uncertainty) about the global optimizer location after observing y at x; it explicitly optimizes information gain (mutual information) rather than immediate improvement.",
            "application_domain": "Black-box optimization where information gathering about the optimum is important (scientific discovery, active learning for global search).",
            "resource_allocation_strategy": "Allocate evaluations to maximize expected reduction in entropy about x*, i.e., maximize information gain per evaluation.",
            "computational_cost_metric": "Number of function evaluations and additional overhead to compute mutual information approximations; computational cost of acquisition evaluation is higher than simple EI.",
            "information_gain_metric": "Mutual information / expected reduction in entropy over the optimum location.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Explicitly exploration-focused by targeting information about the optimizer; exploitation occurs indirectly when information suggests promising regions.",
            "diversity_mechanism": "Encourages sampling in diverse regions if they reduce uncertainty about the optimum.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed evaluation budget/time limit.",
            "budget_constraint_handling": "Selects points that maximize information gain given budgeted number of evaluations; heavier computational cost per acquisition limits throughput.",
            "breakthrough_discovery_metric": "Reduction in uncertainty about optimizer and improvements in incumbent objective through subsequent exploitation.",
            "performance_metrics": null,
            "comparison_baseline": "Compared in literature to EI/PI/UCB methods; mentioned in this paper as an alternative acquisition style.",
            "performance_vs_baseline": null,
            "efficiency_gain": null,
            "tradeoff_analysis": "PES trades higher computational cost per acquisition (due to information computation) for potentially fewer evaluations needed to localize optimum; discussed as an alternative in the paper's background but not experimentally compared here.",
            "optimal_allocation_findings": "Information-theoretic acquisitions can be preferable when each evaluation is extremely expensive and reducing uncertainty is paramount, but cost of acquisition computation matters.",
            "uuid": "e2623.3"
        },
        {
            "name_short": "Knowledge Gradient (KG)",
            "name_full": "Knowledge Gradient",
            "brief_description": "An acquisition strategy that chooses the next evaluation to maximize the expected incremental value (gain in expected best objective) following a single evaluation, i.e., one-step lookahead expected utility.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Knowledge Gradient (KG)",
            "system_description": "KG computes the expected increase in the value of the solution (e.g., expected improvement in the incumbent) that would result from evaluating a candidate; it performs a one-step lookahead optimization of expected value of information.",
            "application_domain": "Sequential experimental design where each evaluation's potential to improve final decision is explicitly valued.",
            "resource_allocation_strategy": "Allocate the next evaluation to the point with highest one-step expected value of information (expected improvement in the final chosen solution).",
            "computational_cost_metric": "Number of function evaluations; additional computation for one-step lookahead increases acquisition evaluation cost.",
            "information_gain_metric": "Expected value of information measured as expected increment in best-so-far objective (one-step lookahead).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Balances exploration and exploitation by quantifying how much an observation at x would change the expected final selected solution.",
            "diversity_mechanism": "No explicit diversity mechanism; diversity arises if diverse samples are expected to produce high KG.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed evaluation budget.",
            "budget_constraint_handling": "KG directly optimizes the expected final-solution value for each candidate within the sequential budget; not used experimentally in this paper.",
            "breakthrough_discovery_metric": "Expected increment in final solution value / incumbent improvement.",
            "performance_metrics": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "efficiency_gain": null,
            "tradeoff_analysis": "KG explicitly internalizes the tradeoff between sampling cost and expected improvement in final outcome, at the expense of higher per-step computation.",
            "optimal_allocation_findings": "KG is theoretically appealing where one-step value-of-information estimates are reliable; mentioned as an alternative acquisition in the paper.",
            "uuid": "e2623.4"
        },
        {
            "name_short": "Li+2020 posterior-sampling",
            "name_full": "Two-level approach sampling GP posterior then choosing config with highest prior (Li et al. 2020)",
            "brief_description": "A two-level BO approach where multiple configurations are sampled from a GP posterior and the one with the highest user prior is selected for evaluation; the prior is never downweighted.",
            "citation_title": "Incorporating expert prior knowledge into experimental design via posterior sampling",
            "mention_or_use": "mention",
            "system_name": "Li et al. (2020) two-level prior-guided posterior sampling",
            "system_description": "Authors sample a number of candidate configurations by maximizing samples from a GP posterior; among these candidates the configuration with the highest user-specified prior is chosen to evaluate. The approach uses the prior to break ties among GP-sampled candidates but does not progressively reduce prior influence; the prior remains in effect throughout.",
            "application_domain": "Experimental design / BO with expert priors.",
            "resource_allocation_strategy": "Allocate evaluations by sampling multiple GP posterior-maximizers and selecting the one with highest prior score.",
            "computational_cost_metric": "Number of evaluations and GP posterior sampling cost; no explicit monetary/CPU cost metric reported here.",
            "information_gain_metric": "Indirectly uses GP posterior samples (which encode uncertainty) but selection criterion focuses on the prior; not an explicit expected information or EI objective.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Exploration arises from posterior sampling; exploitation of the prior is enforced by selecting candidate with highest prior among sampled posterior maxima. No mechanism to decay prior over time.",
            "diversity_mechanism": "Diversity can arise from sampling multiple posterior maxima but no explicit diversity-promoting component described.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed evaluation budget (implicit BO framework).",
            "budget_constraint_handling": "Not explicitly budget-aware beyond standard sequential selection; prior remains constant so mis-specified priors can bias allocation throughout.",
            "breakthrough_discovery_metric": "Not specified here; typical BO metrics like incumbent improvement would apply.",
            "performance_metrics": null,
            "comparison_baseline": "Compared conceptually in paper; BOPrO contrasted as being able to provably recover from misspecified priors while this method keeps prior influence forever.",
            "performance_vs_baseline": null,
            "efficiency_gain": null,
            "tradeoff_analysis": "Paper notes that unlike BOPrO, this method does not wash out a wrong prior, implying a tradeoff where strong priors may help but can be harmful if incorrect.",
            "optimal_allocation_findings": "Keeping the prior permanently in the selection rule can bias allocation and prevent recovery from misleading priors; BOPrO's decaying prior is presented as an improvement.",
            "uuid": "e2623.5"
        },
        {
            "name_short": "Space-warping (Ramachandran)",
            "name_full": "Search-space warping with expert prior (Ramachandran et al.)",
            "brief_description": "A BO approach that warps the input search space using the user prior via the probability integral transform, stretching high-prior regions and shrinking low-prior regions before fitting the surrogate model.",
            "citation_title": "Incorporating expert prior in bayesian optimisation via space warping",
            "mention_or_use": "mention",
            "system_name": "Space-warping prior injection",
            "system_description": "Transforms each input dimension via the probability integral transform induced by the user prior, effectively reparametrizing the search space so areas with high prior density expand; the surrogate is then fitted in the warped space. This biases the model and acquisition toward prior-favored regions.",
            "application_domain": "Bayesian optimization with user priors across engineering and scientific optimization tasks.",
            "resource_allocation_strategy": "After warping, standard BO acquisition selects points; warping changes allocation by altering the geometry of the search space.",
            "computational_cost_metric": "Standard BO metrics (evaluations); additional cost for transforming and inverting warping transforms.",
            "information_gain_metric": "Uses standard BO acquisitions (EI/PI) on the warped space; not explicit information-theoretic criterion beyond that.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Warping biases exploration/exploitation toward prior regions; no explicit dynamic decay of prior influence, making recovery from misleading priors harder.",
            "diversity_mechanism": "Warping can reduce diversity in low-prior regions by shrinking them, not explicitly promoting diversity.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed evaluation budget.",
            "budget_constraint_handling": "Not budget-aware beyond BO; prior influence persists since model is fit in warped space.",
            "breakthrough_discovery_metric": "Not specified in paper; typical BO metrics apply.",
            "performance_metrics": null,
            "comparison_baseline": "Discussed in this paper as alternative prior-injection technique; BOPrO contrasted as being model-agnostic and controlling prior weight via β to allow recovery.",
            "performance_vs_baseline": null,
            "efficiency_gain": null,
            "tradeoff_analysis": "Warping trades stronger incorporation of prior for potential bias; BOPrO argues that fitting the model independently from the prior (and decaying prior) improves robustness.",
            "optimal_allocation_findings": "Transforming the space modifies allocation by stretching/shrinking regions; without decay this can make recovery from wrong priors difficult.",
            "uuid": "e2623.6"
        },
        {
            "name_short": "HyperMapper",
            "name_full": "HyperMapper (Hyper-parameter / design-space exploration framework)",
            "brief_description": "A BO-based design-space exploration tool used for hardware/compiler parameter tuning (Spatial use-case); in this paper it is used as a state-of-the-art baseline employing Random Forest surrogates.",
            "citation_title": "Practical design space exploration",
            "mention_or_use": "use",
            "system_name": "HyperMapper",
            "system_description": "An automated design-space exploration tool that uses Random Forest surrogate models and BO-style acquisition to tune algorithmic/hardware/compiler parameters; used in prior work for Spatial FPGA design tuning.",
            "application_domain": "Hardware design / compiler parameter tuning (FPGA Spatial), general AutoML and design-space exploration.",
            "resource_allocation_strategy": "Sequential selection according to BO with RF surrogate; weighted EI by feasibility classifier when constraints are present (cBO variant used in Spatial experiments).",
            "computational_cost_metric": "Number of evaluations; used in practice where function evaluations are compilation + simulation runs (time-consuming), but the paper reports iteration-based comparisons.",
            "information_gain_metric": "EI (or RF-based expected improvement proxies) with feasibility weighting used in constrained settings.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Standard BO acquisition with RF surrogate; feasibility-weighted EI for constrained problems; exploration via predictive uncertainty of RF.",
            "diversity_mechanism": "No explicit additional diversity mechanism described beyond surrogate uncertainty.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed evaluation budget / expensive evaluations (practical scenario).",
            "budget_constraint_handling": "Runs sequential BO until budget exhausted; for Spatial a constrained BO variant was used with feasibility classifier weighting EI and with initialization possibly from prior.",
            "breakthrough_discovery_metric": "Objective performance improvement (e.g., throughput/efficiency of generated hardware designs) and log regret.",
            "performance_metrics": "In Spatial experiments in this paper, BOPrO outperformed HyperMapper in some settings: MD Grid early-stage up to 1.73× faster between iterations 25–40 and 1.28× better final performance; for CNNs BOPrO converged faster (1.58× and 1.4× faster).",
            "comparison_baseline": "Used as a baseline vs BOPrO in Spatial experiments.",
            "performance_vs_baseline": "BOPrO achieved faster convergence and sometimes better final performance than HyperMapper in the Spatial benchmarks presented.",
            "efficiency_gain": "Reported domain-specific gains when BOPrO used vs HyperMapper: up to 1.73× earlier improvement on MD Grid, and final 1.28× better performance on MD Grid; CNNs converged 1.58× and 1.4× faster respectively.",
            "tradeoff_analysis": "Paper demonstrates that incorporating user priors with BOPrO improves early-stage allocation compared to HyperMapper's RF-only BO; both methods rely on surrogate uncertainty for exploration.",
            "optimal_allocation_findings": "For Spatial, leveraging expert priors (as BOPrO does) can materially speed up discovery of better configurations compared to an RF-BO baseline like HyperMapper.",
            "uuid": "e2623.7"
        },
        {
            "name_short": "Spearmint",
            "name_full": "Spearmint (GP-based Bayesian Optimization implementation)",
            "brief_description": "A GP + EI Bayesian optimization implementation commonly used as a strong baseline for hyperparameter optimization experiments.",
            "citation_title": "Practical bayesian optimization of machine learning algorithms",
            "mention_or_use": "use",
            "system_name": "Spearmint",
            "system_description": "Spearmint uses Gaussian Processes as surrogates and EI as acquisition to sequentially propose configurations; used as a comparative baseline in the paper's synthetic benchmark experiments.",
            "application_domain": "Hyperparameter optimization / black-box optimization.",
            "resource_allocation_strategy": "Maximizes EI under GP surrogate to allocate evaluations.",
            "computational_cost_metric": "Number of evaluations; surrogate training (GP) cost increases with data but is minor relative to expensive function evaluations in typical cases.",
            "information_gain_metric": "Expected Improvement (EI) computed from GP predictive mean and variance.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "EI balances predicted improvement and uncertainty from the GP posterior.",
            "diversity_mechanism": "No explicit diversity mechanism beyond GP predictive uncertainty.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed number of evaluations.",
            "budget_constraint_handling": "Sequential acquisition until budget exhausted; no prior-injection over optimum location like BOPrO.",
            "breakthrough_discovery_metric": "Reduction in objective value or log regret over iterations.",
            "performance_metrics": "Used as baseline: BOPrO reaches the performance Spearmint achieves in 100 iterations after on average 15 iterations (reported ~6.67× faster).",
            "comparison_baseline": "Baseline compared against BOPrO and random search in the paper.",
            "performance_vs_baseline": "Spearmint is outperformed in sample-efficiency by BOPrO when a useful prior is available.",
            "efficiency_gain": null,
            "tradeoff_analysis": "Spearmint represents standard BO without user priors; BOPrO shows advantages when informative priors exist.",
            "optimal_allocation_findings": "Standard GP+EI BO is robust and well-studied, but can be improved in early stages by incorporating user priors as BOPrO does.",
            "uuid": "e2623.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Algorithms for hyper-parameter optimization",
            "rating": 2
        },
        {
            "paper_title": "Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures",
            "rating": 2
        },
        {
            "paper_title": "Predictive entropy search for efficient global optimization of black-box functions",
            "rating": 2
        },
        {
            "paper_title": "Incorporating expert prior knowledge into experimental design via posterior sampling",
            "rating": 2
        },
        {
            "paper_title": "Incorporating expert prior in bayesian optimisation via space warping",
            "rating": 2
        },
        {
            "paper_title": "Practical bayesian optimization of machine learning algorithms",
            "rating": 2
        },
        {
            "paper_title": "BOHB: robust and efficient hyperparameter optimization at scale",
            "rating": 1
        },
        {
            "paper_title": "SMAC v3: Algorithm configuration in python",
            "rating": 1
        }
    ],
    "cost": 0.0230015,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Bayesian Optimization with a Prior for the Optimum</h1>
<p>Artur Souza ${ }^{1}$, Luigi Nardi ${ }^{2,3}$, Leonardo B. Oliveira ${ }^{1}$, Kunle Olukotun ${ }^{3}$, Marius Lindauer ${ }^{4}$, and Frank Hutter ${ }^{5,6}$<br>${ }^{1}$ Universidade Federal de Minas Gerais<br>{arturluis, leob}@dcc.ufmg.br<br>${ }^{2}$ Lund University<br>luigi.nardi@cs.lth.se<br>${ }^{3}$ Stanford University<br>{lnardi, kunle}@stanford.edu<br>${ }^{4}$ Leibniz University Hannover<br>lindauer@tnt.uni-hannover.de<br>${ }^{5}$ University of Freiburg<br>${ }^{6}$ Bosch Center for Artificial Intelligence<br>fh@cs.uni-freiburg.de</p>
<h4>Abstract</h4>
<p>While Bayesian Optimization (BO) is a very popular method for optimizing expensive black-box functions, it fails to leverage the experience of domain experts. This causes BO to waste function evaluations on bad design choices (e.g., machine learning hyperparameters) that the expert already knows to work poorly. To address this issue, we introduce Bayesian Optimization with a Prior for the Optimum (BOPrO). BOPrO allows users to inject their knowledge into the optimization process in the form of priors about which parts of the input space will yield the best performance, rather than BO's standard priors over functions, which are much less intuitive for users. BOPrO then combines these priors with BO's standard probabilistic model to form a pseudo-posterior used to select which points to evaluate next. We show that BOPrO is around $6.67 \times$ faster than state-of-the-art methods on a common suite of benchmarks, and achieves a new state-of-the-art performance on a real-world hardware design application. We also show that BOPrO converges faster even if the priors for the optimum are not entirely accurate and that it robustly recovers from misleading priors.</p>
<h2>1 Introduction</h2>
<p>Bayesian Optimization (BO) is a data-efficient method for the joint optimization of design choices that has gained great popularity in recent years. It is impacting a wide range of areas, including hyperparameter optimization [4311], AutoML [21], robotics [6], computer vision [324], Computer Go [7], hardware design [3325], and many others. It promises greater automation so as to increase both product quality and human productivity. As a result, BO is also established in large tech companies, e.g., Google [14] and Facebook [1].</p>
<p>Nevertheless, domain experts often have substantial prior knowledge that standard BO cannot easily incorporate so far 46. Users can incorporate prior knowledge by narrowing the search space; however, this type of hard prior can lead to poor performance by missing important regions. BO also supports a prior over functions $p(f)$, e.g., via a kernel function. However, this is not the prior domain experts have: they often know which ranges of hyperparameters tend to work best [38], and are able to specify a probability distribution $p_{\text {best }}(\boldsymbol{x})$ to quantify these priors; e.g., many users of the Adam optimizer [23] know that its best learning rate is often in the vicinity of $1 \times 10^{-3}$ (give or take an order of magnitude), yet one may not know the accuracy one may achieve in a new application. Similarly, Clarke et al. [8] derived neural network hyperparameter priors for image datasets based on their experience with five datasets. In these cases, users know potentially good values for a new application, but cannot be certain about them.</p>
<p>As a result, many competent users instead revert to manual search, which can fully incorporate their prior knowledge. A recent survey showed that most NeurIPS 2019 and ICLR 2020 papers reported having tuned hyperparameters used manual search, with only a very small fraction using BO [5. In order for BO to be adopted widely, and help facilitate faster progress in the ML community by tuning hyperparameters faster and better, it is therefore crucial to devise a method that fully incorporates expert knowledge about the location of highperformance areas into BO. In this paper, we introduce Bayesian Optimization with a Prior for the Optimum (BOPrO), a novel BO variant that combines priors for the optimum with a probabilistic model of the observations made. Our technical contributions are:</p>
<ul>
<li>We introduce Bayesian Optimization with a Prior over the Optimum, short BOPrO, which allows users to inject priors that were previously difficult to inject into BO, such as Gaussian, exponential, multimodal, and multivariate priors for the location of the optimum. To ensure robustness against misleading priors, BOPrO gives more importance to the data-driven model as iterations progress, gradually forgetting the prior.</li>
<li>BOPrO's model bridges the gap between the well-established Tree-structured Parzen Estimator (TPE) methodology, which is based on Parzen kernel density estimators, and standard BO probabilistic models, such as Gaussian Processes (GPs). This is made possible by using the Probability of Improvement (PI) criterion to derive from BO's standard posterior over functions $p\left(f \mid\left(\boldsymbol{x}<em i="i">{i}, y</em>$ leading to good function values..}\right)_{i=1}^{t}\right)$ the probability of an input $\boldsymbol{x</li>
<li>We demonstrate the effectiveness of BOPrO on a comprehensive set of synthetic benchmarks and real-world applications, showing that knowledge about the locality of an optimum helps BOPrO to achieve similar performance to current state-of-the-art on average $6.67 \times$ faster on synthetic benchmarks and $1.49 \times$ faster on a real-world application. BOPrO also achieves similar or better final performance on all benchmarks.</li>
</ul>
<p>BOPrO is publicly available as part of the HyperMapper optimization framework 7 .</p>
<h1>2 Background</h1>
<h3>2.1 Bayesian Optimization</h3>
<p>Bayesian Optimization (BO) is an approach for optimizing an unknown function $f: \mathcal{X} \rightarrow \mathbb{R}$ that is expensive to evaluate over an input space $\mathcal{X}$. In this paper, we aim to minimize $f$, i.e., find $\boldsymbol{x}^{<em>} \in \arg \min _{\boldsymbol{x} \in \mathcal{X}} f(\boldsymbol{x})$. BO approximates $\boldsymbol{x}^{</em>}$ with a sequence of evaluations $\boldsymbol{x}<em 2="2">{1}, \boldsymbol{x}</em>}, \ldots \in \mathcal{X}$ that maximizes an utility metric, with each new $\boldsymbol{x<em 1="1">{t+1}$ depending on the previous function values $y</em>}, y_{2}, \ldots, y_{t}$ at $\boldsymbol{x<em t="t">{1}, \ldots, \boldsymbol{x}</em>}$. BO achieves this by building a posterior on $f$ based on the set of evaluated points. At each iteration, a new point is selected and evaluated based on the posterior, and the posterior is updated to include the new point $\left(\boldsymbol{x<em t_1="t+1">{t+1}, y</em>\right)$.</p>
<p>The points explored by BO are dictated by the acquisition function, which attributes an utility to each $\boldsymbol{x} \in \mathcal{X}$ by balancing the predicted value and uncertainty of the prediction for each $\boldsymbol{x}$ [41]. In this work, as the acquisition function we choose Expected Improvement (EI) [31], which quantifies the expected improvement over the best function value found so far:</p>
<p>$$
E I_{y_{\text {inc }}}(\boldsymbol{x}):=\int_{-\infty}^{\infty} \max \left(y_{\text {inc }}-y, 0\right) p(y \mid \boldsymbol{x}) d y
$$</p>
<p>where $y_{\text {inc }}$ is the incumbent function value, i.e., the best objective function value found so far, and $p(y \mid \boldsymbol{x})$ is given by a probabilistic model, e.g., a GP. Alternatives to EI would be Probability of Improvement (PI) [22|26], upper-confidence bounds (UCB) [44], entropy-based methods (e.g. Hernández-Lobato et al. [18]), and knowledge gradient 47 .</p>
<h3>2.2 Tree-structured Parzen Estimator</h3>
<p>The Tree-structured Parzen Estimator (TPE) method is a BO approach introduced by Bergstra et al. [3]. Whereas the standard probabilistic model in BO directly models $p(y \mid \boldsymbol{x})$, the TPE approach models $p(\boldsymbol{x} \mid y)$ and $p(y)$ instead. ${ }^{8}$ This is done by constructing two parametric densities, $g(\boldsymbol{x})$ and $b(\boldsymbol{x})$, which are computed using the observations with function value below and above a given threshold, respectively. The separating threshold $y^{*}$ is defined as a quantile of the observed function values. TPE uses the densities $g(\boldsymbol{x})$ and $b(\boldsymbol{x})$ to define $p(\boldsymbol{x} \mid y)$ as:</p>
<p>$$
p(\boldsymbol{x} \mid y)=g(\boldsymbol{x}) I\left(y&lt;y^{<em>}\right)+b(\boldsymbol{x})\left(1-I\left(y&lt;y^{</em>}\right)\right)
$$</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>where $I\left(y&lt;y^{<em>}\right)$ is 1 when $y&lt;y^{</em>}$ and 0 otherwise. Bergstra et al. 3] show that the parametrization of the generative model $p(\boldsymbol{x}, y)=p(\boldsymbol{x} \mid y) p(y)$ facilitates the computation of EI as it leads to $E I_{y^{<em>}}(\boldsymbol{x}) \propto g(\boldsymbol{x}) / b(\boldsymbol{x})$ and, thus, $\arg \max <em>{\boldsymbol{x} \in \mathcal{X}} E I</em>{y^{</em>}}(\boldsymbol{x})=\arg \max _{\boldsymbol{x} \in \mathcal{X}} g(\boldsymbol{x}) / b(\boldsymbol{x})$.</p>
<h1>3 BO with a Prior for the Optimum</h1>
<p>We now describe our BOPrO approach, which allows domain experts to inject user knowledge about the locality of an optimum into the optimization in the form of priors. BOPrO combines this user-defined prior with a probabilistic model that captures the likelihood of the observed data $\mathcal{D}<em i="i">{t}=\left(\boldsymbol{x}</em>$. BOPrO is independent of the probabilistic model being used; it can be freely combined with, e.g., Gaussian processes (GPs), random forests, or Bayesian NNs.}, y_{i}\right)_{i=1}^{t</p>
<h3>3.1 BOPrO Priors</h3>
<p>BOPrO allows users to inject prior knowledge w.r.t. promising areas into BO. This is done via a prior distribution that informs where in the input space $\mathcal{X}$ we expect to find good $f(\boldsymbol{x})$ values. A point is considered "good" if it leads to low function values, and potentially to a global optimum. We denote the prior distribution $P_{g}(\boldsymbol{x})$, where $g$ denotes that this is a prior on good points and $\boldsymbol{x} \in \mathcal{X}$ is a given point. Examples of priors are shown in Figures 2 and 3, additional examples of continuous and discrete priors are shown in Appendices A and D, respectively. Similarly, we define a prior on where in the input space we expect to have "bad" points. Although we could have a user-defined probability distribution $P_{b}(\boldsymbol{x})$, we aim to keep the decision-making load on users low and thus, for simplicity, only require the definition of $P_{g}(\boldsymbol{x})$ and compute $P_{b}(\boldsymbol{x})=1-P_{g}(\boldsymbol{x}) .{ }^{9} P_{g}(\boldsymbol{x})$ is normalized to $[0,1]$ by min-max scaling before computing $P_{b}(\boldsymbol{x})$.</p>
<p>In practice, $\boldsymbol{x}$ contains several dimensions but it is difficult for domain experts to provide a joint prior distribution $P_{g}(\boldsymbol{x})$ for all of them. However, users can typically easily specify, e.g., sketch out, a univariate or bivariate prior distribution for continuous dimensions or provide a list of probabilities for discrete dimensions. In BOPrO, users are free to define a complex multivariate distribution, but we expect the standard use case to be that users mainly want to specify univariate distributions, implicitly assuming a prior that factors as $P_{g}(\boldsymbol{x})=\prod_{i=1}^{D} P_{g}\left(x_{i}\right)$, where $D$ is the number of dimensions in $\mathcal{X}$ and $x_{i}$ is the $i$-th input dimension of $\boldsymbol{x}$. To not assume unrealistically complex priors and to mimic what we expect most users will provide, in our experiments we use factorized priors; in Appendix E we show that these factorized priors can in fact lead to similar BO performance as multivariate priors.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: Our model is composed by a probabilistic model and the probability of improving over the threshold $f_{\gamma}$, i.e., right tail of the Gaussian. The black curve is the probabilistic model's mean and the shaded area is the model's variance.</p>
<h1>3.2 Model</h1>
<p>Whereas the standard probabilistic model in BO, e.g., a GP, quantifies $p(y \mid \boldsymbol{x})$ directly, that model is hard to combine with the prior $P_{g}(\boldsymbol{x})$. We therefore introduce a method to translate the standard probabilistic model $p(y \mid \boldsymbol{x})$ into a model that is easier to combine with this prior. Similar to the TPE work described in Section 2.2, our generative model combines $p(\boldsymbol{x} \mid y)$ and $p(y)$ instead of directly modeling $p(y \mid \boldsymbol{x})$.</p>
<p>The computation we perform for this translation is to quantify the probability that a given input $\boldsymbol{x}$ is "good" under our standard probabilistic model $p(y \mid \boldsymbol{x})$. As in TPE, we define configurations as "good" if their observed $y$-value is below a certain quantile $\gamma$ of the observed function values (so that $p\left(y&lt;f_{\gamma}\right)=\gamma$ ). We in addition exploit the fact that our standard probabilistic model $p(y \mid \boldsymbol{x})$ has a Gaussian form, and under this Gaussian prediction we can compute the probability $\mathcal{M}_{g}(\boldsymbol{x})$ of the function value lying below a certain quantile using the standard closed-form formula for PI [26]:</p>
<p>$$
\mathcal{M}<em _gamma="\gamma">{g}(\boldsymbol{x})=p\left(f(\boldsymbol{x})&lt;f</em>} \mid \boldsymbol{x}, \mathcal{D<em _gamma="\gamma">{t}\right)=\Phi\left(\frac{f</em>\right)
$$}-\mu_{\boldsymbol{x}}}{\sigma_{\boldsymbol{x}}</p>
<p>where $\mathcal{D}<em i="i">{t}=\left(\boldsymbol{x}</em>\right)}, y_{i<em _boldsymbol_x="\boldsymbol{x">{i=1}^{t}$ are the evaluated configurations, $\mu</em>$, and $\Phi$ is the standard normal CDF, see Figure 1. Note that there are two probabilistic models here:}}$ and $\sigma_{\boldsymbol{x}}$ are the predictive mean and standard deviation of the probabilistic model at $\boldsymbol{x</p>
<ol>
<li>The standard probabilistic model of BO, with a structural prior over functions $p(f)$, updated by data $\mathcal{D}<em t="t">{t}$ to yield a posterior over functions $p\left(f \mid \mathcal{D}</em>}\right)$, allowing us to quantify the probability $\mathcal{M<em _gamma="\gamma">{g}(\boldsymbol{x})=p\left(f(x)&lt;f</em>$} \mid \boldsymbol{x}, \mathcal{D}_{t}\right)$ in Eq. (3). ${ }^{10</li>
<li>The TPE-like generative model that combines $p(y)$ and $p(\boldsymbol{x} \mid y)$ instead of directly modelling $p(y \mid \boldsymbol{x})$.</li>
</ol>
<p>Eq. (3) bridges these two models by using the probability of improvement from BO's standard probabilistic model as the probability $\mathcal{M}<em g="g">{g}(\boldsymbol{x})$ in TPE's model. Ultimately, this is a heuristic since there is no formal connection between the two probabilistic models. However, we believe that the use of BO's familiar, theoretically sound framework of probabilistic modelling of $p(y \mid x)$, followed by the computation of the familiar PI formula is an intuitive choice for obtaining the probability of an input achieving at least a given performance threshold - exactly the term we need for TPE's $\mathcal{M}</em>}(\boldsymbol{x})$. Similarly, we also define a probability $\mathcal{M<em b="b">{b}(\boldsymbol{x})$ of $\boldsymbol{x}$ being bad as $\mathcal{M}</em>)$.}(\boldsymbol{x})=1-\mathcal{M}_{g}(\boldsymbol{x</p>
<h1>3.3 Pseudo-posterior</h1>
<p>BOPrO combines the prior $P_{g}(\boldsymbol{x})$ in Section (3.1) and the model $\mathcal{M}_{g}(\boldsymbol{x})$ in Eq. (3) into a pseudo-posterior on "good" points. This pseudo-posterior represents the updated beliefs on where we can find good points, based on the prior and data that has been observed. The pseudo-posterior is computed as the product:</p>
<p>$$
g(\boldsymbol{x}) \propto P_{g}(\boldsymbol{x}) \mathcal{M}_{g}(\boldsymbol{x})^{\frac{t}{\beta}}
$$</p>
<p>where $t$ is the current optimization iteration, $\beta$ is an optimization hyperparameter, $\mathcal{M}<em g="g">{g}(\boldsymbol{x})$ is defined in Eq. (3), and $P</em>)$ as a pseudo-posterior, to emphasize that it is not a standard posterior probability distribution.}(\boldsymbol{x})$ is the prior defined in Sec 3.1, rescaled to $[0,1]$ using min-max scaling. We note that this pseudo-posterior is not normalized, but this suffices for BOPrO to determine the next $\boldsymbol{x}_{t}$ as the normalization constant cancels out (c.f. Section 3.5). Since $g(\boldsymbol{x})$ is not normalized and we include the exponent $t / \beta$ in Eq. (4), we refer to $g(\boldsymbol{x</p>
<p>The $t / \beta$ fraction in Eq. (4) controls how much weight is given to $\mathcal{M}<em g="g">{g}(\boldsymbol{x})$. As the optimization progresses, more weight is given to $\mathcal{M}</em>}(\boldsymbol{x})$ over $P_{g}(\boldsymbol{x})$. Intuitively, we put more emphasis on $\mathcal{M<em g="g">{g}(\boldsymbol{x})$ as it observes more data and becomes more accurate. We do this under the assumption that the model $\mathcal{M}</em>)$ will eventually be better than the user at predicting where to find good points. This also allows to recover from misleading priors as we show in Section 4.1; similar to, and inspired by Bayesian models, the data ultimately washes out the prior. The $\beta$ hyperparameter defines the balance between prior and model, with higher $\beta$ values giving more importance to the prior and requiring more data to overrule it.}(\boldsymbol{x</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: Breakdown of the prior $P_{g}(\boldsymbol{x})=\mathcal{B}(3,3)$, the model $M_{g}(\boldsymbol{x})$, and the pseudoposterior $g(\boldsymbol{x})$ (top row) on the 1D-Branin function (bottom row) and their evolution over the optimization iterations. In early iterations, the pseudo-posterior is high around the optimum, where both prior and model agree there are good points. In later iterations, it vanishes where the model is certain there will be no improvement and is high where there is uncertainty in the model.</p>
<p>We note that, directly computing Eq (4) can lead to numerical issues. Namely, the pseudo-posterior can reach extremely low values if the $P_{g}(\boldsymbol{x})$ and $\mathcal{M}_{g}(\boldsymbol{x})$ probabilities are low, especially as $t / \beta$ grows. To prevent this, in practice, BOPrO uses the logarithm of the pseudo-posterior instead:</p>
<p>$$
\log (g(\boldsymbol{x})) \propto \log \left(P_{g}(\boldsymbol{x})\right)+\frac{t}{\beta} \cdot \log \left(\mathcal{M}_{g}(\boldsymbol{x})\right)
$$</p>
<p>Once again, we also define an analogous pseudo-posterior distribution on bad $\boldsymbol{x}: b(\boldsymbol{x}) \propto P_{b}(\boldsymbol{x}) \mathcal{M}_{b}(\boldsymbol{x})^{\frac{t}{\beta}}$. We then use these quantities to define a density model $p(\boldsymbol{x} \mid y)$ as follows:</p>
<p>$$
p(\boldsymbol{x} \mid y) \propto \begin{cases}g(\boldsymbol{x}) &amp; \text { if } y&lt;f_{\gamma} \ b(\boldsymbol{x}) &amp; \text { if } y \geq f_{\gamma}\end{cases}
$$</p>
<h1>3.4 Model and Pseudo-posterior Visualization</h1>
<p>We visualize the prior $P_{g}(\boldsymbol{x})$, the model $M_{g}(\boldsymbol{x})$, and the pseudo-posterior $g(\boldsymbol{x})$ and their evolution over the optimization iterations for a 1D-Branin function. We define the 1D-Branin by setting the second dimension of the function to the global optimum $x_{2}=2.275$ and optimizing the first dimension. We use a Beta distribution prior $P_{g}(\boldsymbol{x})=\mathcal{B}(3,3)$, which resembles a truncated Gaussian centered close to the global optimum, and a GP as predictive model. We perform an initial design of $D+1=2$ random points sampled from the prior and then run BOPrO for 20 iterations.</p>
<p>Figure 2 shows the optimization at different stages. Red crosses denote the initial design and blue/green crosses denote BOPrO samples, with green samples denoting later iterations. Figure 2a shows the initialization phase (bottom) and the Beta prior (top). After 5 BO iterations, in Figure 2b (top), the pseudoposterior is high near the global minimum, around $x=\pi$, where both the prior $P_{g}(\boldsymbol{x})$ and the model $\mathcal{M}<em g="g">{g}(\boldsymbol{x})$ agree there are good points. After 10 BO iterations in Figure 2c (top), there are three regions with high pseudo-posterior. The middle region, where BOPrO is exploiting until the optimum is found, and two regions to the right and left, which will lead to future exploration as shown in Figure 2d (bottom) on the right and left of the global optimum in light green crosses. After 20 iterations, see Figure 2d (top), the pseudo-posterior vanishes where the model $\mathcal{M}</em>)$ is certain there will be no improvement, but it is high wherever there is uncertainty in the GP.}(\boldsymbol{x</p>
<h1>3.5 Acquisition Function</h1>
<p>We adopt the EI formulation used in Bergstra et al. [3] by replacing their Adaptive Parzen Estimators with our pseudo-posterior from Eq. (4), i.e.:</p>
<p>$$
\begin{aligned}
E I_{f_{\gamma}}(\boldsymbol{x}) &amp; :=\int_{-\infty}^{\infty} \max \left(f_{\gamma}-y, 0\right) p(y \mid \boldsymbol{x}) d y=\int_{-\infty}^{f_{\gamma}}\left(f_{\gamma}-y\right) \frac{p(\boldsymbol{x} \mid y) p(y)}{p(\boldsymbol{x})} d y \
&amp; \propto\left(\gamma+\frac{b(\boldsymbol{x})}{g(\boldsymbol{x})}(1-\gamma)\right)^{-1}
\end{aligned}
$$</p>
<p>The full derivation of Eq. (7) is shown in Appendix B. Eq. (7) shows that to maximize improvement we would like points $\boldsymbol{x}$ with high probability under $g(\boldsymbol{x})$ and low probability under $b(\boldsymbol{x})$, i.e., minimizing the ratio $b(\boldsymbol{x}) / g(\boldsymbol{x})$. We note that the point that minimizes the ratio for our unnormalized pseudo-posteriors will be the same that minimizes the ratio for the normalized pseudo-posterior and, thus, computing the normalized pseudo-posteriors is unnecessary.</p>
<p>The dynamics of the BOPrO algorithm can be understood in terms of the following proposition (proof in Appendix B):</p>
<p>Proposition 1. Given $f_{\gamma}, P_{g}(\boldsymbol{x}), P_{b}(\boldsymbol{x}), \mathcal{M}<em b="b">{g}(\boldsymbol{x}), \mathcal{M}</em> \mid y)$, and $\beta$ as above, then}(\boldsymbol{x}), g(\boldsymbol{x}), b(\boldsymbol{x}), p(\boldsymbol{x</p>
<p>$$
\lim <em f__gamma="f_{\gamma">{t \rightarrow \infty} \underset{\boldsymbol{x} \in \mathcal{X}}{\arg \max } E I</em>)=\lim }}(\boldsymbol{x<em g="g">{t \rightarrow \infty} \underset{\boldsymbol{x} \in \mathcal{X}}{\arg \max } \mathcal{M}</em>)
$$}(\boldsymbol{x</p>
<p>where $E I_{f_{\gamma}}$ is the Expected Improvement acquisition function as defined in Eq. (7) and $\mathcal{M}_{g}(\boldsymbol{x})$ is as defined in Eq. (3).</p>
<p>In early BO iterations the prior for the optimum will have a predominant role, but in later BO iterations the model will grow more important, and as Proposition 1 shows, if BOPrO is run long enough the prior washes out and BOPrO only trusts the model $\mathcal{M}<em g="g">{g}(\boldsymbol{x})$ informed by the data. Since $\mathcal{M}</em>)$ then, in the}(\boldsymbol{x})$ is the Probability of Improvement (PI) on the probabilistic model $p(y \mid \boldsymbol{x</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 BOPrO Algorithm. \(\mathcal{D}_{t}\) keeps track of all function evaluations so
far: \(\left(\boldsymbol{x}_{i}, y_{i}\right)_{i=1}^{t}\).
    Input: Input space \(\mathcal{X}\), user-defined prior distributions \(P_{g}(\boldsymbol{x})\) and \(P_{b}(\boldsymbol{x})\) ), quantile \(\gamma\)
    and BO budget \(B\).
    Output: Optimized point \(\boldsymbol{x}_{\text {inc }}\).
    \(\mathcal{D}_{1} \leftarrow \operatorname{Initialize}(\mathcal{X})\)
    for \(t=1\) to \(B\) do
        \(\mathcal{M}_{g}(\boldsymbol{x}) \leftarrow\) fit_model_good \(\left(\mathcal{D}_{t}\right) \quad \triangleright\) see Eq. (3)
        \(\mathcal{M}_{b}(\boldsymbol{x}) \leftarrow\) fit_model_bad \(\left(\mathcal{D}_{t}\right)\)
        \(g(\boldsymbol{x}) \leftarrow P_{g}(\boldsymbol{x}) \cdot \mathcal{M}_{g}(\boldsymbol{x})^{\frac{1}{\beta}} \quad \triangleright\) see Eq. (4)
        \(b(\boldsymbol{x}) \leftarrow P_{b}(\boldsymbol{x}) \cdot \mathcal{M}_{b}(\boldsymbol{x})^{\frac{1}{\beta}}\)
        \(\boldsymbol{x}_{t} \in \arg \max <span class="ge">_{\boldsymbol{x} \in \mathcal{X}} E I_</span>{f_{\gamma}}(\boldsymbol{x}) \quad \triangleright\) see Eq. (7)
        \(y_{t} \leftarrow f\left(\boldsymbol{x}_{t}\right)\)
        \(\mathcal{D}_{t+1} \leftarrow \mathcal{D}_{t} \cup\left(\boldsymbol{x}_{t}, y_{t}\right)\)
    end for
    \(\boldsymbol{x}_{\text {inc }} \leftarrow \operatorname{ComputeBest}\left(\mathcal{D}_{t+1}\right)\)
    return \(\boldsymbol{x}_{\text {inc }}
</code></pre></div>

<p>limit, maximizing the acquisition function $E I_{f_{\gamma}}(\boldsymbol{x})$ is equivalent to maximizing the PI acquisition function on the probabilistic model $p(y \mid \boldsymbol{x})$. In other words, for high values of $t$, BOPrO converges to standard BO with a PI acquisition function.</p>
<h1>3.6 Putting It All Together</h1>
<p>Algorithm 1 shows the BOPrO algorithm.In Line 3, BOPrO starts with a design of experiments (DoE) phase, where it randomly samples a number of points from the user-defined prior $P_{g}(\boldsymbol{x})$. After initialization, the BO loop starts at Line 4. In each loop iteration, BOPrO fits the models $\mathcal{M}<em b="b">{g}(\boldsymbol{x})$ and $\mathcal{M}</em>)$ (Lines 7 and 8 respectively). The EI acquisition function is computed next, using the pseudo-posteriors, and the point that maximizes EI is selected as the next point to evaluate at Line 9. The black-box function evaluation is performed at Line 10. This BO loop is repeated for a predefined number of iterations, according to the user-defined budget $B$.}(\boldsymbol{x})$ on the previously evaluated points (Lines 5 and 6) and computes the pseudo-posteriors $g(\boldsymbol{x})$ and $b(\boldsymbol{x</p>
<h2>4 Experiments</h2>
<p>We implement both Gaussian processes (GPs) and random forests (RFs) as predictive models and use GPs in all experiments, except for our real-world experiments (Section 4.3), where we use RFs for a fair comparison. We set the model weight $\beta=10$ and the model quantile to $\gamma=0.05$, see our sensitivity hyperparameter study in Appendices I and J. Before starting the main BO loop in BOPrO, we randomly sample $D+1$ points from the prior as an initial design</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3: BOPrO on the 1D Branin function. The leftmost column shows the exponential prior. The other columns show the model and the log pseudo-posterior after 0 (initialization only), 10, and 20 BO iterations. BOPrO forgets the wrong prior on the local optimum and converges to the global optimum.
consistently on all benchmarks. We optimize our EI acquisition function using a combination of multi-start local search [20] and CMA-ES [16]. We consider four synthetic benchmarks: Branin, SVM, FC-Net, and XGBoost, which are $2,2,6$, and 8 dimensional, respectively. The last three are part of the Profet benchmarks [24], generated by a generative model built using performance data on OpenML or UCI datasets. See Appendix C for more details.</p>
<h1>4.1 Prior Forgetting</h1>
<p>We first show that BOPrO can recover from a misleading prior, thanks to our model $\mathcal{M}<em g="g">{g}(\boldsymbol{x})$ and the $t / \beta$ parameter in the pseudo-posterior computation in Eq. (4). As BO progresses, the model $\mathcal{M}</em>)$ becomes more accurate and receives more weight, guiding optimization away from the wrong prior and towards better values of the function.}(\boldsymbol{x</p>
<p>Figure 3 shows BOPrO on the 1D Branin function with an exponential prior. Columns (b), (c), and (d) show BOPrO after $D+1=2$ initial samples and $0,10,20 \mathrm{BO}$ iterations, respectively. After initialization, as shown in Column (b), the pseudo-posterior is nearly identical to the exponential prior and guides BOPrO towards the region of the space on the right, which is towards the local optimum. This happens until the model $\mathcal{M}<em g="g">{g}(\boldsymbol{x})$ becomes certain there will be no more improvement from sampling that region (Columns (c) and (d)). After that, $\mathcal{M}</em>)$ is certain there will}(\boldsymbol{x})$ guides the pseudo-posterior towards exploring regions with high uncertainty. Once the global minimum region is found, the pseudo-posterior starts balancing exploiting the global minimum and exploring regions with high uncertainty, as shown in Figure 3d (bottom). Notably, the pseudo-posterior after $x&gt;4$ falls to 0 in Figure 3d (top), as the model $\mathcal{M}_{g}(\boldsymbol{x</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4: Log regret comparison of BOPrO with weak and strong priors, sampling from the strong prior, $10,000 \times$ random search (RS), and Spearmint (mean +/ - one std on 5 repetitions). We run each benchmark for 200 iterations.
be no improvement from sampling the region of the local optimum. We provide additional examples of forgetting in Appendix A, and a comparison of BOPrO with misleading priors, no prior, and correct priors in Appendix F.</p>
<h1>4.2 Comparison Against Strong Baselines</h1>
<p>We build two priors for the optimum in a controlled way and evaluate BOPrO's performance with these different prior strengths. We emphasize that in practice, manual priors would be based on the domain experts' expertise on their applications; here, we only use artificial priors to guarantee that our prior is not biased by our own expertise for the benchmarks we used. In practice, users will manually define these priors like in our real-world experiments (Section 4.3).</p>
<p>Our synthetic priors take the form of Gaussian distributions centered near the optimum. For each input $x \in \mathcal{X}$, we inject a prior of the form $\mathcal{N}\left(\mu_{x}, \sigma_{x}^{2}\right)$, where $\mu_{x}$ is sampled from a Gaussian centered at the optimum value $x_{\text {opt }}{ }^{11}$ for that parameter $\mu_{x} \sim \mathcal{N}\left(x_{\text {opt }}, \sigma_{x}^{2}\right)$, and $\sigma_{x}$ is a hyperparameter of our experimental setup determining the prior's strength. For each run of BOPrO, we sample new $\mu_{x}$ 's. This setup provides us with a synthetic prior that is close to the optimum, but not exactly centered at it, and allows us to control the strength of the prior by $\sigma_{x}$. We use two prior strengths in our experiments: a strong prior, computed with $\sigma_{x}=0.01$, and a weak prior, computed with $\sigma_{x}=0.1$.</p>
<p>Figure 4 compares BOPrO to other optimizers using the log simple regret on 5 runs (mean and std error reported) on the synthetic benchmarks. We compare the results of BOPrO with weak and strong priors to $10,000 \times$ random search (RS, i.e., for each BO sample we draw 10,000 uniform random samples), sampling from the strong prior only, and Spearmint [43], a well-adopted BO approach using GPs and EI. In Appendix G, we also show a comparison of BOPrO with TPE, SMAC, and TuRBO [20|28|10]. Also, in Appendix H, we compare BOPrO to other baselines with the same prior initialization and show that the performance of the baselines remains similar.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5: Log regret comparison of random search (RS), HyperMapper, BOPrO, and manual optimization on Spatial. The line and shaded regions show mean and std after 5 repetitions. Vertical lines denote the end of the initialization phase.</p>
<p>BOPrO with a strong prior for the optimum beats $10,000 \times$ RS and BOPrO with a weak prior on all benchmarks. It also outperforms the performance of sampling from the strong prior; this is expected because the prior sampling cannot focus on the real location of the optimum. The two methods are identical during the initialization phase because they both sample from the same prior in that phase.</p>
<p>BOPrO with a strong prior is also more sample efficient and finds better or similar results to Spearmint on all benchmarks. Importantly, in all our experiments, BOPrO with a good prior consistently shows tremendous speedups in the early phases of the optimization process, requiring on average only 15 iterations to reach the performance that Spearmint reaches after 100 iterations ( $6.67 \times$ faster). Thus, in comparison to other traditional BO approaches, BOPrO makes use of the best of both worlds, leveraging prior knowledge and efficient optimization based on BO.</p>
<h1>4.3 The Spatial Use-case</h1>
<p>We next apply BOPrO to the Spatial [25] real-world application. Spatial is a programming language and corresponding compiler for the design of application accelerators, i.e., FPGAs. We apply BOPrO to three Spatial benchmarks, namely, 7D shallow and deep CNNs, and a 10D molecular dynamics grid application (MD Grid). We compare the performance of BOPrO to RS, manual optimization, and HyperMapper [33], the current state-of-the-art BO solution for Spatial. For a fair comparison between BOPrO and HyperMapper, since HyperMapper uses RFs as its surrogate model, here, we also use RFs in BOPrO. The manual optimization and the prior for BOPrO were provided by an unbiased Spatial developer, who is not an author of this paper. The priors were provided once and kept unchanged for the whole project. More details on the setup, including the priors used, are presented in Appendix D.</p>
<p>Figure 5 shows the log regret on the Spatial benchmarks. BOPrO vastly outperforms RS in all benchmarks; notably, RS does not improve over the default</p>
<p>configuration in MD Grid. BOPrO is also able to leverage the expert's prior and outperforms the expert's configuration in all benchmarks ( $2.68 \times, 1.06 \times$, and $10.4 \times$ speedup for shallow CNN, deep CNN, and MD Grid, respectively). In the MD Grid benchmark, BOPrO achieves better performance than HyperMapper in the early stages of optimization (up to $1.73 \times$ speedup between iterations 25 and 40 , see the plot inset), and achieves better final performance ( $1.28 \times$ speedup). For context, this is a significant improvement in the FPGA field, where a $10 \%$ improvement could qualify for acceptance in a top-tier conference. In the CNN benchmarks, BOPrO converges to the minima regions faster than HyperMapper ( $1.58 \times$ and $1.4 \times$ faster for shallow and deep, respectively). Thus, BOPrO leverages both the expert's prior knowledge and BO to provide a new state of the art for Spatial.</p>
<h1>5 Related Work</h1>
<p>TPE by Bergstra et al. [3], the default optimizer in the popular HyperOpt package [2], supports limited hand-designed priors in the form of normal or lognormal distributions. We make three technical contributions that make BOPrO more flexible than TPE. First, we generalize over the TPE approach by allowing more flexible priors; second, BOPrO is agnostic to the probabilistic model used, allowing the use of more sample-efficient models than TPE's kernel density estimators (e.g., we use GPs and RFs in our experiments); and third, BOPrO is inspired by Bayesian models that give more importance to the data as iterations progress. We also show that BOPrO outperforms HyperOpt's TPE in Appendix G.</p>
<p>In parallel work, Li et al. [27] allow users to specify priors via a probability distribution. Their two-level approach samples a number of configurations by maximizing samples from a GP posterior and then chooses the configuration with the highest prior as the next to evaluate. In contrast, BOPrO leverages the information from the prior more directly; is agnostic to the probabilistic model used, which is important for applications with many discrete variables like our real-world application, where RFs outperform GPs; and provably recovers from misspecified priors, while in their approach the prior never gets washed out.</p>
<p>The work of Ramachandran et al. [39] also supports priors in the form of probability distributions. Their work uses the probability integral transform to warp the search space, stretching regions where the prior has high probability, and shrinking others. Once again, compared to their approach, BOPrO is agnostic to the probabilistic model used and directly controls the balance between prior and model via the $\beta$ hyperparameter. Additionally, BOPrO's probabilistic model is fitted independently from the prior, which ensures it is not biased by the prior, while their approach fits the model to a warped version of the space, transformed by the prior, making it difficult to recover from misleading priors.</p>
<p>Black-box optimization tools, such as SMAC [20] or iRace [30] also support simple hand-designed priors, e.g. log-transformations. However, these are not properly reflected in the predictive models and both cannot explicitly recover from bad priors.</p>
<p>Oh et al. [35] and Siivola et al. [42] propose structural priors for highdimensional problems. They assume that users always place regions they expect to be good at the center of the search space and then develop BO approaches that favor configurations near the center. However, this is a rigid assumption about optimum locality, which does not allow users to freely specify their priors. Similarly, Shahriari et al. [40] focus on unbounded search spaces. The priors in their work are not about good regions of the space, but rather a regularization function that penalizes configurations based on their distance to the center of the user-defined search space. The priors are automatically derived from the search space and not provided by users.</p>
<p>Our work also relates to meta-learning for BO [45], where BO is applied to many similar optimization problems in a sequence such that knowledge about the general problem structure can be exploited in future optimization problems. In contrast to meta-learning, BOPrO allows human experts to explicitly specify their priors. Furthermore, BOPrO does not depend on any meta-features 12] and incorporates the human's prior instead of information gained from different experiments [29].</p>
<h1>6 Conclusions and Future Work</h1>
<p>We have proposed a novel BO variant, BOPrO, that allows users to inject their expert knowledge into the optimization in the form of priors about which parts of the input space will yield the best performance. These are different than standard priors over functions which are much less intuitive for users. So far, BO failed to leverage the experience of domain experts, not only causing inefficiency but also driving users away from applying BO approaches because they could not exploit their years of knowledge in optimizing their black-box functions. BOPrO addresses this issue and we therefore expect it to facilitate the adoption of BO. We showed that BOPrO is $6.67 \times$ more sample efficient than strong BO baselines, and $10,000 \times$ faster than random search, on a common suite of benchmarks and achieves a new state-of-the-art performance on a real-world hardware design application. We also showed that BOPrO converges faster and robustly recovers from misleading priors.</p>
<p>In future work, we will study how our approach can be used to leverage prior knowledge from meta-learning. Bringing these two worlds together will likely boost the performance of BO even further.</p>
<h2>7 Acknowledgments</h2>
<p>We thank Matthew Feldman for Spatial support. Luigi Nardi and Kunle Olukotun were supported in part by affiliate members and other supporters of the Stanford DAWN project - Ant Financial, Facebook, Google, Intel, Microsoft, NEC, SAP, Teradata, and VMware. Luigi Nardi was also partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation. Artur Souza and Leonardo</p>
<p>B. Oliveira were supported by CAPES, CNPq, and FAPEMIG. Frank Hutter acknowledges support by the European Research Council (ERC) under the European Union Horizon 2020 research and innovation programme through grant no. 716721. The computations were also enabled by resources provided by the Swedish National Infrastructure for Computing (SNIC) at LUNARC partially funded by the Swedish Research Council through grant agreement no. 2018-05973.</p>
<h1>References</h1>
<ol>
<li>Balandat, M., Karrer, B., Jiang, D., Daulton, S., Letham, B., Wilson, A.G., Bakshy, E.: BoTorch: A framework for efficient Monte-Carlo Bayesian optimization. In: Advances in Neural Information Processing Systems (2020)</li>
<li>Bergstra, J., Yamins, D., Cox, D.D.: Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. In: International Conference on Machine Learning (2013)</li>
<li>Bergstra, J.S., Bardenet, R., Bengio, Y., Kégl, B.: Algorithms for hyper-parameter optimization. In: Advances in Neural Information Processing Systems (2011)</li>
<li>Bodin, B., Nardi, L., Zia, M.Z., Wagstaff, H., Sreekar Shenoy, G., Emani, M., Mawer, J., Kotselidis, C., Nisbet, A., Lujan, M., et al.: Integrating algorithmic parameters into benchmarking and design space exploration in 3d scene understanding. In: International Conference on Parallel Architectures and Compilation (2016)</li>
<li>Bouthillier, X., Varoquaux, G.: Survey of machine-learning experimental methods at NeurIPS2019 and ICLR2020. Research report, Inria Saclay Ile de France (Jan 2020), https://hal.archives-ouvertes.fr/hal-02447823</li>
<li>Calandra, R., Seyfarth, A., Peters, J., Deisenroth, M.P.: Bayesian optimization for learning gaits under uncertainty. Annals of Mathematics and Artificial Intelligence 76(1-2), 5-23 (2016)</li>
<li>Chen, Y., Huang, A., Wang, Z., Antonoglou, I., Schrittwieser, J., Silver, D., de Freitas, N.: Bayesian optimization in alphago. CoRR abs/1812.06855 (2018)</li>
<li>Clarke, A., McMahon, B., Menon, P., Patel, K.: Optimizing hyperparams for image datasets in fastai. https://www.platform.ai/post/ optimizing-hyperparams-for-image-datasets-in-fastai (2020)</li>
<li>Dixon, L.C.W.: The global optimization problem: an introduction. Toward global optimization 2, 1-15 (1978)</li>
<li>Eriksson, D., Pearce, M., Gardner, J.R., Turner, R., Poloczek, M.: Scalable global optimization via local bayesian optimization. In: Advances in Neural Information Processing Systems (2019)</li>
<li>Falkner, S., Klein, A., Hutter, F.: BOHB: robust and efficient hyperparameter optimization at scale. In: International Conference on Machine Learning (2018)</li>
<li>Feurer, M., Springenberg, J.T., Hutter, F.: Initializing bayesian hyperparameter optimization via meta-learning. In: AAAI Conference on Artificial Intelligence (2015)</li>
<li>Gardner, J.R., Kusner, M.J., Xu, Z.E., Weinberger, K.Q., Cunningham, J.P.: Bayesian optimization with inequality constraints. In: International Conference on Machine Learning, ICML (2014)</li>
<li>
<p>Golovin, D., Solnik, B., Moitra, S., Kochanski, G., Karro, J., Sculley, D.: Google vizier: A service for black-box optimization. In: SIGKDD International Conference on Knowledge Discovery and Data Mining (2017)</p>
</li>
<li>
<p>GPy: GPy: A gaussian process framework in python. http://github.com/ SheffieldML/GPy (since 2012)</p>
</li>
<li>Hansen, N., Ostermeier, A.: Adapting arbitrary normal mutation distributions in evolution strategies: the covariance matrix adaptation. In: Proceedings of IEEE International Conference on Evolutionary Computation (1996)</li>
<li>Hansen, N., Akimoto, Y., Baudis, P.: CMA-ES/pycma on Github. Zenodo, DOI:10.5281/zenodo. 2559634</li>
<li>Hernández-Lobato, J.M., Hoffman, M.W., Ghahramani, Z.: Predictive entropy search for efficient global optimization of black-box functions. In: Advances in Neural Information Processing Systems (2014)</li>
<li>Hutter, F., Xu, L., Hoos, H., Leyton-Brown, K.: Algorithm runtime prediction: Methods \&amp; evaluation. Artificial Intelligence 206, 79-111 (2014)</li>
<li>Hutter, F., Hoos, H.H., Leyton-Brown, K.: Sequential model-based optimization for general algorithm configuration. In: International conference on learning and intelligent optimization (2011)</li>
<li>Hutter, F., Kotthoff, L., Vanschoren, J. (eds.): Automated Machine Learning: Methods, Systems, Challenges. Springer (2018), in press, available at http://automl.org/book.</li>
<li>Jones, D.R.: A taxonomy of global optimization methods based on response surfaces. Journal of global optimization 21(4), 345-383 (2001)</li>
<li>Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: International Conference on Learning Representations (2015)</li>
<li>Klein, A., Dai, Z., Hutter, F., Lawrence, N.D., Gonzalez, J.: Meta-surrogate benchmarking for hyperparameter optimization. In: Advances in Neural Information Processing Systems (2019)</li>
<li>Koeplinger, D., Feldman, M., Prabhakar, R., Zhang, Y., Hadjis, S., Fiszel, R., Zhao, T., Nardi, L., Pedram, A., Kozyrakis, C., Olukotun, K.: Spatial: A Language and Compiler for Application Accelerators. In: SIGPLAN Conference on Programming Language Design and Implementation (2018)</li>
<li>Kushner, H.J.: A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise. Journal of Basic Engineering 86(1), $97-106(1964)$</li>
<li>Li, C., Gupta, S., Rana, S., Nguyen, V., Robles-Kelly, A., Venkatesh, S.: Incorporating expert prior knowledge into experimental design via posterior sampling. arXiv preprint arXiv:2002.11256 (2020)</li>
<li>Lindauer, M., Eggensperger, K., Feurer, M., Falkner, S., Biedenkapp, A., Hutter, F.: Smac v3: Algorithm configuration in python. https://github.com/automl/SMAC3 (2017)</li>
<li>Lindauer, M., Hutter, F.: Warmstarting of model-based algorithm configuration. In: AAAI Conference on Artificial Intelligence (2018)</li>
<li>López-Ibáñez, M., Dubois-Lacoste, J., Pérez Cáceres, L., Stützle, T., Birattari, M.: The irace package: Iterated racing for automatic algorithm configuration. Operations Research Perspectives 3, 43-58 (2016)</li>
<li>Mockus, J., Tiesis, V., Zilinskas, A.: The application of bayesian methods for seeking the extremum. Towards global optimization 2(117-129), 2 (1978)</li>
<li>Nardi, L., Bodin, B., Saeedi, S., Vespa, E., Davison, A.J., Kelly, P.H.: Algorithmic performance-accuracy trade-off in 3d vision applications using hypermapper. In: International Parallel and Distributed Processing Symposium Workshops (2017)</li>
<li>
<p>Nardi, L., Koeplinger, D., Olukotun, K.: Practical design space exploration. In: International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (2019)</p>
</li>
<li>
<p>Neal, R.M.: Bayesian learning for neural networks, vol. 118. Springer Science \&amp; Business Media (2012)</p>
</li>
<li>Oh, C., Gavves, E., Welling, M.: BOCK : Bayesian optimization with cylindrical kernels. In: International Conference on Machine Learning (2018)</li>
<li>Paleyes, A., Pullin, M., Mahsereci, M., Lawrence, N., González, J.: Emulation of physical processes with emukit. In: Workshop on Machine Learning and the Physical Sciences, NeurIPS (2019)</li>
<li>Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., Duchesnay, E.: Scikit-learn: Machine learning in Python. Journal of Machine Learning Research 12, 2825-2830 (2011)</li>
<li>Perrone, V., Shen, H., Seeger, M., Archambeau, C., Jenatton, R.: Learning search spaces for bayesian optimization: Another view of hyperparameter transfer learning. In: Advances in Neural Information Processing Systems (2019)</li>
<li>Ramachandran, A., Gupta, S., Rana, S., Li, C., Venkatesh, S.: Incorporating expert prior in bayesian optimisation via space warping. Knowledge-Based Systems 195, 105663 (2020)</li>
<li>Shahriari, B., Bouchard-Côté, A., Freitas, N.: Unbounded bayesian optimization via regularization. In: Artificial intelligence and statistics. pp. 1168-1176 (2016)</li>
<li>Shahriari, B., Swersky, K., Wang, Z., Adams, R.P., De Freitas, N.: Taking the human out of the loop: A review of bayesian optimization. Proceedings of the IEEE 104(1), 148-175 (2015)</li>
<li>Siivola, E., Vehtari, A., Vanhatalo, J., González, J., Andersen, M.R.: Correcting boundary over-exploration deficiencies in bayesian optimization with virtual derivative sign observations. In: International Workshop on Machine Learning for Signal Processing (2018)</li>
<li>Snoek, J., Larochelle, H., Adams, R.P.: Practical bayesian optimization of machine learning algorithms. In: Advances in Neural Information Processing Systems (2012)</li>
<li>Srinivas, N., Krause, A., Kakade, S.M., Seeger, M.W.: Gaussian process optimization in the bandit setting: No regret and experimental design. In: International Conference on Machine Learning (2010)</li>
<li>Vanschoren, J.: Meta-learning. In: Automated Machine Learning - Methods, Systems, Challenges, pp. 35-61. Springer (2019)</li>
<li>Wang, Q., Ming, Y., Jin, Z., Shen, Q., Liu, D., Smith, M.J., Veeramachaneni, K., Qu, H.: Atmseer: Increasing transparency and controllability in automated machine learning. In: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (2019)</li>
<li>Wu, J., Poloczek, M., Wilson, A.G., Frazier, P.I.: Bayesian optimization with gradients. In: Proceedings of Advances in Neural Information Processing Systems 30 (2017)</li>
</ol>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6: BOPrO on the 1D Branin function with a decay prior. The leftmost column shows the log pseudo-posterior before any samples are evaluated, in this case, the pseudo-posterior is equal to the decay prior. The other columns show the model and pseudo-posterior after 0 (only random samples), 10, and 20 BO iterations. 2 random samples are used to initialize the GP model.</p>
<h1>A Prior Forgetting Supplementary Experiments</h1>
<p>In this section, we show additional evidence that BOPrO can recover from wrongly defined priors so to complement section 4.1. Figure 6 shows BOPrO on the 1D Branin function as in Figure 3 but with a decay prior. Column (a) of Figure 6 shows the decay prior and the 1D Branin function. This prior emphasizes the wrong belief that the optimum is likely located on the left side around $\mathrm{x}=-5$ while the optimum is located at the orange dashed line. Columns (b), (c), and (d) of Figure 6 show BOPrO on the 1D Branin after $D+1=2$ initial samples and 0,10 , and 20 BO iterations, respectively. In the beginning of BO, as shown in column (b), the pseudo-posterior is nearly identical to the prior and guides BOPrO towards the left region of the space. As more points are sampled, the model becomes more accurate and starts guiding the pseudo-posterior away from the wrong prior (column (c)). Notably, the pseudo-posterior before $\mathrm{x}=0$ falls to 0 , as the predictive model is certain there will be no improvement from sampling this region. After 20 iterations, BOPrO finds the optimum region, despite the poor start (column (d)). The peak in the pseudo-posterior in column (d) shows BOPrO will continue to exploit the optimum region as it is not certain if the exact optimum has been found. The pseudo-posterior is also high in the high uncertainty region after $x=4$, showing BOPrO will explore that region after it finds the optimum.</p>
<p>Figure 7 shows BOPrO on the standard 2D Branin function. We use exponential priors for both dimensions, which guides optimization towards a region with only poor performing high function values. 7a shows the prior and 7 b shows</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 7: BOPrO on the Branin function with exponential priors for both dimensions. (a) shows the log pseudo-posterior before any samples are evaluated, in this case, the pseudo-posterior is equal to the prior; the green crosses are the optima. (b) shows the result of optimization after 3 initialization samples drawn from the prior at random and 50 BO iterations. The dots in (b) show the points explored by BOPrO, with greener points denoting later iterations. The colored heatmap shows the log of the pseudo-posterior $g(\boldsymbol{x})$.
optimization results after $D+1=3$ initialization samples and 50 BO iterations. Note that, once again, optimization begins near the region incentivized by the prior, but moves away from the prior and towards the optima as BO progresses. After 50 BO iterations, BOPrO finds all three optima regions of the Branin.</p>
<h1>B Mathematical Derivations</h1>
<h2>B. 1 EI Derivation</h2>
<p>Here, we provide a full derivation of Eq. (7):</p>
<p>$$
E I_{f_{\gamma}}(\boldsymbol{x}):=\int_{-\infty}^{\infty} \max \left(f_{\gamma}-y, 0\right) p(y \mid \boldsymbol{x}) d y=\int_{-\infty}^{f_{\gamma}}\left(f_{\gamma}-y\right) \frac{p(\boldsymbol{x} \mid y) p(y)}{p(\boldsymbol{x})} d y
$$</p>
<p>As defined in Section 3.2, $p\left(y&lt;f_{\gamma}\right)=\gamma$ and $\gamma$ is a quantile of the observed objective values $\left{y^{(i)}\right}$. Then $p(\boldsymbol{x})=\int_{\mathbb{R}} p(\boldsymbol{x} \mid y) p(y) d y=\gamma g(\boldsymbol{x})+(1-\gamma) b(\boldsymbol{x})$, where $g(\boldsymbol{x})$ and $b(\boldsymbol{x})$ are the posteriors introduced in Section 3.3. Therefore</p>
<p>$$
\begin{aligned}
\int_{-\infty}^{f_{\gamma}}\left(f_{\gamma}-y\right) p(\boldsymbol{x} \mid y) p(y) d y &amp; =g(\boldsymbol{x}) \int_{-\infty}^{f_{\gamma}}\left(f_{\gamma}-y\right) p(y) d y \
&amp; =\gamma f_{\gamma} g(\boldsymbol{x})-g(\boldsymbol{x}) \int_{-\infty}^{f_{\gamma}} y p(y) d y
\end{aligned}
$$</p>
<p>so that finally</p>
<p>$$
E I_{f_{\gamma}}(\boldsymbol{x})=\frac{\gamma f_{\gamma} g(\boldsymbol{x})-g(\boldsymbol{x}) \int_{-\infty}^{f_{\gamma}} y p(y) d y}{\gamma g(\boldsymbol{x})+(1-\gamma) b(\boldsymbol{x})} \propto\left(\gamma+\frac{b(\boldsymbol{x})}{g(\boldsymbol{x})}(1-\gamma)\right)^{-1}
$$</p>
<h1>B. 2 Proof of Proposition 1</h1>
<p>Here, we provide the proof of Proposition 1:</p>
<p>$$
\begin{aligned}
&amp; \lim <em f__gamma="f_{\gamma">{t \rightarrow \infty} \underset{\boldsymbol{x} \in \mathcal{X}}{\arg \max } E I</em>) \
&amp; =\lim }}(\boldsymbol{x<em -_infty="-\infty">{t \rightarrow \infty} \underset{\boldsymbol{x} \in \mathcal{X}}{\arg \max } \int</em> \mid y) p(y) d y \
&amp; =\lim }^{f_{\gamma}}\left(f_{\gamma}-y\right) p(\boldsymbol{x<em -_infty="-\infty">{t \rightarrow \infty} \underset{\boldsymbol{x} \in \mathcal{X}}{\arg \max } g(\boldsymbol{x}) \int</em>-y\right) p(y) d y \
&amp; =\lim }^{f_{\gamma}}\left(f_{\gamma<em _gamma="\gamma">{t \rightarrow \infty} \underset{\boldsymbol{x} \in \mathcal{X}}{\arg \max }\left(\gamma f</em> y p(y) d y\right) \
&amp; =\lim } g(\boldsymbol{x})-g(\boldsymbol{x}) \int_{-\infty}^{f_{\gamma}<em _gamma="\gamma">{t \rightarrow \infty} \underset{\boldsymbol{x} \in \mathcal{X}}{\arg \max } \frac{\gamma f</em>
\end{aligned}
$$} g(\boldsymbol{x})-g(\boldsymbol{x}) \int_{-\infty}^{f_{\gamma}} y p(y) d y}{\gamma g(\boldsymbol{x})+(1-\gamma) b(\boldsymbol{x})</p>
<p>which, from Eq. (9), is equal to:</p>
<p>$$
=\lim _{t \rightarrow \infty} \underset{\boldsymbol{x} \in \mathcal{X}}{\arg \max }\left(\gamma+\frac{b(\boldsymbol{x})}{g(\boldsymbol{x})}(1-\gamma)\right)^{-1}
$$</p>
<p>we can take Eq. (15) to the power of $\frac{1}{t}$ without changing the expression, since the argument that maximizes EI does not change:</p>
<p>$$
=\lim _{t \rightarrow \infty} \underset{\boldsymbol{x} \in \mathcal{X}}{\arg \max }\left(\gamma+\frac{b(\boldsymbol{x})}{g(\boldsymbol{x})}(1-\gamma)\right)^{-\frac{1}{t}}
$$</p>
<p>substituting $g(x)$ and $b(x)$ using their definitions in Section 3.3:</p>
<p>$$
\begin{aligned}
&amp; =\lim <em b="b">{t \rightarrow \infty} \underset{\boldsymbol{x} \in \mathcal{X}}{\arg \max }\left(\gamma+\frac{P</em>}(\boldsymbol{x}) \mathcal{M<em g="g">{b}(\boldsymbol{x})^{\frac{t}{\beta}}}{P</em>}(\boldsymbol{x}) \mathcal{M<em _infty="\infty" _rightarrow="\rightarrow" t="t">{g}(\boldsymbol{x})^{\frac{t}{\beta}}}(1-\gamma)\right)^{-\frac{1}{t}} \
&amp; =\lim </em>} \underset{\boldsymbol{x} \in \mathcal{X}}{\arg \max }\left(\frac{P_{b}(\boldsymbol{x}) \mathcal{M<em g="g">{b}(\boldsymbol{x})^{\frac{t}{\beta}}}{P</em>}(\boldsymbol{x}) \mathcal{M<em _infty="\infty" _rightarrow="\rightarrow" t="t">{g}(\boldsymbol{x})^{\frac{t}{\beta}}}(1-\gamma)\right)^{-\frac{1}{t}} \
&amp; =\lim </em>} \underset{\boldsymbol{x} \in \mathcal{X}}{\arg \max }\left(\frac{P_{b}(\boldsymbol{x})}{P_{g}(\boldsymbol{x})}\right)^{-\frac{1}{t}}\left(\frac{\mathcal{M<em g="g">{b}(\boldsymbol{x})^{\frac{t}{\beta}}}{\mathcal{M}</em>
\end{aligned}
$$}(\boldsymbol{x})^{\frac{t}{\beta}}}\right)^{-\frac{1}{t}}(1-\gamma)^{-\frac{1}{t}</p>
<p>$$
\begin{aligned}
&amp; =\lim <em b="b">{t \rightarrow \infty} \underset{\boldsymbol{x} \in \mathcal{X}}{\arg \max }\left(\frac{P</em>}(\boldsymbol{x})}{P_{g}(\boldsymbol{x})}\right)^{-\frac{1}{t}}\left(\frac{\mathcal{M<em g="g">{b}(\boldsymbol{x})}{\mathcal{M}</em> \
&amp; =\underset{\boldsymbol{x} \in \mathcal{X}}{\arg \max }\left(\frac{\mathcal{M}}(\boldsymbol{x})}\right)^{-\frac{1}{\beta}}(1-\gamma)^{-\frac{1}{t}<em g="g">{b}(\boldsymbol{x})}{\mathcal{M}</em> \
&amp; =\underset{\boldsymbol{x} \in \mathcal{X}}{\arg \max }\left(\frac{1-\mathcal{M}}(\boldsymbol{x})}\right)^{-\frac{1}{\beta}<em g="g">{g}(\boldsymbol{x})}{\mathcal{M}</em> \
&amp; =\underset{\boldsymbol{x} \in \mathcal{X}}{\arg \max }\left(\frac{1}{\mathcal{M}}(\boldsymbol{x})}\right)^{-\frac{1}{\beta}<em g="g">{g}(\boldsymbol{x})}-1\right)^{-\frac{1}{\beta}} \
&amp; =\underset{\boldsymbol{x} \in \mathcal{X}}{\arg \max }\left(\mathcal{M}</em> \
&amp; =\underset{\boldsymbol{x} \in \mathcal{X}}{\arg \max } \mathcal{M}_{g}(\boldsymbol{x})
\end{aligned}
$$}(\boldsymbol{x})\right)^{\frac{1}{\beta}</p>
<p>This shows that as iterations progress, the model grows more important. If BOPrO is run long enough, the prior washes out and BOPrO only trusts the probabilistic model. Since $\mathcal{M}<em f__gamma="f_{\gamma">{g}(\boldsymbol{x})$ is the Probability of Improvement (PI) on the probabilistic model $p(y \mid \boldsymbol{x})$ then, in the limit, maximizing the acquisition function $E I</em>)$. In other words, for high values of $t$, BOPrO converges to standard BO with a PI acquisition function.}}(\boldsymbol{x})$ is equivalent to maximizing the PI acquisition function on the probabilistic model $p(y \mid \boldsymbol{x</p>
<h1>C Experimental Setup</h1>
<p>We use a combination of publicly available implementations for our predictive models. For our Gaussian Process (GP) model, we use GPy's [15] GP implementation with the Matérn5/2 kernel. We use different length-scales for each input dimensions, learned via Automatic Relevance Determination (ARD) 34. For our Random Forests (RF), we use scikit-learn's RF implementation 37. We set the fraction of features per split to 0.5 , the minimum number of samples for a split to 5 and disable bagging. We also adapt our RF implementation to use the same split selection approach as Hutter et al. 19.</p>
<p>For our constrained Bayesian Optimization (cBO) approach, we use scikitlearn's RF classifier, trained on previously explored configurations, to predict the probability of a configuration being feasible. We then weight our EI acquisition function by this probability of feasibility, as proposed by Gardner et al. 13. We normalize our EI acquisition function before considering the probability of feasibility, to ensure both values are in the same range. This cBO implementation is used in the Spatial use-case as in Nardi et al. 33.</p>
<p>For all experiments, we set the model weight hyperparameter to $\beta=10$ and the model quantile to $\gamma=0.05$, see Appendices J and I. Before starting the main BO loop, BOPrO is initialized by random sampling $D+1$ points from the prior, where $D$ is the number of input variables. We use the public implementation</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{11}$ If the optimum for a benchmark is not known, we approximate it using the best value found during previous BO experiments.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8265 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8265</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8265</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-278171671</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.20771v2.pdf" target="_blank">Computational Reasoning of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> With the rapid development and widespread application of Large Language Models (LLMs), multidimensional evaluation has become increasingly critical. However, current evaluations are often domain-specific and overly complex, limiting their effectiveness as cross-domain proxies for core capabilities. To address these limitations and enable a unified and simple evaluation framework, an ideal proxy task should target a basic capability that generalizes across tasks and is independent of domain-specific knowledge. Turing machine provides a powerful theoretical lens by reducing complex processes to basic, domain-agnostic computational operations. This perspective offers a principled framework for evaluating basic computational abilities essential to a wide range of tasks. Motivated by this abstraction, we introduce \textbf{Turing Machine Bench}, a benchmark designed to assess the ability of LLMs to \textbf{strictly follow rules} and \textbf{accurately manage internal states} for multi-step, referred to as \textbf{computational reasoning}. TMBench incorporates four key features: self-contained and knowledge-agnostic reasoning, a minimalistic multi-step structure, controllable difficulty, and a solid theoretical foundation based on Turing machine. Empirical results demonstrate that TMBench serves as an effective proxy for evaluating computational reasoning on representative LLMs. It produces clear step-wise accuracy curves, revealing LLMs' ability to execute multi-step reasoning processes. By analyzing performance trends across TMBench and established reasoning benchmarks, we find strong correlations with real-world tasks, bridging real-task evaluation with basic ability assessment. These findings suggest that TMBench holds potential as a cross-domain dimension for evaluating reasoning in LLMs. Code and data are available at \href{https://github.com/HaitaoWuTJU/Turing-Machine-Bench}{Repo}.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8265.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8265.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TMBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Turing Machine Bench (m-Tag System Simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-contained, knowledge-agnostic benchmark that prompts LLMs to output each step of an m-Tag system simulation to measure strict rule-following and internal state management across multi-step computations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TMBench evaluation (framework)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a model; a benchmarking framework introduced in this paper that drives models to produce explicit, verifiable step-by-step execution traces for m-Tag systems (m>1).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['explicit step-by-step simulation prompting (CoT-like deterministic trace output)', 'greedy decoding (default)', 'sampling/temperature variations (ablation)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>TMBench uses a prompting protocol that requires the model to produce the execution trace for each tag-system transition (read head symbol, append according to rule, delete m symbols). The paper evaluates models mainly via a 1-shot example that enforces the output format (an explicit intermediate-step trace). Decoding is primarily greedy; temperature and top-p are varied in ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>The benchmark enforces a single explicit stepwise output format across all evaluated models (a CoT-like step trace). The paper does not implement or compare alternative prompting styles (e.g., alternative reasoning methods) as primary experimental conditions; instead it performs ablations on decoding temperature, alphabet, deletion number (m), and unbounded-step execution.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>TMBench: m-Tag system simulation (self-contained, knowledge-agnostic multi-step task); correlated with AIME2024, MATH500, GPQA Diamond, and MMLU Pro for real-task alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Provides step-wise accuracies, Step-Weighted Accuracy (uniform and linear), and Pass Rate. Example top-model results reported: Gemini-2.5-Pro achieves >90% accuracy at 30 steps and high SWA (SWA Uni. ≈96.6, SWA Lin. ≈96.2) with a Pass Rate reported in the paper's table (high, ≈94%); smaller models (<4B) often fail early (even step 1). Exact per-model metrics are in Table 2 of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>TMBench elicits clear step-wise accuracy curves showing error accumulation with steps; models tend to fail as step count increases due to autoregressive/statistical nature. Performance is robust across alphabet types for top models, suggesting reliance on procedural reasoning rather than surface token statistics. Varying the deletion number m produces a smooth difficulty gradient. Temperature values 0–1 yield stable performance, higher temperatures degrade performance.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>TMBench operationalizes computational reasoning (faithful rule execution) and demonstrates that enforcing explicit stepwise trace outputs effectively measures this ability; across models the paper uses a consistent single stepwise method rather than comparing multiple reasoning paradigms, and shows TMBench strongly correlates with several established reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Computational Reasoning of Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8265.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8265.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits intermediate reasoning steps from LLMs to improve multi-step problem solving by having the model produce explicit chains of intermediate thoughts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chain-of-Thought (method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting method discussed in related work; described as encouraging step-by-step reasoning and used widely in prior benchmarks to improve multi-step performance.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (stepwise decomposition)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>CoT prompts the model to provide intermediate reasoning steps (a sequence of sub-steps) before the final answer. The paper references CoT as prior art and describes it as a method to break complex problems into manageable steps; TMBench's stepwise trace is similar in spirit.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (CoT itself enables producing diverse chains when combined with sampling/self-consistency)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Mentioned only in related work; the paper does not run an experiment comparing CoT vs. other prompting techniques on TMBench.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Prior work: applied to various reasoning benchmarks (commonsense, math, symbolic reasoning); in this paper CoT is referenced as context for multi-step evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>No direct CoT-specific performance numbers reported in this paper (CoT is cited in related work but not experimentally varied here).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Paper acknowledges CoT's role in prior advances in multi-step reasoning but focuses its own experiments on a fixed stepwise trace prompting rather than exploring CoT variations.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>CoT is an established approach to elicit step-by-step reasoning; this paper's TMBench enforces an explicit per-step trace similar to CoT but does not compare CoT variants experimentally.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Computational Reasoning of Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8265.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8265.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (CoT extension)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of chain-of-thought that samples multiple reasoning paths and aggregates outputs (e.g., majority vote) to improve reliability of multi-step solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Self-Consistency (method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Method referenced in related work; improves performance by sampling multiple reasoning trajectories and selecting the most consistent final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['sampling multiple reasoning paths + majority voting (self-consistency)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Self-consistency augments CoT by generating diverse chain-of-thought samples (via sampling) and choosing the most common final answer to reduce variance and errors from single sampled chains. The paper cites this method as relevant prior work but does not apply it to TMBench.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Mentioned only in related work; no ablation or experimental comparison between self-consistency and single-path (greedy) execution on TMBench is performed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Previously used on mathematical and reasoning benchmarks; in this paper it is discussed in literature context only.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>No self-consistency experimental results provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>The paper notes self-consistency as a technique that increases reliability in multi-step reasoning in prior work, but TMBench experiments primarily use deterministic/greeedy or temperature ablations rather than multi-path aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Although self-consistency represents a diversity-promoting approach, this paper does not experimentally compare it against the single deterministic trace prompting employed in TMBench.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Computational Reasoning of Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8265.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8265.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scratchpad</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scratchpad / intermediate computation prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique that encourages LLMs to 'show work' by maintaining intermediate computations (a scratchpad) to improve stepwise correctness in algorithmic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Scratchpad (method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced in related work as a method that records intermediate computation steps (scratchpad) to facilitate transparent multi-step computation; conceptually similar to TMBench's per-step trace requirement.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['scratchpad / explicit intermediate computation recording']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Scratchpad prompting requires the model to provide intermediate computational steps or partial results; the paper references it as part of the multi-step reasoning literature but does not experimentally compare scratchpad vs. TMBench prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar (an explicit single structured internal trace)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Mentioned only in related work; TMBench enforces explicit intermediate steps but does not vary between scratchpad-style and other prompting paradigms.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Prior code/math simulation tasks; in this paper referenced for context.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>No scratchpad-specific performance reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>TMBench's requirement that models output the intermediate queue state is functionally a scratchpad-style constraint; authors argue that enforcing such traces yields interpretable, verifiable assessment of computational reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Scratchpad-style intermediate outputs are aligned with TMBench's design; however, the paper does not present ablations that directly compare scratchpad prompting to alternative reasoning methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Computational Reasoning of Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8265.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8265.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-2.5-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini 2.5 Pro</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary high-capability LLM evaluated in the paper that demonstrates strong computational reasoning on TMBench, showing emergent multi-step execution ability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-2.5-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary LLM (architecture details not specified in-paper). Evaluated via TMBench with greedy decoding and 1-shot step-format prompt; also subjected to ablations (temperature, alphabet, unbounded-step).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['explicit step-by-step TMBench trace prompting (CoT-like)', 'greedy decoding (default)', 'sampling with temperature for ablation/unbounded-step experiments']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Gemini was prompted to output each queue state per transition (1-shot example provided in prompt). Default inference used greedy decoding; temperature ablations (0–3.0) were performed, and unbounded-step generation through budget forcing was used to locate maximum correct step before failure.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>No experiment compared diverse reasoning methods; the model was evaluated under the single enforced stepwise-trace prompting. Ablations applied: unbounded-step execution (showing earliest failure at step 16 and latest at 683 in sampled runs), decoding temperature sweep (performance stable 0–1, degrades at higher temps), alphabet variations, and difficulty (varying m).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>TMBench (m-Tag simulation); correlation analysis with AIME2024, MATH500, GPQA Diamond, MMLU Pro.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported strong performance on TMBench: >90% accuracy at 30 steps; SWA (Uniform) ≈ 96.6, SWA (Linear) ≈ 96.2 and a high Pass Rate (table reports ≈94%); robust across alphabets; stable for deletion number m in [2,9], performance drops for m>10.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Gemini shows emergent ability to simulate Turing-like processes and produces consistent traces across alphabets, supporting reliance on procedural reasoning. However, as an autoregressive model it inevitably fails with very long step sequences. Temperature above 1.0 and high deletion numbers (m>10) undermine its performance.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Gemini-2.5-Pro excels at the single enforced stepwise reasoning protocol used by TMBench, correlates well with several real-task reasoning benchmarks, and highlights that enforcing explicit stepwise outputs is an effective way to measure computational reasoning; the paper does not compare Gemini running diverse reasoning strategies (e.g., self-consistency) versus a single strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Computational Reasoning of Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-Thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models. <em>(Rating: 2)</em></li>
                <li>Show your work: Scratchpads for intermediate computation with language models. <em>(Rating: 2)</em></li>
                <li>Executing arithmetic: Fine-tuning large language models as turing machines. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8265",
    "paper_id": "paper-278171671",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "TMBench",
            "name_full": "Turing Machine Bench (m-Tag System Simulation)",
            "brief_description": "A self-contained, knowledge-agnostic benchmark that prompts LLMs to output each step of an m-Tag system simulation to measure strict rule-following and internal state management across multi-step computations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "TMBench evaluation (framework)",
            "model_description": "Not a model; a benchmarking framework introduced in this paper that drives models to produce explicit, verifiable step-by-step execution traces for m-Tag systems (m&gt;1).",
            "reasoning_methods": [
                "explicit step-by-step simulation prompting (CoT-like deterministic trace output)",
                "greedy decoding (default)",
                "sampling/temperature variations (ablation)"
            ],
            "reasoning_methods_description": "TMBench uses a prompting protocol that requires the model to produce the execution trace for each tag-system transition (read head symbol, append according to rule, delete m symbols). The paper evaluates models mainly via a 1-shot example that enforces the output format (an explicit intermediate-step trace). Decoding is primarily greedy; temperature and top-p are varied in ablations.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "The benchmark enforces a single explicit stepwise output format across all evaluated models (a CoT-like step trace). The paper does not implement or compare alternative prompting styles (e.g., alternative reasoning methods) as primary experimental conditions; instead it performs ablations on decoding temperature, alphabet, deletion number (m), and unbounded-step execution.",
            "task_or_benchmark": "TMBench: m-Tag system simulation (self-contained, knowledge-agnostic multi-step task); correlated with AIME2024, MATH500, GPQA Diamond, and MMLU Pro for real-task alignment.",
            "performance_results": "Provides step-wise accuracies, Step-Weighted Accuracy (uniform and linear), and Pass Rate. Example top-model results reported: Gemini-2.5-Pro achieves &gt;90% accuracy at 30 steps and high SWA (SWA Uni. ≈96.6, SWA Lin. ≈96.2) with a Pass Rate reported in the paper's table (high, ≈94%); smaller models (&lt;4B) often fail early (even step 1). Exact per-model metrics are in Table 2 of the paper.",
            "qualitative_findings": "TMBench elicits clear step-wise accuracy curves showing error accumulation with steps; models tend to fail as step count increases due to autoregressive/statistical nature. Performance is robust across alphabet types for top models, suggesting reliance on procedural reasoning rather than surface token statistics. Varying the deletion number m produces a smooth difficulty gradient. Temperature values 0–1 yield stable performance, higher temperatures degrade performance.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "TMBench operationalizes computational reasoning (faithful rule execution) and demonstrates that enforcing explicit stepwise trace outputs effectively measures this ability; across models the paper uses a consistent single stepwise method rather than comparing multiple reasoning paradigms, and shows TMBench strongly correlates with several established reasoning benchmarks.",
            "uuid": "e8265.0",
            "source_info": {
                "paper_title": "Computational Reasoning of Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that elicits intermediate reasoning steps from LLMs to improve multi-step problem solving by having the model produce explicit chains of intermediate thoughts.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Chain-of-Thought (method)",
            "model_description": "Prompting method discussed in related work; described as encouraging step-by-step reasoning and used widely in prior benchmarks to improve multi-step performance.",
            "reasoning_methods": [
                "chain-of-thought (stepwise decomposition)"
            ],
            "reasoning_methods_description": "CoT prompts the model to provide intermediate reasoning steps (a sequence of sub-steps) before the final answer. The paper references CoT as prior art and describes it as a method to break complex problems into manageable steps; TMBench's stepwise trace is similar in spirit.",
            "reasoning_diversity": "both (CoT itself enables producing diverse chains when combined with sampling/self-consistency)",
            "reasoning_diversity_experimental_setup": "Mentioned only in related work; the paper does not run an experiment comparing CoT vs. other prompting techniques on TMBench.",
            "task_or_benchmark": "Prior work: applied to various reasoning benchmarks (commonsense, math, symbolic reasoning); in this paper CoT is referenced as context for multi-step evaluation.",
            "performance_results": "No direct CoT-specific performance numbers reported in this paper (CoT is cited in related work but not experimentally varied here).",
            "qualitative_findings": "Paper acknowledges CoT's role in prior advances in multi-step reasoning but focuses its own experiments on a fixed stepwise trace prompting rather than exploring CoT variations.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "CoT is an established approach to elicit step-by-step reasoning; this paper's TMBench enforces an explicit per-step trace similar to CoT but does not compare CoT variants experimentally.",
            "uuid": "e8265.1",
            "source_info": {
                "paper_title": "Computational Reasoning of Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Self-consistency",
            "name_full": "Self-Consistency (CoT extension)",
            "brief_description": "An extension of chain-of-thought that samples multiple reasoning paths and aggregates outputs (e.g., majority vote) to improve reliability of multi-step solutions.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Self-Consistency (method)",
            "model_description": "Method referenced in related work; improves performance by sampling multiple reasoning trajectories and selecting the most consistent final answer.",
            "reasoning_methods": [
                "sampling multiple reasoning paths + majority voting (self-consistency)"
            ],
            "reasoning_methods_description": "Self-consistency augments CoT by generating diverse chain-of-thought samples (via sampling) and choosing the most common final answer to reduce variance and errors from single sampled chains. The paper cites this method as relevant prior work but does not apply it to TMBench.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Mentioned only in related work; no ablation or experimental comparison between self-consistency and single-path (greedy) execution on TMBench is performed in this paper.",
            "task_or_benchmark": "Previously used on mathematical and reasoning benchmarks; in this paper it is discussed in literature context only.",
            "performance_results": "No self-consistency experimental results provided in this paper.",
            "qualitative_findings": "The paper notes self-consistency as a technique that increases reliability in multi-step reasoning in prior work, but TMBench experiments primarily use deterministic/greeedy or temperature ablations rather than multi-path aggregation.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Although self-consistency represents a diversity-promoting approach, this paper does not experimentally compare it against the single deterministic trace prompting employed in TMBench.",
            "uuid": "e8265.2",
            "source_info": {
                "paper_title": "Computational Reasoning of Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Scratchpad",
            "name_full": "Scratchpad / intermediate computation prompting",
            "brief_description": "A technique that encourages LLMs to 'show work' by maintaining intermediate computations (a scratchpad) to improve stepwise correctness in algorithmic tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Scratchpad (method)",
            "model_description": "Referenced in related work as a method that records intermediate computation steps (scratchpad) to facilitate transparent multi-step computation; conceptually similar to TMBench's per-step trace requirement.",
            "reasoning_methods": [
                "scratchpad / explicit intermediate computation recording"
            ],
            "reasoning_methods_description": "Scratchpad prompting requires the model to provide intermediate computational steps or partial results; the paper references it as part of the multi-step reasoning literature but does not experimentally compare scratchpad vs. TMBench prompting.",
            "reasoning_diversity": "similar (an explicit single structured internal trace)",
            "reasoning_diversity_experimental_setup": "Mentioned only in related work; TMBench enforces explicit intermediate steps but does not vary between scratchpad-style and other prompting paradigms.",
            "task_or_benchmark": "Prior code/math simulation tasks; in this paper referenced for context.",
            "performance_results": "No scratchpad-specific performance reported in this paper.",
            "qualitative_findings": "TMBench's requirement that models output the intermediate queue state is functionally a scratchpad-style constraint; authors argue that enforcing such traces yields interpretable, verifiable assessment of computational reasoning.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Scratchpad-style intermediate outputs are aligned with TMBench's design; however, the paper does not present ablations that directly compare scratchpad prompting to alternative reasoning methods.",
            "uuid": "e8265.3",
            "source_info": {
                "paper_title": "Computational Reasoning of Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Gemini-2.5-Pro",
            "name_full": "Gemini 2.5 Pro",
            "brief_description": "A proprietary high-capability LLM evaluated in the paper that demonstrates strong computational reasoning on TMBench, showing emergent multi-step execution ability.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini-2.5-Pro",
            "model_description": "Proprietary LLM (architecture details not specified in-paper). Evaluated via TMBench with greedy decoding and 1-shot step-format prompt; also subjected to ablations (temperature, alphabet, unbounded-step).",
            "reasoning_methods": [
                "explicit step-by-step TMBench trace prompting (CoT-like)",
                "greedy decoding (default)",
                "sampling with temperature for ablation/unbounded-step experiments"
            ],
            "reasoning_methods_description": "Gemini was prompted to output each queue state per transition (1-shot example provided in prompt). Default inference used greedy decoding; temperature ablations (0–3.0) were performed, and unbounded-step generation through budget forcing was used to locate maximum correct step before failure.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "No experiment compared diverse reasoning methods; the model was evaluated under the single enforced stepwise-trace prompting. Ablations applied: unbounded-step execution (showing earliest failure at step 16 and latest at 683 in sampled runs), decoding temperature sweep (performance stable 0–1, degrades at higher temps), alphabet variations, and difficulty (varying m).",
            "task_or_benchmark": "TMBench (m-Tag simulation); correlation analysis with AIME2024, MATH500, GPQA Diamond, MMLU Pro.",
            "performance_results": "Reported strong performance on TMBench: &gt;90% accuracy at 30 steps; SWA (Uniform) ≈ 96.6, SWA (Linear) ≈ 96.2 and a high Pass Rate (table reports ≈94%); robust across alphabets; stable for deletion number m in [2,9], performance drops for m&gt;10.",
            "qualitative_findings": "Gemini shows emergent ability to simulate Turing-like processes and produces consistent traces across alphabets, supporting reliance on procedural reasoning. However, as an autoregressive model it inevitably fails with very long step sequences. Temperature above 1.0 and high deletion numbers (m&gt;10) undermine its performance.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Gemini-2.5-Pro excels at the single enforced stepwise reasoning protocol used by TMBench, correlates well with several real-task reasoning benchmarks, and highlights that enforcing explicit stepwise outputs is an effective way to measure computational reasoning; the paper does not compare Gemini running diverse reasoning strategies (e.g., self-consistency) versus a single strategy.",
            "uuid": "e8265.4",
            "source_info": {
                "paper_title": "Computational Reasoning of Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-Thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models.",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models.",
            "rating": 2,
            "sanitized_title": "show_your_work_scratchpads_for_intermediate_computation_with_language_models"
        },
        {
            "paper_title": "Executing arithmetic: Fine-tuning large language models as turing machines.",
            "rating": 2,
            "sanitized_title": "executing_arithmetic_finetuning_large_language_models_as_turing_machines"
        }
    ],
    "cost": 0.01549825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Computational Reasoning of Large Language Models
19 May 2025</p>
<p>Haitao Wu 
Tianjin University</p>
<p>Shanghai AI Laboratory</p>
<p>STAR Centre for Frontier AI Research</p>
<p>Zongbo Han 
Tianjin University</p>
<p>Joey Tianyi Zhou 
Huaxi Huang 
Shanghai AI Laboratory</p>
<p>Changqing Zhang 
Tianjin University</p>
<p>Shanghai AI Laboratory</p>
<p>Computational Reasoning of Large Language Models
19 May 2025628461414CF0227279A0346240F171CFarXiv:2504.20771v2[cs.CL]
With the rapid development and widespread application of Large Language Models (LLMs), multidimensional evaluation has become increasingly critical.However, current evaluations are often domain-specific and overly complex, limiting their effectiveness as cross-domain proxies for core capabilities.To address these limitations and enable a unified and simple evaluation framework, an ideal proxy task should target a basic capability that generalizes across tasks and is independent of domain-specific knowledge.Turing machine provides a powerful theoretical lens by reducing complex processes to basic, domain-agnostic computational operations.This perspective offers a principled framework for evaluating foundational computational abilities essential to a wide range of tasks, particularly those involving complex, multi-step reasoning such as mathematics.Motivated by this abstraction, we introduce Turing Machine Bench, a benchmark designed to assess the ability of LLMs to strictly follow rules and accurately manage internal states for multi-step, referred to as computational reasoning.TMBench incorporates four key features: self-contained and knowledge-agnostic reasoning, a minimalistic multi-step structure, controllable difficulty, and a solid theoretical foundation based on Turing machine.Empirical results demonstrate that TMBench serves as an effective proxy for evaluating computational reasoning on representative LLMs.It produces clear step-wise accuracy curves, revealing LLMs' ability to execute multi-step reasoning processes.By analyzing performance trends across TMBench and established reasoning benchmarks, we find strong correlations with real-world tasks, bridging real-task evaluation with basic ability assessment.These findings suggest that TMBench holds potential as a cross-domain dimension for evaluating reasoning in LLMs.Code and data are available at Repo.</p>
<p>Introduction</p>
<p>Recent progress in pre-training, post-training, and scaling has significantly advanced the capabilities of LLMs.These models now demonstrate remarkable capabilities not only in traditional natural language processing tasks, such as text classification and machine translation [1], but also in more complex cognitive domains, including advanced reasoning [2,3], code generation [4,5], instruction following [6], multimodal understanding [7], and even scientific discovery [8].While these advancements significantly broaden the scope of potential applications, the growing complexity and diversity of cognitive tasks undertaken by LLMs present substantial challenges in accurately and reliably evaluating their true capabilities.The intelligence of LLMs can be mainly evaluated along two key dimensions: (1) knowledge and comprehension, and (2) reasoning and decision-making.First, knowledge and comprehension benchmarks primarily focus on the ability of LLM to apply its internal knowledge to understand and solve user questions.These skills are typically evaluated through large-scale question answering tasks (e.g., MMLU [9], TriviaQA [10]).Second, reasoning is the process of inferring and justifying conclusions from a set of premises [11].In particular, reasoning encompasses a broad range of tasks, including logical deduction (e.g., Big-Bench Hard [12]), mathematical reasoning (e.g., AIME [13]), formal proof (e.g., miniF2F [14], ProofNet [15]), programming (e.g., HumanEval [5]) and computational reasoning(e.g., Arithmetic [16], CodeSimulation [6]).Specifically, computational reasoning refers to the ability to accurately interpret formal rules and execute multi-step computational operations without external tools.This ability is fundamental to modern sciences, which are generally founded on rule-based systems.Disciplines such as mathematics, physics, and chemistry rely on axioms, theorems, and laws, collectively referred to as rules.At the most fundamental level, arithmetic represents the simplest form of such rules.Although LLMs are not inherently proficient at direct numerical computation, they have demonstrated the ability to perform accurate arithmetic by simulating a Turing machine designed for arithmetic [16].This suggests that LLMs have the potential to faithfully execute formal rules in multi-step reasoning tasks.By assessing the results across these two dimensions, we obtain a more comprehensive understanding of an LLM's intelligence.However, these dimensions often overlap in practice.For instance, solving a mathematical problem may still rely on background knowledge encoded during pre-training, blurring the distinction between memorization (knowledge) and computation (reasoning).</p>
<p>To systematically evaluate the computational reasoning ability of LLMs, we turn to the foundational paradigm of theoretical computer science: Turing machine.Its minimalist design and computational universality offer a precise and measurable framework to evaluate a model's ability to follow rules and deterministic state transitions.We introduce TMBench, a novel benchmark for evaluating computational reasoning of LLMs by simulating the operation of an m-Tag Turing machine.The m-Tag Turing machine consists of production rules, a dynamic queue, and at each step of the process, it reads the queue head and appends production symbols corresponding to the head symbol at the queue tail, then deleting the first m characters.By iterating this process across multiple steps and comparing the LLM's execution trace with ground truth, we obtain a quantitative measure of its computational reasoning ability as an executor.TMBench presents several four features: (1) Selfcontained and knowledge-agnostic reasoning, as all tasks are solvable from first principles without reliance on external knowledge; (2) Minimalistic multi-step structure, deliberately designed to avoid redundancy, ensuring that each reasoning step is necessary, interpretable, and yields a verifiable intermediate result; (3) Controllable difficulty, enabled by modifying the rule sets, m values, step numbers and input lengths to produce a continuum of task complexity; (4) Computational generality, as any computable function can be represented by a Turing machine, making it a principled proxy for basic abilities such as rule-following and state management.Our main contributions are summarized as follows:</p>
<ol>
<li>
<p>We propose the TMBench to evaluate the computation reasoning ability of LLMs.Evaluations are conducted on a broad range of recent open-source models, from 0.6B to 671B parameters, as well as proprietary models such as Gemini, GPT, Grok, and Claude.</p>
</li>
<li>
<p>Empirical results demonstrate that TMBench strongly correlates with established reasoning benchmarks and real-world task performance, highlighting its practical relevance.It effectively captures LLMs' multi-step reasoning abilities through clear step-wise accuracy trends, making it a promising cross-domain metric for evaluating computational reasoning.</p>
</li>
</ol>
<p>3.</p>
<p>To investigate TMBench's characteristics, we conduct ablation studies on unbounded-step execution, decoding temperature, alphabet type, and task difficulty.Results show that LLM inevitably fails with increasing steps due to its autoregressive nature.Its stable performance across different alphabets suggests reliance on reasoning rather than statistics.Varying deletion counts enables fine-grained control of task difficulty, demonstrating the benchmark's scalability.</p>
<p>2 Related Work</p>
<p>LLM Benchmarks</p>
<p>Benchmarks for large language models can be primarily categorized into two types: closed-ended benchmarks, which provide definitive answers, and open-ended benchmarks, which rely on human preference.For closed-ended benchmarks, a wide range of topics is addressed, including language understanding, mathematics, coding, reasoning, hallucination, toxicity, and stereotypes.Notable benchmarks in this category include MMLU [9], HellaSwag [17], GSM-8K [18], HELM [19], BigBench [12], AGIEval [20], HumanEval [21], and ToxicChat [22].Beyond closed-ended questions,</p>
<p>benchmarks also include open-ended questions that with human preference.These questions are typically assessed through expert ratings or crowd-sourced evaluations.The recent trend includes utilizing GPT-4 for approximating human judgment [23], with notable instances being MT-Bench [24] and AlpacaEval [25].In addition to static benchmarks, dynamic benchmarks featuring continuously updated questions are also available.These questions may be drawn from annual examinations, weekly online contests such as Codeforces [26,27], or collaborative platforms like ChatbotArena [24].Some studies have explored leveraging live human interaction for reinforcement learning from human preferences [28,29,30].In this paper, we introduce Turing Machine Bench, a self-contained, multi-step benchmark for computation reasoning that avoids the influence of external knowledge.</p>
<p>Multi-step Reasoning</p>
<p>Recent large language models demonstrate remarkable performance in solving complex reasoning tasks [31].Multi-step reasoning, essential for tackling such tasks, has gained significant attention, with numerous methods proposed to enhance the reasoning capabilities of LLM.Chain-of-Thought (CoT) [32] and scratchpad [33] encourages step-by-step reasoning by breaking down complex problems into manageable steps, while CoT's extension, self-consistency [34], improves performance by sampling multiple reasoning paths and selecting the most consistent solution through majority voting.These methods have been extensively evaluated across a variety of benchmarks, including commonsense reasoning [35,36], mathematical reasoning [37,38], symbolic reasoning [39,40,41,42], logical reasoning [43,44], and multi-modal reasoning [45,46,47,48].These advancements have enhanced the diversity and reliability of multi-step reasoning evaluations.However, the varying levels of difficulty across steps in most reasoning tasks pose challenges for controlled assessments, particularly in quantifying error propagation and addressing context limitations.Ensuring consistent difficulty across steps is therefore crucial for transparent and reliable evaluation of the reasoning capabilities of large language models, which remains significantly underexplored.</p>
<p>Rule System</p>
<p>Complexity emerges from simple rules.Modern scientific subjects such as mathematics, physics, and chemistry are generally founded on rule-based systems.These rely on axioms [49], theorems [50], and laws [51] to guide reasoning and model phenomena.Arithmetic is the most basic example, providing core operations and relations that support more advanced structures.In physics, Newton's laws of motion describe how forces affect bodies, forming the basis of classical mechanics.Cellular automata illustrate how simple local rules can give rise to complex global behavior [52].Rule system is also essential in program verification and automated reasoning, facilitating formal proofs through Lean theorem prover [53], for example.We define the ability of an LLM to faithfully execute rules step by step as computational reasoning.Based on this definition, we propose TMBench, a rule-based system grounded in Turing machines.In TMBench, LLM is prompted to output the result of each rule execution step, which is then evaluated against ground-truth trajectories for assessment.Accurate intermediate steps are essential to the integrity of the rule-based system.</p>
<p>3 Turing Machine Bench</p>
<p>Motivation</p>
<p>Reasoning is one of the most important abilities of LLMs and has gain significant attention recently.Current reasoning benchmarks focus on the final results and depend on domain-specific knowledge.</p>
<p>For instance, the AIME benchmark, used to evaluate mathematical reasoning abilities, relies on the mastery of mathematical knowledge.Furthermore, there is a risk that models might tailor their responses to specific benchmark metrics, thereby inflating their performance.From an intuitive perspective, reasoning is the process of selecting appropriate rules and applying them faithfully step by step.We refer to this capability as computational reasoning.Computational reasoning is the ability to systematically select and accurately apply rules, ensuring that each step is transparent, verifiable, and grounded within the given rule system.In contrast to logical reasoning, which primarily focuses on the validity of conclusions, computational reasoning emphasizes the faithfulness and traceability of the entire reasoning process.Therefore, we propose an evaluation framework to evaluate computational reasoning by Universal Turing Machine Simulation.</p>
<p>Turing Machine</p>
<p>Turing Machine (TM) is a foundational model of computation [54], defining algorithmic processes and computational limits.Formally, a TM is represented as T = (Q, Γ, δ, q 0 , H), where Q is the set of states, Γ the tape alphabet, δ : Q × Γ → Q × Γ × {L, R} the transition function, q 0 the initial state, and H the set of halting states.A TM manipulates symbols on an infinite tape using a read-write head, guided by a transition function δ, and executes computations step by step until it reaches a halting state.It serves as a fundamental abstraction for formalizing decidability and complexity classes.Beyond theoretical significance, TM are valuable for modeling symbolic reasoning and structured inference.In this work, we leverage Turing Machine principles to construct a rigorous multi-step benchmark, evaluating LLMs' ability to perform structured computations which is vital for reasoning.</p>
<p>Universal Turing Machine (UTM) is a theory model of computation, capable of simulating the behavior of any other Turing machine.As a foundation of computability theory, the UTM formalizes the notion of computational universality and provides the theoretical foundation for the Church-Turing thesis, which asserts that any effectively computable function can be executed by a Turing Machine.By encoding both a machine's description and its input on its tape, a UTM demonstrates that a single device can emulate any computational process, establishing the basis for general-purpose computation and modern computing architectures.</p>
<p>To facilitate theoretical analysis while preserving computational expressiveness, we adopt the m-Tag System, a simple and recognized model.The m-Tag System has been rigorously proven to be Turingcomplete for m &gt; 1 [55,56], making it a suitable abstraction for modeling universal computation within a structured and analyzable framework.</p>
<p>m-Tag System is a formal computational model introduced in [57] as a simplified yet computationally equivalent variant of the Universal Turing Machine.It operates on a queue of symbols, iteratively applying production rules to modify the sequence.A tag system is formally described by a triplet (m, A, P ), where:</p>
<p>• m is the deletion number, specifying how many symbols are removed from the head of queue per step.• A is a finite alphabet of symbols, from which queue elements are drawn.• P is a set of production rules mapping each x ∈ A to a corresponding word P (x), which is appended to tail of the queue.</p>
<p>The single-step process is formally defined as follows:</p>
<p>Step :
x 1 Read x 2 . . . x m X −→ x 1 x 2 . . . x m Delete X P (x 1 ) Write ,(1)
where new symbols P (x 1 ) are appends to the tail of queue generated based on the head symbol x 1 and production rule.Simultaneously, m symbols are deleted from the head of the queue.This process resembles the next-token prediction mechanism employed by LLMs.</p>
<p>Tag systems are proven Turing completeness when m &gt; 1, making them the minimal yet effective computational models, see Appendix A for complete certification.An example of a 2-tag system simulation is provided in Table 1, illustrating the iterative process of reading, writing, and deletion until a halting condition is reached.</p>
<p>Table 1: Example of 2-tag systems simulation.At each step, the head symbol of the queue is read, and new symbols are appended to the tail based on production rules.The first two symbols are then removed.The system halts when the queue contains fewer than two symbols.
Alphabet {A, B, C, D, E} {1, 2, 3, 4, 5} {@, #, $, %, &amp;} Init [B A E E C] [5 2 3 2] [$ @ @ #] P-Rules A : E D A B C B : D C : E E E@ : % $ # $ # : &amp; $ : &amp; % : # # % % &amp; : % # &amp; $ Steps 0. [B A E E C] (Init) 1. [B A E E C D] 2. [E E C D D] 3. [C D D E E E D D] ...160. [$ @ @ #] (Init) 1. [$ @ @ # &amp;] 2. [@ # &amp; % $ # $] 3. [&amp; % $#$%#&amp;$] ... 28. [# &amp; $%#&amp;$&amp;] 29. [$ % # &amp; $ &amp; &amp;] 30. [# &amp; $ &amp; &amp; &amp;]</p>
<p>Evaluation Metrics</p>
<p>To comprehensively evaluate the multi-step instruction following capability of LLMs in the reasoning process, we define three key metrics:</p>
<p>Step Accuracy, Step-Weighted Accuracy, and Pass Rate.</p>
<p>Step Accuracy.It quantifies the proportion of correctly predicted queues at a given step in the reasoning process, providing a fine-grained evaluation of a model's stepwise performance.It is particularly valuable for multi-step reasoning tasks, where errors can accumulate and propagate, significantly impacting overall accuracy.The accuracy at step i is defined as:
ACC(i) = N correct (i) N total (i) ,(2)
where N correct (i) and N total (i) denote the number of correct predictions and the total number of predictions at step i, respectively.</p>
<p>Step-Weighted Accuracy (SWA) To evaluate model performance across a sequence of reasoning steps with an emphasis on later steps, we define the Step-Weighted Accuracy as a weighted average of per-step accuracies:
SWA = 1 T i=1 w i T i=1 w i • ACC(i),(3)
where w i is the weight assigned to step i.Setting w i = 1 corresponds to uniform weighting across all steps, while w i = i linearly increases the emphasis on later steps, thereby prioritizing accurate performance in deeper stages of reasoning.We evaluate performance using Step-Weighted Accuracy (SWA) under both uniform (Uni.) and linear (Lin.)weighting schemes.</p>
<p>Pass Rate.The pass rate quantifies the likelihood of successfully completing a given process without errors before termination or reaching the maximum number of allowed steps.This metric focuses more on the final outcome, whereas the previous metrics emphasize the process.</p>
<p>Experiments and Results</p>
<p>In this section, we present a series of experiments conducted on TMBench using a diverse set of models and analytical approaches.The experiments and their results are outlined as follows.First, we introduce the experimental setup including datasets and LLMs.Next, we evaluate a broad range of recent open-source LLMs, spanning from 0.6B to 671B parameters, alongside proprietary models such as Gemini, Grok, and Claude.We then assess the correlation between the TMBench Pass Rate and real-world benchmarks.Finally, we conduct ablation studies including unbounded-step, temperature, alphabet, and difficulty ablations to analyze TMBench's characteristics, which inevitably fail with increasing steps due to its autoregressive nature, remain robust across different alphabets, and allow continuous and scalable control of task difficulty.</p>
<p>Experimental Setup</p>
<p>We provide a brief overview of the experimental setup, including the datasets and the latest large language models evaluated.Detailed information is provided in the released code and data.</p>
<p>Datasets.Based on the dataset design methodology introduced in Section 3, we sample 100 instances of m-tag systems with an alphabet size of 5, where m = 2.For each system, the rule lengths range from 1 to 5, and the initial string lengths range from 2 to 9. The maximum simulation length is 30, with 11 cases halting early upon meeting the termination condition.Experiments under other parameter settings can be found in Section 4.4.In addition, we evaluate model performance on several established benchmarks, including AIME2024 [13], MATH500 [58], GPQA Diamond [2], and MMLU Pro [59].Further details can be found in the Appendix.</p>
<p>LLMs.We evaluate a diverse set of state-of-the-art LLMs, including open-source models ranging from 0.6B to 671B parameters, as well as proprietary API-only models, covering various model families and architectures to ensure a comprehensive analysis.Our selection includes the instructtuned versions of the LLaMA models, specifically the 1B, 8B, and 70B variants, released by the LLaMA team [60].From the Qwen family, we evaluate both the instruct and preview versions of the 0.6B, 1.7B, 4B, 8B, 14B, 32B, and 72B models [61], along with the language model component of the QVQ-72B-Preview multimodal system [62,63], as well as two MoE models: Qwen3-30B-A3B and Qwen3-235B-A22B.For the Gemma family, we include both Gemma3-12B and Gemma3-27B [64].We also incorporate the QwQ-32B-Preview and Sky-T1-32B-Preview model [65].For proprietary models, including Gemini-2.5-Pro[66], Grok-3 [67], Claude-3.7-Sonnet-Thinking[68], Doubao-1.5-Pro[69], and GPT-4.1 [70].For efficient and high-throughput inference, we leverage the Transformers library [71] alongside vLLM [72], which provides optimized execution for large-scale model evaluations.Unless otherwise specified, we employ a greedy decoding strategy and set the maximum generation length to 16384 tokens.</p>
<p>Evaluation of Computational Reasoning</p>
<p>We evaluate the computation reasoning capabilities of the latest LLMs by analyzing their performance across three metrics: SWA Uniform, SWA Linear, Pass Rate.These metrics offer a comprehensive  2, and the accuracy-step curves are shown in Figure 1.We observe that Gemini-2.5-Proexhibits robust accuracy in computational reasoning, achieving over 90% accuracy at 30 steps, which suggests an emergent ability to simulate Turing Machine.It is found that models smaller than 4B struggle with even the first step.This observation further supports the phenomenon of emergence [73].The QVQ multimodal model exhibits a notable decline, potentially attributable to its multimodal training process.</p>
<p>Correlation with Established Benchmarks</p>
<p>To investigate the relationship between computational reasoning ability and real-task reasoning performance, we analyzed the correlation between TMBench Pass Rate and established benchmarks (AIME2024, MATH500, GPQA Diamond and MMLU Pro) across 12 leading LLMs, as shown in Figure 2. The evaluated models include Gemini-2.5-Pro,Grok-3-beta, Claude-3-7-Sonnet, Doubao-1-5-Pro, DeepSeek-R1, GPT-4.1,Llama-4-Maverick, Qwen2.5-72B,Llama-3.3-70B,Qwen2.5-32B,Gemma-3-27B, and Qwen2.5-14B.The results reveal statistically significant correlations between TMBench and real-task benchmarks (p &lt; 0.05), confirming the validity of computational reasoning as a predictor of real-world performance.Among the benchmarks, the strength of correlation follows the expected order: GPQA &gt; AIME2024 &gt; MATH500 &gt; MMLU.These findings indicate that tasks with higher demands for deep reasoning and computation exhibit stronger correlations with TMBench performance.In contrast, benchmarks like MMLU, which rely more on factual knowledge than reasoning, show lower R square.The bottom Q-Q plots of regression residuals show that the residuals are approximately normally distributed, supporting the assumption of normality in the regression model.Moreover, we compute the average score across AIME2024, MATH500, and GPQA Diamond, obtaining a Pearson correlation coefficient of 0.882 with the TMBench pass rate, as shown in Figure 5.This strong correlation suggests that advanced reasoning performance is closely associated with computational reasoning ability.</p>
<p>Ablation Study</p>
<p>Unbounded-Step Ablation.Gemini-2.5-prodemonstrates strong instruction-following capabilities, achieving a 94% pass rate under a 30-step constraint.To further analyze the upper bound of its instruction-following ability, we conducted an unbounded-step evaluation.Using rejection sampling, we selected 10 samples and ran each for up to 1000 steps.Through budget forcing [74], Gemini was prompted to continue generating until failure with maximum token limit of 1,048,576 tokens.The earliest failure occurred at step 16, and the latest at step 683, as shown in Figure 3a.As an autoregressive model, Gemini inevitably fails with increasing steps due to its statistical nature, underscoring Gemini's computational limits.</p>
<p>Temperature ablation.To examine the effect of temperature on model performance, we conduct an ablation study with temperature values ranging from 0 to 3.0, while setting the top-p value to 0.8, as shown in Figure 3b.Performance remains stable between 0 and 1, but degrades at higher temperatures.</p>
<p>Alphabet ablation.To evaluate the robustness of large language models (LLMs) across diverse alphabet types, we conducted evaluations using four distinct character sets: Roman letters (e.g., a, b, c, d, e), Numerals (e.g., 1, 2, 3, 4, 5), Greek letters (e.g., α, β, γ, δ, ϵ), and Special characters (e.g., @, #, $, %, &amp;).As illustrated in Figure Figure 3c, while most models exhibit performance fluctuations depending on the character set, Gemini-2.5-Proconsistently achieves high accuracy across all categories, including the Special characters, which typically present greater challenges.This robustness indicates that Gemini-2.5-Prorelies on underlying reasoning mechanisms rather than superficial statistical correlations.</p>
<p>Difficulty Ablation.For TMBench, Gemini-2.5-proachieves a 94% pass rate within 30 steps, with successful simulations extending up to 686 steps, demonstrating its strong computational reasoning capabilities.To further assess this ability, we perform a difficulty ablation study with varying values of m, as shown in Figure 3d.The model maintains stable performance when the deletion number m is between 2 and 9, but performance declines steadily beyond m = 10, approaching zero after m = 15.This demonstrates that varying m produces a smooth difficulty gradient, underscoring TMBench's scalability and effectiveness as a benchmarking tool.</p>
<p>Conclusion</p>
<p>In this paper, we propose computational reasoning as a basic ability of LLMs, the ability to strictly follow rules and accurately manage internal states for multi-step, a proxy capability that generalizes across tasks and is independent of domain-specific knowledge.Motivated by this perspective, we introduce Turing Machine Bench (TMBench), a novel benchmark that leverages m-Tag System Simulation to evaluate the computational reasoning of LLMs.TMBench is characterized by four core attributes: (1) self-contained, knowledge-independent reasoning; (2) an interpretable, minimal multi-step structure; (3) adjustable task difficulty; and (4) computational generality grounded in Turing completeness, making it a rigorous proxy for basic ability.In our experiments, we evaluate state-of-the-art open-source LLMs (ranging from 0.6B to 671B parameters), along with leading proprietary models such as Gemini and Grok on TMBench.The empirical results demonstrate that TMBench effectively captures computational reasoning capabilities across models, revealing a clear multi-step performance curve.However, TMBench's knowledge-agnostic nature does not hinder its correlation with real-world tasks.Further analysis of its performance against established reasoning benchmarks reveals a strong alignment with practical tasks, effectively bridging the gap between real-task evaluation and basic ability assessment.Notably, the correlation with reasoning benchmarks such as AIME and GPQA is higher than with knowledge-centric tasks like MMLU.To further investigate the benchmark's robustness and characteristics, we conduct several ablation studies, including unbounded step ablation, temperature ablation, alphabet ablation, and difficulty ablation.As an autoregressive model, Gemini inevitably exhibits performance degradation with increasing step counts due to its statistical nature, highlighting its computational limitations.However, Gemini maintains stable performance on variations involving numerals, Greek letters, and special characters, suggesting that its success relies on underlying reasoning mechanisms rather than superficial statistical correlations.By varying the number of deletions, TMBench provides a continuous and scalability spectrum of task difficulty.</p>
<p>[E C A E E E A E E E A E E E A E E E A E E E A E E E A E E E A E E E A E E E] ### step 19:</p>
<p>-Head Symbol: E -Action: Append A E E E to the end of the queue.Remove E C from the head.</p>
<p>-Queue State:
[A E E E A E E E A E E E A E E E A E E E A E E E A E E E A E E E A E E E _ _ _ _] (Answer) -Queue State: [A E E E A E E E A E E E A E E E A E E E A E E E A E E E A E E E A E E E A E E E] (Ground Truth) ...</p>
<p>B.3 More Results</p>
<p>Token Distrubition.We present the token distribution among 36 models on TMBench, as illustrated in Figure 4.A token count is not necessarily better.Models that perform more reasoning tend to generate more tokens, but this does not always lead to improved performance.In some cases, the large number of tokens results from repetitive output, commonly referred to as the repetition problem.For state-of-the-art models such as Gemini 2.5 Pro, Grok, and Claude, their token distributions are similar.</p>
<p>Benchmark Correlation.We compute the average score across AIME2024, MATH500, and GPQA Diamond, obtaining a Pearson correlation coefficient of 0.882 with the TMBench pass rate, as shown in Figure 5.This strong correlation suggests that advanced reasoning performance is closely associated with computational reasoning ability.</p>
<p>Figure 1 :
1
Figure 1: Illustration of the multi-step performance curve on TMBench across a diverse range of both open-source and proprietary LLMs.Proprietary models demonstrate advantages in computational reasoning abilities, but accuracy still decreases as steps increase.</p>
<p>Figure 2 :
2
Figure 2: Correlation between TMBench Pass Rate (computational reasoning) and established benchmarks (AIME2024, MATH500, GPQA Diamond and MMLU Pro), where both metrics are min-max normalized, across 12 leading LLMs.Top: Scatter plots with linear fit between TMBench and each benchmark.Bottom: Q-Q plots of regression residuals.</p>
<p>Figure 3 :
3
Figure 3: Illustration of ablation results.(a) Maximum correct step achieved by Gemini under unbounded-step execution.(b) Impact of decoding temperature on performance.(c) Effect of alphabet types (Roman, Number, Greek, and Special) on model performance.(d) Task difficulty ablation by varying the number of deletions.</p>
<p>&lt;|im_start|&gt;user</p>
<p>Simulate a m-tag system.Your task is to simulate each transition step-by-step and provide the queue's state at each step.the rules and examples closely, and stop upon reaching the halt condition or 30 steps.Do not generate additional examples or new problems.No code.## Rules for Simulation: 1.In each transition, the machine performs the following steps: -If the queue length is less than m, halt -Read the head symbol of queue -Append symbols to the tail based on the head symbol and the corresponding transition rule -Delete m symbols from the head of the queue 2. The machine halt if: -The queue's length is less than m.Append A to the end of the queue.Remove B C from the head.-Queue State: [A A] ... ### step 4: -Head Symbol: C -Action: Append B to the end of the queue.Remove C B from the head.-Queue State: [B] <halt> --## The Only Problem to Solve: The response generated by Gemini-2.5-Pro,showcasing its impressive capabilities in instruction following and reasoning.&lt;|im_start|&gt;assistant is the step-by-step simulation: : [B D D] ### step 1: -Head Symbol: B -Action: Append E C E C to the end of the queue.Remove B D from the head.-Queue State: [D E C E C] ### step 2: -Head Symbol: D -Action: Append D B B to the end of the queue.Remove D E from the head.-Queue State: [C E C D B B] ... ### step 18: -Head Symbol: E -Action: Append A E E E to the end of the queue.Remove E C from the head.-Queue State:</p>
<p>Figure 4 :
4
Figure 4: Illustration of token distrubition.</p>
<p>Table 2 :
2
Performance of large language models across different scales (0.6B to 605B) and API-only models, evaluated on TMBench.Asterisks (<em>) denote widely recognized reasoning benchmarks.
ModelSWA (Uni.) SWA (Lin.) Pass Rate AIME</em> MATH<em> GPQA</em>0.6B -8BQwen3-0.6B0.90.106.765.021.7Llama-3.2-1B3.30.30018.412.6Qwen3-1.7B1.51.2126.786.432.8Qwen3-4B7.43.5656.791.649.0Llama-3.1-8B4.50.41046.421.2Qwen3-8B7.24.4860.091.456.612B+Gemma-3-12B7.71.6013.383.241.9Qwen2.5-14B10.32.6016.779.243.9R1-Distill-Qwen-14B29.518.01060.087.053.5Qwen3-14B13.89.3276.793.064.127B+Gemma-3-27B26.112.2540.088.846.0Qwen2.5-32B18.26.0710.080.648.0Sky-T1-32B-Preview20.99.1840.086.251.0QwQ-32B-Preview19.89.8453.390.462.1R1-Distill-Qwen-32B33.522.51072.694.362.1Qwen3-32B10.58.3770.094.863.6Qwen3-30B-A3B56.746.81680.495.965.870B+Llama-3.3-70B45.234.31220.075.843.9QVQ-72B-Preview14.46.4333.382.852.5Qwen2.5-Math-72B42.126.61520.085.246.0Qwen2.5-72B45.629.61913.382.845.5Llama-4-Scout7.71.2128.384.458.7Llama-4-Maverick39.127.81939.088.967.1DeepSeek-V387.484.98252.094.265.5DeepSeek-R172.263.84579.897.371.5Qwen3-235B-A22B45.232.62285.793.070.0API onlyQwen-2.5-Max27.011.8723.383.558.7OpenAI-O1-mini37.021.61163.690.060.0Gemini-1.5-Pro40.318.41020.084.256.6OpenAI-O3-mini37.130.5787.398.579.7GPT-4.158.745.72648.191.366.3Doubao-1.5-Pro76.969.15433.388.665.0Claude-3.7-Sonnet85.178.16961.396.278.2Grok-3-Beta94.692.48683.994.880.2Gemini-2.5-Pro96.696.29492.098.984.0evaluation of the models' ability to faithfully execute multi-step reasoning. Detailed results aresummarized in Table
A Theory.In this section, we demonstrate that the m-tag (m&gt;1) system constitutes a universal Turing machine, the construction presented in[56], with minor corrections.A.1 Representation of a Turing MachineWe begin by introducing the definition and step operation for the Turing Machine.Consider a Turing Machine with a binary alphabet Γ = {0, 1}, where each state Q i is defined by a quadruple (S i , D i , Q i0 , Q i1 ), corresponding to the transitions for the symbols 0 and 1.A single step of the Turing Machine, when in state Q i , is performed as follows:1. Write the symbol S i onto the current tape cell.2. Move the tape head one cell in direction D i , where D i ∈ {L, R}.3. Read the symbol S ′ from the cell currently.4. Transition to the next state based on the symbol read S ′ :In this specific transition rule, the symbol written S i and the tape head direction D i are determined solely by the current state Q i .The subsequent state transition depends on the symbol S ′ read from the tape after the move: the machine transitions to state Q i0 if S ′ = 0, and to stateAn instantaneous description of the machine with state Q i at any given step is represented as:where Q i is the state after reading α.The complete instantaneous description can be described by a triplet:where M is the string of symbols to the left of α, N is the string of symbols to the right of α.The symbol α is the current symbol being read, which will soon be replaced by S i depending on the transition rule.For a rightward movement, the next state Q ′ depends on the value of S i .IfThe updating of the strings follows the rules:In the case of a rightward movement, only the values of M and N are swapped, due to the symmetry of the process.For the sake of simplicity, we assume here that the focus is on the rightward move, without loss of generality.A.2 Equivalent Construction with a Tag SystemGiven the instantaneous description of the Turing Machine as a triplet (Q i , M, N ), we can construct a corresponding string using the following form:For simplicity and clarity, we will omit the subscript i in the notation and rewrite it as:Step 0 (Initial)Step 1Rules:whereStep 2Rules:B → S, β → sStep 3Rules:Step 4Rules:Based on the parity of N , the updated string takes different forms:If N is odd:If N is even:Step 5If N is odd:Then, the resulting string is:If N is even:Then, the resulting string is:Step 6 (Final)If N is odd:If N is even:This completes the proof that a single computation step of a Turing machine is successfully simulated by a sequence of transitions in the 2-tag system.B Experiment DetailsB.1 Dataset Details AIME2024[13]is a collection of problems from the 2024 American Invitational Mathematics Examination.It contains 30 high-school level mathematics problems requiring creative problem solving and deep mathematical insight.These problems are typically used to select top students for the USA Mathematical Olympiad.The dataset emphasizes algebraic manipulation, mathematical reasoning, and non-routine problem solving.Math500[58]is a representative benchmark consisting of 500 mathematics problems, uniformly sampled at random from the MATH[38]evaluation dataset by OpenAI.It includes problems of varying difficulty across topics such as algebra, geometry, and number theory.GPQA Diamond[2]is a high-difficulty subset of Graduate-Level Google-Proof Q&amp;A Benchmark (GPQA), consisting of 198 multiple-choice questions in biology, physics, and chemistry.It includes only questions where both experts answer correctly and the majority of non-experts answer incorrectly to ensure highest quality.MMLU Pro[59]is a professional-level extension of the original Massive Multitask Language Understanding (MMLU) benchmark[9], comprising expert-level questions across 57 domains, including mathematics, history, computer science, law, medicine, engineering, the natural sciences, and more.It enhances the original benchmark by introducing more challenging, reasoning-intensive questions, expanding answer choices from four to ten, and removing trivial or noisy items.B.2 Implementation DetailsEnvironment.Our method is implemented with Python 3.10.16,CUDA 12.4, PyTorch 2.6.0 and vLLM 0.8.5.The required libraries are specified in the requirements.txtfile provided in the repository.The experiments are performed on a machine with 96 vCPUs (2.90 GHz) from Intel Xeon processors, eight NVIDIA A100-80GB GPUs, and 1024 GB of RAM.Inference Configuration.For API-only models, we use their official APIs and set a maximum token length limit accordingly.For open-source models that exceed the GPU memory limits of a single machine, such as LLaMA 4 Maverick, DeepSeek-V3, and DeepSeek-R1, we rely on APIs provided by Alibaba Cloud and DeepSeek.For all other models, including Qwen, LLaMA, and Gemma, we use vLLM as the inference backend, which is highly efficient.We apply a sampling strategy based on greedy decoding with a maximum token limit of 16,384.Prompt.We adopt the following 1-shot reasoning prompt template for evaluation, where the shot example is used to guide the output format extraction.
Bert: Pre-training of deep bidirectional transformers language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies. the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies20191</p>
<p>Gpqa: A graduate-level google-proof q&amp;a benchmark. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R Bowman, First Conference on Language Modeling. 2024</p>
<p>Zhen Huang, Zengzhi Wang, Shijie Xia, Xuefeng Li, Haoyang Zou, Ruijie Xu, Run-Ze Fan, Lyumanshan Ye, Ethan Chern, Yixin Ye, arXiv:2406.12753Benchmarking multi-discipline cognitive reasoning for superintelligent ai. 2024arXiv preprint</p>
<p>Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, Karthik Narasimhan, arXiv:2310.06770Swe-bench: Can language models resolve real-world github issues?. 2023arXiv preprint</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet ; Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Josh Achiam, Vedant Misra, Felipe Petroski Such. Jan Leike,. 2021Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code</p>
<p>Code simulation challenges for large language models. La Emanuele, Christoph Malfa, Orazio Weinhuber, Fangru Torre, Samuele Lin, Anthony Marro, Nigel Cohn, Michael Shadbolt, Wooldridge, arXiv:2401.090742024arXiv preprint</p>
<p>Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, Tat-Seng Chua, arXiv:2309.05519Next-gpt: Any-to-any multimodal llm. 2023arXiv preprint</p>
<p>Autonomous chemical research with large language models. Robert Daniil A Boiko, Ben Macknight, Gabe Kline, Gomes, Nature. 62479922023</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, International Conference on Learning Representations. 2020</p>
<p>Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, Daniel S Weld, Luke Zettlemoyer, arXiv:1705.035512017arXiv preprint</p>
<p>Konstantine Arkoudas, arXiv:2308.03762Gpt-4 can't reason. 2023arXiv preprint</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. BIG bench authors. 2023</p>
<p>American invitational mathematics examination -aime. American Invitational Mathematics Examination -AIME 2024. February 2024MAA</p>
<p>Kunhao Zheng, Jesse Michael Han, Stanislas Polu, arXiv:2109.00110Minif2f: a cross-system benchmark for formal olympiad-level mathematics. 2021arXiv preprint</p>
<p>Proofnet: Autoformalizing and formally proving undergraduate-level mathematics. Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W Ayers, Dragomir Radev, Jeremy Avigad, arXiv:2302.124332023arXiv preprint</p>
<p>Executing arithmetic: Fine-tuning large language models as turing machines. Junyu Lai, Jiahe Xu, Yao Yang, Yunpeng Huang, Chun Cao, Jingwei Xu, arXiv:2410.078962024arXiv preprint</p>
<p>Hellaswag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Holistic evaluation of language models. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, arXiv:2211.091102022arXiv preprint</p>
<p>Agieval: A human-centric benchmark for evaluating foundation models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, arXiv:2304.063642023arXiv preprint</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>ToxicChat: Unveiling hidden challenges of toxicity detection in real-world user-AI conversation. Zi Lin, Zihan Wang, Yongqi Tong, Yangkun Wang, Yuxin Guo, Yujia Wang, Jingbo Shang, Findings of the Association for Computational Linguistics: EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, SingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Can large language models be an alternative to human evaluations?. Cheng- , Han Chiang, Hung-Yi Lee, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>Judging LLM-as-a-judge with MT-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>
<p>Alpacaeval: An automatic evaluator of instruction-following models. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023</p>
<p>Competition-level code generation with alphacode. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Science. 37866242022</p>
<p>Competition-level problems are effective llm evaluators. Yiming Huang, Zhenghao Lin, Xiao Liu, Yeyun Gong, Shuai Lu, Fangyu Lei, Yaobo Liang, Yelong Shen, Chen Lin, Nan Duan, arXiv:2312.021432023arXiv preprint</p>
<p>Training a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, arXiv:2204.058622022arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, ; , Ryan Lowe, Jan Leike,. 2022</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Advances in Neural Information Processing Systems. 202235</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Show your work: Scratchpads for intermediate computation with language models. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Maarten Bosma, David Luan, arXiv:2112.001142021arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Unicorn on rainbow: A universal commonsense reasoning model on a new multitask benchmark. Nicholas Lourie, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202135</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, Transactions of the Association for Computational Linguistics. 92021</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021arXiv preprint</p>
<p>Towards logiglue: A brief survey and a benchmark for analyzing logical reasoning capabilities of language models. Man Luo, Shrinidhi Kumbhar, Mihir Parmar, Neeraj Varshney, Pratyay Banerjee, Somak Aditya, Chitta Baral, arXiv:2310.008362023arXiv preprint</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Wenfei Zhou, James Coady, David Peng, Yujie Qiao, Luke Benson, arXiv:2209.00840Natural language reasoning with first-order logic. 2022arXiv preprint</p>
<p>Nisarg Patel, Mohith Kulkarni, Mihir Parmar, Aashna Budhiraja, Mutsumi Nakamura, Neeraj Varshney, Chitta Baral, arXiv:2406.17169Multi-logieval: Towards evaluating multi-step logical reasoning ability of large language models. 2024arXiv preprint</p>
<p>Towards systematic evaluation of logical reasoning ability of large language models. Mihir Parmar, Nisarg Patel, Neeraj Varshney, Mutsumi Nakamura, Man Luo, Santosh Mashetty, Arindam Mitra, Chitta Baral, arXiv:2404.155222024arXiv preprint</p>
<p>Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, arXiv:2007.081242020arXiv preprint</p>
<p>Oyvind Tafjord, Bhavana Dalvi Mishra, Peter Clark, arXiv:2012.13048Proofwriter: Generating implications, proofs, and abductive statements over natural language. 2020arXiv preprint</p>
<p>From recognition to cognition: Visual commonsense reasoning. Rowan Zellers, Yonatan Bisk, Ali Farhadi, Yejin Choi, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Next-qa: Next phase of questionanswering to explaining temporal actions. Junbin Xiao, Xindi Shang, Angela Yao, Tat-Seng Chua, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2021</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan, Advances in Neural Information Processing Systems. 202235</p>
<p>Divij Handa, Pavel Dolin, Shrinidhi Kumbhar, Tran Cao Son, Chitta Baral, arXiv:2406.04046Actionreasoningbench: Reasoning about actions with and without ramification constraints. 2024arXiv preprint</p>
<p>Axiomatic set theory. Patrick Suppes, Courier Corporation. 2012</p>
<p>Principles of mathematics. Bertrand Russell, 2020Routledge</p>
<p>Richard Phillips, Feynman , The feynman lectures on physics. 1963146No Title)</p>
<p>The game of life. John Conway, Scientific American. 223441970</p>
<p>The lean 4 theorem prover and programming language. Leonardo De, Moura , Sebastian Ullrich, Automated Deduction-CADE 28: 28th International on Automated Deduction, Virtual Event. SpringerJuly 12-15, 2021. 202128</p>
<p>On computable numbers, with an application to the entscheidungsproblem. Alan Mathison, Turing , J. of Math. 5851936</p>
<p>Tag systems and lag systems. Hao Wang, John Cocke, Marvin Minsky, Stephen A Cook, Journal of Symbolic Logic. 3621971</p>
<p>Universality of tag systems with p = 2. John Cocke, Marvin Minsky, J. ACM. 111January 1964</p>
<p>Formal reductions of the general combinatorial decision problem. Emil L Post, American journal of mathematics. 6521943</p>
<p>Vineet Hunter Lightman, Kosaraju, Harri Burda, John Edwards ; Leike, Schulman, arXiv:2305.20050Ilya Sutskever, and Karl Cobbe. Let's verify step by step. Bowen Baker, Teddy LeeJan. 2023arXiv preprint</p>
<p>Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024arXiv preprint</p>
<p>. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, arXiv:2412.1511520245 technical report. arXiv preprint</p>
<p>Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, arXiv:2409.121912024arXiv preprint</p>
<p>Qvq: To see the world with wisdom. Qwen Team, December 2024</p>
<p>Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, arXiv:2503.19786Morgane Rivière, et al. Gemma 3 technical report. 2025arXiv preprint</p>
<p>Sky-t1: Train your own o1 preview model within $450. Novasky Team, 2025</p>
<p>Gemini 2.5 pro. Google Deepmind, 2025</p>
<p>Claude 3.7 sonnet thinking. Anthropic, 2025</p>
<p>Doubao 1.5 pro. Doubao, 2025</p>
<p>Openai o1 system card. Openai, Sep. 2024. 2024/09/23</p>
<p>Transformers: Stateof-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations. the 2020 conference on empirical methods in natural language processing: system demonstrations2020</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, Ion Stoica, Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. the ACM SIGOPS 29th Symposium on Operating Systems Principles2023</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022arXiv preprint</p>
<p>Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang , Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, Tatsunori Hashimoto, arXiv:2501.19393Simple test-time scaling. 20251arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
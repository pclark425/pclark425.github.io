<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8651 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8651</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8651</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-270737760</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.18045v3.pdf" target="_blank">PharmaGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have revolutionized Natural Language Processing (NLP) by minimizing the need for complex feature engineering. However, the application of LLMs in specialized domains like biopharmaceuticals and chemistry remains largely unexplored. These fields are characterized by intricate terminologies, specialized knowledge, and a high demand for precision areas where general purpose LLMs often fall short. In this study, we introduce PharmaGPT, a suite of domain specilized LLMs with 13 billion and 70 billion parameters, specifically trained on a comprehensive corpus tailored to the Bio-Pharmaceutical and Chemical domains. Our evaluation shows that PharmaGPT surpasses existing general models on specific-domain benchmarks such as NAPLEX, demonstrating its exceptional capability in domain-specific tasks. Remarkably, this performance is achieved with a model that has only a fraction—sometimes just one-tenth—of the parameters of general-purpose large models. This advancement establishes a new benchmark for LLMs in the bio-pharmaceutical and chemical fields, addressing the existing gap in specialized language modeling. It also suggests a promising path for enhanced research and development, paving the way for more precise and effective NLP applications in these areas</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8651.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8651.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemBERTa: large-scale self-supervised pretraining for molecular property prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based, self-supervised model mentioned as an example of LLM-style models applied to chemical tasks such as molecular property prediction; cited as prior work in the chemistry-related LLM literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chemberta: large-scale self-supervised pretraining for molecular property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChemBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer (BERT-style / self-supervised)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this paper (original work trains on molecular representations; specifics not reported here).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecular property prediction / drug discovery (mentioned as an example application).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Not described here as a molecule-generation model; presented in paper as a property-prediction / representation model rather than a direct generator.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Mentioned as useful for chemistry domain tasks (property prediction) but no details on targeting or tailoring molecules are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Only cited in related-work context; this paper does not report original experiments or numerical results for ChemBERTa.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mentioned alongside other domain-specific models as evidence that LLM-style approaches can aid chemistry tasks; no direct empirical comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>No limitations of ChemBERTa are reported in this paper; the mention is high-level.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PharmaGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8651.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8651.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer-CNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer-CNN: Swiss knife for QSAR modeling and interpretation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid architecture combining transformer and convolutional components, cited here as an example of combining LLM architectures with CNNs to improve feature extraction for tasks such as therapeutic-candidate identification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transformer-CNN: Swiss knife for QSAR modeling and interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer-CNN</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer + CNN hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this paper (cited as prior work; original data sources not reported here).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>QSAR modeling, identification of potential therapeutic candidates / drug discovery (cited usage).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Described as enhancing feature extraction and interpretation for predicting molecule-related properties; not described as a direct molecule-generation method in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Framed as improving feature extraction for chemical/biological datasets; no details given here on how it enforces application-specific constraints (e.g., target binding).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Referenced to illustrate hybrid-model advantages; this paper provides no quantitative results for Transformer-CNN.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Cited to demonstrate benefits of combining architectures versus single approaches, but no empirical comparisons are provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PharmaGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8651.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8651.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Segler retrosynthesis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Planning chemical syntheses with deep neural networks and symbolic AI</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced seminal work that uses deep learning and symbolic AI to automate chemical synthesis planning (retrosynthesis), reducing trial-and-error and speeding identification of efficient synthesis routes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Planning chemical syntheses with deep neural networks and symbolic ai.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Segler et al.'s retrosynthesis models</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>deep neural networks combined with symbolic AI / planning</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this paper (original work used reaction databases; details not reproduced here).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Chemical synthesis planning / retrosynthesis (optimization of synthetic routes).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Automated planning of synthesis pathways (retrosynthetic route suggestion) using learned models and symbolic search techniques — cited as enabling automated synthesis planning.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Aimed at optimizing synthesis routes (application-specificity is route planning rather than molecular novelty assessment); no molecular-novelty metrics are given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as demonstrating that deep learning can automate planning and reduce trial-and-error; this paper does not reproduce or quantify those results.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Described qualitatively as reducing reliance on trial-and-error and expediting route discovery compared to classical manual planning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PharmaGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8651.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8651.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioBERT: a pretrained biomedical language representation for biomedical text mining</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A biomedical-domain pretrained BERT-style model cited as an example of domain-specific language models that excel at extracting scientific knowledge (e.g., predicting protein functionality) from literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Biobert: A pretrained language model for scientific text.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BioBERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>BERT-style transformer (domain-pretrained)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this paper (original BioBERT was trained on biomedical corpora; exact corpora not restated here).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Biomedical text mining, extraction of biological knowledge (e.g., protein function prediction); contextually relevant to bio-pharmaceutical research.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Used for information extraction and representation learning rather than for direct chemical generation in the context of this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Helps extract domain knowledge that can indirectly support molecule design, but no direct molecule-design evaluation is reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as an example of domain-specific pretrained models; no new results on BioBERT are presented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Used as part of the argument that domain-specific pretraining improves performance; no empirical head-to-head comparisons reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PharmaGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8651.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8651.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PharmaGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PharmaGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The domain-specialized multilingual LLMs (models with 13B and 70B parameters, with a 3B variant trained from scratch) developed in this work, trained with continued pretraining, instruction finetuning, and RLHF on a large bio-pharmaceutical and chemistry corpus; evaluated on NLP tasks (NAPLEX, translation, MMLU) but not demonstrated in this paper to directly generate novel chemical structures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PharmaGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive transformer (LLaMA-series continued pretraining, instruction finetuning, RLHF)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B (from-scratch), 13B (post-trained), 70B (post-trained)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Large domain-specific corpus totaling hundreds of billions of tokens across stages (stage 1: 153B tokens; stage 2: 43B tokens), including Web, News, Patents, Papers, Research Reports, Exams, Books, Chats, Codes, supervised instruction data, and specialized datasets (academic papers, clinical reports, NAPLEX, MedMCQA, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Primary: bio-pharmaceutical and chemistry domain NLP (question answering, translation, exam tasks, knowledge extraction). Paper suggests potential applicability to chemistry tasks (e.g., chemical property analysis, protein structure prediction, proposing compounds) in a conceptual sense but provides no direct molecule-generation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Autoregressive text generation, instruction-tuned and RLHF-refined; paper notes integration with retrieval-augmented generation (RAG) in deployed systems for grounding and hallucination reduction, but no direct method for generating chemical structures (e.g., SMILES generation or graph-based proposals) is described or evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Domain specificity enforced via continued pretraining on specialized corpora and weighted instruction finetuning with expert-verified datasets; however, no application-specific molecular optimization (e.g., target binding optimization) experiments are reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Reported NLP-oriented benchmarks: NAPLEX scores, Chinese Pharmacist Exam, BLEU for domain translation, MMLU; no chemical-generation metrics (e.g., Tanimoto similarity, synthesizability, binding affinity) are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>PharmaGPT outperforms GPT-3.5 on domain NLP benchmarks and approaches or surpasses GPT-4 on several biomedical topics; the paper does not present experiments demonstrating generation or design of novel chemicals, nor quantitative outcomes for molecule design.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared empirically to GPT-3.5 and GPT-4 on domain NLP tasks (NAPLEX, translation, MMLU). The paper cites other ML approaches for chemistry (e.g., Segler, ChemBERTa, Transformer-CNN) in related work but does not compare PharmaGPT to specialized molecular-generation methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>The paper explicitly states limitations: potential biases in training data, dependence on prompt quality, interpretability issues, and that detailed RAG/system integration and domain-extended functionalities (which might enable grounded molecule suggestions) are out of scope and deferred to future work; no direct molecule-generation validation is reported, so ability to design novel, synthesizable molecules is unproven here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PharmaGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8651.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8651.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented generation approach referenced as the mechanism by which the authors integrate extensive structured domain data with their LLM to extend capabilities, increase factual grounding, and mitigate hallucinations in deployed systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RAG (retrieval + generation framework)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>retrieval-augmented generation (retriever + autoregressive generator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not fully specified here; described as 'extensive structured domain data' from proprietary and public sources integrated at inference time via retrieval (details deferred).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Grounding LLM outputs in bio-pharmaceutical and chemical knowledge to improve accuracy and reduce hallucination; suggested as a component that could enable more trustworthy domain-specific outputs, potentially including chemical recommendations, in deployed systems.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Retrieve relevant structured documents/data and condition autoregressive generation on retrieved context; specifics of retrieval/indexing, fusion strategy, or how retrieved chemical data would be used to propose molecules are not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Intended to increase domain expertise and accuracy by grounding responses in retrieved sources; claimed to 'greatly mitigate the hallucination problem', but no chemical-design-specific evaluation is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Authors state they developed and deployed a real-world system using RAG to extend the model, which they claim significantly enhanced expertise and reduced hallucination; detailed experiments and results are deferred to future publications.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Implementation details and quantitative evaluations of RAG's impact (especially for molecule-design tasks) are not included in this paper and are to be presented in subsequent work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PharmaGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chemberta: large-scale self-supervised pretraining for molecular property prediction. <em>(Rating: 2)</em></li>
                <li>Transformer-CNN: Swiss knife for QSAR modeling and interpretation. <em>(Rating: 2)</em></li>
                <li>Planning chemical syntheses with deep neural networks and symbolic ai. <em>(Rating: 2)</em></li>
                <li>Biobert: A pretrained language model for scientific text. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8651",
    "paper_id": "paper-270737760",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "ChemBERTa",
            "name_full": "ChemBERTa: large-scale self-supervised pretraining for molecular property prediction",
            "brief_description": "A transformer-based, self-supervised model mentioned as an example of LLM-style models applied to chemical tasks such as molecular property prediction; cited as prior work in the chemistry-related LLM literature.",
            "citation_title": "Chemberta: large-scale self-supervised pretraining for molecular property prediction.",
            "mention_or_use": "mention",
            "model_name": "ChemBERTa",
            "model_type": "transformer (BERT-style / self-supervised)",
            "model_size": null,
            "training_data": "Not specified in this paper (original work trains on molecular representations; specifics not reported here).",
            "application_domain": "Molecular property prediction / drug discovery (mentioned as an example application).",
            "generation_method": "Not described here as a molecule-generation model; presented in paper as a property-prediction / representation model rather than a direct generator.",
            "novelty_of_chemicals": null,
            "application_specificity": "Mentioned as useful for chemistry domain tasks (property prediction) but no details on targeting or tailoring molecules are provided in this paper.",
            "evaluation_metrics": null,
            "results_summary": "Only cited in related-work context; this paper does not report original experiments or numerical results for ChemBERTa.",
            "comparison_to_other_methods": "Mentioned alongside other domain-specific models as evidence that LLM-style approaches can aid chemistry tasks; no direct empirical comparison in this paper.",
            "limitations_and_challenges": "No limitations of ChemBERTa are reported in this paper; the mention is high-level.",
            "uuid": "e8651.0",
            "source_info": {
                "paper_title": "PharmaGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Transformer-CNN",
            "name_full": "Transformer-CNN: Swiss knife for QSAR modeling and interpretation",
            "brief_description": "A hybrid architecture combining transformer and convolutional components, cited here as an example of combining LLM architectures with CNNs to improve feature extraction for tasks such as therapeutic-candidate identification.",
            "citation_title": "Transformer-CNN: Swiss knife for QSAR modeling and interpretation.",
            "mention_or_use": "mention",
            "model_name": "Transformer-CNN",
            "model_type": "Transformer + CNN hybrid",
            "model_size": null,
            "training_data": "Not specified in this paper (cited as prior work; original data sources not reported here).",
            "application_domain": "QSAR modeling, identification of potential therapeutic candidates / drug discovery (cited usage).",
            "generation_method": "Described as enhancing feature extraction and interpretation for predicting molecule-related properties; not described as a direct molecule-generation method in this paper.",
            "novelty_of_chemicals": null,
            "application_specificity": "Framed as improving feature extraction for chemical/biological datasets; no details given here on how it enforces application-specific constraints (e.g., target binding).",
            "evaluation_metrics": null,
            "results_summary": "Referenced to illustrate hybrid-model advantages; this paper provides no quantitative results for Transformer-CNN.",
            "comparison_to_other_methods": "Cited to demonstrate benefits of combining architectures versus single approaches, but no empirical comparisons are provided here.",
            "limitations_and_challenges": null,
            "uuid": "e8651.1",
            "source_info": {
                "paper_title": "PharmaGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Segler retrosynthesis",
            "name_full": "Planning chemical syntheses with deep neural networks and symbolic AI",
            "brief_description": "Referenced seminal work that uses deep learning and symbolic AI to automate chemical synthesis planning (retrosynthesis), reducing trial-and-error and speeding identification of efficient synthesis routes.",
            "citation_title": "Planning chemical syntheses with deep neural networks and symbolic ai.",
            "mention_or_use": "mention",
            "model_name": "Segler et al.'s retrosynthesis models",
            "model_type": "deep neural networks combined with symbolic AI / planning",
            "model_size": null,
            "training_data": "Not specified in this paper (original work used reaction databases; details not reproduced here).",
            "application_domain": "Chemical synthesis planning / retrosynthesis (optimization of synthetic routes).",
            "generation_method": "Automated planning of synthesis pathways (retrosynthetic route suggestion) using learned models and symbolic search techniques — cited as enabling automated synthesis planning.",
            "novelty_of_chemicals": null,
            "application_specificity": "Aimed at optimizing synthesis routes (application-specificity is route planning rather than molecular novelty assessment); no molecular-novelty metrics are given in this paper.",
            "evaluation_metrics": null,
            "results_summary": "Mentioned as demonstrating that deep learning can automate planning and reduce trial-and-error; this paper does not reproduce or quantify those results.",
            "comparison_to_other_methods": "Described qualitatively as reducing reliance on trial-and-error and expediting route discovery compared to classical manual planning.",
            "limitations_and_challenges": null,
            "uuid": "e8651.2",
            "source_info": {
                "paper_title": "PharmaGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "BioBERT",
            "name_full": "BioBERT: a pretrained biomedical language representation for biomedical text mining",
            "brief_description": "A biomedical-domain pretrained BERT-style model cited as an example of domain-specific language models that excel at extracting scientific knowledge (e.g., predicting protein functionality) from literature.",
            "citation_title": "Biobert: A pretrained language model for scientific text.",
            "mention_or_use": "mention",
            "model_name": "BioBERT",
            "model_type": "BERT-style transformer (domain-pretrained)",
            "model_size": null,
            "training_data": "Not specified in this paper (original BioBERT was trained on biomedical corpora; exact corpora not restated here).",
            "application_domain": "Biomedical text mining, extraction of biological knowledge (e.g., protein function prediction); contextually relevant to bio-pharmaceutical research.",
            "generation_method": "Used for information extraction and representation learning rather than for direct chemical generation in the context of this paper.",
            "novelty_of_chemicals": null,
            "application_specificity": "Helps extract domain knowledge that can indirectly support molecule design, but no direct molecule-design evaluation is reported here.",
            "evaluation_metrics": null,
            "results_summary": "Cited as an example of domain-specific pretrained models; no new results on BioBERT are presented in this paper.",
            "comparison_to_other_methods": "Used as part of the argument that domain-specific pretraining improves performance; no empirical head-to-head comparisons reported here.",
            "limitations_and_challenges": null,
            "uuid": "e8651.3",
            "source_info": {
                "paper_title": "PharmaGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "PharmaGPT",
            "name_full": "PharmaGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry",
            "brief_description": "The domain-specialized multilingual LLMs (models with 13B and 70B parameters, with a 3B variant trained from scratch) developed in this work, trained with continued pretraining, instruction finetuning, and RLHF on a large bio-pharmaceutical and chemistry corpus; evaluated on NLP tasks (NAPLEX, translation, MMLU) but not demonstrated in this paper to directly generate novel chemical structures.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "PharmaGPT",
            "model_type": "autoregressive transformer (LLaMA-series continued pretraining, instruction finetuning, RLHF)",
            "model_size": "3B (from-scratch), 13B (post-trained), 70B (post-trained)",
            "training_data": "Large domain-specific corpus totaling hundreds of billions of tokens across stages (stage 1: 153B tokens; stage 2: 43B tokens), including Web, News, Patents, Papers, Research Reports, Exams, Books, Chats, Codes, supervised instruction data, and specialized datasets (academic papers, clinical reports, NAPLEX, MedMCQA, etc.).",
            "application_domain": "Primary: bio-pharmaceutical and chemistry domain NLP (question answering, translation, exam tasks, knowledge extraction). Paper suggests potential applicability to chemistry tasks (e.g., chemical property analysis, protein structure prediction, proposing compounds) in a conceptual sense but provides no direct molecule-generation experiments.",
            "generation_method": "Autoregressive text generation, instruction-tuned and RLHF-refined; paper notes integration with retrieval-augmented generation (RAG) in deployed systems for grounding and hallucination reduction, but no direct method for generating chemical structures (e.g., SMILES generation or graph-based proposals) is described or evaluated in this paper.",
            "novelty_of_chemicals": null,
            "application_specificity": "Domain specificity enforced via continued pretraining on specialized corpora and weighted instruction finetuning with expert-verified datasets; however, no application-specific molecular optimization (e.g., target binding optimization) experiments are reported in this paper.",
            "evaluation_metrics": "Reported NLP-oriented benchmarks: NAPLEX scores, Chinese Pharmacist Exam, BLEU for domain translation, MMLU; no chemical-generation metrics (e.g., Tanimoto similarity, synthesizability, binding affinity) are provided.",
            "results_summary": "PharmaGPT outperforms GPT-3.5 on domain NLP benchmarks and approaches or surpasses GPT-4 on several biomedical topics; the paper does not present experiments demonstrating generation or design of novel chemicals, nor quantitative outcomes for molecule design.",
            "comparison_to_other_methods": "Compared empirically to GPT-3.5 and GPT-4 on domain NLP tasks (NAPLEX, translation, MMLU). The paper cites other ML approaches for chemistry (e.g., Segler, ChemBERTa, Transformer-CNN) in related work but does not compare PharmaGPT to specialized molecular-generation methods.",
            "limitations_and_challenges": "The paper explicitly states limitations: potential biases in training data, dependence on prompt quality, interpretability issues, and that detailed RAG/system integration and domain-extended functionalities (which might enable grounded molecule suggestions) are out of scope and deferred to future work; no direct molecule-generation validation is reported, so ability to design novel, synthesizable molecules is unproven here.",
            "uuid": "e8651.4",
            "source_info": {
                "paper_title": "PharmaGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation (RAG)",
            "brief_description": "A retrieval-augmented generation approach referenced as the mechanism by which the authors integrate extensive structured domain data with their LLM to extend capabilities, increase factual grounding, and mitigate hallucinations in deployed systems.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RAG (retrieval + generation framework)",
            "model_type": "retrieval-augmented generation (retriever + autoregressive generator)",
            "model_size": null,
            "training_data": "Not fully specified here; described as 'extensive structured domain data' from proprietary and public sources integrated at inference time via retrieval (details deferred).",
            "application_domain": "Grounding LLM outputs in bio-pharmaceutical and chemical knowledge to improve accuracy and reduce hallucination; suggested as a component that could enable more trustworthy domain-specific outputs, potentially including chemical recommendations, in deployed systems.",
            "generation_method": "Retrieve relevant structured documents/data and condition autoregressive generation on retrieved context; specifics of retrieval/indexing, fusion strategy, or how retrieved chemical data would be used to propose molecules are not described in this paper.",
            "novelty_of_chemicals": null,
            "application_specificity": "Intended to increase domain expertise and accuracy by grounding responses in retrieved sources; claimed to 'greatly mitigate the hallucination problem', but no chemical-design-specific evaluation is provided.",
            "evaluation_metrics": null,
            "results_summary": "Authors state they developed and deployed a real-world system using RAG to extend the model, which they claim significantly enhanced expertise and reduced hallucination; detailed experiments and results are deferred to future publications.",
            "comparison_to_other_methods": null,
            "limitations_and_challenges": "Implementation details and quantitative evaluations of RAG's impact (especially for molecule-design tasks) are not included in this paper and are to be presented in subsequent work.",
            "uuid": "e8651.5",
            "source_info": {
                "paper_title": "PharmaGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chemberta: large-scale self-supervised pretraining for molecular property prediction.",
            "rating": 2,
            "sanitized_title": "chemberta_largescale_selfsupervised_pretraining_for_molecular_property_prediction"
        },
        {
            "paper_title": "Transformer-CNN: Swiss knife for QSAR modeling and interpretation.",
            "rating": 2,
            "sanitized_title": "transformercnn_swiss_knife_for_qsar_modeling_and_interpretation"
        },
        {
            "paper_title": "Planning chemical syntheses with deep neural networks and symbolic ai.",
            "rating": 2,
            "sanitized_title": "planning_chemical_syntheses_with_deep_neural_networks_and_symbolic_ai"
        },
        {
            "paper_title": "Biobert: A pretrained language model for scientific text.",
            "rating": 2,
            "sanitized_title": "biobert_a_pretrained_language_model_for_scientific_text"
        }
    ],
    "cost": 0.016579499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>PharmaGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry</p>
<p>Linqing Chen chenlinqing@patsnap.com 
Changyang Tu PatSnap Co., LTD. Suzhou
Lin Tie, Yixin Wang, Shengjie Yang, Lu Jin, Lisha Zhang, Zhongkai Ye, Lidong PeiYufu Wang, Chaochao Wang, Jianping Lu, Cheng Sun, Yuancheng Li, Fu Bian, Jiangsu</p>
<p>Weilei Wang wangweilei@patsnap.com 
Changyang Tu PatSnap Co., LTD. Suzhou
Lin Tie, Yixin Wang, Shengjie Yang, Lu Jin, Lisha Zhang, Zhongkai Ye, Lidong PeiYufu Wang, Chaochao Wang, Jianping Lu, Cheng Sun, Yuancheng Li, Fu Bian, Jiangsu</p>
<p>Peng Zilongbai 
Changyang Tu PatSnap Co., LTD. Suzhou
Lin Tie, Yixin Wang, Shengjie Yang, Lu Jin, Lisha Zhang, Zhongkai Ye, Lidong PeiYufu Wang, Chaochao Wang, Jianping Lu, Cheng Sun, Yuancheng Li, Fu Bian, Jiangsu</p>
<p>Yan Xu 
Changyang Tu PatSnap Co., LTD. Suzhou
Lin Tie, Yixin Wang, Shengjie Yang, Lu Jin, Lisha Zhang, Zhongkai Ye, Lidong PeiYufu Wang, Chaochao Wang, Jianping Lu, Cheng Sun, Yuancheng Li, Fu Bian, Jiangsu</p>
<p>Jie Fang 
Changyang Tu PatSnap Co., LTD. Suzhou
Lin Tie, Yixin Wang, Shengjie Yang, Lu Jin, Lisha Zhang, Zhongkai Ye, Lidong PeiYufu Wang, Chaochao Wang, Jianping Lu, Cheng Sun, Yuancheng Li, Fu Bian, Jiangsu</p>
<p>Wentao Fang 
Changyang Tu PatSnap Co., LTD. Suzhou
Lin Tie, Yixin Wang, Shengjie Yang, Lu Jin, Lisha Zhang, Zhongkai Ye, Lidong PeiYufu Wang, Chaochao Wang, Jianping Lu, Cheng Sun, Yuancheng Li, Fu Bian, Jiangsu</p>
<p>Lizhi Wu 
Changyang Tu PatSnap Co., LTD. Suzhou
Lin Tie, Yixin Wang, Shengjie Yang, Lu Jin, Lisha Zhang, Zhongkai Ye, Lidong PeiYufu Wang, Chaochao Wang, Jianping Lu, Cheng Sun, Yuancheng Li, Fu Bian, Jiangsu</p>
<p>Ruiji Zhou 
Changyang Tu PatSnap Co., LTD. Suzhou
Lin Tie, Yixin Wang, Shengjie Yang, Lu Jin, Lisha Zhang, Zhongkai Ye, Lidong PeiYufu Wang, Chaochao Wang, Jianping Lu, Cheng Sun, Yuancheng Li, Fu Bian, Jiangsu</p>
<p>Yubin Zhang 
Changyang Tu PatSnap Co., LTD. Suzhou
Lin Tie, Yixin Wang, Shengjie Yang, Lu Jin, Lisha Zhang, Zhongkai Ye, Lidong PeiYufu Wang, Chaochao Wang, Jianping Lu, Cheng Sun, Yuancheng Li, Fu Bian, Jiangsu</p>
<p>Chaobo Xia 
Changyang Tu PatSnap Co., LTD. Suzhou
Lin Tie, Yixin Wang, Shengjie Yang, Lu Jin, Lisha Zhang, Zhongkai Ye, Lidong PeiYufu Wang, Chaochao Wang, Jianping Lu, Cheng Sun, Yuancheng Li, Fu Bian, Jiangsu</p>
<p>Ran Xu 
Changyang Tu PatSnap Co., LTD. Suzhou
Lin Tie, Yixin Wang, Shengjie Yang, Lu Jin, Lisha Zhang, Zhongkai Ye, Lidong PeiYufu Wang, Chaochao Wang, Jianping Lu, Cheng Sun, Yuancheng Li, Fu Bian, Jiangsu</p>
<p>Licong Hu 
Changyang Tu PatSnap Co., LTD. Suzhou
Lin Tie, Yixin Wang, Shengjie Yang, Lu Jin, Lisha Zhang, Zhongkai Ye, Lidong PeiYufu Wang, Chaochao Wang, Jianping Lu, Cheng Sun, Yuancheng Li, Fu Bian, Jiangsu</p>
<p>Qijun Xu 
Changyang Tu PatSnap Co., LTD. Suzhou
Lin Tie, Yixin Wang, Shengjie Yang, Lu Jin, Lisha Zhang, Zhongkai Ye, Lidong PeiYufu Wang, Chaochao Wang, Jianping Lu, Cheng Sun, Yuancheng Li, Fu Bian, Jiangsu</p>
<p>Haoran Cai 
Changyang Tu PatSnap Co., LTD. Suzhou
Lin Tie, Yixin Wang, Shengjie Yang, Lu Jin, Lisha Zhang, Zhongkai Ye, Lidong PeiYufu Wang, Chaochao Wang, Jianping Lu, Cheng Sun, Yuancheng Li, Fu Bian, Jiangsu</p>
<p>Jing Hua 
Changyang Tu PatSnap Co., LTD. Suzhou
Lin Tie, Yixin Wang, Shengjie Yang, Lu Jin, Lisha Zhang, Zhongkai Ye, Lidong PeiYufu Wang, Chaochao Wang, Jianping Lu, Cheng Sun, Yuancheng Li, Fu Bian, Jiangsu</p>
<p>Jin Sun 
Changyang Tu PatSnap Co., LTD. Suzhou
Lin Tie, Yixin Wang, Shengjie Yang, Lu Jin, Lisha Zhang, Zhongkai Ye, Lidong PeiYufu Wang, Chaochao Wang, Jianping Lu, Cheng Sun, Yuancheng Li, Fu Bian, Jiangsu</p>
<p>Tian Liu 
Changyang Tu PatSnap Co., LTD. Suzhou
Lin Tie, Yixin Wang, Shengjie Yang, Lu Jin, Lisha Zhang, Zhongkai Ye, Lidong PeiYufu Wang, Chaochao Wang, Jianping Lu, Cheng Sun, Yuancheng Li, Fu Bian, Jiangsu</p>
<p>Haowen Qiu 
Changyang Tu PatSnap Co., LTD. Suzhou
Lin Tie, Yixin Wang, Shengjie Yang, Lu Jin, Lisha Zhang, Zhongkai Ye, Lidong PeiYufu Wang, Chaochao Wang, Jianping Lu, Cheng Sun, Yuancheng Li, Fu Bian, Jiangsu</p>
<p>Meng Liu 
Changyang Tu PatSnap Co., LTD. Suzhou
Lin Tie, Yixin Wang, Shengjie Yang, Lu Jin, Lisha Zhang, Zhongkai Ye, Lidong PeiYufu Wang, Chaochao Wang, Jianping Lu, Cheng Sun, Yuancheng Li, Fu Bian, Jiangsu</p>
<p>Xiuwen Hu 
Changyang Tu PatSnap Co., LTD. Suzhou
Lin Tie, Yixin Wang, Shengjie Yang, Lu Jin, Lisha Zhang, Zhongkai Ye, Lidong PeiYufu Wang, Chaochao Wang, Jianping Lu, Cheng Sun, Yuancheng Li, Fu Bian, Jiangsu</p>
<p>Li 
Changyang Tu PatSnap Co., LTD. Suzhou
Lin Tie, Yixin Wang, Shengjie Yang, Lu Jin, Lisha Zhang, Zhongkai Ye, Lidong PeiYufu Wang, Chaochao Wang, Jianping Lu, Cheng Sun, Yuancheng Li, Fu Bian, Jiangsu</p>
<p>PharmaGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry
F8151C88FE6DEE96ECDEBCF1648569A4
Large language models (LLMs) have revolutionized Natural Language Processing (NLP) by minimizing the need for complex feature engineering.However, the application of LLMs in specialized domains like biopharmaceuticals and chemistry remains largely unexplored.These fields are characterized by intricate terminologies, specialized knowledge, and a high demand for precision areas where general purpose LLMs often fall short.In this study, we introduce PharmaGPT, a suite of domain specilized LLMs with 13 billion and 70 billion parameters, specifically trained on a comprehensive corpus tailored to the Bio-Pharmaceutical and Chemical domains.Our evaluation shows that PharmaGPT surpasses existing general models on specific-domain benchmarks such as NAPLEX, demonstrating its exceptional capability in domain-specific tasks.Remarkably, this performance is achieved with a model that has only a fraction-sometimes just one-tenth-of the parameters of general-purpose large models.This advancement establishes a new benchmark for LLMs in the bio-pharmaceutical and chemical fields, addressing the existing gap in specialized language modeling.It also suggests a promising path for enhanced research and development, paving the way for more precise and effective NLP applications in these areas.</p>
<p>Introduction</p>
<p>The development of Large Language Models (LLMs) has significantly transformed the landscape of natural language processing (NLP).Recent advancements, as evidenced by models such as GPT [Radford et al., 2018], have reduced the dependence on extensive feature engineering, thereby simplifying the creation of complex NLP systems [Sarzynska-Wawer et al., 2021;Howard and Ruder, 2018].These models have demonstrated remarkable capabilities in understanding and generating nuanced text based on few prompts.Despite these advances, current LLMs exhibit significant limitations when applied to specialized vertical domains, particularly in the biopharmaceutical sector.Most state-of-theart LLMs are proprietary and primarily designed for generalpurpose applications, predominantly in English.This focus has resulted in a lack of depth and precision necessary for domains requiring specialized knowledge.</p>
<p>To address these shortcomings, we introduce PharmaGPT, a suite of multilingual LLMs with configurations of 13 billion and 70 billion parameters.These models have been meticulously trained on a diverse corpus of billions of tokens, ensuring a robust linguistic foundation across multiple languages.Our evaluations on benchmarks like Naplex have demonstrated that PharmaGPT not only competes with but often surpasses existing models in specialized NLP tasks.</p>
<p>PharmaGPT is distinctively proficient in the biopharmaceutical and chemical sectors, showcasing exceptional command over specialized terminology and concepts.This proficiency is crucial, as effective domain-specific language models require a deep understanding of both general and specialized language contexts.</p>
<p>Through this work, we aim to contribute powerful models to both the academic and professional communities, fostering further innovation in the development and application of large-scale, multilingual, domain-specific language models.PharmaGPT promotes inclusivity and broadens the scope for global collaboration, pushing the boundaries of what is achievable in NLP.Despite the inclusive nature of LLM development, a significant portion of the research community remains underrepresented, primarily due to the English-centric training data prevalent in most models.While there are advancements in models processing languages like Chinese [Wang et al., 2021;Zeng et al., 2022;Kim et al., 2021], the representation of linguistic diversity is still limited.Furthermore, the accessibility and applicability of 'vertical domain LLMs' are often confined to their intended uses, particularly in critical domains such as biopharmaceuticals and chemicals.</p>
<p>In response to these challenges, this paper introduces the PharmaGPT, a domain-Specific large language models for bio-Pharmaceutical and chemistry.Our development process was rigorously structured, involving the creation of a di-arXiv:2406.18045v3[cs.CL] 9 Jul 2024</p>
<p>verse training dataset, a nuanced training objective, and an advanced engineering strategy for efficient distributed learning, detailed in Sections 2.2, 3.1, and 3.2, respectively.A comprehensive analysis of the model's performance capabilities is outlined in Section 4, demonstrating its competitive edge over existing multilingual LLMs.Our primary goal extends beyond introducing another LLM; we provide a detailed roadmap of our systematic approach to its development, aiming to set a new standard for future developments in domainspecific NLP.</p>
<p>Background</p>
<p>To provide a solid foundation for understanding the Phar-maGPT model, this section first outlines the crucial background on Large Language Models (LLMs).We discuss the evolution of LLMs, their underlying technologies, and their pivotal impact on the field of natural language processing (NLP).This exploration not only highlights the technological advancements but also sets the stage for appreciating the innovative applications of these models in various domains.Subsequently, we present an organizational overview of the PharmaGPT initiative.This overview details the project's scope, objectives, and the collaborative dynamics that have driven the development of this sophisticated, domain-specific language model.By establishing this comprehensive context, we aim to deepen the reader's understanding of the model's architecture, training methodologies, and its distinctive contributions to the fields of Bio-Pharmaceutical and Chemical research.The introduction of this model marks a significant milestone in applying advanced NLP techniques to specialized scientific domains.Through this section, we prepare the reader to fully appreciate the technical nuances and sectorspecific innovations that PharmaGPT brings to the forefront of NLP application in science.</p>
<p>Language Modeling</p>
<p>Language modeling constitutes a pivotal task in computational linguistics, aiming to quantify the likelihood of a sequence of tokens within a given text.A 'token' herein refers to a discrete unit of textual content, which can vary in granularity from words, subwords, characters, to bytes, as elucidated by Mielke[Mielke et al., 2021].This study-aligning with the prevailing paradigms in language modeling-endeavors to ascertain the joint probability distribution of tokens in a textual corpus as delineated by the following equation:
p(X ) = P (xi|x1, . . . , xi−1) = n i=1 P (xi|x1, x2, . . . , xi−1) (1)
where X represents a sequence of tokens, x i denotes the i th token in the sequence, and x &lt;i symbolizes the sequence of tokens preceding the i th token.This methodology, termed autoregressive language modeling, epitomizes the process of sequentially predicting the probability of the ensuing token, thus facilitating a more nuanced understanding and generation of natural language.This approach not only adheres to the foundational principles of probabilistic modeling but also leverages contemporary computational techniques to enhance predictive accuracy and linguistic coherence.</p>
<p>Neural Language Models The inception of pre-trained models in natural language processing marked a significant shift towards leveraging vast amounts of unlabeled data for model initialization prior to fine-tuning on task-specific datasets.These early models, exemplified by Word2Vec [Mikolov et al., 2013], GloVe [Pennington et al., 2014], andELMo [Shahbaz et al., 2019], introduced the concept of embedding words into continuous vector spaces, capturing semantic and syntactic nuances to a degree previously unattainable with discrete representations.Word2Vec and GloVe focused on static word embeddings, where each word was assigned a single vector regardless of its context, limiting their ability to address the polysemy inherent in natural language.ELMo, on the other hand, advanced the field by generating context-dependent representations, using a bidirectional LSTM architecture trained on a language modeling objective.These foundational models set the stage for subsequent advancements in pre-trained language models, paving the way for more sophisticated architectures that better capture the complexities of language through deep learning.Their introduction has not only revolutionized the approach to natural language processing tasks but also established a paradigm in which pre-training on large-scale corpora has become a cornerstone of modern NLP methodologies.</p>
<p>Neural language Models (NLMs) emerged as a sophisticated alternative to traditional n-gram models, initially proposed by [Miikkulainen and Dyer, 1991] and Schmidhuber and [Schmidhuber and Heil, 1996], with their significance further underscored by [Bengio et al., 2000] through the adoption of neural networks for predicting the probability of subsequent tokens based on preceding ones.Initial implementations predominantly utilized feed-forward neural networks, constrained by a fixed-length history window.However, a paradigm shift was heralded by the work of [Mikolov et al., 2010;Sutskever et al., 2011;Graves, 2013], who advocated for the use of recurrent neural networks (RNNs) to model token sequences, thereby markedly enhancing model performance through their ability to process variable-length sequences.The advent of the Transformer architecture by [Vaswani et al., 2017] represented a further evolution, demonstrating superior efficacy over RNNs in language modeling tasks as evidenced by subsequent studies [Radford et al., 2018;Al-Rfou et al., 2019;Kaplan et al., 2020].This has led to the Transformer architecture being adopted as the de facto standard for contemporary neural language models, owing to its unparalleled capability in capturing long-range dependencies and enabling more effective learning of context and semantics in text.</p>
<p>Advances in NLP Model Pretraining</p>
<p>Recent advancements in NLP have seen a significant shift towards utilizing transfer learning within language modeling frameworks.Transfer learning typically involves pretraining a model on a data-rich task, followed by fine-tuning it on a specific downstream task.Initially, the dominant method for pretraining involved using word vectors, as highlighted by [Mikolov et al., 2013], where vectors are trained to maximize the dot product of co-occurring words.However, pioneering studies by [Collobert et al., 2011] introduced a more holistic approach, involving the pretraining of the entire model, which has proven to yield superior results [Peters et al., 2018;Howard and Ruder, 2018;Radford et al., 2018;Devlin et al., 2018].This strategy was further advanced by the application of pretrained Transformer models [Radford et al., 2018;Devlin et al., 2018], setting a new benchmark and sparking the development of progressively enhanced models [Liu et al., 2019;Yang et al., 2019;Lewis et al., 2019;Raffel et al., 2020;Zhang et al., 2019].</p>
<p>Direct Apply Downstream Task with Pretrained Models While fine-tuning pretrained models has been effective in achieving high performance with limited labeled data, emerging research demonstrates that pretrained language models also facilitate task execution without further training.Initial observations of task-specific behavior in neural dialog models by [Vinyals and Le, 2015] led to significant breakthroughs by [Radford et al., 2019], who showed that Transformerbased models, trained on extensive web-scraped text, could adeptly handle diverse tasks.Notably, [Radford et al., 2019] observed improvements in performance correlating with increases in model scale.This insight has inspired extensive research aimed at understanding [Kaplan et al., 2020;Hoffmann et al., 2022] and leveraging [Shoeybi et al., 2019;Brown et al., 2020;Smith et al., 2022;Chowdhery et al., 2023;Rae et al., 2021;Wang et al., 2021;Zeng et al., 2021;Zhang et al., 2022] the scaling effects.A crucial aspect of this approach's success is the strategic design of "prompts," which involves crafting natural-language task descriptions and incorporating example inputs and outputs [Brown et al., 2020].This method not only clarifies the task for the model but also optimizes its performance in few-and zero-shot scenarios.</p>
<p>Applications of Language Models in Biopharmaceutical and Chemical Sciences</p>
<p>The advent and subsequent evolution of Large Language Models (LLMs) have heralded a new era in the fields of biopharmaceuticals and chemical sciences, presenting innovative methodologies for drug discovery, chemical synthesis optimization, and the elucidation of complex biological pathways.Unlike conventional computational methods, LLMs like BioBERT [Lee et al., 2020] and ChemBERTa [Chithrananda et al., 2020] excel in deciphering the nuanced lexicon of scientific literature, patents, and experimental reports, facilitating an unparalleled depth of knowledge extraction and hypothesis generation.These models, trained on extensive corpora of scientific texts, embody the capability to predict the functionality of novel proteins, to propose viable chemical compounds with desired properties, and to simulate reaction mechanisms with accuracy approaching that of human experts.Moreover, LLMs have been instrumental in parsing and synthesizing information across disparate domains, integrating data from genomics, proteomics, and chemical databases to offer holistic insights into drug-target interactions [Zeng et al., 2016].For instance, models such as Transformer-CNN [Karpov et al., 2020] demonstrate the power of combining LLM architectures with convolutional neural networks to enhance feature extraction in complex datasets, leading to breakthroughs in identifying potential therapeutic candidates against emerging pathogens.</p>
<p>Large Language Models (LLMs) have profoundly transformed the field of chemical synthesis by enhancing the prediction of reaction outcomes and optimizing synthesis pathways.The seminal work by Segler [Segler et al., 2018] highlights the capability of deep learning models to automate the planning of chemical syntheses.This advancement significantly reduces dependence on traditional trial-and-error methods, thereby expediting the identification of efficient synthesis routes.Furthermore, the integration of LLMs with robotic automation technologies marks a significant shift towards high-throughput experimental setups.In these setups, AI-driven systems not only perform synthesis tasks but also continuously refine the protocols, substantially speeding up both discovery and development processes in chemical research.The impact of LLMs is not confined to enhancing research efficiency but also extends to the democratization of scientific knowledge.By granting wider access to stateof-the-art discoveries, LLMs facilitate interdisciplinary collaborations that merge computational science with practical laboratory research.As these models advance, their adaptability and scalability are poised to open new avenues in fields such as personalized medicine and green chemistry.This underscores the transformative role of artificial intelligence in redefining the frontiers of scientific inquiry and its profound potential to shape the future of various scientific domains [Zhang et al., 2023;Nguyen et al., 2024;Wang et al., 2023b].</p>
<p>PharmaGPT Workshop</p>
<p>This section provides an in-depth overview of the Phar-maGPT initiative, detailing the project's scope, objectives, and the collaborative efforts behind the development of this advanced domain-specific language model.This comprehensive overview sets the stage for subsequent sections that explore the architecture, training methodologies, and unique contributions of PharmaGPT to bio-pharmaceutical and chemical research.By establishing this context, we aim to enrich the reader's understanding and highlight the strategic importance of this model in integrating cutting-edge natural language processing techniques with domain-specific research needs.This connection is crucial for advancing research and development in these critical scientific areas.</p>
<p>Organization of the Large Model Research Team The Large Model Research Team (LMRT) as shown in Fig 1 is at the forefront of advancing natural language processing (NLP) technologies.By employing a comprehensive and structured approach that encompasses data handling, model development, evaluation, cross-disciplinary collaboration, and domain-specific applications, LMRT is pushing the boundaries of domain-specific LLM research, innovation and application.This section provides an overview of LMRT's organizational structure, highlighting the team's focus on data integrity, advanced modeling techniques, rigorous evaluation methodologies, and the application of NLP in biomedical and chemical domains.Data At the core of LMRT's methodology is a robust data infrastructure, characterized by meticulous data preparation, ethical sourcing, stringent governance policies, advanced tooling, and in-depth analysis.The team recognizes the critical importance of high-quality, diverse, and relevant data sets in the development of powerful and responsible NLP models.By focusing on data integrity and employing state-of-the-art data handling techniques, LMRT ensures that the models are trained on reliable and representative data, laying the foundation for accurate and meaningful results.</p>
<p>Modeling LMRT's modeling framework is incorporating advanced tokenization techniques, metadata integration, support for multilingual capabilities, innovative architecture designs, efficient information retrieval methods, and versatile prompting strategies.These elements combine to enhance the models' understanding of complex language patterns and improve their applicability across various NLP tasks.The team's expertise in developing cutting-edge modeling techniques enables LMRT to create NLP models that are not only powerful but also adaptable to a wide range of real-world scenarios.</p>
<p>Evaluation Rigorous evaluation is a crucial aspect of LMRT's research process.The team employs both few-shot and zero-shot learning paradigms to assess model performance across a wide range of scenarios with limited to no task-specific training data.This approach ensures the adaptability and generalizability of the developed models, enabling them to perform effectively in novel and unseen contexts.By employing rigorous evaluation methodologies, LMRT can identify the strengths and limitations of their models, guiding further research and development efforts.Domains Specialized teams within LMRT focus on leveraging NLP advancements in the biomedical and chemical sectors, aiming to revolutionize drug discovery, patient care, and chemical synthesis through AI-driven insights and analyses.</p>
<p>Cross Areas LMRT recognizes the importance of crossdisciplinary collaboration in advancing NLP research and innovation.The team fosters strong collaborations among experts in engineering, computational linguistics, and domainspecific knowledge areas.This collaborative environment promotes the integration of NLP technologies with practical applications, enabling the development of solutions that are not only technically advanced but also relevant and impactful in real-world contexts.</p>
<p>Expanding Capacity</p>
<p>The goal of LMRT is to advance the application of LLMs in specific domains, ushering the biopharmaceutical research field into the AI era.To achieve this goal, our research cannot be limited to just training base models.Therefore, we have integrated extensive structured data from specific domains with our large model through RAG technology.We have developed and deployed a real-world system that endows our large model with numerous extended capabilities, significantly enhancing the expertise and accuracy of domain-specific knowledge while greatly mitigating the hallucination problem.Since this work does not fall within the scope of the base model discussed in this paper, we will elaborate on it in subsequent papers.</p>
<p>Ethical Considerations in the PharmaGPT Workshop</p>
<p>The deployment of the large language models in life sciences raises multiple ethical concerns.Key among these is data privacy, particularly when handling sensitive patient data essential for training such models.Ensuring data security and anonymity is crucial due to the serious implications a breach could have on individual privacy and research integrity.Additionally, the use of large language models for synthesizing new chemical entities or predicting drug interactions necessitates rigorous validation to ensure reliability.</p>
<p>Data Privacy and Security: Implement advanced encryption, access controls, and differential privacy, alongside compliance with regulations like GDPR and HIPAA, to protect sensitive user data.</p>
<p>Reliability and Validation of Predictions: Ensure rigorous validation of PharmaGPT across diverse datasets, establish continuous monitoring protocols, and maintain transparency about model limitations to prevent misuse.</p>
<p>Equitable Access: Partner with realated organizations and consider tiered pricing or open-source licensing to facilitate broader access, especially in low-resource settings.</p>
<p>These strategic measures can help mitigate risks and enhance the responsible deployment of PharmaGPT in the life sciences, ensuring safety, equity, and sustainability.</p>
<p>PharmaGPT</p>
<p>This section provides a comprehensive overview of the design principles, architecture, and ethical framework underpinning PharmaGPT.Designed to advance research and applications within the biopharmaceutical and chemical sectors, PharmaGPT utilizes innovative methods to process and generate domain-specific language effectively.Ethical Considerations Traditional methods of dataset compilation, often undervalued as mere "Data work," designed to maximize the acquisition of "high-quality" data efficiently.However, these methods frequently fail to consider the rights and needs of the data subjects, narrowly defining 'quality' solely in terms of its capacity to improve performance on downstream tasks.This approach may lead to the exclusion of content considered offensive by developers, thereby reinforcing inherent biases present in the source  materials (e.g., CommonCrawl dumps) and those introduced through the filtering processes.For example, the use of blocklists to exclude 'pornographic' content has inadvertently suppressed narratives relevant to the Bio-Pharmaceutical &amp; Chemical Domains from datasets.Additionally, the reliance on specific data sources like Reddit for corpus generation may skew models towards a US-centric perspective, further limiting their global applicability.To address these issues, our dataset curation process for the PharmaGPT has been designed to be both methodologically rigorous and ethically informed.This approach reflects our commitment to scientific excellence and social responsibility, ensuring that our model serves a broad and diverse set of global needs without perpetuating existing biases.</p>
<p>Dataset</p>
<p>Selection of Languages</p>
<p>In our previous discussions, we have repeatedly emphasized our intention to explore and leverage the specialized capabilities of large language models (LLMs) within vertical domains, specifically through our we manage and continually update a dynamic dataset that, while including generic data, primarily focuses on vertical domain-specific data.This dataset has been consistently refined and expanded through the dedicated efforts of our data team, becoming both more extensive and specialized over time.</p>
<p>Specialized Training Datasets The emphasis on vertical domain data underscores our commitment to developing a Large Language Model (LLM) with unparalleled expertise in the biomedicine and chemistry fields.This approach aligns with the findings of recent studies, which highlight the importance of domain-specific training datasets in enhancing the performance of LLMs in specialized tasks [Lee et al., 2020;Beltagy et al., 2019].By prioritizing depth and relevance in our data selection, we ensure that our model, PharmaGPT, not only advances the frontier in its capacity to understand and generate domain-specific content but also serves as a benchmark for future developments in the field.</p>
<p>Structured Agreements Central to this strategy are structured agreements between data contributors and repositories, clearly outlining permissible uses of the data.These agreements are crucial in maintaining ethical standards and legal compliance, ensuring that all data utilization respects the rights of the data subjects and rights-holders.</p>
<p>Implementation Process Despite the ambitious goal of establishing a leading domain-specific model, the time constraints from the project's inception to the commencement of model training necessitated pragmatic adaptations.Our approach was comprehensive, addressing several key aspects of data management: (i) Data Quality and Accuracy: We ensured that the data input into the model was of high quality and accuracy.This typically involved cleaning the data to remove errors and duplicates, and employing technologies to enhance the consistency and reliability of the data.</p>
<p>(ii) Data Security and Privacy: We implemented strategies and technologies to protect data from unauthorized access and breaches.This included the use of encryption technologies to safeguard data storage and transfer, as well as adherence to relevant data protection regulations, such as the General Data Protection Regulation (GDPR) of the European Union.</p>
<p>(iii) Data Access and Sharing: We established rules to determine which users and systems could access and use the data.This initiative helped ensure the proper use of data and prevented its misuse.</p>
<p>(iv) Data Storage and Archiving: We developed effective data storage strategies to support the long-term preservation and retrieval of data.This involved selecting appropriate storage solutions and regularly assessing the relevance and value of data to determine whether it should be archived or deleted.</p>
<p>(v) Compliance and Standardization: We ensured that our data management practices adhered to industry standards and regulatory requirements.This included the formulation and enforcement of policies to meet legal and ethical standards, particularly implementing special protective measures when handling sensitive information.</p>
<p>These strategies were critical in accommodating the rapid timeline from project inception to model training, ensuring that our data handling processes were efficient, secure, and compliant with international standards.</p>
<p>Data Sources and Preprocessing</p>
<p>Following the identification of data sources, the preprocessing of data entailed a series of meticulously designed steps aimed at refining and preparing the dataset for the training of PharmaGPT, a large language model specialized in biomedical and chemical domains.Specific-Domain Data To improve PharmaGPT's performance in the biopharmaceutical vertical domain, we have incorporated some more specialized data into the model.As depicted in the Fig 3, we manage and maintain a dynamic dataset, include our domain-specific dataset, such as academic papers and clinical reports.Source Data Acquisition The initial phase of preprocessing involved the acquisition of text data from a diverse array of sources.This process included downloading and extracting text from various NLP dataset formats (e.g., question answering, summarization, dialogue), scraping scientific articles from PDF archives, and preprocessing web content from hundreds catalogued websites alongside an additional sites identified by data working group members for geographic diversity.New tools were developed for extracting text from HTML in Common Crawl WARC files, enhancing the scope and efficiency of data collection.Quality Filtering : Human-Centric Text Selection.A critical step in preprocessing was the filtration of content to ensure the inclusion of high-quality, human-generated text.We defined quality based on the principle of text "written by humans for humans," emphasizing the exclusion of non-natural language elements such as preprocessing errors, SEO-driven content, and spam.A detailed framework for identifying high-quality text was developed, incorporating language-specific parameters and qualitative assessments by fluent speakers.This nuanced approach allowed for tailored filtration, ensuring the integrity and naturalness of the dataset.Deduplication and Privacy Considerations To uphold data quality and privacy, we implemented stringent deduplication and privacy measures.The process involved two layers of deduplication to remove nearly identical documents, followed by the redaction of personally identifiable information, prioritizing datasets with the highest privacy risks.Regular expression (regex)-based redaction techniques were employed, with an understanding of potential false positives, to ensure the protection of privacy without significantly compromising data utility.</p>
<p>In conclusion, the preprocessing stage of our project was conducted with a comprehensive and ethically informed approach, incorporating advanced techniques and tools to ensure the creation of a high-quality, diverse, and privacyrespecting dataset.This methodology not only facilitates the training of more accurate and reliable models but also contributes to the broader discourse on responsible AI development in critical domains.</p>
<p>Data for Instruction Finetuning and RLHF Instruction Finetuning Dataset</p>
<p>The paradigm of multitask learning, specifically through instruction-based finetun-ing, has revolutionized the efficiency and applicability of pretrained language models.This approach, known as instruction tuning or prompted finetuning, involves refining a pretrained model using a diversified set of tasks articulated through natural language prompts.A pioneering implementation of this, T0 [Sanh et al., 2021], showcased remarkable zero-shot learning capabilities across a broad array of tasks.T0, an initiative under the BigScience project, leveraged the Public Pool of Prompts (P3), a comprehensive assembly of prompts for diverse, open-source English datasets and demonstrated superior performance even compared to significantly larger, unfinetuned models.</p>
<p>Building on this foundational work, our project extends the concept of prompted finetuning into the realm of biomedicine and chemistry with the development of PharmaGPT.Drawing inspiration from T0's methodology, PharmaGPT underwent a similar multitask finetuning process, employing an enriched set of natural language datasets.This refined approach not only strengthens PharmaGPT's foundation in handling complex, domain-specific tasks but also aligns with the cuttingedge in NLP research, demonstrating a commitment to advancing the capabilities of language models within specialized fields.</p>
<p>To enable our model to achieve superior performance in the fields of biomedicine and chemistry with a smaller parameter scale compared to general-purpose LLMs, we have meticulously designed a series of targeted fine-tuning interventions as shown in Table 3.These interventions are based on a substantial dataset, which includes a large volume of high-quality data produced and verified by our team of domain experts.This strategic approach ensures that our model not only excels in specialized areas but does so with greater efficiency and precision.RLHF Dataset We collected a dataset consisting of 50,000 human preference expert-annotated instructions for reward modeling.The dataset includes prompts followed by several responses generated by our proprietary PharmaGPT models of various sizes, as well as commercial large language models (LLMs) such as GPT-4 and ChatGPT-3.5, to enhance the diversity of responses.Expert annotators ranked these responses from best to worst using standard annotation guidelines and constructed comparison pairs based on these rankings.</p>
<p>Training</p>
<p>The training of large language models (LLMs) for biomedical and chemical domains requires an exquisitely refined approach, as show in Fig 4, it integrating a blend of foundational pretraining, targeted multitask finetuning, and reinforcement learning from human feedback (RLHF).This section outlines the strategic methodologies employed, highlighting recent advancements and incorporating new, pertinent references to support our approaches.As mentioned earlier, our PharmaGPT consists of models with varying parameter sizes.Specifically, the 3B model is trained from scratch, while the 13B and 70B models undergo post-training based on the LLaMa series models.</p>
<p>We adopt this reliable technological route for the necessity for LLMs to have a broad and sufficient base of gen- eral knowledge to function effectively as a backbone for interaction capabilities.The LLaMa series models provide an excellent foundation due to their proven general performance and adaptability.In subsequent sections of this paper, we will demonstrate the significance and impact of domainspecific post-training through case studies and public benchmark tests.These examples illustrations will show how our approach not only enhances the model's performance in specific tasks but also establishes a new standard for domainspecific training in the fields of biomedicine and chemistry.</p>
<p>In this section, we detail the training process for PharmaGPT-70B, a model with a substantial parameter count of 70 billion.</p>
<p>Continue Pretraining</p>
<p>The continue pretraining phase augments the specific-domain knowledge of our large language models (LLMs), leveraging extensive corpora to learn general language representations prior to specialization.For Phar-maGPT, we utilized a diverse compilation of biomedical literature, chemical patents, and research articles.This ensured that the model acquired a comprehensive understanding of domain-specific language and concepts, which is crucial for the effectiveness of subsequent finetuning stages.Critical to this phase is the selection of pretraining corpora that are both comprehensive and of high quality, thus avoiding biases and inaccuracies that could mislead the model's learning trajectory [Devlin et al., 2018;Brown et al., 2020].</p>
<p>Prior to continue pretraining, we developed a new tokenizer using byte-pair encoding (BPE) [Shibata et al., 1999] from SentencePiece [Kudo and Richardson, 2018], based on our pretraining data.This tokenizer was then merged with the LLaMA2 tokenizer, resulting in a new tokenizer with an extended vocabulary size of 55,296.This adjustment aims to enhance token compression efficiency for Chinese text and specialized domains.The enhanced tokenizer, with an addition of 23,296 tokens compared to the LLaMA2 tokenizer, is utilized across all PharmaGPT models.To accommodate this new tokenizer, we resized the word embedding and output layers from a shape of V × H to V ′ × H, where V = 32, 000 represents the original vocabulary size and V ′ = 55, 296 denotes the new vocabulary size.New rows were appended to the end of the original embedding matrices, ensuring that the embeddings for tokens in the original vocabulary were not affected.The additional parameters were then further trained during the extended pretraining phase.</p>
<p>We employed a two-stage continue pretraining approach for the PharmaGPT models, consuming 153 billion tokens in stage 1 and 43 billion tokens in stage 2, respectively.Figure 3 illustrates the proportion of various data types used in pretraining stage relative to the total amount of corresponding pretraining data.The distribution of data categories was strategically non-random across the two stages.Stage 1 predominantly utilized data from Web, News, Patents, and Papers to instill basic knowledge into the PharmaGPT models.In stage 2, the focus shifted to extensively using data from Research Reports and Exams, along with significant portions from Books, Chats, Codes, and Supervised Data.It is important to note that, due to the significantly smaller total token count in stage 2 compared to stage 1, the relative proportions of data from Books, Chats, Exams, Codes and Research Reports were increased.This adjustment ensures that the PharmaGPT models are adequately trained in the nuanced knowledge and tasks pertinent to the bio-pharmaceutical and chemistry sectors.</p>
<p>Instruction Finetuning Following the pretraining phase, instruction finetuning adapts the Large Language Model (LLM) to perform a variety of domain-specific tasks, such as protein structure prediction, chemical property analysis, and patient data interpretation.This phase involves the integration of multiple, distinct datasets, each representing a different task within the biomedical and chemical fields.By employing natural language prompts and task-specific objectives, we guide the LLM to apply its general knowledge to these specialized tasks, thereby enhancing its versatil-ity and utility.The finetuning process is influenced by the T0 model's approach, leveraging a dataset of diverse tasks through prompted learning [Sanh et al., 2021].This strategy not only improves model performance across a broad range of tasks but also instills a deeper understanding of domainspecific challenges and nuances.</p>
<p>Inspired by [Wang et al., 2023a], we utilized a weighted autoregressive objective and zeroed out the loss on tokens from the user instructions to better align with human intentions.The loss function can be expressed as follows:
LSF T (Θ) = Ex∼D SF T −α i∈output log p (xi | x0, x1, . . . , xi−1; Θ) (2) α = 1, if x ∈ Dexp 0.1, if x ∈ Dgen(3)
In our approach, we utilize several hundred thousand generic instructions to enable the model to achieve conversational capabilities tailored to the biomedical, chemical sectors, and downstream application scenarios.Instructions with higher relevance are assigned greater weight in the training process.These methods ensure that our model exhibits differentiated and significantly enhanced performance within the domain.</p>
<p>where Θ represents the model's parameters, D SF T is the fine-tuning dataset, x = (x 0 , x 1 , . . ., x i−1 ) represents the tokenized input sequence, output dominates the tokens that belong to the output segments.RLHF Reinforcement Learning from Human Feedback (RLHF) is employed as a refinement step, further enhancing the model's performance by aligning it more closely with human judgment and ethical considerations.In this stage, the LLM is fine-tuned based on feedback from domain experts who evaluate the model's outputs on specific tasks, such as drug interaction predictions or treatment recommendation generation.feedback loop allows for the direct incorporation of expert knowledge and ethical considerations into the model, ensuring that its recommendations are not only accurate but also align with professional standards and ethical guidelines [Ouyang et al., 2022].</p>
<p>This training methodology, integrating pretraining, multitask finetuning, and RLHF, positions our LLM at the forefront of the biomedical and chemical domains.It not only equips the model with a deep understanding of complex, domainspecific content but also ensures its outputs are practical, accurate, and ethically sound.By leveraging the latest advancements in NLP and incorporating expert feedback, we aim to pave the way for LLMs that significantly contribute to research and practice in these critical fields.</p>
<p>Following the findings of Lee et al.</p>
<p>[ Bai et al., 2022], which suggest that larger RMs yield superior performance in reward modeling tasks, we utilized the pretrained PharmaGPT-70B model to initialize the parameters of our RM.This RM is employed for reinforcement learning across all subsequent PharmaGPT model iterations.To adapt the RM for its role, we aligned the end token in each sample through left-padding and enhanced the RM with two multilayer perceptrons (MLPs), enabling it to output a scalar score indicative of human preference.The RM was optimized using a binary ranking loss, defined as:
L ranking = − log (σ (r θ (x, y c ) − r θ (x, y r )))(4)
where r θ (x, y c ) represents the scalar score for a given prompt x and its corresponding annotator-preferred response y c , y r denotes the rejected response, and σ is the sigmoid function.</p>
<p>To stabilize the training of our reinforcement learning model, we employed Proximal Policy Optimization (PPO) [Schulman et al., 2017], with the reward signal provided by the RM score.During the RLHF (Reinforcement Learning from Human Feedback) training process, both the actor and reference models were initialized with the fine-tuned models described earlier, while the critic model utilized the RM.In each training step, the actor model generated four responses from a given prompt.The response yielding the highest RM score was selected to optimize the actor model.</p>
<p>Evaluation</p>
<p>In this study, we rigorously evaluate the PharmaGPT model across a spectrum of benchmark scenarios to assess its comparative effectiveness relative to existing large language models (LLMs) within the anticipated pragmatic application contexts in the biomedical and chemical domains.Our objective is to meticulously document and analyze the performance of PharmaGPT across an array of tasks, including, but not limited to, machine translation and summarization, evaluating both in zero-shot and one-shot scenarios, as well as outcomes from multitask fine-tuning.</p>
<p>To ensure the rigor and contemporaneity of our methodology, we have integrated insights and methodologies from recent literature, including advancements in few-shot learning and language model generalization techniques [Brown et al., 2020;Rae et al., 2021].These references serve as foundational pillars, guiding our evaluation strategy and providing a comprehensive framework for assessing the nuanced capabilities of PharmaGPT within our targeted domains.</p>
<p>Through this comprehensive evaluation approach, our research endeavors to establish PharmaGPT as a pivotal model that not only advances the state of the art in LLMs for the biomedical and chemical sciences but also lays a foundation for future explorations and applications in these critical fields.Our findings underscore the model's significant promise in addressing complex domain-specific challenges, thereby catalyzing further innovations in natural language processing (NLP) and domain-specific research endeavors.</p>
<p>Experimental Design</p>
<p>This section details the experimental framework designed to assess the efficacy of our Large Language Model (LLM) across a spectrum of tasks pertinent to the biomedical and chemical fields.Our selection of tasks is strategically chosen to demonstrate the model's adeptness in both understanding and generating text.These tasks encompass a variety of formats and languages, underscoring the model's versatility.Multitask Multilingual Language Understanding (MMLU) The Multitask Multilingual Language Understanding (MMLU) subset assesses the Large Language Model's (LLM) proficiency across a diverse spectrum of languages and domains.This evaluation tests the model's ability to generalize the knowledge acquired during training to new, unseen tasks presented in natural language prompts.Our aim is to benchmark our model's performance against the standards established by recent studies in MMLU assessments, thereby demonstrating its competitive edge in language understanding.Machine Translation (MT) For the Machine Translation (MT) task, we concentrate on the model's capacity to accurately translate medical and chemical texts between English and Mandarin.This task involves handling specialized vocabulary and technical expressions, which presents a significant challenge and requires advanced understanding and generation capabilities.We will benchmark our model against leading models in the field to demonstrate its efficacy and accuracy in translating complex domain-specific content.North American Pharmacist Licensure Examination (NAPLEX) The North American Pharmacist Licensure Examination (NAPLEX) component tests the model's understanding of pharmaceutical knowledge and its application in practice.To evaluate the model's performance, we simulate real-world scenarios and incorporate questions from past exams.This approach helps in assessing the model's ability to provide accurate and relevant responses, thus demonstrating its potential utility in professional pharmaceutical environments.Chinese Pharmacist Examination Similarly, we assess the model's performance on the Chinese Pharmacist Examination, focusing on its ability to comprehend and respond to questions in Mandarin.This not only tests language proficiency but also the model's understanding of pharmacological principles as applied within the Chinese healthcare system.The methodology provides a comprehensive analysis of AI applications in Chinese medical licensure exams.Consistent high performance Across all three NAPLEX sections, the PharmaGPT models achieve scores in the 70-80% range.This consistent performance demonstrates the models' strong capabilities in understanding and applying pharmaceutical knowledge in a licensing examination context.The high scores suggest that the PharmaGPT models have been effectively trained on a comprehensive corpus of pharmaceutical literature and practice materials.</p>
<p>Results</p>
<p>Superiority over GPT-3.5-turboIn all NAPLEX sections, both PharmaGPT models significantly outperform GPT-3.5turbo.This performance gap highlights the advantage of domain-specific training for pharmaceutical NLP tasks.While GPT-3.5-turbo is a powerful general-purpose language model, its lack of focused training in the pharmaceutical domain limits its ability to compete with the specialized Phar-maGPT models on the NAPLEX.</p>
<p>Iterative model improvement Comparing the two Phar-maGPT versions, PharmaGPT 0.7 consistently achieves higher scores than PharmaGPT 0.5 across all NAPLEX sections.This improvement demonstrates the value of iterative post-training, fine-tuning and optimization in enhancing the model's pharmaceutical knowledge and examination performance.As the PharmaGPT model continues to be refined, it is likely to further improve its ability to understand and respond to complex pharmaceutical questions.</p>
<p>In conclusion, the PharmaGPT models demonstrate impressive performance on the North American Pharmacist Licensure Examination, showcasing the benefits of domainspecific training in pharmaceutical NLP.Their superiority over GPT-3.5-turbo and the iterative improvements between versions highlight the potential for specialized language models to support and enhance pharmaceutical research and industry.As the PharmaGPT models continue to evolve, they hold great promise for contributing to the advancement of the pharmaceutical field as a whole.Strong domain-specific performance Across all four exam categories, both PharmaGPT models achieve scores in the 70-80% range, demonstrating robust capabilities in pharmaceutical knowledge and related fields.This consistently high performance suggests the PharmaGPT models have been effectively fine-tuned on a substantial corpus of relevant biomedical and pharmaceutical literature.</p>
<p>Chinese Pharmacist Examination</p>
<p>Outperforming GPT-3.5-turboIn all exam categories, the PharmaGPT models surpass the performance of GPT-3.5-turbo by considerable margins.This indicates that the domain-specific training of PharmaGPT translates to superior performance on pharmaceutical knowledge tests compared to the more general-purpose GPT-3.5 model.</p>
<p>Outperforming to GPT-4 in key areas Notably, the Phar-maGPT models achieve scores higher than GPT-4 in the four categories.This is impressive given GPT-4's much larger scale and suggests PharmaGPT's focused training regimen allows it to rival GPT-4's pharmaceutical knowledge without the need for vast computational resources.</p>
<p>Version comparison and improvement Comparing the two PharmaGPT versions, PharmaGPT 0.7 consistently outperforms PharmaGPT 0.5 across all categories.This indicates the iterative post-training, fine-tuning and optimization of the PharmaGPT model leads to meaningful performance gains in the pharmaceutical domain.</p>
<p>In conclusion, as shown in Fig 6, the results highlight the PharmaGPT models' strong performance on the Chinese Pharmacist Examination, showcasing their effectiveness in capturing and applying pharmaceutical knowledge.The domain-specific training approach allows PharmaGPT to surpass GPT-3.5-turbo and GPT-4 in key areas, while operating at a smaller scale.This positions PharmaGPT as a powerful tool for pharmaceutical NLP tasks, offering both high accuracy and efficiency.As the model continues to be refined, it holds great promise for supporting and advancing pharmaceutical research and applications.</p>
<p>Translation for Specified Domain In the field of natural language processing (NLP), the translation of biomedical papers poses unique challenges due to the complex terminology and domain-specific knowledge required.This section aims to compare the performance of various language models in translating biomedical papers, with our PharmaGPT0.7 model.</p>
<p>The performance of four language models -Phar-maGPT0.7,GPT3.5, CLAUDE3, and GOOGLE -was evaluated on the task of translating biomedical papers.The models were assessed at three levels of granularity: paragraph, sentence, and word.The translation quality was quantified using BLEU [Papineni et al., 2002], with higher indicating better performance.</p>
<p>PharmaGPT0.7 demonstrates a clear advantage over the other language models in translating biomedical papers.At the paragraph level, PharmaGPT0.7 achieves an impressive score of 30, outperforming GPT3.5 (27), CLAUDE3 (26), and GOOGLE (27).This trend persists at the word level, with PharmaGPT0.7 maintaining a score of 10, while GPT3.5, CLAUDE3, and GOOGLE score 8, 9, and 9, respectively.Even at the sentence level, PharmaGPT0.7 excels with a score of 18, considerably higher than GPT3.5 (15), CLAUDE3 (16).</p>
<p>The results highlight the exceptional performance of Phar-maGPT0.7 in translating biomedical papers.Its consistent lead of granularity suggests that PharmaGPT0.7 is wellsuited for capturing the nuances and complexities of biomedical language.The model's ability to maintain high translation quality at the paragraph, sentence, and word levels indicates its robustness and adaptability to various contexts within biomedical papers.</p>
<p>Several factors may contribute to PharmaGPT0.7's superior performance.First, the model's pre-training process likely involved a large corpus of biomedical texts, enabling it to acquire domain-specific knowledge and terminology.Second, the architecture and training techniques employed in PharmaGPT0.7 may be particularly effective for handling the intricacies of biomedical language.Finally, the model's ability to capture long-range dependencies and maintain coherence across larger units of text (i.e., paragraphs) may be advantageous for translating complex scientific content.</p>
<p>This comparative analysis shown in Fig 7 demonstrates the outstanding performance of PharmaGPT0.7 in translating biomedical papers.Its consistent lead across paragraph, sentence, and word levels highlights its potential as a valuable tool for researchers and professionals in the biomedical field.Further research is needed to explore the factors contributing to PharmaGPT0.7's success and to investigate its applicability to other domain-specific translation tasks.</p>
<p>Scaling Laws</p>
<p>Furthermore, empirical evidence indicating that a language model's efficacy often improves with increases in its size-exhibiting both predictable [Hestness et al., 2017;Kaplan et al., 2020;Hoffmann et al., 2022] and occasionally abrupt enhancements [Wei et al., 2022] has catalyzed a trend towards larger-scale models [Wang et al., 2021;Altaher et al., 2022;Madabushi et al., 2022;Tay et al., 2022].This observation underscores the critical relationship between model dimensions and performance, emphasizing the strategic shift towards augmenting model capacities to achieve superior outcomes.As demonstrated in the Table 4, the trend is clear: as we increase the scale of the model parameters we train, our PharmaGPT's performance on both the U.S. NAPLEX and the Chinese pharmaceutical examination datasets continues to improve, ultimately surpassing that of GPT.This enhancement in performance with increased model size underscores the potential of large language models (LLMs) in the fields of biomedicine and chemistry, highlighting PharmaGPT's capability to achieve superior outcomes in specialized domains.</p>
<p>Limitations</p>
<p>While our experimental design is comprehensive, we acknowledge several limitations.These include potential biases in the training data, the model's dependency on the quality and diversity of the input prompts, and challenges related to accurately assessing performance in highly specialized tasks without domain expert evaluation.Further, as with any LLM, there remains a question of interpretability and the implications of its use in sensitive areas such as healthcare and pharmaceuticals.</p>
<p>Conclusion</p>
<p>In this work, present PharmaGPT, a serias multilingual language model.PharmaGPT was created by our Large Language Model Team, a collaboration of dozens of researchers.In this paper, we chronicled the development of PharmaGPT, from the creation of its training dataset to the design of its architecture.We also discuss evaluation results of PharmaGPT and other large language models, finding it has competitive specific-domian performance.</p>
<p>We hope that the products build on our powerful multilingual language domain model unlocks new applications and research directions for large language models.Further, we hope that documenting our experience will help the machine learning research community organize new large-scale projects similar to PharmaGPT.Outperforming GPT-3.5-turboIn the majority of tasks, particularly those related to biology, medicine, anatomy, and physiology, PharmaGPT not only achieves higher scores than GPT-3.5-turbobut often by significant margins.This highlights the benefits of domain-specific training, which enhances PharmaGPT's performance in biomedical question answering compared to more generalized models like GPT-3.5.</p>
<p>Comparable to GPT-4 Notably, PharmaGPT scores very closely to GPT-4 on a broad spectrum of topics and slightly outperforms GPT-4 in areas such as physiology, health sciences, and biology.This achievement is particularly impressive considering GPT-4's larger scale, indicating that PharmaGPT's focused training regimen allows it to match, and occasionally surpass, GPT-4's biomedical knowledge without the need for vast computational resources.</p>
<p>Areas for Improvement Despite its strengths, PharmaGPT shows room for improvement in topics such as world religions, philosophy, and other miscellaneous areas, where it significantly lags behind GPT-4.This limitation likely reflects the scope and nature of PharmaGPT's training data.However, given the model's specialized focus, this minor underperformance in non-core subjects is understandable and does not detract from its overall effectiveness for biomedical applications.</p>
<p>The results underscore PharmaGPT's state-of-the-art performance in biomedical language understanding, positioning it as a powerful tool for domain-specific NLP tasks.Its focused training approach not only allows it to surpass GPT-3.5 but also to match, and in some cases exceed, GPT-4's capabilities in key life science and healthcare topics, all while operating on a smaller scale.This makes PharmaGPT an attractive option for biomedical researchers and organizations seeking high accuracy and efficiency in their NLP implementations.</p>
<p>Figure 1 :
1
Figure 1: Organization of the PharmaGPT Large Model Research Team.</p>
<p>Figure 2 :
2
Figure 2: Monthly increase in PharmaGPT Specified Domain training data.</p>
<p>Overview of PharmaGPT DatasetThe PharmaGPT model underwent rigorous training utilizing an expansive dataset, the composition of which is depicted in Figure2and further detailed specidic-domian data in Figure3.These figures categorize the data by type and macro-area, offering a comprehensive view of the dataset's diversity.Figure2specifically illustrates the growth of proprietary data within the bio-pharmaceutical domain for the PharmaGPT.Our data and content teams have collaborated closely to continually accumulate a considerable amount of specialized data for training the large model.Here, we provide a succinct overview of the methodology used to compile the corpus, with a deeper examination of the curation process and its outcomes elaborated upon in the subsequent sections.This approach emphasizes the significance of a meticulously assembled dataset in training specialized large language models (LLMs), as supported by literature indicating the critical role of domain-specific data in enhancing model performance[Lee et al., 2020;Beltagy et al., 2019].</p>
<p>Figure 3 :
3
Figure 3: The result of statistics on PharmaGPT dataset with text type as dimension.</p>
<p>Figure 4 :
4
Figure 4: training process.</p>
<p>Figure 5 :
5
Figure 5: Test results of ChatGPT-3.5Turboand Pharm GPTs in the North American Pharmacist Licensure Examination.</p>
<p>Figure 6 :
6
Figure 6: Test results of ChatGPT-3.5Turbo, GPT-4, and PharmaG-PTs in the Chinese Pharmacist Examination.</p>
<p>Figure 7 :
7
Figure 7: Test results of ChatGPT-3.5Turbo, CLAUDE3, GOOGLE and PharmaGPTs in the translation for biomedical papers.</p>
<p>Figure 8 :
8
Figure 8: Test Performance of PharmaGPT on the General Test MMLU.</p>
<p>Table 1 :
1
[Zhao et al., 2020;Bender et al., 2021]the biopharmaceutical field for PharmaGPT research on PharmaGPT.The diversity of languages within a large language model can potentially dilute its effectiveness; therefore, we exercised caution in selecting the languages for our training data.We primarily focused on Chinese and English, supplemented by a minimal inclusion of other widely spoken languages.This strategic selection is grounded in the hypothesis that concentrating on fewer languages enhances the model's proficiency in domain-specific tasks.This approach aligns with findings from literature suggesting that language models exhibit improved performance on specialized tasks when trained with carefully curated datasets that minimize linguistic diversity[Zhao et al., 2020;Bender et al., 2021].By focusing on Chinese and English, we aim to optimize PharmaGPT's utility in biomedicine and chemistry, sectors where these languages are predominant, thereby ensuring that our model is not only proficient but also highly relevant to its intended applications.
DatasetDatasetCode of Federal RegulationsBiorxiv &amp; MedrxivNCI Dictionary of Cancer TermsAmerican Stroke Association's International Stroke ConferenceEuropean Hematology AssociationAmerican Diabetes Association's annual Scientific SessionsNCI Dictionary of Genetics TermsAmerican Urological Association Annual meetingNCI Dictionary of DrugThe North American Pharmacist Licensure Examination (NAPLEX)MedMCQAInternational Committee on Thrombosis and Haemostasis</p>
<p>Table 2 :
2
Examples of instruction data type.
CategorySubclass</p>
<p>Table 3 :
3
Training hyperparameter settings.</p>
<p>Angel et al. [Angel et al., 2023]re ExaminationInspired by the work ofAngel et al. [Angel et al., 2023], we conducted a comparative analysis of our model, PharmaGPT, against other models using the NAPLEX exam dataset.As shown in Fig 5.This methodological approach not only benchmarks our model's capabilities in a real-world scenario but also highlights its potential in clinical and pharmaceutical applications.</p>
<p>Table 4 :
4
Results of ChatGPT-3.5Turbo, GPT-4, and PharmaGPT in the NAPLEX.
Model0.1 PharmaGPT 0.3 PharmaGPT 0.5 PharmaGPT 0.7NAPLEX I5425766NAPLEX II2.5485968NAPLEX III3.546.55876
Ethical StatementThere are no ethical issues.What are the common immunotherapy drugs combined with GP chemotherapy?PharmaGPT: GP chemotherapy (gemcitabine + cisplatin) is a commonly used chemotherapy regimen, usually for treating non-small cell lung cancer, esophageal cancer, gastric cancer, and other tumors.Due to its different mechanism of action compared to immunotherapy drugs, the clinical application of GP chemotherapy combined with immunotherapy drugs is relatively rare.However, in some cases, GP chemotherapy may be combined with immunotherapy drugs to enhance the treatment effect.The following are some possible combinations: 1. GP chemotherapy + PD-1/PD-L1 inhibitors: PD-1/PD-L1 inhibitors can enhance the immune system's ability to attack tumor cells.When used in combination with GP chemotherapy, they may produce a synergistic effect.For example, Nivolumab (Opdivo) and Pembrolizumab (Keytruda) and other PD-1 inhibitors have been approved for the treatment of various cancer types, including non-small cell lung cancer, melanoma, etc. 2. GP chemotherapy + CTLA-4 inhibitors: CTLA-4 inhibitors can prevent tumor cells from escaping immune system surveillance.When used in combination with GP chemotherapy, they may improve the treatment effect.For example, Ipilimumab (Yervoy) is a CTLA-4 inhibitor that has been approved for the treatment of metastatic melanoma.3. GP chemotherapy + VEGF inhibitors: Vascular endothelial growth factor (VEGF) inhibitors can prevent tumor cells from obtaining nutrients through neovascularization.When used in combination with GP chemotherapy, they may enhance anti-tumor activity.For example, Bevacizumab (Avastin) is a VEGF inhibitor that has been approved for the treatment of various cancer types, including colorectal cancer, non-small cell lung cancer, etc.It should be noted that due to the different conditions of each patient, the specific treatment plan should be determined by the doctor after comprehensive consideration of the patient's condition, medical history, and other factors.Table5: Case study.In this section, we detail specific examples that demonstrate the superior functionality of PharmaGPT0.7, particularly in the fields of biomedicine and oncological treatments.These instances highlight the model's advanced capabilities in handling and elucidating complex, domain-specific information, which sets a new industry standard for AI-driven data interpretation in biomedical sciences.Detailed Response Analysis of PharmaGPT0.7:The responses provided by PharmaGPT0.7 for both Chinese (ZH) and English (EN) queries about the use of GP chemotherapy in combination with immunotherapy illustrate its exceptional proficiency.GP chemotherapy, consisting of gemcitabine and cisplatin, is a regimen noted for its effectiveness against various tumors such as non-small cell lung cancer, esophageal cancer, and gastric cancer.Specialization and Depth: PharmaGPT0.7 not only identifies the typical combinations of GP chemotherapy with immunotherapy drugs but also explains the synergy between these treatments in a clinically relevant context:• PD-1/PD-L1 Inhibitors: The model correctly identifies drugs like Nivolumab and Pembrolizumab, explaining their mechanism to enhance immune system capability against tumor cells, a crucial piece of information for oncologists.• CTLA-4 Inhibitors: Detailing the role of CTLA-4 inhibitors like Ipilimumab in preventing tumor cells from evading immune surveillance, PharmaGPT0.7 provides actionable insights into their combination with chemotherapy.• VEGF Inhibitors: By mentioning Bevacizumab and its role in inhibiting neovascularization, the model showcases a deep understanding of how angiogenesis inhibitors can complement cytotoxic chemotherapy.Clinical Relevance: PharmaGPT0.7 further enhances its responses by noting that the choice of specific immunotherapy drugs alongside GP chemotherapy must be personalized, taking into consideration the patient's medical history and current health condition.This approach reflects a sophisticated understanding of oncology, where treatment customization is paramount.Comparison with Other Models: Unlike other models, PharmaGPT0.7's responses are not only rich in content but are also specifically tailored to reflect the latest trends and clinical trials in oncology.This allows healthcare professionals to receive the most current and relevant information, aiding in better decision-making processes.Conclusion: PharmaGPT0.7's detailed, accurate, and contextually relevant responses set it apart in the landscape of AIdriven tools in biomedicine, making it an invaluable resource for professionals who require up-to-date and detailed information to make informed clinical decisions.C. Comparison on the MMLU DatasetWe conducted a comparative analysis of our model, PharmaGPT, with leading Large Language Models (LLMs) such as GPT-4, using the widely recognized MMLU dataset.Our findings are twofold: firstly, PharmaGPT achieves impressive results in general capabilities despite having significantly fewer parameters than its counterparts.Secondly, it demonstrates superior proficiency in specialized domains including biomedicine, chemistry, and related fields, surpassing the advanced capabilities of general-purpose large models.Overall Strong Performance PharmaGPT consistently maintains MMLU scores in the 80-90% range across nearly all tasks, illustrating its robust capabilities in the biomedical and life sciences domains.This high performance indicates effective fine-tuning on a substantial corpus of relevant scientific literature.
Character-level language modeling with deeper self-attention. Al-Rfou , Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence2019. 201933</p>
<p>Clinical knowledge and reasoning abilities of large language models in pharmacy: A comparative study on the naplex exam. Altaher, arXiv:2208.00932arXiv:2204.058622023 Tenth International Conference on Social Networks Analysis, Management and Security (SNAMS). Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom HenighanIEEE2022. 2022. 2023. 2023. 2022arXiv preprintMasader plus: A new interface for exploring+ 500 arabic nlp datasets. Bai et al., 2022. et al. Training a helpful and harmless assistant with reinforcement learning from human feedback</p>
<p>Beltagy, arXiv:1903.10676Scibert: A pretrained language model for scientific text. 2019. 2019arXiv preprint</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. Bender, Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. the 2021 ACM conference on fairness, accountability, and transparencyBenjamin Mann, Nick Ryder, Melanie SubbiahTom Brown2021. 2021. 2000. 2000. 202013Language models are few-shot learners. Advances in neural information processing systems</p>
<p>Chemberta: large-scale self-supervised pretraining for molecular property prediction. Chithrananda, arXiv:2010.09885Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. 2020. 2020arXiv preprint</p>
<p>Natural language processing (almost) from scratch. Chowdhery, Journal of Machine Learning Research. Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa242402023. 2023. 2011Journal of machine learning research</p>
<p>Devlin, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. 2018. 2018arXiv preprint</p>
<p>Generating sequences with recurrent neural networks. Hestness Graves, arXiv:1308.0850arXiv:1712.004092013. 2013. 2017. 2017arXiv preprintDeep learning scaling is predictable</p>
<p>. Hoffmann, 2022Jordan HoffmannSebastian</p>
<p>Data governance in the age of large-scale data-driven language technology. Arthur Borgeaud, Elena Mensch, Trevor Buchatskaya, Eliza Cai, Diego Rutherford, De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, arXiv:2203.15556arXiv:1801.06146Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. the 2022 ACM Conference on Fairness, Accountability, and TransparencyHoward and Ruder2022. 2018. 2018. 2022arXiv preprintUniversal language model fine-tuning for text classification</p>
<p>Kaplan, arXiv:2001.08361Scaling laws for neural language models. 2020. 2020arXiv preprint</p>
<p>Transformer-cnn: Swiss knife for qsar modeling and interpretation. Karpov, Journal of cheminformatics. 122020. 2020</p>
<p>What changes can large-scale language models bring? intensive study on hyperclova: Billions-scale korean generative pretrained transformers. Kim, arXiv:2109.04650arXiv:1808.06226Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. Chan Ho So2021. 2021. 2018. 2018arXiv preprintLee et al., 2020</p>
<p>Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Jaewoo Kang, ; Lewis, arXiv:1910.13461Bioinformatics. 3642020. 2019. 2019arXiv preprintBiobert: a pre-trained biomedical language representation for biomedical text mining</p>
<p>Liu, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019. 2019arXiv preprint</p>
<p>Between words and characters: A brief history of open-vocabulary modeling and tokenization in nlp. Madabushi, arXiv:2204.10050arXiv:2112.10508Semeval-2022 task 2: Multilingual idiomaticity detection and sentence embedding. Martin Karafiát, Lukas Burget, Jan Cernockỳ, Sanjeev Khudanpur, Makuhari2022. 2022. 2021. 2021. 1991. 1991. 2010. 2013. 201315arXiv preprintAdvances in neural information processing systems</p>
<p>Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution. Nguyen, arXiv:1808.08949Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). the 2014 conference on empirical methods in natural language processing (EMNLP)2024. 2024. 2022. 2022. 2002. 2002. 2014. 2014. 2018. 2018. 2018. 2018. 2019. 2019369arXiv preprintLanguage models are unsupervised multitask learners</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Rae , arXiv:2112.11446Journal of machine learning research. 211402021. 2021. 2020. 2020arXiv preprintScaling language models: Methods, analysis &amp; insights from training gopher</p>
<p>Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zeroshot task generalization. Sanh, arXiv:2110.082072021. 2021arXiv preprint</p>
<p>Detecting formal thought disorder by deep contextualized word representations. Sarzynska-Wawer, arXiv:1707.06347Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. 2021. 2021. 1996. 1996. 2017. 2017304arXiv preprintPsychiatry Research</p>
<p>Planning chemical syntheses with deep neural networks and symbolic ai. Segler, Proceedings of the ACM Special Interest Group on Data Communication. the ACM Special Interest Group on Data Communication2018. 2018. 2019. 2019555Elmo: Source routed multicast for public clouds</p>
<p>Byte pair encoding: A text compression scheme that accelerates pattern matching. Shibata, arXiv:1909.08053Megatron-lm: Training multi-billion parameter language models using model parallelism. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick Legresley, Jared Casper, Bryan Catanzaro, 1999. 1999. 2019. 2019arXiv preprint</p>
<p>Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. Smith, arXiv:2201.119902022. 2022arXiv preprint</p>
<p>Transcending scaling laws with 0.1% extra compute. Sutskever, arXiv:2210.11399Proceedings of the 28th international conference on machine learning (ICML-11). Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, the 28th international conference on machine learning (ICML-11)2011. 2011. 2022. 2022. 2017. 201730arXiv preprintAdvances in neural information processing systems</p>
<p>Ernie 3.0 titan: Exploring larger-scale knowledge enhanced pretraining for language understanding and generation. Le ; Vinyals, Wang, arXiv:1506.05869arXiv:2112.12731Oriol Vinyals and Quoc Le. A neural conversational model. 2015. 2015. 2021. 2021arXiv preprint</p>
<p>Openchat: Advancing open-source language models with mixedquality data. Wang , arXiv:2309.11235bioRxiv. Wang et al., 2023b] Xi Wang, Ruichu Gu, Zhiyuan Chen, Yongge Li, Xiaohong Ji, Guolin Ke, and Han Wen2023a. 2023. 2023arXiv preprintUnirna: universal pre-trained models revolutionize rna research</p>
<p>Xlnet: Generalized autoregressive pretraining for language understanding. Wei, arXiv:2206.07682Advances in neural information processing systems. 2022. 2022. 2019. 201932arXiv preprintEmergent abilities of large language models</p>
<p>Convolutional neural network architectures for predicting dna-protein binding. Zeng, Bioinformatics. 32122016. 2016</p>
<p>Pangu-alpha: Large-scale autoregressive pretrained chinese language models with auto-parallel computation. Zeng, arXiv:2104.123692021. 2021arXiv preprint</p>
<p>Glm-130b: An open bilingual pre-trained model. Zeng, arXiv:2210.024142022. 2022arXiv preprint</p>
<p>Zhang, arXiv:1905.07129Ernie: Enhanced language representation with informative entities. 2019. 2019arXiv preprint</p>
<p>On the limitations of cross-lingual encoders as exposed by reference-free machine translation evaluation. Zhang, arXiv:2205.01068arXiv:2005.01196bioRxiv. 2022. 2022. 2023. 2023. 2020. 2020arXiv preprintDnagpt: A generalized pretrained tool for multiple dna sequence analysis tasks</p>            </div>
        </div>

    </div>
</body>
</html>
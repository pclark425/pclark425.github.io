<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-256 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-256</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-256</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-269626215</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.04550v1.pdf" target="_blank">Exploring a Cognitive Architecture for Learning Arithmetic Equations</a></p>
                <p><strong>Paper Abstract:</strong> The acquisition and performance of arithmetic skills and basic operations such as addition, subtraction, multiplication, and division are essential for daily functioning, and reflect complex cognitive processes. This paper explores the cognitive mechanisms powering arithmetic learning, presenting a neurobiologically plausible cognitive architecture that simulates the acquisition of these skills. I implement a number vectorization embedding network and an associative memory model to investigate how an intelligent system can learn and recall arithmetic equations in a manner analogous to the human brain. I perform experiments that provide insights into the generalization capabilities of connectionist models, neurological causes of dyscalculia, and the influence of network architecture on cognitive performance. Through this interdisciplinary investigation, I aim to contribute to ongoing research into the neural correlates of mathematical cognition in intelligent systems.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e256.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e256.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Associative memory network</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Connectionist associative memory network (Leabra-based)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Leabra-trained connectionist model that maps sparse distributed number embeddings and an operator code to an embedding for the result; implemented with a 3×1×3×10 input arrangement and a 30×30 hidden layer and a 3×10 output embedding layer and used to learn addition tables.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Associative memory network (Leabra)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>connectionist associative memory network (feedforward with 2D hidden layer; Leabra learning)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Addition (trained on addition tables); operator encoding supports subtraction, multiplication, division but those were not trained in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Numbers represented in embeddings for integers 0–81; experiments used addition tables for single operands (e.g., "1 + x", "2 + x") drawn from that range</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Supervised training using Leabra algorithm in Emergent; trained until PctErr = 0 for 5 epochs on training set; sequential training on new addition tables to probe catastrophic interference; neuron lesioning of hidden layer</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Reached 0% error on the training examples (fully learned training set); when trained sequentially on a new addition table, performance on the original table fell to ~100% error (i.e., nearly completely overwritten). Lesion experiments: 0% lesion -> 100% correct (132 epochs), 10% lesion -> 100% correct (157 epochs), 12% lesion -> 98.77% correct (400 epochs), 50% lesion -> 51.85% correct (400 epochs) (values from Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Operates primarily via rote associative memorization of input-output mappings over sparse distributed number embeddings; hidden representations show little overlap across different equations, explaining failure to generalize; suffers from catastrophic interference when trained sequentially (new training overwrites stored associations).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performance is highly sensitive to hidden-layer damage/size (degrades rapidly beyond ~10–12% lesion); sequential learning causes catastrophic forgetting (previously learned tables overwritten by new training). No analysis of scaling with larger model parameter counts reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Catastrophic interference on sequential learning (old memories overwritten), inability to generalize to held-out equations (failed all 20% held-out test examples), high sensitivity to lesions in hidden layer (performance collapses as lesion proportion increases).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Before vs after sequential training on new addition tables; different lesion proportions on hidden layer (0%, 10%, 12%, 50% reported).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>The associative memory successfully memorizes addition tables from SDR embeddings (0% training error) but does so via rote mappings with little abstractization, leading to no generalization and extreme catastrophic interference and fragility to modest lesions in the hidden layer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring a Cognitive Architecture for Learning Arithmetic Equations', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e256.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e256.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Numerical embedding network</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Number vectorization / sparse distributed representation (SDR) embedding network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A network that maps one-hot encodings of integers 0–81 to compact sparse distributed 30-unit embeddings (SDRs) used as inputs/outputs for the associative memory model; functions as dimensionality reduction and produces sparse activations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Numerical embedding network (SDR)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>one-hot input to SDR bottleneck layer (SDR layer smaller than input); trained using Leabra in Emergent</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>None directly — provides numeric embeddings used for arithmetic mapping (used for addition experiments downstream)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Encodes integers 0–81 into 30-unit embeddings (30 units active layer size); intended sparsity ~15% activity</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Supervised training using Leabra algorithm in Emergent; trained until PctErr = 0 for 5 epochs (reached ~40 epochs across runs); neuron lesioning of SDR layer to simulate encoding failure (dyscalculia simulation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Reached 0% error on encoding numbers 0–81 at ~40 epochs. Lesioning results (Table 1): 0% lesion -> 100% correct (38 epochs), 40% lesion -> 100% correct (74 epochs), 45% lesion -> 100% correct (200 epochs), 50% lesion -> 98.78% correct (200 epochs), 60% lesion -> 62.20% correct (200 epochs).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Encodes numbers as sparse distributed representations which enable compact, distributed numeric codes used by associative memory; provides dimensionality reduction and fixed-size embeddings so associative memory input size is constant; network-level sparsity provides fault-tolerance up to a point (tolerates ~40% lesion before degradation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Embedding layer tolerates up to ~40% neuron lesions without loss of ability to uniquely represent training numbers; beyond ~45% lesion performance rapidly degrades. No analysis of scaling with more numbers or larger embedding dimensionality reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Excessive lesioning/reduction in SDR dimensionality leads to conflation of different numbers and loss of correct encodings (performance drops beyond ~45–50% lesion).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Lesion proportion comparisons (0%, 40%, 45%, 50%, 60%) as reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>SDR embeddings can compactly and robustly represent a finite set of integers (0–81) and enable an associative memory to learn arithmetic tables, but embedding capacity is limited and collapses when dimensionality/neuron integrity falls beyond a threshold (~45–50%).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring a Cognitive Architecture for Learning Arithmetic Equations', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e256.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e256.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Leabra</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Leabra (Local, Error-driven and Associative, Biologically Realistic Algorithm)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A biologically motivated learning algorithm combining Hebbian and error-driven local learning rules; used as the training algorithm for both embedding and associative memory networks in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The leabra model of neural interactions and learning in the neocortex.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Leabra learning algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>learning algorithm / local learning rules (not a network architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Used as the training rule in Emergent to train both the numerical embedding and associative memory networks; emphasized as more biologically plausible than backpropagation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Leabra's hybrid Hebbian + error-driven local updates provide a biologically plausible mechanism for learning the SDR encodings and the associative mappings; the paper uses Leabra to argue neurobiological plausibility of the mechanisms that produced rote arithmetic memory.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Contrasted in discussion with standard backpropagation (not used) and cited as more biologically plausible.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Using Leabra, the models can learn exact arithmetic mappings (training tables) and SDR encodings, supporting a neurobiologically plausible account of rote arithmetic memorization, but Leabra-trained networks still suffer catastrophic interference and limited generalization in this setup.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring a Cognitive Architecture for Learning Arithmetic Equations', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e256.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e256.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Seq2Seq / encoder-decoder</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequence-to-sequence (encoder-decoder) generative language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned in background as an extension of neural sequence modeling (Sutskever et al., 2014) and as the architecture family employed within encoder/decoder generative language models; referenced only in passing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sequence to Sequence Learning with Neural Networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Sequence-to-sequence models (encoder-decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>encoder-decoder (sequence-to-sequence) architecture (mentioned broadly)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Mentioned as an extension of neural models to sequences and used within encoder/decoder generative language models; no arithmetic-specific interventions described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>This paper only references seq2seq/encoder-decoder language-model architectures in background; it does not analyze or report arithmetic performance of language models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring a Cognitive Architecture for Learning Arithmetic Equations', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sequence to Sequence Learning with Neural Networks <em>(Rating: 2)</em></li>
                <li>Reducing the Dimensionality of Data with Neural Networks <em>(Rating: 2)</em></li>
                <li>Efficient estimation of word representations in vector space <em>(Rating: 1)</em></li>
                <li>Catastrophic interference in connectionist networks: The sequential learning problem <em>(Rating: 2)</em></li>
                <li>Lesioning an attractor network: Investigations of acquired dyslexia <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-256",
    "paper_id": "paper-269626215",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "Associative memory network",
            "name_full": "Connectionist associative memory network (Leabra-based)",
            "brief_description": "A Leabra-trained connectionist model that maps sparse distributed number embeddings and an operator code to an embedding for the result; implemented with a 3×1×3×10 input arrangement and a 30×30 hidden layer and a 3×10 output embedding layer and used to learn addition tables.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Associative memory network (Leabra)",
            "model_size": null,
            "model_architecture": "connectionist associative memory network (feedforward with 2D hidden layer; Leabra learning)",
            "arithmetic_operation_type": "Addition (trained on addition tables); operator encoding supports subtraction, multiplication, division but those were not trained in experiments",
            "number_range_or_complexity": "Numbers represented in embeddings for integers 0–81; experiments used addition tables for single operands (e.g., \"1 + x\", \"2 + x\") drawn from that range",
            "method_or_intervention": "Supervised training using Leabra algorithm in Emergent; trained until PctErr = 0 for 5 epochs on training set; sequential training on new addition tables to probe catastrophic interference; neuron lesioning of hidden layer",
            "performance_result": "Reached 0% error on the training examples (fully learned training set); when trained sequentially on a new addition table, performance on the original table fell to ~100% error (i.e., nearly completely overwritten). Lesion experiments: 0% lesion -&gt; 100% correct (132 epochs), 10% lesion -&gt; 100% correct (157 epochs), 12% lesion -&gt; 98.77% correct (400 epochs), 50% lesion -&gt; 51.85% correct (400 epochs) (values from Table 2).",
            "mechanistic_insight": "Operates primarily via rote associative memorization of input-output mappings over sparse distributed number embeddings; hidden representations show little overlap across different equations, explaining failure to generalize; suffers from catastrophic interference when trained sequentially (new training overwrites stored associations).",
            "performance_scaling": "Performance is highly sensitive to hidden-layer damage/size (degrades rapidly beyond ~10–12% lesion); sequential learning causes catastrophic forgetting (previously learned tables overwritten by new training). No analysis of scaling with larger model parameter counts reported.",
            "failure_modes": "Catastrophic interference on sequential learning (old memories overwritten), inability to generalize to held-out equations (failed all 20% held-out test examples), high sensitivity to lesions in hidden layer (performance collapses as lesion proportion increases).",
            "comparison_baseline": "Before vs after sequential training on new addition tables; different lesion proportions on hidden layer (0%, 10%, 12%, 50% reported).",
            "key_finding": "The associative memory successfully memorizes addition tables from SDR embeddings (0% training error) but does so via rote mappings with little abstractization, leading to no generalization and extreme catastrophic interference and fragility to modest lesions in the hidden layer.",
            "uuid": "e256.0",
            "source_info": {
                "paper_title": "Exploring a Cognitive Architecture for Learning Arithmetic Equations",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Numerical embedding network",
            "name_full": "Number vectorization / sparse distributed representation (SDR) embedding network",
            "brief_description": "A network that maps one-hot encodings of integers 0–81 to compact sparse distributed 30-unit embeddings (SDRs) used as inputs/outputs for the associative memory model; functions as dimensionality reduction and produces sparse activations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Numerical embedding network (SDR)",
            "model_size": null,
            "model_architecture": "one-hot input to SDR bottleneck layer (SDR layer smaller than input); trained using Leabra in Emergent",
            "arithmetic_operation_type": "None directly — provides numeric embeddings used for arithmetic mapping (used for addition experiments downstream)",
            "number_range_or_complexity": "Encodes integers 0–81 into 30-unit embeddings (30 units active layer size); intended sparsity ~15% activity",
            "method_or_intervention": "Supervised training using Leabra algorithm in Emergent; trained until PctErr = 0 for 5 epochs (reached ~40 epochs across runs); neuron lesioning of SDR layer to simulate encoding failure (dyscalculia simulation).",
            "performance_result": "Reached 0% error on encoding numbers 0–81 at ~40 epochs. Lesioning results (Table 1): 0% lesion -&gt; 100% correct (38 epochs), 40% lesion -&gt; 100% correct (74 epochs), 45% lesion -&gt; 100% correct (200 epochs), 50% lesion -&gt; 98.78% correct (200 epochs), 60% lesion -&gt; 62.20% correct (200 epochs).",
            "mechanistic_insight": "Encodes numbers as sparse distributed representations which enable compact, distributed numeric codes used by associative memory; provides dimensionality reduction and fixed-size embeddings so associative memory input size is constant; network-level sparsity provides fault-tolerance up to a point (tolerates ~40% lesion before degradation).",
            "performance_scaling": "Embedding layer tolerates up to ~40% neuron lesions without loss of ability to uniquely represent training numbers; beyond ~45% lesion performance rapidly degrades. No analysis of scaling with more numbers or larger embedding dimensionality reported.",
            "failure_modes": "Excessive lesioning/reduction in SDR dimensionality leads to conflation of different numbers and loss of correct encodings (performance drops beyond ~45–50% lesion).",
            "comparison_baseline": "Lesion proportion comparisons (0%, 40%, 45%, 50%, 60%) as reported in Table 1.",
            "key_finding": "SDR embeddings can compactly and robustly represent a finite set of integers (0–81) and enable an associative memory to learn arithmetic tables, but embedding capacity is limited and collapses when dimensionality/neuron integrity falls beyond a threshold (~45–50%).",
            "uuid": "e256.1",
            "source_info": {
                "paper_title": "Exploring a Cognitive Architecture for Learning Arithmetic Equations",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Leabra",
            "name_full": "Leabra (Local, Error-driven and Associative, Biologically Realistic Algorithm)",
            "brief_description": "A biologically motivated learning algorithm combining Hebbian and error-driven local learning rules; used as the training algorithm for both embedding and associative memory networks in the study.",
            "citation_title": "The leabra model of neural interactions and learning in the neocortex.",
            "mention_or_use": "use",
            "model_name": "Leabra learning algorithm",
            "model_size": null,
            "model_architecture": "learning algorithm / local learning rules (not a network architecture)",
            "arithmetic_operation_type": null,
            "number_range_or_complexity": null,
            "method_or_intervention": "Used as the training rule in Emergent to train both the numerical embedding and associative memory networks; emphasized as more biologically plausible than backpropagation.",
            "performance_result": null,
            "mechanistic_insight": "Leabra's hybrid Hebbian + error-driven local updates provide a biologically plausible mechanism for learning the SDR encodings and the associative mappings; the paper uses Leabra to argue neurobiological plausibility of the mechanisms that produced rote arithmetic memory.",
            "performance_scaling": null,
            "failure_modes": null,
            "comparison_baseline": "Contrasted in discussion with standard backpropagation (not used) and cited as more biologically plausible.",
            "key_finding": "Using Leabra, the models can learn exact arithmetic mappings (training tables) and SDR encodings, supporting a neurobiologically plausible account of rote arithmetic memorization, but Leabra-trained networks still suffer catastrophic interference and limited generalization in this setup.",
            "uuid": "e256.2",
            "source_info": {
                "paper_title": "Exploring a Cognitive Architecture for Learning Arithmetic Equations",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Seq2Seq / encoder-decoder",
            "name_full": "Sequence-to-sequence (encoder-decoder) generative language models",
            "brief_description": "Mentioned in background as an extension of neural sequence modeling (Sutskever et al., 2014) and as the architecture family employed within encoder/decoder generative language models; referenced only in passing.",
            "citation_title": "Sequence to Sequence Learning with Neural Networks",
            "mention_or_use": "mention",
            "model_name": "Sequence-to-sequence models (encoder-decoder)",
            "model_size": null,
            "model_architecture": "encoder-decoder (sequence-to-sequence) architecture (mentioned broadly)",
            "arithmetic_operation_type": null,
            "number_range_or_complexity": null,
            "method_or_intervention": "Mentioned as an extension of neural models to sequences and used within encoder/decoder generative language models; no arithmetic-specific interventions described in this paper.",
            "performance_result": null,
            "mechanistic_insight": null,
            "performance_scaling": null,
            "failure_modes": null,
            "comparison_baseline": null,
            "key_finding": "This paper only references seq2seq/encoder-decoder language-model architectures in background; it does not analyze or report arithmetic performance of language models.",
            "uuid": "e256.3",
            "source_info": {
                "paper_title": "Exploring a Cognitive Architecture for Learning Arithmetic Equations",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sequence to Sequence Learning with Neural Networks",
            "rating": 2,
            "sanitized_title": "sequence_to_sequence_learning_with_neural_networks"
        },
        {
            "paper_title": "Reducing the Dimensionality of Data with Neural Networks",
            "rating": 2,
            "sanitized_title": "reducing_the_dimensionality_of_data_with_neural_networks"
        },
        {
            "paper_title": "Efficient estimation of word representations in vector space",
            "rating": 1,
            "sanitized_title": "efficient_estimation_of_word_representations_in_vector_space"
        },
        {
            "paper_title": "Catastrophic interference in connectionist networks: The sequential learning problem",
            "rating": 2,
            "sanitized_title": "catastrophic_interference_in_connectionist_networks_the_sequential_learning_problem"
        },
        {
            "paper_title": "Lesioning an attractor network: Investigations of acquired dyslexia",
            "rating": 1,
            "sanitized_title": "lesioning_an_attractor_network_investigations_of_acquired_dyslexia"
        }
    ],
    "cost": 0.0114835,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Exploring a Cognitive Architecture for Learning Arithmetic Equations
May 2024</p>
<p>Cole Gawin gawin@usc.edu 
University of Southern California</p>
<p>Exploring a Cognitive Architecture for Learning Arithmetic Equations
May 2024CBA1BBF209B4E9533A8093A3BFD07E4E
The acquisition and performance of arithmetic skills and basic operations such as addition, subtraction, multiplication, and division are essential for daily functioning, and reflect complex cognitive processes.This paper explores the cognitive mechanisms powering arithmetic learning, presenting a neurobiologically plausible cognitive architecture that simulates the acquisition of these skills.I implement a number vectorization embedding network and an associative memory model to investigate how an intelligent system can learn and recall arithmetic equations in a manner analogous to the human brain.I perform experiments that provide insights into the generalization capabilities of connectionist models, neurological causes of dyscalculia, and the influence of network architecture on cognitive performance.Through this interdisciplinary investigation, I aim to contribute to ongoing research into the neural correlates of mathematical cognition in intelligent systems.</p>
<p>Introduction</p>
<p>Understanding how humans learn to perform arithmetic is a compelling aspect of cognitive science with far-reaching implications for education, cognitive psychology, and artificial intelligence.Across cultures and ages, the ability to perform basic arithmetic computations like addition or division is essential for everyday functioning.The neurological mechanisms underlying arithmetic ability learning remain a topic of intense investigation, particularly in the context of computational models that seek to simulate and elucidate these processes.</p>
<p>One lens through which researchers analyze cognitive processes is through the development of cognitive architectures.As described by the USC Institute for Creative Technologies (n.d.), cognitive architectures are computational frameworks that attempt to capture the structure and 1 arXiv:2405.04550v1[q-bio.NC] 5 May 2024 function of the human mind, providing a theoretical basis for understanding cognitive processes such as perception, memory, language, and reasoning.These architectures serve as blueprints for building computational models that simulate human-like cognitive abilities, offering insights into the underlying mechanisms of cognition.</p>
<p>Within the realm of cognitive architectures, connectionist models 1 stand out for their ability to capture the emergent properties of complex systems through the interaction of simple processing units, analogous to neurons in the brain.By representing arithmetic knowledge as patterns of activity distributed across interconnected nodes, these models can simulate the long-term memory storage of arithmetic operations, shedding light on the underlying mechanisms that support this critical ability.</p>
<p>In this paper, I produce a neurobiologically plausible cognitive architecture for learning arithmetic operations.Specifically, I explore a hypothetical framework through which an intelligent system can encode numerical information as sparse distributed representations and memorize addition tables.I do so by implementing a novel number vectorization embedding network, as well as a connectionist associative memory model.In training such a model on arithmetic operations, I aim to elucidate the mechanisms underlying mathematical learning and cognition.</p>
<p>By systematically investigating topics such as generalization, neural substrates of dyscalculia, and the impact of network architecture, I indend to deepen the field's appreciation of how individuals acquire, represent, and utilize arithmetic knowledge.This interdisciplinary approach integrates insights from cognitive psychology, neuroscience, and artificial intelligence.I endeavor to bridge the gap between cognitive theory and computational modeling, thereby advancing both our theoretical understanding of arithmetic learning and the practical applications of connectionist models in education, psychology, and AI.</p>
<p>2 Background &amp; Literature Review</p>
<p>Knowledge Acquisition via Rote Memorization</p>
<p>Rote memorization, often characterized as the repetitive learning of information without understanding its underlying meaning, has long been a traditional method for acquiring knowledge in various domains, including mathematics.Though this approach is of intense debate regarding its neglect towards fostering deep understanding and critical thinking skills, it remains prevalent in educational settings due to its practicality in aiding recall and mastery of factual information (Brown, 2014).</p>
<p>In the context of mathematics, rote memorization plays a pivotal role in establishing a foun-1 Connectionist models are often referred to as artificial neural networks (ANNs) in the field of computer science.This term marks a comparison to biological neural networks found in the human brain.</p>
<p>dation of mathematical thinking.For instance, "skill-and-drill" instruction has long been a widely accepted methodology for the acquisition of basic arithmetic knowledge.Specifically, teachers often employ tools like addition and multiplication tables to have students memorize answers to basic equations.Since argued as the optimal technique to instill this knowledge into students' long-term memory by Thorndike (1921), this has been the primary method used by teachers in early education to teach arithmetic.</p>
<p>Supervised Learning</p>
<p>Supervised learning is a fundamental paradigm in machine learning where a model learns to map input data to corresponding output labels based on example pairs provided during training.This approach is akin to a teacher providing labeled examples to a student, allowing them to learn to associate input patterns with correct outputs.</p>
<p>In supervised learning, the goal is to approximate or learn the underlying mapping function f that relates input features X to output labels y.Mathematically, this relationship can be represented as y = f (X) + ϵ, where ϵ denotes random error.The model aims to minimize the discrepancy between its predictions and the true labels in the training data.This is done by training models through numerous epochs of training to learn from the mistakes it made on each trial.</p>
<p>The interactive activation and competition (IAC) model developed by Rumelhart and McClelland (1986) provides a cognitive perspective on supervised learning.Inspired by the architecture of the human brain, the model employs feedback mechanisms to drive learning and refine representations overtime.This process of error-driven learning mirrors the principles of supervised learning, where feedback from correct and incorrect predictions guides the adjustment of model parameters to minimize errors.</p>
<p>Connectionist Models and Associative Memory</p>
<p>Connectionist modeling has garnered significant attention in cognitive science for their ability to simulate complex cognitive processes.These models are inspired by the structure and function of the brain, consisting of interconnected nodes (neurons) that process and transmit information through weighted connections.One fundamental aspect of connectionist models is their capacity to encode and retrieve information through associative memory mechanisms.</p>
<p>Inspired by the distributed and parallel processing observed in biological neural networks, connectionist models offer a biologically plausible framework for studying cognitive phenomena (Rumelhart, McClelland, &amp; The PDP Research Group, 1986).Owing to their ability to learn from examples via trial and error and identify non-trivial patterns in information, these models can be particularly well-suited for recreating neurological tasks involving pattern recognition, learning, and memory.</p>
<p>A common implementation of connectionist models is associative memory networks.Associative memory involves forming connections between patterns or concepts, thereby enabling the retrieval of information based on learned associations.This property mirrors the functioning of human memory in the hippocampus, specifically the parietal-hippocampal network (Wagner et al., 2005), where activation of one concept can trigger the retrieval of related information.</p>
<p>Sparse Distributed Representations</p>
<p>Sparse distributed representations (SDRs) have also been a topic of interest in neural network models.In SDRs, information is represented by a binary vector where only a small fraction of the elements are active (typically around 15%), while the majority remain inactive.Despite their sparsity, SDRs can represent a large number of unique patterns due to the potential for overlap between different active bits.This is in contrast to one-hot encodings, in which each individual element of a set is represented as a single active element in a vector; one-hot encodings lack the efficiency in representations present in SDRs and suffer from high dimensionality, making them improbable in biological neural networks.Barlow (1972) suggested sparseness as a critical aspect in neural representations of stimuli-specifically, that neural systems tend to represent information using the smallest possible number of active neurons.This is present for sensory stimuli as well as encoding throughout the cerebral cortex, especially in the hippocampus.More recent research (Ahmad &amp; Hawkins, 2015) investigated the importance of SDRs in cortical encoding, and found that SDRs are crucial to higher order memory.This could explain how the hippocampus, a structure in the brain associated with memory and learning, is able to encode semantic memories through forming distributed representations that capture the relationships between concepts (McClelland et al., 1995).Save for extreme theories such as the "grandmother cell" hypothesis (Barwich, 2019), it is generally accepted that sparse distributed representations offer the most efficient and effective means of encoding information in both biological neural networks and deep learning models.</p>
<p>A prominent application of SDRs in the realm of machine learning is word vectorization techniques such as word2vec, introduced by Mikolov et al. (2013).Word embeddings have revolutionized natural language processing (NLP) tasks by encoding semantic and syntactic information into vector representations.Traditional approaches to representing words in NLP, like one-hot encodings, suffer from high dimensionality and lack of semantic relationships between words.In contrast, word embeddings capture semantic similarities between words by placing them in a continuous vector space where similar words are represented by vectors that are close in proximity.This is akin to how the hippocampus and cortex form and store connections between related memories.</p>
<p>Learning Mechanisms</p>
<p>The most commonly employed learning mechanism in ANNs is backpropagation.Backpropagation involves calculating gradients of the loss function with respect to the network's weights and then adjusting those weights accordingly to minimize the error (Rumelhart, Hinton, &amp; Williams, 1986).However, backpropagation is generally considered to be biologically implausible.2This process requires knowledge of the network's output and an explicit error signal, which is not readily available in the brain.Moreover, the precise and synchronized adjustments of weights across layers, as backpropagation entails, are not biologically realistic.The brain operates in a more distributed and parallel manner, with learning mechanisms that are inherently local and activity-dependent (O'Reilly, 1996).</p>
<p>One alternative framework that strives for greater biological plausibility is the Leabra algorithm (Local, Error-driven and Associative, Biologically Realistic Algorithm), proposed by O'Reilly (1996).Leabra aims to capture key principles of neural computation observed in the brain (O'Reilly et al., 2015).Unlike backpropagation, Leabra emphasizes local learning rules, where each neuron's synaptic weights are updated based on local information and without the need for explicit error signals.</p>
<p>In Leabra, learning occurs through a combination of Hebbian learning, which strengthens connections between neurons that are simultaneously active, and error-driven learning, which adjusts synaptic weights based on the difference between actual and expected outcomes.This hybrid approach mimics the interplay between associative learning and error correction mechanisms observed in biological systems.</p>
<p>Methodology</p>
<p>Cognitive Architecture Design</p>
<p>For this study, I developed a cognitive architecture that consists of two simuations:</p>
<p>1.A numerical embedding network that learns to develop sparse distributed representations of numbers 0-81.</p>
<ol>
<li>A associative memory model implemented with a connectionist model that learns to associate equation inputs with corresponding outputs.</li>
</ol>
<p>Both of these components necessitated the development of separate simulation models.The associative memory model is trained using inputs developed by the numerical embedding network, hence these simulations were trained sequentially.These simulations were developed using the Emergent software system (Aisa et al., 2008), which implements the Leabra learning algorithm (O'Reilly, 1996).These systems allow for the development of biologically plausible neural networks, a critical aspect of computational models of cognitive processes.are bidirectionally connected to the SDR layer.This results in a network in which, when one one-hot encoding layer is provided with a specific input, the activations in the SDR layer that result from its learned weights will activate the same input in the second one-hot encoding layer.</p>
<p>Model Implementations</p>
<p>Numerical Embedding Network</p>
<p>The activations in the SDR layer for each of the numbers the network is trained on are then considered the number's embeddings.The SDR layer being smaller than the input layers means this process is a method of dimensionality reduction.Moreover, this property allows for the weights it learns to result in sparse activations.This approach to generating embeddings in a lower-level vector space than the inputs was popularized by cognitive psychologist and AI pioneer These embeddings are used as inputs and outputs in the associative memory network, allowing a relatively small neural network to be trained on data using a large set of possible numbers.Furthermore, the network (theoretically) wouldn't have to reconfigure its size when more inputs/outputs are incorporated since the size of the embeddings is a fixed value.The input layer (the "Equation ") is a 4D layer of size 3 × 1 × 3 × 10.Both of the first two rows represent the first two operands in the equation.They consist of 30 units since that is the size of the embeddings produced by the numerical embedding network.The third row is simply a one-hot encoding of the operator: 0001 for addition, 0010 for subtraction, 0100 for multiplication, and 1000 for division. 4he hidden layer is a 2D layer of size 30×30.This is an arbitrarily large number; the magnitude of the size of this layer, rather than the exact number of units, is of most importance.As will be discussed later, the size of this layer influences the performance of the model.The magnitude of the size of this layer is crucial to allow the network to accommodate to the large training dataset.</p>
<p>Associative Memory Network</p>
<p>The output layer is a 2D layer of size 3 × 10, which again corresponds to the size of the embeddings in use.This layer simply represents the output of the equation.</p>
<p>Together, these layers model a simple associative memory network.The architecture of this model influences its ability to generalize to equations it has not been taught yet, as well as its ability to sequentially learn new equations, as will be discussed in section 5.</p>
<p>Training Procedure</p>
<p>Using a series of Python notebooks run via Google Colab, I generated training datasets in the form of .tsvfiles for both the numerical embedding network and the associative memory model.The dataset for the numerical embedding network is simply a list of 82 rows one one-hot encodings for numbers 0-81.The dataset for the associative memory model uses the embeddings generated by the numerical embedding network to represent, in each row, the two operands and the output (the operator is simply a one-hot encoding, as previously mentioned).</p>
<p>Both networks were trained until the PctErr metric reached 0 for 5 epochs.For the numerical embedding network, this was reached at around 40 epochs across multiple trial runs (see Fig. 3).</p>
<p>For the associative memory model, this was reached between 120-150 epoch across different trial runs (see Fig. 4).Because 0% error was achieved in both models, we can conclude that they both successfully learned their entire training datasets.</p>
<p>Experimental Design</p>
<p>Simulating Dyscalculia via Neuronal Lesioning</p>
<p>Dyscalculia is a neurodevelopmental disorder characterized by difficulty in understanding and processing numerical information, often resulting in impaired arithmetic abilities.Operational dyscalculia in particular is "manifested by impaired ability to perform mathematical operations, add, subtract, multiply, and divide" (Viktorin, 2023).</p>
<p>Taking inspiration from work performed by Hinton and Shallice (1991) to simulate acquired dyslexia by lesioning neurons, I aim to simulate dyscalculia through lesioning the neural network models in this cognitive architecture.I experimentally lesioned neurons in both the numerical embedding network and the associative memory model to investigate two hypothetical causes of dyscalculia: failure to properly encode numbers into sparse distributed representations, and failure to develop associations between equation inputs and outputs.By inducing controlled lesions within these models, I seek to investigate how specific disruptions to the neural network affect its ability to learn and perform arithmetic tasks, thus providing insights into the underlying mechanisms of dyscalculia.</p>
<p>Catastrophic Interference</p>
<p>Catastrophic interference is a phenomenon observed in connectionist models where the learning of new information disrupts previously learned information, leading to a degradation in performance on tasks that were previously mastered.Catastrophic interference primarily arises due to the overwriting of previously learned information when new information is learned and interference between representations (McCloskey &amp; Cohen, 1989).In the context of this study, catastrophic interference may occur when training the connectionist model sequentially on new addition and multiplication tables.</p>
<p>To investigate whether catastrophic interference would impact the associative memory model involved in this study, I conducted experiments where the connectionist model was trained on additional arithmetic tasks after mastering initial addition and multiplication tables.I tested the models on the initial dataset before and after training on additional data to investigate whether performance on these tasks diminished, thereby identifying the impact of catastrophic interference in this architecture.</p>
<p>Generalization</p>
<p>The associative memory model involved in this study was trained to learn equations from a provided dataset.While it is trivial to confirm or refute whether the model is able to successfully learn associations between equation inputs and outputs in the training data, if the model is able to generalize to tasks beyond what it has been trained on, that would suggest the model successfully "learns" to perform arithmetic beyond rote memorization of specific equations.</p>
<p>To investigate whether this model would be capable of generalization, I selectively excluded some equations from the dataset the associative memory model was trained on.I then tested the model on these excluded equations to determine whether it successfully solved them.</p>
<p>Results</p>
<p>Simulating Dyscalculia via Neuron Lesioning</p>
<p>Beginning at the entry point of the cognitive architecture, I experimented with lesioning the SDR embedding layer of the numerical embedding network to see if the model could still successfully learn to encode each number 0-81.If the model failed to do so, this would cause errors in training the associative memory model since different numbers would be confounded with each other, thereby serving as a cause of operational dyscalculia.I ran each experiment for a maximum of 200 epochs, and recorded the final percent error achieved by each size, as shown in Table 1.Intriguingly, the model still successfully learns the entire dataset even when 40% of the neurons are lesioned, implying that the embedding layer of this network could be shrunk by around 40% without losing the ability to produce SDRs for all of the training examples.However, any more reduction in dimensionality will lead to a reduction in performance, as at around 45% lesion proportion, the model quickly begins to falter.The same experiment procedires were performed on the associative memory network, except each trial run was permitted to train for a maximum of 400 epochs (see Table 2).Once again, the model quickly begins to fail to learn after more than 10% lesioning of the hidden layer.The rate at which the performance of these models begins to degrade once lesioned to a certain size demonstrates the precision required by these networks, and showcases how even relatively minimal neuronal damage can result in difficulties with arithmetic.As a result of this experiment, we can conclusively state that this cognitive architecture is heavily impacted by catastrophic interference.This suggests that the associative memory model is not entirely robust and is susceptible to the disruption or erasure of previously learned information.There was a critical threshold beyond which the models' performance deteriorated rapidly, indicating the importance of preserving the integrity of neural representations for effective arithmetic learning.</p>
<p>Catastrophic Interference</p>
<p>Generalization</p>
<p>The inability of the associative memory model to generalize to unseen data underscores the limitations of the current architecture in capturing abstract arithmetic concepts.The lack of overlap between hidden representations of different equations suggests that the model's learning is highly specific to the training data, hindering its ability to generalize to novel tasks.This limitation is consistent with findings in cognitive neuroscience, where studies have emphasized the importance of abstract representations in mathematical cognition.</p>
<p>For instance, Stanislas Dehaene, a prominent cognitive neuroscientist, has extensively studied the neural mechanisms underlying numerical cognition and arithmetic processing.Dehaene's work emphasizes the role of abstract numerical representations, such as number symbols and magnitude codes, in mathematical reasoning (Dehaene et al., 1999).According to Dehaene's triple-code model, numerical information is processed through three distinct systems in the brain: a verbal number system, a visual number form area, and a magnitude comparison network, mechanisms which research suggest may take place in the intraparietal sulcus (Dehaene et al., 2003).These systems interact to facilitate various arithmetic operations and numerical tasks beyond simply rote memorization.</p>
<p>The lack of abstract representations also may contribute to the model's susceptibility to catas- Additionally, the reliance on simplified simulations may oversimplify the complexity of realworld arithmetic learning.As previously mentioned, arithmetic learning likely involves more than associating equation inputs with outputs since no mathematical concepts are developed through this process (Dehaene et al., 1999).Future work could extend the scope of this cognitive architecture to encompass a broader range of concepts in mathematical cognition, such as magnitude comparison, to improve the model's number sense and potentially increase its ability to generalize beyond its training examples.</p>
<p>In conclusion, this current study represents a step toward understanding arithmetic learning within a connectionist framework.Continued interdisciplinary research should look to refine and extend the proposed cognitive architecture for broader applications in cognitive science and artificial intelligence.</p>
<p>Fig. 1 :
1
Fig. 1: Numerical embedding network simulation.</p>
<p>Geoffery</p>
<p>Hinton and computer scientist Russ Salakhutdinov in their seminal paper, Reducing the Dimensionality of Data with Neural Networks (2006). 3</p>
<p>Fig. 2 :
2
Fig. 2: Associative memory network simulation.</p>
<p>Fig. 3 :
3
Fig. 3: Train Epoch Plot for numerical embedding network.</p>
<p>Fig. 4 :
4
Fig. 4: Train Epoch Plot for associative memory network.</p>
<p>After training the associative memory model on the initial training set, it reached a 0% error rate when tested on these examples.However, when tested on the original examples after being trained on the new training data, the model exhibited an error rate of 100% (see Fig. 5).This means that training the model on the new training data nearly entirely overwrote the model's memory of previous training data.</p>
<p>Fig. 5 :
5
Fig. 5: Testing results on addition table for 1 following training on 2.</p>
<p>To test the model's generalization capabilities, I randomly split up the training data into training and testing datasets.20% of the entire training data was marked as testing examples, and the remaining 80% were marked as training examples.After successfully training the model on exclusively the training examples, I tested all of the testing examples to see whether the model could generalize to these examples.Unsurprisingly, the model failed all testing examples and only succeeded on training examples (see Fig. 6).Because the model fails to generalize to these examples, this suggests there is little to no overlap between the hidden representations of different equations in this current model. of Results The results of the experiments shed light on the capabilities and limitations of the proposed cognitive architecture for learning arithmetic operations.The successful training of both the numerical embedding network and the associative memory model demonstrates the feasibility of simulating arithmetic learning within a connectionist framework.However, several key findings warrant further discussion.The experiments simulating dyscalculia through neuronal lesioning empirically demonstrate the precision and intricacy involved in these neural networks-they must simultaneously strike a balance between efficient neural representations and upscaling to accomodate learning new examples.</p>
<p>Fig. 6 :
6
Fig. 6: Testing results on testing then training data.</p>
<p>trophic interference.Catastrophic interference is almost exclusively present in ANNs-the neuroplasticity of biological neural networks present in the human brain makes it much less susceptible to this shortcoming.This neuroplasticity enables the formation of abstract representations that capture the underlying principles and relationships within a domain, facilitating generalization and transfer of learning.In contrast, ANNs typically lack the same degree of flexibility and adaptability, making them more prone to catastrophic interference when exposed to new training data,5.2Future DirectionsWhile the current study provides valuable insights into the cognitive mechanisms underlying arithmetic learning, several limitations merit consideration.Firstly, this study only investigates training this architecture on addition tables.However, the scope of this training data could be trivially expanded in future work.Doing so would involve creating new training examples for the associative memory model to learn from, which may require expanding the size of the hidden layer to accommodate for the growth in the size of the training dataset.Moreover, the absence of end-to-end experiments integrating changes in the numerical embedding model with the associative memory network limits the comprehensive understanding of the architecture's performance.Future research could explore the interplay between different components of the cognitive architecture to elucidate their collective contribution to learning and memory processes.</p>
<p>Table 1 :
1
Results of lesion experiments on the numerical embedding network.
Lesion ProportionPercent Correct# of Epochs0%100%3840%100%7445%100%20050%98.78%20060%62.20%200</p>
<p>Table 2 :
2
Results of lesion experiments on the associative memory network.
Lesion ProportionPercent Correct# of Epochs0%100%13210%100%15712%98.77%40050%51.85%400</p>
<p>To test catastrophic interference within this cognitive architecture, I first trained the model on an addition table for the number 1 (i.e. 1 + 1, 1 + 2, etc.).I then tried training the model on an addition table for the number 2, thereby replicating sequential learning.To do so, I split the complete training dataset into individual training sets for each table.</p>
<p>Hinton has himself recognized this shortcoming (2007), and fellow AI forefather Yoshua Bengio and colleagues have investigated from a computer science perspective how deep learning models could be developed using biologically plausible mechanisms (2015).
Later work bySutskever et al. (2014) in Sequence to Sequence Learning with Neural Networks extended this concept to the domain of sequence-to-sequence learning, which is employed within encoder/decoder generative language models.
Note that the current model is only trained on addition tables. As discussed in section
, future work may focus on expanding the scope of the training data.</p>
<p>Properties of sparse distributed representations and their application to hierarchical temporal memory. S Ahmad, J Hawkins, ArXiv, abs/1503.074692015</p>
<p>The emergent neural modeling system. B Aisa, B Mingus, R O'reilly, Neural Networks. 200821</p>
<p>Single units and sensation: A neuron doctrine for perceptual psychology?. H B Barlow, Perception. 141972</p>
<p>The value of failure in science: The story of grandmother cells in neuroscience. A.-S Barwich, Front. Neurosci. 1311212019</p>
<p>Y Bengio, D.-H Lee, J Bornschein, Z Lin, ArXiv, abs/1502.04156Towards biologically plausible deep learning. 2015</p>
<p>Will the mathematics teaching methods used in eastern countries benefit western learners?. D Brown, Br. J. Educ. Soc. Behav. Sci. 422014</p>
<p>Sources of mathematical thinking: Behavioral and brain-imaging evidence. S Dehaene, E Spelke, P Pinel, R Stanescu, S Tsivkin, Science. 28454161999</p>
<p>Three parietal circuits for number processing. S Dehaene, M Piazza, P Pinel, L Cohen, Cogn. Neuropsychol. 2032003</p>
<p>Reducing the dimensionality of data with neural networks. G E Hinton, R R Salakhutdinov, Science. 57862006</p>
<p>How to do backpropagation in a brain. G E Hinton, 2007Invited talk at the NIPS'2007 Deep Learning Workshop</p>
<p>Lesioning an attractor network: Investigations of acquired dyslexia. G E Hinton, T Shallice, Psychol. Rev. 9811991</p>
<p>Institute for Creative Technologies. Cognitive architecture. University of Southern California. </p>
<p>Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory. J L Mcclelland, B L Mcnaughton, R C O'reilly, Psychological Review. 10231995</p>
<p>Catastrophic interference in connectionist networks: The sequential learning problem. M Mccloskey, N J Cohen, Psychology of learning and motivation. Elsevier1989</p>
<p>T Mikolov, K Chen, G S Corrado, J Dean, Efficient estimation of word representations in vector space. International Conference on Learning Representations. 2013</p>
<p>The leabra model of neural interactions and learning in the neocortex. R O'reilly, 1996, SeptemberDoctoral dissertation</p>
<p>Biologically plausible error-driven learning using local activation differences: The generalized recirculation algorithm. R C O'reilly, Neural Computation. 851996</p>
<p>The leabra cognitive architecture. R C O'reilly, T E Hazy, S A Herd, S. E. F. Chipman2015, AprilOnline Book4th Edition</p>
<p>Learning representations by backpropagating errors. D E Rumelhart, G E Hinton, R J Williams, Nature. 32360881986</p>
<p>Parallel distributed processing. D E Rumelhart, J L Mcclelland, The, Group Research, 1986. JanuaryMIT Press</p>
<p>Sequence to sequence learning with neural networks. I Sutskever, O Vinyals, Q V Le, Advances in Neural Information Processing Systems. 201427</p>
<p>The psychology of drill in arithmetic: The amount of practice. E L Thorndike, J. Educ. Psychol. 1241921</p>
<p>Specific learning disabilities. J Viktorin, Reference module in neuroscience and biobehavioral psychology. Elsevier2023</p>
<p>Parietal lobe contributions to episodic memory retrieval. A D Wagner, B J Shannon, I Kahn, R L Buckner, Trends Cogn. Sci. 992005</p>            </div>
        </div>

    </div>
</body>
</html>
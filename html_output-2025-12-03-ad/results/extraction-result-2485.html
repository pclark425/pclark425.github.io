<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2485 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2485</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2485</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-265142545</p>
                <p><strong>Paper Title:</strong> Knowledge-driven learning, optimization, and experimental design under uncertainty for materials discovery</p>
                <p><strong>Paper Abstract:</strong> Summary Significant acceleration of the future discovery of novel functional materials requires a fundamental shift from the current materials discovery practice, which is heavily dependent on trial-and-error campaigns and high-throughput screening, to one that builds on knowledge-driven advanced informatics techniques enabled by the latest advances in signal processing and machine learning. In this review, we discuss the major research issues that need to be addressed to expedite this transformation along with the salient challenges involved. We especially focus on Bayesian signal processing and machine learning schemes that are uncertainty aware and physics informed for knowledge-driven learning, robust optimization, and efficient objective-driven experimental design.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2485.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2485.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IBR-MOCU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Intrinsically Bayesian Robust operator + Mean Objective Cost of Uncertainty framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Bayesian, objective-driven framework that (1) designs robust operators under model uncertainty by minimizing expected cost across a model uncertainty class (IBR) and (2) quantifies the portion of uncertainty that degrades operational performance using MOCU to drive optimal experiment selection that reduces task-relevant uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge-driven learning, optimization, and experimental design under uncertainty for materials discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>IBR-MOCU framework</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Combines Intrinsically Bayesian Robust (IBR) operator design with the Mean Objective Cost of Uncertainty (MOCU) as an objective-aware uncertainty metric. IBR selects operators j_IBR = argmin_j E_q[C_q(j)] under a prior p(q) over model parameters q in an uncertainty class Q. MOCU(Q) = E_q[C_q(j_IBR) - C_q(j_q)] measures expected regret relative to model-specific optimal operators j_q and is used to rank candidate experiments by expected remaining MOCU after observation. The design loop: (a) encode prior knowledge as uncertainty class Q and prior p(q), (b) compute current IBR operator and MOCU, (c) evaluate candidate experiments by expected reduction in MOCU (one-step look-ahead R(Q|x) = E_x[MOCU(Q|x)]), (d) select experiment minimizing expected remaining MOCU, (e) update posterior and repeat.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials discovery and more generally experimental design for complex scientific systems under model uncertainty (examples: piezoelectrics, SMAs, Kuramoto oscillator networks)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate experiments by maximizing expected reduction in task-relevant uncertainty (minimize expected remaining MOCU). At each iteration compute for each candidate experiment x the expected post-experiment MOCU and select x that minimizes it (one-step look-ahead). The strategy inherently trades off testing costly candidates that strongly reduce operator-regret vs cheaper/less-informative ones.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Not fixed to a single scalar in the paper; cost considerations are operationalized via counting expensive queries (e.g., number of DFT simulations or high-cost TDGL evaluations) and noting model-computation complexity; computational cost is therefore measured implicitly as number of experiments/simulations and the cost of surrogate vs full forward model evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Mean Objective Cost of Uncertainty (MOCU) — expected regret reduction for the operational objective; contrasted with generic entropy or predictive variance measures.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Implicit via the MOCU criterion: experiments are chosen to reduce objective-relevant uncertainty (exploration when uncertainty impacts the objective, exploitation when candidate is promising and reduces expected regret). This produces a principled task-centered balance rather than a separate exploration/exploitation hyperparameter.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity-promoting penalty is mandated by the framework; diversity arises indirectly because experiments that reduce MOCU may target different regions of model space. The framework can be instantiated with search strategies (exhaustive over a finite pool, dynamic programming / knowledge gradient, gradient-based search, sampling or evolutionary algorithms) that may incorporate diversity constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of experiments / computational budget (implicit), and cost-per-experiment when evaluating expensive forward models (e.g., DFT, TDGL).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>The method explicitly optimizes expected remaining MOCU under the current posterior and can be applied iteratively until the experiment budget is exhausted; optimization over candidate experiments can be done by exhaustive search for pool-based problems, dynamic programming (KG-like), gradient-based methods for continuous spaces, or heuristic search when computation of MOCU is expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not a separate novelty score; breakthroughs are operationally recognized when the chosen design reaches the true-model-specific optimal action (MOCU → 0) or when objective values (e.g., material properties) exceed previous bests; MOCU reduction is a proxy for increasing probability of identifying breakthrough designs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics are task-specific: (a) SMA TDGL surrogate: average number of iterations to find optimal dopant/concentration (MOCU-based OED found optimum after 2 iterations on average over 10,000 simulations); (b) Kuramoto synchronization: speed/amount of MOCU reduction compared to random and entropy-based selection (qualitative improvement shown); (c) in general: reduction of expected regret (MOCU) and faster attainment of objectives measured in fewer expensive experiments/simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Random selection, pure exploitation (greedy best predicted objective), entropy-based uncertainty selection, and sometimes knowledge-gradient/EI policies (when mathematically comparable).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Qualitative and quantitative improvements reported: SMA example — MOCU-based OED identified optimum after 2 iterations on average vs exploitation/random which did not find optimum after 10 iterations; Kuramoto example — MOCU sharply reduces objective-relevant uncertainty faster than random or entropy-based strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>In SMA example, an effective reduction from >10 iterations (exploitation/random failure) to ~2 iterations on average to find the optimum (interpreted as a substantial reduction in number of expensive experiments). Other examples report accelerated convergence to Pareto front (MAX phases) and faster MOCU decline (Kuramoto), but exact percentages beyond the SMA example are not numerically stated.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>The paper contrasts MOCU (objective-aware uncertainty) with entropy and predictive variance, arguing that entropy may not align with the operational objective and so can waste budget reducing irrelevant uncertainty. It discusses minimax vs Bayesian robust strategies and risk measures (CVaR) but does not present a formal multi-objective cost vs diversity analysis; rather, it emphasizes that MOCU quantifies the uncertainty that practically matters and thus yields better experiment allocation under cost constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Optimal allocation should target experiments that minimize expected remaining MOCU rather than those that maximize generic information measures (entropy) or immediate improvement alone. When experiments are expensive, allocating to those with greatest expected objective-relevant uncertainty reduction yields faster attainment of operational goals; implementation can use exhaustive pool search, KG/dynamic programming, gradient-based optimization, or heuristic search depending on design space structure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-driven learning, optimization, and experimental design under uncertainty for materials discovery', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2485.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2485.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BMA+BO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Model Averaging combined with Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-fusion-driven autonomous experimental design approach that embeds Bayesian model averaging into a Bayesian optimization loop (GP surrogates and acquisition functions) to both learn ensemble predictive models and select expensive experiments adaptively for materials discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge-driven learning, optimization, and experimental design under uncertainty for materials discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bayesian Model Averaging + Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Maintains an ensemble of surrogate predictive models (e.g., Gaussian processes with different feature sets or kernels) and computes posterior model probabilities P(i|D). The predictive distribution at x is a mixture P(f|x,D)=Σ_i P(i|D) P(f|x,D,m_i). Acquisition functions (MOCU-based utility, Expected Improvement (EI), or Expected Hypervolume Improvement (EHVI) for multiobjective) are computed from the mixture predictive and used to select the next experiment. Model posteriors are updated after each experiment, enabling autoselection and weighting of models during the design loop.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Computational and experimental materials discovery (demonstrated on MAX phase search for mechanical properties), multiobjective materials optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates expensive computational/experimental evaluations by maximizing an acquisition function computed from the BMA predictive mixture (e.g., MOCU-utility, EI, EHVI). This implicitly balances querying locations with high expected performance (exploitation) and regions where model uncertainty across ensemble members indicates high expected information value (exploration).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of expensive simulations/experiments (e.g., DFT evaluations) and time to fit/update GP hyperparameters and compute acquisition over the design pool; cost of ensemble evaluation is higher than single-model BO because acquisition integrates over multiple models.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Uses mixture predictive uncertainty via BMA to compute utility; can use MOCU-based expected regret reduction or standard BO utilities (EI, EHVI) computed from the weighted Gaussian predictive distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration/exploitation arises through the acquisition function computed on the BMA predictive: regions with high expected improvement or high expected regret reduction are favored; as posterior concentrates on better models, exploitation dominates in promising regions while exploration continues in areas with model disagreement.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity regularizer beyond ensemble-induced exploration: diversity of hypotheses is maintained by keeping multiple models in the ensemble with posterior weights that evolve based on data, which can implicitly promote diverse candidate selection when model disagreement is high.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed experiment/computation budget (e.g., number of DFT evaluations) and per-evaluation cost variability across information sources (multi-information-source context mentioned).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Design loop optimizes acquisition per iteration until budget exhausted; can include multi-information-source optimization to choose the information source with best expected utility per cost. Ensemble averaging allows robust performance even when some models are poor, reducing wasted budget on wrong-feature choices.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Task-specific: e.g., attaining maximum bulk modulus or points on Pareto front for multiobjective problems; success operationalized as reaching high objective values or Pareto-optimal designs within few iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>MAX phases example: experiments averaged over 1,500 runs initialized from 10 training samples; BMA-based methods showed robust performance (higher average max bulk modulus over iterations), posterior concentration to best feature set, and approach to Pareto front in small number of sequential iterations. Exact numeric curves are in figures; reported dataset: 1,500 DFT-calculated designs and 10 initial samples per run.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Single GP surrogates trained on individual feature sets (six feature sets), worst/best single-feature-set surrogates, random search, and standard BO policies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>BMA achieved robust performance across experiments, outperforming poor individual feature-set surrogates and matching or exceeding the best single-feature surrogate as more iterations accrue. Qualitatively, BMA reduced the number of experiments needed to approach Pareto front compared with baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Reported as considerable reduction in number of experiments/computations needed to identify desired solutions in the MAX-phase design problem (no single aggregate percentage given); demonstrated faster approach to Pareto front and more reliable selection of high-performing candidates compared to using a single poor feature set.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper discusses tradeoffs from using BMA: computational overhead of maintaining multiple models vs robustness to model-misspecification and reduced risk of selecting poor features; suggests that BMA+BO is particularly beneficial when prior knowledge about best feature set/model is weak and per-evaluation costs are high.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>When model/form uncertainty is significant, allocating resources guided by ensemble predictions (BMA) leads to more robust experiment choices and fewer wasted expensive evaluations; using posterior model probabilities to weight predictions improves acquisition utility estimation and thus resource allocation compared to committing to a single surrogate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-driven learning, optimization, and experimental design under uncertainty for materials discovery', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2485.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2485.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MOCU-based OED (applications)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mean Objective Cost of Uncertainty-driven Optimal Experimental Design</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An experimental design policy that selects experiments by minimizing expected remaining MOCU to reduce objective-relevant uncertainty; demonstrated on Kuramoto synchronization, shape-memory alloy (SMA) dopant search, and others.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge-driven learning, optimization, and experimental design under uncertainty for materials discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MOCU-based Optimal Experimental Design</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Given an uncertainty class Q over system models and an operational cost C_q(j), compute current MOCU and for each candidate experiment x compute conditional MOCU(Q|x) and expected remaining MOCU R(Q|x)=E_x[MOCU(Q|x)]. Select experiment x* that minimizes expected remaining MOCU. Implementation strategies vary by problem: exhaustive pool search for finite candidate sets, dynamic programming / knowledge-gradient for structured problems, gradient-based local search for continuous design spaces, or sampling/genetic algorithms when MOCU computations are expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Experimental design for materials problems (SMA dopant selection, MAX phases computational screening), control/synchronization of dynamical systems (Kuramoto oscillator network), and biological network intervention examples in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Directly allocates limited experimental budget to experiments that maximize expected reduction in objective-regret (MOCU). In pool-based scenarios, chooses highest-scoring candidate per iteration; in continuous spaces uses gradient/sampling approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Counts of expensive experiments/simulations (e.g., DFT runs, TDGL oracle evaluations), and internal computational cost to estimate MOCU (Monte Carlo sampling over model posterior and computing IBR operators); cost of MOCU evaluation itself can be significant and is handled with approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected reduction in MOCU (task-focused information gain about objective-relevant model parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Unified in MOCU: selecting experiments that reduce expected regret may choose exploratory experiments (if current regret is driven by uncertain regions) or exploitative ones (if promising high-performance designs both improve the objective and reduce regret); thus policy is adaptive based on current posterior and objective sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity metric; breadth of exploration follows from where objective-relevant uncertainty is concentrated (i.e., if multiple hypothesis modes contribute to regret, experiments will be diverse to discriminate them).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of experiments / simulation budget; high per-experiment cost situations emphasized.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Plan experiments sequentially to minimize cumulative expected remaining MOCU until budget depletion; use computational approximations and surrogate models to reduce per-iteration cost when forward-model evaluations are costly.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Success measured by reaching optimal design or significant objective improvement faster (e.g., attaining minimal dissipation in SMA or achieving Pareto-optimal points), and by reduction of MOCU toward zero.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>SMA case: average iterations to find optimal dopant/concentration = 2 (averaged over 10,000 simulations); Kuramoto case: faster MOCU decrease vs random/entropy policies (figures show sharp reduction but no absolute numeric count in text).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Random selection, entropy-based selection, pure exploitation policies, and standard BO (EI/KG) when applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>MOCU-based OED outperformed random and entropy-based strategies in objective-relevant uncertainty reduction and found SMA optimum in far fewer iterations than baselines; qualitative superior performance in Kuramoto synchronization example.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Illustrative: reduction from >10 iterations (baseline failure) to ~2 iterations (MOCU) in SMA experiments; other domains show faster convergence but no single aggregate metric provided.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Emphasizes that objective-aware measures (MOCU) avoid wasting budget reducing irrelevant uncertainty (as can occur with entropy). Paper contrasts minimax and Bayesian robust strategies and notes possibility of incorporating different risk metrics (e.g., CVaR) to tune conservativeness.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Optimal experimental allocation under objective-centric goals is to minimize expected remaining MOCU; exhaustive search or dynamic programming may be used depending on design-space structure, and surrogate/sampling approximations are practical necessities when forward models are expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-driven learning, optimization, and experimental design under uncertainty for materials discovery', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2485.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2485.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Physics-constrained AFE (DQN-FGT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Physics-constrained Automatic Feature Engineering with Deep Q-Network guided Feature Generation Tree</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interpretable, scalable automatic feature engineering system that builds a constrained feature-generation tree (FGT) and uses a DQN policy with experience replay to select algebraic operations (guided by physics constraints) to create compact, predictive descriptors under data scarcity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge-driven learning, optimization, and experimental design under uncertainty for materials discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Physics-constrained AFE (DQN-guided FGT exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructs a Feature Generation Tree (FGT) where nodes represent sets of descriptors and edges represent algebraic operations (predefined operation set O informed by physics). A deep Q-network (DQN) learns a generation policy p that chooses operations to expand nodes. Rewards reflect predictive improvement (accuracy) penalized by complexity; batch sampling of candidate combinations controls combinatorial blow-up. Prior physics knowledge is injected as constraints on permitted operations or feature groups. The learned policy sequentially constructs top-d descriptors for model interpretability and efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials science descriptor discovery, symbolic/interpretable model construction, interatomic potential functional-form search (demonstrated for copper interatomic potential and other materials tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates computational effort in descriptor generation/search by (a) prioritizing operations with larger expected reward (predicted improvement per complexity), (b) batch sampling to limit combinatorial expansion per iteration, and (c) using physics constraints to prune operation choices—thus trading computation for predicted information/value of features.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Wall-clock CPU time for descriptor search and model fitting (example: copper potential search achieved MAE in 12 hours); also complexity metric c counting number of algebraic operations in generated descriptor which acts as a proxy for model complexity and computational expense.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Reward function: R(F_i, op) = max_{F0 subset} (1.001 - A_L(F0; y)) - 1, where A_L is normalized predictive accuracy (0..1); reward approximates expected predictive gain per generated descriptor, implicitly an information-reward tradeoff accounting for complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>DQN policy approximates Q-values of operations (expected future reward), balancing exploration via epsilon-greedy or experience replay components and exploitation by selecting high-Q operations; batch sampling allows tunable depth/breadth exploration to manage computational budget.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Batch sampling of feature subspaces and selection of top-n features per node yield diverse candidate descriptors; physics constraints prevent irrelevant or unphysical operations, promoting diversity among physically plausible hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Computational time and combinatorial complexity (memory) constraints; complexity limit c_max on descriptor operations used as an explicit budget for feature complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Controls search breadth via batch sampling size, top-n selection per node, maximal allowed descriptor complexity c_max, and DQN-guided prioritization to focus CPU time on most promising descriptor branches.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Descriptor predictive performance improvements (e.g., reduced MAE on held-out energies) and interpretability (compact analytic forms); in interatomic potential example, lower MAE is proxy for breakthrough functional forms.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Copper interatomic potential example: AFE-generated descriptor model achieved MAE = 3.73 meV/atom in 12 hours; comparative genetic-programming model (GP1) reported MAE = 4.13 meV/atom after 360 CPU hours on same data split.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Genetic programming-based symbolic regression (Hernandez et al. GP1), brute-force deep feature synthesis, other automated FE tools (Autolearn, Cognito) mentioned as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>AFE achieved lower MAE (3.73 vs 4.13 meV/atom) and vastly reduced runtime (12 hours vs 360 CPU hours) compared to the reported GP1 symbolic-regression baseline on the same dataset and split.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Substantial computational-efficiency improvement reported: from 360 CPU hours (GP1) to 12 hours (AFE) for comparable or better predictive accuracy (≈10% lower MAE); generation/search reduced memory/combinatorial costs via batch sampling and DQN guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper explicitly frames a performance-complexity tradeoff controlled by descriptor complexity c and batch sampling size; physics constraints reduce search space and avoid overfitting on small datasets, prioritizing interpretable and low-complexity descriptors that give good predictive returns per computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Efficient descriptor discovery under constrained compute should prioritize physics-plausible operations and use learned policies (DQN) to approximate expected value-of-information for candidate operations; limiting descriptor complexity and batch sampling provides a controllable tradeoff between search cost and predictive gain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-driven learning, optimization, and experimental design under uncertainty for materials discovery', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2485.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2485.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BO with GP + EI/EHVI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Optimization with Gaussian Process surrogates and acquisition functions (Expected Improvement / Expected Hypervolume Improvement)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard Bayesian optimization approach using GP surrogates to model expensive black-box objectives and acquisition functions (EI for single objective, EHVI for multiobjective) to select next queries; used here as special cases and baselines and embedded inside BMA or compared to MOCU-based policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge-driven learning, optimization, and experimental design under uncertainty for materials discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Gaussian Process Bayesian Optimization (EI/EHVI)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Fit GP predictive models to observed expensive evaluations, compute acquisition functions such as Expected Improvement (EI) for single-objective or Expected Hypervolume Improvement (EHVI) for multiobjective settings, and select next candidate that maximizes acquisition. Can be combined with BMA by averaging acquisition over model ensemble or replacing predictive distribution with BMA mixture.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials discovery (computational and experimental), multiobjective optimization of material properties, global optimization of expensive black-box functions.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Select next expensive experiment by maximizing acquisition function which approximates expected improvement in the objective per evaluation; in BMA setting acquisition integrates model uncertainty across candidate surrogate models.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Counts of expensive evaluations (e.g., DFT runs); GP hyperparameter optimization and acquisition maximization runtime; EHVI can be computationally expensive in many-objective contexts and requires computation time per candidate.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Acquisition functions (EI, EHVI) quantify expected improvement in objective value or hypervolume; not explicitly mutual information but related expected utility.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Acquisition functions inherently trade exploration and exploitation: EI favors points with high predicted mean and/or high predictive variance; KG is an alternative that explicitly quantifies value-of-information (mentioned elsewhere).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>In multiobjective EHVI, selection tends to explore Pareto frontier diversity indirectly; no explicit diversity regularizer unless incorporated into acquisition design or BMA ensemble.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of expensive evaluations and per-evaluation cost; computational cost of acquisition optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Sequential planning until budget exhausted; for pool-based problems exhaustive scoring of acquisition per candidate is feasible; multi-fidelity or multi-information-source extensions mentioned to trade cost vs utility.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Improvement in objective value (e.g., bulk modulus) or attainment of Pareto-optimal points measured as best-found objective across iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used as baseline/acquisition in experiments: achieves steady improvement in objective in sequential design; specific figures shown in MAX-phase experiments comparing EI/EHVI and BMA variants (plots in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Random search, pure exploitation, entropy-based heuristics, and MOCU-based policies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>BO with GP and EI/EHVI is effective but can be myopic relative to MOCU when objective-relevant uncertainty is structured; BMA+BO and MOCU-based methods are shown to outperform naive BO in scenarios with strong model/form uncertainty or when objective-aware uncertainty matters.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Context-dependent; BO reduces number of expensive evaluations relative to random search but may perform worse than MOCU when objective-relevant uncertainty is not well captured by surrogate priors.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper discusses that BO balances exploration/exploitation in a surrogate-driven fashion but may not improve system-level knowledge (CPSP relationships) and thus can leave a scientific gap; MOCU places uncertainty on the underlying process and ties experiment selection directly to objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>BO is suitable for black-box objective optimization when surrogate priors are adequate; when scientific knowledge or system-model uncertainty is important, coupling BO with BMA or objective-aware criteria (MOCU) yields better allocation of limited expensive experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-driven learning, optimization, and experimental design under uncertainty for materials discovery', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2485.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2485.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG / EGO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Gradient (KG) and Efficient Global Optimization (EGO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two popular Bayesian experimental design/acquisition strategies: KG (decision-theoretic one-step look-ahead value-of-information policy) and EGO (EI-based BO algorithm) are referenced as special cases embedded in the broader framework and as baseline acquisition policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Knowledge Gradient (KG) and Efficient Global Optimization (EGO)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>KG computes the expected value-of-information of sampling a candidate (one-step look-ahead) to select the sample with greatest expected increase in final decision utility; EGO uses Expected Improvement (EI) as acquisition for GP-based BO to select points expected to improve the best-so-far value. The paper positions both as special cases of Bayesian experimental-design under certain modeling assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Bayesian experimental design and Bayesian optimization across many scientific domains, notably materials discovery and expensive black-box optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>KG: allocate next query to maximize expected one-step improvement in final decision outcome (value-of-information). EGO: allocate next query to maximize expected improvement in the objective.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Measured as number of expensive evaluations and computational effort to compute acquisition (KG can be computationally heavier due to nested expectations).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>KG uses expected value-of-information with respect to final decision utility; EGO uses expected improvement (EI).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Both encode exploration/exploitation in the acquisition: KG explicitly optimizes expected posterior decision utility (value-of-information); EGO/EI trades off mean and variance via EI formula.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Not explicitly diversity-promoting beyond acquisition-induced exploration; KG can encourage informative diverse samples if they improve expected decision utility.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of experiments/computations; per-evaluation cost is important for KG due to computational overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Sequential selection until budget exhaustion; KG may be deployed with approximations for scalability (dynamic programming, approximate DP).</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Expected improvement in decision utility or objective value; not a separate novelty score.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Discussed as baselines/related methods; specific performance metrics given elsewhere in the literature rather than in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared conceptually and sometimes empirically to MOCU-based policies and BO/EI in the literature; in this paper KG/EGO are positioned as special/related cases.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>KG and EGO are effective in many problems but may be suboptimal when uncertainty is on system models (CPSP relationships) rather than only on reward functions; MOCU is argued to be preferable for objective-aware system-model uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Context dependent; KG can be more sample-efficient than EI in some decision-theoretic settings but also more computationally intensive.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper situates KG/EGO as instances under the broader framework and contrasts them with MOCU-based design which places uncertainty on the system model and focuses on objective-relevant uncertainty rather than general informativeness.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>KG and EGO are appropriate when surrogate models and acquisition assumptions align with objectives, but when the experiment's role is to reduce system-model uncertainty relevant to operator performance, MOCU-driven allocation may be superior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-driven learning, optimization, and experimental design under uncertainty for materials discovery', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Optimal experimental design for materials discovery <em>(Rating: 2)</em></li>
                <li>Autonomous efficient experiment design for materials discovery with Bayesian model averaging <em>(Rating: 2)</em></li>
                <li>Experimental design via generalized mean objective cost of uncertainty <em>(Rating: 2)</em></li>
                <li>Uncertainty-aware active learning for optimal Bayesian classifier <em>(Rating: 2)</em></li>
                <li>Fast, accurate, and transferable many-body interatomic potentials by symbolic regression <em>(Rating: 1)</em></li>
                <li>Active learning of uniformly accurate interatomic potentials for materials simulation <em>(Rating: 1)</em></li>
                <li>Batch active learning for accelerating the development of interatomic potentials <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2485",
    "paper_id": "paper-265142545",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "IBR-MOCU",
            "name_full": "Intrinsically Bayesian Robust operator + Mean Objective Cost of Uncertainty framework",
            "brief_description": "A Bayesian, objective-driven framework that (1) designs robust operators under model uncertainty by minimizing expected cost across a model uncertainty class (IBR) and (2) quantifies the portion of uncertainty that degrades operational performance using MOCU to drive optimal experiment selection that reduces task-relevant uncertainty.",
            "citation_title": "Knowledge-driven learning, optimization, and experimental design under uncertainty for materials discovery",
            "mention_or_use": "use",
            "system_name": "IBR-MOCU framework",
            "system_description": "Combines Intrinsically Bayesian Robust (IBR) operator design with the Mean Objective Cost of Uncertainty (MOCU) as an objective-aware uncertainty metric. IBR selects operators j_IBR = argmin_j E_q[C_q(j)] under a prior p(q) over model parameters q in an uncertainty class Q. MOCU(Q) = E_q[C_q(j_IBR) - C_q(j_q)] measures expected regret relative to model-specific optimal operators j_q and is used to rank candidate experiments by expected remaining MOCU after observation. The design loop: (a) encode prior knowledge as uncertainty class Q and prior p(q), (b) compute current IBR operator and MOCU, (c) evaluate candidate experiments by expected reduction in MOCU (one-step look-ahead R(Q|x) = E_x[MOCU(Q|x)]), (d) select experiment minimizing expected remaining MOCU, (e) update posterior and repeat.",
            "application_domain": "Materials discovery and more generally experimental design for complex scientific systems under model uncertainty (examples: piezoelectrics, SMAs, Kuramoto oscillator networks)",
            "resource_allocation_strategy": "Allocate experiments by maximizing expected reduction in task-relevant uncertainty (minimize expected remaining MOCU). At each iteration compute for each candidate experiment x the expected post-experiment MOCU and select x that minimizes it (one-step look-ahead). The strategy inherently trades off testing costly candidates that strongly reduce operator-regret vs cheaper/less-informative ones.",
            "computational_cost_metric": "Not fixed to a single scalar in the paper; cost considerations are operationalized via counting expensive queries (e.g., number of DFT simulations or high-cost TDGL evaluations) and noting model-computation complexity; computational cost is therefore measured implicitly as number of experiments/simulations and the cost of surrogate vs full forward model evaluations.",
            "information_gain_metric": "Mean Objective Cost of Uncertainty (MOCU) — expected regret reduction for the operational objective; contrasted with generic entropy or predictive variance measures.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Implicit via the MOCU criterion: experiments are chosen to reduce objective-relevant uncertainty (exploration when uncertainty impacts the objective, exploitation when candidate is promising and reduces expected regret). This produces a principled task-centered balance rather than a separate exploration/exploitation hyperparameter.",
            "diversity_mechanism": "No explicit diversity-promoting penalty is mandated by the framework; diversity arises indirectly because experiments that reduce MOCU may target different regions of model space. The framework can be instantiated with search strategies (exhaustive over a finite pool, dynamic programming / knowledge gradient, gradient-based search, sampling or evolutionary algorithms) that may incorporate diversity constraints.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Fixed number of experiments / computational budget (implicit), and cost-per-experiment when evaluating expensive forward models (e.g., DFT, TDGL).",
            "budget_constraint_handling": "The method explicitly optimizes expected remaining MOCU under the current posterior and can be applied iteratively until the experiment budget is exhausted; optimization over candidate experiments can be done by exhaustive search for pool-based problems, dynamic programming (KG-like), gradient-based methods for continuous spaces, or heuristic search when computation of MOCU is expensive.",
            "breakthrough_discovery_metric": "Not a separate novelty score; breakthroughs are operationally recognized when the chosen design reaches the true-model-specific optimal action (MOCU → 0) or when objective values (e.g., material properties) exceed previous bests; MOCU reduction is a proxy for increasing probability of identifying breakthrough designs.",
            "performance_metrics": "Reported metrics are task-specific: (a) SMA TDGL surrogate: average number of iterations to find optimal dopant/concentration (MOCU-based OED found optimum after 2 iterations on average over 10,000 simulations); (b) Kuramoto synchronization: speed/amount of MOCU reduction compared to random and entropy-based selection (qualitative improvement shown); (c) in general: reduction of expected regret (MOCU) and faster attainment of objectives measured in fewer expensive experiments/simulations.",
            "comparison_baseline": "Random selection, pure exploitation (greedy best predicted objective), entropy-based uncertainty selection, and sometimes knowledge-gradient/EI policies (when mathematically comparable).",
            "performance_vs_baseline": "Qualitative and quantitative improvements reported: SMA example — MOCU-based OED identified optimum after 2 iterations on average vs exploitation/random which did not find optimum after 10 iterations; Kuramoto example — MOCU sharply reduces objective-relevant uncertainty faster than random or entropy-based strategies.",
            "efficiency_gain": "In SMA example, an effective reduction from &gt;10 iterations (exploitation/random failure) to ~2 iterations on average to find the optimum (interpreted as a substantial reduction in number of expensive experiments). Other examples report accelerated convergence to Pareto front (MAX phases) and faster MOCU decline (Kuramoto), but exact percentages beyond the SMA example are not numerically stated.",
            "tradeoff_analysis": "The paper contrasts MOCU (objective-aware uncertainty) with entropy and predictive variance, arguing that entropy may not align with the operational objective and so can waste budget reducing irrelevant uncertainty. It discusses minimax vs Bayesian robust strategies and risk measures (CVaR) but does not present a formal multi-objective cost vs diversity analysis; rather, it emphasizes that MOCU quantifies the uncertainty that practically matters and thus yields better experiment allocation under cost constraints.",
            "optimal_allocation_findings": "Optimal allocation should target experiments that minimize expected remaining MOCU rather than those that maximize generic information measures (entropy) or immediate improvement alone. When experiments are expensive, allocating to those with greatest expected objective-relevant uncertainty reduction yields faster attainment of operational goals; implementation can use exhaustive pool search, KG/dynamic programming, gradient-based optimization, or heuristic search depending on design space structure.",
            "uuid": "e2485.0",
            "source_info": {
                "paper_title": "Knowledge-driven learning, optimization, and experimental design under uncertainty for materials discovery",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "BMA+BO",
            "name_full": "Bayesian Model Averaging combined with Bayesian Optimization",
            "brief_description": "A model-fusion-driven autonomous experimental design approach that embeds Bayesian model averaging into a Bayesian optimization loop (GP surrogates and acquisition functions) to both learn ensemble predictive models and select expensive experiments adaptively for materials discovery.",
            "citation_title": "Knowledge-driven learning, optimization, and experimental design under uncertainty for materials discovery",
            "mention_or_use": "use",
            "system_name": "Bayesian Model Averaging + Bayesian Optimization",
            "system_description": "Maintains an ensemble of surrogate predictive models (e.g., Gaussian processes with different feature sets or kernels) and computes posterior model probabilities P(i|D). The predictive distribution at x is a mixture P(f|x,D)=Σ_i P(i|D) P(f|x,D,m_i). Acquisition functions (MOCU-based utility, Expected Improvement (EI), or Expected Hypervolume Improvement (EHVI) for multiobjective) are computed from the mixture predictive and used to select the next experiment. Model posteriors are updated after each experiment, enabling autoselection and weighting of models during the design loop.",
            "application_domain": "Computational and experimental materials discovery (demonstrated on MAX phase search for mechanical properties), multiobjective materials optimization.",
            "resource_allocation_strategy": "Allocates expensive computational/experimental evaluations by maximizing an acquisition function computed from the BMA predictive mixture (e.g., MOCU-utility, EI, EHVI). This implicitly balances querying locations with high expected performance (exploitation) and regions where model uncertainty across ensemble members indicates high expected information value (exploration).",
            "computational_cost_metric": "Number of expensive simulations/experiments (e.g., DFT evaluations) and time to fit/update GP hyperparameters and compute acquisition over the design pool; cost of ensemble evaluation is higher than single-model BO because acquisition integrates over multiple models.",
            "information_gain_metric": "Uses mixture predictive uncertainty via BMA to compute utility; can use MOCU-based expected regret reduction or standard BO utilities (EI, EHVI) computed from the weighted Gaussian predictive distribution.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Exploration/exploitation arises through the acquisition function computed on the BMA predictive: regions with high expected improvement or high expected regret reduction are favored; as posterior concentrates on better models, exploitation dominates in promising regions while exploration continues in areas with model disagreement.",
            "diversity_mechanism": "No explicit diversity regularizer beyond ensemble-induced exploration: diversity of hypotheses is maintained by keeping multiple models in the ensemble with posterior weights that evolve based on data, which can implicitly promote diverse candidate selection when model disagreement is high.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed experiment/computation budget (e.g., number of DFT evaluations) and per-evaluation cost variability across information sources (multi-information-source context mentioned).",
            "budget_constraint_handling": "Design loop optimizes acquisition per iteration until budget exhausted; can include multi-information-source optimization to choose the information source with best expected utility per cost. Ensemble averaging allows robust performance even when some models are poor, reducing wasted budget on wrong-feature choices.",
            "breakthrough_discovery_metric": "Task-specific: e.g., attaining maximum bulk modulus or points on Pareto front for multiobjective problems; success operationalized as reaching high objective values or Pareto-optimal designs within few iterations.",
            "performance_metrics": "MAX phases example: experiments averaged over 1,500 runs initialized from 10 training samples; BMA-based methods showed robust performance (higher average max bulk modulus over iterations), posterior concentration to best feature set, and approach to Pareto front in small number of sequential iterations. Exact numeric curves are in figures; reported dataset: 1,500 DFT-calculated designs and 10 initial samples per run.",
            "comparison_baseline": "Single GP surrogates trained on individual feature sets (six feature sets), worst/best single-feature-set surrogates, random search, and standard BO policies.",
            "performance_vs_baseline": "BMA achieved robust performance across experiments, outperforming poor individual feature-set surrogates and matching or exceeding the best single-feature surrogate as more iterations accrue. Qualitatively, BMA reduced the number of experiments needed to approach Pareto front compared with baselines.",
            "efficiency_gain": "Reported as considerable reduction in number of experiments/computations needed to identify desired solutions in the MAX-phase design problem (no single aggregate percentage given); demonstrated faster approach to Pareto front and more reliable selection of high-performing candidates compared to using a single poor feature set.",
            "tradeoff_analysis": "Paper discusses tradeoffs from using BMA: computational overhead of maintaining multiple models vs robustness to model-misspecification and reduced risk of selecting poor features; suggests that BMA+BO is particularly beneficial when prior knowledge about best feature set/model is weak and per-evaluation costs are high.",
            "optimal_allocation_findings": "When model/form uncertainty is significant, allocating resources guided by ensemble predictions (BMA) leads to more robust experiment choices and fewer wasted expensive evaluations; using posterior model probabilities to weight predictions improves acquisition utility estimation and thus resource allocation compared to committing to a single surrogate.",
            "uuid": "e2485.1",
            "source_info": {
                "paper_title": "Knowledge-driven learning, optimization, and experimental design under uncertainty for materials discovery",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "MOCU-based OED (applications)",
            "name_full": "Mean Objective Cost of Uncertainty-driven Optimal Experimental Design",
            "brief_description": "An experimental design policy that selects experiments by minimizing expected remaining MOCU to reduce objective-relevant uncertainty; demonstrated on Kuramoto synchronization, shape-memory alloy (SMA) dopant search, and others.",
            "citation_title": "Knowledge-driven learning, optimization, and experimental design under uncertainty for materials discovery",
            "mention_or_use": "use",
            "system_name": "MOCU-based Optimal Experimental Design",
            "system_description": "Given an uncertainty class Q over system models and an operational cost C_q(j), compute current MOCU and for each candidate experiment x compute conditional MOCU(Q|x) and expected remaining MOCU R(Q|x)=E_x[MOCU(Q|x)]. Select experiment x* that minimizes expected remaining MOCU. Implementation strategies vary by problem: exhaustive pool search for finite candidate sets, dynamic programming / knowledge-gradient for structured problems, gradient-based local search for continuous design spaces, or sampling/genetic algorithms when MOCU computations are expensive.",
            "application_domain": "Experimental design for materials problems (SMA dopant selection, MAX phases computational screening), control/synchronization of dynamical systems (Kuramoto oscillator network), and biological network intervention examples in cited work.",
            "resource_allocation_strategy": "Directly allocates limited experimental budget to experiments that maximize expected reduction in objective-regret (MOCU). In pool-based scenarios, chooses highest-scoring candidate per iteration; in continuous spaces uses gradient/sampling approximations.",
            "computational_cost_metric": "Counts of expensive experiments/simulations (e.g., DFT runs, TDGL oracle evaluations), and internal computational cost to estimate MOCU (Monte Carlo sampling over model posterior and computing IBR operators); cost of MOCU evaluation itself can be significant and is handled with approximations.",
            "information_gain_metric": "Expected reduction in MOCU (task-focused information gain about objective-relevant model parameters).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Unified in MOCU: selecting experiments that reduce expected regret may choose exploratory experiments (if current regret is driven by uncertain regions) or exploitative ones (if promising high-performance designs both improve the objective and reduce regret); thus policy is adaptive based on current posterior and objective sensitivity.",
            "diversity_mechanism": "No explicit diversity metric; breadth of exploration follows from where objective-relevant uncertainty is concentrated (i.e., if multiple hypothesis modes contribute to regret, experiments will be diverse to discriminate them).",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Fixed number of experiments / simulation budget; high per-experiment cost situations emphasized.",
            "budget_constraint_handling": "Plan experiments sequentially to minimize cumulative expected remaining MOCU until budget depletion; use computational approximations and surrogate models to reduce per-iteration cost when forward-model evaluations are costly.",
            "breakthrough_discovery_metric": "Success measured by reaching optimal design or significant objective improvement faster (e.g., attaining minimal dissipation in SMA or achieving Pareto-optimal points), and by reduction of MOCU toward zero.",
            "performance_metrics": "SMA case: average iterations to find optimal dopant/concentration = 2 (averaged over 10,000 simulations); Kuramoto case: faster MOCU decrease vs random/entropy policies (figures show sharp reduction but no absolute numeric count in text).",
            "comparison_baseline": "Random selection, entropy-based selection, pure exploitation policies, and standard BO (EI/KG) when applicable.",
            "performance_vs_baseline": "MOCU-based OED outperformed random and entropy-based strategies in objective-relevant uncertainty reduction and found SMA optimum in far fewer iterations than baselines; qualitative superior performance in Kuramoto synchronization example.",
            "efficiency_gain": "Illustrative: reduction from &gt;10 iterations (baseline failure) to ~2 iterations (MOCU) in SMA experiments; other domains show faster convergence but no single aggregate metric provided.",
            "tradeoff_analysis": "Emphasizes that objective-aware measures (MOCU) avoid wasting budget reducing irrelevant uncertainty (as can occur with entropy). Paper contrasts minimax and Bayesian robust strategies and notes possibility of incorporating different risk metrics (e.g., CVaR) to tune conservativeness.",
            "optimal_allocation_findings": "Optimal experimental allocation under objective-centric goals is to minimize expected remaining MOCU; exhaustive search or dynamic programming may be used depending on design-space structure, and surrogate/sampling approximations are practical necessities when forward models are expensive.",
            "uuid": "e2485.2",
            "source_info": {
                "paper_title": "Knowledge-driven learning, optimization, and experimental design under uncertainty for materials discovery",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Physics-constrained AFE (DQN-FGT)",
            "name_full": "Physics-constrained Automatic Feature Engineering with Deep Q-Network guided Feature Generation Tree",
            "brief_description": "An interpretable, scalable automatic feature engineering system that builds a constrained feature-generation tree (FGT) and uses a DQN policy with experience replay to select algebraic operations (guided by physics constraints) to create compact, predictive descriptors under data scarcity.",
            "citation_title": "Knowledge-driven learning, optimization, and experimental design under uncertainty for materials discovery",
            "mention_or_use": "use",
            "system_name": "Physics-constrained AFE (DQN-guided FGT exploration)",
            "system_description": "Constructs a Feature Generation Tree (FGT) where nodes represent sets of descriptors and edges represent algebraic operations (predefined operation set O informed by physics). A deep Q-network (DQN) learns a generation policy p that chooses operations to expand nodes. Rewards reflect predictive improvement (accuracy) penalized by complexity; batch sampling of candidate combinations controls combinatorial blow-up. Prior physics knowledge is injected as constraints on permitted operations or feature groups. The learned policy sequentially constructs top-d descriptors for model interpretability and efficiency.",
            "application_domain": "Materials science descriptor discovery, symbolic/interpretable model construction, interatomic potential functional-form search (demonstrated for copper interatomic potential and other materials tasks).",
            "resource_allocation_strategy": "Allocates computational effort in descriptor generation/search by (a) prioritizing operations with larger expected reward (predicted improvement per complexity), (b) batch sampling to limit combinatorial expansion per iteration, and (c) using physics constraints to prune operation choices—thus trading computation for predicted information/value of features.",
            "computational_cost_metric": "Wall-clock CPU time for descriptor search and model fitting (example: copper potential search achieved MAE in 12 hours); also complexity metric c counting number of algebraic operations in generated descriptor which acts as a proxy for model complexity and computational expense.",
            "information_gain_metric": "Reward function: R(F_i, op) = max_{F0 subset} (1.001 - A_L(F0; y)) - 1, where A_L is normalized predictive accuracy (0..1); reward approximates expected predictive gain per generated descriptor, implicitly an information-reward tradeoff accounting for complexity.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "DQN policy approximates Q-values of operations (expected future reward), balancing exploration via epsilon-greedy or experience replay components and exploitation by selecting high-Q operations; batch sampling allows tunable depth/breadth exploration to manage computational budget.",
            "diversity_mechanism": "Batch sampling of feature subspaces and selection of top-n features per node yield diverse candidate descriptors; physics constraints prevent irrelevant or unphysical operations, promoting diversity among physically plausible hypotheses.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Computational time and combinatorial complexity (memory) constraints; complexity limit c_max on descriptor operations used as an explicit budget for feature complexity.",
            "budget_constraint_handling": "Controls search breadth via batch sampling size, top-n selection per node, maximal allowed descriptor complexity c_max, and DQN-guided prioritization to focus CPU time on most promising descriptor branches.",
            "breakthrough_discovery_metric": "Descriptor predictive performance improvements (e.g., reduced MAE on held-out energies) and interpretability (compact analytic forms); in interatomic potential example, lower MAE is proxy for breakthrough functional forms.",
            "performance_metrics": "Copper interatomic potential example: AFE-generated descriptor model achieved MAE = 3.73 meV/atom in 12 hours; comparative genetic-programming model (GP1) reported MAE = 4.13 meV/atom after 360 CPU hours on same data split.",
            "comparison_baseline": "Genetic programming-based symbolic regression (Hernandez et al. GP1), brute-force deep feature synthesis, other automated FE tools (Autolearn, Cognito) mentioned as related work.",
            "performance_vs_baseline": "AFE achieved lower MAE (3.73 vs 4.13 meV/atom) and vastly reduced runtime (12 hours vs 360 CPU hours) compared to the reported GP1 symbolic-regression baseline on the same dataset and split.",
            "efficiency_gain": "Substantial computational-efficiency improvement reported: from 360 CPU hours (GP1) to 12 hours (AFE) for comparable or better predictive accuracy (≈10% lower MAE); generation/search reduced memory/combinatorial costs via batch sampling and DQN guidance.",
            "tradeoff_analysis": "Paper explicitly frames a performance-complexity tradeoff controlled by descriptor complexity c and batch sampling size; physics constraints reduce search space and avoid overfitting on small datasets, prioritizing interpretable and low-complexity descriptors that give good predictive returns per computational cost.",
            "optimal_allocation_findings": "Efficient descriptor discovery under constrained compute should prioritize physics-plausible operations and use learned policies (DQN) to approximate expected value-of-information for candidate operations; limiting descriptor complexity and batch sampling provides a controllable tradeoff between search cost and predictive gain.",
            "uuid": "e2485.3",
            "source_info": {
                "paper_title": "Knowledge-driven learning, optimization, and experimental design under uncertainty for materials discovery",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "BO with GP + EI/EHVI",
            "name_full": "Bayesian Optimization with Gaussian Process surrogates and acquisition functions (Expected Improvement / Expected Hypervolume Improvement)",
            "brief_description": "Standard Bayesian optimization approach using GP surrogates to model expensive black-box objectives and acquisition functions (EI for single objective, EHVI for multiobjective) to select next queries; used here as special cases and baselines and embedded inside BMA or compared to MOCU-based policies.",
            "citation_title": "Knowledge-driven learning, optimization, and experimental design under uncertainty for materials discovery",
            "mention_or_use": "use",
            "system_name": "Gaussian Process Bayesian Optimization (EI/EHVI)",
            "system_description": "Fit GP predictive models to observed expensive evaluations, compute acquisition functions such as Expected Improvement (EI) for single-objective or Expected Hypervolume Improvement (EHVI) for multiobjective settings, and select next candidate that maximizes acquisition. Can be combined with BMA by averaging acquisition over model ensemble or replacing predictive distribution with BMA mixture.",
            "application_domain": "Materials discovery (computational and experimental), multiobjective optimization of material properties, global optimization of expensive black-box functions.",
            "resource_allocation_strategy": "Select next expensive experiment by maximizing acquisition function which approximates expected improvement in the objective per evaluation; in BMA setting acquisition integrates model uncertainty across candidate surrogate models.",
            "computational_cost_metric": "Counts of expensive evaluations (e.g., DFT runs); GP hyperparameter optimization and acquisition maximization runtime; EHVI can be computationally expensive in many-objective contexts and requires computation time per candidate.",
            "information_gain_metric": "Acquisition functions (EI, EHVI) quantify expected improvement in objective value or hypervolume; not explicitly mutual information but related expected utility.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Acquisition functions inherently trade exploration and exploitation: EI favors points with high predicted mean and/or high predictive variance; KG is an alternative that explicitly quantifies value-of-information (mentioned elsewhere).",
            "diversity_mechanism": "In multiobjective EHVI, selection tends to explore Pareto frontier diversity indirectly; no explicit diversity regularizer unless incorporated into acquisition design or BMA ensemble.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Fixed number of expensive evaluations and per-evaluation cost; computational cost of acquisition optimization.",
            "budget_constraint_handling": "Sequential planning until budget exhausted; for pool-based problems exhaustive scoring of acquisition per candidate is feasible; multi-fidelity or multi-information-source extensions mentioned to trade cost vs utility.",
            "breakthrough_discovery_metric": "Improvement in objective value (e.g., bulk modulus) or attainment of Pareto-optimal points measured as best-found objective across iterations.",
            "performance_metrics": "Used as baseline/acquisition in experiments: achieves steady improvement in objective in sequential design; specific figures shown in MAX-phase experiments comparing EI/EHVI and BMA variants (plots in paper).",
            "comparison_baseline": "Random search, pure exploitation, entropy-based heuristics, and MOCU-based policies.",
            "performance_vs_baseline": "BO with GP and EI/EHVI is effective but can be myopic relative to MOCU when objective-relevant uncertainty is structured; BMA+BO and MOCU-based methods are shown to outperform naive BO in scenarios with strong model/form uncertainty or when objective-aware uncertainty matters.",
            "efficiency_gain": "Context-dependent; BO reduces number of expensive evaluations relative to random search but may perform worse than MOCU when objective-relevant uncertainty is not well captured by surrogate priors.",
            "tradeoff_analysis": "Paper discusses that BO balances exploration/exploitation in a surrogate-driven fashion but may not improve system-level knowledge (CPSP relationships) and thus can leave a scientific gap; MOCU places uncertainty on the underlying process and ties experiment selection directly to objectives.",
            "optimal_allocation_findings": "BO is suitable for black-box objective optimization when surrogate priors are adequate; when scientific knowledge or system-model uncertainty is important, coupling BO with BMA or objective-aware criteria (MOCU) yields better allocation of limited expensive experiments.",
            "uuid": "e2485.4",
            "source_info": {
                "paper_title": "Knowledge-driven learning, optimization, and experimental design under uncertainty for materials discovery",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "KG / EGO",
            "name_full": "Knowledge Gradient (KG) and Efficient Global Optimization (EGO)",
            "brief_description": "Two popular Bayesian experimental design/acquisition strategies: KG (decision-theoretic one-step look-ahead value-of-information policy) and EGO (EI-based BO algorithm) are referenced as special cases embedded in the broader framework and as baseline acquisition policies.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Knowledge Gradient (KG) and Efficient Global Optimization (EGO)",
            "system_description": "KG computes the expected value-of-information of sampling a candidate (one-step look-ahead) to select the sample with greatest expected increase in final decision utility; EGO uses Expected Improvement (EI) as acquisition for GP-based BO to select points expected to improve the best-so-far value. The paper positions both as special cases of Bayesian experimental-design under certain modeling assumptions.",
            "application_domain": "Bayesian experimental design and Bayesian optimization across many scientific domains, notably materials discovery and expensive black-box optimization.",
            "resource_allocation_strategy": "KG: allocate next query to maximize expected one-step improvement in final decision outcome (value-of-information). EGO: allocate next query to maximize expected improvement in the objective.",
            "computational_cost_metric": "Measured as number of expensive evaluations and computational effort to compute acquisition (KG can be computationally heavier due to nested expectations).",
            "information_gain_metric": "KG uses expected value-of-information with respect to final decision utility; EGO uses expected improvement (EI).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Both encode exploration/exploitation in the acquisition: KG explicitly optimizes expected posterior decision utility (value-of-information); EGO/EI trades off mean and variance via EI formula.",
            "diversity_mechanism": "Not explicitly diversity-promoting beyond acquisition-induced exploration; KG can encourage informative diverse samples if they improve expected decision utility.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Fixed number of experiments/computations; per-evaluation cost is important for KG due to computational overhead.",
            "budget_constraint_handling": "Sequential selection until budget exhaustion; KG may be deployed with approximations for scalability (dynamic programming, approximate DP).",
            "breakthrough_discovery_metric": "Expected improvement in decision utility or objective value; not a separate novelty score.",
            "performance_metrics": "Discussed as baselines/related methods; specific performance metrics given elsewhere in the literature rather than in this paper.",
            "comparison_baseline": "Compared conceptually and sometimes empirically to MOCU-based policies and BO/EI in the literature; in this paper KG/EGO are positioned as special/related cases.",
            "performance_vs_baseline": "KG and EGO are effective in many problems but may be suboptimal when uncertainty is on system models (CPSP relationships) rather than only on reward functions; MOCU is argued to be preferable for objective-aware system-model uncertainty.",
            "efficiency_gain": "Context dependent; KG can be more sample-efficient than EI in some decision-theoretic settings but also more computationally intensive.",
            "tradeoff_analysis": "Paper situates KG/EGO as instances under the broader framework and contrasts them with MOCU-based design which places uncertainty on the system model and focuses on objective-relevant uncertainty rather than general informativeness.",
            "optimal_allocation_findings": "KG and EGO are appropriate when surrogate models and acquisition assumptions align with objectives, but when the experiment's role is to reduce system-model uncertainty relevant to operator performance, MOCU-driven allocation may be superior.",
            "uuid": "e2485.5",
            "source_info": {
                "paper_title": "Knowledge-driven learning, optimization, and experimental design under uncertainty for materials discovery",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Optimal experimental design for materials discovery",
            "rating": 2,
            "sanitized_title": "optimal_experimental_design_for_materials_discovery"
        },
        {
            "paper_title": "Autonomous efficient experiment design for materials discovery with Bayesian model averaging",
            "rating": 2,
            "sanitized_title": "autonomous_efficient_experiment_design_for_materials_discovery_with_bayesian_model_averaging"
        },
        {
            "paper_title": "Experimental design via generalized mean objective cost of uncertainty",
            "rating": 2,
            "sanitized_title": "experimental_design_via_generalized_mean_objective_cost_of_uncertainty"
        },
        {
            "paper_title": "Uncertainty-aware active learning for optimal Bayesian classifier",
            "rating": 2,
            "sanitized_title": "uncertaintyaware_active_learning_for_optimal_bayesian_classifier"
        },
        {
            "paper_title": "Fast, accurate, and transferable many-body interatomic potentials by symbolic regression",
            "rating": 1,
            "sanitized_title": "fast_accurate_and_transferable_manybody_interatomic_potentials_by_symbolic_regression"
        },
        {
            "paper_title": "Active learning of uniformly accurate interatomic potentials for materials simulation",
            "rating": 1,
            "sanitized_title": "active_learning_of_uniformly_accurate_interatomic_potentials_for_materials_simulation"
        },
        {
            "paper_title": "Batch active learning for accelerating the development of interatomic potentials",
            "rating": 1,
            "sanitized_title": "batch_active_learning_for_accelerating_the_development_of_interatomic_potentials"
        }
    ],
    "cost": 0.02282575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Knowledge-driven learning, optimization, and experimental design under uncertainty for materials discovery</p>
<p>Xiaoning Qian xqian@ece.tamu.edu 
Department of Electrical &amp; Computer Engineering
Texas A&amp;M University
77843College StationTXUSA</p>
<p>Computational Science Initiative
Brookhaven National Laboratory
Upton11973NYUSA</p>
<p>Byung-Jun Yoon 
Department of Electrical &amp; Computer Engineering
Texas A&amp;M University
77843College StationTXUSA</p>
<p>Computational Science Initiative
Brookhaven National Laboratory
Upton11973NYUSA</p>
<p>Raymundo Arro 
Department of Materials Science &amp; Engineering
Texas A&amp;M University
77843College StationTXUSA</p>
<p>Xiaofeng Qian 
Department of Materials Science &amp; Engineering
Texas A&amp;M University
77843College StationTXUSA</p>
<p>Edward R Dougherty 
Department of Electrical &amp; Computer Engineering
Texas A&amp;M University
77843College StationTXUSA</p>
<p>Knowledge-driven learning, optimization, and experimental design under uncertainty for materials discovery
5F9D7FCF21A333ECCC7C5F99F76762AC10.1016/j.patter.2023.100863
Significant acceleration of the future discovery of novel functional materials requires a fundamental shift from the current materials discovery practice, which is heavily dependent on trial-and-error campaigns and highthroughput screening, to one that builds on knowledge-driven advanced informatics techniques enabled by the latest advances in signal processing and machine learning.In this review, we discuss the major research issues that need to be addressed to expedite this transformation along with the salient challenges involved.We especially focus on Bayesian signal processing and machine learning schemes that are uncertainty aware and physics informed for knowledge-driven learning, robust optimization, and efficient objective-driven experimental design.</p>
<p>INTRODUCTION</p>
<p>Accelerating the development of novel functional materials with desirable properties is a worldwide imperative because it can facilitate advances in diverse fields across science, engineering, and biomedicine with significant potential contributions to economic growth.For example, the US Materials Genome Initiative (MGI) calls for cutting the cost and time for bringing new materials from discovery to deployment by half by integrating experiments, computer simulations, and data analytics. 1,2However, the current prevailing practice in materials discovery primarily relies on trial-and-error experimental campaigns or highthroughput virtual screening approaches by computational simulations, neither of which can efficiently explore the huge materials design space to develop materials that possess targeted functional properties.</p>
<p>4][5] When applying SP and ML methods in materials science, several unique challenges arise, which include (1) a limited amount of data (if any) for investigating and exploring new materials systems, (2) data of varying and inconsistent quality because of technical limitations and a lack of common profiling prototypes, (3) significant complexity and uncertainty in existing computational simulation and surrogate models, 6 and (4) incomplete domain knowledge.</p>
<p>To cope with the aforementioned challenges and to effectively discover novel functional materials with the desired target properties, robust decision-making strategies are critical for efficient THE BIGGER PICTURE Thanks to the rapid advances in artificial intelligence, AI for science (AI4Science) has emerged as one of the new promising research directions for modern science and engineering.In this review, we focus on recent efforts to develop knowledge-driven Bayesian learning and experimental design methods for accelerating the discovery of novel functional materials as well as enhancing the understanding of composition-process-structure-property relationships.We specifically discuss the challenges and opportunities in integrating prior scientific knowledge and physics principles with AI and machine learning (ML) models for accelerating materials and knowledge discovery.The current state-of-the-art methods in knowledge-based prior construction, model fusion, uncertainty quantification, optimal experimental design, and symbolic regression are detailed in the review, along with several detailed case studies and results in materials discovery.</p>
<p>exploration of the immense materials design space through effective learning, optimization, and experimental design under significant uncertainty.Directly applying existing data-driven SP and ML methods falls short of achieving these goals, and comprehensive theoretical and methodological developments tailored to address these unique challenges are crucial.Some salient issues that need to be addressed include the following:</p>
<p>(1) Knowledge-based prior construction: mapping scientific knowledge into a prior distribution that reflects model uncertainty to alleviate the issues stemming from data scarcity (2) Model fusion: updating the prior distribution to a posterior distribution with multiple uncertain models and data sources of different data quality (3) Uncertainty quantification (UQ): quantification of the cost of uncertainty relative to one or more objectives for efficient materials discovery (4) Optimization under uncertainty (OUU): derivation of an optimal operator from the posterior distribution (5) Optimal experimental design (OED): efficient experimental design and data acquisition schemes to improve the model to explore the materials design space more effectively (6) Knowledge discovery: closing the knowledge gap in the current model, such as composition-process-structureproperty (CPSP) relationships relevant to the materials discovery objectives, based on newly acquired data or increased model knowledge</p>
<p>In this article, we will review the recent advances related to the aforementioned research issues.Especially, we will provide an in-depth review of Bayesian SP and ML approaches for knowledge-driven learning, objective-based UQ, and efficient experimental design for materials discovery under substantial model and data uncertainties.The core foundation underlying these strategies is a Bayesian framework that enables mathematical representation of the model and data uncertainties, encoding available domain knowledge into a Bayesian prior, seamlessly integrating experimental (or simulation) data with the domain knowledge to obtain a posterior, quantifying the impact of the uncertainty on the objective, and effective design of strategies that can reduce this uncertainty.It is important to note that the guiding principle of the aforementioned Bayesian framework is to have a (knowledge-based) prior represent an uncertainty class of models.The prior characterizes the state of our knowledge about the model representing the system, based on which we can design operators to achieve the scientific objectives.Artificial intelligence for science (AI4Science) has emerged as an enormous modern research field.][13] In the following sections, we first introduce the UQ framework that encompasses the various components in knowledge-driven learning, optimization, and experimental design.This will be followed by in-depth discussion of the individual research themes, where we will review the latest research results along these directions.</p>
<p>Bayesian learning, UQ, and experimental design</p>
<p>Engineering generally aims at optimization to achieve operational objectives when studying complex systems.Because all but very simple systems must account for randomness, modern engineering may be defined as the study of optimal operators on random processes.Besides the mathematical and computational challenges that arise with classical system identification (learning) and operator optimization (control or filtering, for example) problems, such as nonstationary processes, high dimensions, and nonlinear operators, another profound issue is model uncertainty.For instance, with linear filtering there may be incomplete knowledge regarding the covariance functions or power spectra in the case of Wiener filtering.In such cases, not only must optimization of the operator (i.e., filter in this example) be relative to the original cost function but also relative to an uncertainty class of random processes.This naturally leads to the need for postulation of a new cost function that integrates the original cost function with the model uncertainty.If there is a prior (or posterior) distribution governing the likelihood of a model within the uncertainty class, then one can choose an operator that minimizes the expected cost over all possible models in the uncertainty class.In what follows, we first lay out the mathematical foundations pertinent to quantifying and handling model uncertainty and then review relevant existing literature, with recent efforts focusing on materials science research.</p>
<p>Mathematical backgrounds</p>
<p>The design of optimal operators can take different forms depending on the random process constituting the scientific model and the operator class of interest.The operators might be filters, classifiers, or controllers.The underlying random process might be a random signal/image for filtering, a feature-label distribution for classification, or a Markov process for control.Optimal operator design involves a mathematical model representing the underlying (materials) system and a class of operators from which the best operator that minimizes the cost function reflecting the objective should be selected.It takes the general form
j opt = argmin j ˛JCðjÞ; (Equation 1)
where J is the operator class, and CðjÞ is the cost of applying operator c on the system.The genesis of such an operator design formulation can be traced back to the Wiener-Kolmogorov theory in SP for optimal linear filters developed in the 1930s, 14,15 where the operational objective is to recover the underlying signals given noisy observations with the minimum mean squared error (MSE).In this class of filtering problems, the operators mentioned above are filters.The underlying system can be modeled by a joint random process ðXðtÞ; YðsÞÞ, t ˛T; s ˛S.Optimal filtering involves estimating the signal YðsÞ at time s via a filter c given observations fXðtÞg t ˛T .A filter j ˛J is a mapping on the space of possible observed signals, and a cost function takes the form CðYðsÞ; b Y ðsÞÞ, with b Y ðsÞ = jðXÞðsÞ.For fixed s ˛S, an optimal filter is defined by Equation 1with CðjÞ = CðYðsÞ; jðXÞðsÞÞ = E½ðYðsÞ À jðXÞðsÞÞ 2 .Similar operator design formulations have been adopted in control 16 and, more recently, in ML, 17 where the corresponding operators are controllers that can desirably alter the system behavior or predictive models for system properties of interest (e.g., classifiers).For example, the operator may be a predictor that tries to characterize the property of a given material based on input features (such as its composition and structure).</p>
<p>When the true model is not known with certainty, it would be prudent to consider the entire uncertainty class Q of possible models that contains that true model q ˛Q, where q may be typically a parameter vector specifying the model rather than aiming at accurate inference of the true model.Given Q, the goal would then be to design a robust operator that guarantees good performance over all possible models.For example, there have been significant research efforts taking a minimax strategy to design robust operators:
j Q minimax = argmin j ˛Jmax
q ˛Q C q ðjÞ; (Equation 2)</p>
<p>Where C q ðjÞ characterizes the cost of the operator c for model q.Taking filtering as an example, C q ðjÞ = CðYðs; qÞ; jðXÞðs; qÞÞ, where q denotes the model parameters for the signal and observation random processes.Such a minimax robust strategy is risk averse because it aims to find an operator whose worst performance over an uncertainty class of models Q is the best among all operators in J 18,19 .Minimax robustness has been applied in many optimization frameworks; for example, for filtering [20][21][22][23] with a general formulation in the context of game theory, 24 as well as recently in ML. 25,26 One critical downside of minimax robustness is that, in avoiding the worst-case scenario, the average performance of the designed operator can be poor, in particular when the prior knowledge about the uncertainty class Q is available and the worst-case model is unlikely.There has been extensive research on alleviating this potential issue by developing risk measures, such as conditional value at risk in a recently proposed risk quadrangle scheme, 18 to achieve better trade-off between the attainment of the operational objective and the aversion of potential risk because of uncertainty.Unlike such minimax robust strategies, we focus on Bayesian robust strategies that try to optimize the expected performance in the presence of uncertainty.This leads to the design of the intrinsically Bayesian robust (IBR) operator, which is defined as
j Q IBR = argmin j ˛JE Q ½C q ðjÞ; (Equation 3)
where the expectation is with respect to a prior probability distribution pðqÞ of the uncertain model q ˛Q.While not as risk averse as minimax robust operators, these Bayesian robust operators guarantee optimal performance on average.The prior pðqÞ probabilistically characterizes our prior knowledge as to which models are more likely to be the true model than the others.If there is no prior knowledge beyond the uncertainty class itself, then a uniform (non-informative) prior may be used.</p>
<p>Related works</p>
<p>Before we delve into the Bayesian framework for learning, UQ, and experimental design, here we provide a literature review of related topics.We first review the history of operator design, in particular related to filtering, classification, and control.For optimal operator design in filtering, Kalman-Bucy recursive filtering was proposed in the 1960s 27 after the Wiener filter. 14,15ptimal control began in the 1950s, as did classification as now understood.In all three areas, it was quickly recognized that often the underlying scientific model would not be known-hence the development of adaptive linear/Kalman filters and adaptive controllers. 28,29Classification became dependent on classification rules that make no effort to estimate the true feature-label distribution. 17From the perspective of model uncertainty classes, control theorists delved into Bayesian robust control for Markov decision processes in the work of Bellman and Kalaba, 30 Silver, 31 and Martin [30][31][32] in the 1960s, but computation was prohibitive, and adaptive methods prevailed.Optimal linear filtering was approached via minimax in the late 1970s in the work of Kuznetsov, 20 Kassam and Lim, 21 Poor, 22 and Verdu and Poor. 24Model-constrained Bayesian robust (MCBR) MSE linear filtering and classification appeared in the early 2000s. 33,34hen considering uncertainty in optimization, there has been extensive research in designing different risk metrics for UQ.For example, different values at risk 18 and quantities of interest (QoIs) 19 have been proposed based on different statistics when modeling random processes or the corresponding model parameters as random variables, including the ones based on prediction variance 35 and predictive entropy. 36,378][39] While introducing additional risk metrics enables balancing the trade-off between the operational objectives and the potential risk (or regret) because of uncertainty, incorporating different metrics with different strategies can be subjective.For example, there may be large predictive variance or entropy, but it may not always directly affect the operational objectives and, thereafter, consequent decisionmaking.</p>
<p>1][42][43][44] In this framework, UQ can be naturally measured by the loss of performance because of the utilization of a robust operator to cope with uncertainty.1][32] Such an experimental design framework, rooted in the foundation of modern engineering, closes the loop from scientific knowledge on a complex system, models for the complex system under uncertainty, data generated by the system, and experiments to enhance the current system knowledge to better attain the objectives.In this paper, we focus on this closed-loop framework, which distinguishes itself from (1) other existing schemes that are purely data driven [45][46][47][48] or (2) experimental design frameworks based on high-throughput simulations, such as P 4U 49 and DAKOTA. 50ata-driven frameworks heavily depend on the availability of data, upon which ''black box'' surrogate models are trained.They typically model the operators (used to achieve the objectives) of interest rather than modeling the system itself when designing experiments.2][53] While BO may be useful for optimizing the properties, the acquired data do not improve our knowledge regarding the materials system.As a consequence, there is often a scientific gap in making prior assumptions on these ''black box'' models and their uncertainty. 54To better integrate scientific knowledge, such as materials' process-structureproperty relationships, as detailed under ''Knowledge-driven prior construction,'' the model uncertainty should be directly imposed on the system model that incorporates inter-relationships among the underlying random processes.For simulation-based frameworks, including P 4U and DAKOTA, UQ, sensitivity analysis, and experimental design are mostly based on forward model simulations, which do not provide a natural way to propagate the data generated by the selected experiments back to the system to fill the gap in our system knowledge and to improve the current model, which is precisely what our proposed paradigm aims to do.The emphasis here is that (1) the uncertainty is placed directly on the underlying random process (i.e., current knowledge regarding the materials system) and not on surrogate models that reflect operational performance on this uncertain process and that (2) the experimental design is centered around attaining specific objectives.A wide range of approaches can emerge, depending on the assumptions made regarding the uncertainty class, action space, and experiment space.Popular Bayesian experimental design policies, such as knowledge gradient (KG) 46,47 and efficient global optimization (EGO), 45 are special cases in this framework under their modeling assumptions.These approaches often adopt generic surrogate models with the uncertainty placed on the reward function; therefore, there is no direct connection between the prior model assumptions and the underlying process/system.</p>
<p>6][57][58][59] BO's ability to balance the exploration and exploitation is ideally suited in materials discovery tasks because queries to the materials design space (either through computations or experiments) are extremely resource intensive.Most approaches focused on materials discovery are myopic in the sense that increased knowledge of the materials space being explored is not necessarily part of the objective.1][62][63][64] In materials discovery applications, the complexity and stochasticity because of substantial model and data uncertainty call for SP and ML approaches in a Bayesian setting that can provide a unified closed-loop framework for objective-based learning and optimal design of robust operators and effective experiments under uncertainty.This is illustrated in Figure 1.</p>
<p>IBR operator and mean objective cost of uncertainty (MOCU)-based UQ</p>
<p>In this section, we focus on the objective-based UQ (objective-UQ) framework using the MOCU, 65,66 which measures the expected loss with respect to the final operational objective because of the model uncertainty.Uncertainty is directly imposed on the model representing the underlying system and not on the parameters of the operator, as typically done in the ML community.Because the uncertainty is on the system model, reduction of this uncertainty inevitably leads to improving our knowledge regarding the system, leaving no discrepancy between what is learned (through data acquisition or experiments) about the model and what we know about the underlying system (and the relevant science).</p>
<p>Consider a stochastic model M with uncertainty class Q composed of possible parameter vectors.Let C be a cost function and J a class of operators on M. For each operator j ˛J, C q ðjÞ denotes the cost of applying c on the model parametrized by q ˛Q.An IBR operator on M is an operator j IBR ˛J so that the expected value over Q of the cost C q ðjÞ is minimized by j IBR as formulated in Equation 3, 67 the expected value being with respect to a prior probability distribution pðqÞ capturing model uncertainty over Q.Here, each parameter vector q ˛Q corresponds to a model, and pðqÞ quantifies the likelihood that a model is Q and therefore reflects prior knowledge.If there is no prior knowledge beyond the uncertainty class itself, then it is taken to be uniform with all models being equally likely.Given a data sample S sampled independently from the full model, the IBR theory can be used with a posterior distribution p Ã ðqÞ = pðqjSÞ, giving the optimal Bayesian operator.Because of the optimality of the IBR operator j IBR over Q, E Q ½C q ðj IBR Þ % E Q ½C q ðjÞ for any operator c.For q ˛Q, the objective cost of uncertainty relative to q is the difference between C q ðj IBR Þ and C q ðj q Þ. Averaging this loss differential provides our basic UQ, the MOCU: 65 MOCUðQÞ = E q ½C q ðj IBR Þ À C q ðj q Þ;</p>
<p>(Equation 4)</p>
<p>where j q denotes the optimal operator with respect to the model specified by the model parameter q.The expectation is computed with respect to the distribution pðqÞ of the model q in the uncertainty class Q.While the entropy of the prior (or posterior) has been commonly used to measure model uncertainty, entropy, however, does not focus on the objective.In other words, there may be large entropy, but it may not directly affect the operational objective because it may not affect the expected cost in Equation 13.Unlike entropy, MOCU aims to quantify the uncertainty that practically ''matters'' as it pertains to a specific objective (Figure 1).</p>
<p>IBR (with a prior) and optimal Bayesian (with an updated posterior given observed data) operator design have been applied in modern engineering, statistics, and ML.9][70] When systems understanding and operator design are the objectives of modeling complex systems, Bayesian experimental design and decision-making are often with respect to the uncertainty class of models and the cost function related to the operator of interest.More importantly, MOCU provides a natural measure for the cost of uncertainty that quantifies the potential operator performance degradation because of uncertainty, directly focusing on operational objectives.Therefore, this IBR-MOCU framework not only provides the robust operator design and objective-oriented UQ but also leads to experimental design to choose an experiment to optimally reduce performance loss by adding to existing scientific knowledge.The IBR-MOCU paradigm follows in line from the early thinking of Wiener and Kolmogorov, and it extends and unifies previous work on robust filtering, classification, and control.The historical context of the IBR-MOCU framework is depicted in Figure 2.</p>
<p>In the following sections, we focus on recent developments on the corresponding components of this IBR-MOCU framework, including prior construction, model fusion, OED, and automated feature engineering for knowledge discovery in the context of materials science applications.</p>
<p>Knowledge-driven prior construction</p>
<p>The first challenge of applying SP/ML methods in the MOCU framework to materials science is modeling and quantifying uncertainty because there rarely exist sufficient data for satisfactory system identification because of the enormous search space and the complicated CPSP relationships. 4 Small samples are commonplace in materials applications, in particular when the research focus is to discover novel complex functional mate- rials.2][73] While Bayesian methods naturally model the uncertainty because of their distributionbased nature to treat model parameters as random variables, the salient obstacle confronting Bayesian methods is how to appropriately impose model prior.</p>
<p>Regarding prior construction, Jaynes 74 has remarked, ''.there must exist a general formal theory of determination of priors by logical analysis of prior information-and that to develop it is today the top priority research problem of Bayesian theory.''However, the most common practice of Bayesian methods is to adopt either non-informative or conjugate prior for computational convenience.When there are limited data or strong scientific prior knowledge, it is precisely then that the formal structure as commented by Jaynes 74 is critical for appropriate prior construction.</p>
<p>In this section, we first briefly review traditional prior construction methods and then focus on the formal structure for prior construction involving a constrained optimization, in which the constraints incorporate existing scientific knowledge augmented by slackness variables.The constraints tighten the prior distribution in accordance with prior knowledge while at the same time avoiding inadvertent over-restriction of the prior, an important consideration with small samples.</p>
<p>Traditional priors</p>
<p>Starting from Jeffreys' 75 non-informative prior, there was a series of information-theoretic and statistical methods: maximal data information priors (MDIP), 76 non-informative priors for integers, 77 entropic priors, 78 reference (non-informative) priors obtained through maximization of the missing information, 79 and least informative priors. 80As discussed in the literature, [81][82][83] the principle of maximum entropy can be seen as a method of constructing least informative priors, 84,85 though it was first introduced in statistical mechanics for assigning probabilities.Except in the Jeffreys' 75 prior, almost all of the methods are based on optimization: maximizing or minimizing an objective function, usually an information theoretic one.The least informative prior 80 is found among a restricted set of distributions, whereas the feasible region is a set of convex combinations of certain types of distributions.Zellner 86 proposed several non-informative and informative priors for different problems.All of these methods emphasize the separation of prior knowledge and observed sample data.</p>
<p>A priori knowledge in the form of graphical models (e.g., Markov random fields) has also been widely utilized to either constrain the model space (for example, in covariance matrix estimation in Gaussian graphical models) 87,88 or impose regularization terms. 89In these studies, using a given graphical model illustrating the interactions between variables, different problems have been addressed; e.g., constraints on the matrix structure 87,90,91 or known independencies between variables. 88,92Nonetheless, these studies rely on a fundamental assumption: the given prior knowledge is complete and hence provides one single solution.However, in many applications, the given prior knowledge is uncertain, incomplete, and may contain errors.Therefore, instead of interpreting the prior knowledge as a single solution (e.g., a single deterministic covariance matrix), we aim to construct a prior distribution on an uncertainty class.</p>
<p>CPSP relationships in materials science</p>
<p>A central tenet in the field of materials science and engineering is that the processing history controls the material's internal structure, which, in turn, controls the effective (macroscale) properties or performance characteristics exhibited by the material.Exploration and exploitation of the materials space thus necessitate the generation of CPSP linkages. 93,94Given the multiscale nature of the material' structures, 93 such (abstract) sets of CPSP linkages can be visualized as a large connected and nested network of models that mediate the flow of information about the material's state and behavior up and down the scales.</p>
<p>Any single model in this large network of models can be formally expressed as fðm;4Þ, where m represents the appropriate CPSP variables (i.e., related to process history, material structure, or material property), and 4 denotes variables describing the physics controlling the material phenomenon of interest.Established domain knowledge can be used to construct a prior on 4. Seeking fðm; 4Þ allows us to explicitly capture physics in formulating our ML/AI models.This allows us to use physics-based simulation data to train fðm; 4Þ by independently varying m and 4. Given the enormous challenges associated with the development of concurrent multiscale CPSP relationships, materials analysis tends to be carried out (most of the time) at different, not necessarily strongly coupled scales.At the mesoscale level and beyond (i.e., larger than the atomic scale), several efforts have been made to predict materials' behavior by using datadriven approaches.Most successful efforts at this scale have exploited low-dimensional representation of microstructure information to build effective property models. 95To date, however, there is not much work on the direct use of physical principles to constrain the models used to establish these CPSP linkages.In this regard, more success has been achieved when considering the structure-property connections at the atomic scale.</p>
<p>From the atomic point of view, materials are fundamentally composed of atoms of similar or different types of chemical elements located on real-space sites.The equilibrium atomic structures of materials are reached through the minimization of total energy originated from the complex interaction among ions and electrons in the presence/absence of the external field.It consists of the Coulomb and kinetic energy of electrons and ions and the additional important contributions from quantum mechanical effects, such as (1) exchange energy because of the fermionic spin statistics of electrons, (2) static and dynamical correlation energy beyond the single Slater determinant approximated electronic wave functions, and (3) nuclear quantum effects when tunneling and delocalization of ions become important. 96Recently, a graph convolutional neural network has been applied to describe crystal and molecular structures of materials because atoms and bonds can be perfectly represented by graph nodes and edges, respectively.Recent examples include the crystal graph convolutional neural networks (CGCNN), 97 the improved CGCNN (iCGCNN), 98 the materials graph network (MEGNet), 99 etc.An underlying physical prior hypothesis is the locality of interactions; that is, the physical knowledge of interest can be learned from the local chemical interactions.For example, in the CGCNN, 97 the feature vector v i for atom i is updated via iterative convolution as (Equation 5)
v ðt+1Þ i = v ðtÞ i + X j;k s zðtÞ
where z ðtÞ ði;jÞ k = v ðtÞ i 4v ðtÞ j 4u ði;jÞ k is the concatenated neighbor vector consisting of atom i's feature vector v i , feature vector v j of atom j located on the k-th bond of atom i, and the corresponding bond feature u ði;jÞ k .s is a sigmoid function, and g is a nonlinear softmax activation function.W and b denote the convolution weight matrix and bias of the corresponding layer, respectively.In these convolutional filters, the summation only runs through the local neighboring sites via local coordination determination 97 or Voronoi tessellation. 98The results from these graph convolutional neural network approaches are promising because it is generally true that the physical interaction decreases as the distance of ði; jÞ atom pair (i.e., bond length), increases.This a priori physical knowledge is built inside these graph networks as an implicit constraint.While the bare Coulomb operator decays slowly with 1=r, the destructive interference of electronic wave functions in many-particle systems leads to the nearsightedness of electronic matter in the absence of long-range ionic interactions; 100,101 i.e., local electronic properties, such as electron density, depend mostly on the effective external potential at nearby locations.However, for ionic systems, the long-range Coulomb interaction can have a non-negligible contribution to the total energy and atomic forces even when the ði; jÞ atom pair is separated far away, and further consideration to include these long-range interactions will be of great importance to more accurate describe the physical properties of ionic materials.In addition to these interaction-based physics principles, another important consideration when developing ML methods for materials systems is to make sure that the input feature and the derived descriptor representations should be invariant to the symmetries of the system, such as rotation, reflection, translation, and permutation of atoms of the same species.Kernel-based methods and topological invariants based on group theory have been recently investigated to help improve the accuracy of predictions in the ML modeling of solid state materials. 102ximal knowledge-driven prior (MKDIP) construction Knowledge-driven prior construction utilizes first principles and expert domain knowledge to alleviate the model/data uncertainty and the small sample size issues through constraining the model space or deriving the uncertainty class of models based on physical and chemical constraints.Incorporating scientific knowledge to directly constrain Bayesian predictive models can achieve robust predictions, which would be impossible by using data alone.In materials science, there is a substantial body of knowledge in the form of phenomenological models and physical theories for prior construction.Such knowledge can be used in choosing features or descriptors and constrain the model space for predicting novel materials with desired properties.</p>
<p>To translate more general materials knowledge into Bayesian learning, a general prior construction framework can be developed to map the known physical, chemical, and structural constraints into prior distributions in Bayesian learning.4][105][106] We call the final prior probability constructed via this framework an MKDIP.The new MKDIP construction constitutes two steps: (1) functional information quantification, where prior knowledge manifested as functional relationships is quantified as constraints to regularize the prior probabilities in an information theoretic way, and (2) objectivebased prior selection, where, by combining sample data and prior knowledge, we build an objective function in which the expected mean log likelihood is regularized by the quantified information in step (1).As a special case, where we do not have any sample data, or where there is only one data point available for constructing the prior probability, the proposed framework is reduced to a regularized extension of the maximum entropy principle (MaxEnt). 107y introducing general constraints, which can appear as conditional statements based on expert domain knowledge or physics principles, the idea here is to maximally constrain the model uncertainty with respect to the prior knowledge characterized by these constraints.To give a simple example, assuming that we know a priori, based on physics principles, that certain microstructural properties R for a target material are determined by its composition X, we then can derive the corresponding constraint E p ½H q ðRjXÞ % x, where H q ðRjXÞ denotes the conditional Shannon entropy of R given X under the probabilistic model determined by q.If our prior knowledge is correct, then H q ðRjXÞ/ 0 for any appropriate model.Hence, under the uncertainty characterized by the prior distribution pðqÞ, we aim to derive the MKDIP with the expected conditional entropy as small as possible.Depending on different types of prior knowledge, we can write different forms of such constraints.Specifically, the MKDIP construction integrates materials science and statistical learning by (1) model prior knowledge quantification, where general materials knowledge, from physical theories or expert domain knowledge, is quantified via quantitative constraints or conditional probabilities and (2) optimization, where MKDIP construction requires solving the constrained optimization problems depending on different applications and data types of available observed measurements.When sufficient data exist, we can also split the data for prior construction and for updating the posterior, appropriately integrating prior knowledge and existing data.</p>
<p>In particular, MKDIP aims to derive the solution to the following optimization problem: argmin p ˛PE p ½C q ðx; DÞ; (Equation 6)</p>
<p>where P is the set of all proper priors, and C q ðx; DÞ is a cost function that depends on 1 q, the random vector parameterizing the underlying probability distribution; (2) x, our state of (prior) knowl-edge; and (3) D, partial observations.Alternatively, by parameterizing the prior probability as pðq; gÞ, with g ˛G denoting the hyperparameters, the MKDIP can be found by solving argmin g ˛GE pðq;gÞ ½C q ðx; DÞ: (Equation 7)</p>
<p>We have considered cost functions C q that can be decomposed into three terms: 106
C q ðx; DÞ = l 1 h ð1 À bÞgð1Þq ðgÞ + bgð2Þq ðDÞ i + l 2 g ð3Þ q ðxÞ;
where b, l 1 , and l 2 are non-negative regularization parameters.</p>
<p>Here, g ð1Þ ð $Þ denotes the information-theoretic cost, which can take different forms, including MaxEnt; 107 g ð2Þ ð $Þ is the cost that involves the partially observed data when they are available, including regularized MDIP and regularized expected mean log likelihood prior; 103 and, more critically, g ð3Þ ð $Þ denotes the knowledge-driven constraints that convert prior knowledge into functional constraints to further regularize the prior as detailed in Boluki et al. 106 Using this cost function, we formulate the MKDIP construction problem as the following optimization problem:
argmin g ˛GE q h ð1 À bÞgð1Þq ðgÞ + bgð2Þq ðDÞ i Subject to : E q h gð3Þ
q;i i &gt; 0; i ˛f1; .; n c g;</p>
<p>(Equation 8) where g ð3Þ q;i , ci ˛f1; .; n c g, are constraints resulting from our state of knowledge x via the mapping T : x/E q ½g ð3Þ q;i &gt; 0, ci ˛f1; .; n c g; for example, based on the aforementioned composition-structure relationship E p ½H q ðRjXÞ.The overall MKDIP scheme is illustrated in Figure 3.</p>
<p>In contrast to non-informative priors, MKDIP aims to incorporate the available prior knowledge and uses part of the data to construct an informative prior.While, in theory, the observed data can be entirely used in the optimization problem in Equation 8, in practice one should be cautious to avoid overfitting to the given data.The MKDIP construction here introduces a formal procedure for incorporating prior knowledge.It allows the incorporation of the knowledge of functional relationships and any constraints on the conditional probabilities.Finally, we shall note that deriving the solution to the MKDIP optimization problem Equation 8 can be challenging because of the non-convexity of the objective function and constraints.Nevertheless, feasible and local optimal solutions, especially with the specific distribution families and constraint forms, can be derived. 103tegrating prior knowledge in materials science Xue et al. 52 have applied Bayesian learning and experimental design based on materials knowledge using results from the Landau-Devonshire theory for piezoelectric materials.In particular, a Bayesian regression model, 54 constrained by the Landau functional form and the constraints on morphotropic phase boundaries (MPBs), was developed to guide the design of novel materials with the functional response of interest and to help navigate the search space efficiently so that the desired composition can be achieved in a few trials.The Landau-Devonshire theory has been widely used to reproduce phase diagrams for many piezoelectrics and to investigate their performance at the MPB.The ferroelectric nanodomain phases can be characterized by different polarization vectors, p ! = n !p, where n != ½n 1 ; n 2 ; n 3 T is a unit vector in the direction of polarization, and p is its magnitude. 108The free energy, g, of the ferroelectric system (e.g., BaTiO 3 -based piezoelectrics) can be described by a Landau polynomial that depends on the modulus of the polarization vector (p) and the polarization direction ( n ! ) at a given temperature t:
gðp; n ! ; tÞ = a 2 À n 2 1 + n 2 2 + n 2 3 Á p 2 + b 1 4 À n 2 1 +n 2 2 +n 2 3 Á 2 p 4 + b 2 4 À n 4 1 + n 4 2 + n 4 3 Á p 4 + g 1 6 À n 2 1 +n 2 2 +n 2 3 Á 3 p 6 + g 2 6 À n 6 1 + n 6 2 + n 6 3 Á p 6 + g 3 6 À n 2 1 n 2 2 n 2 3 Á p 6 ;
where the coefficients a, b 0 s, and g 0 s are materials dependent and often determined from experiments; for example, b 2 ðt; xÞ depends on the temperature (t) and composition (x).The MPB is a phase boundary where the two phases (i.e., tetragonal [T] and rhombohedral [R] phases in BaTiO 3 -based piezoelectrics) coexist and have degenerate free energy.Therefore, at MPB (t = t MPB and x = x MPB ), g T = g R , which leads to
b 2 ðt MPB ; x MPB Þ + 24g 2 À g 3 27 p 2 eq ðt MPB ; x MPB Þ = 0;
p 2 eq ðt; xÞ denotes the polarization at equilibrium and has the functional form rðt À t C ðxÞÞ, where r is a constant, and t C ðxÞ is the composition-dependent Curie temperature.Based on these relationships (more details can be found in Xue et al. 52 ), the MPB curve has the following quadratic form:
t MPB ðxÞ = u 1 x 2 + u 2 x + C 1 ;
where u 1 , u 2 , and C 1 are the corresponding model parameters to learn from experimental data.This serves as the prior knowledge to constrain our Bayesian regression model to map the material composition x to the MPB curves.</p>
<p>As illustrated in Figure 4, with the minimal collected data (only 20 characterized BaTiO 3 -based piezoelectrics), the Bayesian regression model with the aforementioned functional constraints provides reliable phase boundaries and faithful uncertainty estimates.More importantly, we demonstrated our approach for finding BaTiO3-based piezoelectrics with the desired target of a vertical When the prior knowledge, including different functional forms and constraints, is available, the MKDIP framework can help take the best advantage of them to explicitly determine the predictive models as well as their corresponding predictors for specific functional responses of interest.Besides such explicit functional-form prior knowledge, which allows us to directly constrain predictive models, the existing prior knowledge on CPSP relationships may simply be in the form of correlation, conditional relationships, and inequality constraints.To enable users, especially materials domain experts, to easily explore and integrate existing phenomenological knowledge into Bayesian learning, infrastructure and friendly user interfaces should be developed to help prior construction via active knowledge acquisition from either materials scientists or even more recent large language foundation models as the unprecedented knowledge base. 9,13The current practice is mostly hand crafted based on different problems and how data scientists work with their collaborating materials scientists.More interfacing efforts between data scientists and materials domain experts are required to achieve more synergistic collaboration in materials science.</p>
<p>Bayesian model averaging (BMA) with experimental design</p>
<p>With a derived surrogate model, we would like to exploit it in combination with experiments to accelerate the development of new materials.However, often, because of incomplete prior knowledge, there are multiple feasible surrogate models within the uncertainty class.We further explore a Bayesian experimental design framework that is capable of adaptively selecting or aggregating competing models connecting materials composition and processing features to performance metrics through BMA. 109,110eview on Bayesian model fusion Bayesian model fusion methods have been studied extensively to achieve better predictive accuracy as well as robust risk and uncertainty estimates. 70,109,111,112There are different Bayesian model ensemble strategies stemming from the Bayes' theorem from Bayesian inference, 70 including Bayesian model selection, Bayesian model combination, and BMA.They all start with an ensemble of candidate models as the uncertainty model class and then update the model posterior probabilities given observed data.The main difference among these different strategies lies in how the updated posterior probabilities guide the way to derive posterior predictive probabilities.For example, Bayesian model selection aims to identify the best predicting model(s) with different criteria, including the Bayesian information criterion (BIC) and Akaike information criterion (AIC). 113,114ayesian model combination often samples the best model subsets based on the updated model posterior, hoping to achieve better convergence. 115In this paper, we focus on BMA, which essentially relies on the weighted ensemble of the models in the uncertainty class by the model posterior. 109,112he theoretical properties of BMA have been studied in the literature.For example, BMA can achieve better prediction performance than any model in the uncertainty class. 109,116The corresponding implementations addressing model uncertainty have also been investigated for more effective and efficient inference procedures. 112MA with MOCU for OED For Bayesian experimental design in general, there can be three categories of objective functions to guide the experimental design.In the first case, we have a parametric model where the parameters come from an underlying physical system.One such example is in biomedicine, where the objective function is the likelihood of the cell being in a cancerous state, given a state-space model based on genetic regulatory pathways. 117nother example is in imaging (for example, for image reconstruction or filtering), where the parameters characterize the image appearance, and the objective function is an error measure between two images.</p>
<p>In the second category, the features are given, and the parameters come from a surrogate model used in place of the actual physical model but are believed to be appropriately related to the physical model.For example, in the materials science applications under ''OED with MOCU,'' the surrogate model is based on the time-dependent Ginzburg-Landau (TDGL) theory and simulates the free energy given dopant parameters, the objective function is the energy dissipation, and the action is to find an optimal dopant and concentration. 5To see how the approach in Dehghannasiri et al. 5 fits the above general theory, the reader can refer to Boluki et al. 118 In the third category, we do not know the physical model, and we lack sufficient knowledge to posit a surrogate model with known features/forms relating to our objective.This case arises in many scenarios where the objective function is a ''black box'' function.Nevertheless, we can adopt a model, albeit one with known predictive properties.This model can be a kernel-based model, such as a GP. 119Moreover, this model can consist of a set of possible parametric families, a kernel-based model with different possible feature sets, or even kernel-based models with different choices for the kernel function.In such scenarios, we do not a priori have any knowledge about which feature set or model family would be the best, and reliable model selection cannot be performed before starting the experiment design loop because of the limited number of observed samples.Considering the average prediction from models based on different feature sets or model families weighted by their posterior probability of being the correct model, namely BMA, is one possible approach.</p>
<p>In the context of materials discovery, we can frame the model averaging problem in a hierarchy to define a family of uncertain model classes in which, for example, different features contribute differently to functional property prediction differently.With such a hierarchical Bayesian model, BMA, essentially weighing all the possible models by their corresponding probability of being the true model, is embedded in BO for OED to realize a system not only capable of autonomously and adaptively learning the surrogate predictive models for the most  promising materials of desired properties but also utilizing the models to efficiently guide exploration of the design space.With more acquired data, the uncertainty of different models will be quantified, and improved predictive models as well as efficient experimental design can be attained.</p>
<p>Again, assume an uncertainty class Q with the probability measure P, characterizing predictive models on a design space X.The experimental design goal is to optimize an objective function f : Q 3 X/R.For example, we want to find a design x ˛X that minimizes an unknown true objective function fðx; q t Þ over X, where q t ˛Q denotes the true model.When there is no strong prior knowledge on functional forms of the objective function, often GP regression (GPR) is adopted and iteratively updated given data from performed experiments D n : Pðfjx; D n Þ $ GPðyjm; KÞ, where fm; Kg denote the corresponding mean and kernel parameters.</p>
<p>To account for potential model uncertainty, BMA can be used for more robust modeling of the objective function: 9)
Pðfjx; D n Þ = X L i = 1 PðijD n ÞPðfjx; D n ; m i ; K i Þ; (Equation
where i is the index of the candidate models in the uncertainty class.</p>
<p>As explained under ''Bayesian learning, UQ, and experimental design,'' a robust design is an element x R ˛X that minimizes the average of the objective function across all possibilities in the uncertainty class relative to a probability distribution governing the corresponding space.This probability at each experimental design iteration is the posterior distribution given the observed data points available up to that step.Mathematically,</p>
<p>x R n = argmin</p>
<p>x ˛X E q ½fðx; qÞjD n ;</p>
<p>(Equation 10)</p>
<p>where D n denotes the observed data till the nth iteration.MOCU in this context can be defined as the average gain in the attained objective between the robust design and the actual optimal designs across the possibilities:
MOCU X n ðQÞ = E q Â f À x R n ; q Á À f À x Ã q ; q Á D n Ã ;
(Equation 11)</p>
<p>where x Ã q denotes the optimal action for a given model parameterized by q, including both GPR parameters and additional parameters from BMA.Note that, if we actually knew the true (correct) model, then we would simply take the optimal design for that model, and MOCU would be 0. Denoting the set of possible experiments by X, the best experiment x Ã n at each time step (in one-step look-ahead scenario) is the one that maximally reduces the expected MOCU following the experiment; i.e.,
x Ã n = argmin x ˛X E x Â E q Â f À x R n+1 ; q Á x; D n ÃÃ À E q Â f À x R n ; q Á D n Ã :
(Equation 12)</p>
<p>In most cases in materials discovery, each experiment is synthesizing the corresponding materials design and measuring its actual properties (or their noisy versions).Thus, the experiment space is equivalent to the design space.</p>
<p>It is beneficial to recognize that MOCU can be viewed as the minimum expected value of a Bayesian loss function, where the Bayesian loss function maps an operator (the materials design in this context) to its differential objective value (for using the given operator instead of an optimal operator), and its minimum expectation is attained by an optimal robust operator that minimizes the average differential objective value.In decision theory, this differential objective value has been referred to as the regret.Under certain conditions, MOCU-based experimental design is, in fact, equivalent to KG and EGO. 118</p>
<p>BMA for materials science applications</p>
<p>We have integrated BMA with the MOCU-based experimental design to deploy an autonomous computational materials discovery framework that is capable of performing optimal sequential computational experiments to find optimal materials and updating the knowledge on materials system model at the same time.One of our recent exercises 120 consisted of implementing the BMA approach for robust selection of computational experiments to optimize properties of the MAX phase crystal system. 121Employing BMA approaches using a set of GPR functions based on different feature sets, we demonstrated that the framework was robust against selection of poor feature sets because the approach considers all the feature sets at once, updating their relative statistical weights according to their ability to predict (successful) outcomes of unrealized simulations.More critically, we have demonstrated the effectiveness of our computational materials discovery platform for single and multiobjective optimization problems.</p>
<p>This framework has been used efficiently for objective-oriented exploration of materials design spaces (MDSs) through computational models and, more importantly to guide experiments by focusing on gathering data in sections of the MDS that will result in the most efficient path to achieving the optimal material within resource budgets.Additionally, the BO approach was successfully combined with BMA for autonomous and adaptive learning, which may be used to autoselect the best models in the MDS, thereby eliminating the requirement of knowing the best model a priori.Thus, this framework constitutes a paradigm shift in the approach to materials discovery by simultaneously (1) accounting for the need to adaptively build increasingly effective models for the accelerated discovery of materials while (2) accounting for the uncertainty in the models themselves.It enables a longdesired seamless connection between computation and experiments, each informing the other, while progressing optimally toward the target material.</p>
<p>In our implementation for MAX phase crystal systems, after training the GPs based on the current and previous observations, solving for the GP hyperparameters to maximize the marginal likelihood of the observed data, each GP provides a Gaussian distribution over the objective function value of each design.Averaging several GPs based on their posterior model probabilities is like mixing weighted Gaussian distributions over the objective value of each design.Based on the sum of weighted Gaussian distributions, the MOCU-based utility function or other acquisition functions, including expected improvement (EI) with a single objective 45 or expected hypervolume improvement (EHVI) with multiobjectives, 122 can be calculated for all possible designs, and the maximizer is chosen as the next experiment.In our experiments, six sets of basic compositional and structural features were chosen a priori without assuming any knowledge of their suitability for the underlying true model that generates data.We have investigated whether the updated model posterior in BMA captured the expected CPSP relationships. 120The goal of experimental design is to discover MAX phases with maximum bulk modulus and minimum shear modulus, which were computed through density functional theory (DFT) calculations 123,124 for 1,500 randomly sampled MAX ternary carbide/nitride crystals.Among these DFT-calculated results, there were 10 MAX phases belonging to the Pareto front when considering the design goals.All of the reported performances of Bayesian experimental design were based on the average values of 1,500 runs starting from the random initial sets of 10 training samples.In Figure 5A, we show the change of the average maximum bulk modulus with the iterations of sequential experimental design.It is clear that, among six models with different features, the feature set F 2 achieves the best experimental design performance because the average maximum bulk modulus is consistently higher than the other models.On the other hand, F 6 has the worst performance.When adopting BMA (either based on first-order or second-order maximum likelihood inference [BMA1 or BMA2, respectively]), it is clear that BMA achieves robust performance even when some models may not have good predictive power (Figure 5B).With the increasing number of iterations, it is also clear that the posterior probability of the best model, F 2 , gets higher (Figure 5C).Last but not least, as shown in Figure 5D, our BMA-based multiobjective experimental design can approach the Pareto front within a small number of sequential design iterations considering the vast MAX ternary carbide/ nitride space.All of these experimental results on the maximization/minimization of mechanical properties of MAX phases suggest that BMA-based model fusion can lead to considerable reduction in the number of experiments/computations that need to be carried out to identify the desired solutions to this specific materials design problem.</p>
<p>Along these directions, we can develop robust Bayesian learning methods by model fusion that exploit correlations among sources/models.Together with a multiinformation source optimization framework driven by scientific knowledge, they will reliably and efficiently identify, given the current knowledge, the next best information source to query and guide the materials design. 125D with MOCU In the context of OED, it has a long history in science and engineering as a properly designed experimental procedure that provides much greater efficiency than simply making random probes.Indeed, Francis Bacon's call for experimental design in 1620 is often taken to be the beginning of modern science.126 MOCU-based OED Because the MOCU 65,66 can be used to quantify the objectivebased uncertainty, it provides an effective means to estimate the expected impact of potential experiments on the objective (i.e., operational goal) through the reduction of model uncertainty.Suppose we are given a set of potential experiments from which the next experiment could be chosen.Which among the possible experiments should be selected if we wish to optimally improve the operational performance of the operator based on the expected experimental outcome?A natural way to select the best possible experiment would be to choose the one that would lead to the minimum expected remaining MOCU after observing its outcome.To be more specific, let x ˛X be an experiment in the experimental design space X.Given x, the MOCU conditioned on this experiment can be computed as
MOCUðQjxÞ = E qjx h C q j Qjx IBR À C q ðj q Þ i ; (Equation 13)
where j</p>
<p>Qjx IBR is the IBR operator that is optimally robust for the uncertainty class of models Qjx that is now conditioned on this experiment x, and the expectation is taken with respect to the conditional distribution pðqjxÞ.The expected remaining MOCU can be evaluated by RðQjxÞ = E x ½MOCUðQjxÞ;</p>
<p>(Equation 14)</p>
<p>and the optimal experiment x Ã is the one that minimizes the expected remaining MOCU in Equation 14 so that it satisfies</p>
<p>x Ã = argmin x ˛X RðQjxÞ:</p>
<p>(Equation 15)</p>
<p>While this strategy does not guarantee that the selected experiment will indeed minimize the uncertainty impacting the objective among all experiments (because the experimental outcome is not known in advance with certainty), it will be optimal on average.Recently, this MOCU-based experimental design scheme has been developed for a variety of systems and applications, which include enhancing the performance of gene-regulatory network intervention with partial network knowledge, 117,127 synchronization of an uncertain Kuramoto model that consists of interconnected oscillators with uncertain interaction strength, 128,129 optimal sequential sampling, 130 Bayesian classification through active learning, 44,131 and robust filtering of uncertain stochastic differential equation (SDE) systems. 42or materials discovery via OED guided by MOCU, as shown in Equation 15, optimization algorithms have to be developed based on the structure of the input design space as well as the properties of the MOCU computation based on different problem settings.For example, if we are investigating pool-based high-throughput screening or discovery problems with a finite set of candidates, either exhaustive search as in typical BO implementations 5,53,131 or dynamic programming algorithms based on KGs 47,132-134 can be developed for solving the optimization problems.When the input design space is continuous and the gradient of MOCU can be estimated, gradient-based local search algorithms can be implemented, as discussed in Zhao et al. 135 There are also other solution strategies that can be used to solve OED guided by MOCU, including sampling and genetic and other evolutionary algorithms. 136igure 6 shows the performance of the MOCU-based OED strategy in reducing the uncertainty that impacts the synchronization cost of a Kuramoto model that consists of 5 oscillators, where the coupling strength between oscillators is uncertain and known only up to a range. 128In this example, an experiment picks an oscillator pair and observes whether the selected oscillator pair is synchronized in the absence of external control.The observation can be used to reduce the range of the uncertain coupling strength between the oscillators.For this Kuramoto model, there exist 5 2 potential experiments in the experimental design space X, and Figure 6 shows how MOCU de-creases as a function of experimental updates.As can be seen, the MOCU-based OED strategy leads to a sharp reduction in uncertainty within a few updates, outperforming random selection (which selects one of the possible experiments from X with uniform probability) or an entropy-based approach (which selects the experiment for the oscillator pair whose coupling strength has the largest uncertain range).</p>
<p>OED for shape memory alloy (SMA)</p>
<p>In materials design, the MOCU-based OED strategy has been applied to a computational problem for shape memory alloy (SMA) design with desired stress-strain profiles for a particular dopant at a given concentration utilizing the TDGL theory. 5The TDGL model simulates the free energy for a specific dopant with a specified concentration, given the dopant's parameters, which is considered an oracle in the experiments.Because the computational complexity of the TDGL model is enormous, an uncertain surrogate model is first trained to approximately predict a dissipation energy for a specified dopant and concentration.In particular, based on TDGL, a reciprocal function is adopted to model the energy dissipation at a specific temperature as a function of dopant potency, dopant spread, and dopant concentration.The experimental design goal is to discover SMAs with the minimum energy dissipation, and therefore this surrogate model is used as the cost function to define MOCU to efficiently guide throughout the experimental design iterations for an optimal dopant and concentration.With the MOCU defined based on this Landau mesoscale surrogate for SMAs as the cost function, the expected remaining MOCU, given the corresponding dopant and its corresponding concentration levels, can be computed by the definition in Equation 14.The optimal experiment can then be determined to minimize the expected remaining MOCU under model uncertainty as in Equation 15.</p>
<p>In the reported experiments, 5 MOCU-based OED was compared with the pure exploitation and random selection policies.Averaged over 10,000 simulations, our MOCU-based OED strategy, which strives to minimize the uncertainty in the model pertaining to the design objective, identified the dopant and concentration with the optimal dissipation after only two iterations on average, while either exploitation or random selection policies cannot find the optimal dopant even after 10 iterations.Getting optimal results after fewer iterations is especially crucial in materials discovery, where measurements by either high-throughput simulation models or synthesis and profiling experiments are expensive and time consuming.</p>
<p>Automatic feature engineering (AFE)</p>
<p>Finally, with accumulated knowledge and data from experimental design based on objective-UQ using MOCU, we may help fill in the missing gap of the understanding in materials systems under study.In materials science, the fundamental paradigm is the existence of causal relationships connecting composition and processing (i.e., the modifications to a material's current state), structure (i.e., the multiscale arrangement of the material), and properties (i.e., the response of the material to an external stimulus); i.e., CPSP relationships.The navigation of this CPSP space is enormously resource intensive, regardless of whether this query is on physical experiments or computational ones.As a result, it typically takes more than 20 years to identify, develop, and finally deploy one material in real-world applications-a key bottleneck for the MGI. 1,3,4Attempting to use physics-agnostic models to build these relationships is limited by the scarcity of the training data itself.Moreover, one would be interested in discovering derived relationships that connect features to properties/behavior because these relationships can further be used to design/discover materials with optimal properties.Besides designing and discovering promising new materials with desired functional properties, identifying critical input features (related to composition, process, structure) that determine function properties as well as principled CPSP relationships can provide a systematic understanding of the underlying physics for different materials systems.Such knowledge can be explored and updated, as illustrated in the previous examples under ''Integrating prior knowledge in materials science'' and ''BMA for materials science applications.''One such knowledge discovery strategy is AFE, which enables us to use physics constraints on learning surrogate models while facilitating discovery of fundamental materials design rules at the same time.</p>
<p>Engineered features obeying physics principles provide valuable interpretability that is critical to help new knowledge discovery and consequent critical decision-making.It is worth noting that, in scientific ML (sciML) involving complex systems, training data tend to be scarce and noisy because obtaining data can be difficult, time consuming, and costly.Materials problems clearly reflect these challenges.</p>
<p>Related work in feature engineering</p>
<p>8][139] Although ''black box'' deep AFE models 140 have shown great potential to improve the corresponding ML algorithm performance, we focus on feature engineering, aiming to derive features based on explicit functional forms in this survey.Desirable feature engineering should attain considerable improvement of prediction performance and generalizability as well as good interpretability with little manual labor.Among the existing methods, deep feature synthesis 141 extracts features based on explicit functional relationships without experts' domain knowledge through stacking multiple primary features and implementing operations or transformations on them, but it suffers from efficiency and scalability problems because of its brute-force way to generate and select features.Kaul et al. 142 proposed Autolearn by regression-based feature learning through mining pairwise feature associations.While it avoids overfitting, to which deep learning-based FE methods are amenable, and improves the efficiency by selecting subsets of engineered features according to stability and information gain, it does not directly produce interpretable features.Khurana et al. 143 introduced Cognito, which formulates the feature engineering problem as a search on the transformation tree with an incremental search strategy to explore the prominent features and later extended the framework by combining reinforcement learning (RL) with a linear functional approximation 144 to improve the efficiency.A similar framework has recently been developed in Zhang et al., 145 where the deep reinforcement learning (DRL) policy is learned on a tree-like transformation graph.It improves the policy learning capability compared with Cognito.However, both frameworks do not explicitly incorporate available prior knowledge into the AFE procedures.</p>
<p>For AFE in materials science applications, we are interested in finding the actuating mechanisms of the materials' functional properties of interest by identifying a set of physically meaningful variables and their relationships. 146Such a set of physical variables with corresponding parameters that uniquely describe the materials' properties of interest can be denoted as ''descriptors.''Discovering descriptors in materials science can help better predict target functional properties with potential interpretability for a given complete class of materials. 147Several methods have been developed, such as a method based on compressed sensing 147 and the more recent Sure Independent Screening and Sparse Operation (SISSO) 148 by brute-force search to generate and select subsets of generated features by sure independent screening 149 together with sparse operators such as least absolute shrinkage and selection operator (LASSO). 150These methods pose a scalability challenge with the exponentially growing memory requirement to store intermediate features and high computational complexity to search for features.</p>
<p>Physics-constrained AFE</p>
<p>In our recently developed AFE framework, 151 a feature generation tree (FGT) was constructed with physics constraints to explore the engineered feature (descriptor) space more efficiently based on first principles, which was demonstrated in several materials problems to be able to take advantage of prior chemical and physical knowledge of the materials systems under study.</p>
<p>Our FGT-based AFE framework focuses on sciML applications, where interpretability is critical to help consequent critical decision-making under data scarcity and uncertainty.Specifically, AFE strategies have been developed by combining FGT exploration with Deep Reinforcement Learning (DRL) 152 to address the interpretability and scalability challenges.Instead of employing a brute-force way to perform algebraic operations on the raw features in a given dataset and then selecting important descriptors, we combine the descriptor generating and selecting processes together by constructing FGTs and developing the corresponding tree exploration policies guided by a deep Q network (DQN).An efficient exploration of the prominent descriptors can be attained in the growing feature space based on the allowed algebraic operations.Our FGT-based AFE strategies construct interpretable descriptors based on a list of operations according to the DRL learned policies, which are more scalable and flexible with the performance-complexity tradeoff with the help of adjustable batch size for generating intermediate features.More critical to materials science and other sciML problems, our FGT provides a flexible framework for incorporating prior knowledge (e.g., physics constraints) to generate and select features.This is important for knowledge discovery via interpretable learning with physics constraints under data scarcity and uncertainty because the space connecting intrinsic materials attributes/features to materials behavior is vast, sparse, and complex in nature.</p>
<p>In particular, let x 0 denote the finite set of p variables as raw or primary features fx 1 0 ; .; x p 0 g and y the target output vector.AFE is to develop an algorithm to construct sets of engineered features as interpretable and predictive descriptors F i = fg 1 ðx 0 ; c 1 Þ; g 2 ðx 0 ; c 2 Þ; .g based on explicit functional forms with allowed algebraic operations that accurately predict y.The set of algebraic operations 4 in an operation set O can be constructed based on prior knowledge; for example, with the following unary and binary operations:
O = fexpð $Þ; logð $Þ; ð$Þ 2 ; ð$Þ 3 ; ð$Þ À 1 ; ffiffi ffi $ p ;
ffiffi ffi $ 3 p ; + ; À ; 3 ; Og.For each function gðx 0 ; cÞ, c denotes the complexity of the corresponding generated descriptor-the number of algebraic operations.For example, the function exp
ðx 0 0 Þ3ðx 1 0 Þ 2 + ffiffiffiffiffiffiffiffi ffi ðx 2 0 Þ
q has a complexity of 5.</p>
<p>The operation set O can be pre-defined based on the prior knowledge about the system under study.If we denote the primary features x 0 by F 0 , then F i denotes the iteratively generated set of descriptors with the maximum allowed complexity c i .Our goal is to find an optimal descriptor set F Ã that maximizes the prediction performance score; for example, by classification or regression accuracy, A L fF 0 ; yg:
F Ã = argmax cf k ˛F;f k ˛FA L fF; yg;
(Equation 16)</p>
<p>where L denotes the prediction model (for example, linear regression or Support Vector Machine (SVM) for interpretability with generated descriptors), and f k is any descriptor (including primary features) in F, the set of all generated features with the maximum allowed complexity c max .</p>
<p>The combinatorial optimization problem in Equation 16 is NP hard.We solve it approximately by introducing the FGT to iteratively construct the descriptor space and transform the problem into a tree search problem for efficient AFE.Each node in the FGT represents a set of descriptors F i , and each edge represents an operation 4. We denote
ðF d Þ Ã = fðf 1 Þ Ã ; ðf 2 Þ Ã ; .; ðf d Þ Ã g
as the top d optimal features when we choose the cardinality of F Ã as d and ðf d Þ Ã as the selected optimal feature for the dth dimension of ðF d Þ Ã .The FGT exploration aims to search for the best descriptors ðf 1 Þ Ã ; ðf 2 Þ Ã ; .one by one based on the testing accuracy given the observed data.The corresponding complete AFE procedure constructs the feature subspace F d sequentially as the search space of each ðf d Þ Ã exploration, starting from the root node F 0 with the primary feature set.At each node F i , we would like to learn a generation policy p to choose an operation 4 i to generate the new descriptor set F j ðj &gt; iÞ as the corresponding child node, with which the current optimal ðF d Þ 0 and A L fðF d Þ 0 ; yg will be updated accordingly.The FGT will grow by repeating the operations above until it reaches the maximum complexity c max .</p>
<p>To learn the FGT generation policy p, we adopt a DQN with experience replay. 152Formally, we define the states, actions and rewards as follows:</p>
<p>d state F d i , denoting a set of primary features or generated descriptors when looking for the dth optimal descriptor; d action pðF d i Þ = 4 i , denoting an operation in the set O; d reward: RðF d i ; 4 i Þ = max F 0 ð1:001 À A L fF 0 ; ygÞ À 1 , where 0 % A L fF 0 ; yg % 1.</p>
<p>The pseudo-code for learning DQN-based FGT exploration is given in Algorithm 1.To have a flexible exploration procedure for performance-complexity trade-off and incorporation of prior knowledge, each ðf d Þ Ã in F Ã can be chosen from the top n features with highest rewards in the corresponding feature subspace F d , composing a candidate set S d .So ðF d À 1 Þ Ã can have multiple combinations according to the whole candidate sets S = fS 1 ; .; S d À 1 g, and F 0 also has multiple combinations according to different ðF d À 1 Þ Ã and f d .Consequently the reward is computed as the maximum reward over F 0 .</p>
<p>Note that when we apply binary operations on F i , beside the one feature in the F i , we have to choose another feature in the end for 18: end for 19: S) Candidate set S d with n features of highest R i 20: end for 21: Output: Optimal feature set F Ã chosen from S generated descriptor space, leading to the exponentially exploding number of new descriptors.To achieve appropriate performance-complexity trade-off, we introduce flexible batch sampling to randomly sample a feature subspace B from F as a ''batch set'' each time and enumerate f s only from B and take the maximum reward from all of the combinations as the reward.When prior knowledge is available as physics constraints on applying corresponding operations to specific feature groups, this batch sampling procedure can naturally take care of them.</p>
<p>AFE to learn interatomic potential models for copper</p>
<p>First-principles DFT 123,124 has been extensively applied in materials science, physics, and chemistry.However, it is often constrained to simulate materials with 100-1,000 atoms for several thousands ab initio molecular dynamics steps, covering about 10 picoseconds.In contrast, classic interatomic potentials have been widely adopted in the past, allowing large-scale molecular dynamics simulation of millions of atoms for millions of time steps (that is, covering &gt;10 nanoseconds).However, the construction of the functional form and the optimization of the corresponding parameters of classic potentials are highly nontrivial.4][155][156] Very recently, a feature engineering method has been pursued where genetic programming was applied to develop fast and accurate classic interatomic potentials with explicit functional forms from physically meaningful hypothesis space. 136Particularly, genetic programming was applied to optimize the exact functional form of pairwise and many-body potentials as well as other potential forms, highlighting an important avenue toward the development of physics-constrained models with analytic functional form.</p>
<p>Different from the above genetic programming approach, we have adopted our FGT-based AFE and evaluated its ability to find potential models from data generated by DFT.To compare with the genetic programming symbolic regression approach, 136 we used the same 150 snapshots of 32-atom DFT molecular dynamics simulations on fcc copper in Hernandez et al., 136 where each snapshot was generated every 100 steps with a time step of 1 fs.We adopted the same 150 snapshots, including 50 snapshots from ab initio molecular dynamics performed at 300 K in the canonical (NVT) ensemble, 50 snapshots at 1,400 K in the NVT ensemble, and 50 snapshots at 1,400 K in the isothermalisobaric (NPT) ensemble with pressure at 100 kPa.The 150 total energies calculated by Hernandez et al. 136 were considered the target output of interest, 157 with the random split of 125 structures and their corresponding total energies for training, and the remaining 125 structures and total energies for validation, for feature engineering.</p>
<p>We have compared our AFE with the recently developed physics-informed genetic programming method 136 to arrive at analytical many-body classical interatomic potential models.Figure 7 shows the plots of predicted total energy of these different copper structures vs. the simulated total energy based on primary features (left), genetic-programming-generated descriptors (center), and AFE-generated descriptors (right) on the same held-out testing data.With the same simulated molecular dynamics data and experimental setup in the paper, our AFE has achieved the total energy prediction with a mean absolute error (MAE) of 3.73 meV/atom within 12 h.By contrast, the reported model GP1 by genetic programming in Hernandez et al. 136 had a prediction MAE of 4.13 meV/atom after 360 CPU hours on the same training and test sets.</p>
<p>Our proposed AFE strategies approximate the expected future reward of engineered descriptors through DQN-based policy learning and replace the exhaustive feature generation by DQNguided FGT exploration considering physics prior knowledge.Consequently, our AFE enhances scalability and computational efficiency without sacrificing prediction performance, as demonstrated in the reported experiment as well as other materials systems in Xiang et al. 151 The results of these real-world materials science experiments have demonstrated the potential of our DQNguided FGT exploration in reducing the runtime and enhancing the scalability for AFE.More importantly, the engineered descriptors are interpretable with the corresponding lists of algebraic operations on the original primary features.Our physics-constrained AFE aims at generalizable learning under data scarcity and uncertainty.Interpretable instead of ''black box'' learning helps new knowledge discovery and better decision-making.</p>
<p>Conclusions and future work</p>
<p>When facing real-world complex systems in various science and engineering domains-such as complex materials systems, which are our focus in this paper-where acquisition of ample data is practically difficult and formidably expensive, currently existing ML methods fail to produce reliable and generalizable predictions.To cope with the current shortcomings of existing SP and ML schemes in materials discovery, there is a pressing need for novel methods that enable robust optimal decisionmaking under challenging conditions, such as small data size, enormous system complexity, nonstationarity, as well as data and model uncertainty.In this paper, we have presented recent efforts in knowledge-driven learning, optimization, and experimental design, and we have also provided their historical context against the rich research history in the SP community revolving around robust filtering and control, which can probably be dated back to the 1950s.Specifically, we have shown several examples in an objective-based UQ framework-via MOCU-to develop sciML methods to address the aforementioned challenges in accelerating materials discovery, focusing on learning and experimental design under uncertainty.The problems of studying complex systems will persist in diverse science and engineering disciplines, and we expect that the learning and optimization schemes based on objective-based UQ presented in this paper would provide a useful guideline for developing new sciML methods that more effectively incorporate scientific knowledge, design surrogate ML models that are better suited for the given systems under study, and devise computational solutions that are more scalable and efficient.</p>
<p>Figure 1 .
1
Figure 1.Illustration of the knowledge-driven optimal experimental design (OED) cycle for materials discovery</p>
<p>Figure 2 .
2
Figure 2. Illustration of the historical context of the intrinsically Bayesian robust (IBR) framework and the concept of mean objective cost of uncertainty (MOCU)</p>
<p>k W ðtÞ s + b ðtÞ s ;</p>
<p>Figure 3 .
3
Figure 3. Illustration of knowledge-based prior construction via MKDIP</p>
<p>Figure 4 .
4
Figure 4. Bayesian learning and experimental design constrained by the Landau functional for discovery of BaTiO 3 -based piezoelectrics as described in the text Shown are predicted (solid lines) and experimental (dots) phase diagrams for BZT-m50-n30, together with uncertainty estimates, from Bayesian regression.The solid lines show the mean phase boundaries, and the dashed lines mark the 95% confidence intervals.Notice the uncertainty reduction given more data.</p>
<p>Figure 5 .
5
Figure 5. Bayesian experimental design with BMA for MAX phases as described in the text (A) The change of average maximum bulk modulus for the original six feature sets with the number of design iterations.(B) The change of average maximum bulk modulus comparing BMA surrogates with the best and worst feature sets.(C) The change of posterior model probabilities corresponding to six feature sets.(D) The average number of sampled Pareto front points when considering bulk modulus and shear modulus.</p>
<p>Figure 6 .
6
Figure 6.Experimental design results based on a 5-oscillator Kuramoto model with uncertain coupling strength between the as described in the textThe MOCU-based OED scheme quickly reduces the model uncertainty that impacts the performance.</p>
<p>Figure 7 .
7
Figure 7. Copper energy regression results for different interatomic potential models</p>
<p>Patterns 4, November 10, 2023 Review
Patterns 4, November 10, 2023 3 Review
Patterns 4, November 10, 2023Review
Patterns 4, November 10, 2023 5
Patterns 4, November 10, 2023 Review
Patterns 4, November 10, 2023 7 Review
Patterns 4, November 10, 2023Review
Patterns 4, November 10, 2023 9Review
Patterns 4, November 10, 2023 Review
Patterns 4, November 10, 2023 13 Review
Patterns 4, November 10, 2023 15
Patterns 4, November 10, 2023 17 Review
Patterns 4, November 10, 2023 Review
Patterns 4, November 10, 2023 Review
ACKNOWLEDGMENTSThe authors would like to thank the collaborators involved in the reviewed projects: Shahin Boluki, Guang Zhao, Mingzhou Fan, Ziyu Xiang, Tao Hu, Roozbeh Dehghannasiri, Nathan Wilson, and Daniel Willhelm.The authors were supported in part by National Science Foundation (NSF) awards CCF-1553281, IIS-1812641, OAC-1835690, DMR-1753054, DMR-2119103, CMMI-2226908, SHF-2215573, and IIS-2212419 as well as by the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, Mathematical Multifaceted Integrated Capability Centers program under award DE-SC0019303.DECLARATION OF INTERESTSThe authors declare no competing interests.
CALPHAD, first and second generation-Birth of the materials genome. L Kaufman, J ˚gren, 10.1016/j.scriptamat.2012.12.003Scripta Mater. 702014</p>
<p>The materials innovation ecosystem: a key enabler for the materials genome initiative. D L Mcdowell, S R Kalidindi, 10.1557/mrs.2016.61MRS Bull. 412016</p>
<p>Big data of materials science: Critical role of the descriptor. L M Ghiringhelli, J Vybiral, S V Levchenko, C Draxl, M Scheffler, 10.1103/PhysRevLett.114.105503Phys. Rev. Lett. 1141055032015</p>
<p>From organized highthroughput data to phenomenological theory using machine learning: The example of dielectric breakdown. C Kim, G Pilania, R Ramprasad, 10.1021/acs.chemmater.5b04109Chem. Mater. 282016</p>
<p>Optimal experimental design for materials discovery. R Dehghannasiri, D Xue, P V Balachandran, M R Yousefi, L A Dalton, T Lookman, E R Dougherty, 10.1016/j.commatsci.2016.11.041Comput. Mater. Sci. 1292017</p>
<p>Handbook of Materials Modeling. S Yip, 10.1007/978-1-4020-3286-82005Springer</p>
<p>On scientific understanding with artificial intelligence. M Krenn, R Pollice, S Y Guo, M Aldeghi, A Cervera-Lierta, P Friederich, G Dos Passos Gomes, F H€ Ase, A Jinich, A K Nigam, 10.1038/s42254-022-00518-3Nat. Rev. Phys. 42022</p>
<p>Scientific discovery in the age of artificial intelligence. H Wang, T Fu, Y Du, W Gao, K Huang, Z Liu, P Chandak, S Liu, P Van Katwyk, A Deac, 10.1038/s41586-023-06221-2Nature. 6202023</p>
<p>Artificial intelligence for science in quantum, atomistic, and continuum systems. X Zhang, L Wang, H Jacob, Y Luo, C Fu, Y Xie, M Liu, Y Lin, X Zhao, K Yan, 10.48550/arXiv.2307.084232023Preprint at arXiv</p>
<p>Artificial Intelligence for Science: A Deep Learning Revolution. A Choudhary, G Fox, T Hey, 10.1142/131232023World Scientific Publishing CoPte Ltd</p>
<p>A perspective on Bayesian methods applied to materials discovery and design. R Arro ´yave, D Khatamsaz, B Vela, R Couperthwaite, A Molkeri, P Singh, D D Johnson, X Qian, A Srivastava, D Allaire, 10.1557/s43579-022-00288-0MRS Communications. 122022</p>
<p>Deep generative models for materials discovery and machine learning-accelerated innovation. A S Fuhr, B G Sumpter, 10.3389/fmats.2022.865270Front. Mater. 98652702022</p>
<p>Accelerating materials discovery using artificial intelligence, high performance computing and robotics. E O Pyzer-Knapp, J W Pitera, P W J Staar, S Takeda, T Laino, D P Sanders, J Sexton, J R Smith, A Curioni, 10.1038/s41524-022-00765-zComput. Mater. 8842022</p>
<p>Extrapolation, Interpolation, and Smoothing of Stationary Time Series. N Wiener, 10.7551/mitpress/2946.001.00011949Wiley</p>
<p>T Kailath, A H Sayed, B Hassibi, Linear Estimation. Prentice-Hall2000</p>
<p>Practical Methods for Optimal Control Using Nonlinear Programming. J T Betts, 10.1137/1.97808987185772010SIAM Press2nd ed.</p>
<p>L Dalton, E R Dougherty, Optimal Bayesian Classification. SPIE Press2020</p>
<p>The fundamental risk quadrangle in risk management, optimization and statistical estimation. R T Rockafellar, S Uryasev, 10.1016/j.sorms.2013.03.001Surveys in Operations Research and Management Science. 182013</p>
<p>Goal-oriented optimal approximations of Bayesian linear inverse problems. A Spantini, T Cui, K Willcox, L Tenorio, Y Marzouk, 10.1137/16m1082123SIAM J. Sci. Comput. 392017</p>
<p>Stable detection when signal and spectrum of normal noise are inaccurately known. V P Kuznetsov, Telecommun. Radio Eng. 301976</p>
<p>Robust Wiener filters. S A Kassam, T L Lim, 10.1016/0016-0032(77)90011-4J. Franklin Inst. 3041977</p>
<p>Poor. On robust Wiener filtering. H Poor, 10.1109/TAC.1980.1102349IEEE Trans. Automat. Control. 251980</p>
<p>Minimax robust deconvolution filters under stochastic parametric and noise uncertainties. Y Chen, B Chen, 10.1109/78.258119IEEE Trans. Signal Process. 421994</p>
<p>Minimax linear observers and regulators for stochastic systems with uncertain second-order statistics. S Verdu, H Poor, 10.1109/TAC.1984.1103576IEEE Trans. Automat. Control. 291984</p>
<p>Minimax Gaussian classification &amp; clustering. T Li, X Yi, C Carmanis, P Ravikumar, Proceedings of the 20th International Review Conference on Artificial Intelligence and Statistics. J Singh, Zhu, the 20th International Review Conference on Artificial Intelligence and Statistics201754</p>
<p>Stable regression: On the power of optimization over randomization. D Bertsimas, I Paskov, J. Mach. Learn. Res. 212020</p>
<p>New results in linear filtering and prediction theory. R E Kalman, R S Bucy, 10.1115/1.3658902J. Basic Eng. 831961</p>
<p>Approaches to adaptive filtering. R Mehra, 10.1109/TAC.1972.1100100IEEE Trans. Automat. Control. 171972</p>
<p>The Kalman filter: A robust estimator for some classes of linear quadratic problems. J Morris, 10.1109/TIT.1976.1055611IEEE Trans. Inf. Theor. 221976</p>
<p>Dynamic programming and adaptive processes: Mathematical foundation. R Bellman, R Kalaba, 10.1109/TAC.1960.6429288IRE Trans. Automatic Control. 51960</p>
<p>Markovian Decision Processes with Uncertain Transition Probabilities or Rewards (MIT Cambridge Operations Research Center). E A Silver, 1963Technical report</p>
<p>Bayesian Decision Problems and Markov Chains. J J Martin, 1967Wiley</p>
<p>Bayesian robust optimal linear filters. Signal Process. A M Grigoryan, E R Dougherty, 10.1016/S0165-1684(01)00144-X200181144</p>
<p>E R Dougherty, J Hua, Z Xiong, Y Chen, 10.1016/j.patcog.2005.01.019Optimal robust classifiers. Pattern Recogn. 200538</p>
<p>C Anthony, Atkinson , 10.1002/9781118445112.stat04090.pub2Optimal Design. American Cancer Society2015</p>
<p>Maximum entropy sampling and optimal Bayesian experimental design. P Sebastiani, H P Wynn, 10.1111/1467-9868.00225J. Roy. Stat. Soc. B. 622000</p>
<p>On the relationship between data efficiency and error for uncertainty sampling Jennifer Dy and Andreas Krause. S Mussmann, P Liang, Proceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine LearningPMLR201880of Proceedings of Machine Learning Research (Stockholmsm€ assan)</p>
<p>Optimal Design for Nonlinear Response Models. V V Fedorov, S L Leonov, 10.1201/b150542014Chapman and Hall/CRC Press</p>
<p>A semi-infinite programming based algorithm for determining t-optimum designs for model discrimination. B P M Duarte, W K Wong, A C Atkinson, 10.1016/j.jmva.2014.11.006J. Multivariate Anal. 1352015</p>
<p>V Dennis, Lindley. Bayesian Statistics, A Review (SIAM). 1972</p>
<p>Gradient-based stochastic optimization methods in Bayesian experimental design. X Huan, Y Marzouk, 10.1615/int.j.uncertaintyquantification.2014006730Int. J. Uncertain. Quantification. 42014. 2014006730</p>
<p>Model-based robust filtering and experimental design for stochastic differential equation systems. G Zhao, X Qian, B.-J Yoon, F J Alexander, E R Dougherty, 10.1109/TSP.2020.3001384IEEE Trans. Signal Process. 682020</p>
<p>A unified stochastic gradient approach to designing Bayesian-optimal experiments. A Foster, J Martin, M O'meara, W T Yee, T Rainforth, Proceedings of the 23th International Conference on Artificial Intelligence and Statistics (AISTATS). the 23th International Conference on Artificial Intelligence and Statistics (AISTATS)2020108</p>
<p>Uncertainty-aware active learning for optimal Bayesian classifier. G Zhao, E Dougherty, B.-J Yoon, F Alexander, X Qian, 9th International Conference on Learning Representations (ICLR). 2021</p>
<p>Efficient global optimization of expensive black-box functions. D R Jones, M Schonlau, W J Welch, 10.1023/a:1008306431147J. Global Optim. 131998</p>
<p>A knowledge-gradient policy for sequential information collection. P I Frazier, W B Powell, S Dayanik, 10.1137/070693424SIAM J. Control Optim. 472008</p>
<p>The knowledge-gradient policy for correlated normal beliefs. P Frazier, W Powell, S Dayanik, 10.1287/ijoc.1080.0314Inf. J. Comput. 212009</p>
<p>Learning to perform physics experiments via deep reinforcement learning. M Denil, P Agrawal, T D Kulkarni, E Tom, P Battaglia, N Freitas, International Conference on Learning Representations (ICLR). 2017</p>
<p>P4U: A high performance computing framework for Bayesian uncertainty quantification of complex models. P E Hadjidoukas, P Angelikopoulos, C Papadimitriou, P Koumoutsakos, Journal of Computational Physics. 2842015</p>
<p>A Multilevel Parallel Object-Oriented Framework for Design Optimization, Parameter Estimation, Uncertainty Quantification, and Sensitivity Analysis: Version 6.15 User's Manual. B M Adams, W J Bohnhoff, K R Dalbey, M S Ebeida, J P Eddy, M S Eldred, R W Hooper, P D Hough, K T Hu, J D Jakeman, SAND2020-124952021SandiaTechnical Report</p>
<p>Accelerated search for materials with targeted properties by adaptive design. D Xue, P V Balachandran, J Hogden, J Theiler, D Xue, T Lookman, 10.1038/ncomms11241Nat. Commun. 7112412016</p>
<p>Accelerated search for BaTiO 3 -based piezoelectrics with vertical morphotropic phase boundary using Bayesian learning. D Xue, P V Balachandran, R Yuan, T Hu, X Qian, E R Dougherty, T Lookman, 10.1073/pnas.1607412113Proc. Natl. Acad. Sci. USA. 1132016</p>
<p>Bayesian optimization for materials design. I F Peter, J Wang, 10.1007/978-3-319-23871-5_3Information Science for Materials Discovery and Design. Springer2016</p>
<p>Bayesian regression with network prior: Optimal Bayesian filtering perspective. X Qian, E R Dougherty, 10.1109/TSP.2016.2605072IEEE Trans. Signal Process. 642016</p>
<p>COMBO: An efficient bayesian optimization library for materials science. T Ueno, T D Rhone, Z Hou, T Mizoguchi, K Tsuda, 10.1016/j.md.2016.04.001Materials Discovery. 42016</p>
<p>Machine learning with systematic density-functional theory calculations: Application to melting temperatures of single-and binary-component solids. A Seko, T Maekawa, K Tsuda, I Tanaka, 10.1103/PhysRevB.89.054303Phys. Rev. B. 89543032014</p>
<p>Designing nanostructures for phonon transport via Bayesian optimization. S Ju, T Shiga, L Feng, Z Hou, K Tsuda, J Shiomi, 10.1103/PhysRevX.7.021024Phys. Rev. X. 7210242017</p>
<p>Multi-objective optimization for materials discovery via adaptive design. A M Gopakumar, P V Balachandran, D Xue, J E Gubernatis, T Lookman, 10.1038/s41598-018-21936-3Sci. Rep. 837382018</p>
<p>Adaptive active subspace-based efficient multifidelity materials design. D Khatamsaz, A Molkeri, R Couperthwaite, J James, R Arro ´yave, A Srivastava, D Allaire, 10.1016/j.matdes.2021.110001Mater. Des. 2092021. 110001</p>
<p>Bayesian estimation of single ply anisotropic elastic constants from spherical indentations on multi-laminate polymer-matrix fiber-reinforced composite samples. A R Castillo, S R Kalidindi, 10.1007/s11012-020-01154-wMeccanica. 562021</p>
<p>Autonomous development of a machine-learning model for the plastic response of two-phase composites from micromechanical finite element models. A Marshall, S R Kalidindi, 10.1007/s11837-021-04696-wJOM. 732021</p>
<p>A top-down characterization of NiTi single-crystal inelastic properties within confidence bounds through Bayesian inference. P Honarmandi, M A Hossain, R Arroyave, T Baxevanis, 10.1007/s40830-021-00311-8Shap. Mem. Superelasticity. 72021</p>
<p>Bayesian learning of thermodynamic integration and numerical convergence for accurate phase diagrams. V Ladygin, I Beniya, E Makarov, A Shapeev, 10.1103/PhysRevB.104.104102Phys. Rev. B. 1041041022021</p>
<p>Bayesian neural networks for uncertainty quantification in data-driven materials modeling. A Olivier, M D Shields, L Graham-Brady, 10.1016/j.cma.2021.114079Comput. Methods Appl. Mech. Eng. 3861140792021</p>
<p>Quantifying the objective cost of uncertainty in complex dynamical systems. B.-J Yoon, X Qian, E R Dougherty, 10.1109/TSP.2013.2251336IEEE Trans. Signal Process. 612013</p>
<p>Quantifying the multiobjective cost of uncertainty. B.-J Yoon, X Qian, E R Dougherty, 10.1109/ACCESS.2021.3085486IEEE Access. 92021</p>
<p>Intrinsically optimal Bayesian robust filtering. L A Dalton, E R Dougherty, 10.1109/TSP.2013.2291213IEEE Trans. Signal Process. 622014</p>
<p>G E P Box, G C Tiao, 10.1002/9781118033197Bayesian Inference in Statistical Analysis. Wiley1973</p>
<p>Statistical Decision Theory and Bayesian Analysis. J O Berger, 10.1007/978-1-4757-4286-21985Springer-Verlag</p>
<p>M Christopher, Bishop. Pattern Recognition and Machine Learning. Springer2006</p>
<p>The illusion of distribution-free small-sample classification in genomics. E R Dougherty, A Zollanvari, U M Braga-Neto, 10.2174/138920211796429763Curr. Genom. 122011</p>
<p>Scientific knowledge is possible with small-sample classification. E R Dougherty, L A Dalton, 10.1186/1687-4153-2013-10EURASIP J. Bioinf. Syst. Biol. 2013. 2013</p>
<p>Big data need big theory too. P V Coveney, E R Dougherty, R R Highfield, 10.1098/rsta.2016.0153Phil. Trans. R. Soc. A. 3742016. 20160153</p>
<p>What Is the Question?. E T Jaynes, 1980Bayesian Statistics</p>
<p>An invariant form for the prior probability in estimation problems. H Jeffreys, 10.1098/rspa.1946.0056Proc. R. Soc. Lond. A Math. Phys. Sci. 1861946</p>
<p>Department of Economics. Past and Recent Results on Maximal Data Information Priors. A Zellner, Working Paper Series in Economics and Econometrics. 1995University of Chicago, Graduate School of Business, Department of Economics)University of Chicago</p>
<p>A universal prior for integers and estimation by minimum description length. J Rissanen, 10.1214/aos/1176346150Ann. Stat. 111983</p>
<p>Entropic priors for discrete probabilistic networks and for mixtures of Gaussian models. C C Rodriguez, 10.1063/1.1477063AIP Conf. Proc. 2002</p>
<p>On the development of reference priors. J O Berger, J M Bernardo, Bayesian statistics. 41992</p>
<p>Least-informative Bayesian prior distributions for finite samples based on information theory. J C Spall, S D Hill, 10.1109/CDC.1989.70640IEEE Trans. Automat. Control. 351990</p>
<p>Reference posterior distributions for Bayesian inference. J M Bernardo, 10.1111/j.2517-6161.1979.tb01066.xJ. Roy. Stat. Soc. B. 411979</p>
<p>The selection of prior distributions by formal rules. R E Kass, L Wasserman, 10.1080/01621459.1996.10477003J. Am. Stat. Assoc. 911996</p>
<p>Objective priors for discrete parameter spaces. J O Berger, J M Bernardo, D Sun, 10.1080/01621459.2012.682538J. Am. Stat. Assoc. 1072012</p>
<p>Information theory and statistical mechanics. E T Jaynes, 10.1103/PhysRev.106.620Phys. Rev. 1061957</p>
<p>Jaynes. Prior probabilities. E Jaynes, 10.1109/TSSC.1968.300117IEEE Trans. Syst. Sci. Cybern. 41968</p>
<p>Models, prior information, and Bayesian analysis. A Zellner, 10.1016/0304-4076(95)01768-2J. Econom. 751996</p>
<p>On estimation of covariance matrices with kronecker product structure. K Werner, M Jansson, P Stoica, 10.1109/TSP.2007.907834IEEE Trans. Signal Process. 562008</p>
<p>Covariance estimation in decomposable Gaussian graphical models. A Wiesel, Y C Eldar, A O Hero, 10.1109/TSP.2009.2037350IEEE Trans. Signal Process. 582010</p>
<p>Generalized SURE for exponential families: Applications to regularization. Y C Eldar, 10.1109/TSP.2008.2008212IEEE Trans. Signal Process. 572009</p>
<p>Estimation of structured covariance matrices. J P Burg, D G Luenberger, D L Wenger, 10.1109/PROC.1982.12427Proc. IEEE. IEEE198270</p>
<p>Bayesian joint modeling of multiple gene networks and diverse genomic data to identify target genes of a transcription factor. P Wei, W Pan, 10.1214/11-aoas502Ann. Appl. Stat. 62012</p>
<p>Distributed covariance estimation in Gaussian graphical models. A Wiesel, A O Hero, IEEE Sensor Array and Multichannel Signal Processing Workshop. SAM2010. 2010IEEE</p>
<p>S R Kalidindi, 10.1016/C2012-0-07337-1Hierarchical Materials Informatics: Novel Analytics for Materials Data. Elsevier2015</p>
<p>Multi-information source fusion and optimization to realize ICME: Application to dual-phase materials. S F Ghoreishi, A Molkeri, A Srivastava, R Arroyave, D Allaire, 10.1115/1.4041034J. Mech. Des. N. Y. 1402018</p>
<p>A Bayesian framework for materials knowledge systems. S R Kalidindi, 10.1557/mrc.2019.56MRS Communications. 92019</p>
<p>Nuclear quantum effects enter the mainstream. T E Markland, M Ceriotti, Michele Markland, Ceriotti, 10.1038/s41570-017-0109Nat. Rev. Chem. 21092018</p>
<p>Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties. T Xie, J C Grossman, 10.1103/PhysRevLett.120.145301Phys. Rev. Lett. 1201453012018</p>
<p>Developing an improved crystal graph convolutional neural network framework for accelerated materials discovery. C W Park, C Wolverton, 10.1103/PhysRevMaterials.4.063801Phys. Rev. Mater. 463801Jun 2020</p>
<p>Graph networks as a universal machine learning framework for molecules and crystals. C Chen, W Ye, Y Zuo, C Zheng, S P Ong, 10.1021/acs.chemmater.9b01294Chem. Mater. 312019</p>
<p>Density functional and density matrix method scaling linearly with the number of atoms. W Kohn, 10.1103/PhysRevLett.76.3168Phys. Rev. Lett. 761996</p>
<p>Nearsightedness of electronic matter. E Prodan, W Kohn, 10.1073/pnas.0505436102Proc. Natl. Acad. Sci. USA. 1022005</p>
<p>Recent advances and applications of machine learning in solid-state materials science. J Schmidt, M R G Marques, S Botti, M A L Marques, 10.1038/s41524-019-0221-0Comput. Mater. 52019</p>
<p>Incorporation of biological pathway knowledge in the construction of priors for optimal Bayesian classification. M S Esfahani, E R Dougherty, 10.1109/TCBB.2013.143IEEE ACM Trans. Comput. Biol. Bioinf. 112014</p>
<p>An optimization-based framework for the transformation of incomplete biological knowledge into a probabilistic structure and its application to the utilization of gene/protein signaling pathways in discrete phenotype classification. M S Esfahani, E Shahrokh, E R Dougherty, 10.1109/TCBB.2015.2424407IEEE ACM Trans. Comput. Biol. Bioinf. 122015</p>
<p>Constructing pathway-based priors within a constructing pathway-based priors within a Gaussian mixture model for Bayesian regression and classification. S Boluki, M S Esfahani, X Qian, E R Dougherty, 10.1109/TCBB.2017.2778715IEEE ACM Trans. Comput. Biol. Bioinf. 162019</p>
<p>Incorporating biological prior knowledge for Bayesian learning via maximal knowledge-driven information priors. S Boluki, M S Esfahani, X Qian, E R Dougherty, 10.1186/s12859-017-1893-42017bBMC Bioinf18552Suppl 14</p>
<p>The principle of maximum entropy. S Guiasu, A Shenitzer, 10.1007/bf03023004Math. Intel. 71985</p>
<p>Thermodynamics of ferroelectric solid solutions with morphotropic phase boundaries. A A Heitmann, G A Rossetti, 10.1111/jace.12979J. Am. Ceram. Soc. 972014</p>
<p>Bayesian model averaging: A tutorial. J Hoeting, D Madigan, A Raftery, C Volinsky, Stat. Sci. 41999</p>
<p>Bayesian model selection and model averaging. L Wasserman, 10.1006/jmps.1999.1278J. Math. Psychol. 442000</p>
<p>Comparing Bayes model averaging and stacking when model approximation error cannot be ignored. B Clarke, 10.1162/153244304773936090J. Mach. Learn. Res. 42003</p>
<p>Bayesian adaptive sampling for variable selection and model averaging. M A Clyde, J Ghosh, M L Littman, 10.1198/jcgs.2010.09049J. Comput. Graph Stat. 202011</p>
<p>Calibration and empirical Bayes variable selection. E George, D Foster, 10.1093/biomet/87.4.731Biometrika. 872000</p>
<p>Regression with multiple candidate models: Selecting or mixing?. Y Yang, Stat. Sin. 132003</p>
<p>Turning Bayesian model averaging into Bayesian model combination. K Monteith, J L Carroll, K Seppi, T Martinez, 10.1109/IJCNN.2011.6033566The 2011 International Joint Conference on Neural Networks. 2011</p>
<p>Model selection and accounting for model uncertainty in graphical models using Occam's window. D Madigan, A E Raftery, 10.2307/2291017J. Am. Stat. Assoc. 891994</p>
<p>Optimal experimental design for gene regulatory networks in the presence of uncertainty. R Dehghannasiri, B.-J Yoon, E R Dougherty, 10.1109/TCBB.2014.2377733IEEE ACM Trans. Comput. Biol. Bioinf. 122015</p>
<p>Experimental design via generalized mean objective cost of uncertainty. S Boluki, X Qian, E R Dougherty, 10.1109/ACCESS.2018.2886576IEEE Access. 72019</p>
<p>C E Rasmussen, C K I Williams, 10.7551/mitpress/3206.001.0001Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning. The MIT Press2005</p>
<p>Autonomous efficient experiment design for materials discovery with Bayesian model averaging. A Talapatra, S Boluki, T Duong, X Qian, E Dougherty, R Arro ´yave, 10.1103/PhysRevMaterials.2.113803Phys. Rev. Mater. 21138032018</p>
<p>M W Barsoum, 10.1002/9783527654581MAX Phases: Properties of Machinable Ternary Carbides and Nitrides. John Wiley &amp; Sons2013</p>
<p>Hypervolumebased expected improvement: Monotonicity properties and exact computation. M T M Emmerich, ´ Andre, H D , Jan , W K , IEEE Congress of Evolutionary Computation. 2011. 2011IEEE</p>
<p>Inhomogeneous electron gas. P Hohenberg, W Kohn, 10.1103/PhysRev.136.B864Phys. Rev. 1361964</p>
<p>Self-consistent equations including exchange and correlation effects. W Kohn, L J Sham, 10.1103/PhysRev.140.A1133Phys. Rev. 1401965</p>
<p>Experiment design frameworks for accelerated discovery of targeted materials across scales. A Talapatra, S Boluki, P Honarmandi, A Solomou, G Zhao, S F Ghoreishi, A Molkeri, D Allaire, A Srivastava, X Qian, 10.3389/fmats.2019.00082Front. Mater. 6822019</p>
<p>F Bacon, 10.1017/CBO9781139164030.006The New Organon. Cambridge University Press2000</p>
<p>Efficient experimental design for uncertainty reduction in gene regulatory networks. R Dehghannasiri, B.-J Yoon, E R Dougherty, 10.1186/1471-2105-16-S13-S2S2. BMC Bioinf201516</p>
<p>Optimal experimental design for uncertain systems based on coupled differential equations. Y Hong, B Kwon, B.-J Yoon, 10.1109/ACCESS.2021.3071038IEEE Access. 92021</p>
<p>Accelerating optimal experimental design for robust synchronization of uncertain kuramoto oscillator model using machine learning. H.-M Woo, Y Hong, B Kwon, B.-J Yoon, 10.1109/TSP.2021.3130967IEEE Trans. Signal Process. 692021</p>
<p>Discrete optimal Bayesian classification with error-conditioned sequential sampling. Pattern Recogn. A Broumand, M S Esfahani, B.-J Yoon, E R Dougherty, 10.1016/j.patcog.2015.03.023201548</p>
<p>Bayesian active learning by soft mean objective cost of uncertainty. G Zhao, E Dougherty, B.-J Yoon, F Alexander, X Qian, 24th International Conference on Artificial Intelligence and Statistics (AISTATS). 2021</p>
<p>Sequential DOE via dynamic programming. IIE Trans. I Ben-Gal, M Caramanis, 10.1023/A:1019670414725200234</p>
<p>Powell. Approximate Dynamic Programming: Solving the Curses of Dimensionality. B Warren, 10.1002/97811180291762011John Wiley &amp; Sons, Inc2nd edition</p>
<p>Sequential Bayesian optimal experimental design via approximate dynamic programming. X Huan, Y M Marzouk, 10.48550/arXiv.1604.083202016Preprint at arXiv</p>
<p>Efficient active learning for Gaussian process classification by error reduction. G Zhao, E Dougherty, B.-J Yoon, F Alexander, X Qian, 35th Conference on Neural Information Processing Systems (NeurIPS). 2021</p>
<p>Fast, accurate, and transferable many-body interatomic potentials by symbolic regression. A Hernandez, A Balasubramanian, F Yuan, S A M Mason, T Mueller, 10.1038/s41524-019-0249-1Comput. Mater. 51122019</p>
<p>Dictionaries for sparse representation modeling. R Rubinstein, A M Bruckstein, M Elad, 10.1109/JPROC.2010.2040551Proc. IEEE. IEEE201098</p>
<p>Representation learning: A review and new perspectives. Y Bengio, A Courville, P Vincent, 10.1109/TPAMI.2013.50IEEE Trans. Pattern Anal. Mach. Intell. 352013</p>
<p>A survey of multi-view representation learning. Y Li, M Yang, Z Zhang, 10.1109/TKDE.2018.2872063IEEE Trans. Knowl. Data Eng. 312019</p>
<p>Deep learning-based feature engineering for stock price movement prediction. W Long, Z Lu, L Cui, 10.1016/j.knosys.2018.10.034Knowl. Base Syst. 1642019</p>
<p>Deep feature synthesis: Towards automating data science endeavors. M K James, K Veeramachaneni, 10.1109/DSAA.2015.73448582015 IEEE International Conference on Data Science and Advanced Analytics (DSAA) (IEEE). 2015</p>
<p>Autolearn-automated feature generation and selection. A Kaul, S Maheshwary, V Pudi, 2017 IEEE International Conference on Data Mining (ICDM) (IEEE). 2017</p>
<p>Cognito: Automated feature engineering for supervised learning. U Khurana, D Turaga, H Samulowitz, P Srinivasan, 2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW) (IEEE). 2016</p>
<p>Feature engineering for predictive modeling using reinforcement learning. U Khurana, H Samulowitz, D Turaga, Proc of AAAI. 322018. 2018</p>
<p>Automatic feature engineering by deep reinforcement learning. J Zhang, H Jianye, ´ Fogelman-Soulie, F Wang, Z , Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems. the 18th International Conference on Autonomous Agents and MultiAgent Systems2019</p>
<p>Big data of materials science: critical role of the descriptor. L M Ghiringhelli, J Vybiral, S V Levchenko, C Draxl, M Scheffler, 10.1103/PhysRevLett.114.105503Phys. Rev. Lett. 1141055032015</p>
<p>Learning physical descriptors for materials science by compressed sensing. L M Ghiringhelli, J Vybiral, E Ahmetcik, R Ouyang, S V Levchenko, C Draxl, M Scheffler, 10.1088/1367-2630/aa57bfNew J. Phys. 19230172017</p>
<p>SISSO: A compressed-sensing method for identifying the best low-dimensional descriptor in an immensity of offered candidates. R Ouyang, S Curtarolo, E Ahmetcik, M Scheffler, L M Ghiringhelli, 10.1103/PhysRevMaterials.2.083802Phys. Rev. Mater. 2838022018</p>
<p>Sure independence screening for ultrahigh dimensional feature space. J Fan, J Lv, 10.1111/j.1467-9868.2008.00674.xJ. Roy. Stat. Soc. B. 702008</p>
<p>Regression shrinkage and selection via the lasso. R Tibshirani, J. Roy. Stat. Soc. B. 581996</p>
<p>Physics-constrained automatic feature engineering for predictive modeling in materials science. Z Xiang, M Fan, Guillermo Vazquez Tovar, Trehern , W Yoon, B.-J Qian, X Arro ´yave, R Qian, X , the 35th AAAI Conference on Artificial Intelligence (AAAI 2021). 2021</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, 10.1038/nature14236Nature. 5182015</p>
<p>Active learning of linearly parametrized interatomic potentials. E V Podryabinkin, A V Shapeev, 10.1016/j.commatsci.2017.08.031Comput. Mater. Sci. 1402017</p>
<p>Active learning of uniformly accurate interatomic potentials for materials simulation. L Zhang, D.-Y Lin, H Wang, R Car, E Weinan, 10.1103/PhysRevMaterials.3.023804Phys. Rev. Mater. 323804Feb 2019</p>
<p>On-the-fly active learning of interpretable Bayesian force fields for atomistic rare events. J Vandermause, S B Torrisi, S Batzner, Y Xie, L Sun, A M Kolpak, B Kozinsky, 10.1038/s41524-020-0283-zComput. Mater. 6202020</p>
<p>Batch active learning for accelerating the development of interatomic potentials. N Wilson, D Willhelm, X Qian, R Arro ´yave, X Qian, 10.1016/j.commatsci.2022.111330Comput. Mater. Sci. 2082022. 111330</p>
<p>Accurate force field for molybdenum by machine learning large materials data. C Chen, Z Deng, R Tran, H Tang, I.-H Chu, S P Ong, 10.1103/PhysRevMaterials.1.043603Phys. Rev. Mater. 1436032017</p>            </div>
        </div>

    </div>
</body>
</html>
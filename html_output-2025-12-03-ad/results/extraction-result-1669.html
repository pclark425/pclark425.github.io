<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1669 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1669</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1669</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-252815541</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2210.04932v1.pdf" target="_blank">NeRF2Real: Sim2real Transfer of Vision-guided Bipedal Motion Skills using Neural Radiance Fields</a></p>
                <p><strong>Paper Abstract:</strong> We present a system for applying sim2real approaches to"in the wild"scenes with realistic visuals, and to policies which rely on active perception using RGB cameras. Given a short video of a static scene collected using a generic phone, we learn the scene's contact geometry and a function for novel view synthesis using a Neural Radiance Field (NeRF). We augment the NeRF rendering of the static scene by overlaying the rendering of other dynamic objects (e.g. the robot's own body, a ball). A simulation is then created using the rendering engine in a physics simulator which computes contact dynamics from the static scene geometry (estimated from the NeRF volume density) and the dynamic objects' geometry and physical properties (assumed known). We demonstrate that we can use this simulation to learn vision-based whole body navigation and ball pushing policies for a 20 degrees of freedom humanoid robot with an actuated head-mounted RGB camera, and we successfully transfer these policies to a real robot. Project video is available at https://sites.google.com/view/nerf2real/home</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1669.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1669.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NeRF2Real</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NeRF2Real: Sim2real Transfer of Vision-guided Bipedal Motion Skills using Neural Radiance Fields</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that uses a Neural Radiance Field (NeRF) learned from a short handheld video to produce photorealistic renderings and an occupancy-derived collision mesh, which are combined with a MuJoCo physics simulation to train vision-based whole-body control policies for a 20-DoF humanoid and transfer them zero-shot to the real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Robotis OP3 humanoid</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A small (≈35 cm, 3.5 kg) 20-DoF bipedal humanoid robot operated in position-control mode; on-board sensors: joint encoders, gyroscope, accelerometer, head-mounted Logitech C920 RGB camera; policies run on the robot's on-board CPU at 40 Hz.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics — vision-guided bipedal locomotion and object interaction</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>MuJoCo physics simulator + NeRF-based rendering / occupancy mesh</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Physics simulated in MuJoCo (rigid-body dynamics, contacts, actuators) using a collision mesh extracted from the NeRF occupancy field; static scene visuals rendered by NeRF (novel-view RGB synthesis) and dynamic objects (robot body, ball) rendered by MuJoCo and overlaid on NeRF renderings (composite rendering). Camera intrinsics/distortion matched to real robot camera.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>High-fidelity photorealistic static-scene rendering (NeRF) combined with high-fidelity rigid-body/contact physics for robot and dynamic objects in MuJoCo; simplified composite rendering (no occlusion compositing).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Photorealistic RGB appearance of the static scene (NeRF), approximate scene geometry / collision mesh (from NeRF occupancy via marching cubes), rigid-body contact dynamics in MuJoCo, robot actuator position-control behavior, IMU sensor filtering, actuator/sensor latencies (artificially modeled), randomized external perturbations (pushes).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Composite rendering is an overlay (dynamic objects always in foreground) so occlusions between NeRF static content and dynamic objects are not modeled; dynamic object visual detail simplified (e.g., ball rendered as simple orange sphere); fine-grained visual intensity/gain differences between capture camera and robot camera not adjustable inside NeRF; some scene surface properties (e.g. friction distribution, micro-geometry) are not learned from data but assumed/measured or coarsened; floor and low-texture areas manually post-processed (floor replaced by flat primitive).</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Indoor 'in-the-wild' scene (~5m x 4m) captured with ~5-6 minute video from a Google Pixel 6; same physical scene used for robot evaluations with the Robotis OP3 equipped with a Logitech C920 camera; motion-capture used only for offline evaluation (not policy input).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Vision-guided point-to-point navigation with obstacle avoidance (reach goals without falling) and an object-interaction ball-pushing task (move a basketball to a target corner).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (DMPO) with a recurrent image encoder (small ResNet + LSTM) combined with proprioception; asymmetric actor-critic with privileged critic using simulator state.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Episode success rate (fraction of episodes in which the robot reaches goal without falling), median time to reach target, and median localization error (agent belief vs ground truth position).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Navigation: simulation success rates near-perfect (Table I reports 99%/100% for 30x40 across scene splits; authors state simulation and real performance are 'remarkably similar'); Ball pushing: simulation performance is higher than real (exact numeric totals reported in paper's Table II but not reproduced fully in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Navigation: 60x80 policy reached goal in 47/54 episodes (≈87% ±5%); 30x40 policy reached goal in 37/54 episodes (≈69% ±6%). Reported per-target breakdowns show lower real success on some targets (e.g., near potted plant) and higher localization error correlated with failures. Ball pushing: authors report successful qualitative transfer but a larger sim2real gap (especially for 'Wall' initializations) — exact numeric success rates are provided in the paper's Table II (not fully quoted in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Limited domain randomization: random external pushes during training; per-episode constant delays uniformly sampled from 10–50 ms plus 5 ms jitter on sensor data; random attachment of up to 0.5 kg mass at random torso positions at episode start; random IMU position shifts up to 0.5 cm and tilt up to 2°; for ball tasks, randomize ball mass (0.5–0.9 kg) and radius (11.5–12.5 cm). Additionally, image augmentations (random brightness, saturation, hue, contrast, small translations) were applied; camera gain was manually tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Mismatch in image intensity/gain between NeRF training camera and robot camera (camera gain sensitivity caused failure modes); NeRF rendering cannot easily modulate image intensity/gain; composite rendering ignores occlusions (dynamic objects always overlaid in foreground); contact-rich interactions reveal limitations in contact/friction modeling and geometry coarsening (ball can get stuck near walls in reality); floor and low-texture areas required manual postprocessing; robot compute limits constrained image resolution at inference, affecting perception fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Photorealistic NeRF rendering of static scene combined with an occupancy-derived collision mesh to model contacts; careful matching of camera intrinsics and calibration; explicit modeling of sensor filtering and latencies in simulation; limited but targeted domain randomization (mass, IMU pose, pushes, delays); reward regularization to shape transferable gaits (yaw-turn penalty, pose L2, walking speed reward); image augmentations and asymmetric critic (privileged information) for efficient learning; zero-shot transfer achieved without real-world fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Authors emphasize the importance of (1) photorealistic RGB rendering of static scene (NeRF) for vision-based policies, (2) accurate geometry/collision mesh for contact interactions, and (3) correctly modeled sensor/actuator filtering and latencies; they do not supply a quantitative numerical threshold (e.g., percent error) but identify these aspects as critical for transfer and note contact-rich tasks have higher fidelity requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining NeRF-based photorealistic novel-view synthesis and occupancy-derived geometry with a physics simulator enables zero-shot sim-to-real transfer of vision-guided whole-body behaviors for a bipedal humanoid; accurate sensor/actuator modeling, camera calibration, limited domain randomization, reward shaping, and image augmentations materially improve transfer; contact-rich tasks (ball pushing) show a larger sim2real gap largely due to occlusion/interaction and fine contact dynamics limitations; camera gain/intensity mismatch is a major failure mode.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1669.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1669.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Solving Rubik's Cube (OpenAI/robot-hand)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Solving Rubik's Cube with a Robot Hand</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A notable example of sim-to-real success for dexterous manipulation where policies were trained in simulation and transferred to a real dexterous robot hand to solve a Rubik's cube.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Solving rubik's cube with a robot hand</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics — dexterous manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Dexterous in-hand manipulation (Rubik's cube solving)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (as referenced by the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an example where sim-to-real has enabled complex real-world control via careful simulation and transfer techniques.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1669.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1669.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DomainRandomization2017</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Seminal work proposing domain randomization: randomizing simulator visual/physical parameters so policies see diverse simulated variations and generalize to real-world variability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Domain randomization for transferring deep neural networks from simulation to the real world</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics / vision</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Various vision-based control tasks (general technique)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning / supervised training with heavy simulator randomization</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited for strategies that reduce sim2real gap by randomizing simulation parameters; the NeRF2Real work uses limited domain randomization inspired by such approaches.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1669.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1669.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ClosingSim2Real2019</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work on adapting simulation randomization parameters using real-world data to iteratively reduce the sim-to-real gap.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Closing the sim-to-real loop: Adapting simulation randomization with real world experience</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics / sim-to-real</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as prior work that adapts simulation parameters using real data to improve transfer.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1669.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1669.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cad2RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cad2RL: Real Single-Image Flight Without a Single Real Image</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Demonstration that policies trained entirely in simulation (on CAD-derived renderings) can transfer to real-world flight from a single real image using domain randomization techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cad2rl: Real single-image flight without a single real image</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics / vision-based flight</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Vision-based flight / navigation</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Simulation-trained (domain randomization)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an example where synthetic training enabled real-world deployment without real-image training data.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1669.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1669.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sim2RealBiped</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim-to-Real Transfer for Biped Locomotion (Yu et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work on transferring biped locomotion policies from simulation to real hardware; cited as related literature on sim2real for bipeds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-real transfer for biped locomotion</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics — bipedal locomotion</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Bipedal locomotion controllers</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited among works demonstrating sim-to-real for legged/bipedal robots; NeRF2Real extends to vision-guided whole-body policies using RGB cameras.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Solving rubik's cube with a robot hand <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Closing the sim-to-real loop: Adapting simulation randomization with real world experience <em>(Rating: 2)</em></li>
                <li>Cad2rl: Real single-image flight without a single real image <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer for biped locomotion <em>(Rating: 2)</em></li>
                <li>Learning robust perceptive locomotion for quadrupedal robots in the wild <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1669",
    "paper_id": "paper-252815541",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "NeRF2Real",
            "name_full": "NeRF2Real: Sim2real Transfer of Vision-guided Bipedal Motion Skills using Neural Radiance Fields",
            "brief_description": "A pipeline that uses a Neural Radiance Field (NeRF) learned from a short handheld video to produce photorealistic renderings and an occupancy-derived collision mesh, which are combined with a MuJoCo physics simulation to train vision-based whole-body control policies for a 20-DoF humanoid and transfer them zero-shot to the real robot.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Robotis OP3 humanoid",
            "agent_system_description": "A small (≈35 cm, 3.5 kg) 20-DoF bipedal humanoid robot operated in position-control mode; on-board sensors: joint encoders, gyroscope, accelerometer, head-mounted Logitech C920 RGB camera; policies run on the robot's on-board CPU at 40 Hz.",
            "domain": "general robotics — vision-guided bipedal locomotion and object interaction",
            "virtual_environment_name": "MuJoCo physics simulator + NeRF-based rendering / occupancy mesh",
            "virtual_environment_description": "Physics simulated in MuJoCo (rigid-body dynamics, contacts, actuators) using a collision mesh extracted from the NeRF occupancy field; static scene visuals rendered by NeRF (novel-view RGB synthesis) and dynamic objects (robot body, ball) rendered by MuJoCo and overlaid on NeRF renderings (composite rendering). Camera intrinsics/distortion matched to real robot camera.",
            "simulation_fidelity_level": "High-fidelity photorealistic static-scene rendering (NeRF) combined with high-fidelity rigid-body/contact physics for robot and dynamic objects in MuJoCo; simplified composite rendering (no occlusion compositing).",
            "fidelity_aspects_modeled": "Photorealistic RGB appearance of the static scene (NeRF), approximate scene geometry / collision mesh (from NeRF occupancy via marching cubes), rigid-body contact dynamics in MuJoCo, robot actuator position-control behavior, IMU sensor filtering, actuator/sensor latencies (artificially modeled), randomized external perturbations (pushes).",
            "fidelity_aspects_simplified": "Composite rendering is an overlay (dynamic objects always in foreground) so occlusions between NeRF static content and dynamic objects are not modeled; dynamic object visual detail simplified (e.g., ball rendered as simple orange sphere); fine-grained visual intensity/gain differences between capture camera and robot camera not adjustable inside NeRF; some scene surface properties (e.g. friction distribution, micro-geometry) are not learned from data but assumed/measured or coarsened; floor and low-texture areas manually post-processed (floor replaced by flat primitive).",
            "real_environment_description": "Indoor 'in-the-wild' scene (~5m x 4m) captured with ~5-6 minute video from a Google Pixel 6; same physical scene used for robot evaluations with the Robotis OP3 equipped with a Logitech C920 camera; motion-capture used only for offline evaluation (not policy input).",
            "task_or_skill_transferred": "Vision-guided point-to-point navigation with obstacle avoidance (reach goals without falling) and an object-interaction ball-pushing task (move a basketball to a target corner).",
            "training_method": "Reinforcement learning (DMPO) with a recurrent image encoder (small ResNet + LSTM) combined with proprioception; asymmetric actor-critic with privileged critic using simulator state.",
            "transfer_success_metric": "Episode success rate (fraction of episodes in which the robot reaches goal without falling), median time to reach target, and median localization error (agent belief vs ground truth position).",
            "transfer_performance_sim": "Navigation: simulation success rates near-perfect (Table I reports 99%/100% for 30x40 across scene splits; authors state simulation and real performance are 'remarkably similar'); Ball pushing: simulation performance is higher than real (exact numeric totals reported in paper's Table II but not reproduced fully in main text).",
            "transfer_performance_real": "Navigation: 60x80 policy reached goal in 47/54 episodes (≈87% ±5%); 30x40 policy reached goal in 37/54 episodes (≈69% ±6%). Reported per-target breakdowns show lower real success on some targets (e.g., near potted plant) and higher localization error correlated with failures. Ball pushing: authors report successful qualitative transfer but a larger sim2real gap (especially for 'Wall' initializations) — exact numeric success rates are provided in the paper's Table II (not fully quoted in main text).",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Limited domain randomization: random external pushes during training; per-episode constant delays uniformly sampled from 10–50 ms plus 5 ms jitter on sensor data; random attachment of up to 0.5 kg mass at random torso positions at episode start; random IMU position shifts up to 0.5 cm and tilt up to 2°; for ball tasks, randomize ball mass (0.5–0.9 kg) and radius (11.5–12.5 cm). Additionally, image augmentations (random brightness, saturation, hue, contrast, small translations) were applied; camera gain was manually tuned.",
            "sim_to_real_gap_factors": "Mismatch in image intensity/gain between NeRF training camera and robot camera (camera gain sensitivity caused failure modes); NeRF rendering cannot easily modulate image intensity/gain; composite rendering ignores occlusions (dynamic objects always overlaid in foreground); contact-rich interactions reveal limitations in contact/friction modeling and geometry coarsening (ball can get stuck near walls in reality); floor and low-texture areas required manual postprocessing; robot compute limits constrained image resolution at inference, affecting perception fidelity.",
            "transfer_enabling_conditions": "Photorealistic NeRF rendering of static scene combined with an occupancy-derived collision mesh to model contacts; careful matching of camera intrinsics and calibration; explicit modeling of sensor filtering and latencies in simulation; limited but targeted domain randomization (mass, IMU pose, pushes, delays); reward regularization to shape transferable gaits (yaw-turn penalty, pose L2, walking speed reward); image augmentations and asymmetric critic (privileged information) for efficient learning; zero-shot transfer achieved without real-world fine-tuning.",
            "fidelity_requirements_identified": "Authors emphasize the importance of (1) photorealistic RGB rendering of static scene (NeRF) for vision-based policies, (2) accurate geometry/collision mesh for contact interactions, and (3) correctly modeled sensor/actuator filtering and latencies; they do not supply a quantitative numerical threshold (e.g., percent error) but identify these aspects as critical for transfer and note contact-rich tasks have higher fidelity requirements.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Combining NeRF-based photorealistic novel-view synthesis and occupancy-derived geometry with a physics simulator enables zero-shot sim-to-real transfer of vision-guided whole-body behaviors for a bipedal humanoid; accurate sensor/actuator modeling, camera calibration, limited domain randomization, reward shaping, and image augmentations materially improve transfer; contact-rich tasks (ball pushing) show a larger sim2real gap largely due to occlusion/interaction and fine contact dynamics limitations; camera gain/intensity mismatch is a major failure mode.",
            "uuid": "e1669.0"
        },
        {
            "name_short": "Solving Rubik's Cube (OpenAI/robot-hand)",
            "name_full": "Solving Rubik's Cube with a Robot Hand",
            "brief_description": "A notable example of sim-to-real success for dexterous manipulation where policies were trained in simulation and transferred to a real dexterous robot hand to solve a Rubik's cube.",
            "citation_title": "Solving rubik's cube with a robot hand",
            "mention_or_use": "mention",
            "agent_system_name": null,
            "agent_system_description": null,
            "domain": "robotics — dexterous manipulation",
            "virtual_environment_name": null,
            "virtual_environment_description": null,
            "simulation_fidelity_level": null,
            "fidelity_aspects_modeled": null,
            "fidelity_aspects_simplified": null,
            "real_environment_description": null,
            "task_or_skill_transferred": "Dexterous in-hand manipulation (Rubik's cube solving)",
            "training_method": "Reinforcement learning (as referenced by the paper)",
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": null,
            "transfer_enabling_conditions": null,
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Cited as an example where sim-to-real has enabled complex real-world control via careful simulation and transfer techniques.",
            "uuid": "e1669.1"
        },
        {
            "name_short": "DomainRandomization2017",
            "name_full": "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World",
            "brief_description": "Seminal work proposing domain randomization: randomizing simulator visual/physical parameters so policies see diverse simulated variations and generalize to real-world variability.",
            "citation_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "mention_or_use": "mention",
            "agent_system_name": null,
            "agent_system_description": null,
            "domain": "robotics / vision",
            "virtual_environment_name": null,
            "virtual_environment_description": null,
            "simulation_fidelity_level": null,
            "fidelity_aspects_modeled": null,
            "fidelity_aspects_simplified": null,
            "real_environment_description": null,
            "task_or_skill_transferred": "Various vision-based control tasks (general technique)",
            "training_method": "Reinforcement learning / supervised training with heavy simulator randomization",
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": null,
            "transfer_enabling_conditions": null,
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Cited for strategies that reduce sim2real gap by randomizing simulation parameters; the NeRF2Real work uses limited domain randomization inspired by such approaches.",
            "uuid": "e1669.2"
        },
        {
            "name_short": "ClosingSim2Real2019",
            "name_full": "Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience",
            "brief_description": "Work on adapting simulation randomization parameters using real-world data to iteratively reduce the sim-to-real gap.",
            "citation_title": "Closing the sim-to-real loop: Adapting simulation randomization with real world experience",
            "mention_or_use": "mention",
            "agent_system_name": null,
            "agent_system_description": null,
            "domain": "robotics / sim-to-real",
            "virtual_environment_name": null,
            "virtual_environment_description": null,
            "simulation_fidelity_level": null,
            "fidelity_aspects_modeled": null,
            "fidelity_aspects_simplified": null,
            "real_environment_description": null,
            "task_or_skill_transferred": null,
            "training_method": null,
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": null,
            "transfer_enabling_conditions": null,
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Cited as prior work that adapts simulation parameters using real data to improve transfer.",
            "uuid": "e1669.3"
        },
        {
            "name_short": "Cad2RL",
            "name_full": "Cad2RL: Real Single-Image Flight Without a Single Real Image",
            "brief_description": "Demonstration that policies trained entirely in simulation (on CAD-derived renderings) can transfer to real-world flight from a single real image using domain randomization techniques.",
            "citation_title": "Cad2rl: Real single-image flight without a single real image",
            "mention_or_use": "mention",
            "agent_system_name": null,
            "agent_system_description": null,
            "domain": "robotics / vision-based flight",
            "virtual_environment_name": null,
            "virtual_environment_description": null,
            "simulation_fidelity_level": null,
            "fidelity_aspects_modeled": null,
            "fidelity_aspects_simplified": null,
            "real_environment_description": null,
            "task_or_skill_transferred": "Vision-based flight / navigation",
            "training_method": "Simulation-trained (domain randomization)",
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": null,
            "transfer_enabling_conditions": null,
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Cited as an example where synthetic training enabled real-world deployment without real-image training data.",
            "uuid": "e1669.4"
        },
        {
            "name_short": "Sim2RealBiped",
            "name_full": "Sim-to-Real Transfer for Biped Locomotion (Yu et al.)",
            "brief_description": "Prior work on transferring biped locomotion policies from simulation to real hardware; cited as related literature on sim2real for bipeds.",
            "citation_title": "Sim-to-real transfer for biped locomotion",
            "mention_or_use": "mention",
            "agent_system_name": null,
            "agent_system_description": null,
            "domain": "robotics — bipedal locomotion",
            "virtual_environment_name": null,
            "virtual_environment_description": null,
            "simulation_fidelity_level": null,
            "fidelity_aspects_modeled": null,
            "fidelity_aspects_simplified": null,
            "real_environment_description": null,
            "task_or_skill_transferred": "Bipedal locomotion controllers",
            "training_method": null,
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": null,
            "transfer_enabling_conditions": null,
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Cited among works demonstrating sim-to-real for legged/bipedal robots; NeRF2Real extends to vision-guided whole-body policies using RGB cameras.",
            "uuid": "e1669.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Solving rubik's cube with a robot hand",
            "rating": 2,
            "sanitized_title": "solving_rubiks_cube_with_a_robot_hand"
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Closing the sim-to-real loop: Adapting simulation randomization with real world experience",
            "rating": 2,
            "sanitized_title": "closing_the_simtoreal_loop_adapting_simulation_randomization_with_real_world_experience"
        },
        {
            "paper_title": "Cad2rl: Real single-image flight without a single real image",
            "rating": 2,
            "sanitized_title": "cad2rl_real_singleimage_flight_without_a_single_real_image"
        },
        {
            "paper_title": "Sim-to-real transfer for biped locomotion",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_for_biped_locomotion"
        },
        {
            "paper_title": "Learning robust perceptive locomotion for quadrupedal robots in the wild",
            "rating": 1,
            "sanitized_title": "learning_robust_perceptive_locomotion_for_quadrupedal_robots_in_the_wild"
        }
    ],
    "cost": 0.018741749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>NeRF2Real: Sim2real Transfer of Vision-guided Bipedal Motion Skills using Neural Radiance Fields</p>
<p>Arunkumar Byravan 
Equal Contributions</p>
<p>Jan Humplik 
Equal Contributions</p>
<p>Leonard Hasenclever 
Equal Contributions</p>
<p>Arthur Brussee 
Equal Contributions</p>
<p>Francesco Nori 
Equal Contributions</p>
<p>Tuomas Haarnoja 
Equal Contributions</p>
<p>Ben Moran 
Equal Contributions</p>
<p>Steven Bohez 
Equal Contributions</p>
<p>Fereshteh Sadeghi 
Equal Contributions</p>
<p>Bojan Vujatovic 
Equal Contributions</p>
<p>Nicolas Heess Deepmind 
Equal Contributions</p>
<p>NeRF2Real: Sim2real Transfer of Vision-guided Bipedal Motion Skills using Neural Radiance Fields
Project video is available at https://sites.google.com/ view/nerf2real/home.
Fig. 1: Zero-shot sim2real transfer results of vision-based bipedal locomotion policies trained using reinforcement learning in two separate simulations created using our NeRF2Real setup. Left: time lapse of a transfer result on a navigation task with a comparison of the robot's head-mounted camera views vs NeRF renderings: 'Real': views from the real-robot, i.e. evaluation inputs to the policy; 'NeRF': train time NeRF rendered images. Right: time lapse of a result on a task where the robot has to push a ball towards the target region in front of the red cones. The policy was trained in simulation by overlaying simple rendering of an orange ball on top of the scene's NeRF rendering.Abstract-We present a system for applying sim2real approaches to "in the wild" scenes with realistic visuals, and to policies which rely on active perception using RGB cameras. Given a short video of a static scene collected using a generic phone, we learn the scene's contact geometry and a function for novel view synthesis using a Neural Radiance Field (NeRF). We augment the NeRF rendering of the static scene by overlaying the rendering of other dynamic objects (e.g. the robot's own body, a ball). A simulation is then created using the rendering engine in a physics simulator which computes contact dynamics from the static scene geometry (estimated from the NeRF volume density) and the dynamic objects' geometry and physical properties (assumed known). We demonstrate that we can use this simulation to learn vision-based whole body navigation and ball pushing policies for a 20 degrees of freedom humanoid robot with an actuated head-mounted RGB camera, and we successfully transfer these policies to a real robot.</p>
<p>I. INTRODUCTION</p>
<p>Thanks to progress in large-scale deep reinforcement learning and scalable simulation infrastructure, training control policies in simulation and transferring them to real robots (sim2real) has become a popular paradigm in robotics [1]- [4]. This approach avoids many of the issues such as state estimation, safety, and data efficiency which make it challenging to learn directly on hardware. However, creating accurate and realistic simulations is time consuming. Therefore, for sim2real to live up to its full potential, we must make it easier to recreate real scenes in simulation while accurately modelling how robots sense and interact with the world.</p>
<p>Reducing the gap between simulation and the real world often involves the collection of small amounts of data followed by manual tuning, the use of established system identification tools, or more recently by learning neural network models of parts of the system, e.g. [1]. It is especially difficult to accurately model the geometry and visual appearance of unstructured scenes which affect how the robot makes contact with the world and how it senses its surroundings e.g. when using a RGB camera. The need for modeling RGB cameras can partially be alleviated by using depth sensors or LiDARs which are easier to simulate and thus have a smaller sim2real gap, but such a compromise can restrict the set of tasks a robot can learn. Existing approaches to photorealistic scene reconstruction and rendering, e.g. those used for the creation of the datasets in [5]- [8], work poorly in outdoor scenes and use specialized 3D scanning setups which are not widely available, hence limiting their applicability.</p>
<p>In this paper we begin to address some of these challenges, and describe a system for the semi-automated generation of simulation models for visually complex scenes with highly realistic rendering of RGB camera views and accurate geometry, primarily using videos from commodity mobile cameras. To this end, we take advantage of recent advances in neural scene representations using Neural Radiance Fields (NeRF) [9], [10]. NeRFs are a fast developing class of scene representations that allow synthesizing novel photorealistic views from a sparse set of input views. Unlike prior work, NeRFs can be learned directly from videos or photographs from commodity mobile devices and admit access not just to a rendering function but also to the underlying scene geometry. They can be trained within minutes [11], [12], work in both indoor and outdoor settings and scale well even to large scenes such as city blocks [13]. Together with extensions to handle dynamic scenes [14], deformable objects [15], and scene decompositions with novel re-combinations of objects [16], NeRFs can enable a general system for recreating the visuals of real-word scenes in simulation.</p>
<p>Our primary contribution is an approach for combining NeRF scene representations, specifically the rendering and static geometry, learned from short (5-6) minute videos of a scene, with a physics simulation of dynamic objects such as a robot and a ball whose physical and visual properties are assumed known (see Fig. 2). We present a semi-automated pipeline for setting up these simulations and demonstrate that they have high enough fidelity to enable simulation-toreality transfer of vision-guided control policies. Specifically, we use a physically accurate simulation of a 20 degreeof-freedom Robotis OP3 humanoid robot together with the NeRF and end-to-end deep reinforcement learning to train vision-based whole-body navigation and ball pushing policies and we show a strong alignment between the performance of these policies in simulation and when transferred zero-shot to real robot (see Fig. 1 for a visualization of our results).</p>
<p>II. RELATED WORK</p>
<p>A. Neural Radiance Fields</p>
<p>Neural Radiance Fields (NeRF) [9] have recently become popular as an implicit scene representation capable of synthesizing novel photorealistic views. NeRF and its variants can represent accurate scene geometry [17], capture large scenes [13] and can be trained from photo collections without the need for localization or specialized hardware [18]- [20]. Recent work has also shown training and rendering can be extremely fast and efficient [11], [12], or even be real-time on commodity handheld devices [21].</p>
<p>NeRF in Robotics: NeRFs have been used in robotics for pose estimation [22], representation learning [23], grasping [24] and dynamics model learning [25]. There is also closely related work on obstacle avoidance within simulated NeRF environments leveraging a traditional state estimation and planning pipeline [26], but transfer to the real-world was not explored. Unlike this work we use reinforcement learning to train a policy that tightly integrates perception and control for a bipedal robot and demonstrate transfer to the real world on both visual navigation and object interaction tasks.</p>
<p>B. Visual Navigation with (Visually) Realistic Simulators</p>
<p>There is a long line of work on modeling real-world indoor scenes including datasets such as Matterport3D [5], Gibson [6], Replica [7] and Habitat-Matterport3D [27]. Unlike these datasets, which were predominantly created using purpose-built scanning setups, often with access to depth or LIDAR, we use a small amount of video data from off-theshelf mobile cameras to train a NeRF to represent our scene.</p>
<p>Visual Navigation in simulation: Several simulation suites have been proposed for embodied visual navigation tasks combining 3D simulators with the different 3D scene datasets mentioned previously, such as Habitat [8], [28], iGibson [29] and AI2/ROBO-THOR [30], [31]. These simulators have been used for learning visual navigation policies [32]- [34], solve object-based navigation [35], [36], also incorporating language commands [37]. These approaches primarily consider dynamically simple platforms (wheeled robots) and operate purely in simulation.</p>
<p>Sim2Real for Visual Navigation: Several works have demonstrated that policies trained in these photorealistic simulators can be transferred to real-world robots. The majority focuses on wheeled-base robots [4], [38], but some recent work has extended this to quadrupeds [39], [40]. These approaches use RGB-D sensors and/or LiDAR, assume access to localization, and in case of the quadrupeds, work on top of existing low-level controllers; all these assumptions help reduce the sim2real gap and thereby result in good transfer performance. In contrast, we successfully transfer wholebody vision-based control policies for a bipedal robot using only an RGB camera (which has a high sim2real gap) without access to localization or low-level controllers.</p>
<p>C. Sim2Real in Robotics</p>
<p>Sim2real transfer has made it possible to use reinforcement learning to solve several challenging real-world control problems. Careful system identification and techniques such as Domain Randomization [41], Domain Adaptation [42] and Real2sim [43] have helped to reduce the discrepancies between simulation and reality for the system dynamics and sensor model, enabling successes on tasks such as Rubik's cube solving with a dexterous hand [3], grasping [44], stacking [45], autonomous flight [46], quadruped [47]- [50] and biped locomotion [51]- [53]. We rely on many of the lessons learned in these works, and propose a system for high-fidelity replication of real-world scenes in simulation with which we demonstrate successful zero-shot transfer of complex vision based policies on a 20 DoF humanoid robot. Fig. 2 presents an overview of our approach for recreating a static scene in simulation, and its extension to scenes with simple dynamic objects. Our approach consists of 6 steps: video recording, localization, NeRF training, postprocessing to extract a rendering function and a collision mesh, and combining these with a physics simulator to create the simulation (see Fig 3). We describe each step below.</p>
<p>III. INTEGRATING NERF WITH A PHYSICS SIMULATOR</p>
<p>A. Capturing a video of the real world scene</p>
<p>We capture a short ∼ 5−6 minute video of the scene using an off-the-shelf mobile camera (Google Pixel 6's rear camera in this work). A human operator walks around the scene and captures it from different viewpoints while ensuring that the camera moves slowly and evenly to reduce motion blur and minimize drastic viewpoint changes. For consistent lighting we set the white balance and brightness to a fixed (arbitrary) Fig. 2: Overview of our system for recreating a scene in a simulator. A. We collect a video of the scene using a generic phone. B. We use structure-from-motion software to label a subset of the video with camera poses. C. We train a NeRF on these labeled images. D. We render the scene from novel views using the calibrated intrinsics of the robot's head-mounted camera. E. We use the same NeRF to extract the scene geometry as a mesh. We coarsen the mesh and replace the floor with a flat primitive. F. We combine the simplified mesh with a model of a robot, and any other dynamic objects, in a physics simulator. See Fig. 3 for further details on this step. value. We found that high-resolution (≥1080p) videos led to better localization and improved NeRF results.</p>
<p>B. Localization</p>
<p>Next, we extract N ∼ 1000 keyframes from the captured video. We use COLMAP [54]- [56], an open-source Structure-from-Motion (SfM) package, to estimate the intrinsics of the camera, and extrinsics for each keyframe (see VI-A for details).</p>
<p>C. NeRF training</p>
<p>Given a dataset of images and corresponding camera poses, we train a NeRF [9] to render the scene from novel viewpoints. We use recent NeRF extensions for better reconstructions, improved reconstructed geometry, and decreased rendering times. To avoid artifacts while rendering at low resolutions, we sample the average of the volume over a normal distribution [57]. We use a space squashing formulation to support large capture areas, as well as a separate 'proposal' network, and a 'distortion' loss that encourages compact representations [10]. To improve the reconstructed geometry we optimise a separate specular and diffuse color [17].</p>
<p>NeRF rendering can be compute intensive even at low resolutions. While this is not critical in our context as we use the NeRF only for offline learning in simulation, to allow for faster experiment turnaround we implement a multi scale spatial hash grid approach [11]. This provides a significant speedup (order of magnitude), enabling rendering one frame in 6ms on a V100 GPU. We use a similar architecture as described in [11], adding a layer normalization [58] before the final MLP layer, and use swish activations [59] rather than ReLU activations. Additionally, we adapted this approach to allow sampling the radiance volume over a distribution. We blur training samples with a Gaussian blur with a random variance σ blur ∈ [σ min , σ max ], and provide Σ = Σ sample * (1 + (σ blur − σ min )) as an extra input to the final MLP [60]. This augmentation allows the network to interpolate samples in scale-space and improves our reconstruction significantly at lower resolutions (∼ 31.5 vs ∼ 35.4 average PSNR in a few held out images). NeRF hyperparameters are listed in Sec. III.</p>
<p>D. Rendering in Simulation</p>
<p>The trained NeRF can be used for rendering the scene from novel viewpoints and camera intrinsics. In particular, we will use it to model the robot's camera (Logitech C920) which is different from the one used to collect images for training. We match camera intrinsics between sim and real by calibrating the robot's camera, and use the obtained focal length and distortion parameters to render the NeRF.</p>
<p>E. Collision mesh extraction</p>
<p>The NeRF learns a function to predict the radiance and occupancy in space, i.e. the underlying scene geometry. We voxelize the predicted occupancy and compute a mesh via the marching cubes algorithm [61]; this mesh is used for collisions within our simulation.</p>
<p>The camera poses obtained from COLMAP, and hence also the collision mesh vertices, are expressed in an arbitrary reference frame (including an arbitrary scale). Therefore, we need to estimate a rigid transformation and scale between this frame of reference and the simulator's world frame. We do this by solving a least-squares optimization that constrains the normal vector to the dominant floor plane in the mesh to be aligned with the z-axis in the simulator. We use Blender [62] to manually select points on the mesh's floor for this purpose. We then manually rotate the mesh around the z-axis to a desired alignment with the simulator's world frame and compute the relative scale between the NeRF and the world by comparing the size of an object within the mesh and the real world. We also replace the floor vertices in the mesh (which can have artifacts due to a lack of texture) with a flat plane. Lastly, for faster collision computation, we crop the mesh to the extents needed for simulation. See Table IV for details.</p>
<p>F. Physics simulation</p>
<p>We use MuJoCo [63] as our physics simulator. The simulated scene consists of the mesh extracted from NeRF attached to the world frame as a fixed object which can collide with the robot body or other simulated dynamic objects such as a ball. Physical and visual properties of these additional virtual objects are assumed known.</p>
<p>Realistic rendering of such composite scenes is an active area of research [64]. We opt for a straightforward approach which is suitable for our tasks. We assume that the dynamic objects (all rendered with the MuJoCo built-in renderer) are always in the foreground of the static scene (rendered with NeRF); note that this doesn't handle occlusions. Under this assumption the combined rendering is obtained by overlaying the dynamic objects rendering on top of the NeRF rendering (see Fig. 3 for a visualization).</p>
<p>IV. SIM2REAL TRAINING SETUP WITH NERF + MUJOCO</p>
<p>Once the combined MuJoCo simulation is set up we can train a policy purely in this simulation and deploy it directly in the real-world. We describe our training setup below.</p>
<p>A. Humanoid Robot platform</p>
<p>We use a Robotis OP3 [65] robot for all our experiments. This low cost platform is a small humanoid (about 35 cm tall, 3.5 kg in weight) with 20 actuated degrees of freedom (see Fig. 1). Actuators, and hence our learned policies, are operated in a position control mode with both D and I gains set to 0. Our policies run at 40Hz and rely on the robot's onboard computer (2-core Intel NUC i3) and on-board sensors only. These include joint encoders, gyroscope, accelerometer, and a Logitech C920 camera attached to the robot's head which is actuated via two joints attached to its torso. The gyroscope and accelerometer data are filtered at 125Hz to obtain an estimate of the gravity direction in the robot's body frame using a Madgwick filter [66]. To encourage smoother movements, we apply an exponential filter with strength 0.8 to the control signals before passing them to the actuators.</p>
<p>B. Reducing the dynamics sim2real gap</p>
<p>We ensured that the robot's sensors and actuators are modeled accurately in simulation. Specifically, we ensured that the simulated gyroscope and accelerometer data are lowpass filtered in the same way in simulation as on the real IMU chip. With these accurate models we found that policies trained in simulation transferred well to the real robot; hence we used only limited domain randomization on top of these models. Particularly, we applied random pushes to the robot during training. We also applied constant delays per episode, sampled uniformly in the range of 10ms -50ms as well as a 5 ms jitter to all simulated sensor data to reflect various latencies on the robot. At the beginning of each episode, we attach a random mass (up to 0.5 kg) to a random position on the robot's torso and randomize the IMU's position on the torso (we shift it by up to 0.5 cm, and tilt it by up to 2 degrees). In tasks with a ball, we additionally randomize the ball's mass (0.5 -0.9kg) and radius (11.5 -12.5cm) at the start of each episode (the real ball weighs 0.651kg with a radius of 12cm).</p>
<p>C. Regularizing policy learning for better sim2real transfer</p>
<p>Carefully choosing rewards for regularizing the robot's behavior is important for successful transfer. In all of our tasks, we use the following reward components as a regularization: 1. a constant penalty whenever the robot's yaw angular speed is larger than π rad s −1 to encourage the robot to turn slowly; 2. L2 regularization on joint angles towards a default standing pose; and, 3. a walking reward encouraging the average of feet velocities in the robot's forward direction to be 0.3 m s −1 . These rewards encourage the agent to learn gaits that transfer better, and also encourage better exploration for faster learning. See Section VI-D.2 for an exact specification of these rewards.</p>
<p>D. Tasks</p>
<p>To demonstrate that our approach can scale to realistic scenes with complex geometries and supports simple object interactions we choose two tasks:</p>
<p>Navigation and obstacle avoidance: We demonstrate our approach on a point to point visual navigation task where the robot has to reach multiple goals (specified as (x,y) coordinates) while avoiding different obstacles such as a large plant, a chair, and walls; see We chose three targets in different parts of the space that the robot has to reach. We automatically compute the free areas of the scene using the NeRF's mesh and, during simulation, we randomly initialize the robot to a position and orientation within these areas.</p>
<p>The reward for training consists of the regularization terms described in Section IV-E, and two task-specific terms: 1. a sparse bonus upon reaching the goal location; 2. a walking reward like the one we use as a regularization but instead encouraging moving in the direction of the goal at a speed of 0.3 m s −1 (see Section VI-D.2). Episodes terminate whenever the robot's body parts other than the feet touch the scene's mesh. We consider an episode to be successful if the robot gets to ≤25cm of the target without falling and does not collide with any obstacles.</p>
<p>Ball pushing: As a proof of concept that we can combine static NeRF scenes with dynamic interacting objects, we consider a task in which the robot has to move a basketball to a corner of a 3m x 3m workspace (see Fig. 1, right). We model the basketball as a simple orange ball ignoring the fine black print which is barely visible at the resolutions we use (see Fig. 3 for an example simulated image). During training in simulation, each episode starts with the ball and robot randomly positioned. In half of all episodes, we initialize the ball just in front of the robot to speed up learning. We again use the regularization terms in Section IV-E and two task-specific terms: 1. a reward for minimizing the distance between the ball and the goal region; and, 2. a reward for minimizing the distance between the robot and the ball if the ball is not moving towards the goal (see Section VI-D.2). Episodes are terminated whenever the robot falls. We consider an episode as successful if the robot gets the ball to the correct 1m x 1m corner square within 60 seconds. This task is much more challenging than navigation due to significant partial observability &amp; interactions; the robot has to search for the ball, localize itself and the ball, and move it to the goal.</p>
<p>E. Policy training</p>
<p>All our policies are trained using DMPO [67], a state-ofthe-art algorithm which combines distributional deep reinforcement learning [68] and MPO [69], [70]. Our policies take vision and proprioception as input; the policy network (see Fig. 4) consists of a recurrent image encoder (to handle partial observability) which passes the RGB camera images (30x40 or 60x80 resolution) through a small ResNet followed by a LSTM. The encoded images are then combined with a history of past 5 proprioceptive observations (gyroscope, accelerometer, gravity direction estimate, joint positions, and previous control signal) and passed through an MLP which outputs a diagonal Gaussian for sampling actions. Hyperparameters used for training are listed in Section VI-D.3.</p>
<p>We use an asymmetric actor-critic setup for training in simulation where the critic, a separate neural network that is not evaluated on the robot, receives privileged information. Specifically, the critic shares the same network structure as the actor but we replace the image encoder with the simulation's ground truth state (robot/object poses and velocities). This step is crucial for efficient learning in simulation.</p>
<p>Image augmentations: While the NeRF significantly reduces the sim2real gap with realistic scene renderings, we cannot easily modulate image intensity properties such as brightness or gain (see Fig. 1 (left) for comparison of real vs rendered images). Thus, we apply image augmentations during training: we randomize the brightness, saturation, hue, and contrast, and apply random translations to the image, see Section VI-E for details.</p>
<p>V. EXPERIMENTAL RESULTS</p>
<p>We now present our results. We highly encourage the reader to watch the accompanying video at https:// sites.google.com/view/nerf2real/home.</p>
<p>A. Training time</p>
<p>We train policies in simulation and transfer them zeroshot to the robot for evaluation. Given ∼1000 frames from a 5-6 minute video, COLMAP localization takes about 3-4 hours. Training the NeRF on this dataset takes about 20 minutes on a cluster of 8 V100s, though this could be significantly sped up further [11]. Mesh extraction and post-processing for setting up the simulation takes a few hours, and finally training the policies takes around 24 hours (about 8M gradient updates, and 128M environment steps) for navigation and twice as long for ball pushing.</p>
<p>B. Navigation and obstacle avoidance results</p>
<p>Evaluation setup &amp; metrics: We compare the performance of learned goal-conditioned policies in simulation to zero-shot transfer performance in real. We use the following evaluation protocol: for each of the 3 goals we chose 3 unique initial positions and 3 orientations, forming a total of 27 combinations. We perform two trials for each combination, for a total of 54 real-world episodes. We consider an episode to be successful if the robot reaches the goal (≤25 cm distance) without falling. We report the overall success percentage, i.e. the fraction of episodes the policy was successful, and the median time taken to reach the target. As the evaluation space is equipped with a motion capture system we can use this to compute the ground-truth position of the robot-this ground-truth information is only used for analysis and evaluation, not as an input to the policy.</p>
<p>As an analysis tool, our policies are trained together with an auxiliary prediction MLP which predicts the robot's belief about its 2D position and yaw from the policy's recurrent image embedding (we do not propagate any gradients to the policy's parameters). The belief is modelled as a mixture of five Gaussians and we use it to help us disambiguate between policy failures due to confusing visual inputs and those due to other factors (e.g. the robot tripping). We use the difference between the mean of the belief distribution and the groundtruth position when quantifying the policy's localization error which is averaged over the entire episode.</p>
<p>Results: Table I presents the results of our evaluation. We evaluated policies with two different input resolutions, 30x40 and 60x80; due to hardware limitations on the robot (2 CPU cores, no GPU) and the need to run a policy step within 25ms, we were unable to evaluate higher resolutions.</p>
<p>We draw attention to a few interesting trends: 1) Our policies do not exhibit a significant gap between performance in simulation and on the real robot. On the real robot, the 60x80 policy successfully reached the goal in 47/54 episodes (87±5%). The lower resolution 30x40 policy performs slightly worse and was successful in 37/54 episodes (69±6%). This is remarkably similar to the performance of these policies in simulation. Based on monitoring the policy's belief about its pose, we estimate that about half of the failures of the 40x30 policy were due to collisions with obstacles and/or the robot falling down, and the rest due to localization failures, but it is impossible to perfectly disambiguate failure modes. 2) The policies take similar    amounts of time to reach the goal in simulation and on the robot demonstrating that dynamical properties of the behavior such as the gait velocity also do not suffer from a sim2real gap. 3) As expected, our policies learn representations which are informative about the robot's current pose; these transfer well to the real-world. The median localization error across all control steps and trials is ∼0.25m. We saw that several failures of the policy correlated with higher localization error, specifically on a single target near the potted plant. This was particularly evident for the 40x30 policy (0.33m for failures vs 0.23m for successes). We show a visualization of the belief for a successful trajectory in Fig. 5, which demonstrates that the agent is able to quickly localize itself accurately with ∼2 seconds of data and rarely loses track throughout the trial. Videos of both simulated and real-world evaluations can be found on our website.</p>
<p>C. Ball pushing results</p>
<p>Evaluation setup &amp; metrics: Similar to the navigation task, we compare the average success of policies between simulation and real-world. We consider two different evaluation setups in real, a Center setting where the ball is initialized in the center cell of the workspace (Fig. 1, right) and the robot is initialized in the center of any of the eight cells near the wall with 4 different orientations per cell (32 episodes total), and a Wall setting where the robot is initialized in the center cell facing the target corner, and the ball is initialized near the center of one of the remaining 7 cells (except the target cell) with two trials each (14 episodes total). The target is always fixed to be the corner with the two cones (see Fig. 1, right), and an episode is considered successful if the robot can get the ball to the 1m x 1m target cell within 60 seconds without falling down. Table II presents the evaluation results. We highlight two key points: 1) As can be seen in the accompanying video, our policies use the robot's hands to move the ball, and exhibits active perception when searching and tracking the ball. These behaviors emerge from the task requirements and are not explicitly encouraged by the reward function; they also transfer successfully from simulation to the real world.</p>
<p>Results:</p>
<p>2) While our policies show good performance, the sim2real gap is larger for this contact-rich task. This is especially true for the Wall initializations; unless the robot executes a perfect push, the ball often gets stuck near the walls and the robot has a hard time moving it.</p>
<p>Videos of all our results, and comparisons of renderings from the NeRF, COLMAP reconstructions (which we show as a baseline) and real images can also be found in the supplementary material and on our website.</p>
<p>VI. DISCUSSION AND LIMITATIONS</p>
<p>We have presented a pipeline for creating simulation environments of visually complex scenes in a way that allows training vision guided policies for sim2real transfer. To this end we combine the scene geometry and rendering function derived from a NeRF with a known physics model of the robot and (optionally) additional objects.</p>
<p>In principle, our approach is embodiment independent and it can be automated further in future work. For instance, new NeRF-like models such as [17], [71], [72], may improve scene geometry reconstruction thus eliminating the need for manual postprocessing of minimally textured areas like the floor. Evaluating the approach on contact-rich tasks such as climbing on objects will allow us to better assess the current limitations of the approach, and may guide future NeRF and scene modeling developments.</p>
<p>We have currently opted for a very simple approach to composing the rendering of an a priori known object with the rendering of a static NeRF scene. However, the field has been actively working on NeRF-like approaches to photorealistic rendering of composite scenes [73], including ways for segmenting a static scene into dynamic objects and representing each using a separate NeRF [16]. If necessary, our pipeline can leverage any of these improvements (and will have to for more complex dynamic scenes).</p>
<p>Similarly, we believe that recent work on eliminating the computationally expensive localization step from NeRF pipelines [18]- [20], and speeding up both NeRF [11] and RL training [74] will soon enable going from a video of a scene to a trained policy within minutes or hours instead of 1-2 days, potentially enabling running our setup online on the robot during deployment.</p>
<p>Impact statement: This work presents an approach to train vision guided policies for general robotics systems. While in its current form the approach is unlikely to enable real world applications, future research may make possible a range of applications that can benefit humanity. We strongly oppose any applications designed to bring harm to humans.</p>
<p>ACKNOWLEDGMENTS</p>
<p>We would like to thank Neil Sreendra, Marlon Gwira, Kushal Patel, Nathan Batchelor, and Federico Casarini for maintaining and repairing robots used in this project, Jon Scholz and Francesco Romano for reviewing the paper, and Claudio Fantacci for helping with paper figures.</p>
<p>APPENDIX</p>
<p>A. COLMAP details</p>
<p>To train the NeRF model, we need a paired set of images and camera poses. We divide the video into N ∼ 1000 equal partitions, and for each partition we use a heuristic to pick the least motion blurred frame. We use the average variance of the Laplacian of each frame to approximate how sharp a frame is.</p>
<p>After extracting these keyframes, we feed them into COLMAP [54]- [56]. We use the sequential matcher to generate camera poses with mostly default settings, except for using affine SIFT features, guided matching, and forcing a single OPENCV style camera.</p>
<p>For the comparisons between the NeRF reconstruction and COLMAP reconstruction shown in the accompanying video and on our website, we reconstruct a dense mesh from the sparse results using the Poisson mesher with the default settings.</p>
<p>B. NeRF implementation details</p>
<p>As described in Section III-C, we use a NeRF with a similar architecture to the one used in [11]. We use 'swish' [59] rather than 'relu' activations however, and add a final layernorm [58] before the last MLP layer.</p>
<p>As described in Section III-C, we've also integrated mip-NeRF style sampling with the hashtable style NeRF by appending the diagonalised variance to the sample position, and feeding in blurred training samples. Figure 6 compares results with and without this method when rendering at lower resolutions. Table III lists the hyperparameters of the NeRF architecture and hyperparameters for training the model.  C. Mesh processing details Table IV visualizes the different stages of the mesh processing needed to go from the raw mesh extracted from NeRF's occupancy network to a simplified mesh suitable for MuJoCo simulation.</p>
<p>D. Policy training details 1) Control limits: We clipped the inputs to actuator position controllers to be within the limits specified in Table V. For the ball pushing task, we further extended the limits of the head pan joint to be within (-2.5, 2.5) so that the policy can learn to actively look at the ball. In both tasks we found it important to limit the upper bound of the head tilt joint to prevent the policy from looking at the ceiling which is not well captured by the NeRF due to a lack of data in the input video.</p>
<p>2) Rewards: We used the following regularization rewards (from Section IV-C) to shape the robot's behavior and improve sim2real transfer:
r turn = −1. if ω yaw &gt; π else 0., r pose = 1 20 20 i=1 (q i − ref i ) 2 range 2 i , r speed = 0.3 − |v feet x − 0.3| 0.3 ,
where ω yaw is the angular velocity of the robot's gravityaligned body frame. The gravity-aligned body frame is obtained from the robot's torso frame by rotating it using the smallest rotation which aligns robot's frame negative zaxis with gravity direction. q are the robot's joint positions, ref are the corresponding reference poses, and range are the joint ranges (see Table V for specific values). v feet is the  Two examples of how the NeRF meshes are processed to make them suitable for simulation. The leftmost column shows the meshes of two different static environments, obtained by running marching cubes on a discretized occupancy grid from the trained NeRF and rotating, translating and cropping the resulting mesh to the extents of the scene. The central column shows the same meshes with the floor removed (as described in Section III-E). Note that for the raw mesh in the bottom left, a partial floor removal was done by choosing a high threshold for the occupancy within the marching cubes procedure. In the rightmost column the mesh is decomposed into convex sub-components (each sub-components in a different colour), which are passed to MuJoCo for collision detection. The convex decomposition step is specific to MuJoCo as it does not handle collisions between non-convex objects.</p>
<p>velocity of the midpoint of the robot's feet expressed in the gravity-aligned body frame. The task-specific reward components we used for the navigation task (Section IV-D) are r navigate sparse = 1. if ||x goal || &lt; 0.25 else 0.,
r navigate = 0.3 − v feet
x,y − 0.3
x goal ||x goal || /0.3,
where x goal is the 2d goal position in the robot's gravityaligned frame. The full reward we used for training the navigation policies is r navigation task = r turn + 0.5r speed + 0.5r pose + r navigate sparse + 0.25r navigate .</p>
<p>The task-specific reward for the ball pushing task (Section IV-D) works in two stages. If the ball is near the goal or is moving towards the goal, then we do not encourage any robot behavior. If this is not the case, then we encourage the robot to move towards the ball if it is away from it, otherwise we encourage it to stay close to the ball. Specifically, let d ball = ||x robot − x ball ||, d goal = ||x ball − x goal ||, and v ball ,  v goal the time derivatives of these distances. Let
ρ(d, v) = e −d 2 −v 2 + 1 − e −d 2 1 − e − min(0,v) 2 .
The task specific reward is
r ball = ρ(d goal , v goal )+(1 − ρ(d goal , v goal )) ρ(d ball , v ball )/2,
and the full reward for training ball pushing policies is r ball pushing task = r turn + 0.5r speed + 0.5r pose + r ball .</p>
<p>3) DMPO: Our DMPO implementation is based on</p>
<p>the open-source reference implementation https://github.com/deepmind/acme/tree/ master/acme/agents/tf/dmpo [67]. Readers can find more details about this algorithm in the MPO [69], [70] and distributional RL [68] papers, as well as in [76] which used the exact same implementation we are using. Here we only mention the differences between ours and the linked reference implementation:</p>
<ol>
<li>We use JAX [77] instead of TensorFlow. 2. We use a distributed asynchronous setting with separate actor and learner processes. 3. In order to support recurrent architectures, we calculate losses and gradients over batches of multi-step trajectories instead of batches of single-step transitions. 4. N-step returns are estimated from multi-step trajectories inside the critic loss rather than being accumulated in the actor process. Hyperparameters are listed in Table VI.</li>
</ol>
<p>E. Camera calibration and image augmentations</p>
<p>We calibrated the robot's Logitech C920 camera's focal length and distortion parameters using the camera calibration ROS package. In order to address the mismatch between the image intensity settings of the robot's camera and the camera used for data collection, we use image augmentations during policy training. Specifically we use the random brightness, random hue,   random contrast, and random saturation functions from the dm pix library [78] with the parameters listed in Table VII. We also apply random translations by up to 5% of the image height/width. However, these augmentations are not a replacement for calibration. We found the gain parameter of the camera to be particularly important and so we manually tuned it by comparing to the images used for training NeRF (we used gain=50 for the navigation task, and gain=128 for the ball pushing task).</p>
<p>The sensitivity to gain is quantified in Table VIII and the effects of varying gain on the camera images is visualized in Fig. 7. The policy is especially sensitive to high gain values (leading to brighter images) which cause it to walk in a small circle. It is less sensitive to small gain values for which it still navigates to the target but sometimes gets lost or hits obstacles.</p>
<p>Gain</p>
<p>Success rate Behavior 128 0/5 Robot walks in a small circle. 90 0/5 Robot walks in a small circle. 50 4/5 Robot consistently reaches the target. This is the default setting for our evaluation results shown in Section V-B 10 1/5 Robot often reaches the target (4/5) but hits obstacles on the way. 1 2/5 Robot occasionally reaches the target but often gets lost.   </p>
<p>F. Ablations and other results</p>
<p>We qualitatively evaluated our navigation policies in perturbed settings such as moving some of the fixed objects in the scene or having humans to walk around the scene. These evaluation runs are shown in the accompanying videos on our website. We were surprised that the "low-level" locomotion of the robot was disentangled from the "high-level" scene understanding even though the policies were trained end-toend. For example, when we changed the scene setup, the robot might walk in the wrong direction but it would retain a stable gait. Similarly, when we blocked the robot's camera, the robot would stop walking but it would not fall (and it would start walking again once we unblocked its view). Additionally, we often saw that the robot would reach the successful target even in these perturbed settings.</p>
<p>Fig. 3 :
3Our MuJoCo simulation is created by combining: (1) the learnt static scene mesh (Section III-E), (2) the dynamic object meshes and (3) the learnt static scene NeRF rendering (Section III-D) on which (4) the Mujoco rendering of dynamic objects (a ball and robot's left arm in the camera image above) are overlaid. Other dynamic parameters (e.g. friction) are assumed known or measured.</p>
<p>Fig. 1 (
1left) and Fig. 5 (bottom) for a visualization of our scene which measures 5m x 4m.</p>
<p>Fig. 4 :
4The policy's network architecture.</p>
<p>Fig. 5 :
5Localization performance of the agent. Top row: Robot camera images from a successful zero-shot transfer trial. The target is next to the potted plant. Bottom row: Visualization of the robot's belief over it's 2D position in the scene, shown as a heatmap on a top-down view of the scene. White X: Ground truth position from motion capture (not input to policy); Green Triangle: Target position.</p>
<p>Fig. 6 :
6Two different views of one of the captured scenes. For each view we show an example rendered from a model trained without (left) and with (right) our gaussian blur augmentation. We concatenate the diagonalised variance and train with augemented samples. This allows the network to effectively interpolate in scale space. Zooming in shows the reduced 'stair-stepping' artifacts in the renders.</p>
<p>VIII: Performance of the 80x60 resolution navigation policy when varying the camera's gain parameter. Both the goal position and the robot's initial position were fixed in all trials. Higher gain corresponds to brighter image, and 50 is the default value used in the navigation experiments.</p>
<p>Fig. 7 :
7The effect of changing the camera gain. The policy breaks when the gain is 128, and sometimes gets lost when it is 1.</p>
<p>TABLE I :
ISim &amp; Real performance (with standard errors) of the trained policies on the navigation and obstacle avoidance task. Time taken &amp; Localization error values are median statistics across all evaluation episodes.Sim Success 
Real Success 
Policy resolution 
Center 
Wall 
Center 
Wall 
30 x 40 
99% 
100% 
78±7% 
43±14% </p>
<p>TABLE II :
IISim &amp; real performance (with standard errors) of trained policies on the ball pushing task with ball initialized in the center of the arena vs in different cells near the wall.</p>
<p>TABLE III :
IIIList of NeRF hyperparameters.</p>
<p>TABLE IV :
IVMesh preprocessing.</p>
<p>TABLE V :
VJoint reference poses and limits.</p>
<p>TABLE VI :
VIList of DMPO hyperparameters.dm pix function 
function arguments 
random brightness 
max delta=32. / 255. 
random hue 
max delta=1. / 24. 
random contrast 
lower=0.5, upper=1.5 
random saturation 
lower=0.5, upper=1.5 </p>
<p>TABLE VII :
VIIImage augmentation settings.</p>
<p>TABLE</p>
<p>Learning agile and dynamic motor skills for legged robots. J Hwangbo, J Lee, A Dosovitskiy, D Bellicoso, V Tsounis, V Koltun, M Hutter, Sci Robot. 4J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis, V. Koltun, and M. Hutter, "Learning agile and dynamic motor skills for legged robots," Sci Robot, vol. 4, Jan. 2019.</p>
<p>Learning robust perceptive locomotion for quadrupedal robots in the wild. T Miki, J Lee, J Hwangbo, L Wellhausen, V Koltun, M Hutter, Science Robotics. 7622822T. Miki, J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter, "Learning robust perceptive locomotion for quadrupedal robots in the wild," Science Robotics, vol. 7, no. 62, p. eabk2822, 2022.</p>
<p>Solving rubik's cube with a robot hand. I Akkaya, M Andrychowicz, M Chociej, M Litwin, B Mcgrew, A Petron, A Paino, M Plappert, G Powell, R Ribas, arXiv:1910.07113arXiv preprintI. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A. Paino, M. Plappert, G. Powell, R. Ribas, et al., "Solving rubik's cube with a robot hand," arXiv preprint arXiv:1910.07113, 2019.</p>
<p>Sim-to-real transfer for vision-and-language navigation. P Anderson, A Shrivastava, J Truong, A Majumdar, D Parikh, D Batra, S Lee, PMLRConference on Robot Learning. 2021P. Anderson, A. Shrivastava, J. Truong, A. Majumdar, D. Parikh, D. Batra, and S. Lee, "Sim-to-real transfer for vision-and-language navigation," in Conference on Robot Learning, pp. 671-681, PMLR, 2021.</p>
<p>Matterport3d: Learning from rgb-d data in indoor environments. A Chang, A Dai, T Funkhouser, M Halber, M Niessner, M Savva, S Song, A Zeng, Y Zhang, arXiv:1709.06158arXiv preprintA. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner, M. Savva, S. Song, A. Zeng, and Y. Zhang, "Matterport3d: Learning from rgb-d data in indoor environments," arXiv preprint arXiv:1709.06158, 2017.</p>
<p>Gibson env: Real-world perception for embodied agents. F Xia, A R Zamir, Z He, A Sax, J Malik, S Savarese, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionF. Xia, A. R. Zamir, Z. He, A. Sax, J. Malik, and S. Savarese, "Gibson env: Real-world perception for embodied agents," in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 9068-9079, 2018.</p>
<p>J Straub, T Whelan, L Ma, Y Chen, E Wijmans, S Green, J J Engel, R Mur-Artal, C Ren, S Verma, arXiv:1906.05797The replica dataset: A digital replica of indoor spaces. arXiv preprintJ. Straub, T. Whelan, L. Ma, Y. Chen, E. Wijmans, S. Green, J. J. Engel, R. Mur-Artal, C. Ren, S. Verma, et al., "The replica dataset: A digital replica of indoor spaces," arXiv preprint arXiv:1906.05797, 2019.</p>
<p>Habitat 2.0: Training home assistants to rearrange their habitat. A Szot, A Clegg, E Undersander, E Wijmans, Y Zhao, J Turner, N Maestre, M Mukadam, D S Chaplot, O Maksymets, Advances in Neural Information Processing Systems. 34A. Szot, A. Clegg, E. Undersander, E. Wijmans, Y. Zhao, J. Turner, N. Maestre, M. Mukadam, D. S. Chaplot, O. Maksymets, et al., "Habitat 2.0: Training home assistants to rearrange their habitat," Advances in Neural Information Processing Systems, vol. 34, pp. 251- 266, 2021.</p>
<p>Nerf: Representing scenes as neural radiance fields for view synthesis. B Mildenhall, P P Srinivasan, M Tancik, J T Barron, R Ramamoorthi, R Ng, CoRR. B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoor- thi, and R. Ng, "Nerf: Representing scenes as neural radiance fields for view synthesis," CoRR, vol. abs/2003.08934, 2020.</p>
<p>Mip-nerf 360: Unbounded anti-aliased neural radiance fields. J T Barron, B Mildenhall, D Verbin, P P Srinivasan, P Hedman, abs/2111.12077CoRR. J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and P. Hedman, "Mip-nerf 360: Unbounded anti-aliased neural radiance fields," CoRR, vol. abs/2111.12077, 2021.</p>
<p>Instant neural graphics primitives with a multiresolution hash encoding. T Müller, A Evans, C Schied, A Keller, ACM Trans. Graph. 4115T. Müller, A. Evans, C. Schied, and A. Keller, "Instant neural graphics primitives with a multiresolution hash encoding," ACM Trans. Graph., vol. 41, pp. 102:1-102:15, July 2022.</p>
<p>Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps. C Reiser, S Peng, Y Liao, A Geiger, abs/2103.13744CoRR. C. Reiser, S. Peng, Y. Liao, and A. Geiger, "Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps," CoRR, vol. abs/2103.13744, 2021.</p>
<p>Block-nerf: Scalable large scene neural view synthesis. M Tancik, V Casser, X Yan, S Pradhan, B Mildenhall, P P Srinivasan, J T Barron, H Kretzschmar, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionM. Tancik, V. Casser, X. Yan, S. Pradhan, B. Mildenhall, P. P. Srinivasan, J. T. Barron, and H. Kretzschmar, "Block-nerf: Scalable large scene neural view synthesis," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8248- 8258, 2022.</p>
<p>Dnerf: Neural radiance fields for dynamic scenes. A Pumarola, E Corona, G Pons-Moll, F Moreno-Noguer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionA. Pumarola, E. Corona, G. Pons-Moll, and F. Moreno-Noguer, "D- nerf: Neural radiance fields for dynamic scenes," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10318-10327, 2021.</p>
<p>K Park, U Sinha, J T Barron, S Bouaziz, D B Goldman, S M Seitz, R Martin-Brualla, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionNerfies: Deformable neural radiance fieldsK. Park, U. Sinha, J. T. Barron, S. Bouaziz, D. B. Goldman, S. M. Seitz, and R. Martin-Brualla, "Nerfies: Deformable neural radiance fields," in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5865-5874, 2021.</p>
<p>Decomposing 3d scenes into objects via unsupervised volume segmentation. K Stelzner, K Kersting, A R Kosiorek, arXiv:2104.01148arXiv preprintK. Stelzner, K. Kersting, and A. R. Kosiorek, "Decomposing 3d scenes into objects via unsupervised volume segmentation," arXiv preprint arXiv:2104.01148, 2021.</p>
<p>Ref-nerf: Structured view-dependent appearance for neural radiance fields. D Verbin, P Hedman, B Mildenhall, T E Zickler, J T Barron, P P Srinivasan, abs/2112.03907CoRR. D. Verbin, P. Hedman, B. Mildenhall, T. E. Zickler, J. T. Barron, and P. P. Srinivasan, "Ref-nerf: Structured view-dependent appearance for neural radiance fields," CoRR, vol. abs/2112.03907, 2021.</p>
<p>Z Wang, S Wu, W Xie, M Chen, V A Prisacariu, arXiv:2102.07064Nerf-: Neural radiance fields without known camera parameters. arXiv preprintZ. Wang, S. Wu, W. Xie, M. Chen, and V. A. Prisacariu, "Nerf- : Neural radiance fields without known camera parameters," arXiv preprint arXiv:2102.07064, 2021.</p>
<p>imap: Implicit mapping and positioning in real-time. E Sucar, S Liu, J Ortiz, A J Davison, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionE. Sucar, S. Liu, J. Ortiz, and A. J. Davison, "imap: Implicit map- ping and positioning in real-time," in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 6229-6238, 2021.</p>
<p>Nerf in the wild: Neural radiance fields for unconstrained photo collections. R Martin-Brualla, N Radwan, M S Sajjadi, J T Barron, A Dosovitskiy, D Duckworth, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionR. Martin-Brualla, N. Radwan, M. S. Sajjadi, J. T. Barron, A. Doso- vitskiy, and D. Duckworth, "Nerf in the wild: Neural radiance fields for unconstrained photo collections," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7210- 7219, 2021.</p>
<p>Mobilenerf: Exploiting the polygon rasterization pipeline for efficient neural field rendering on mobile architectures. Z Chen, T Funkhouser, P Hedman, A Tagliasacchi, arXiv:2208.00277arXiv preprintZ. Chen, T. Funkhouser, P. Hedman, and A. Tagliasacchi, "Mobilenerf: Exploiting the polygon rasterization pipeline for efficient neural field rendering on mobile architectures," arXiv preprint arXiv:2208.00277, 2022.</p>
<p>L Yen-Chen, P Florence, J T Barron, A Rodriguez, P Isola, T.-Y. Lin, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2021inerf: Inverting neural radiance fields for pose estimationL. Yen-Chen, P. Florence, J. T. Barron, A. Rodriguez, P. Isola, and T.-Y. Lin, "inerf: Inverting neural radiance fields for pose estimation," in 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1323-1330, IEEE, 2021.</p>
<p>L Yen-Chen, P Florence, J T Barron, T.-Y Lin, A Rodriguez, P Isola, arXiv:2203.01913Nerf-supervision: Learning dense object descriptors from neural radiance fields. arXiv preprintL. Yen-Chen, P. Florence, J. T. Barron, T.-Y. Lin, A. Rodriguez, and P. Isola, "Nerf-supervision: Learning dense object descriptors from neural radiance fields," arXiv preprint arXiv:2203.01913, 2022.</p>
<p>Dex-nerf: Using a neural radiance field to grasp transparent objects. J Ichnowski, Y Avigal, J Kerr, K Goldberg, arXiv:2110.14217arXiv preprintJ. Ichnowski, Y. Avigal, J. Kerr, and K. Goldberg, "Dex-nerf: Using a neural radiance field to grasp transparent objects," arXiv preprint arXiv:2110.14217, 2021.</p>
<p>Learning multi-object dynamics with compositional neural radiance fields. D Driess, Z Huang, Y Li, R Tedrake, M Toussaint, arXiv:2202.11855arXiv preprintD. Driess, Z. Huang, Y. Li, R. Tedrake, and M. Toussaint, "Learning multi-object dynamics with compositional neural radiance fields," arXiv preprint arXiv:2202.11855, 2022.</p>
<p>Vision-only robot navigation in a neural radiance world. M Adamkiewicz, T Chen, A Caccavale, R Gardner, P Culbertson, J Bohg, M Schwager, IEEE Robotics and Automation Letters. 72M. Adamkiewicz, T. Chen, A. Caccavale, R. Gardner, P. Culbertson, J. Bohg, and M. Schwager, "Vision-only robot navigation in a neural radiance world," IEEE Robotics and Automation Letters, vol. 7, no. 2, pp. 4606-4613, 2022.</p>
<p>. S K Ramakrishnan, A Gokaslan, E Wijmans, O Maksymets, A Clegg, J Turner, E Undersander, W Galuba, A Westbury, A X , S. K. Ramakrishnan, A. Gokaslan, E. Wijmans, O. Maksymets, A. Clegg, J. Turner, E. Undersander, W. Galuba, A. Westbury, A. X.</p>
<p>Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai. Chang, arXiv:2109.08238arXiv preprintChang, et al., "Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai," arXiv preprint arXiv:2109.08238, 2021.</p>
<p>Habitat: A platform for embodied ai research. M Savva, A Kadian, O Maksymets, Y Zhao, E Wijmans, B Jain, J Straub, J Liu, V Koltun, J Malik, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionM. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain, J. Straub, J. Liu, V. Koltun, J. Malik, et al., "Habitat: A platform for embodied ai research," in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9339-9347, 2019.</p>
<p>igibson 1.0: a simulation environment for interactive tasks in large realistic scenes. B Shen, F Xia, C Li, R Martín-Martín, L Fan, G Wang, C Pérez-D&apos;arpino, S Buch, S Srivastava, L Tchapmi, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2021B. Shen, F. Xia, C. Li, R. Martín-Martín, L. Fan, G. Wang, C. Pérez- D'Arpino, S. Buch, S. Srivastava, L. Tchapmi, et al., "igibson 1.0: a simulation environment for interactive tasks in large realistic scenes," in 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 7520-7527, IEEE, 2021.</p>
<p>Ai2-thor: An interactive 3d environment for visual ai. E Kolve, R Mottaghi, W Han, E Vanderbilt, L Weihs, A Herrasti, D Gordon, Y Zhu, A Gupta, A Farhadi, arXiv:1712.05474arXiv preprintE. Kolve, R. Mottaghi, W. Han, E. VanderBilt, L. Weihs, A. Herrasti, D. Gordon, Y. Zhu, A. Gupta, and A. Farhadi, "Ai2-thor: An interactive 3d environment for visual ai," arXiv preprint arXiv:1712.05474, 2017.</p>
<p>Robothor: An open simulation-to-real embodied ai platform. M Deitke, W Han, A Herrasti, A Kembhavi, E Kolve, R Mottaghi, J Salvador, D Schwenk, E Vanderbilt, M Wallingford, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionM. Deitke, W. Han, A. Herrasti, A. Kembhavi, E. Kolve, R. Mottaghi, J. Salvador, D. Schwenk, E. VanderBilt, M. Wallingford, et al., "Robothor: An open simulation-to-real embodied ai platform," in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 3164-3174, 2020.</p>
<p>On evaluation of embodied navigation agents. P Anderson, A Chang, D S Chaplot, A Dosovitskiy, S Gupta, V Koltun, J Kosecka, J Malik, R Mottaghi, M Savva, arXiv:1807.06757arXiv preprintP. Anderson, A. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta, V. Koltun, J. Kosecka, J. Malik, R. Mottaghi, M. Savva, et al., "On evaluation of embodied navigation agents," arXiv preprint arXiv:1807.06757, 2018.</p>
<p>Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames. E Wijmans, A Kadian, A Morcos, S Lee, I Essa, D Parikh, M Savva, D Batra, arXiv:1911.00357arXiv preprintE. Wijmans, A. Kadian, A. Morcos, S. Lee, I. Essa, D. Parikh, M. Savva, and D. Batra, "Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames," arXiv preprint arXiv:1911.00357, 2019.</p>
<p>Learning exploration policies for navigation. T Chen, S Gupta, A Gupta, arXiv:1903.01959arXiv preprintT. Chen, S. Gupta, and A. Gupta, "Learning exploration policies for navigation," arXiv preprint arXiv:1903.01959, 2019.</p>
<p>Simple but effective: Clip embeddings for embodied ai. A Khandelwal, L Weihs, R Mottaghi, A Kembhavi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionA. Khandelwal, L. Weihs, R. Mottaghi, and A. Kembhavi, "Simple but effective: Clip embeddings for embodied ai," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14829-14838, 2022.</p>
<p>Objectnav revisited: On evaluation of embodied agents navigating to objects. D Batra, A Gokaslan, A Kembhavi, O Maksymets, R Mottaghi, M Savva, A Toshev, E Wijmans, arXiv:2006.13171arXiv preprintD. Batra, A. Gokaslan, A. Kembhavi, O. Maksymets, R. Mottaghi, M. Savva, A. Toshev, and E. Wijmans, "Objectnav revisited: On evaluation of embodied agents navigating to objects," arXiv preprint arXiv:2006.13171, 2020.</p>
<p>Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. P Anderson, Q Wu, D Teney, J Bruce, M Johnson, N Sünderhauf, I Reid, S Gould, A Van Den, Hengel, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionP. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. Sünderhauf, I. Reid, S. Gould, and A. Van Den Hengel, "Vision-and-language nav- igation: Interpreting visually-grounded navigation instructions in real environments," in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3674-3683, 2018.</p>
<p>Bi-directional domain adaptation for sim2real transfer of embodied navigation agents. J Truong, S Chernova, D Batra, IEEE Robotics and Automation Letters. 62J. Truong, S. Chernova, and D. Batra, "Bi-directional domain adap- tation for sim2real transfer of embodied navigation agents," IEEE Robotics and Automation Letters, vol. 6, no. 2, pp. 2634-2641, 2021.</p>
<p>Learning navigation skills for legged robots with learned robot embeddings. J Truong, D Yarats, T Li, F Meier, S Chernova, D Batra, A Rai, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2021J. Truong, D. Yarats, T. Li, F. Meier, S. Chernova, D. Batra, and A. Rai, "Learning navigation skills for legged robots with learned robot embeddings," in 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 484-491, IEEE, 2021.</p>
<p>Coupling vision and proprioception for navigation of legged robots. Z Fu, A Kumar, A Agarwal, H Qi, J Malik, D Pathak, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionZ. Fu, A. Kumar, A. Agarwal, H. Qi, J. Malik, and D. Pathak, "Coupling vision and proprioception for navigation of legged robots," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 17273-17283, 2022.</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEEJ. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, "Domain randomization for transferring deep neural networks from simulation to the real world," in 2017 IEEE/RSJ international con- ference on intelligent robots and systems (IROS), pp. 23-30, IEEE, 2017.</p>
<p>Unsupervised pixel-level domain adaptation with generative adversarial networks. K Bousmalis, N Silberman, D Dohan, D Erhan, D Krishnan, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionK. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Krishnan, "Unsupervised pixel-level domain adaptation with generative adver- sarial networks," in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3722-3731, 2017.</p>
<p>Closing the sim-to-real loop: Adapting simulation randomization with real world experience. Y Chebotar, A Handa, V Makoviychuk, M Macklin, J Issac, N Ratliff, D Fox, 2019 International Conference on Robotics and Automation (ICRA). IEEEY. Chebotar, A. Handa, V. Makoviychuk, M. Macklin, J. Issac, N. Ratliff, and D. Fox, "Closing the sim-to-real loop: Adapting simula- tion randomization with real world experience," in 2019 International Conference on Robotics and Automation (ICRA), pp. 8973-8979, IEEE, 2019.</p>
<p>Using simulation and domain adaptation to improve efficiency of deep robotic grasping. K Bousmalis, A Irpan, P Wohlhart, Y Bai, M Kelcey, M Kalakrishnan, L Downs, J Ibarz, P Pastor, K Konolige, 2018 IEEE international conference on robotics and automation (ICRA). IEEEK. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kelcey, M. Kalakr- ishnan, L. Downs, J. Ibarz, P. Pastor, K. Konolige, et al., "Using simulation and domain adaptation to improve efficiency of deep robotic grasping," in 2018 IEEE international conference on robotics and automation (ICRA), pp. 4243-4250, IEEE, 2018.</p>
<p>Beyond pick-and-place: Tackling robotic stacking of diverse shapes. A X Lee, C M Devin, Y Zhou, T Lampe, K Bousmalis, J T Springenberg, A Byravan, A Abdolmaleki, N Gileadi, D Khosid, 5th Annual Conference on Robot Learning. A. X. Lee, C. M. Devin, Y. Zhou, T. Lampe, K. Bousmalis, J. T. Springenberg, A. Byravan, A. Abdolmaleki, N. Gileadi, D. Khosid, et al., "Beyond pick-and-place: Tackling robotic stacking of diverse shapes," in 5th Annual Conference on Robot Learning, 2021.</p>
<p>Cad2rl: Real single-image flight without a single real image. F Sadeghi, S Levine, arXiv:1611.04201arXiv preprintF. Sadeghi and S. Levine, "Cad2rl: Real single-image flight without a single real image," arXiv preprint arXiv:1611.04201, 2016.</p>
<p>Learning agile and dynamic motor skills for legged robots. J Hwangbo, J Lee, A Dosovitskiy, D Bellicoso, V Tsounis, V Koltun, M Hutter, Science Robotics. 426J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis, V. Koltun, and M. Hutter, "Learning agile and dynamic motor skills for legged robots," Science Robotics, vol. 4, no. 26, 2019.</p>
<p>Learning quadrupedal locomotion over challenging terrain. J Lee, J Hwangbo, L Wellhausen, V Koltun, M Hutter, Science Robotics. 55986J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter, "Learning quadrupedal locomotion over challenging terrain," Science Robotics, vol. 5, p. eabc5986, Oct 2020.</p>
<p>Learning agile robotic locomotion skills by imitating animals. X B Peng, E Coumans, T Zhang, T.-W Lee, J Tan, S Levine, arXiv:2004.00784arXiv preprintX. B. Peng, E. Coumans, T. Zhang, T.-W. Lee, J. Tan, and S. Levine, "Learning agile robotic locomotion skills by imitating animals," arXiv preprint arXiv:2004.00784, 2020.</p>
<p>S Bohez, S Tunyasuvunakool, P Brakel, F Sadeghi, L Hasenclever, Y Tassa, E Parisotto, J Humplik, T Haarnoja, R Hafner, M Wulfmeier, M Neunert, B Moran, N Siegel, A Huber, F Romano, N Batchelor, F Casarini, J Merel, R Hadsell, N Heess, Imitate and repurpose: Learning reusable robot movement skills from human and animal behaviors. arXivS. Bohez, S. Tunyasuvunakool, P. Brakel, F. Sadeghi, L. Hasen- clever, Y. Tassa, E. Parisotto, J. Humplik, T. Haarnoja, R. Hafner, M. Wulfmeier, M. Neunert, B. Moran, N. Siegel, A. Huber, F. Romano, N. Batchelor, F. Casarini, J. Merel, R. Hadsell, and N. Heess, "Imitate and repurpose: Learning reusable robot movement skills from human and animal behaviors," arXiv, Mar. 2022.</p>
<p>Sim-to-real transfer for biped locomotion. W Yu, V C Kumar, G Turk, C K Liu, arXiv:1903.01390arXiv preprintW. Yu, V. C. Kumar, G. Turk, and C. K. Liu, "Sim-to-real transfer for biped locomotion," arXiv preprint arXiv:1903.01390, 2019.</p>
<p>Reinforcement learning for robust parameterized locomotion control of bipedal robots. Z Li, X Cheng, X B Peng, P Abbeel, S Levine, G Berseth, K Sreenath, arXiv:2103.14295arXiv preprintZ. Li, X. Cheng, X. B. Peng, P. Abbeel, S. Levine, G. Berseth, and K. Sreenath, "Reinforcement learning for robust parameterized loco- motion control of bipedal robots," arXiv preprint arXiv:2103.14295, 2021.</p>
<p>Blind bipedal stair traversal via sim-to-real reinforcement learning. J Siekmann, K Green, J Warila, A Fern, J Hurst, arXiv:2105.08328arXiv preprintJ. Siekmann, K. Green, J. Warila, A. Fern, and J. Hurst, "Blind bipedal stair traversal via sim-to-real reinforcement learning," arXiv preprint arXiv:2105.08328, 2021.</p>
<p>Structure-from-motion revisited. J L Schönberger, J.-M Frahm, Conference on Computer Vision and Pattern Recognition (CVPR). J. L. Schönberger and J.-M. Frahm, "Structure-from-motion revisited," in Conference on Computer Vision and Pattern Recognition (CVPR), 2016.</p>
<p>Pixelwise view selection for unstructured multi-view stereo. J L Schönberger, E Zheng, M Pollefeys, J.-M Frahm, European Conference on Computer Vision (ECCV). J. L. Schönberger, E. Zheng, M. Pollefeys, and J.-M. Frahm, "Pixel- wise view selection for unstructured multi-view stereo," in European Conference on Computer Vision (ECCV), 2016.</p>
<p>A vote-and-verify strategy for fast spatial verification in image retrieval. J L Schönberger, T Price, T Sattler, J.-M Frahm, M Pollefeys, Asian Conference on Computer Vision (ACCV). J. L. Schönberger, T. Price, T. Sattler, J.-M. Frahm, and M. Pollefeys, "A vote-and-verify strategy for fast spatial verification in image retrieval," in Asian Conference on Computer Vision (ACCV), 2016.</p>
<p>J T Barron, B Mildenhall, M Tancik, P Hedman, R Martin-Brualla, P P Srinivasan, abs/2103.13415Mip-nerf: A multiscale representation for antialiasing neural radiance fields. J. T. Barron, B. Mildenhall, M. Tancik, P. Hedman, R. Martin-Brualla, and P. P. Srinivasan, "Mip-nerf: A multiscale representation for anti- aliasing neural radiance fields," CoRR, vol. abs/2103.13415, 2021.</p>
<p>Layer normalization. J L Ba, J R Kiros, G E Hinton, J. L. Ba, J. R. Kiros, and G. E. Hinton, "Layer normalization," 2016.</p>
<p>Searching for activation functions. P Ramachandran, B Zoph, Q V Le, abs/1710.05941CoRR. P. Ramachandran, B. Zoph, and Q. V. Le, "Searching for activation functions," CoRR, vol. abs/1710.05941, 2017.</p>
<p>Nerftex: Neural reflectance field textures. H Baatz, J Granskog, M Papas, F Rousselle, J Novák, Eurographics Symposium on Rendering, The Eurographics Association. H. Baatz, J. Granskog, M. Papas, F. Rousselle, and J. Novák, "Nerf- tex: Neural reflectance field textures," in Eurographics Symposium on Rendering, The Eurographics Association, June 2021.</p>
<p>Marching cubes: A high resolution 3d surface construction algorithm. W E Lorensen, H E Cline, ACM siggraph computer graphics. 214W. E. Lorensen and H. E. Cline, "Marching cubes: A high resolution 3d surface construction algorithm," ACM siggraph computer graphics, vol. 21, no. 4, pp. 163-169, 1987.</p>
<p>Blender -a 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation. B O Community, AmsterdamB. O. Community, Blender -a 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018.</p>
<p>Mujoco: A physics engine for model-based control. E Todorov, T Erez, Y Tassa, 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEEE. Todorov, T. Erez, and Y. Tassa, "Mujoco: A physics engine for model-based control," in 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033, IEEE, 2012.</p>
<p>Learning object-compositional neural radiance field for editable scene rendering. B Yang, Y Zhang, Y Xu, Y Li, H Zhou, H Bao, G Zhang, Z Cui, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionB. Yang, Y. Zhang, Y. Xu, Y. Li, H. Zhou, H. Bao, G. Zhang, and Z. Cui, "Learning object-compositional neural radiance field for ed- itable scene rendering," in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 13779-13788, 2021.</p>
<p>Robotis op3. "Robotis op3." https://emanual.robotis.com/docs/en/ platform/op3/introduction/.</p>
<p>An efficient orientation filter for inertial and inertial/magnetic sensor arrays. S Madgwick, Report x-io and University of Bristol (UK). 25S. Madgwick et al., "An efficient orientation filter for inertial and inertial/magnetic sensor arrays," Report x-io and University of Bristol (UK), vol. 25, pp. 113-118, 2010.</p>
<p>Acme: A research framework for distributed reinforcement learning. M Hoffman, B Shahriari, J Aslanides, G Barth-Maron, F Behbahani, T Norman, A Abdolmaleki, A Cassirer, F Yang, K Baumli, S Henderson, A Novikov, S G Colmenarejo, S Cabi, C Gulcehre, T L Paine, A Cowie, Z Wang, B Piot, N De Freitas, arXiv:2006.00979arXiv preprintM. Hoffman, B. Shahriari, J. Aslanides, G. Barth-Maron, F. Behba- hani, T. Norman, A. Abdolmaleki, A. Cassirer, F. Yang, K. Baumli, S. Henderson, A. Novikov, S. G. Colmenarejo, S. Cabi, C. Gulcehre, T. L. Paine, A. Cowie, Z. Wang, B. Piot, and N. de Freitas, "Acme: A research framework for distributed reinforcement learning," arXiv preprint arXiv:2006.00979, 2020.</p>
<p>A distributional perspective on reinforcement learning. M G Bellemare, W Dabney, R Munos, PMLRInternational Conference on Machine Learning. M. G. Bellemare, W. Dabney, and R. Munos, "A distributional per- spective on reinforcement learning," in International Conference on Machine Learning, pp. 449-458, PMLR, 2017.</p>
<p>Maximum a posteriori policy optimisation. A Abdolmaleki, J T Springenberg, Y Tassa, R Munos, N Heess, M Riedmiller, A. Abdolmaleki, J. T. Springenberg, Y. Tassa, R. Munos, N. Heess, and M. Riedmiller, "Maximum a posteriori policy optimisation," 2018.</p>
<p>Relative entropy regularized policy iteration. A Abdolmaleki, J T Springenberg, J Degrave, S Bohez, Y Tassa, D Belov, N Heess, M Riedmiller, A. Abdolmaleki, J. T. Springenberg, J. Degrave, S. Bohez, Y. Tassa, D. Belov, N. Heess, and M. Riedmiller, "Relative entropy regularized policy iteration," 2018.</p>
<p>Volume rendering of neural implicit surfaces. L Yariv, J Gu, Y Kasten, Y Lipman, abs/2106.12052CoRR. L. Yariv, J. Gu, Y. Kasten, and Y. Lipman, "Volume rendering of neural implicit surfaces," CoRR, vol. abs/2106.12052, 2021.</p>
<p>Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. P Wang, L Liu, Y Liu, C Theobalt, T Komura, W Wang, abs/2106.10689CoRR. P. Wang, L. Liu, Y. Liu, C. Theobalt, T. Komura, and W. Wang, "Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction," CoRR, vol. abs/2106.10689, 2021.</p>
<p>Object-centric neural scene rendering. M Guo, A Fathi, J Wu, T Funkhouser, arXiv:2012.08503arXiv preprintM. Guo, A. Fathi, J. Wu, and T. Funkhouser, "Object-centric neural scene rendering," arXiv preprint arXiv:2012.08503, 2020.</p>
<p>Learning to walk in minutes using massively parallel deep reinforcement learning. N Rudin, D Hoeller, P Reist, M Hutter, 5th Annual Conference on Robot Learning. N. Rudin, D. Hoeller, P. Reist, and M. Hutter, "Learning to walk in minutes using massively parallel deep reinforcement learning," in 5th Annual Conference on Robot Learning, 2021.</p>
<p>Squareplus: A softplus-like algebraic rectifier. J T Barron, abs/2112.11687CoRR. J. T. Barron, "Squareplus: A softplus-like algebraic rectifier," CoRR, vol. abs/2112.11687, 2021.</p>
<p>Towards real robot learning in the wild: A case study in bipedal locomotion. M Bloesch, J Humplik, V Patraucean, R Hafner, T Haarnoja, A Byravan, N Y Siegel, S Tunyasuvunakool, F Casarini, N Batchelor, F Romano, S Saliceti, M Riedmiller, S M A Eslami, N Heess, 5th Annual Conference on Robot Learning. M. Bloesch, J. Humplik, V. Patraucean, R. Hafner, T. Haarnoja, A. Byravan, N. Y. Siegel, S. Tunyasuvunakool, F. Casarini, N. Batch- elor, F. Romano, S. Saliceti, M. Riedmiller, S. M. A. Eslami, and N. Heess, "Towards real robot learning in the wild: A case study in bipedal locomotion," in 5th Annual Conference on Robot Learning, 2021.</p>
<p>JAX: composable transformations of Python+NumPy programs. J Bradbury, R Frostig, P Hawkins, M J Johnson, C Leary, D Maclaurin, G Necula, A Paszke, J Vanderplas, S Wanderman-Milne, Q Zhang, J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman- Milne, and Q. Zhang, "JAX: composable transformations of Python+NumPy programs," 2018.</p>
<p>I Babuschkin, K Baumli, A Bell, S Bhupatiraju, J Bruce, P Buchlovsky, D Budden, T Cai, A Clark, I Danihelka, C Fantacci, J Godwin, C Jones, R Hemsley, T Hennigan, M Hessel, S Hou, S Kapturowski, T Keck, I Kemaev, M King, M Kunesch, L Martens, H Merzic, V Mikulik, T Norman, J Quan, G Papamakarios, R Ring, F Ruiz, A Sanchez, R Schneider, E Sezener, S Spencer, S Srinivasan, L Wang, W Stokowiec, F Viola, The DeepMind JAX Ecosystem. I. Babuschkin, K. Baumli, A. Bell, S. Bhupatiraju, J. Bruce, P. Buchlovsky, D. Budden, T. Cai, A. Clark, I. Danihelka, C. Fan- tacci, J. Godwin, C. Jones, R. Hemsley, T. Hennigan, M. Hessel, S. Hou, S. Kapturowski, T. Keck, I. Kemaev, M. King, M. Kunesch, L. Martens, H. Merzic, V. Mikulik, T. Norman, J. Quan, G. Papa- makarios, R. Ring, F. Ruiz, A. Sanchez, R. Schneider, E. Sezener, S. Spencer, S. Srinivasan, L. Wang, W. Stokowiec, and F. Viola, "The DeepMind JAX Ecosystem," 2020.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-163 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-163</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-163</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-8.html">extraction-schema-8</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of evaluations of LLMs on theory-of-mind tasks, focusing on model architecture, training data, task performance, explicit mental state and recursive belief representations, limitations, and proposed improvements.</div>
                <p><strong>Paper ID:</strong> paper-6559a72f4b63681542f63508268fa139d8693101</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e163.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e163.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of evaluations of LLMs on theory-of-mind tasks, focusing on model architecture, training data, task performance, explicit mental state and recursive belief representations, limitations, and proposed improvements.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T@MBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Theory of Mind Benchmark for Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A systematic evaluation framework designed to assess the theory-of-mind capabilities of large language models (LLMs) through 8 tasks and 31 abilities in social cognition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>T@MBench: Benchmarking Theory of Mind in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A state-of-the-art autoregressive transformer model developed by OpenAI, designed to perform a wide range of tasks including natural language understanding and generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unknown</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_nature</strong></td>
                            <td>Trained on diverse text corpora, potentially including social scenarios, which may influence its performance on ToM tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>False Belief Task</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluates the ability to understand that others can hold beliefs about the world that differ from reality, specifically in scenarios where one character is unaware of changes made to an object.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>First-order belief</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GPT-4's performance on the false belief task was noted to be higher than human performance in some instances, but overall, it lagged behind human performance by over 10 percentage points.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Vanilla and CoT prompting methods were used for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_reported</strong></td>
                            <td>LLMs showed significant performance drops when faced with trivial alterations to test samples, indicating reliance on spurious correlations rather than true understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_mental_state_representation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_recursive_belief_modeling</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>multimodal_grounding_reported</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>symbolic_hybrid_methods</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>LLMs like GPT-4 can outperform humans in specific tasks, suggesting some level of ToM capability.</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_improvements</strong></td>
                            <td>Future work should focus on expanding the range of ToM assessments and improving LLMs' understanding of mental states through enhanced training methods.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_of_model_size</strong></td>
                            <td>Larger models like GPT-4 generally perform better on ToM tasks, but the exact correlation is not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_performance</strong></td>
                            <td>GPT-4's average performance is significantly lower than human levels by over 10 percentage points across various ToM tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sparks of artificial general intelligence: Early experiments with gpt-4 <em>(Rating: 2)</em></li>
                <li>Theory of mind may have spontaneously emerged in large language models <em>(Rating: 1)</em></li>
                <li>Large language models fail on trivial alterations to theory-of-mind tasks <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-163",
    "paper_id": "paper-6559a72f4b63681542f63508268fa139d8693101",
    "extraction_schema_id": "extraction-schema-8",
    "extracted_data": [
        {
            "name_short": "T@MBench",
            "name_full": "Theory of Mind Benchmark for Large Language Models",
            "brief_description": "A systematic evaluation framework designed to assess the theory-of-mind capabilities of large language models (LLMs) through 8 tasks and 31 abilities in social cognition.",
            "citation_title": "T@MBench: Benchmarking Theory of Mind in Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "A state-of-the-art autoregressive transformer model developed by OpenAI, designed to perform a wide range of tasks including natural language understanding and generation.",
            "model_size": "unknown",
            "training_data_nature": "Trained on diverse text corpora, potentially including social scenarios, which may influence its performance on ToM tasks.",
            "task_name": "False Belief Task",
            "task_description": "Evaluates the ability to understand that others can hold beliefs about the world that differ from reality, specifically in scenarios where one character is unaware of changes made to an object.",
            "task_type": "First-order belief",
            "performance": "GPT-4's performance on the false belief task was noted to be higher than human performance in some instances, but overall, it lagged behind human performance by over 10 percentage points.",
            "evaluation_method": "Vanilla and CoT prompting methods were used for evaluation.",
            "limitations_reported": "LLMs showed significant performance drops when faced with trivial alterations to test samples, indicating reliance on spurious correlations rather than true understanding.",
            "evidence_of_mental_state_representation": true,
            "evidence_of_recursive_belief_modeling": false,
            "multimodal_grounding_reported": null,
            "symbolic_hybrid_methods": "None reported.",
            "counter_evidence": "LLMs like GPT-4 can outperform humans in specific tasks, suggesting some level of ToM capability.",
            "proposed_improvements": "Future work should focus on expanding the range of ToM assessments and improving LLMs' understanding of mental states through enhanced training methods.",
            "impact_of_model_size": "Larger models like GPT-4 generally perform better on ToM tasks, but the exact correlation is not quantified.",
            "comparison_to_human_performance": "GPT-4's average performance is significantly lower than human levels by over 10 percentage points across various ToM tasks.",
            "uuid": "e163.0"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "rating": 2
        },
        {
            "paper_title": "Theory of mind may have spontaneously emerged in large language models",
            "rating": 1
        },
        {
            "paper_title": "Large language models fail on trivial alterations to theory-of-mind tasks",
            "rating": 2
        }
    ],
    "cost": 0.004554,
    "model_str": null
}</code></pre>
        </div>

    </div>
</body>
</html>
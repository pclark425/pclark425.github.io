<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8919 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8919</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8919</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-156.html">extraction-schema-156</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <p><strong>Paper ID:</strong> paper-26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810" target="_blank">ConceptNet 5.5: An Open Multilingual Graph of General Knowledge</a></p>
                <p><strong>Paper Venue:</strong> AAAI Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> A new version of the linked open data resource ConceptNet is presented that is particularly well suited to be used with modern NLP techniques such as word embeddings, with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improvements on applications of word vectors, including solving SAT-style analogies.</p>
                <p><strong>Paper Abstract:</strong> 
 
 Machine learning about language can be improved by supplying it with specific knowledge and sources of external information. We present here a new version of the linked open data resource ConceptNet that is particularly well suited to be used with modern NLP techniques such as word embeddings. ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use. When ConceptNet is combined with word embeddings acquired from distributional semantics (such as word2vec), it provides applications with understanding that they would not acquire from distributional semantics alone, nor from narrower resources such as WordNet or DBPedia. We demonstrate this with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improvements on applications of word vectors, including solving SAT-style analogies.
 
</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8919.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8919.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ConceptNet (knowledge graph)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ConceptNet 5.5 (knowledge graph / semantic network)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large, multilingual semantic network that represents common-sense knowledge as nodes for words/phrases and labeled, weighted directed edges for a closed set of semantic relations; used here as both a symbolic relational resource and as a basis for embedding-based semantic spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>knowledge graph / semantic network</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Conceptual knowledge is stored as a graph whose nodes are words or phrases (often ambiguous lexical forms) and whose edges are labeled relations (IsA, PartOf, UsedFor, SimilarTo, etc.); each assertion is a triple (start node, relation, end node) with weights and provenance metadata, enabling traversal, linking to external resources, and serving as constraints that relate terms across languages and sources.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>symbolic / relational (graph-based)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Intrinsic word relatedness evaluations (MEN-3000, WordSim-353, RW, MTurk-771), downstream tasks of SAT-style proportional analogies, Story Cloze Test (choosing sensible story endings), multilingual propagation of meaning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using ConceptNet as a source of relational knowledge improves measurable semantic performance when incorporated with distributional methods; a ConceptNet-only embedding (ConceptNet-PPMI) can be derived and the graph's edges enable multilingual propagation. ConceptNet-based hybrid embeddings (ConceptNet Numberbatch) achieve state-of-the-art correlations on multiple word-relatedness datasets and strong performance on SAT analogies (Numberbatch accuracy = 56.1% overall, 58.8% on held-out half) and high Spearman correlations on MEN (.866 final), WordSim-353 (.828), MTurk-771 (.810), Rare Words (.601).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Compared directly to distributional-only embeddings (word2vec, GloVe, LexVec) and to purely distributional PPMI/SVD methods, ConceptNet as a relational resource combined with distributional vectors (via retrofitting/merging) yields superior performance; ConceptNet alone (as ConceptNet-PPMI) is useful but the hybrid outperforms either source alone, indicating complementary strengths.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>The graph by itself does not solve tasks that require fine event-structure understanding (bag-of-vectors on Story Cloze yields only ~59% accuracy when using any high-quality embeddings), and ConceptNet's use of ambiguous lexical nodes may limit fine-grained sense distinctions. Practical limitations include the need to prune low-degree nodes when building matrices and that directionality of some relations is functionally unimportant (some relations marked symmetric).</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Relational, symbolic common-sense knowledge represented in a graph can and should be combined with continuous distributional representations to produce more human-aligned semantic spaces; the graph provides multilingual links and explicit semantic constraints that improve generalization and downstream reasoning-like tasks when fused with distributional data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ConceptNet 5.5: An Open Multilingual Graph of General Knowledge', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8919.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8919.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Distributional word embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distributional word embeddings (word2vec, GloVe, LexVec and similar)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Represent words as dense real-valued vectors learned from co-occurrence statistics in large corpora; semantic relatedness corresponds to vector similarity in continuous space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>distributed representation / vector space model</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Conceptual knowledge is represented functionally as points in a continuous high-dimensional vector space learned from distributional patterns (prediction or count-based methods); proximity and vector arithmetic capture similarity, relatedness, and some relational structure.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>distributed / continuous</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Word relatedness benchmarking (MEN-3000, WordSim-353, RW, MTurk-771); proportional analogy solving via vector arithmetic; Story Cloze Test via bag-of-vectors sentence representations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretrained distributional embeddings (word2vec, GloVe, LexVec) provide strong baselines for relatedness and analogy tasks, but in the experiments reported here they underperform the hybrid ConceptNet Numberbatch. Example: word2vec achieved ~.486 accuracy on SAT analogies (in Table 2) while Numberbatch reached .561, indicating gains from adding relational knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Distributional embeddings capture continuous similarity and can be improved by incorporating symbolic relational knowledge (retrofitting). The hybrid (Numberbatch) outperforms distributional-only methods on multiple evaluations, showing that distributional information plus relational constraints is superior to distributional alone for these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Pure distributional vectors lack explicit relational labels and struggle with some relational analogy tasks unless tuned (also they may fail to capture multilingual links without additional resources). They also exhibit shortcomings on rare-word benchmarks unless trained on massive corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Distributed vectors are functionally effective for measuring relatedness and supporting simple compositional operations (averaging, vector arithmetic), but they benefit from explicit relational knowledge to better align with human semantic judgments and to generalize across languages.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ConceptNet 5.5: An Open Multilingual Graph of General Knowledge', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8919.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8919.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ConceptNet-PPMI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ConceptNet-PPMI (count-based embeddings from ConceptNet)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Embeddings derived directly from the ConceptNet graph by constructing a term-term cooccurrence matrix from graph edges, computing PPMI with context smoothing, and reducing dimensionality with truncated SVD to produce dense vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>count-based PPMI matrix → dense vectors</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Treat the ConceptNet graph as a sparse symmetric term-term cooccurrence matrix where each cell aggregates edge weights between terms; compute pointwise mutual information (with smoothing), clip negatives to obtain PPMI, and apply truncated SVD to yield 300-dimensional dense vectors that functionally encode graph connectivity as vector similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>distributed (count-based) derived from symbolic graph — hybrid methodology</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Used as an intrinsic semantic space for word relatedness and as a component in downstream tasks (analogy), compared to other embedding sources.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ConceptNet-PPMI provides embeddings that implicitly represent ConceptNet's graph structure and allow measuring connectedness between nodes; when used alone it is less effective than the hybrid Numberbatch but provides useful relational signal that, when combined with distributional embeddings, improves performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Compared to pure distributional embeddings, ConceptNet-PPMI is derived from explicit relational data rather than text co-occurrence and thus encodes commonsense relations; in the experiments the hybrid that includes both ConceptNet-PPMI and distributional sources outperforms ConceptNet-PPMI alone, indicating complementarity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Requires pruning of low-degree nodes for computational tractability (nodes with fewer than three edges removed during matrix construction), and restored vectors for pruned nodes are approximated by neighbors' averages; as a graph-derived count method it may miss distributional nuances captured by large text corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>A count-based transformation of symbolic knowledge into distributed vectors provides a principled bridge between relational knowledge and vector-space semantics, enabling graph connectivity to functionally operate as contextual cooccurrence for distributional methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ConceptNet 5.5: An Open Multilingual Graph of General Knowledge', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8919.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8919.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrofitting / Expanded retrofitting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrofitting word vectors to semantic lexicons (and expanded retrofitting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A post-processing optimization that adjusts pre-trained word vectors to be closer to their neighbors in a knowledge graph while remaining close to their original values; expanded retrofitting extends this to the larger vocabulary of the graph including OOV terms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrofitting word vectors to semantic lexicons</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>graph-constrained vector adjustment (retrofitting)</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Functionally, retrofitting finds new vectors q_i that minimize a loss balancing fidelity to original distributional vectors and agreement with graph neighbors (sum over ||q_i - original||^2 + sum over edges ||q_i - q_j||^2 weighted by alphas/betas), thus imposing relational coherence on a distributed representation without re-training on raw corpora; expanded retrofitting sets alpha=0 for graph-only terms to learn vectors for OOV terms.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>hybrid (distributed constrained by symbolic relations)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Applied to intrinsic word relatedness tasks and to downstream analogy and story tasks; used to produce ConceptNet Numberbatch by retrofitting word2vec/GloVe with ConceptNet edges.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Retrofitting (and expanded retrofitting) effectively integrates relational knowledge into distributional embeddings, contributing to the observed state-of-the-art performance of ConceptNet Numberbatch across word-relatedness benchmarks and analogies; expanded retrofitting enables multilingual propagation by incorporating non-English nodes from ConceptNet.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Compared to re-training embeddings with graph-aware objectives or to purely symbolic reasoning, retrofitting is a flexible post-hoc method that preserves original distributional structure while adding relational consistency; the hybrid produced by retrofitting outperforms both distributional-only and ConceptNet-PPMI alone in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Retrofitting can cause hub effects where vectors move toward highly connected nodes ('person'), necessitating mean subtraction and renormalization to preserve discriminability; it does not by itself incorporate relation labels' semantics (only that nodes should be close), so relation-specific reasoning (e.g., direction-dependent inference) is not directly encoded.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Retrofitting operationalizes the claim that symbolic relational constraints can be applied as soft priors on distributed representations, enhancing semantic alignment with human judgments and enabling vectors for terms unseen in original training corpora via graph propagation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ConceptNet 5.5: An Open Multilingual Graph of General Knowledge', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8919.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8919.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ConceptNet Numberbatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ConceptNet Numberbatch 16.09 (hybrid semantic space)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid semantic space combining pre-trained distributional embeddings (word2vec, GloVe), ConceptNet-PPMI, expanded retrofitting, alignment and merging via SVD and post-processing (mean subtraction and renormalization) to produce multilingual, relationally-informed word vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>hybrid distributed representation (retrofitted and merged embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Functionally, it is a single dense vector space where each term's vector is influenced both by corpus-derived distributional statistics and by explicit graph relations from ConceptNet; the process includes retrofitting, cross-model linear alignment (concatenation + SVD), expansion to graph vocab, and normalization.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>hybrid (distributed + symbolic constraints)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Evaluated on word relatedness (MEN-3000, RW, WordSim-353, MTurk-771), SAT-style proportional analogies (Turney dataset), and Story Cloze Test (narrative ending selection).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ConceptNet Numberbatch achieves state-of-the-art or near state-of-the-art results on multiple intrinsic relatedness benchmarks (MEN-3000 ρ = .866; WordSim-353 ρ = .828; MTurk-771 ρ = .810; Rare Words ρ = .601) and ties/outscores prior bests on SAT analogies (accuracy = .561) while producing ~59.4% on Story Cloze using a bag-of-vectors baseline; these results show that combining distributional and relational information yields improvements beyond either source alone.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Outperforms distributional-only embeddings (word2vec, GloVe) and ConceptNet-only PPMI embeddings in the evaluations presented, showing a clear advantage for the hybrid approach; performance is comparable to best non-embedding analogy systems (LRA, SuperSim) on SAT analogies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>On narrative/story understanding (Story Cloze) the bag-of-vectors approach built on Numberbatch attains only modest gains (~59%), indicating that richer event-structure or sequence-aware representations are needed for deeper story understanding; analogy solving still relies on vector arithmetic and does not utilize ConceptNet's relation labels, suggesting room for improvement by exploiting labeled relations.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Demonstrates the practical benefit of integrating symbolic commonsense knowledge with continuous representations; implies that semantic representations used in downstream tasks should combine distributional statistics with relational constraints to better match human judgments and support multilingual transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ConceptNet 5.5: An Open Multilingual Graph of General Knowledge', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8919.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8919.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Holographic embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Holographic embeddings of knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A knowledge-graph embedding technique where entity embeddings are combined with circular correlation to represent relations, designed to model labeled relations in a distributed vector space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Holographic embeddings of knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>knowledge-graph embeddings (holographic)</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Functionally, entities are represented as dense vectors and relations are modeled by operations (circular correlation) on these vectors such that scoring functions can predict labeled triples; intended to capture relational structure in a distributed format suitable for learning.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>distributed representation for relational data (graph embedding)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Mentioned as a potentially relevant method for learning from ConceptNet; would be applied to relational prediction and knowledge completion tasks rather than human cognitive benchmarks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Authors attempted to implement holographic embeddings on ConceptNet but report convergence was too slow to experiment further; they note it seems relevant but not used in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Contrasted implicitly with the authors' PPMI/SVD and retrofitting pipeline; holographic embeddings learn relation-specific distributed encodings rather than deriving vectors from count-PPMI matrices or post-hoc retrofitting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Practical training speed and convergence issues prevented experimentation here; no empirical cognitive-task evidence presented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Represents an approach that could encode relation labels functionally into distributed vectors (potentially improving analogy reasoning and relation-specific inference), but requires further computational optimization to be applied at ConceptNet scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ConceptNet 5.5: An Open Multilingual Graph of General Knowledge', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8919.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8919.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Symbolic ontology (Cyc)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cyc (predicate-logic ontology of common-sense knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A long-standing project constructing a hand-built ontology and knowledge base expressed in predicate logic to represent commonsense knowledge and support logical inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Building large knowledge-based systems: representation and inference in the Cyc project</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>symbolic predicate-logic ontology</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Conceptual knowledge is encoded as logical predicates, formal axioms, and hierarchies where entities and relations are represented explicitly in logic, enabling symbolic inference and rule-based reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>symbolic / logical</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Mentioned as a contrasting approach to ConceptNet in related work; historically used for knowledge representation and symbolic reasoning tasks rather than the distributional benchmarks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Not evaluated experimentally in this paper; described as an alternative design choice (deep logical representation) compared to ConceptNet's lightweight graph of lexical relations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Cyc's deep predicate-logic representation contrasts with ConceptNet's lighter-weight semantic network oriented to words/phrases and compatibility with distributional vectors; ConceptNet targets ease of use with NLP and embeddings, whereas Cyc focuses on rigorous logical representation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Cyc's resource-intensive construction and formal complexity make it less directly compatible with large-scale distributional embedding workflows used here; no empirical comparison in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Illustrates a theoretical spectrum from formal symbolic ontologies to distributional/hybrid representations; suggests the practical usefulness of lighter-weight graphs for integration with vector-space methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ConceptNet 5.5: An Open Multilingual Graph of General Knowledge', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8919.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8919.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WordNet (lexical relations)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WordNet (lexical database of semantic relations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A manually curated lexical resource organizing words into synsets linked by relations like hyponymy/hypernymy and meronymy, widely used for lexical semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>WordNet</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>lexical relational network (sense-disambiguated synsets)</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Represents conceptual knowledge via synsets (sense-disambiguated lexical units) connected by typed relations (IsA/hypernym, part/whole, synonyms), providing a sense-level symbolic structure for lexical semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>symbolic / lexical-network</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Referenced as a source and contrast for ConceptNet's representation; relevant to sense-level semantic tasks and lexical relations but not directly used in embeddings here except as one of ConceptNet's linked data imports.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ConceptNet imports links to WordNet and can align its underspecified lexical nodes to WordNet senses via ExternalURL and SenseOf edges; WordNet provides more disambiguated structure than ConceptNet's default ambiguous nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>WordNet is more fine-grained and sense-specific compared to ConceptNet's typical use of ambiguous lexical forms; ConceptNet trades off sense granularity for broader multilingual and commonsense coverage and ease of use with distributional methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>WordNet's focus on curated lexical senses and closed vocabulary can limit coverage and multilingual reach relative to ConceptNet's aggregated multilingual sources.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Illustrates a representational choice about node granularity (sense-disambiguated vs ambiguous lexical nodes) that affects how conceptual knowledge is linked to distributional evidence and downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ConceptNet 5.5: An Open Multilingual Graph of General Knowledge', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8919.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8919.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Lexical ambiguity / node granularity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ambiguous-term nodes vs sense-disambiguated nodes (node granularity strategy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A representational choice in ConceptNet to use underspecified lexical nodes (ambiguous forms like '/c/en/lead') by default, while allowing optional sense-disambiguated nodes (e.g., '/c/en/lead/n') to be included and linked via SenseOf.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>lexical-granularity representation (ambiguous vs disambiguated nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Functionally, nodes represent common words/phrases in an underspecified form to align with distributional data extracted from ambiguous natural language; finer-grained nodes (part-of-speech or sense-tagged URIs) can be included and connected, enabling partial disambiguation while preserving ease of extraction and linking.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>symbolic representation choice affecting hybrid behavior</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Affects performance on word relatedness benchmarks and the ease of merging with distributional vectors that are typically learned over ambiguous word forms; relevant to sense-sensitive tasks if sense nodes are used.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using ambiguous lexical nodes aligns ConceptNet with distributional learning from raw text, facilitating integration with word embeddings; ConceptNet also provides optional sense nodes linked by SenseOf for limited disambiguation when sources supply POS/sense info.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Ambiguous-node strategy contrasts with sense-disambiguated resources like WordNet; the paper argues ambiguous nodes are advantageous for compatibility with distributional semantics, while sense nodes remain available for tasks requiring disambiguation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Ambiguity can obscure sense-specific relations and harm tasks that need precise sense distinctions; the paper notes finer disambiguation is possible but not currently exploited broadly.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Choosing ambiguous lexical nodes functionally supports integration with distributional methods and broad multilingual coverage at the cost of fine-grained sense representation; a mixed strategy (allowing sense nodes linked to ambiguous nodes) offers pragmatic balance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ConceptNet 5.5: An Open Multilingual Graph of General Knowledge', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8919.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8919.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Relation schema / closed relation set</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ConceptNet core relation set (closed class of 36 relations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ConceptNet represents edges using a closed set of generalized relation types (e.g., IsA, PartOf, UsedFor, SimilarTo) intended to be language- and source-independent, providing a structured schema for commonsense relations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>typed relational schema (qualia-like relations)</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Functionally, conceptual knowledge is structured around a small, fixed vocabulary of relation types that capture common semantic roles between lexical terms; this schema acts like a shallow qualia structure enabling consistent cross-lingual and cross-source assertions.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>symbolic / schema-driven (relation-typed)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>These relations are the basis for graph edges used in retrofitting and ConceptNet-derived embeddings and underpin the relational signal evaluated in word relatedness and analogy tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a core set of relations allows ConceptNet to generalize across languages and sources; relations such as UsedFor and HasPrerequisite tend to connect common words while general relations like RelatedTo connect rarer words. Relation directionality is present but sometimes functionally unimportant (some relations treated symmetric).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>The core relation set is functionally similar to WordNet relations and to the qualia roles of Generative Lexicon theory; it is designed to be coarser and more language-agnostic than highly formal predicate logic ontologies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>A closed relation set necessarily abstracts over fine-grained distinctions that might be needed for precise inferencing; relation labels are not deeply exploited in the presented embedding-based analogy solver, leaving potential relational signal unused.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>A compact, interpretable set of relation types can provide effective constraints to improve distributed semantic representations and enable multilingual propagation while remaining practical for large-scale crowd-sourced knowledge collection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ConceptNet 5.5: An Open Multilingual Graph of General Knowledge', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrofitting word vectors to semantic lexicons <em>(Rating: 2)</em></li>
                <li>Efficient estimation of word representations in vector space <em>(Rating: 2)</em></li>
                <li>GloVe: Global vectors for word representation <em>(Rating: 2)</em></li>
                <li>Improving distributional similarity with lessons learned from word embeddings <em>(Rating: 2)</em></li>
                <li>Holographic embeddings of knowledge graphs <em>(Rating: 2)</em></li>
                <li>An ensemble method to produce high-quality word embeddings <em>(Rating: 2)</em></li>
                <li>Building large knowledge-based systems: representation and inference in the Cyc project <em>(Rating: 1)</em></li>
                <li>WordNet <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8919",
    "paper_id": "paper-26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810",
    "extraction_schema_id": "extraction-schema-156",
    "extracted_data": [
        {
            "name_short": "ConceptNet (knowledge graph)",
            "name_full": "ConceptNet 5.5 (knowledge graph / semantic network)",
            "brief_description": "A large, multilingual semantic network that represents common-sense knowledge as nodes for words/phrases and labeled, weighted directed edges for a closed set of semantic relations; used here as both a symbolic relational resource and as a basis for embedding-based semantic spaces.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representational_format_name": "knowledge graph / semantic network",
            "representational_format_description": "Conceptual knowledge is stored as a graph whose nodes are words or phrases (often ambiguous lexical forms) and whose edges are labeled relations (IsA, PartOf, UsedFor, SimilarTo, etc.); each assertion is a triple (start node, relation, end node) with weights and provenance metadata, enabling traversal, linking to external resources, and serving as constraints that relate terms across languages and sources.",
            "format_type": "symbolic / relational (graph-based)",
            "cognitive_task_or_phenomenon": "Intrinsic word relatedness evaluations (MEN-3000, WordSim-353, RW, MTurk-771), downstream tasks of SAT-style proportional analogies, Story Cloze Test (choosing sensible story endings), multilingual propagation of meaning.",
            "key_findings": "Using ConceptNet as a source of relational knowledge improves measurable semantic performance when incorporated with distributional methods; a ConceptNet-only embedding (ConceptNet-PPMI) can be derived and the graph's edges enable multilingual propagation. ConceptNet-based hybrid embeddings (ConceptNet Numberbatch) achieve state-of-the-art correlations on multiple word-relatedness datasets and strong performance on SAT analogies (Numberbatch accuracy = 56.1% overall, 58.8% on held-out half) and high Spearman correlations on MEN (.866 final), WordSim-353 (.828), MTurk-771 (.810), Rare Words (.601).",
            "comparison_with_other_formats": "Compared directly to distributional-only embeddings (word2vec, GloVe, LexVec) and to purely distributional PPMI/SVD methods, ConceptNet as a relational resource combined with distributional vectors (via retrofitting/merging) yields superior performance; ConceptNet alone (as ConceptNet-PPMI) is useful but the hybrid outperforms either source alone, indicating complementary strengths.",
            "limitations_or_counter_evidence": "The graph by itself does not solve tasks that require fine event-structure understanding (bag-of-vectors on Story Cloze yields only ~59% accuracy when using any high-quality embeddings), and ConceptNet's use of ambiguous lexical nodes may limit fine-grained sense distinctions. Practical limitations include the need to prune low-degree nodes when building matrices and that directionality of some relations is functionally unimportant (some relations marked symmetric).",
            "theoretical_claims_or_implications": "Relational, symbolic common-sense knowledge represented in a graph can and should be combined with continuous distributional representations to produce more human-aligned semantic spaces; the graph provides multilingual links and explicit semantic constraints that improve generalization and downstream reasoning-like tasks when fused with distributional data.",
            "uuid": "e8919.0",
            "source_info": {
                "paper_title": "ConceptNet 5.5: An Open Multilingual Graph of General Knowledge",
                "publication_date_yy_mm": "2016-12"
            }
        },
        {
            "name_short": "Distributional word embeddings",
            "name_full": "Distributional word embeddings (word2vec, GloVe, LexVec and similar)",
            "brief_description": "Represent words as dense real-valued vectors learned from co-occurrence statistics in large corpora; semantic relatedness corresponds to vector similarity in continuous space.",
            "citation_title": "",
            "mention_or_use": "use",
            "representational_format_name": "distributed representation / vector space model",
            "representational_format_description": "Conceptual knowledge is represented functionally as points in a continuous high-dimensional vector space learned from distributional patterns (prediction or count-based methods); proximity and vector arithmetic capture similarity, relatedness, and some relational structure.",
            "format_type": "distributed / continuous",
            "cognitive_task_or_phenomenon": "Word relatedness benchmarking (MEN-3000, WordSim-353, RW, MTurk-771); proportional analogy solving via vector arithmetic; Story Cloze Test via bag-of-vectors sentence representations.",
            "key_findings": "Pretrained distributional embeddings (word2vec, GloVe, LexVec) provide strong baselines for relatedness and analogy tasks, but in the experiments reported here they underperform the hybrid ConceptNet Numberbatch. Example: word2vec achieved ~.486 accuracy on SAT analogies (in Table 2) while Numberbatch reached .561, indicating gains from adding relational knowledge.",
            "comparison_with_other_formats": "Distributional embeddings capture continuous similarity and can be improved by incorporating symbolic relational knowledge (retrofitting). The hybrid (Numberbatch) outperforms distributional-only methods on multiple evaluations, showing that distributional information plus relational constraints is superior to distributional alone for these tasks.",
            "limitations_or_counter_evidence": "Pure distributional vectors lack explicit relational labels and struggle with some relational analogy tasks unless tuned (also they may fail to capture multilingual links without additional resources). They also exhibit shortcomings on rare-word benchmarks unless trained on massive corpora.",
            "theoretical_claims_or_implications": "Distributed vectors are functionally effective for measuring relatedness and supporting simple compositional operations (averaging, vector arithmetic), but they benefit from explicit relational knowledge to better align with human semantic judgments and to generalize across languages.",
            "uuid": "e8919.1",
            "source_info": {
                "paper_title": "ConceptNet 5.5: An Open Multilingual Graph of General Knowledge",
                "publication_date_yy_mm": "2016-12"
            }
        },
        {
            "name_short": "ConceptNet-PPMI",
            "name_full": "ConceptNet-PPMI (count-based embeddings from ConceptNet)",
            "brief_description": "Embeddings derived directly from the ConceptNet graph by constructing a term-term cooccurrence matrix from graph edges, computing PPMI with context smoothing, and reducing dimensionality with truncated SVD to produce dense vectors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representational_format_name": "count-based PPMI matrix → dense vectors",
            "representational_format_description": "Treat the ConceptNet graph as a sparse symmetric term-term cooccurrence matrix where each cell aggregates edge weights between terms; compute pointwise mutual information (with smoothing), clip negatives to obtain PPMI, and apply truncated SVD to yield 300-dimensional dense vectors that functionally encode graph connectivity as vector similarity.",
            "format_type": "distributed (count-based) derived from symbolic graph — hybrid methodology",
            "cognitive_task_or_phenomenon": "Used as an intrinsic semantic space for word relatedness and as a component in downstream tasks (analogy), compared to other embedding sources.",
            "key_findings": "ConceptNet-PPMI provides embeddings that implicitly represent ConceptNet's graph structure and allow measuring connectedness between nodes; when used alone it is less effective than the hybrid Numberbatch but provides useful relational signal that, when combined with distributional embeddings, improves performance.",
            "comparison_with_other_formats": "Compared to pure distributional embeddings, ConceptNet-PPMI is derived from explicit relational data rather than text co-occurrence and thus encodes commonsense relations; in the experiments the hybrid that includes both ConceptNet-PPMI and distributional sources outperforms ConceptNet-PPMI alone, indicating complementarity.",
            "limitations_or_counter_evidence": "Requires pruning of low-degree nodes for computational tractability (nodes with fewer than three edges removed during matrix construction), and restored vectors for pruned nodes are approximated by neighbors' averages; as a graph-derived count method it may miss distributional nuances captured by large text corpora.",
            "theoretical_claims_or_implications": "A count-based transformation of symbolic knowledge into distributed vectors provides a principled bridge between relational knowledge and vector-space semantics, enabling graph connectivity to functionally operate as contextual cooccurrence for distributional methods.",
            "uuid": "e8919.2",
            "source_info": {
                "paper_title": "ConceptNet 5.5: An Open Multilingual Graph of General Knowledge",
                "publication_date_yy_mm": "2016-12"
            }
        },
        {
            "name_short": "Retrofitting / Expanded retrofitting",
            "name_full": "Retrofitting word vectors to semantic lexicons (and expanded retrofitting)",
            "brief_description": "A post-processing optimization that adjusts pre-trained word vectors to be closer to their neighbors in a knowledge graph while remaining close to their original values; expanded retrofitting extends this to the larger vocabulary of the graph including OOV terms.",
            "citation_title": "Retrofitting word vectors to semantic lexicons",
            "mention_or_use": "use",
            "representational_format_name": "graph-constrained vector adjustment (retrofitting)",
            "representational_format_description": "Functionally, retrofitting finds new vectors q_i that minimize a loss balancing fidelity to original distributional vectors and agreement with graph neighbors (sum over ||q_i - original||^2 + sum over edges ||q_i - q_j||^2 weighted by alphas/betas), thus imposing relational coherence on a distributed representation without re-training on raw corpora; expanded retrofitting sets alpha=0 for graph-only terms to learn vectors for OOV terms.",
            "format_type": "hybrid (distributed constrained by symbolic relations)",
            "cognitive_task_or_phenomenon": "Applied to intrinsic word relatedness tasks and to downstream analogy and story tasks; used to produce ConceptNet Numberbatch by retrofitting word2vec/GloVe with ConceptNet edges.",
            "key_findings": "Retrofitting (and expanded retrofitting) effectively integrates relational knowledge into distributional embeddings, contributing to the observed state-of-the-art performance of ConceptNet Numberbatch across word-relatedness benchmarks and analogies; expanded retrofitting enables multilingual propagation by incorporating non-English nodes from ConceptNet.",
            "comparison_with_other_formats": "Compared to re-training embeddings with graph-aware objectives or to purely symbolic reasoning, retrofitting is a flexible post-hoc method that preserves original distributional structure while adding relational consistency; the hybrid produced by retrofitting outperforms both distributional-only and ConceptNet-PPMI alone in experiments.",
            "limitations_or_counter_evidence": "Retrofitting can cause hub effects where vectors move toward highly connected nodes ('person'), necessitating mean subtraction and renormalization to preserve discriminability; it does not by itself incorporate relation labels' semantics (only that nodes should be close), so relation-specific reasoning (e.g., direction-dependent inference) is not directly encoded.",
            "theoretical_claims_or_implications": "Retrofitting operationalizes the claim that symbolic relational constraints can be applied as soft priors on distributed representations, enhancing semantic alignment with human judgments and enabling vectors for terms unseen in original training corpora via graph propagation.",
            "uuid": "e8919.3",
            "source_info": {
                "paper_title": "ConceptNet 5.5: An Open Multilingual Graph of General Knowledge",
                "publication_date_yy_mm": "2016-12"
            }
        },
        {
            "name_short": "ConceptNet Numberbatch",
            "name_full": "ConceptNet Numberbatch 16.09 (hybrid semantic space)",
            "brief_description": "A hybrid semantic space combining pre-trained distributional embeddings (word2vec, GloVe), ConceptNet-PPMI, expanded retrofitting, alignment and merging via SVD and post-processing (mean subtraction and renormalization) to produce multilingual, relationally-informed word vectors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representational_format_name": "hybrid distributed representation (retrofitted and merged embeddings)",
            "representational_format_description": "Functionally, it is a single dense vector space where each term's vector is influenced both by corpus-derived distributional statistics and by explicit graph relations from ConceptNet; the process includes retrofitting, cross-model linear alignment (concatenation + SVD), expansion to graph vocab, and normalization.",
            "format_type": "hybrid (distributed + symbolic constraints)",
            "cognitive_task_or_phenomenon": "Evaluated on word relatedness (MEN-3000, RW, WordSim-353, MTurk-771), SAT-style proportional analogies (Turney dataset), and Story Cloze Test (narrative ending selection).",
            "key_findings": "ConceptNet Numberbatch achieves state-of-the-art or near state-of-the-art results on multiple intrinsic relatedness benchmarks (MEN-3000 ρ = .866; WordSim-353 ρ = .828; MTurk-771 ρ = .810; Rare Words ρ = .601) and ties/outscores prior bests on SAT analogies (accuracy = .561) while producing ~59.4% on Story Cloze using a bag-of-vectors baseline; these results show that combining distributional and relational information yields improvements beyond either source alone.",
            "comparison_with_other_formats": "Outperforms distributional-only embeddings (word2vec, GloVe) and ConceptNet-only PPMI embeddings in the evaluations presented, showing a clear advantage for the hybrid approach; performance is comparable to best non-embedding analogy systems (LRA, SuperSim) on SAT analogies.",
            "limitations_or_counter_evidence": "On narrative/story understanding (Story Cloze) the bag-of-vectors approach built on Numberbatch attains only modest gains (~59%), indicating that richer event-structure or sequence-aware representations are needed for deeper story understanding; analogy solving still relies on vector arithmetic and does not utilize ConceptNet's relation labels, suggesting room for improvement by exploiting labeled relations.",
            "theoretical_claims_or_implications": "Demonstrates the practical benefit of integrating symbolic commonsense knowledge with continuous representations; implies that semantic representations used in downstream tasks should combine distributional statistics with relational constraints to better match human judgments and support multilingual transfer.",
            "uuid": "e8919.4",
            "source_info": {
                "paper_title": "ConceptNet 5.5: An Open Multilingual Graph of General Knowledge",
                "publication_date_yy_mm": "2016-12"
            }
        },
        {
            "name_short": "Holographic embeddings",
            "name_full": "Holographic embeddings of knowledge graphs",
            "brief_description": "A knowledge-graph embedding technique where entity embeddings are combined with circular correlation to represent relations, designed to model labeled relations in a distributed vector space.",
            "citation_title": "Holographic embeddings of knowledge graphs",
            "mention_or_use": "mention",
            "representational_format_name": "knowledge-graph embeddings (holographic)",
            "representational_format_description": "Functionally, entities are represented as dense vectors and relations are modeled by operations (circular correlation) on these vectors such that scoring functions can predict labeled triples; intended to capture relational structure in a distributed format suitable for learning.",
            "format_type": "distributed representation for relational data (graph embedding)",
            "cognitive_task_or_phenomenon": "Mentioned as a potentially relevant method for learning from ConceptNet; would be applied to relational prediction and knowledge completion tasks rather than human cognitive benchmarks in this paper.",
            "key_findings": "Authors attempted to implement holographic embeddings on ConceptNet but report convergence was too slow to experiment further; they note it seems relevant but not used in their experiments.",
            "comparison_with_other_formats": "Contrasted implicitly with the authors' PPMI/SVD and retrofitting pipeline; holographic embeddings learn relation-specific distributed encodings rather than deriving vectors from count-PPMI matrices or post-hoc retrofitting.",
            "limitations_or_counter_evidence": "Practical training speed and convergence issues prevented experimentation here; no empirical cognitive-task evidence presented in this paper.",
            "theoretical_claims_or_implications": "Represents an approach that could encode relation labels functionally into distributed vectors (potentially improving analogy reasoning and relation-specific inference), but requires further computational optimization to be applied at ConceptNet scale.",
            "uuid": "e8919.5",
            "source_info": {
                "paper_title": "ConceptNet 5.5: An Open Multilingual Graph of General Knowledge",
                "publication_date_yy_mm": "2016-12"
            }
        },
        {
            "name_short": "Symbolic ontology (Cyc)",
            "name_full": "Cyc (predicate-logic ontology of common-sense knowledge)",
            "brief_description": "A long-standing project constructing a hand-built ontology and knowledge base expressed in predicate logic to represent commonsense knowledge and support logical inference.",
            "citation_title": "Building large knowledge-based systems: representation and inference in the Cyc project",
            "mention_or_use": "mention",
            "representational_format_name": "symbolic predicate-logic ontology",
            "representational_format_description": "Conceptual knowledge is encoded as logical predicates, formal axioms, and hierarchies where entities and relations are represented explicitly in logic, enabling symbolic inference and rule-based reasoning.",
            "format_type": "symbolic / logical",
            "cognitive_task_or_phenomenon": "Mentioned as a contrasting approach to ConceptNet in related work; historically used for knowledge representation and symbolic reasoning tasks rather than the distributional benchmarks in this paper.",
            "key_findings": "Not evaluated experimentally in this paper; described as an alternative design choice (deep logical representation) compared to ConceptNet's lightweight graph of lexical relations.",
            "comparison_with_other_formats": "Cyc's deep predicate-logic representation contrasts with ConceptNet's lighter-weight semantic network oriented to words/phrases and compatibility with distributional vectors; ConceptNet targets ease of use with NLP and embeddings, whereas Cyc focuses on rigorous logical representation.",
            "limitations_or_counter_evidence": "Cyc's resource-intensive construction and formal complexity make it less directly compatible with large-scale distributional embedding workflows used here; no empirical comparison in the paper.",
            "theoretical_claims_or_implications": "Illustrates a theoretical spectrum from formal symbolic ontologies to distributional/hybrid representations; suggests the practical usefulness of lighter-weight graphs for integration with vector-space methods.",
            "uuid": "e8919.6",
            "source_info": {
                "paper_title": "ConceptNet 5.5: An Open Multilingual Graph of General Knowledge",
                "publication_date_yy_mm": "2016-12"
            }
        },
        {
            "name_short": "WordNet (lexical relations)",
            "name_full": "WordNet (lexical database of semantic relations)",
            "brief_description": "A manually curated lexical resource organizing words into synsets linked by relations like hyponymy/hypernymy and meronymy, widely used for lexical semantics.",
            "citation_title": "WordNet",
            "mention_or_use": "mention",
            "representational_format_name": "lexical relational network (sense-disambiguated synsets)",
            "representational_format_description": "Represents conceptual knowledge via synsets (sense-disambiguated lexical units) connected by typed relations (IsA/hypernym, part/whole, synonyms), providing a sense-level symbolic structure for lexical semantics.",
            "format_type": "symbolic / lexical-network",
            "cognitive_task_or_phenomenon": "Referenced as a source and contrast for ConceptNet's representation; relevant to sense-level semantic tasks and lexical relations but not directly used in embeddings here except as one of ConceptNet's linked data imports.",
            "key_findings": "ConceptNet imports links to WordNet and can align its underspecified lexical nodes to WordNet senses via ExternalURL and SenseOf edges; WordNet provides more disambiguated structure than ConceptNet's default ambiguous nodes.",
            "comparison_with_other_formats": "WordNet is more fine-grained and sense-specific compared to ConceptNet's typical use of ambiguous lexical forms; ConceptNet trades off sense granularity for broader multilingual and commonsense coverage and ease of use with distributional methods.",
            "limitations_or_counter_evidence": "WordNet's focus on curated lexical senses and closed vocabulary can limit coverage and multilingual reach relative to ConceptNet's aggregated multilingual sources.",
            "theoretical_claims_or_implications": "Illustrates a representational choice about node granularity (sense-disambiguated vs ambiguous lexical nodes) that affects how conceptual knowledge is linked to distributional evidence and downstream tasks.",
            "uuid": "e8919.7",
            "source_info": {
                "paper_title": "ConceptNet 5.5: An Open Multilingual Graph of General Knowledge",
                "publication_date_yy_mm": "2016-12"
            }
        },
        {
            "name_short": "Lexical ambiguity / node granularity",
            "name_full": "Ambiguous-term nodes vs sense-disambiguated nodes (node granularity strategy)",
            "brief_description": "A representational choice in ConceptNet to use underspecified lexical nodes (ambiguous forms like '/c/en/lead') by default, while allowing optional sense-disambiguated nodes (e.g., '/c/en/lead/n') to be included and linked via SenseOf.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representational_format_name": "lexical-granularity representation (ambiguous vs disambiguated nodes)",
            "representational_format_description": "Functionally, nodes represent common words/phrases in an underspecified form to align with distributional data extracted from ambiguous natural language; finer-grained nodes (part-of-speech or sense-tagged URIs) can be included and connected, enabling partial disambiguation while preserving ease of extraction and linking.",
            "format_type": "symbolic representation choice affecting hybrid behavior",
            "cognitive_task_or_phenomenon": "Affects performance on word relatedness benchmarks and the ease of merging with distributional vectors that are typically learned over ambiguous word forms; relevant to sense-sensitive tasks if sense nodes are used.",
            "key_findings": "Using ambiguous lexical nodes aligns ConceptNet with distributional learning from raw text, facilitating integration with word embeddings; ConceptNet also provides optional sense nodes linked by SenseOf for limited disambiguation when sources supply POS/sense info.",
            "comparison_with_other_formats": "Ambiguous-node strategy contrasts with sense-disambiguated resources like WordNet; the paper argues ambiguous nodes are advantageous for compatibility with distributional semantics, while sense nodes remain available for tasks requiring disambiguation.",
            "limitations_or_counter_evidence": "Ambiguity can obscure sense-specific relations and harm tasks that need precise sense distinctions; the paper notes finer disambiguation is possible but not currently exploited broadly.",
            "theoretical_claims_or_implications": "Choosing ambiguous lexical nodes functionally supports integration with distributional methods and broad multilingual coverage at the cost of fine-grained sense representation; a mixed strategy (allowing sense nodes linked to ambiguous nodes) offers pragmatic balance.",
            "uuid": "e8919.8",
            "source_info": {
                "paper_title": "ConceptNet 5.5: An Open Multilingual Graph of General Knowledge",
                "publication_date_yy_mm": "2016-12"
            }
        },
        {
            "name_short": "Relation schema / closed relation set",
            "name_full": "ConceptNet core relation set (closed class of 36 relations)",
            "brief_description": "ConceptNet represents edges using a closed set of generalized relation types (e.g., IsA, PartOf, UsedFor, SimilarTo) intended to be language- and source-independent, providing a structured schema for commonsense relations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representational_format_name": "typed relational schema (qualia-like relations)",
            "representational_format_description": "Functionally, conceptual knowledge is structured around a small, fixed vocabulary of relation types that capture common semantic roles between lexical terms; this schema acts like a shallow qualia structure enabling consistent cross-lingual and cross-source assertions.",
            "format_type": "symbolic / schema-driven (relation-typed)",
            "cognitive_task_or_phenomenon": "These relations are the basis for graph edges used in retrofitting and ConceptNet-derived embeddings and underpin the relational signal evaluated in word relatedness and analogy tasks.",
            "key_findings": "Using a core set of relations allows ConceptNet to generalize across languages and sources; relations such as UsedFor and HasPrerequisite tend to connect common words while general relations like RelatedTo connect rarer words. Relation directionality is present but sometimes functionally unimportant (some relations treated symmetric).",
            "comparison_with_other_formats": "The core relation set is functionally similar to WordNet relations and to the qualia roles of Generative Lexicon theory; it is designed to be coarser and more language-agnostic than highly formal predicate logic ontologies.",
            "limitations_or_counter_evidence": "A closed relation set necessarily abstracts over fine-grained distinctions that might be needed for precise inferencing; relation labels are not deeply exploited in the presented embedding-based analogy solver, leaving potential relational signal unused.",
            "theoretical_claims_or_implications": "A compact, interpretable set of relation types can provide effective constraints to improve distributed semantic representations and enable multilingual propagation while remaining practical for large-scale crowd-sourced knowledge collection.",
            "uuid": "e8919.9",
            "source_info": {
                "paper_title": "ConceptNet 5.5: An Open Multilingual Graph of General Knowledge",
                "publication_date_yy_mm": "2016-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrofitting word vectors to semantic lexicons",
            "rating": 2
        },
        {
            "paper_title": "Efficient estimation of word representations in vector space",
            "rating": 2
        },
        {
            "paper_title": "GloVe: Global vectors for word representation",
            "rating": 2
        },
        {
            "paper_title": "Improving distributional similarity with lessons learned from word embeddings",
            "rating": 2
        },
        {
            "paper_title": "Holographic embeddings of knowledge graphs",
            "rating": 2
        },
        {
            "paper_title": "An ensemble method to produce high-quality word embeddings",
            "rating": 2
        },
        {
            "paper_title": "Building large knowledge-based systems: representation and inference in the Cyc project",
            "rating": 1
        },
        {
            "paper_title": "WordNet",
            "rating": 2
        }
    ],
    "cost": 0.0178865,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ConceptNet 5.5: An Open Multilingual Graph of General Knowledge</h1>
<p>Robyn Speer<br>Luminoso Technologies, Inc.<br>675 Massachusetts Avenue<br>Cambridge, MA 02139</p>
<p>Joshua Chin
Union College
807 Union St.
Schenectady, NY 12308</p>
<p>Catherine Havasi<br>Luminoso Technologies, Inc.<br>675 Massachusetts Avenue<br>Cambridge, MA 02139</p>
<h4>Abstract</h4>
<p>Machine learning about language can be improved by supplying it with specific knowledge and sources of external information. We present here a new version of the linked open data resource ConceptNet that is particularly well suited to be used with modern NLP techniques such as word embeddings.</p>
<p>ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expertcreated resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use.</p>
<p>When ConceptNet is combined with word embeddings acquired from distributional semantics (such as word2vec), it provides applications with understanding that they would not acquire from distributional semantics alone, nor from narrower resources such as WordNet or DBPedia. We demonstrate this with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improvements on applications of word vectors, including solving SAT-style analogies.</p>
<h2>Introduction</h2>
<p>ConceptNet is a knowledge graph that connects words and phrases of natural language (terms) with labeled, weighted edges (assertions). The original release of ConceptNet (Liu and Singh 2004) was intended as a parsed representation of Open Mind Common Sense (Singh 2002), a crowd-sourced knowledge project. This paper describes the release of ConceptNet 5.5, which has expanded to include lexical and world knowledge from many different sources in many languages.</p>
<p>ConceptNet represents relations between words such as:</p>
<ul>
<li>A net is used for catching fish.</li>
<li>"Leaves" is a form of the word "leaf".</li>
<li>The word cold in English is studený in Czech.</li>
<li>O alimento é usado para comer [Food is used for eating].</li>
</ul>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>In this paper, we will concisely represent assertions such as the above as triples of their start node, relation label, and end node: the assertion that "a dog has a tail" can be represented as (dog, HasA, tail).</p>
<p>ConceptNet also represents links between knowledge resources. In addition to its own knowledge about the English term astronomy, for example, ConceptNet contains links to URLs that define astronomy in WordNet, Wiktionary, OpenCyc, and DBPedia.</p>
<p>The graph-structured knowledge in ConceptNet can be particularly useful to NLP learning algorithms, particularly those based on word embeddings, such as (Mikolov et al. 2013). We can use ConceptNet to build semantic spaces that are more effective than distributional semantics alone.</p>
<p>The most effective semantic space is one that learns from both distributional semantics and ConceptNet, using a generalization of the "retrofitting" method (Faruqui et al. 2015). We call this hybrid semantic space "ConceptNet Numberbatch", to clarify that it is a separate artifact from ConceptNet itself.</p>
<p>ConceptNet Numberbatch performs significantly better than other systems across many evaluations of word relatedness, and this increase in performance translates to improvements on downstream tasks such as analogies. On a corpus of SAT-style analogy questions (Turney 2006), its accuracy of $56.1 \%$ outperforms other systems based on word embeddings and ties the previous best overall system, Turney's LRA. This level of accuracy is only slightly lower than the performance of the average human test-taker.</p>
<p>Building word embeddings is not the only application of ConceptNet, but it is a way to apply ConceptNet that achieves clear benefits and is compatible with ongoing research in distributional semantics.</p>
<p>After introducing related work, we will begin by describing ConceptNet 5.5 and its features, show how to use ConceptNet alone as a semantic space and a measure of word relatedness, and then proceed to describe and evaluate the hybrid system ConceptNet Numberbatch on these various semantic tasks.</p>
<h2>Related Work</h2>
<p>ConceptNet is the knowledge graph version of the Open Mind Common Sense project (Singh 2002), a common sense</p>
<p>knowledge base of the most basic things a person knows. It was last published as version 5.2 (Speer and Havasi 2013).</p>
<p>Many projects strive to create lexical resources of general knowledge. Cyc (Lenat and Guha 1989) has built an ontology of common-sense knowledge in predicate logic form over the decades. DBPedia (Auer et al. 2007) extracts knowledge from Wikipedia infoboxes, providing a large number of facts, largely focused on named entities that have Wikipedia articles. The Google Knowledge Graph (Singhal 2012) is perhaps the largest and most general knowledge graph, though its content is not freely available. It focuses largely on named entities that can be disambiguated, with a motto of "things, not strings".</p>
<p>ConceptNet's role compared to these other resources is to provide a sufficiently large, free knowledge graph that focuses on the common-sense meanings of words (not named entities) as they are used in natural language. This focus on words makes it particularly compatible with the idea of representing word meanings as vectors.</p>
<p>Word embeddings represent words as dense unit vectors of real numbers, where vectors that are close together are semantically related. This representation is appealing because it represents meaning as a continuous space, where similarity and relatedness can be treated as a metric. Word embeddings are often produced as a side-effect of a machine learning task, such as predicting a word in a sentence from its neighbors. This approach to machine learning about semantics is sometimes referred to as distributional semantics or distributed word representations, and it contrasts with the knowledge-driven approach of semantic networks or knowledge graphs.</p>
<p>Two prominent matrices of embeddings are the word2vec embeddings trained on 100 billion words of Google News using skip-grams with negative sampling (Mikolov et al. 2013), and the GloVe 1.2 embeddings trained on 840 billion words of the Common Crawl (Pennington, Socher, and Manning 2014). These matrices are downloadable, and we will be using them both as a point of comparison and as inputs to an ensemble. Levy, Goldberg, and Dagan (2015) evaluated multiple embedding techniques and the effects of various explicit and implicit hyperparameters, produced their own performant word embeddings using a truncated SVD of words and their contexts, and provided recommendations for the engineering of word embeddings.</p>
<p>Holographic embeddings (Nickel, Rosasco, and Poggio 2016) are embeddings learned from a labeled knowledge graph, under the constraint that a circular correlation of these embeddings gives a vector representing a relation. This representation seems extremely relevant to ConceptNet. In our attempt to implement it on ConceptNet so far, it has converged too slowly to experiment with, but this could be overcome eventually with some optimization and additional computing power.</p>
<h2>Structure of ConceptNet</h2>
<h2>Knowledge Sources</h2>
<p>ConceptNet 5.5 is built from the following sources:
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: ConceptNet's browsable interface (conceptnet.io) shows facts about the English word "bicycle".</p>
<ul>
<li>Facts acquired from Open Mind Common Sense (OMCS) (Singh 2002) and sister projects in other languages (Anacleto et al. 2006)</li>
<li>Information extracted from parsing Wiktionary, in multiple languages, with a custom parser ("Wikiparsec")</li>
<li>"Games with a purpose" designed to collect common knowledge (von Ahn, Kedia, and Blum 2006) (Nakahara and Yamada 2011) (Kuo et al. 2009)</li>
<li>Open Multilingual WordNet (Bond and Foster 2013), a linked-data representation of WordNet (Miller et al. 1998) and its parallel projects in multiple languages</li>
<li>JMDict (Breen 2004), a Japanese-multilingual dictionary</li>
<li>OpenCyc, a hierarchy of hypernyms provided by Cyc (Lenat and Guha 1989), a system that represents common sense knowledge in predicate logic</li>
<li>A subset of DBPedia (Auer et al. 2007), a network of facts extracted from Wikipedia infoboxes
With the combination of these sources, ConceptNet contains over 21 million edges and over 8 million nodes. Its English vocabulary contains approximately $1,500,000$ nodes, and there are 83 languages in which it contains at least 10,000 nodes.</li>
</ul>
<p>The largest source of input for ConceptNet is Wiktionary, which provides 18.1 million edges and is mostly responsible for its large multilingual vocabulary. However, much of the character of ConceptNet comes from OMCS and the various games with a purpose, which express many different kinds of relations between terms, such as PartOf ("a wheel is part of a car") and UsedFor ("a car is used for driving").</p>
<h2>Relations</h2>
<p>ConceptNet uses a closed class of selected relations such as IsA, UsedFor, and CapableOf, intended to represent a relationship independently of the language or the source of the terms it connects.</p>
<p>ConceptNet 5.5 aims to align its knowledge resources on its core set of 36 relations. These generalized relations are similar in purpose to WordNet's relations such as hyponym and meronym, as well as to the qualia of the Generative Lexicon theory (Pustejovsky 1991). ConceptNet's edges are directed, but as a new feature in ConceptNet 5.5, some relations are designated as being symmetric, such as SimilarTo. The directionality of these edges is unimportant.</p>
<p>The core relations are:</p>
<ul>
<li>Symmetric relations: Antonym, DistinctFrom, EtymologicallyRelatedTo, LocatedNear, RelatedTo, SimilarTo, and Synonym</li>
<li>Asymmetric relations: AtLocation, CapableOf, Causes, CausesDesire, CreatedBy, DefinedAs, DerivedFrom, Desires, Entails, ExternalURL, FormOf, HasA, HasContext, HasFirstSubevent, HasLastSubevent, HasPrerequisite, HasProperty, InstanceOf, IsA, MadeOf, MannerOf, MotivatedByGoal, ObstructedBy, PartOf, ReceivesAction, SenseOf, SymbolOf, and UsedFor</li>
</ul>
<p>Definitions and examples of these relations appear in a page of the ConceptNet 5.5 documentation ${ }^{1}$.</p>
<p>Relations with specific semantics, such as UsedFor and HasPrerequisite, tend to connect common words and phrases, while rarer words are connected by more general relations such as Synonym and RelatedTo.</p>
<p>An example of edges in ConceptNet, in a browsable interface that groups them by their relation expressed in natural English, appears in Figure 1.</p>
<h2>Term Representation</h2>
<p>ConceptNet represents terms in a standardized form. The text is Unicode-normalized in NFKC form ${ }^{2}$ using Python's unicodedata implementation, lowercased, and split into non-punctuation tokens using the tokenizer in the Python package wordfreq (Speer et al. 2016), which builds on the standard Unicode word segmentation algorithm. The tokens are joined with underscores, and this text is prepended with the URI /c/lang, where lang is the BCP 47 language code ${ }^{3}$ for the language the term is in. As an example, the English term "United States" becomes /c/en/united_states.</p>
<p>Relations have a separate namespace of URIs prefixed with /r, such as /r/PartOf. These relations are given artificial names in English, but apply to all languages. The statement that was obtained in Portuguese as "O alimento é usado para comer" is still represented with the relation /r/UsedFor.</p>
<p>The most significant change from ConceptNet 5.4 and earlier is in the representation of terms. ConceptNet 5.4 required terms in English to be in lemmatized form, so that, for example, "United States" had to be represented as /c/en/unite_state. In this representation, "drive" and "driving" were the same term, allowing the assertions (car, UsedFor, driving) and (drive, HasPrerequisite, have license) to be connected. ConceptNet 5.5 removes the lemmatizer, and instead relates inflections of words using the FormOf relation. The two assertions above are now linked by the third assertion (driving, FormOf, drive), and both "driving" and "drive" can be looked up in ConceptNet.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Vocabulary</h2>
<p>When building a knowledge graph, the decision of what a node should represent has significant effects on how the graph is used. It also has implications that can make linking and importing other resources non-trivial, because different resources make different decisions about their representation.</p>
<p>In ConceptNet, a node is a word or phrase of a natural language, often a common word in its undisambiguated form. The word "lead" in English is a term in ConceptNet, represented by the URI / c/en/lead, even though it has multiple meanings. The advantage of ambiguous terms is that they can be extracted easily from natural language, which is also ambiguous. This ambiguous representation is equivalent to that used by systems that learn distributional semantics from text.</p>
<p>ConceptNet's representation allows for more specific, disambiguated versions of a term. The URI / c/en/lead/n refers to noun senses of the word "lead", and is effectively included within / c/en/lead when searching or traversing ConceptNet, and linked to it with the implicit relation SenseOf. Many data sources provide information about parts of speech, allowing us to use this as a common representation that provides a small amount of disambiguation. Further disambiguation is allowed by the URI structure, but not currently used.</p>
<h2>Linked Data</h2>
<p>ConceptNet imports knowledge from some other systems, such as WordNet, into its own representation. These other systems have their own target vocabularies that need to be aligned with ConceptNet, which is usually an underspecified, many-to-many alignment.</p>
<p>A term that is imported from another knowledge graph will be connected to ConceptNet nodes via the relation ExternalURL, pointing to an absolute URL that represents that term in that external resource. This newly-introduced relation preserves the provenance of the data and enables looking up what the untransformed data was. ConceptNet terms can also be represented as absolute URLs, so this allows ConceptNet to connect bidirectionally to the broader ecosystem of Linked Open Data.</p>
<h2>Applying ConceptNet to Word Embeddings Computing ConceptNet Embeddings Using PPMI</h2>
<p>We can represent the ConceptNet graph as a sparse, symmetric term-term matrix. Each cell contains the sum of the weights of all edges that connect the two corresponding terms. For performance reasons, when building this matrix, we prune the ConceptNet graph by discarding terms connected to fewer than three edges.</p>
<p>We consider this matrix to represent terms and their contexts. In a corpus of text, the context of a term would be the terms that appear nearby in the text; here, the context is the other nodes it is connected to in ConceptNet. We can calculate word embeddings directly from this sparse matrix by following the practical recommendations of Levy, Goldberg, and Dagan (2015).</p>
<p>As in Levy et al., we determine the pointwise mutual information of the matrix entries with context distributional smoothing, clip the negative values to yield positive pointwise mutual information (PPMI), reduce the dimensionality of the result to 300 dimensions with truncated SVD, and combine the terms and contexts symmetrically into a single matrix of word embeddings.</p>
<p>This gives a matrix of word embeddings we call ConceptNet-PPMI. These embeddings implicitly represent the overall graph structure of ConceptNet, and allow us to compute the approximate connectedness of any pair of nodes.</p>
<p>We can expand ConceptNet-PPMI to restore the nodes that we pruned away, assigning them vectors that are the average of their neighboring nodes.</p>
<h2>Combining ConceptNet with Distributional Word Embeddings</h2>
<p>Having created embeddings from ConceptNet alone, we would now like to create a more robust set of embeddings that represents both ConceptNet and distributional word embeddings learned from text.</p>
<p>Retrofitting (Faruqui et al. 2015) is a process that adjusts an existing matrix of word embeddings using a knowledge graph. Retrofitting infers new vectors $q_{i}$ with the objective of being close to their original values, $\hat{q}_{i}$, and also close to their neighbors in the graph with edges $E$, by minimizing this objective function:</p>
<p>$$
\Psi(Q)=\sum_{i=1}^{n}\left[\alpha_{i}\left|q_{i}-\hat{q}<em E="E" _i_="(i," _in="\in" j_="j)">{i}\right|^{2}+\sum</em>\right]
$$} \beta_{i j}\left|q_{i}-q_{j}\right|^{2</p>
<p>Faruqui et al. give a simple iterative process to minimize this function over the vocabulary of the original embeddings.</p>
<p>The process of "expanded retrofitting" (Speer and Chin 2016) can optimize this objective over a larger vocabulary, including terms from the knowledge graph that do not appear in the vocabulary of the word embeddings. This effectively sets $\alpha_{i}=0$ for terms whose original values are undefined. We set $\beta_{i j}$ according to the weights of the edges in ConceptNet.</p>
<p>The particular benefit of expanded retrofitting to ConceptNet is that it can benefit from the multilingual connections in ConceptNet. It learns more about English words via their translations in other languages, and also gives these foreignlanguage terms useful embeddings in the same space as the English terms. The effect is similar to the work of Xiao and Guo (2014), who also propagate multilingual embeddings using crowd-sourced Wiktionary entries.</p>
<p>We add one more step to retrofitting, which is to subtract the mean of the vectors that result from retrofitting, then renormalize them to unit vectors. Retrofitting has a tendency to move all vectors closer to the vectors for highly-connected terms such as "person". Subtracting the mean helps to ensure that terms remain distinguishable from each other.</p>
<h2>Combining Multiple Sources of Embeddings</h2>
<p>Retrofitting can be applied to any existing matrix of word embeddings, without needing access to the data that was
used to train them. This is particularly useful because it allows building on publicly-released matrices of embeddings whose input data is unavailable or difficult to acquire.</p>
<p>As described in the "Related Work" section, word2vec and GloVe both provide recommended pre-trained matrices. These matrices represent somewhat different domains of text and have complementary strengths, and the way that we can benefit from them the most is by taking both of them as input.</p>
<p>To do this, we apply retrofitting to both matrices, then find a globally linear projection that aligns the results on their common vocabulary. This process was inspired by Zhao, Hassan, and Auli (2015). We find the projection by concatenating the columns of the matrices and reducing them to 300 dimensions using truncated SVD. We then use this alignment to infer compatible embeddings for terms that are missing from one of the vocabularies.</p>
<p>In ongoing work, we are experimenting with additionally including distributional word embeddings from corpora of non-English text in this merger. Preliminary results show that this improves the multilingual performance of the embeddings.</p>
<p>After retrofitting and merging, we have a labeled matrix of word embeddings whose vocabulary is derived from word2vec, GloVe, and the pruned ConceptNet graph. As in ConceptNet-PPMI, we re-introduce all the nodes from ConceptNet by looking up and averaging their neighboring nodes.</p>
<h2>Evaluation</h2>
<p>To compare the performance of fully-built systems of word embeddings, we will first compare their results on intrinsic evaluations of word relatedness, then apply the word embeddings to the downstream tasks of solving proportional analogies and choosing the sensible ending to a story, to evaluate whether better embeddings translate to better performance on semantic tasks.</p>
<p>The hybrid system described above is the system we name ConceptNet Numberbatch, with the version number 16.09 indicating that it was built in September 2016. We now compare results from ConceptNet Numberbatch 16.09 to other systems that make their word embeddings available, both those that were used in building ConceptNet Numberbatch and a recently-released system, LexVec, that was not. The systems we evaluate are:</p>
<ul>
<li>word2vec SGNS (Mikolov et al. 2013), trained on Google News text</li>
<li>GloVe 1.2 (Pennington, Socher, and Manning 2014), trained on the Common Crawl</li>
<li>LexVec (Salle, Idiart, and Villavicencio 2016), trained on the English Wikipedia and NewsCrawl 2014</li>
<li>ConceptNet-PPMI, described here and trained on ConceptNet 5.5 alone</li>
<li>ConceptNet Numberbatch 16.09, the hybrid of ConceptNet 5.5, word2vec, and GloVe described here</li>
</ul>
<h2>Evaluations of Word Relatedness</h2>
<p>One way to evaluate the intrinsic performance of a semantic space is to ask it to rank the relatedness of pairs of words, and compare its judgments to human judgments. ${ }^{4}$ If one word in a pair is out-of-vocabulary, the pair is assumed to have a relatedness of 0 . A good semantic space will provide a ranking of relatedness that is highly correlated with the human gold-standard ranking, as measured by its Spearman correlation $(\rho)$.</p>
<p>Many gold standards of word relatedness are in common use. Here, we focus on MEN-3000 (Bruni, Tran, and Baroni 2014), a large crowd-sourced ranking of common words; RW (Luong, Socher, and Manning 2013), a ranking of rare words; WordSim-353 (Finkelstein et al. 2001), a smaller evaluation that has been used as a benchmark for many methods; and MTurk-771 (Halawi et al. 2012), another crowd-sourced evaluation of a variety of words.</p>
<p>To avoid manually overfitting by designing our semantic space around a particular evaluation, we experimented using smaller development sets, holding out some test data until it was time to include results in this paper:</p>
<ul>
<li>MEN-3000 is already divided into a 2000-item development set and a 1000-item test set. We use the results from the test set as the final results.</li>
<li>RW has no standard dev/test breakdown. We sampled 2/3 of its items as a development set and held out the other $1 / 3$ (every third line of the file, starting with the third).</li>
<li>We used all of WordSim-353 in development. We examine its results both in English and in its Spanish translation (Hassan and Mihalcea 2009).</li>
<li>We did not use MTurk-771 in development, holding out the entire set as a final test, showing that ConceptNet Numberbatch performs well on a previously-unseen evaluation.</li>
</ul>
<p>We use the Spanish WordSim-353 as an example of a prominent non-English evaluation, indicating that expanded retrofitting is sufficient to learn vectors for non-English languages, even when all the distributional semantics takes place in English. However, a thorough multilingual evaluation is beyond the scope of this paper; the systems we compare to have only made English vectors available, and it would add considerable complexity to the evaluation to reproduce other systems of multilingual embeddings, accounting for their various ways of handling morphology and OOV words.</p>
<h2>Solving SAT-style Analogies</h2>
<p>Proportional analogies are statements of the form " $a_{1}$ is to $b_{1}$ as $a_{2}$ is to $b_{2}$ ". The task of filling in missing values of a proportional analogy was common until recently on standardized tests such as the SAT. Now, it is popular as a way to show that a semantic space can approximate relationships</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>between words, even without taking explicit relationships into account.</p>
<p>Much of the groundwork for evaluating systems' ability to solve proportional analogies was laid by Peter Turney, including his method of Latent Relational Analysis (Turney 2006), which was quite effective at solving proportional analogies by repeatedly searching the Web for the words involved in them. A newer method called SuperSim (Turney 2013) does not require Web searching. These methods are evaluated on a dataset of 374 SAT questions that Turney and his collaborators have collected.</p>
<p>Many of the best results on this evaluation have been achieved by Turney in his own work. One interesting system not by Turney is BagPack (Herdağdelen and Baroni 2009), which could learn about analogies either from unstructured text or from ConceptNet 4.</p>
<p>Solving analogies over word embeddings is often described as comparing the difference $b_{2}-a_{2}$ to $b_{1}-a_{1}$ (Mikolov et al. 2013), but for the task of filling in the best pair for $a_{2}$ and $b_{2}$, it helps to take advantage of more of the structure of the question to provide more constraint than this single comparison.</p>
<p>In a sensible analogy, the words on the right side of the analogy will be related in some way to the words on the left side, so we should aim for some amount of relatedness between $a_{1}$ and $a_{2}$, and between $b_{1}$ and $b_{2}$, regardless of what the other terms are. Also, in many cases, a satisfying analogy will still make sense when it is transposed to " $a_{1}$ is to $a_{2}$ as $b_{1}$ is to $b_{2}$ ". The analogy "fire : hot :: ice : cold", for example, can be transposed to "fire : ice :: hot : cold". Recognizing this structure helps in picking the best answer to difficult analogy questions.</p>
<p>This gives us three components that we can weigh to evaluate whether a pair $\left(a_{2}, b_{2}\right)$ completes an analogy: their separate similarity to $a_{1}$ and $b_{1}$, the dot product of differences between the pairs, and the dot product of differences between the transposed pairs. The total weight does not matter, so we can put these together into a vector equation with two free parameters:</p>
<p>$$
\begin{aligned}
&amp; s=a_{1} \cdot a_{2}+b_{1} \cdot b_{2} \
&amp; \quad+w_{1}\left(b_{2}-a_{2}\right) \cdot\left(b_{1}-a_{1}\right)+w_{2}\left(b_{2}-b_{1}\right) \cdot\left(a_{2}-a_{1}\right)
\end{aligned}
$$</p>
<p>The appropriate values of $w_{1}$ and $w_{2}$ depend on the nature of the relationships in the analogy questions, and also on how these relationships appear in the vector space. We optimize these parameters separately for each system we test, using grid search over a number of possible values so that each system can achieve its best performance. The grid search is performed on odd-numbered questions, holding out the even-numbered questions as a test set.</p>
<p>The weights found for ConceptNet Numberbatch 16.09 were $w_{1}=0.2$ and $w_{2}=0.6$. This indicates, surprisingly, that the comparisons being made by the transposed form of the analogy were often more important than the directly stated form of the analogy for choosing the best answer pair.</p>
<h2>An Evaluation of Common-Sense Stories</h2>
<p>The Story Cloze Test (Mostafazadeh et al. 2016) is a recent evaluation of semantic understanding that tests whether a</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Performance of word embeddings across multiple evaluations. Error bars show $95 \%$ confidence intervals.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Evaluation</th>
<th style="text-align: center;">Dev</th>
<th style="text-align: center;">Test</th>
<th style="text-align: center;">Final</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MEN-3000 $(\rho)$</td>
<td style="text-align: center;">.859</td>
<td style="text-align: center;">.866</td>
<td style="text-align: center;">.866</td>
</tr>
<tr>
<td style="text-align: left;">Rare Words $(\rho)$</td>
<td style="text-align: center;">.609</td>
<td style="text-align: center;">.586</td>
<td style="text-align: center;">.601</td>
</tr>
<tr>
<td style="text-align: left;">MTurk-771 $(\rho)$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">.810</td>
<td style="text-align: center;">.810</td>
</tr>
<tr>
<td style="text-align: left;">WordSim-353 $(\rho)$</td>
<td style="text-align: center;">.828</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">.828</td>
</tr>
<tr>
<td style="text-align: left;">WordSim-353 Spanish $(\rho)$</td>
<td style="text-align: center;">.685</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">.685</td>
</tr>
<tr>
<td style="text-align: left;">Story Cloze Test (acc)</td>
<td style="text-align: center;">.604</td>
<td style="text-align: center;">.594</td>
<td style="text-align: center;">.594</td>
</tr>
<tr>
<td style="text-align: left;">SAT Analogies (acc)</td>
<td style="text-align: center;">.535</td>
<td style="text-align: center;">.588</td>
<td style="text-align: center;">.561</td>
</tr>
</tbody>
</table>
<p>Table 1: The Spearman correlation ( $\rho$ ) or accuracy (acc) of ConceptNet Numberbatch 16.09, our hybrid system, on data used in development and data held out for testing.
method can choose the sensible ending to a simple story. Prompts consist of four sentences that tell a story, and two choices are provided for a fifth sentence that concludes the story, only one of which makes sense.</p>
<p>This task is distinguished by being very challenging for computers but very easy for humans, because of the extent that it relies on implicit, common sense knowledge. Most systems that have been evaluated on the Story Cloze Test score only marginally above the random baseline of $50 \%$, while human agreement is near $100 \%$.</p>
<p>Our preliminary attempt to apply ConceptNet Numberbatch to the Story Cloze Test is to use a very simple "bag-ofvectors" model, by averaging the embeddings of the words in the sentence and choosing the ending whose average is closest. This allows us to compare directly to one of the original results presented by Mostafazadeh et al., in which a bag of vectors using GenSim's implementation of word2vec scores $53.9 \%$ on the test set.</p>
<p>This bag-of-vectors model uses no knowledge of how one event might sensibly follow from another, only which words are related in context. Improving the score of this model should not be portrayed as actual "story understanding", but it recognizes that sensible stories do not suddenly change topic.</p>
<h2>Results and Discussion</h2>
<h2>Word Relatedness</h2>
<p>Figure 2 compares the performance of the systems we compared across all evaluations. For word-relatedness evaluations, the Y-axis represents the Spearman correlation ( $\rho$ ), using the Fisher transformation to compute a $95 \%$ confidence interval that assumes the given word pairs are sampled from</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Analogy-solving system</th>
<th style="text-align: left;">Accuracy</th>
<th style="text-align: left;">$\mathbf{9 5 \%}$ conf.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BagPack (2009)</td>
<td style="text-align: left;">.441</td>
<td style="text-align: left;">$.390-.493$</td>
</tr>
<tr>
<td style="text-align: left;">word2vec (2013)</td>
<td style="text-align: left;">.486</td>
<td style="text-align: left;">$.436-.537$</td>
</tr>
<tr>
<td style="text-align: left;">SuperSim (2013)</td>
<td style="text-align: left;">.548</td>
<td style="text-align: left;">$.496-.599$</td>
</tr>
<tr>
<td style="text-align: left;">LRA (2006)</td>
<td style="text-align: left;">.561</td>
<td style="text-align: left;">$.510-.612$</td>
</tr>
<tr>
<td style="text-align: left;">ConceptNet Numberbatch</td>
<td style="text-align: left;">.561</td>
<td style="text-align: left;">$.510-.612$</td>
</tr>
</tbody>
</table>
<p>Table 2: The accuracy of different techniques for solving SAT analogies, including ConceptNet Numberbatch 16.09, our hybrid system.
an unobservable larger set (Bonett and Wright 2000). For the analogy and story evaluations, the Y-axis is simply the proportion of questions answered correctly, with $95 \%$ confidence intervals calculated using the binomial exact test.</p>
<p>The scores of our system on all these evaluations appear in Table 1, including a development/test breakdown that shows no apparent overfitting. The "Final" column is meant for comparisons to other papers and used in the graph. It uses the standard test set that other publications use, if it exists (which is the case for MEN-3000 and Story Cloze), or all of the data otherwise.</p>
<p>On all of the four word-relatedness evaluations, ConceptNet Numberbatch 16.09 (the complete system described in this paper) is state of the art, performing better than all other systems evaluated to an extent that exceeds the confidence interval of the choice of questions. Its high scores on both the Rare Words dataset and the crowd-sourced MEN-3000 and MTurk-771 datasets, exceeding the performance of other embeddings with high confidence, shows both the breadth and the depth of its understanding of words.</p>
<h2>SAT Analogies</h2>
<p>ConceptNet Numberbatch performed the best among the word-embedding systems at SAT analogies, getting $56.1 \%$ of the questions correct ( $58.8 \%$ on the half that was held out for final testing). These analogy results outperform analogies based on other word embeddings, when evaluated in the same framework, as shown by Figure 2.</p>
<p>The analogy results also tie or slightly outperform the performance of best-in-class systems on this evaluation ${ }^{5}$. Table 2 compares our results to the other systems introduced in the "Solving SAT-Style Analogies" section: BagPack (Herdağdelen and Baroni 2009), the previous use of ConceptNet on this evaluation; LRA (Turney 2006), the system whose record has stood for a decade, which spends nine days searching the Web during its evaluation; and SuperSim (Turney 2013), the more recent system that held the record among self-contained systems. We also include the optimized results we found for word2vec (Mikolov et al. 2013), which scored best among other word-embedding systems on this task.</p>
<p>The results of three systems - SuperSim, LRA, and our ConceptNet Numberbatch - are all within each other's $95 \%$ confidence intervals, indicating that the ranking of the re-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>sults could easily change with a different selection of questions. Our score of $56.1 \%$ is also within the confidence interval of the performance of the average human college applicant on these questions, said to be $57.0 \%$ (Turney 2006).</p>
<p>We have shown that knowledge-informed word embeddings are up to the challenge of real SAT analogies; they perform the same as or slightly better than non-wordembedding systems on the same evaluation, when other word embeddings perform worse. In practice, recent word embeddings have instead been evaluated on simpler, synthetic analogy data sets (Mikolov et al. 2013), and have not usually been compared to existing non-embedding-based methods of solving analogies.</p>
<p>We achieve this performance even though the system, like other systems that form analogies from word embeddings, is only adding and subtracting values that measure the relatedness of terms; it uses no particular representation of what the relationships between these terms actually are. There is likely a way to take ConceptNet's relation labels into account and perform even better at analogies.</p>
<h2>Story Cloze Test</h2>
<p>The performance of our system on the Story Cloze Test was acceptable but unremarkable. ConceptNet Numberbatch chose the correct ending $59.4 \%$ of the time, which is in fact slightly better than any results reported by Mostafazadeh et al. (2016), including neural nets trained on the task. However, we could also achieve a similar score by using the same bag-of-vectors approach on other word embeddings. The best score of $59.9 \%$ was achieved by LexVec, with ConceptNet Numberbatch, GloVe, and word2vec all within its confidence interval.</p>
<p>This result should perhaps be comforting to those who aim to improve the computational understanding of stories. A bag-of-vectors approach may be marginally more successful at choosing the correct ending to a story than other approaches, but the performance of this approach has likely reached a plateau. It seems that any sufficiently high-quality word embeddings can choose the correct ending about $59 \%$ of the time, based on nothing but the assumption that the end of a story should be similar to the rest of it. Consider this a baseline: any representation designed to usefully represent the events in stories should get more than $59 \%$ correct.</p>
<h2>Conclusion</h2>
<p>We have compared word embeddings that represent only distributional semantics (word2vec, GloVe, and LexVec), word embeddings that represent only relational knowledge (ConceptNet PPMI), and the combination of the two (ConceptNet Numberbatch), and we have shown that the whole is more than the sum of its parts.</p>
<p>ConceptNet continues to be important in a field that has come to focus on word embeddings, because word embeddings can benefit from what ConceptNet knows. ConceptNet can make word embeddings more robust and more correlated with human judgments, as shown by the state-of-theart results that ConceptNet Numberbatch achieves at matching human annotators on multiple evaluations.</p>
<p>Any technique built on word embeddings should consider including a source of relational knowledge, or starting from a pre-trained set of word embeddings that has taken relational knowledge into account. One of the many goals of ConceptNet is to provide this knowledge in a convenient form that can be applied across many domains and many languages.</p>
<h2>Availability of the Code and Data</h2>
<p>The code and documentation of ConceptNet 5.5 can be found on GitHub at https://github.com/commonsense/ conceptnet5, and the knowledge graph can be browsed at http://conceptnet.io. The full build process, as well as the evaluation graph, can be reproduced using the instructions included in the README file for using Snakemake, a build system for data science (Köster and Rahmann 2012), and optionally using Docker Compose to reproduce the system environment. The version of the repository as of the submission of this paper has been tagged as aaai2017.</p>
<p>The ConceptNet Numberbatch word embeddings that resulted from this build process in September 2016 are the ones evaluated in this paper; they can be downloaded as prebuilt embeddings from https://github.com/commonsense/ conceptnet-numberbatch, tag 16.09.</p>
<h2>Acknowledgments</h2>
<p>We would like to thank the tens of thousands of volunteers who provided the crowd-sourced knowledge that makes ConceptNet possible. This includes contributors to Open Mind Common Sense and its related projects, as well as contributors to Wikipedia and Wiktionary, who are improving the state of knowledge for humans and computers alike.</p>
<h2>References</h2>
<p>Anacleto, J.; Lieberman, H.; Tsutsumi, M.; Neris, V.; Carvalho, A.; Espinosa, J.; Godoi, M.; and Zem-Mascarenhas, S. 2006. Can common sense uncover cultural differences in computer applications? In Artificial intelligence in theory and practice. Springer. 1-10.
Auer, S.; Bizer, C.; Kobilarov, G.; Lehmann, J.; Cyganiak, R.; and Ives, Z. 2007. DBpedia: A nucleus for a web of open data. Springer.
Bond, F., and Foster, R. 2013. Linking and Extending an Open Multilingual Wordnet. In 51st Annual Meeting of the Association for Computational Linguistics: ACL-2013, $1352-1362$.
Bonett, D. G., and Wright, T. A. 2000. Sample size requirements for estimating Pearson, Kendall and Spearman correlations. Psychometrika 65(1):23-28.
Breen, J. 2004. JMDict: a Japanese-multilingual dictionary. In Proceedings of the Workshop on Multilingual Linguistic Resources, 71-79. Association for Computational Linguistics.
Bruni, E.; Tran, N.-K.; and Baroni, M. 2014. Multimodal distributional semantics. J. Artif. Intell. Res. (JAIR) 49:1-47.</p>
<p>Faruqui, M.; Dodge, J.; Jauhar, S. K.; Dyer, C.; Hovy, E.; and Smith, N. A. 2015. Retrofitting word vectors to semantic lexicons. In Proceedings of NAACL.
Finkelstein, L.; Gabrilovich, E.; Matias, Y.; Rivlin, E.; Solan, Z.; Wolfman, G.; and Ruppin, E. 2001. Placing search in context: The concept revisited. In Proceedings of the 10th international conference on World Wide Web, 406414. ACM.</p>
<p>Halawi, G.; Dror, G.; Gabrilovich, E.; and Koren, Y. 2012. Large-scale learning of word relatedness with constraints. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, 1406-1414. ACM.
Hassan, S., and Mihalcea, R. 2009. Cross-lingual semantic relatedness using encyclopedic knowledge. In Proceedings of the conference on Empirical Methods in Natural Language Processing.
Herdağdelen, A., and Baroni, M. 2009. Bagpack: A general framework to represent semantic relations. In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, GEMS '09, 33-40. Stroudsburg, PA, USA: Association for Computational Linguistics.
Köster, J., and Rahmann, S. 2012. Snakemake-a scalable bioinformatics workflow engine. Bioinformatics 28(19):2520-2522.
Kuo, Y.-L.; Lee, J.-C.; Chiang, K.-Y.; Wang, R.; Shen, E.; Chan, C.-W.; and Hsu, J. Y.-J. 2009. Community-based game design: experiments on social games for commonsense data collection. In Proceedings of the ACM SIGKDD Workshop on Human Computation, 15-22. ACM.
Lenat, D. B., and Guha, R. V. 1989. Building large knowledge-based systems: representation and inference in the Cyc project. Addison-Wesley Longman.
Levy, O.; Goldberg, Y.; and Dagan, I. 2015. Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics 3:211-225.
Liu, H., and Singh, P. 2004. ConceptNet - a practical commonsense reasoning tool-kit. BT Technology Journal 22(4):211-226.
Luong, M.-T.; Socher, R.; and Manning, C. D. 2013. Better word representations with recursive neural networks for morphology. CoNLL-2013 104.
Mikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013. Efficient estimation of word representations in vector space. CoRR abs/1301.3781.
Miller, G.; Fellbaum, C.; Tengi, R.; Wakefield, P.; Langone, H.; and Haskell, B. 1998. WordNet. MIT Press Cambridge.</p>
<p>Mostafazadeh, N.; Chambers, N.; He, X.; Parikh, D.; Batra, D.; Vanderwende, L.; Kohli, P.; and Allen, J. 2016. A corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of NAACL: Human Language Technologies, 839-849. San Diego, California: Association for Computational Linguistics.
Nakahara, K., and Yamada, S. 2011. Development and evaluation of a Web-based game for common-sense knowledge
acquisition in Japan. Unisys Technical Report 30(4):295305.</p>
<p>Nickel, M.; Rosasco, L.; and Poggio, T. 2016. Holographic embeddings of knowledge graphs. In AAAI.
Pennington, J.; Socher, R.; and Manning, C. D. 2014. GloVe: Global vectors for word representation. Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014) 12:1532-1543.
Pustejovsky, J. 1991. The generative lexicon. Computational linguistics 17(4):409-441.
Salle, A.; Idiart, M.; and Villavicencio, A. 2016. Enhancing the LexVec distributed word representation model using positional contexts and external memory. arXiv preprint arXiv:1606.01283.
Singh, P. 2002. The public acquisition of commonsense knowledge. In Proceedings of AAAI Spring Symposium: Acquiring (and Using) Linguistic (and World) Knowledge for Information Access. AAAI.
Singhal, A. 2012. Introducing the knowledge graph: things, not strings. Official Google blog. Retrieved from https://googleblog.blogspot.com/2012/05/introducing-knowledge-graph-things-not.html on Dec. 1, 2016.
Speer, R., and Chin, J. 2016. An ensemble method to produce high-quality word embeddings. arXiv preprint arXiv:1604.01692.
Speer, R., and Havasi, C. 2013. Conceptnet 5: A large semantic network for relational knowledge. In The People's Web Meets NLP. Springer. 161-176.
Speer, R.; Chin, J.; Lin, A.; Nathan, L.; and Jewett, S. 2016. wordfreq: v1.5.1. DOI 10.5281/zenodo. 61937.
Turney, P. D. 2006. Similarity of semantic relations. Computational Linguistics 32(3):379-416.
Turney, P. D. 2013. Distributional semantics beyond words: Supervised learning of analogy and paraphrase. 1:353-366. von Ahn, L.; Kedia, M.; and Blum, M. 2006. Verbosity: a game for collecting common-sense facts. In Proceedings of the SIGCHI conference on Human Factors in computing systems, 75-78. ACM.
Xiao, M., and Guo, Y. 2014. Distributed word representation learning for cross-lingual dependency parsing. In CoNLL, $119-129$.
Zhao, K.; Hassan, H.; and Auli, M. 2015. Learning translation models from monolingual continuous representations. In Proceedings of NAACL.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ See http://www.aclweb.org/aclwiki/index.php?title= SAT_Analogy_Questions for a thorough list of results.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
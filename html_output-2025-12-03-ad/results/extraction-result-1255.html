<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1255 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1255</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1255</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-251979354</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2209.00588v2.pdf" target="_blank">Transformers are Sample-Efficient World Models</a></p>
                <p><strong>Paper Abstract:</strong> Deep reinforcement learning agents are notoriously sample inefficient, which considerably limits their application to real-world problems. Recently, many model-based methods have been designed to address this issue, with learning in the imagination of a world model being one of the most prominent approaches. However, while virtually unlimited interaction with a simulated environment sounds appealing, the world model has to be accurate over extended periods of time. Motivated by the success of Transformers in sequence modeling tasks, we introduce IRIS, a data-efficient agent that learns in a world model composed of a discrete autoencoder and an autoregressive Transformer. With the equivalent of only two hours of gameplay in the Atari 100k benchmark, IRIS achieves a mean human normalized score of 1.046, and outperforms humans on 10 out of 26 games, setting a new state of the art for methods without lookahead search. To foster future research on Transformers and world models for sample-efficient reinforcement learning, we release our code and models at https://github.com/eloialonso/iris.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1255.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1255.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IRIS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IRIS (Imagination with auto-Regression over an Inner Speech)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent that learns policies entirely in the imagination of a learned world model composed of a discrete autoencoder and an autoregressive GPT-like Transformer; optimized for sample-efficient RL on Atari 100k.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>IRIS world model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Hybrid latent world model: a discrete image autoencoder (VQVAE/VQGAN-style) encodes each 64×64 image into K discrete tokens from a vocabulary of size N; a GPT-like autoregressive Transformer G models sequences of interleaved frame tokens and actions to predict next-frame tokens, rewards and episode terminations; the decoder D reconstructs images from predicted tokens for policy/value rollouts in imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (discrete-token + autoregressive transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari 100k benchmark (Atari 2600 games)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction L1 loss (autoencoder), commitment loss, perceptual loss for autoencoder; cross-entropy for token predictions (transition and termination) and MSE or cross-entropy for reward prediction for Transformer; qualitative pixel-level prediction accuracy and task performance (human-normalized score) used as fidelity proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Qualitative: pixel-perfect predictions in some games (Pong) and correct reward/termination predictions in Breakout and Gopher; default autoencoder uses 16 tokens/frame (K=16) with vocabulary N=512; increasing tokens to 64 improved reconstruction and per-game returns (e.g., Alien return from 420 to 570, Asterix from 853.6 to 1890.4, BankHeist from 53.1 to 282.5 as reported in Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Partly interpretable: discrete tokens form a symbolic 'language' for frames allowing visualization of reconstructions and imagined trajectories; model outputs (predicted tokens, reconstructed frames, predicted rewards/terminals) are visualizable and used for qualitative assessment; overall Transformer dynamics remain a black-box neural sequence model.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of decoded imagined trajectories, reconstructed frames, and predicted rewards/episode ends; inspection of multiple imagined futures from the same conditioning frame to assess uncertainty representation; no formal latent disentanglement methods reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Training reported using 8 NVIDIA A100 40GB GPUs; with two Atari environments per GPU training takes ~7 days total, averaging ~3.5 days per environment (with 5 random seeds per env run); Transformer configuration: L=(timesteps)=20, M=10 layers, D=256 embedding dim (see appendix). Inference (imagination) uses autoregressive token generation for H=20-step rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Sample efficiency: achieves state-of-the-art among no-lookahead-search methods on Atari 100k using only 100k environment steps (~2 hours of gameplay); computationally more efficient than search-based methods at decision time (no MCTS) though training uses multiple GPUs; compared to SimPLe, IRIS trains faster (SimPLe reported 3 weeks on P100 for one env) and to MuZero/EfficientZero IRIS is designed for the low-data regime and outperforms MuZero in this sample-limited setting per the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>After 100k environment steps IRIS achieves a mean human-normalized score of 1.046 and is superhuman on 10 out of 26 Atari games (reported as new state-of-the-art for methods without lookahead search); IQM=0.501, optimality gap=0.512 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>The world model's fidelity is critical because the policy is trained entirely in imagination; high-quality reconstructions and accurate reward/termination predictions translate to strong policy performance in many games (pixel-perfect predictions correspond to good task behavior in Pong/Breakout), but gains are task-dependent: increasing token fidelity helped some games substantially (Asterix, BankHeist) while only marginally improving others (Alien), indicating that higher fidelity does not uniformly translate to proportional task gains due to exploration/hard-RL factors.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Tokens vs compute tradeoff: more tokens per frame (e.g., 64 vs 16) improve visual fidelity and sometimes task performance but at the cost of increased sequence length for the Transformer, higher memory and computation. Double exploration tradeoff: the agent must first discover rare events in real data for the world model to internalize them; otherwise imagined training cannot exploit those mechanics. Scaling (more data, larger model) improves performance but increases compute/time.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Discrete autoencoder (VQVAE/VQGAN-like) to compress images into K discrete tokens (default K=16, N=512); GPT-like Transformer (minGPT-style) modeling interleaved token/action sequences with autoregressive token-level generation; training autoencoder with L1+commitment+perceptual losses and Transformer with cross-entropy/MSE losses; imagination horizon H=20; actor-critic trained on reconstructed frames using DreamerV2-like objectives; burn-in of 20 frames for LSTM actor-critic hidden state.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to DreamerV2 (RSSM latent dynamics), IRIS replaces recurrent latent dynamics with an autoregressive Transformer over discrete visual tokens and achieves superior sample efficiency on Atari 100k among no-search methods. Compared to SimPLe (video-prediction + PPO), IRIS attains better mean performance and faster training. Compared to search-based methods (MuZero/EfficientZero), IRIS avoids expensive MCTS at decision time and is better-suited to the strict 100k-step/data-limited regime though those search methods can excel with more compute/data.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends choices rather than a single optimum: use discrete tokenization + Transformer for sequence modeling; increase tokens/frame for visually complex games (e.g., from 16 to 64) at the expense of compute; scale data and model capacity when resources allow; set imagination horizon (H) to 20 and train policy/value with λ-returns; trade fidelity vs compute per task — no single universal optimum provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformers are Sample-Efficient World Models', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1255.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1255.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Discrete Autoencoder (VQVAE / VQGAN family)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Discrete image autoencoder (VQVAE/VQGAN-style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A convolutional encoder/decoder that maps high-dimensional image frames to a small sequence of discrete token indices (vocabulary) and back, trained with reconstruction, commitment and perceptual losses to enable tractable Transformer sequence modeling over images.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Taming transformers for high-resolution image synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VQVAE / VQGAN-style discrete autoencoder (used in IRIS)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>CNN encoder produces K×d outputs quantized to nearest embedding vectors from a vocabulary of size N (vector-quantized latent codes), and a CNN decoder reconstructs 64×64 RGB frames from the K discrete tokens; trained with L1 reconstruction, commitment losses, and a perceptual loss; straight-through estimator used for backprop through discrete indices.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>discrete latent representation model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Image compression for model-based RL (Atari frames); also referenced for image/video generation domains</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction L1 loss, commitment loss, perceptual loss; qualitative reconstruction fidelity and downstream policy performance used as practical metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Default setup: vocabulary N=512, tokens per frame K=16, token embedding dim d=512; qualitative reconstructions are sufficient for many games, but some visually challenging games require more tokens (K=64) to correctly render important small sprites (example: Alien reconstructions improved with 64 tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Provides partially interpretable discrete symbolic codes per image patch—enables inspection of decoded frames and reasoning about token-level generation; not fully disentangled or semantically labeled in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visual comparison of real frames vs reconstructions at different token counts (16 vs 64); visualization of decoded imagined sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Smaller K (e.g., 16 tokens) reduces Transformer sequence length and compute; increasing tokens to 64 increases Transformer input length 4×, raising memory and compute requirements. Specific training details: autoencoder batch size 256, training starts after 5 epochs in the IRIS training loop.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Using discrete tokens enables Transformers to operate on compressed visual sequences instead of raw pixels, making autoregressive modeling computationally feasible; tradeoff: higher K improves fidelity but costs more compute than lower-K configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Improving autoencoder fidelity (more tokens) led to improved downstream returns on some games (Asterix +121%, BankHeist +432% with K=64), but only marginal gains on others (Alien +36%), indicating heterogeneous task utility.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Discrete compression prioritizes task-relevant fidelity by enabling tractable long-horizon sequence modeling; however, insufficient token budget can erase small but task-critical details (player/enemy sprites), hurting policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Token count vs compute vs task return: larger token budgets increase fidelity and sometimes returns but at higher computation and memory costs; diminishing returns observed for some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Default N=512 vocabulary, K=16 tokens/frame, d=512 embedding dim; losses: L1 reconstruction + commitment + perceptual; straight-through estimator used for training; discriminator removed (VQGAN -> VQVAE+perceptual loss).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to operating Transformers on raw pixels (infeasible due to quadratic scaling), discrete autoencoders permit practical autoregressive modeling; compared to continuous latent RSSMs (DreamerV2), discrete tokens give a symbolic representation amenable to autoregressive Transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests increasing tokens/frame for visually complex games and balancing token count against Transformer sequence length and compute; no single optimal K is universal—task-dependent recommendation (e.g., K=64 for Alien).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformers are Sample-Efficient World Models', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1255.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1255.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Autoregressive Transformer (GPT-like)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-like autoregressive Transformer dynamics model (minGPT-based)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive Transformer that models interleaved sequences of discrete frame tokens and actions to predict next-frame token distributions, rewards and episode terminations for rollouts in imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-like Transformer dynamics model (G)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Inputs are sequences of tokenized frames and discrete actions interleaved across time; the model autoregressively predicts next-frame tokens token-by-token conditioned on past tokens and actions, and also predicts scalar rewards and binary episode terminations; implemented with GPT2-like blocks, self-attention and per-position MLPs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>neural autoregressive sequence model for dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Modeling environment dynamics for imagined rollouts in Atari games</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Cross-entropy loss on next-token prediction and termination, MSE or cross-entropy on reward prediction; qualitative correctness of decoded imagined frames and reward/termination timing.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Configured with timesteps L=20, M=10 transformer layers, D=256 embedding dim, 4 attention heads; yields accurate short-horizon predictions (H=20) sufficient for training policies in imagination and demonstrates generating multiple plausible futures.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Transformer internals are a black-box, but outputs (predicted tokens and decoded frames, predicted rewards/ends) are interpretable and visualized; attention maps or other interpretability analyses are not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visual inspection of decoded imagined sequences and predicted reward/termination events; no attention visualization or probing reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Sequence length equals L*(K+1) tokens (K tokens per frame + action), so computational cost scales linearly with K and L and quadratically per-layer with sequence length in standard self-attention; Transformer batch size 64 during training; embedding/attention dropout and weight decay used.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Enables massively parallel imagination rollouts without online search overhead; more compute-efficient at decision time than MCTS-based approaches since no lookahead search is performed, but training the Transformer still requires GPU clusters (8 A100s used by authors).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Drives IRIS performance: enables sample-efficient policy learning resulting in mean human-normalized 1.046 on Atari 100k when combined with the discrete autoencoder and actor-critic training in imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Transformer-based dynamics effectively captures short-to-medium horizon dynamics and stochasticity (generating diverse plausible futures) that enable policy learning in imagination; limited coverage of rare events in training data (double exploration) can prevent modeling of some mechanics.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Sequence length (K, L) vs compute: longer sequences (more tokens/frame or longer memory) improve fidelity but increase training memory/time; autoregressive token-level generation allows fine-grained modeling but requires many steps per predicted frame.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>MinGPT-based implementation, L=20 timesteps, K tokens/frame default 16, action embeddings included, cross-entropy/MSE objectives, M=10 layers, D=256, 4 heads, dropout and weight decay hyperparameters specified in appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to recurrent latent dynamics (RSSM in DreamerV2), autoregressive Transformers operate directly on discrete visual tokens and can model long-range dependencies without RNN recurrence; compared to trajectory-level generative models, token-level autoregression trades increased token steps for fine-grained visual fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests balancing tokens per frame and transformer timesteps to match visual complexity and computational budget; default recommended configuration worked well across many Atari games but increasing tokens is advisable for visually dense games.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformers are Sample-Efficient World Models', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1255.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1255.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DreamerV2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DreamerV2 (latent dynamics with RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior imagination-based agent that learns latent dynamics via a recurrent state-space model (RSSM) and trains policies from imagined rollouts; referenced as a strong baseline for world-model-based RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering atari with discrete world models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DreamerV2 world model (RSSM latent dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Convolutional autoencoder combined with a recurrent state-space model (RSSM) to learn compact latent dynamics; used to generate imagined trajectories for policy/value training in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (continuous latent RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari and other pixel-based RL benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Latent prediction losses and reconstruction losses in RSSM-based training; evaluated by downstream RL performance (e.g., Atari scores) per citations in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Paper references DreamerV2 as the best agent learning in imagination in larger-data regimes but notes DreamerV2 was developed and evaluated with 200 million frames (far from the 100k sample-efficient regime). No specific numeric fidelity metrics presented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent RSSM is a continuous latent model and generally considered less directly interpretable than discrete-token models; paper notes Transformer-based world models can be advantageous for sequence modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not discussed in detail in this paper; DreamerV2 uses latent variables but no explicit interpretability methods are described here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>DreamerV2 in cited work was trained with far more frames (200M) implying significant compute; specifics not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>DreamerV2 is effective with large-scale data; IRIS is designed to be more sample efficient in the 100k-step regime by using discrete token Transformers instead of RSSM recurrence.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Cited as achieving human-level performance in Atari 50M benchmark in original work; in this paper DreamerV2 is used as a reference point for imagination-based approaches rather than an experimental baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>RSSM-based imagination works well with large data budgets but was not optimized for the strict 100k frame/sample-efficient setting targeted by IRIS.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Recurrent latent models may require more data to learn stable dynamics than autoregressive Transformer models over discrete tokens in low-data regimes (as suggested by the authors' motivation).</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Not detailed in this paper beyond noting the RSSM + convolutional autoencoder architecture used by Dreamer family.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Paper positions IRIS (discrete-token + Transformer) as an alternative to DreamerV2's RSSM latent dynamics for better sample efficiency in Atari 100k.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not specified in this paper; DreamerV2 is referenced for its strong performance with large datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformers are Sample-Efficient World Models', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1255.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1255.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SimPLe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SimPLe (model-based RL for Atari)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A video-prediction-based world-model method that trains a policy (PPO) inside a learned video model; an early imagination-based approach evaluated on Atari 100k.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Model based reinforcement learning for atari</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SimPLe video-prediction world model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learns a video-generation model (pixel-level prediction) of the environment and trains policies via PPO inside the learned simulator; emphasizes learning from limited data.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>video-prediction world model (pixel-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari 100k benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Pixel prediction losses and downstream RL returns; success measured by policy performance on Atari 100k.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>SimPLe demonstrated promise on Atari 100k in prior work; in this paper SimPLe is a baseline and reported results are shown in Table 1 for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Pixel-level video predictions are directly interpretable visually; no further interpretability analysis reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visual inspection of generated frames in prior SimPLe work; referenced qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>In prior work cited, SimPLe trained for 3 weeks on a P100 GPU for a single environment (as reported in this paper's computational resources section).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>SimPLe is an earlier model-based attempt at sample-efficient RL; IRIS reports better mean performance and faster training given resources described.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Used as baseline; specific per-game numbers for SimPLe appear in Table 1 for comparison with IRIS.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Video-prediction world models can support policy learning in small-data regimes, but IRIS argues that discrete-token + Transformer dynamics offers advantages in modeling complexity and sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Pixel-level models can be expensive to roll out and may struggle with stochasticity/long horizons; discrete-token Transformers aim to mitigate some of these issues.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>SimPLe uses video generation + PPO; contrasted with IRIS's discrete-token autoregressive approach.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>IRIS outperforms SimPLe on Atari 100k per reported results in the paper and is faster to train in the reported compute budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed in this paper beyond being a baseline for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformers are Sample-Efficient World Models', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1255.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1255.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuZero</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuZero</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based RL algorithm that learns a model of environment dynamics and uses Monte Carlo Tree Search (MCTS) over latent predictions as a planning operator at decision time; a high-performing search-based baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering atari, go, chess and shogi by planning with a learned model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MuZero (learned model + MCTS)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learns a latent dynamics model and value/policy heads and performs MCTS over learned latent states to perform planning at decision time; combines model learning and search for strong performance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>hybrid: learned latent model + search/solver (MCTS)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Board games, Atari, and other control domains where planning helps</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Quality of learned model judged by planning performance (game scores) and search outcomes; not measured by pixel reconstruction here.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>MuZero achieves strong performance in large-data / compute regimes; in this paper MuZero's performance on Atari 100k is reported for comparison and is noted as not designed for the sample-efficient regime.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Planning traces via MCTS are interpretable at the trajectory level; the learned latent model itself is a neural model and largely black-box.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>MCTS rollouts can be inspected; paper does not report attention or latent probing.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>MuZero originally used 40 TPUs for 12 hours per Atari environment (cited in this paper); MCTS incurs significant decision-time computation compared to no-search methods.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Compared to IRIS (no lookahead), MuZero has much higher decision-time compute due to MCTS; IRIS outperforms MuZero in the sample-limited Atari 100k regime according to the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>MuZero is a top-performing method in many benchmarks but is not optimized for the strict 100k-step sample budget; reported MuZero results in Table 1 are provided for reference.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Search with a learned model can yield high performance but comes with a large computational cost; in low-data regimes a fast, accurate world model without search (IRIS) may be preferable.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Planning (MCTS) improves performance but increases computation and engineering complexity; no-search world models (IRIS) favor computational simplicity at decision time.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Combination of learned dynamics + MCTS; specifics beyond scope of this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>IRIS lacks lookahead search but attains state-of-the-art among no-search methods and outperforms MuZero in the 100k regime per the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not specified within this paper; MuZero is noted as resource-intensive.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformers are Sample-Efficient World Models', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1255.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1255.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EfficientZero</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EfficientZero</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An improved search-based algorithm building on MuZero with self-supervised consistency losses and efficiency gains, used as an example of lookahead search methods in Atari.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering atari games with limited data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>EfficientZero (search-enhanced world model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Enhances MuZero by introducing self-supervised consistency losses, predicting returns over short horizons, and correcting off-policy trajectories using its world model; still uses MCTS-like planning at decision time.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>hybrid: learned latent model + search</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games, limited-data settings (as per cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Return prediction and planning performance; not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported by cited work to be very strong; in this paper EfficientZero results are listed for comparison (Table 1/8) but EfficientZero is not used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Planning traces interpretable at trajectory level; internal model same caveats as MuZero.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Cited implementation trains in ~7 hours with 4 RTX 3090 GPUs in Ye et al. (2021) (per paper), typically requires distributed infrastructure and an MCTS implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>EfficientZero reduces some compute compared to MuZero but still uses search at decision time; IRIS avoids this and focuses on low-data sample efficiency without search.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Strong performance in cited work; used as a high-performing lookahead-search baseline for comparisons in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Search methods are powerful but come with code complexity and decision-time compute; IRIS emphasizes efficient imagination without search.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Search gains vs compute and complexity; EfficientZero trades some compute for consistency losses to be more efficient than earlier search methods.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Cited as using short-horizon return prediction and consistency losses; details in the EfficientZero paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared at high level to IRIS as representative of search-based methods; IRIS is competitive/outperforms in the 100k regime according to presented results.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformers are Sample-Efficient World Models', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1255.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1255.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Trajectory/Decision Transformer family</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Trajectory Transformer / Decision Transformer / Transdreamer (sequence-modeling world models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sequence-modeling approaches that treat offline or online RL as sequence modeling: Trajectory Transformer models trajectories for planning, Decision Transformer conditions on returns to produce actions, and Transdreamer replaces recurrent RSSM with Transformer in Dreamer-style world models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Offline reinforcement learning as one big sequence modeling problem</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Trajectory Transformer / Decision Transformer / Transdreamer (transformer-based sequence models for RL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Trajectory Transformer: autoregressive modeling of offline trajectories to enable planning via beam search; Decision Transformer: conditional sequence model that outputs actions given desired returns; Transdreamer: DreamerV2 variant replacing the RSSM with a Transformer for latent dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>sequence-modeling world models / offline sequence RL models</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Offline RL, planning from images/trajectories, and extensions to online RL (various domains including low-dimensional and pixel domains)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Likelihood/cross-entropy of token/trajectory predictions and downstream policy/planning performance; for Transdreamer, latent prediction/return prediction losses similar to DreamerV2 variants.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Paper notes Trajectory Transformer is limited to low-dimensional states for planning with reward-driven beam search; Decision Transformer can handle image inputs but cannot straightforwardly be extended as a generative world model for imagination; Transdreamer (cited Chen et al., 2022) explored replacing RSSM with Transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Outputs are sequence predictions that can be inspected; Decision Transformer policies are conditioned on returns, enabling some interpretability in behavior conditioning; no detailed interpretability methods reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not detailed in this paper beyond being sequence models whose outputs can be visualized.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Sequence models can be costly for long-horizon/video-sized sequences; Trajectory Transformer planning requires beam search at inference which increases compute.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Decision Transformer trades planning/search for supervised sequence modeling but is not directly a world model for imagination; Transdreamer suggests Transformer latent dynamics can be effective as an RSSM replacement.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Discussed as related approaches with different strengths/limitations compared to IRIS; no direct experimental comparison in this paper beyond qualitative discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Sequence modeling is a promising paradigm; however, Trajectory Transformer and Decision Transformer have limitations for being used as full world models for imagination over high-dimensional images, whereas IRIS leverages discrete tokens to make autoregressive modeling tractable.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Sequence models may struggle with long pixel sequences or require adaptations (video-level discrete autoencoders) to scale; Decision Transformer is simple but not a natural world model for imagination training.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Trajectory Transformer uses trajectory-level discrete latent variables; Decision Transformer frames RL as sequence modeling of state-action-return tuples; Transdreamer replaces recurrent dynamics with Transformer in Dreamer-style architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>IRIS positions its discrete-token + Transformer approach as practical for image-based world modeling compared to direct application of Trajectory/Decision Transformers to pixel sequences without compression.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper references these works for context and suggests discrete-token compression + Transformer is an effective design for image-based imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformers are Sample-Efficient World Models', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mastering atari with discrete world models <em>(Rating: 2)</em></li>
                <li>Model based reinforcement learning for atari <em>(Rating: 2)</em></li>
                <li>Mastering atari, go, chess and shogi by planning with a learned model <em>(Rating: 2)</em></li>
                <li>Mastering atari games with limited data <em>(Rating: 2)</em></li>
                <li>Taming transformers for high-resolution image synthesis <em>(Rating: 2)</em></li>
                <li>Offline reinforcement learning as one big sequence modeling problem <em>(Rating: 1)</em></li>
                <li>Decision transformer: Reinforcement learning via sequence modeling <em>(Rating: 1)</em></li>
                <li>Transdreamer: Reinforcement learning with transformer world models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1255",
    "paper_id": "paper-251979354",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "IRIS",
            "name_full": "IRIS (Imagination with auto-Regression over an Inner Speech)",
            "brief_description": "An agent that learns policies entirely in the imagination of a learned world model composed of a discrete autoencoder and an autoregressive GPT-like Transformer; optimized for sample-efficient RL on Atari 100k.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "IRIS world model",
            "model_description": "Hybrid latent world model: a discrete image autoencoder (VQVAE/VQGAN-style) encodes each 64×64 image into K discrete tokens from a vocabulary of size N; a GPT-like autoregressive Transformer G models sequences of interleaved frame tokens and actions to predict next-frame tokens, rewards and episode terminations; the decoder D reconstructs images from predicted tokens for policy/value rollouts in imagination.",
            "model_type": "latent world model (discrete-token + autoregressive transformer)",
            "task_domain": "Atari 100k benchmark (Atari 2600 games)",
            "fidelity_metric": "Reconstruction L1 loss (autoencoder), commitment loss, perceptual loss for autoencoder; cross-entropy for token predictions (transition and termination) and MSE or cross-entropy for reward prediction for Transformer; qualitative pixel-level prediction accuracy and task performance (human-normalized score) used as fidelity proxies.",
            "fidelity_performance": "Qualitative: pixel-perfect predictions in some games (Pong) and correct reward/termination predictions in Breakout and Gopher; default autoencoder uses 16 tokens/frame (K=16) with vocabulary N=512; increasing tokens to 64 improved reconstruction and per-game returns (e.g., Alien return from 420 to 570, Asterix from 853.6 to 1890.4, BankHeist from 53.1 to 282.5 as reported in Table 7).",
            "interpretability_assessment": "Partly interpretable: discrete tokens form a symbolic 'language' for frames allowing visualization of reconstructions and imagined trajectories; model outputs (predicted tokens, reconstructed frames, predicted rewards/terminals) are visualizable and used for qualitative assessment; overall Transformer dynamics remain a black-box neural sequence model.",
            "interpretability_method": "Visualization of decoded imagined trajectories, reconstructed frames, and predicted rewards/episode ends; inspection of multiple imagined futures from the same conditioning frame to assess uncertainty representation; no formal latent disentanglement methods reported.",
            "computational_cost": "Training reported using 8 NVIDIA A100 40GB GPUs; with two Atari environments per GPU training takes ~7 days total, averaging ~3.5 days per environment (with 5 random seeds per env run); Transformer configuration: L=(timesteps)=20, M=10 layers, D=256 embedding dim (see appendix). Inference (imagination) uses autoregressive token generation for H=20-step rollouts.",
            "efficiency_comparison": "Sample efficiency: achieves state-of-the-art among no-lookahead-search methods on Atari 100k using only 100k environment steps (~2 hours of gameplay); computationally more efficient than search-based methods at decision time (no MCTS) though training uses multiple GPUs; compared to SimPLe, IRIS trains faster (SimPLe reported 3 weeks on P100 for one env) and to MuZero/EfficientZero IRIS is designed for the low-data regime and outperforms MuZero in this sample-limited setting per the paper.",
            "task_performance": "After 100k environment steps IRIS achieves a mean human-normalized score of 1.046 and is superhuman on 10 out of 26 Atari games (reported as new state-of-the-art for methods without lookahead search); IQM=0.501, optimality gap=0.512 (Table 1).",
            "task_utility_analysis": "The world model's fidelity is critical because the policy is trained entirely in imagination; high-quality reconstructions and accurate reward/termination predictions translate to strong policy performance in many games (pixel-perfect predictions correspond to good task behavior in Pong/Breakout), but gains are task-dependent: increasing token fidelity helped some games substantially (Asterix, BankHeist) while only marginally improving others (Alien), indicating that higher fidelity does not uniformly translate to proportional task gains due to exploration/hard-RL factors.",
            "tradeoffs_observed": "Tokens vs compute tradeoff: more tokens per frame (e.g., 64 vs 16) improve visual fidelity and sometimes task performance but at the cost of increased sequence length for the Transformer, higher memory and computation. Double exploration tradeoff: the agent must first discover rare events in real data for the world model to internalize them; otherwise imagined training cannot exploit those mechanics. Scaling (more data, larger model) improves performance but increases compute/time.",
            "design_choices": "Discrete autoencoder (VQVAE/VQGAN-like) to compress images into K discrete tokens (default K=16, N=512); GPT-like Transformer (minGPT-style) modeling interleaved token/action sequences with autoregressive token-level generation; training autoencoder with L1+commitment+perceptual losses and Transformer with cross-entropy/MSE losses; imagination horizon H=20; actor-critic trained on reconstructed frames using DreamerV2-like objectives; burn-in of 20 frames for LSTM actor-critic hidden state.",
            "comparison_to_alternatives": "Compared to DreamerV2 (RSSM latent dynamics), IRIS replaces recurrent latent dynamics with an autoregressive Transformer over discrete visual tokens and achieves superior sample efficiency on Atari 100k among no-search methods. Compared to SimPLe (video-prediction + PPO), IRIS attains better mean performance and faster training. Compared to search-based methods (MuZero/EfficientZero), IRIS avoids expensive MCTS at decision time and is better-suited to the strict 100k-step/data-limited regime though those search methods can excel with more compute/data.",
            "optimal_configuration": "Paper recommends choices rather than a single optimum: use discrete tokenization + Transformer for sequence modeling; increase tokens/frame for visually complex games (e.g., from 16 to 64) at the expense of compute; scale data and model capacity when resources allow; set imagination horizon (H) to 20 and train policy/value with λ-returns; trade fidelity vs compute per task — no single universal optimum provided.",
            "uuid": "e1255.0",
            "source_info": {
                "paper_title": "Transformers are Sample-Efficient World Models",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Discrete Autoencoder (VQVAE / VQGAN family)",
            "name_full": "Discrete image autoencoder (VQVAE/VQGAN-style)",
            "brief_description": "A convolutional encoder/decoder that maps high-dimensional image frames to a small sequence of discrete token indices (vocabulary) and back, trained with reconstruction, commitment and perceptual losses to enable tractable Transformer sequence modeling over images.",
            "citation_title": "Taming transformers for high-resolution image synthesis",
            "mention_or_use": "use",
            "model_name": "VQVAE / VQGAN-style discrete autoencoder (used in IRIS)",
            "model_description": "CNN encoder produces K×d outputs quantized to nearest embedding vectors from a vocabulary of size N (vector-quantized latent codes), and a CNN decoder reconstructs 64×64 RGB frames from the K discrete tokens; trained with L1 reconstruction, commitment losses, and a perceptual loss; straight-through estimator used for backprop through discrete indices.",
            "model_type": "discrete latent representation model",
            "task_domain": "Image compression for model-based RL (Atari frames); also referenced for image/video generation domains",
            "fidelity_metric": "Reconstruction L1 loss, commitment loss, perceptual loss; qualitative reconstruction fidelity and downstream policy performance used as practical metrics.",
            "fidelity_performance": "Default setup: vocabulary N=512, tokens per frame K=16, token embedding dim d=512; qualitative reconstructions are sufficient for many games, but some visually challenging games require more tokens (K=64) to correctly render important small sprites (example: Alien reconstructions improved with 64 tokens).",
            "interpretability_assessment": "Provides partially interpretable discrete symbolic codes per image patch—enables inspection of decoded frames and reasoning about token-level generation; not fully disentangled or semantically labeled in the paper.",
            "interpretability_method": "Visual comparison of real frames vs reconstructions at different token counts (16 vs 64); visualization of decoded imagined sequences.",
            "computational_cost": "Smaller K (e.g., 16 tokens) reduces Transformer sequence length and compute; increasing tokens to 64 increases Transformer input length 4×, raising memory and compute requirements. Specific training details: autoencoder batch size 256, training starts after 5 epochs in the IRIS training loop.",
            "efficiency_comparison": "Using discrete tokens enables Transformers to operate on compressed visual sequences instead of raw pixels, making autoregressive modeling computationally feasible; tradeoff: higher K improves fidelity but costs more compute than lower-K configurations.",
            "task_performance": "Improving autoencoder fidelity (more tokens) led to improved downstream returns on some games (Asterix +121%, BankHeist +432% with K=64), but only marginal gains on others (Alien +36%), indicating heterogeneous task utility.",
            "task_utility_analysis": "Discrete compression prioritizes task-relevant fidelity by enabling tractable long-horizon sequence modeling; however, insufficient token budget can erase small but task-critical details (player/enemy sprites), hurting policy learning.",
            "tradeoffs_observed": "Token count vs compute vs task return: larger token budgets increase fidelity and sometimes returns but at higher computation and memory costs; diminishing returns observed for some tasks.",
            "design_choices": "Default N=512 vocabulary, K=16 tokens/frame, d=512 embedding dim; losses: L1 reconstruction + commitment + perceptual; straight-through estimator used for training; discriminator removed (VQGAN -&gt; VQVAE+perceptual loss).",
            "comparison_to_alternatives": "Compared to operating Transformers on raw pixels (infeasible due to quadratic scaling), discrete autoencoders permit practical autoregressive modeling; compared to continuous latent RSSMs (DreamerV2), discrete tokens give a symbolic representation amenable to autoregressive Transformers.",
            "optimal_configuration": "Paper suggests increasing tokens/frame for visually complex games and balancing token count against Transformer sequence length and compute; no single optimal K is universal—task-dependent recommendation (e.g., K=64 for Alien).",
            "uuid": "e1255.1",
            "source_info": {
                "paper_title": "Transformers are Sample-Efficient World Models",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Autoregressive Transformer (GPT-like)",
            "name_full": "GPT-like autoregressive Transformer dynamics model (minGPT-based)",
            "brief_description": "An autoregressive Transformer that models interleaved sequences of discrete frame tokens and actions to predict next-frame token distributions, rewards and episode terminations for rollouts in imagination.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-like Transformer dynamics model (G)",
            "model_description": "Inputs are sequences of tokenized frames and discrete actions interleaved across time; the model autoregressively predicts next-frame tokens token-by-token conditioned on past tokens and actions, and also predicts scalar rewards and binary episode terminations; implemented with GPT2-like blocks, self-attention and per-position MLPs.",
            "model_type": "neural autoregressive sequence model for dynamics",
            "task_domain": "Modeling environment dynamics for imagined rollouts in Atari games",
            "fidelity_metric": "Cross-entropy loss on next-token prediction and termination, MSE or cross-entropy on reward prediction; qualitative correctness of decoded imagined frames and reward/termination timing.",
            "fidelity_performance": "Configured with timesteps L=20, M=10 transformer layers, D=256 embedding dim, 4 attention heads; yields accurate short-horizon predictions (H=20) sufficient for training policies in imagination and demonstrates generating multiple plausible futures.",
            "interpretability_assessment": "Transformer internals are a black-box, but outputs (predicted tokens and decoded frames, predicted rewards/ends) are interpretable and visualized; attention maps or other interpretability analyses are not reported.",
            "interpretability_method": "Visual inspection of decoded imagined sequences and predicted reward/termination events; no attention visualization or probing reported in the paper.",
            "computational_cost": "Sequence length equals L*(K+1) tokens (K tokens per frame + action), so computational cost scales linearly with K and L and quadratically per-layer with sequence length in standard self-attention; Transformer batch size 64 during training; embedding/attention dropout and weight decay used.",
            "efficiency_comparison": "Enables massively parallel imagination rollouts without online search overhead; more compute-efficient at decision time than MCTS-based approaches since no lookahead search is performed, but training the Transformer still requires GPU clusters (8 A100s used by authors).",
            "task_performance": "Drives IRIS performance: enables sample-efficient policy learning resulting in mean human-normalized 1.046 on Atari 100k when combined with the discrete autoencoder and actor-critic training in imagination.",
            "task_utility_analysis": "Transformer-based dynamics effectively captures short-to-medium horizon dynamics and stochasticity (generating diverse plausible futures) that enable policy learning in imagination; limited coverage of rare events in training data (double exploration) can prevent modeling of some mechanics.",
            "tradeoffs_observed": "Sequence length (K, L) vs compute: longer sequences (more tokens/frame or longer memory) improve fidelity but increase training memory/time; autoregressive token-level generation allows fine-grained modeling but requires many steps per predicted frame.",
            "design_choices": "MinGPT-based implementation, L=20 timesteps, K tokens/frame default 16, action embeddings included, cross-entropy/MSE objectives, M=10 layers, D=256, 4 heads, dropout and weight decay hyperparameters specified in appendix.",
            "comparison_to_alternatives": "Compared to recurrent latent dynamics (RSSM in DreamerV2), autoregressive Transformers operate directly on discrete visual tokens and can model long-range dependencies without RNN recurrence; compared to trajectory-level generative models, token-level autoregression trades increased token steps for fine-grained visual fidelity.",
            "optimal_configuration": "Paper suggests balancing tokens per frame and transformer timesteps to match visual complexity and computational budget; default recommended configuration worked well across many Atari games but increasing tokens is advisable for visually dense games.",
            "uuid": "e1255.2",
            "source_info": {
                "paper_title": "Transformers are Sample-Efficient World Models",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "DreamerV2",
            "name_full": "DreamerV2 (latent dynamics with RSSM)",
            "brief_description": "A prior imagination-based agent that learns latent dynamics via a recurrent state-space model (RSSM) and trains policies from imagined rollouts; referenced as a strong baseline for world-model-based RL.",
            "citation_title": "Mastering atari with discrete world models",
            "mention_or_use": "mention",
            "model_name": "DreamerV2 world model (RSSM latent dynamics)",
            "model_description": "Convolutional autoencoder combined with a recurrent state-space model (RSSM) to learn compact latent dynamics; used to generate imagined trajectories for policy/value training in latent space.",
            "model_type": "latent world model (continuous latent RSSM)",
            "task_domain": "Atari and other pixel-based RL benchmarks",
            "fidelity_metric": "Latent prediction losses and reconstruction losses in RSSM-based training; evaluated by downstream RL performance (e.g., Atari scores) per citations in paper.",
            "fidelity_performance": "Paper references DreamerV2 as the best agent learning in imagination in larger-data regimes but notes DreamerV2 was developed and evaluated with 200 million frames (far from the 100k sample-efficient regime). No specific numeric fidelity metrics presented in this paper.",
            "interpretability_assessment": "Latent RSSM is a continuous latent model and generally considered less directly interpretable than discrete-token models; paper notes Transformer-based world models can be advantageous for sequence modeling.",
            "interpretability_method": "Not discussed in detail in this paper; DreamerV2 uses latent variables but no explicit interpretability methods are described here.",
            "computational_cost": "DreamerV2 in cited work was trained with far more frames (200M) implying significant compute; specifics not detailed in this paper.",
            "efficiency_comparison": "DreamerV2 is effective with large-scale data; IRIS is designed to be more sample efficient in the 100k-step regime by using discrete token Transformers instead of RSSM recurrence.",
            "task_performance": "Cited as achieving human-level performance in Atari 50M benchmark in original work; in this paper DreamerV2 is used as a reference point for imagination-based approaches rather than an experimental baseline.",
            "task_utility_analysis": "RSSM-based imagination works well with large data budgets but was not optimized for the strict 100k frame/sample-efficient setting targeted by IRIS.",
            "tradeoffs_observed": "Recurrent latent models may require more data to learn stable dynamics than autoregressive Transformer models over discrete tokens in low-data regimes (as suggested by the authors' motivation).",
            "design_choices": "Not detailed in this paper beyond noting the RSSM + convolutional autoencoder architecture used by Dreamer family.",
            "comparison_to_alternatives": "Paper positions IRIS (discrete-token + Transformer) as an alternative to DreamerV2's RSSM latent dynamics for better sample efficiency in Atari 100k.",
            "optimal_configuration": "Not specified in this paper; DreamerV2 is referenced for its strong performance with large datasets.",
            "uuid": "e1255.3",
            "source_info": {
                "paper_title": "Transformers are Sample-Efficient World Models",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "SimPLe",
            "name_full": "SimPLe (model-based RL for Atari)",
            "brief_description": "A video-prediction-based world-model method that trains a policy (PPO) inside a learned video model; an early imagination-based approach evaluated on Atari 100k.",
            "citation_title": "Model based reinforcement learning for atari",
            "mention_or_use": "mention",
            "model_name": "SimPLe video-prediction world model",
            "model_description": "Learns a video-generation model (pixel-level prediction) of the environment and trains policies via PPO inside the learned simulator; emphasizes learning from limited data.",
            "model_type": "video-prediction world model (pixel-based)",
            "task_domain": "Atari 100k benchmark",
            "fidelity_metric": "Pixel prediction losses and downstream RL returns; success measured by policy performance on Atari 100k.",
            "fidelity_performance": "SimPLe demonstrated promise on Atari 100k in prior work; in this paper SimPLe is a baseline and reported results are shown in Table 1 for comparison.",
            "interpretability_assessment": "Pixel-level video predictions are directly interpretable visually; no further interpretability analysis reported here.",
            "interpretability_method": "Visual inspection of generated frames in prior SimPLe work; referenced qualitatively.",
            "computational_cost": "In prior work cited, SimPLe trained for 3 weeks on a P100 GPU for a single environment (as reported in this paper's computational resources section).",
            "efficiency_comparison": "SimPLe is an earlier model-based attempt at sample-efficient RL; IRIS reports better mean performance and faster training given resources described.",
            "task_performance": "Used as baseline; specific per-game numbers for SimPLe appear in Table 1 for comparison with IRIS.",
            "task_utility_analysis": "Video-prediction world models can support policy learning in small-data regimes, but IRIS argues that discrete-token + Transformer dynamics offers advantages in modeling complexity and sample efficiency.",
            "tradeoffs_observed": "Pixel-level models can be expensive to roll out and may struggle with stochasticity/long horizons; discrete-token Transformers aim to mitigate some of these issues.",
            "design_choices": "SimPLe uses video generation + PPO; contrasted with IRIS's discrete-token autoregressive approach.",
            "comparison_to_alternatives": "IRIS outperforms SimPLe on Atari 100k per reported results in the paper and is faster to train in the reported compute budgets.",
            "optimal_configuration": "Not discussed in this paper beyond being a baseline for comparison.",
            "uuid": "e1255.4",
            "source_info": {
                "paper_title": "Transformers are Sample-Efficient World Models",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "MuZero",
            "name_full": "MuZero",
            "brief_description": "A model-based RL algorithm that learns a model of environment dynamics and uses Monte Carlo Tree Search (MCTS) over latent predictions as a planning operator at decision time; a high-performing search-based baseline.",
            "citation_title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "mention_or_use": "mention",
            "model_name": "MuZero (learned model + MCTS)",
            "model_description": "Learns a latent dynamics model and value/policy heads and performs MCTS over learned latent states to perform planning at decision time; combines model learning and search for strong performance.",
            "model_type": "hybrid: learned latent model + search/solver (MCTS)",
            "task_domain": "Board games, Atari, and other control domains where planning helps",
            "fidelity_metric": "Quality of learned model judged by planning performance (game scores) and search outcomes; not measured by pixel reconstruction here.",
            "fidelity_performance": "MuZero achieves strong performance in large-data / compute regimes; in this paper MuZero's performance on Atari 100k is reported for comparison and is noted as not designed for the sample-efficient regime.",
            "interpretability_assessment": "Planning traces via MCTS are interpretable at the trajectory level; the learned latent model itself is a neural model and largely black-box.",
            "interpretability_method": "MCTS rollouts can be inspected; paper does not report attention or latent probing.",
            "computational_cost": "MuZero originally used 40 TPUs for 12 hours per Atari environment (cited in this paper); MCTS incurs significant decision-time computation compared to no-search methods.",
            "efficiency_comparison": "Compared to IRIS (no lookahead), MuZero has much higher decision-time compute due to MCTS; IRIS outperforms MuZero in the sample-limited Atari 100k regime according to the authors.",
            "task_performance": "MuZero is a top-performing method in many benchmarks but is not optimized for the strict 100k-step sample budget; reported MuZero results in Table 1 are provided for reference.",
            "task_utility_analysis": "Search with a learned model can yield high performance but comes with a large computational cost; in low-data regimes a fast, accurate world model without search (IRIS) may be preferable.",
            "tradeoffs_observed": "Planning (MCTS) improves performance but increases computation and engineering complexity; no-search world models (IRIS) favor computational simplicity at decision time.",
            "design_choices": "Combination of learned dynamics + MCTS; specifics beyond scope of this paper.",
            "comparison_to_alternatives": "IRIS lacks lookahead search but attains state-of-the-art among no-search methods and outperforms MuZero in the 100k regime per the paper.",
            "optimal_configuration": "Not specified within this paper; MuZero is noted as resource-intensive.",
            "uuid": "e1255.5",
            "source_info": {
                "paper_title": "Transformers are Sample-Efficient World Models",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "EfficientZero",
            "name_full": "EfficientZero",
            "brief_description": "An improved search-based algorithm building on MuZero with self-supervised consistency losses and efficiency gains, used as an example of lookahead search methods in Atari.",
            "citation_title": "Mastering atari games with limited data",
            "mention_or_use": "mention",
            "model_name": "EfficientZero (search-enhanced world model)",
            "model_description": "Enhances MuZero by introducing self-supervised consistency losses, predicting returns over short horizons, and correcting off-policy trajectories using its world model; still uses MCTS-like planning at decision time.",
            "model_type": "hybrid: learned latent model + search",
            "task_domain": "Atari games, limited-data settings (as per cited work)",
            "fidelity_metric": "Return prediction and planning performance; not detailed in this paper.",
            "fidelity_performance": "Reported by cited work to be very strong; in this paper EfficientZero results are listed for comparison (Table 1/8) but EfficientZero is not used in experiments.",
            "interpretability_assessment": "Planning traces interpretable at trajectory level; internal model same caveats as MuZero.",
            "interpretability_method": "Not described in this paper.",
            "computational_cost": "Cited implementation trains in ~7 hours with 4 RTX 3090 GPUs in Ye et al. (2021) (per paper), typically requires distributed infrastructure and an MCTS implementation.",
            "efficiency_comparison": "EfficientZero reduces some compute compared to MuZero but still uses search at decision time; IRIS avoids this and focuses on low-data sample efficiency without search.",
            "task_performance": "Strong performance in cited work; used as a high-performing lookahead-search baseline for comparisons in the paper.",
            "task_utility_analysis": "Search methods are powerful but come with code complexity and decision-time compute; IRIS emphasizes efficient imagination without search.",
            "tradeoffs_observed": "Search gains vs compute and complexity; EfficientZero trades some compute for consistency losses to be more efficient than earlier search methods.",
            "design_choices": "Cited as using short-horizon return prediction and consistency losses; details in the EfficientZero paper.",
            "comparison_to_alternatives": "Compared at high level to IRIS as representative of search-based methods; IRIS is competitive/outperforms in the 100k regime according to presented results.",
            "optimal_configuration": "Not specified in this paper.",
            "uuid": "e1255.6",
            "source_info": {
                "paper_title": "Transformers are Sample-Efficient World Models",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Trajectory/Decision Transformer family",
            "name_full": "Trajectory Transformer / Decision Transformer / Transdreamer (sequence-modeling world models)",
            "brief_description": "Sequence-modeling approaches that treat offline or online RL as sequence modeling: Trajectory Transformer models trajectories for planning, Decision Transformer conditions on returns to produce actions, and Transdreamer replaces recurrent RSSM with Transformer in Dreamer-style world models.",
            "citation_title": "Offline reinforcement learning as one big sequence modeling problem",
            "mention_or_use": "mention",
            "model_name": "Trajectory Transformer / Decision Transformer / Transdreamer (transformer-based sequence models for RL)",
            "model_description": "Trajectory Transformer: autoregressive modeling of offline trajectories to enable planning via beam search; Decision Transformer: conditional sequence model that outputs actions given desired returns; Transdreamer: DreamerV2 variant replacing the RSSM with a Transformer for latent dynamics.",
            "model_type": "sequence-modeling world models / offline sequence RL models",
            "task_domain": "Offline RL, planning from images/trajectories, and extensions to online RL (various domains including low-dimensional and pixel domains)",
            "fidelity_metric": "Likelihood/cross-entropy of token/trajectory predictions and downstream policy/planning performance; for Transdreamer, latent prediction/return prediction losses similar to DreamerV2 variants.",
            "fidelity_performance": "Paper notes Trajectory Transformer is limited to low-dimensional states for planning with reward-driven beam search; Decision Transformer can handle image inputs but cannot straightforwardly be extended as a generative world model for imagination; Transdreamer (cited Chen et al., 2022) explored replacing RSSM with Transformer.",
            "interpretability_assessment": "Outputs are sequence predictions that can be inspected; Decision Transformer policies are conditioned on returns, enabling some interpretability in behavior conditioning; no detailed interpretability methods reported here.",
            "interpretability_method": "Not detailed in this paper beyond being sequence models whose outputs can be visualized.",
            "computational_cost": "Sequence models can be costly for long-horizon/video-sized sequences; Trajectory Transformer planning requires beam search at inference which increases compute.",
            "efficiency_comparison": "Decision Transformer trades planning/search for supervised sequence modeling but is not directly a world model for imagination; Transdreamer suggests Transformer latent dynamics can be effective as an RSSM replacement.",
            "task_performance": "Discussed as related approaches with different strengths/limitations compared to IRIS; no direct experimental comparison in this paper beyond qualitative discussion.",
            "task_utility_analysis": "Sequence modeling is a promising paradigm; however, Trajectory Transformer and Decision Transformer have limitations for being used as full world models for imagination over high-dimensional images, whereas IRIS leverages discrete tokens to make autoregressive modeling tractable.",
            "tradeoffs_observed": "Sequence models may struggle with long pixel sequences or require adaptations (video-level discrete autoencoders) to scale; Decision Transformer is simple but not a natural world model for imagination training.",
            "design_choices": "Trajectory Transformer uses trajectory-level discrete latent variables; Decision Transformer frames RL as sequence modeling of state-action-return tuples; Transdreamer replaces recurrent dynamics with Transformer in Dreamer-style architectures.",
            "comparison_to_alternatives": "IRIS positions its discrete-token + Transformer approach as practical for image-based world modeling compared to direct application of Trajectory/Decision Transformers to pixel sequences without compression.",
            "optimal_configuration": "Paper references these works for context and suggests discrete-token compression + Transformer is an effective design for image-based imagination.",
            "uuid": "e1255.7",
            "source_info": {
                "paper_title": "Transformers are Sample-Efficient World Models",
                "publication_date_yy_mm": "2022-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mastering atari with discrete world models",
            "rating": 2,
            "sanitized_title": "mastering_atari_with_discrete_world_models"
        },
        {
            "paper_title": "Model based reinforcement learning for atari",
            "rating": 2,
            "sanitized_title": "model_based_reinforcement_learning_for_atari"
        },
        {
            "paper_title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "rating": 2,
            "sanitized_title": "mastering_atari_go_chess_and_shogi_by_planning_with_a_learned_model"
        },
        {
            "paper_title": "Mastering atari games with limited data",
            "rating": 2,
            "sanitized_title": "mastering_atari_games_with_limited_data"
        },
        {
            "paper_title": "Taming transformers for high-resolution image synthesis",
            "rating": 2,
            "sanitized_title": "taming_transformers_for_highresolution_image_synthesis"
        },
        {
            "paper_title": "Offline reinforcement learning as one big sequence modeling problem",
            "rating": 1,
            "sanitized_title": "offline_reinforcement_learning_as_one_big_sequence_modeling_problem"
        },
        {
            "paper_title": "Decision transformer: Reinforcement learning via sequence modeling",
            "rating": 1,
            "sanitized_title": "decision_transformer_reinforcement_learning_via_sequence_modeling"
        },
        {
            "paper_title": "Transdreamer: Reinforcement learning with transformer world models",
            "rating": 2,
            "sanitized_title": "transdreamer_reinforcement_learning_with_transformer_world_models"
        }
    ],
    "cost": 0.0225205,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Published as a conference paper at ICLR 2023 TRANSFORMERS ARE SAMPLE-EFFICIENT WORLD MODELS</p>
<p>Vincent Micheli 
University of Geneva
University of Geneva
University of Geneva</p>
<p>Eloi Alonso 
University of Geneva
University of Geneva
University of Geneva</p>
<p>François Fleuret 
University of Geneva
University of Geneva
University of Geneva</p>
<p>Published as a conference paper at ICLR 2023 TRANSFORMERS ARE SAMPLE-EFFICIENT WORLD MODELS</p>
<p>Deep reinforcement learning agents are notoriously sample inefficient, which considerably limits their application to real-world problems. Recently, many model-based methods have been designed to address this issue, with learning in the imagination of a world model being one of the most prominent approaches. However, while virtually unlimited interaction with a simulated environment sounds appealing, the world model has to be accurate over extended periods of time. Motivated by the success of Transformers in sequence modeling tasks, we introduce IRIS, a data-efficient agent that learns in a world model composed of a discrete autoencoder and an autoregressive Transformer. With the equivalent of only two hours of gameplay in the Atari 100k benchmark, IRIS achieves a mean human normalized score of 1.046, and outperforms humans on 10 out of 26 games, setting a new state of the art for methods without lookahead search. To foster future research on Transformers and world models for sample-efficient reinforcement learning, we release our code and models at https://github.com/eloialonso/iris. , et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015. Silver. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839): 604-609, 2020.</p>
<p>INTRODUCTION</p>
<p>Deep Reinforcement Learning (RL) has become the dominant paradigm for developing competent agents in challenging environments. Most notably, deep RL algorithms have achieved impressive performance in a multitude of arcade (Mnih et al., 2015;Schrittwieser et al., 2020;Hafner et al., 2021), real-time strategy (Vinyals et al., 2019;Berner et al., 2019), board (Silver et al., 2016;Schrittwieser et al., 2020) and imperfect information (Schmid et al., 2021;Brown et al., 2020a) games. However, a common drawback of these methods is their extremely low sample efficiency. Indeed, experience requirements range from months of gameplay for DreamerV2 (Hafner et al., 2021) in Atari 2600 games (Bellemare et al., 2013b) to thousands of years for OpenAI Five in Dota2 (Berner et al., 2019). While some environments can be sped up for training agents, real-world applications often cannot. Besides, additional cost or safety considerations related to the number of environmental interactions may arise (Yampolskiy, 2018). Hence, sample efficiency is a necessary condition to bridge the gap between research and the deployment of deep RL agents in the wild.</p>
<p>Model-based methods (Sutton &amp; Barto, 2018) constitute a promising direction towards data efficiency. Recently, world models were leveraged in several ways: pure representation learning (Schwarzer et al., 2021), lookahead search (Schrittwieser et al., 2020;Ye et al., 2021), and learning in imagination (Ha &amp; Schmidhuber, 2018;Kaiser et al., 2020;Hafner et al., 2020;. The latter approach is particularly appealing because training an agent inside a world model frees it from sample efficiency constraints. Nevertheless, this framework relies heavily on accurate world models since the policy is purely trained in imagination. In a pioneering work, Ha &amp; Schmidhuber (2018) successfully built imagination-based agents in toy environments. SimPLe recently showed promise in the more challenging Atari 100k benchmark (Kaiser et al., 2020). Currently, the best Atari agent learning in imagination is DreamerV2 (Hafner et al., 2021), although it was developed and evaluated with two hundred million frames available, far from the sample-efficient regime. Therefore, designing new world model architectures, capable of handling visually complex and partially observable environments with few samples, is key to realize their potential as surrogate training grounds.</p>
<p>The Transformer architecture (Vaswani et al., 2017) is now ubiquitous in Natural Language Processing (Devlin et al., 2019;Radford et al., 2019;Brown et al., 2020b;Raffel et al., 2020), and is also gaining traction in Computer Vision (Dosovitskiy et al., 2021;He et al., 2022), as well as in Offline The green arrows correspond to the encoder E and the decoder D of a discrete autoencoder, whose task is to represent frames in its learnt symbolic language. The backbone G of the world model is a GPT-like Transformer, illustrated with blue arrows. For each action that the policy π takes, G simulates the environment dynamics, by autoregressively unfolding new frame tokens that D can decode. G also predicts a reward and a potential episode termination. More specifically, an initial frame x 0 is encoded with E into tokens z 0 = (z 1 0 , . . . , z K 0 ) = E(x 0 ). The decoder D reconstructs an imagex 0 = D(z 0 ), from which the policy π predicts the action a 0 . From z 0 and a 0 , G predicts the rewardr 0 , episode termination d 0 ∈ {0, 1}, and in an autoregressive mannerẑ 1 = (ẑ 1 1 , . . . ,ẑ K 1 ), the tokens for the next frame. A dashed box indicates image tokens for a given time step, whereas a solid box represents the input sequence of G, i.e. (z 0 , a 0 ) at t = 0, (z 0 , a 0 ,ẑ 1 , a 1 ) at t = 1, etc. The policy π is purely trained with imagined trajectories, and is only deployed in the real environment to improve the world model (E, D, G).</p>
<p>Reinforcement Learning (Janner et al., 2021;Chen et al., 2021). In particular, the GPT (Radford et al., 2018;Brown et al., 2020b) family of models delivered impressive results in language understanding tasks. Similarly to world models, these attention-based models are trained with highdimensional signals and a self-supervised learning objective, thus constituting ideal candidates to simulate an environment.</p>
<p>Transformers particularly shine when they operate over sequences of discrete tokens (Devlin et al., 2019;Brown et al., 2020b). For textual data, there are simple ways (Schuster &amp; Nakajima, 2012;Kudo &amp; Richardson, 2018) to build a vocabulary, but this conversion is not straightforward with images. A naive approach would consist in treating pixels as image tokens, but standard Transformer architectures scale quadratically with sequence length, making this idea computationally intractable. To address this issue, VQGAN (Esser et al., 2021) and DALL-E (Ramesh et al., 2021) employ a discrete autoencoder (Van Den Oord et al., 2017) as a mapping from raw pixels to a much smaller amount of image tokens. Combined with an autoregressive Transformer, these methods demonstrate strong unconditional and conditional image generation capabilities. Such results suggest a new approach to design world models.</p>
<p>In the present work, we introduce IRIS (Imagination with auto-Regression over an Inner Speech), an agent trained in the imagination of a world model composed of a discrete autoencoder and an autoregressive Transformer. IRIS learns behaviors by accurately simulating millions of trajectories. Our approach casts dynamics learning as a sequence modeling problem, where an autoencoder builds a language of image tokens and a Transformer composes that language over time. With minimal tuning, IRIS outperforms a line of recent methods (Kaiser et al., 2020;Hessel et al., 2018;Laskin et al., 2020;Yarats et al., 2021;Schwarzer et al., 2021) for sample-efficient RL in the Atari 100k benchmark (Kaiser et al., 2020). After only two hours of real-time experience, it achieves a mean human normalized score of 1.046, and reaches superhuman performance on 10 out of 26 games. We describe IRIS in Section 2 and present our results in Section 3. Figure 2: Four imagined trajectories in KungFuMaster. We use the same conditioning frame across the four rows, in green, and let the world model imagine the rest. As the initial frame only contains the player, there is no information about the enemies that will come next. Consequently, the world model generates different types and numbers of opponents in each simulation. It is also able to reflect an essential game mechanic, highlighted in the blue box, where the first enemy disappears after getting hit by the player.</p>
<p>METHOD</p>
<p>We formulate the problem as a Partially Observable Markov Decision Process (POMDP) with image observations x t ∈ R h×w×3 , discrete actions a t ∈ {1, . . . , A}, scalar rewards r t ∈ R, episode termination d t ∈ {0, 1}, discount factor γ ∈ (0, 1), initial observation distribution ρ 0 , and environment dynamics x t+1 , r t , d t ∼ p(x t+1 , r t , d t | x ≤t , a ≤t ). The reinforcement learning objective is to train a policy π that yields actions maximizing the expected sum of rewards E π [ t≥0 γ t r t ].</p>
<p>Our method relies on the three standard components to learn in imagination (Sutton &amp; Barto, 2018): experience collection, world model learning, and behavior learning. In the vein of Ha &amp; Schmidhuber (2018);Kaiser et al. (2020);Hafner et al. (2020;, our agent learns to act exclusively within its world model, and we only make use of real experience to learn the environment dynamics.</p>
<p>We repeatedly perform the three following steps:</p>
<p>• collect_experience: gather experience in the real environment with the current policy.</p>
<p>• update_world_model: improve rewards, episode ends and next observations predictions.</p>
<p>• update_behavior: in imagination, improve the policy and value functions.</p>
<p>The world model is composed of a discrete autoencoder (Van Den Oord et al., 2017), to convert an image to tokens and back, and a GPT-like autoregressive Transformer (Vaswani et al., 2017;Radford et al., 2019;Brown et al., 2020b), whose task is to capture environment dynamics. Figure 1 illustrates the interplay between the policy and these two components during imagination. We first describe the autoencoder and the Transformer in Sections 2.1 and 2.2, respectively. Section 2.3 then details the procedure to learn the policy and value functions in imagination. Appendix A provides a comprehensive description of model architectures and hyperparameters. Algorithm 1 summarizes the training protocol.</p>
<p>FROM IMAGE OBSERVATIONS TO TOKENS</p>
<p>The discrete autoencoder (E, D) learns a symbolic language of its own to represent high-dimensional images as a small number of tokens. The back and forth between frames and tokens is illustrated with green arrows in Figure 1. The top row displays a test trajectory collected in the real environment. The bottom row depicts the reenactment of that trajectory inside the world model. More precisely, we condition the world model with the first two frames of the true sequence, in green. We then sequentially feed it the true actions and let it imagine the subsequent frames. After only 120 games of training, the world model perfectly simulates the ball's trajectory and players' movements. Notably, it also captures the game mechanic of updating the scoreboard after winning an exchange, as shown in the blue box.</p>
<p>More precisely, the encoder E : R h×w×3 → {1, . . . , N } K converts an input image x t into K tokens from a vocabulary of size N . Let E = {e i } N i=1 ∈ R N ×d be the corresponding embedding table of d-dimensional vectors. The input image x t is first passed through a Convolutional Neural Network (CNN) (LeCun et al., 1989) producing output y t ∈ R K×d . We then obtain the output tokens z t = (z 1 t , . . . , z K t ) ∈ {1, . . . , N } K as z k t = argmin i y k t − e i 2 , the index of the closest embedding vector in E (Van Den Oord et al., 2017;Esser et al., 2021). Conversely, the CNN decoder D : {1, . . . , N } K → R h×w×3 turns K tokens back into an image.</p>
<p>This discrete autoencoder is trained on previously collected frames, with an equally weighted combination of a L 1 reconstruction loss, a commitment loss (Van Den Oord et al., 2017;Esser et al., 2021), and a perceptual loss (Esser et al., 2021;Johnson et al., 2016;Larsen et al., 2016). We use a straight-through estimator (Bengio et al., 2013) to enable backpropagation training.</p>
<p>MODELING DYNAMICS</p>
<p>At a high level, the Transformer G captures the environment dynamics by modeling the language of the discrete autoencoder over time. Its central role of unfolding imagination is highlighted with the blue arrows in Figure 1.</p>
<p>Specifically, G operates over sequences of interleaved frame and action tokens. An input sequence (z 1 0 , . . . , z K 0 , a 0 , z 1 1 , . . . , z K 1 , a 1 , . . . , z 1 t , . . . , z K t , a t ) is obtained from the raw sequence (x 0 , a 0 , x 1 , a 1 , . . . , x t , a t ) by encoding the frames with E, as described in Section 2.1.</p>
<p>At each time step t, the Transformer models the three following distributions:
Transition:ẑ t+1 ∼ p G ẑ t+1 | z ≤t , a ≤t withẑ k t+1 ∼ p G ẑ k t+1 | z ≤t , a ≤t , z &lt;k t+1 (1) Reward:r t ∼ p G r t | z ≤t , a ≤t(2)Termination:d t ∼ p G d t | z ≤t , a ≤t(3)
Note that the conditioning for the k-th token also includes z &lt;k t+1 := (z 1 t+1 , . . . , z k−1 t+1 ), the tokens that were already predicted, i.e. the autoregressive process happens at the token level. We train G in a self-supervised manner on segments of L time steps, sampled from past experience. We use a cross-entropy loss for the transition and termination predictors, and a mean-squared error loss or a cross-entropy loss for the reward predictor, depending on the reward function.</p>
<p>LEARNING IN IMAGINATION</p>
<p>Together, the discrete autoencoder (E, D) and the Transformer G form a world model, capable of imagination. The policy π, depicted with purple arrows in Figure 1, exclusively learns in this imagination MDP. Figure 4: Imagining rewards and episode ends in Breakout (top) and Gopher (bottom). Each row depicts an imagined trajectory initialized with a single frame from the real environment. Yellow boxes indicate frames where the world model predicts a positive reward. In Breakout, it captures that breaking a brick yields rewards, and the brick is correctly removed from the following frames. In Gopher, the player has to protect the carrots from rodents. The world model successfully internalizes that plugging a hole or killing an enemy leads to rewards. Predicted episode terminations are highlighted with red boxes. The world model accurately reflects that missing the ball in Breakout, or letting an enemy reach the carrots in Gopher, will result in the end of an episode.</p>
<p>At time step t, the policy observes a reconstructed image observationx t and samples action a t ∼ π(a t |x ≤t ). The world model then predicts the rewardr t , the episode endd t , and the next observation
x t+1 = D(ẑ t+1 ), withẑ t+1 ∼ p G (ẑ t+1 | z 0 , a 0 ,ẑ 1 , a 1 , . . . ,ẑ t , a t )
. This imagination procedure is initialized with a real observation x 0 sampled from past experience, and is rolled out for H steps, the imagination horizon hyperparameter. We stop if an episode end is predicted before reaching the horizon. Figure 1 illustrates the imagination procedure.</p>
<p>As we roll out imagination for a fixed number of steps, we cannot simply use a Monte Carlo estimate for the expected return. Hence, to bootstrap the rewards that the agent would get beyond a given time step, we have a value network V that estimates V (x t ) E π τ ≥t γ τ −tr τ . Many actor-critic methods could be employed to train π and V in imagination (Sutton &amp; Barto, 2018;Kaiser et al., 2020;Hafner et al., 2020). For the sake of simplicity, we opt for the learning objectives and hyperparameters of DreamerV2 (Hafner et al., 2021), that delivered strong performance in Atari games. Appendix B gives a detailed breakdown of the reinforcement learning objectives.</p>
<p>EXPERIMENTS</p>
<p>Sample-efficient reinforcement learning is a growing field with multiple benchmarks in complex visual environments (Hafner, 2022;Kanervisto et al., 2022). In this work, we focus on the well established Atari 100k benchmark (Kaiser et al., 2020). We present the benchmark and its baselines in Section 3.1. We describe the evaluation protocol and discuss the results in Section 3.2. Qualitative examples of the world model's capabilities are given in Section 3.3. Table 1: Returns on the 26 games of Atari 100k after 2 hours of real-time experience, and humannormalized aggregate metrics. Bold numbers indicate the top methods without lookahead search while underlined numbers specify the overall best methods. IRIS outperforms learning-only methods in terms of number of superhuman games, mean, interquartile mean (IQM), and optimality gap. We make a distinction between methods with and without lookahead search. Indeed, algorithms relying on search at decision time (Silver et al., 2016;Schrittwieser et al., 2020) can vastly improve agent performance, but they come at a premium in computational resources and code complexity. MuZero (Schrittwieser et al., 2020) and EfficientZero (Ye et al., 2021) </p>
<p>RESULTS</p>
<p>The human normalized score is the established measure of performance in Atari 100k. It is defined as score_agent−score_random score_human−score_random , where score_random comes from a random policy, and score_human is obtained from human players (Wang et al., 2016).  Agarwal et al. (2021) discuss the limitations of mean and median scores, and show that substantial discrepancies arise between standard point estimates and interval estimates in RL benchmarks. Following their recommendations, we summarize in Figure 5 the human normalized scores with stratified bootstrap confidence intervals for mean, median, and interquartile mean (IQM). For finer comparisons, we also provide performance profiles and probabilities of improvement in Figure 6.</p>
<p>With the equivalent of only two hours of gameplay, IRIS achieves a superhuman mean score of 1.046 (+70%), an IQM of 0.501 (+49%), an optimality gap of 0.512 (+11%), and outperforms human players on 10 out of 26 games (+67%), where the relative improvements are computed with respect to SPR (Schwarzer et al., 2021). These results constitute a new state of the art for methods without lookahead search in the Atari 100k benchmark. We also note that IRIS outperforms MuZero, although the latter was not designed for the sample-efficient regime. Figure 7: Three consecutive levels in the games Frostbite (left) and Krull (right). In our experiments, the world model struggles to simulate subsequent levels in Frostbite, but not in Krull. Indeed, exiting the first level in Frostbite requires a long and unlikely sequence of actions to first build the igloo, and then go back to it from the bottom of the screen. Such rare events prevent the world model from internalizing new aspects of the game, which will therefore not be experienced by the policy in imagination. While Krull features more diverse levels, the world model successfully reflects this variety, and IRIS even sets a new state of the art in this environment. This is likely due to more frequent transitions from one stage to the next in Krull, resulting in a sufficient coverage of each level.</p>
<p>In addition, performance profiles ( Figure 6a) reveal that IRIS is on par with the strongest baselines for its bottom 50% of games, at which point it stochastically dominates (Agarwal et al., 2021;Dror et al., 2019) the other methods. Similarly, the probability of improvement is greater than 0.5 for all baselines (Figure 6b).</p>
<p>In terms of median score, IRIS overlaps with other methods ( Figure 5). Interestingly, Schwarzer et al.</p>
<p>(2021) note that the median is only influenced by a few decisive games, as evidenced by the width of the confidence intervals for median scores, even with 100 runs for DrQ, CURL and SPR.</p>
<p>We observe that IRIS is particularly strong in games that do not suffer from distributional shifts as the training progresses. Examples of such games include Pong, Breakout, and Boxing. On the contrary, the agent struggles when a new level or game mechanic is unlocked through an unlikely event. This sheds light on a double exploration problem. IRIS has to first discover a new aspect of the game for its world model to internalize it. Only then may the policy rediscover and exploit it. Figure 7 details this phenomenon in Frostbite and Krull, two games with multiple levels. In summary, as long as transitions between levels do not depend on low-probability events, the double exploration problem does not hinder performance.</p>
<p>Another kind of games difficult to simulate are visually challenging environments where capturing small details is important. As discussed in Appendix E, increasing the number of tokens to encode frames improves performance, albeit at the cost of increased computation.</p>
<p>WORLD MODEL ANALYSIS</p>
<p>As IRIS learns behaviors entirely in its imagination, the quality of the world model is the cornerstone of our approach. For instance, it is key that the discrete autoencoder correctly reconstructs elements like a ball, a player, or an enemy. Similarly, the potential inability of the Transformer to capture important game mechanics, like reward attribution or episode termination, can severely hamper the agent's performance. Hence, no matter the amount of imagined trajectories, the agent will learn suboptimal policies if the world model is flawed.</p>
<p>While Section 3.2 provides a quantitative evaluation, we aim to complement the analysis with qualitative examples of the abilities of the world model. Figure 2 shows the generation of many plausible futures in the face of uncertainty. Figure 3 depicts pixel-perfect predictions in Pong. Finally, we illustrate in Figure 4 predictions for rewards and episode terminations, which are crucial to the reinforcement learning objective.</p>
<p>RELATED WORK LEARNING IN THE IMAGINATION OF WORLD MODELS</p>
<p>The idea of training policies in a learnt model of the world was first investigated in tabular environments (Sutton &amp; Barto, 2018 (Esser et al., 2021) and DALL-E (Ramesh et al., 2021) use discrete autoencoders to compress a frame into a small sequence of tokens, that a transformer can then model autoregressively. Other works extend the approach to video generation. GODIVA (Wu et al., 2021) models sequences of frames instead of a single frame for text conditional video generation. VideoGPT (Yan et al., 2021) introduces video-level discrete autoencoders, and Transformers with spatial and temporal attention patterns, for unconditional and action conditional video generation.</p>
<p>CONCLUSION</p>
<p>We introduced IRIS, an agent that learns purely in the imagination of a world model composed of a discrete autoencoder and an autoregressive Transformer. IRIS sets a new state of the art in the Atari 100k benchmark for methods without lookahead search. We showed that its world model acquires a deep understanding of game mechanics, resulting in pixel perfect predictions in some games. We also illustrated the generative capabilities of the world model, providing a rich gameplay experience when training in imagination. Ultimately, with minimal tuning compared to existing battle-hardened agents, IRIS opens a new path towards efficiently solving complex environments.</p>
<p>In the future, IRIS could be scaled up to computationally demanding and challenging tasks that would benefit from the speed of its world model. Besides, its policy currently learns from reconstructed frames, but it could probably leverage the internal representations of the world model. Another exciting avenue of research would be to combine learning in imagination with MCTS. Indeed, both approaches deliver impressive results, and their contributions to agent performance might be complementary.</p>
<p>REPRODUCIBILITY STATEMENT</p>
<p>The different components and their training objectives are introduced in Section 2 and Appendix B. We describe model architectures and list hyperparameters in Appendix A. We specify the resources used to produce our results in Appendix G. Algorithm 1 makes explicit the interplay between components in the training loop. In Section 3.2, we provide the source of the reported results for the baselines, as well as the evaluation protocol.</p>
<p>The code is open-source to ensure reproducible results and foster future research. Minimal dependencies are required to run the codebase and we provide a thorough user guide to get started. Training and evaluation can be launched with simple commands, customization is possible with configuration files, and we include scripts to visualize agents playing and let users interact with the world model.</p>
<p>ETHICS STATEMENT</p>
<p>The development of autonomous agents for real-world environments raises many safety and environmental concerns. During its training period, an agent may cause serious harm to individuals and damage its surroundings. It is our belief that learning in the imagination of world models greatly reduces the risks associated with training new autonomous agents. Indeed, in this work, we propose a world model architecture capable of accurately modeling environments with very few samples. However, in a future line of research, one could go one step further and leverage existing data to eliminate the necessity of interacting with the real world. </p>
<p>ACKNOWLEDGMENTS</p>
<p>A MODELS AND HYPERPARAMETERS</p>
<p>A.1 DISCRETE AUTOENCODER Our discrete autoencoder is based on the implementation of VQGAN (Esser et al., 2021). We removed the discriminator, essentially turning the VQGAN into a vanilla VQVAE (Van Den Oord et al., 2017) with an additional perceptual loss (Johnson et al., 2016;Larsen et al., 2016).</p>
<p>The training objective is the following:
L(E, D, E) = x − D(z) 1 + sg(E(x)) − E(z) 2 2 + sg(E(z)) − E(x) 2 2 + L perceptual (x, D(z))
Here, the first term is the reconstruction loss, the next two terms constitute the commitment loss (where sg(·) is the stop-gradient operator), and the last term is the perceptual loss.  Note that during experience collection in the real environment, frames still go through the autoencoder to keep the input distribution of the policy unchanged. See Algorithm 1 for details.</p>
<p>A.2 TRANSFORMER</p>
<p>Our autoregressive Transformer is based on the implementation of minGPT (Karpathy, 2020). It takes as input a sequence of L(K + 1) tokens and embeds it into a L(K + 1) × D tensor using an A × D embedding table for actions, and a N × D embedding table for frames tokens. This tensor is forwarded through M Transformer blocks. We use GPT2-like blocks (Radford et al., 2019), i.e. each block consists of a self-attention module with layer normalization of the input, wrapped with a residual connection, followed by a per-position multi-layer perceptron with layer normalization of the input, wrapped with another residual connection. </p>
<p>A.3 ACTOR-CRITIC</p>
<p>The weights of the actor and critic are shared except for the last layer. The actor-critic takes as input a 64 × 64 × 3 frame, and forwards it through a convolutional block followed by an LSTM cell (Mnih et al., 2016;Hochreiter &amp; Schmidhuber, 1997;Gers et al., 2000). The convolutional block consists of the same layer repeated four times: a 3x3 convolution with stride 1 and padding 1, a ReLU activation, and 2x2 max-pooling with stride 2. The dimension of the LSTM hidden state is 512. Before starting the imagination procedure from a given frame, we burn-in (Kapturowski et al., 2019) the 20 previous frames to initialize the hidden state. </p>
<p>B ACTOR-CRITIC LEARNING OBJECTIVES</p>
<p>We follow Dreamer (Hafner et al., 2020; in using the generic λ-return, that balances bias and variance, as the regression target for the value network. Given an imagined trajectory (x 0 , a 0 ,r 0 ,d 0 , . . . ,x H−1 , a H−1 ,r H−1 ,d H−1 ,x H ), the λ-return can be defined recursively as follows:
Λ t = r t + γ(1 −d t ) (1 − λ)V (x t+1 ) + λΛ t+1 if t &lt; H V (x H ) if t = H(4)
The value network V is trained to minimize L V , the expected squared difference with λ-returns over imagined trajectories.
L V = E π H−1 t=0 V (x t ) − sg(Λ t ) 2(5)
Here, sg(·) denotes the gradient stopping operation, meaning that the target is a constant in the gradient-based optimization, as classically established in the literature (Mnih et al., 2015;Hessel et al., 2018;Hafner et al., 2020).</p>
<p>As large amounts of trajectories are generated in the imagination MDP, we can use a straightforward reinforcement learning objective for the policy, such as REINFORCE (Sutton &amp; Barto, 2018). To reduce the variance of REINFORCE gradients, we use the value V (x t ) as a baseline (Sutton &amp; Barto, 2018). We also add a weighted entropy maximization objective to maintain a sufficient exploration. The actor is trained to minimize the following REINFORCE objective over imagined trajectories:
L π = −E π H−1 t=0
log(π(a t |x ≤t )) sg(Λ t − V (x t )) + η H(π(a t |x ≤t )) (6) x 0 ← env.reset() for t = 0 to n − 1 dô x t ← D(E(x t )) // forward frame through discrete autoencoder
Sample a t ∼ π(a t |x t ) x t+1 , r t , d t ← env.step(a t ) if d t = 1 then x t+1 ← env.reset() D ← D ∪ {x t , a t , r t , d t } n−1 t=0 Procedure update_world_model(): Sample {x t , a t , r t , d t } τ +L−1 t=τ ∼ D Compute z t := E(x t ) andx t := D(z t ) for t = τ, . . . , τ + L − 1 Update E and D Compute p G (ẑ t+1 ,r t ,d t | z τ , a τ , . . . , z t , a t ) for t = τ, . . . , τ + L − 1 Update G Procedure update_behavior(): Sample x 0 ∼ D z 0 ← E(x 0 ) x 0 ← D(z 0 ) for t = 0 to H − 1 do Sample a t ∼ π(a t |x t ) Sampleẑ t+1 ,r t ,d t ∼ p G (ẑ t+1 ,r t ,d t | z 0 , a 0 , . . . ,ẑ t , a t ) x t+1 ← D(ẑ t+1 )
Compute V (x t ) for t = 0, . . . , H Update π and V</p>
<p>E AUTOENCONDING FRAMES WITH VARYING AMOUNTS OF TOKENS</p>
<p>The sequence length of the Transformer is determined by the number of tokens used to encode a single frame and the number of timesteps in memory. Increasing the number of tokens per frame results in better reconstructions, although it requires more compute and memory.</p>
<p>This tradeoff is particularly important in visually challenging games with a high number of possible configurations, where the discrete autoencoder struggles to properly encode frames with only 16 tokens. For instance, Figure 9 shows that, when increasing the number of tokens per frame to 64 in Alien, the discrete autoencoder correctly reconstructs the player, its enemies, and rewards. Figure 9: Tradeoff between the number of tokens per frame and reconstructions quality in Alien. Each column displays a 64 × 64 frame from the real environment (top), its reconstruction with a discrete encoding of 16 tokens (center), and its reconstruction with a discrete encoding of 64 tokens (bottom). In Alien, the player is the dark blue character, and the enemies are the large colored sprites. With 16 tokens per frame, the autoencoder often erases the player, switches colors, and misplaces rewards. When increasing the amount of tokens, it properly reconstructs the frame. Table 7 displays the final performance of IRIS trained with 64 tokens per frame in three games. Interestingly, even though the world model is more accurate, the performance in Alien only increases marginally (+36%). This observation suggests that Alien poses a hard reinforcement learning problem, as evidenced by the low performance of other baselines in that game. On the contrary, IRIS greatly benefits from having more tokens per frame for Asterix (+121%) and BankHeist (+432%). F BEYOND THE SAMPLE-EFFICIENT SETTING IRIS can be scaled up by increasing the number of tokens used to encode frames, adding capacity to the model, taking more optimization steps per environment steps, or using more data. In this experiment, we investigate data scaling properties by increasing the number of environment steps from 100k to 10M. However, to maintain a training time within our computational resources, we lower the ratio of optimization steps per environment steps from 1:1 to 1:50. As a consequence, the results of this experiment at 100k frames would be worse than those reported in the paper.  Table 8 illustrates that increasing the number of environment steps from 100k to 10M drastically improves performance for most games, providing evidence that IRIS could be scaled up beyond the sample-efficient regime. On some games, more data only yields marginal improvements, most likely due to hard exploration problems or visually challenging domains that would benefit from a higher number of tokens to encode frames (Appendix E).</p>
<p>G COMPUTATIONAL RESOURCES</p>
<p>For each Atari environment, we repeatedly trained IRIS with 5 different random seeds. We ran our experiments with 8 Nvidia A100 40GB GPUs. With two Atari environments running on the same GPU, training takes around 7 days, resulting in an average of 3.5 days per environment.</p>
<p>SimPLe (Kaiser et al., 2020), the only baseline that involves learning in imagination, trains for 3 weeks with a P100 GPU on a single environment. As for SPR (Schwarzer et al., 2021), the strongest baseline without lookahead search, it trains notably fast in 4.6 hours with a P100 GPU.</p>
<p>Regarding baselines with lookahead search, MuZero (Schrittwieser et al., 2020) originally used 40 TPUs for 12 hours to train in a single Atari environment. Ye et al. (2021) train both EfficientZero and their reimplementation of MuZero in 7 hours with 4 RTX 3090 GPUs. EfficientZero's implementation relies on a distributed infrastructure with CPU and GPU threads running in parallel, and a C++/Cython implementation of MCTS. By contrast, IRIS and the baselines without lookahead search rely on straightforward single GPU / single CPU implementations.</p>
<p>H EXPLORATION IN FREEWAY</p>
<p>The reward function in Freeway is sparse since the agent is only rewarded when it completely crosses the road. In addition, bumping into cars will drag it down, preventing it from smoothly ascending the highway. This poses an exploration problem for newly initialized agents because a random policy will almost surely never obtain a non-zero reward with a 100k frames budget. Figure 10: A game of Freeway. Cars will bump the player down, making it very unlikely to cross the road and be rewarded for random policies.</p>
<p>The solution to this problem is actually straightforward and simply requires stretches of time when the UP action is oversampled. Most Atari 100k baselines fix the issue with epsilon-greedy schedules and argmax action selection, where at some point the network configuration will be such that the UP action is heavily favored. In this work, we opted for the simpler strategy of having a fixed epsilon-greedy parameter and sampling from the policy. However, we lowered the sampling temperature from 1 to 0.01 for Freeway, in order to avoid random walks that would not be conducive to learning in the early stages of training. As a consequence, once it received its first few rewards through exploration, IRIS was able to internalize the sparse reward function in its world model.</p>
<p>Figure 1 :
1Unrolling imagination over time. This figure shows the policy π, depicted with purple arrows, taking a sequence of actions in imagination.</p>
<p>Figure 3 :
3Pixel perfect predictions in Pong.</p>
<p>Figure 5 :
5Mean, median, and interquartile mean human normalized scores, computed with stratified bootstrap confidence intervals. 5 runs for IRIS and SimPLe, 100 runs for SPR, CURL, and DrQ(Agarwal et al., 2021). Probabilities of improvement, i.e. how likely it is for IRIS to outperform baselines on any game.</p>
<p>Figure 6 :
6Performance profiles (left) and probabilities of improvement (right)(Agarwal et al., 2021).</p>
<p>Atari 100k consists of 26 Atari games(Bellemare et al., 2013a) with various mechanics, evaluating a wide range of agent capabilities. In this benchmark, an agent is only allowed 100k actions in each environment. This constraint is roughly equivalent to 2 hours of human gameplay. By way of comparison, unconstrained Atari agents are usually trained for 50 million steps, a 500 fold increase in experience.Multiple baselines were compared on the Atari 100k benchmark. SimPLe (Kaiser et al., 2020) trains a policy with PPO (Schulman et al., 2017) in a video generation model. CURL (Laskin et al., 2020) develops off-policy agents from high-level image features obtained with contrastive learning. DrQ (Yarats et al., 2021) augments input images and averages Q-value estimates over several transformations. SPR (Schwarzer et al., 2021) enforces consistent representations of input images across augmented views and neighbouring time steps. The aforementioned baselines carry additional techniques to improve performance, such as prioritized experience replay (Schaul et al., 2016), epsilon-greedy scheduling, or data augmentation.Lookahead search 
No lookahead search </p>
<p>Game 
Random 
Human MuZero EfficientZero SimPLe CURL 
DrQ 
SPR IRIS (ours) </p>
<p>Alien 
227.8 
7127.7 
530.0 
808.5 
616.9 
711.0 
865.2 
841.9 
420.0 
Amidar 
5.8 
1719.5 
38.8 
148.6 
74.3 
113.7 
137.8 
179.7 
143.0 
Assault 
222.4 
742.0 
500.1 
1263.1 
527.2 
500.9 
579.6 
565.6 
1524.4 
Asterix 
210.0 
8503.3 
1734.0 
25557.8 
1128.3 
567.2 
763.6 
962.5 
853.6 
BankHeist 
14.2 
753.1 
192.5 
351.0 
34.2 
65.3 
232.9 
345.4 
53.1 
BattleZone 
2360.0 37187.5 
7687.5 
13871.2 
4031.2 8997.8 10165.3 14834.1 
13074.0 
Boxing 
0.1 
12.1 
15.1 
52.7 
7.8 
0.9 
9.0 
35.7 
70.1 
Breakout 
1.7 
30.5 
48.0 
414.1 
16.4 
2.6 
19.8 
19.6 
83.7 
ChopperCommand 
811.0 
7387.8 
1350.0 
1117.3 
979.4 
783.5 
844.6 
946.3 
1565.0 
CrazyClimber 
10780.5 35829.4 56937.0 
83940.2 62583.6 9154.4 21539.0 36700.5 
59324.2 
DemonAttack 
152.1 
1971.0 
3527.0 
13003.9 
208.1 
646.5 
1321.5 
517.6 
2034.4 
Freeway 
0.0 
29.6 
21.8 
21.8 
16.7 
28.3 
20.3 
19.3 
31.1 
Frostbite 
65.2 
4334.7 
255.0 
296.3 
236.9 1226.5 
1014.2 
1170.7 
259.1 
Gopher 
257.6 
2412.5 
1256.0 
3260.3 
596.8 
400.9 
621.6 
660.6 
2236.1 
Hero 
1027.0 30826.4 
3095.0 
9315.9 
2656.6 4987.7 
4167.9 
5858.6 
7037.4 
Jamesbond 
29.0 
302.8 
87.5 
517.0 
100.5 
331.0 
349.1 
366.5 
462.7 
Kangaroo 
52.0 
3035.0 
62.5 
724.1 
51.2 
740.2 
1088.4 
3617.4 
838.2 
Krull 
1598.0 
2665.5 
4890.8 
5663.3 
2204.8 3049.2 
4402.1 
3681.6 
6616.4 
KungFuMaster 
258.5 22736.3 18813.0 
30944.8 14862.5 8155.6 11467.4 14783.2 
21759.8 
MsPacman 
307.3 
6951.6 
1265.6 
1281.2 
1480.0 1064.0 
1218.1 
1318.4 
999.1 
Pong 
-20.7 
14.6 
-6.7 
20.1 
12.8 
-18.5 
-9.1 
-5.4 
14.6 
PrivateEye 
24.9 69571.3 
56.3 
96.7 
35.0 
81.9 
3.5 
86.0 
100.0 
Qbert 
163.9 13455.0 
3952.0 
13781.9 
1288.8 
727.0 
1810.7 
866.3 
745.7 
RoadRunner 
11.5 
7845.0 
2500.0 
17751.3 
5640.6 5006.1 11211.4 12213.1 
9614.6 
Seaquest 
68.4 42054.7 
208.0 
1100.2 
683.3 
315.2 
352.3 
558.1 
661.3 
UpNDown 
533.4 11693.2 
2896.9 
17264.2 
3350.3 2646.4 
4324.5 10859.2 
3546.2 </p>
<h1>Superhuman (↑)</h1>
<p>0 
N/A 
5 
14 
1 
2 
3 
6 
10 
Mean (↑) 
0.000 
1.000 
0.562 
1.943 
0.332 
0.261 
0.465 
0.616 
1.046 
Median (↑) 
0.000 
1.000 
0.227 
1.090 
0.134 
0.092 
0.313 
0.396 
0.289 
IQM (↑) 
0.000 
1.000 
N/A 
N/A 
0.130 
0.113 
0.280 
0.337 
0.501 
Optimality Gap (↓) 
1.000 
0.000 
N/A 
N/A 
0.729 
0.768 
0.631 
0.577 
0.512 </p>
<p>3.1 BENCHMARK AND BASELINES </p>
<p>are the current standard for search-based methods in Atari 100k. MuZero leverages Monte Carlo Tree Search (MCTS)(Kocsis &amp; Szepesvári, 2006;Coulom, 2007) as a policy improvement operator, by unrolling multiple hypothetical trajectories in the latent space of a world model. EfficientZero improves upon MuZero by introducing a self-supervised consistency loss, predicting returns over short horizons in one shot, and correcting off-policy trajectories with its world model.0.50 
0.75 
1.00 </p>
<p>SimPLe </p>
<p>CURL </p>
<p>DrQ </p>
<p>SPR </p>
<p>IRIS (ours) </p>
<p>Mean </p>
<p>0.1 
0.2 
0.3 
0.4 </p>
<p>Median </p>
<p>0.15 
0.30 
0.45 </p>
<p>Interquartile Mean </p>
<p>Human Normalized Score </p>
<p>Table 1
1displays returns across games and human-normalized aggregate metrics. For MuZero and 
EfficientZero, we report the averaged results published by Ye et al. (2021) (3 runs). We use results 
from the Atari 100k case study conducted by Agarwal et al. (2021) for the other baselines (100 new 
runs for CURL, DrQ, SPR, and 5 existing runs for SimPLe). Finally, we evaluate IRIS by computing 
an average over 100 episodes collected at the end of training for each game (5 runs). </p>
<p>make the observation that the standard Transformer architecture is difficult to optimize with RL objectives. The authors propose to replace residual connections by gating layers to stabilize the learning procedure. Our world model does not require such modifications, which is most likely due to its self-supervised learning objective. The Trajectory Transformer (Janner et al., 2021) and the Decision Transformer(Chen et al., 2021)  represent offline trajectories as a static dataset of sequences, and the Online Decision Transformer(Zheng et al., 2022) extends the latter to the online setting. The Trajectory Transformer is trained to predict future returns, states and actions. At inference time, it can thus plan for the optimal action with a reward-driven beam search, yet the approach is limited to low-dimensional states. On the contrary, Decision Transformers can handle image inputs but cannot be easily extended as world models.Ozair et al. (2021)  introduce an offline variant of MuZero(Schrittwieser et al., 2020)  capable of handling stochastic environments by performing an hybrid search with a Transformer over both actions and trajectory-level discrete latent variables.VIDEO GENERATION WITH DISCRETE AUTOENCODERS AND TRANSFORMERS). Ha &amp; Schmidhuber (2018) showed that simple visual environments 
could be simulated with autoencoders and recurrent networks. SimPLe (Kaiser et al., 2020) demon-
strated that a PPO policy (Schulman et al., 2017) trained in a video prediction model outperformed 
humans in some Atari games. Improving upon Dreamer (Hafner et al., 2020), DreamerV2 (Hafner 
et al., 2021) was the first agent learning in imagination to achieve human-level performance in the 
Atari 50M benchmark. Its world model combines a convolutional autoencoder with a recurrent 
state-space model (RSSM) (Hafner et al., 2019) for latent dynamics learning. More recently, Chen 
et al. (2022) explored a variant of DreamerV2 where a Transformer replaces the recurrent network in 
the RSSM and Seo et al. (2022) enhance DreamerV2 in the setting where an offline dataset of videos 
is available for pretraining. </p>
<p>REINFORCEMENT LEARNING WITH TRANSFORMERS </p>
<p>Following spectacular advances in natural language processing (Manning &amp; Goldie, 2022), the 
reinforcement learning community has recently stepped into the realm of Transformers. Parisotto 
et al. (2020) VQGAN </p>
<p>We would like to thank Maxim Peter, Bálint Máté, Daniele Paliotta, Atul Sinha, and Alexandre Dupuis for insightful discussions and comments. Vincent Micheli was supported by the Swiss National Science Foundation under grant number FNS-187494. Danijar Hafner. Benchmarking the spectrum of agent capabilities. In International Conference on Learning Representations, 2022. Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In International conference on machine learning, pp. 2555-2565. PMLR, 2019.Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning 
behaviors by latent imagination. In International Conference on Learning Representations, 2020. </p>
<p>Danijar Hafner, Timothy P Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with 
discrete world models. In International Conference on Learning Representations, 2021. </p>
<p>Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked 
autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on 
Computer Vision and Pattern Recognition, pp. 16000-16009, 2022. </p>
<p>Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan 
Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in 
deep reinforcement learning. In Thirty-second AAAI conference on artificial intelligence, 2018. </p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9(8): 
1735-1780, 1997. </p>
<p>Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence 
modeling problem. Advances in neural information processing systems, 34, 2021. </p>
<p>Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and 
super-resolution. In European conference on computer vision, pp. 694-711. Springer, 2016. </p>
<p>Łukasz Kaiser, Mohammad Babaeizadeh, Piotr Miłos, Błażej Osiński, Roy H Campbell, Konrad 
Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model based 
reinforcement learning for atari. In International Conference on Learning Representations, 2020. </p>
<p>Anssi Kanervisto, Stephanie Milani, Karolis Ramanauskas, Nicholay Topin, Zichuan Lin, Junyou Li, 
Jianing Shi, Deheng Ye, Qiang Fu, Wei Yang, Weijun Hong, Zhongyue Huang, Haicheng Chen, 
Guangjun Zeng, Yue Lin, Vincent Micheli, Eloi Alonso, François Fleuret, Alexander Nikulin, Yury 
Belousov, Oleg Svidchenko, and Aleksei Shpilman. Minerl diamond 2021 competition: Overview, 
results, and lessons learned. In Proceedings of the NeurIPS 2021 Competitions and Demonstrations 
Track, Proceedings of Machine Learning Research, 2022. URL https://proceedings.mlr. 
press/v176/kanervisto22a.html. </p>
<p>Steven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney. Recurrent 
experience replay in distributed reinforcement learning. In International conference on learning 
representations, 2019. </p>
<p>Andrej Karpathy. minGPT: A minimal PyTorch re-implementation of the OpenAI GPT (Generative 
Pretrained Transformer) training, 2020. URL https://github.com/karpathy/minGPT. </p>
<p>Levente Kocsis and Csaba Szepesvári. Bandit based monte-carlo planning. In European conference 
on machine learning, 2006. </p>
<p>Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword 
tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on 
Empirical Methods in Natural Language Processing: System Demonstrations, pp. 66-71, 2018. </p>
<p>Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, and Ole Winther. Autoen-
coding beyond pixels using a learned similarity metric. In International conference on machine 
learning, pp. 1558-1566. PMLR, 2016. </p>
<p>Michael Laskin, Aravind Srinivas, and Pieter Abbeel. CURL: Contrastive unsupervised representa-
tions for reinforcement learning. In International Conference on Machine Learning, pp. 5639-5650. 
PMLR, 2020. </p>
<p>Table 2 :
2Encoder / Decoder hyperparameters. We list the hyperparameters for the encoder, the same 
ones apply for the decoder. </p>
<p>Hyperparameter 
Value </p>
<p>Frame dimensions (h, w) 
64 × 64 
Layers 
4 
Residual blocks per layer 
2 
Channels in convolutions 
64 
Self-attention layers at resolution 
8 / 16 </p>
<p>Table 3 :
3Embedding table hyperparameters.Hyperparameter 
Value </p>
<p>Vocabulary size (N) 
512 
Tokens per frame (K) 
16 
Token embedding dimension (d) 
512 </p>
<p>Table 4 :
4Transformer hyperparametersHyperparameter 
Value </p>
<p>Timesteps (L) 
20 
Embedding dimension (D) 
256 
Layers (M) 
10 
Attention heads 
4 
Weight decay 
0.01 
Embedding dropout 
0.1 
Attention dropout 
0.1 
Residual dropout 
0.1 </p>
<p>Table 5 :
5Training loop &amp; Shared hyperparametersHyperparameter 
Value </p>
<p>Epochs 
600 </p>
<h1>Collection epochs</h1>
<p>500 
Environment steps per epoch 
200 
Collection epsilon-greedy 
0.01 
Eval sampling temperature 
0.5 
Start autoencoder after epochs 
5 
Start transformer after epochs 
25 
Start actor-critic after epochs 
50 
Autoencoder batch size 
256 
Transformer batch size 
64 
Actor-critic batch size 
64 </p>
<p>Training steps per epoch 
200 
Learning rate 
1e-4 
Optimizer 
Adam 
Adam β 1 
0.9 
Adam β 2 
0.999 
Max gradient norm 
10.0 </p>
<p>Table 6 :
6RL training hyperparametersFigure 8: Optimality gap, lower is better. The amount by which the algorithm fails to reach a human-level score(Agarwal et al., 2021).Hyperparameter 
Value </p>
<p>Imagination horizon (H) 
20 
γ 
0.995 
λ 
0.95 
η 
0.001 </p>
<p>Table 7 :
7Returns on Alien, Asterix, and BankHeist with 64 tokens per frame instead of 16.Game 
Random Human SimPLe CURL 
DrQ 
SPR IRIS (16 tokens) IRIS (64 tokens) </p>
<p>Alien 
227.8 
7127.7 
616.9 
711.0 865.2 841.9 
420.0 
570.0 
Asterix 
210.0 
8503.3 
1128.3 
567.2 763.6 962.5 
853.6 
1890.4 
BankHeist 
14.2 
753.1 
34.2 
65.3 232.9 345.4 
53.1 
282.5 </p>
<p>Table 8 :
8Increasing the number of environment steps from 100k to 10M.Game 
Random 
Human IRIS (100k) IRIS (10M) </p>
<p>Alien 
227.8 
7127.7 
420.0 
1003.1 
Amidar 
5.8 
1719.5 
143.0 
213.4 
Assault 
222.4 
742.0 
1524.4 
9355.6 
Asterix 
210.0 
8503.3 
853.6 
6861.0 
BankHeist 
14.2 
753.1 
53.1 
921.6 
BattleZone 
2360.0 37187.5 
13074.0 
34562.5 
Boxing 
0.1 
12.1 
70.1 
98.0 
Breakout 
1.7 
30.5 
83.7 
493.9 
ChopperCommand 
811.0 
7387.8 
1565.0 
9814.0 
CrazyClimber 
10780.5 35829.4 
59324.2 
111068.8 
DemonAttack 
152.1 
1971.0 
2034.4 
96218.6 
Freeway 
0.0 
29.6 
31.1 
34.0 
Frostbite 
65.2 
4334.7 
259.1 
290.3 
Gopher 
257.6 
2412.5 
2236.1 
97370.6 
Hero 
1027.0 30826.4 
7037.4 
19212.0 
Jamesbond 
29.0 
302.8 
462.7 
5534.4 
Kangaroo 
52.0 
3035.0 
838.2 
1793.8 
Krull 
1598.0 
2665.5 
6616.4 
7344.0 
KungFuMaster 
258.5 22736.3 
21759.8 
39643.8 
MsPacman 
307.3 
6951.6 
999.1 
1233.0 
Pong 
-20.7 
14.6 
14.6 
21.0 
PrivateEye 
24.9 69571.3 
100.0 
100.0 
Qbert 
163.9 13455.0 
745.7 
4012.1 
RoadRunner 
11.5 
7845.0 
9614.6 
30609.4 
Seaquest 
68.4 42054.7 
661.3 
1815.0 
UpNDown 
533.4 11693.2 
3546.2 
114690.1 </p>
<h1>Superhuman (↑)</h1>
<p>0 
N/A 
10 
15 
Mean (↑) 
0.000 
1.000 
1.046 
7.488 
Median (↑) 
0.000 
1.000 
0.289 
1.207 
IQM (↑) 
0.000 
1.000 
0.501 
2.239 
Optimality Gap (↓) 
1.000 
0.000 
0.512 
0.282 </p>
<p>Deep reinforcement learning at the edge of the statistical precipice. Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, Marc Bellemare, Advances in neural information processing systems. 34Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare. Deep reinforcement learning at the edge of the statistical precipice. Advances in neural information processing systems, 34:29304-29320, 2021.</p>
<p>The arcade learning environment: An evaluation platform for general agents. Yavar Marc G Bellemare, Joel Naddaf, Michael Veness, Bowling, Journal of Artificial Intelligence Research. 47Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ- ment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47: 253-279, 2013a.</p>
<p>The arcade learning environment: An evaluation platform for general agents. Yavar Marc G Bellemare, Joel Naddaf, Michael Veness, Bowling, Journal of Artificial Intelligence Research. 47Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ- ment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47: 253-279, 2013b.</p>
<p>Estimating or propagating gradients through stochastic neurons for conditional computation. Yoshua Bengio, Nicholas Léonard, Aaron Courville, arXiv:1308.3432arXiv preprintYoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.</p>
<p>Dota 2 with large scale deep reinforcement learning. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław Dębiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, arXiv:1912.06680arXiv preprintChristopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław Dębiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.</p>
<p>Combining deep reinforcement learning and search for imperfect-information games. Noam Brown, Anton Bakhtin, Adam Lerer, Qucheng Gong, Advances in neural information processing systems. 33Noam Brown, Anton Bakhtin, Adam Lerer, and Qucheng Gong. Combining deep reinforcement learning and search for imperfect-information games. Advances in neural information processing systems, 33:17057-17069, 2020a.</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 33Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020b.</p>
<p>Transdreamer: Reinforcement learning with transformer world models. Chang Chen, Yi-Fu Wu, Jaesik Yoon, Sungjin Ahn, arXiv:2202.09481arXiv preprintChang Chen, Yi-Fu Wu, Jaesik Yoon, and Sungjin Ahn. Transdreamer: Reinforcement learning with transformer world models. arXiv preprint arXiv:2202.09481, 2022.</p>
<p>Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Advances in neural information processing systems. 342021Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34, 2021.</p>
<p>Computing "elo ratings" of move patterns in the game of go. Rémi Coulom, ICGA journal. 304Rémi Coulom. Computing "elo ratings" of move patterns in the game of go. ICGA journal, 30(4): 198-208, 2007.</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies1Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, International Conference on Learning Representations. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.</p>
<p>Deep dominance-how to properly compare deep neural models. Rotem Dror, Segev Shlomov, Roi Reichart, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsRotem Dror, Segev Shlomov, and Roi Reichart. Deep dominance-how to properly compare deep neural models. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 2773-2785, 2019.</p>
<p>Taming transformers for high-resolution image synthesis. Patrick Esser, Robin Rombach, Bjorn Ommer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionPatrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution im- age synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12873-12883, 2021.</p>
<p>Learning to forget: Continual prediction with LSTM. F A Gers, J Schmidhuber, F Cummins, Neural Computation. 1210F. A. Gers, J. Schmidhuber, and F. Cummins. Learning to forget: Continual prediction with LSTM. Neural Computation, 12(10):2451-2471, 2000.</p>
<p>Recurrent world models facilitate policy evolution. David Ha, Jürgen Schmidhuber, Advances in neural information processing systems. 31David Ha and Jürgen Schmidhuber. Recurrent world models facilitate policy evolution. Advances in neural information processing systems, 31, 2018.</p>
<p>Mastering the game of go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den, Julian Driessche, Ioannis Schrittwieser, Veda Antonoglou, Marc Panneershelvam, Lanctot, Nature. 5297587David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016.</p>
<p>A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Science. 3626419David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419): 1140-1144, 2018.</p>
<p>Reinforcement Learning: An Introduction. Richard S Sutton, Andrew G Barto, Bradford Book, Cambridge, MA, USARichard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. A Bradford Book, Cambridge, MA, USA, 2018.</p>
<p>Neural discrete representation learning. Advances in neural information processing systems. Aaron Van Den, Oriol Oord, Vinyals, 30Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017.</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 30Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.</p>
<p>Grandmaster level in StarCraft II using multi-agent reinforcement learning. Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, H David, Richard Choi, Timo Powell, Petko Ewalds, Georgiev, Nature. 5757782Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.</p>
<p>Dueling network architectures for deep reinforcement learning. Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, Nando Freitas, International conference on machine learning. PMLRZiyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling network architectures for deep reinforcement learning. In International conference on machine learning, pp. 1995-2003. PMLR, 2016.</p>
<p>Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, Nan Duan Godiva, arXiv:2104.14806Generating open-domain videos from natural descriptions. arXiv preprintChenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and Nan Duan. Godiva: Generating open-domain videos from natural descriptions. arXiv preprint arXiv:2104.14806, 2021.</p>
<p>. V Roman, Yampolskiy, Artificial Intelligence Safety and Security. Chapman &amp; Hall/CRC. Roman V Yampolskiy. Artificial Intelligence Safety and Security. Chapman &amp; Hall/CRC, 2018.</p>
<p>Videogpt: Video generation using vq-vae and transformers. Wilson Yan, Yunzhi Zhang, Pieter Abbeel, Aravind Srinivas, arXiv:2104.10157arXiv preprintWilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021.</p>
<p>Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. Denis Yarats, Ilya Kostrikov, Rob Fergus, International Conference on Learning Representations. Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. In International Conference on Learning Representations, 2021.</p>
<p>Mastering atari games with limited data. Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, Yang Gao, Advances in neural information processing systems. 342021Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. Mastering atari games with limited data. Advances in neural information processing systems, 34, 2021.</p>
<p>Online decision transformer. Qinqing Zheng, Amy Zhang, Aditya Grover, International Conference on Machine Learning. PMLRQinqing Zheng, Amy Zhang, and Aditya Grover. Online decision transformer. In International Conference on Machine Learning, pp. 27042-27059. PMLR, 2022.</p>            </div>
        </div>

    </div>
</body>
</html>
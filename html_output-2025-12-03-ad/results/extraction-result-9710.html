<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9710 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9710</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9710</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-276903104</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.05712v1.pdf" target="_blank">Automatic Evaluation Metrics for Artificially Generated Scientific Research</a></p>
                <p><strong>Paper Abstract:</strong> Foundation models are increasingly used in scientific research, but evaluating AI-generated scientific work remains challenging. While expert reviews are costly, large language models (LLMs) as proxy reviewers have proven to be unreliable. To address this, we investigate two automatic evaluation metrics, specifically citation count prediction and review score prediction. We parse all papers of OpenReview and augment each submission with its citation count, reference, and research hypothesis. Our findings reveal that citation count prediction is more viable than review score prediction, and predicting scores is more difficult purely from the research hypothesis than from the full paper. Furthermore, we show that a simple prediction model based solely on title and abstract outperforms LLM-based reviewers, though it still falls short of human-level consistency.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9710.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9710.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CitationCountPred</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Citation count prediction (log average citations per month)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic evaluation metric that predicts a paper's influence by estimating the logarithm of its average citations per month from paper content (title/abstract, sections, or hypothesis) using SPECTER2 embeddings and simple predictors (MLP / transformer encoder + MLP).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-3.5-turbo (used to annotate research hypotheses); target is LLM-generated scientific outputs in general</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>GPT-3.5-turbo (16k context window variant) was used in this work only to annotate research hypotheses from papers; the citation-prediction models are content-based predictors (SPECTER2 embeddings + MLP/transformer) applied to papers including those that could be generated by LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science / Machine Learning (OpenReview corpus: ICLR, NeurIPS, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Supervised prediction of log average citations per month via (a) pairwise ranking loss (binary classification of which of two papers has more citations) and (b) direct regression (L1 loss) using SPECTER2 embeddings of paper text; when needed, context embeddings (references or other sections) are concatenated and processed by a one-layer transformer encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Spearman correlation (ρ_s) and Pearson correlation to ground-truth log citations, pairwise comparison accuracy, and regression L1 distance (on log avg citations per month).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Augmented OpenReview dataset (all publicly available OpenReview submissions parsed via GROBID), and ACL-OCL dataset, both enriched with Semantic Scholar citation counts, reference title/abstracts, and GPT-3.5-turbo annotated research hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Citation prediction is the most viable automatic metric studied. On ACL-OCL Title+Abstract inputs the pairwise comparison accuracy was 0.665 ± 0.010 with Spearman ρ_s ≈ 0.481 ± 0.026; regression L1 ≈ 0.921 ± 0.001 and regression ρ_s ≈ 0.498 ± 0.002. Title+Abstract representations outperformed hypothesis-only representations; adding full-paper context or reference abstracts yielded only marginal gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Citation counts are skewed and affected by external confounders (author network, field popularity, temporal effects); dataset sizes and removal of figures/tables limit models; temporal generalization and reliance on content-only signals (no author metadata) remain challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Citation counts measure community influence rather than immediate scientific 'quality' and only weakly correlate with reviewer scores (weak positive Pearson correlations across venues); however, citation prediction is easier to learn from content than review-score prediction and is more scalable than human review.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use log average citations per month as the target; represent papers with Title+Abstract SPECTER2 embeddings; use pairwise ranking for ranking generated items and regression for absolute scoring; account for temporal splits and confounders, and prefer larger datasets and full-text models when possible.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Evaluation Metrics for Artificially Generated Scientific Research', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9710.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9710.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReviewScorePred</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Review score prediction (RSP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Predicts peer-review-derived quality scores (average overall score and impact score) from paper content using pairwise ranking and direct regression to serve as an automatic proxy for scientific quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Compared against LLM-based reviewer systems (e.g., Sakana from Lu et al., 2024 and pairwise LLM reviewer from Si et al., 2024); GPT-3.5-turbo used for hypothesis annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>LLM-based reviewers (as evaluated) are systems that take papers or pairs of papers and output review text and/or comparative judgments; specifics vary by system (Sakana and the Si et al. pairwise reviewer were used as baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science / Machine Learning (peer-reviewed conference submissions: ICLR, NeurIPS, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Predict average review scores from paper representations via (a) pairwise comparison loss (binary cross-entropy on which paper has higher score) and (b) direct L1 regression; models use SPECTER2 embeddings of Title+Abstract (and optionally context) and are evaluated venue-agnostic and venue-specific.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Pairwise comparison accuracy, Pearson correlation between predicted and actual review scores, and L1 distance for direct prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>OpenReview dataset (unified review schema across venues) and ACL-OCL; models trained on venue-aggregated and venue-specific subsets (e.g., ICLR, NeurIPS).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Review-score prediction is substantially harder than citation prediction. On the full OpenReview dataset venue-agnostic models perform near chance. Venue-specific models for ICLR and NeurIPS reached approx. ~60% pairwise comparison accuracy. A Title+Abstract-based prediction model achieved mean Pearson correlation ≈ 0.330 ± 0.030 to human mean review scores (higher than the Sakana LLM reviewer at 0.161, but lower than single human reviewer consistency ≈ 0.412 ± 0.044).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Heterogeneous review formats and differing standards across venues, topic variability, limited labeled review-score data, and overfitting with larger models; review scores themselves are noisy and influenced by reviewer-field fit and other non-content factors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Automatic review-score predictors outperform evaluated LLM-based reviewers on correlation/accuracy in this work but still fall short of human reviewer consistency and nuance; human reviews remain the gold standard.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Prefer venue-specific models when feasible, use Title+Abstract inputs, standardize review schema across venues, and combine score prediction with other signals (e.g., citations) rather than relying solely on predicted review scores.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Evaluation Metrics for Artificially Generated Scientific Research', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9710.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9710.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PairwiseRankTask</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pairwise ranking (learning-to-rank via pairwise comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A ranking approach that trains a model to predict which of two papers has a higher target score (citation or review), using a binary cross-entropy loss on the sigmoid of the modeled score difference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Applies to evaluation of outputs from various LLMs (e.g., LLM-generated hypotheses or papers); GPT-3.5-turbo used in annotations in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Framework intended to rank generated scientific artifacts (papers/hypotheses) produced by foundation models by making pairwise comparisons and assembling a ranking (round-robin or tournament style).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General scientific outputs; evaluated here on ML/CS conference submissions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Train on pairs (d1,d2) with label x=1[s1>s2]; minimize BCE on σ(f(d1)-f(d2)); evaluate via pairwise comparison accuracy and ability to produce global rankings by repeated pairwise comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Pairwise comparison accuracy (proportion of correctly ordered pairs), stability across seeds, and downstream ranking quality.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>OpenReview and ACL-OCL datasets used to construct pairwise training and test splits.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Pairwise training yielded strong performance for citation prediction (e.g., Title+Abstract pairwise accuracy ≈ 0.665) and is effective for ranking generated content; less effective for review-score prediction at cross-venue scale but viable in venue-specific settings (~60% accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Pairwise comparisons scale quadratically if applied naively; quality depends on consistency of target scores across venues (which is limited for review scores); temporal generalization must be considered.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Pairwise automatic ranking can approximate human judgments for citations better than for review scores; humans remain more consistent for nuanced review judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use pairwise methods to rank collections of generated items (e.g., round-robin or Swiss-style tournaments), restrict comparisons within coherent topic/venue slices, and combine pairwise models with absolute scoring for calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Evaluation Metrics for Artificially Generated Scientific Research', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9710.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9710.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RegressionTask</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct score regression (L1 loss)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Direct prediction of a scalar target score (log citations or average review score) from paper embeddings by minimizing L1 distance between predicted and true score.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Applies to outputs from LLMs broadly; GPT-3.5-turbo used for hypothesis annotation here.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Direct-score regressors produce an absolute numeric estimate for comparison or thresholding of generated scientific outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science / Machine Learning (conference paper corpus used to train/evaluate models).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Train an MLP (no-context) or transformer-encoder + MLP (with context) to minimize |f(ω_e,c_e) − s|; evaluate via L1 distance and Spearman/Pearson correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Regression L1 distance on log citations or review scores, Spearman rank correlation (ρ_s), Pearson correlation to ground-truth mean review score.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>OpenReview and ACL-OCL (augmented) with ground-truth citation counts from Semantic Scholar and mapped review scores.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Direct regression achieved competitive correlations for citation prediction (regression ρ_s ≈ 0.498 for Title+Abstract on ACL-OCL) but is less successful for review-score prediction at corpus-wide scale.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Direct regression sensitive to skewed target distributions and outliers; review scores are noisy and heterogeneous across venues, reducing regression effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Regression provides interpretable numeric predictions but lacks human-level nuance for review-style judgments; works better for influence-like metrics (citations).</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Predict log-transformed citation rates to stabilize distribution, apply robust losses (L1), and evaluate both correlation and rank-based metrics for downstream ranking use.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Evaluation Metrics for Artificially Generated Scientific Research', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9710.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9710.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPECTER2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SPECTER2 (scientific text embedding model with regression adapter)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scientific-document embedding model used to compute high-quality fixed-size embeddings for titles, abstracts, sections, or hypotheses; embeddings are frozen and averaged over chunks when needed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Not an LLM for generation of theories; used as an embedding backbone for score-prediction models (applies to evaluation of LLM-generated outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>SPECTER2 is a scientific-text encoder (with a regression adapter in this work) designed to produce embeddings optimized for scholarly downstream tasks; used here frozen as the feature extractor.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scholarly documents across Computer Science / ML (used on OpenReview and ACL-OCL corpora).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute per-chunk embeddings (NLTK sentence tokenizer) then average; feed embedding vectors into MLP (no-context) or transformer encoder + MLP (with context) for score prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Downstream predictive performance (pairwise accuracy, Spearman/Pearson correlations, L1 distance) when used as input features.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Used on the paper text derived from OpenReview and ACL-OCL datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>SPECTER2 embeddings provided effective representations for both citation and review-score prediction tasks; frozen embeddings with modest models achieved the reported performance without heavy fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>When input exceeds context length, chunking and averaging may lose structural information; full-text or multimodal features (tables/figures) might improve prediction but were not used.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Embeddings enable scalable automatic evaluation pipelines that outperform some LLM-based reviewers on numerical proxies but do not replace expert human assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use SPECTER2 (or similar scientific encoders) as frozen feature extractors for content-based metrics; consider full-text and multimodal embedding extensions for future improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Evaluation Metrics for Artificially Generated Scientific Research', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9710.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9710.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenReview-Aug</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Augmented OpenReview dataset (papers + unified reviews + citations + hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curated and unified dataset derived from OpenReview submissions, where PDFs are parsed (GROBID), sections classified, reviews normalized to a unified schema, references and citation counts from Semantic Scholar retrieved, and research hypotheses annotated using GPT-3.5-turbo.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-3.5-turbo (used for research hypothesis annotation; the dataset is intended to evaluate models including those applied to LLM-generated outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Dataset construction used GPT-3.5-turbo to produce concise research hypotheses for each submission; the dataset is intended to benchmark automatic evaluation metrics of scientific outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science / Machine Learning conference submissions (ICLR, NeurIPS, ACL, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Provides paired paper text, unified review scores, citation counts, and hypothesis annotations enabling supervised training and evaluation of citation- and review-score predictors and comparison to LLM-based reviewers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Supports computation of Spearman/Pearson correlations, pairwise comparison accuracy, L1 regression errors, and qualitative comparisons of generated reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>This augmented OpenReview corpus (publicly released in part due to licensing) and an updated ACL-OCL are the primary benchmarks used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Dataset enabled the experiments showing that citation prediction from Title+Abstract is viable, that review-score prediction is harder, and that simple models can outperform evaluated LLM reviewers on pairwise tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not all submissions are redistributable due to licenses; parsing errors (GROBID) and imperfect GPT-3.5-turbo hypothesis annotations (surveyed authors rated completeness as improvable) introduce noise; venues vary in review-field structure.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Dataset links machine-predictable signals (citations) to reviewer judgments to enable automated evaluation research, complementing but not replacing expert review.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use the unified schema for cross-venue experiments, validate GPT-annotated hypotheses with authors when possible, and consider licensing and parsing noise in downstream evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Evaluation Metrics for Artificially Generated Scientific Research', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9710.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9710.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SakanaLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sakana LLM reviewer (LLM-based review system used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based reviewer system (from Lu et al., 2024) used as a baseline; generates review text and scores for papers but was found to provide lower correlation to human review scores and to produce more generic critiques.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Sakana (LLM reviewer system)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>A published LLM-based reviewer system (referred to as Sakana) that outputs review text and score estimates; exact internal model details are from the cited source (Lu et al., 2024) and not reimplemented in full here.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Applied to ML/CS paper reviewing (ICLR-2024 subset used for comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compare Sakana's predicted scores and review text to human reviewer scores (mean of other reviewers) and to automatic review-score prediction models; compute Pearson correlations and qualitatively compare review contents.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Pearson correlation to human mean review scores, qualitative assessment of review depth (generic vs detailed), pairwise comparison accuracy where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>ICLR-2024 subset of the augmented OpenReview dataset used for quantitative and qualitative comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Sakana's predicted review scores had mean Pearson correlation ≈ 0.161 to ground-truth mean review scores on the tested subset; review texts tended to list generic strengths and weaknesses and lacked detailed semantic critique compared to human reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>LLM reviewer produced generic and shallow reviews lacking deep methodological insight; score assignments show weak correlation with human reviewers; not reliable as standalone replacement for human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Sakana underperforms human reviewers in consistency and insight; a simple content-based prediction model in this work achieved higher correlation to human mean review scores.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use caution when interpreting LLM-derived reviews; prefer augmenting human review with LLM-generated feedback rather than replacing expert assessment; evaluate LLM reviewers quantitatively (correlation/accuracy) and qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Evaluation Metrics for Artificially Generated Scientific Research', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9710.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9710.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SiPairwiseLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pairwise LLM reviewer (Si et al., 2024 style baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pairwise LLM-based reviewer approach (from Si et al., 2024) that compares two items and predicts which is more likely to receive a higher review score; used here as a baseline comparator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Pairwise LLM reviewer (as in Si et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>An LLM prompted to compare two proposals/papers and judge which is likely to receive a higher human review score; in the original Si et al. setup it evaluated project proposals, here applied for comparison on full paper subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>LLM-related proposals and ML/CS paper comparisons (tested on ICLR and NeurIPS subsets).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>LLM is given pairs of papers and asked to select which will receive a higher review score; evaluated by pairwise comparison accuracy against actual human review scores.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Pairwise comparison accuracy on held-out subsets (200-paper subsamples per venue in this work).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Subsamples from test sets of ICLR-2024 and NeurIPS-2024 from the augmented OpenReview dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The simple Title+Abstract content-based prediction models in this study outperformed the Si et al.-style pairwise LLM reviewer on the tested ICLR and NeurIPS subsets; the LLM reviewer performed comparably to prior reported results but below the simple ML predictor.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Pairwise LLM reviewers may be unreliable for nuanced, novel research evaluation and perform worse than targeted content-based predictors trained on historical data; results depend on prompt, domain, and scope of comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Pairwise LLM reviewers are less consistent than human reviewers and in this study were outperformed by simple supervised predictors trained on historical data.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>When using pairwise LLM reviewers, calibrate and validate with held-out human-reviewed datasets; consider combining LLM judgments with learned predictors for robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic Evaluation Metrics for Artificially Generated Scientific Research', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The AI scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 2)</em></li>
                <li>Can llms generate novel research ideas? A large-scale human study with 100+ NLP researchers <em>(Rating: 2)</em></li>
                <li>Reviewergpt? an exploratory study on using large language models for paper reviewing <em>(Rating: 2)</em></li>
                <li>Realistic citation count prediction task for newly published papers <em>(Rating: 2)</em></li>
                <li>Scirepeval: A multi-format benchmark for scientific document representations <em>(Rating: 1)</em></li>
                <li>Predicting the citations of scholarly paper <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9710",
    "paper_id": "paper-276903104",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "CitationCountPred",
            "name_full": "Citation count prediction (log average citations per month)",
            "brief_description": "An automatic evaluation metric that predicts a paper's influence by estimating the logarithm of its average citations per month from paper content (title/abstract, sections, or hypothesis) using SPECTER2 embeddings and simple predictors (MLP / transformer encoder + MLP).",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-3.5-turbo (used to annotate research hypotheses); target is LLM-generated scientific outputs in general",
            "llm_description": "GPT-3.5-turbo (16k context window variant) was used in this work only to annotate research hypotheses from papers; the citation-prediction models are content-based predictors (SPECTER2 embeddings + MLP/transformer) applied to papers including those that could be generated by LLMs.",
            "scientific_domain": "Computer Science / Machine Learning (OpenReview corpus: ICLR, NeurIPS, etc.)",
            "evaluation_method": "Supervised prediction of log average citations per month via (a) pairwise ranking loss (binary classification of which of two papers has more citations) and (b) direct regression (L1 loss) using SPECTER2 embeddings of paper text; when needed, context embeddings (references or other sections) are concatenated and processed by a one-layer transformer encoder.",
            "evaluation_criteria": "Spearman correlation (ρ_s) and Pearson correlation to ground-truth log citations, pairwise comparison accuracy, and regression L1 distance (on log avg citations per month).",
            "benchmark_or_dataset": "Augmented OpenReview dataset (all publicly available OpenReview submissions parsed via GROBID), and ACL-OCL dataset, both enriched with Semantic Scholar citation counts, reference title/abstracts, and GPT-3.5-turbo annotated research hypotheses.",
            "results_summary": "Citation prediction is the most viable automatic metric studied. On ACL-OCL Title+Abstract inputs the pairwise comparison accuracy was 0.665 ± 0.010 with Spearman ρ_s ≈ 0.481 ± 0.026; regression L1 ≈ 0.921 ± 0.001 and regression ρ_s ≈ 0.498 ± 0.002. Title+Abstract representations outperformed hypothesis-only representations; adding full-paper context or reference abstracts yielded only marginal gains.",
            "limitations_or_challenges": "Citation counts are skewed and affected by external confounders (author network, field popularity, temporal effects); dataset sizes and removal of figures/tables limit models; temporal generalization and reliance on content-only signals (no author metadata) remain challenges.",
            "comparison_to_human_or_traditional": "Citation counts measure community influence rather than immediate scientific 'quality' and only weakly correlate with reviewer scores (weak positive Pearson correlations across venues); however, citation prediction is easier to learn from content than review-score prediction and is more scalable than human review.",
            "recommendations_or_best_practices": "Use log average citations per month as the target; represent papers with Title+Abstract SPECTER2 embeddings; use pairwise ranking for ranking generated items and regression for absolute scoring; account for temporal splits and confounders, and prefer larger datasets and full-text models when possible.",
            "uuid": "e9710.0",
            "source_info": {
                "paper_title": "Automatic Evaluation Metrics for Artificially Generated Scientific Research",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "ReviewScorePred",
            "name_full": "Review score prediction (RSP)",
            "brief_description": "Predicts peer-review-derived quality scores (average overall score and impact score) from paper content using pairwise ranking and direct regression to serve as an automatic proxy for scientific quality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Compared against LLM-based reviewer systems (e.g., Sakana from Lu et al., 2024 and pairwise LLM reviewer from Si et al., 2024); GPT-3.5-turbo used for hypothesis annotation.",
            "llm_description": "LLM-based reviewers (as evaluated) are systems that take papers or pairs of papers and output review text and/or comparative judgments; specifics vary by system (Sakana and the Si et al. pairwise reviewer were used as baselines).",
            "scientific_domain": "Computer Science / Machine Learning (peer-reviewed conference submissions: ICLR, NeurIPS, etc.)",
            "evaluation_method": "Predict average review scores from paper representations via (a) pairwise comparison loss (binary cross-entropy on which paper has higher score) and (b) direct L1 regression; models use SPECTER2 embeddings of Title+Abstract (and optionally context) and are evaluated venue-agnostic and venue-specific.",
            "evaluation_criteria": "Pairwise comparison accuracy, Pearson correlation between predicted and actual review scores, and L1 distance for direct prediction.",
            "benchmark_or_dataset": "OpenReview dataset (unified review schema across venues) and ACL-OCL; models trained on venue-aggregated and venue-specific subsets (e.g., ICLR, NeurIPS).",
            "results_summary": "Review-score prediction is substantially harder than citation prediction. On the full OpenReview dataset venue-agnostic models perform near chance. Venue-specific models for ICLR and NeurIPS reached approx. ~60% pairwise comparison accuracy. A Title+Abstract-based prediction model achieved mean Pearson correlation ≈ 0.330 ± 0.030 to human mean review scores (higher than the Sakana LLM reviewer at 0.161, but lower than single human reviewer consistency ≈ 0.412 ± 0.044).",
            "limitations_or_challenges": "Heterogeneous review formats and differing standards across venues, topic variability, limited labeled review-score data, and overfitting with larger models; review scores themselves are noisy and influenced by reviewer-field fit and other non-content factors.",
            "comparison_to_human_or_traditional": "Automatic review-score predictors outperform evaluated LLM-based reviewers on correlation/accuracy in this work but still fall short of human reviewer consistency and nuance; human reviews remain the gold standard.",
            "recommendations_or_best_practices": "Prefer venue-specific models when feasible, use Title+Abstract inputs, standardize review schema across venues, and combine score prediction with other signals (e.g., citations) rather than relying solely on predicted review scores.",
            "uuid": "e9710.1",
            "source_info": {
                "paper_title": "Automatic Evaluation Metrics for Artificially Generated Scientific Research",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "PairwiseRankTask",
            "name_full": "Pairwise ranking (learning-to-rank via pairwise comparisons)",
            "brief_description": "A ranking approach that trains a model to predict which of two papers has a higher target score (citation or review), using a binary cross-entropy loss on the sigmoid of the modeled score difference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Applies to evaluation of outputs from various LLMs (e.g., LLM-generated hypotheses or papers); GPT-3.5-turbo used in annotations in this study.",
            "llm_description": "Framework intended to rank generated scientific artifacts (papers/hypotheses) produced by foundation models by making pairwise comparisons and assembling a ranking (round-robin or tournament style).",
            "scientific_domain": "General scientific outputs; evaluated here on ML/CS conference submissions.",
            "evaluation_method": "Train on pairs (d1,d2) with label x=1[s1&gt;s2]; minimize BCE on σ(f(d1)-f(d2)); evaluate via pairwise comparison accuracy and ability to produce global rankings by repeated pairwise comparisons.",
            "evaluation_criteria": "Pairwise comparison accuracy (proportion of correctly ordered pairs), stability across seeds, and downstream ranking quality.",
            "benchmark_or_dataset": "OpenReview and ACL-OCL datasets used to construct pairwise training and test splits.",
            "results_summary": "Pairwise training yielded strong performance for citation prediction (e.g., Title+Abstract pairwise accuracy ≈ 0.665) and is effective for ranking generated content; less effective for review-score prediction at cross-venue scale but viable in venue-specific settings (~60% accuracy).",
            "limitations_or_challenges": "Pairwise comparisons scale quadratically if applied naively; quality depends on consistency of target scores across venues (which is limited for review scores); temporal generalization must be considered.",
            "comparison_to_human_or_traditional": "Pairwise automatic ranking can approximate human judgments for citations better than for review scores; humans remain more consistent for nuanced review judgments.",
            "recommendations_or_best_practices": "Use pairwise methods to rank collections of generated items (e.g., round-robin or Swiss-style tournaments), restrict comparisons within coherent topic/venue slices, and combine pairwise models with absolute scoring for calibration.",
            "uuid": "e9710.2",
            "source_info": {
                "paper_title": "Automatic Evaluation Metrics for Artificially Generated Scientific Research",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "RegressionTask",
            "name_full": "Direct score regression (L1 loss)",
            "brief_description": "Direct prediction of a scalar target score (log citations or average review score) from paper embeddings by minimizing L1 distance between predicted and true score.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Applies to outputs from LLMs broadly; GPT-3.5-turbo used for hypothesis annotation here.",
            "llm_description": "Direct-score regressors produce an absolute numeric estimate for comparison or thresholding of generated scientific outputs.",
            "scientific_domain": "Computer Science / Machine Learning (conference paper corpus used to train/evaluate models).",
            "evaluation_method": "Train an MLP (no-context) or transformer-encoder + MLP (with context) to minimize |f(ω_e,c_e) − s|; evaluate via L1 distance and Spearman/Pearson correlations.",
            "evaluation_criteria": "Regression L1 distance on log citations or review scores, Spearman rank correlation (ρ_s), Pearson correlation to ground-truth mean review score.",
            "benchmark_or_dataset": "OpenReview and ACL-OCL (augmented) with ground-truth citation counts from Semantic Scholar and mapped review scores.",
            "results_summary": "Direct regression achieved competitive correlations for citation prediction (regression ρ_s ≈ 0.498 for Title+Abstract on ACL-OCL) but is less successful for review-score prediction at corpus-wide scale.",
            "limitations_or_challenges": "Direct regression sensitive to skewed target distributions and outliers; review scores are noisy and heterogeneous across venues, reducing regression effectiveness.",
            "comparison_to_human_or_traditional": "Regression provides interpretable numeric predictions but lacks human-level nuance for review-style judgments; works better for influence-like metrics (citations).",
            "recommendations_or_best_practices": "Predict log-transformed citation rates to stabilize distribution, apply robust losses (L1), and evaluate both correlation and rank-based metrics for downstream ranking use.",
            "uuid": "e9710.3",
            "source_info": {
                "paper_title": "Automatic Evaluation Metrics for Artificially Generated Scientific Research",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "SPECTER2",
            "name_full": "SPECTER2 (scientific text embedding model with regression adapter)",
            "brief_description": "A scientific-document embedding model used to compute high-quality fixed-size embeddings for titles, abstracts, sections, or hypotheses; embeddings are frozen and averaged over chunks when needed.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "Not an LLM for generation of theories; used as an embedding backbone for score-prediction models (applies to evaluation of LLM-generated outputs).",
            "llm_description": "SPECTER2 is a scientific-text encoder (with a regression adapter in this work) designed to produce embeddings optimized for scholarly downstream tasks; used here frozen as the feature extractor.",
            "scientific_domain": "Scholarly documents across Computer Science / ML (used on OpenReview and ACL-OCL corpora).",
            "evaluation_method": "Compute per-chunk embeddings (NLTK sentence tokenizer) then average; feed embedding vectors into MLP (no-context) or transformer encoder + MLP (with context) for score prediction.",
            "evaluation_criteria": "Downstream predictive performance (pairwise accuracy, Spearman/Pearson correlations, L1 distance) when used as input features.",
            "benchmark_or_dataset": "Used on the paper text derived from OpenReview and ACL-OCL datasets.",
            "results_summary": "SPECTER2 embeddings provided effective representations for both citation and review-score prediction tasks; frozen embeddings with modest models achieved the reported performance without heavy fine-tuning.",
            "limitations_or_challenges": "When input exceeds context length, chunking and averaging may lose structural information; full-text or multimodal features (tables/figures) might improve prediction but were not used.",
            "comparison_to_human_or_traditional": "Embeddings enable scalable automatic evaluation pipelines that outperform some LLM-based reviewers on numerical proxies but do not replace expert human assessment.",
            "recommendations_or_best_practices": "Use SPECTER2 (or similar scientific encoders) as frozen feature extractors for content-based metrics; consider full-text and multimodal embedding extensions for future improvements.",
            "uuid": "e9710.4",
            "source_info": {
                "paper_title": "Automatic Evaluation Metrics for Artificially Generated Scientific Research",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "OpenReview-Aug",
            "name_full": "Augmented OpenReview dataset (papers + unified reviews + citations + hypotheses)",
            "brief_description": "A curated and unified dataset derived from OpenReview submissions, where PDFs are parsed (GROBID), sections classified, reviews normalized to a unified schema, references and citation counts from Semantic Scholar retrieved, and research hypotheses annotated using GPT-3.5-turbo.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-3.5-turbo (used for research hypothesis annotation; the dataset is intended to evaluate models including those applied to LLM-generated outputs).",
            "llm_description": "Dataset construction used GPT-3.5-turbo to produce concise research hypotheses for each submission; the dataset is intended to benchmark automatic evaluation metrics of scientific outputs.",
            "scientific_domain": "Computer Science / Machine Learning conference submissions (ICLR, NeurIPS, ACL, etc.).",
            "evaluation_method": "Provides paired paper text, unified review scores, citation counts, and hypothesis annotations enabling supervised training and evaluation of citation- and review-score predictors and comparison to LLM-based reviewers.",
            "evaluation_criteria": "Supports computation of Spearman/Pearson correlations, pairwise comparison accuracy, L1 regression errors, and qualitative comparisons of generated reviews.",
            "benchmark_or_dataset": "This augmented OpenReview corpus (publicly released in part due to licensing) and an updated ACL-OCL are the primary benchmarks used in experiments.",
            "results_summary": "Dataset enabled the experiments showing that citation prediction from Title+Abstract is viable, that review-score prediction is harder, and that simple models can outperform evaluated LLM reviewers on pairwise tasks.",
            "limitations_or_challenges": "Not all submissions are redistributable due to licenses; parsing errors (GROBID) and imperfect GPT-3.5-turbo hypothesis annotations (surveyed authors rated completeness as improvable) introduce noise; venues vary in review-field structure.",
            "comparison_to_human_or_traditional": "Dataset links machine-predictable signals (citations) to reviewer judgments to enable automated evaluation research, complementing but not replacing expert review.",
            "recommendations_or_best_practices": "Use the unified schema for cross-venue experiments, validate GPT-annotated hypotheses with authors when possible, and consider licensing and parsing noise in downstream evaluations.",
            "uuid": "e9710.5",
            "source_info": {
                "paper_title": "Automatic Evaluation Metrics for Artificially Generated Scientific Research",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "SakanaLLM",
            "name_full": "Sakana LLM reviewer (LLM-based review system used as baseline)",
            "brief_description": "An LLM-based reviewer system (from Lu et al., 2024) used as a baseline; generates review text and scores for papers but was found to provide lower correlation to human review scores and to produce more generic critiques.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "Sakana (LLM reviewer system)",
            "llm_description": "A published LLM-based reviewer system (referred to as Sakana) that outputs review text and score estimates; exact internal model details are from the cited source (Lu et al., 2024) and not reimplemented in full here.",
            "scientific_domain": "Applied to ML/CS paper reviewing (ICLR-2024 subset used for comparison).",
            "evaluation_method": "Compare Sakana's predicted scores and review text to human reviewer scores (mean of other reviewers) and to automatic review-score prediction models; compute Pearson correlations and qualitatively compare review contents.",
            "evaluation_criteria": "Pearson correlation to human mean review scores, qualitative assessment of review depth (generic vs detailed), pairwise comparison accuracy where applicable.",
            "benchmark_or_dataset": "ICLR-2024 subset of the augmented OpenReview dataset used for quantitative and qualitative comparison.",
            "results_summary": "Sakana's predicted review scores had mean Pearson correlation ≈ 0.161 to ground-truth mean review scores on the tested subset; review texts tended to list generic strengths and weaknesses and lacked detailed semantic critique compared to human reviews.",
            "limitations_or_challenges": "LLM reviewer produced generic and shallow reviews lacking deep methodological insight; score assignments show weak correlation with human reviewers; not reliable as standalone replacement for human evaluation.",
            "comparison_to_human_or_traditional": "Sakana underperforms human reviewers in consistency and insight; a simple content-based prediction model in this work achieved higher correlation to human mean review scores.",
            "recommendations_or_best_practices": "Use caution when interpreting LLM-derived reviews; prefer augmenting human review with LLM-generated feedback rather than replacing expert assessment; evaluate LLM reviewers quantitatively (correlation/accuracy) and qualitatively.",
            "uuid": "e9710.6",
            "source_info": {
                "paper_title": "Automatic Evaluation Metrics for Artificially Generated Scientific Research",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "SiPairwiseLLM",
            "name_full": "Pairwise LLM reviewer (Si et al., 2024 style baseline)",
            "brief_description": "A pairwise LLM-based reviewer approach (from Si et al., 2024) that compares two items and predicts which is more likely to receive a higher review score; used here as a baseline comparator.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "Pairwise LLM reviewer (as in Si et al., 2024)",
            "llm_description": "An LLM prompted to compare two proposals/papers and judge which is likely to receive a higher human review score; in the original Si et al. setup it evaluated project proposals, here applied for comparison on full paper subsets.",
            "scientific_domain": "LLM-related proposals and ML/CS paper comparisons (tested on ICLR and NeurIPS subsets).",
            "evaluation_method": "LLM is given pairs of papers and asked to select which will receive a higher review score; evaluated by pairwise comparison accuracy against actual human review scores.",
            "evaluation_criteria": "Pairwise comparison accuracy on held-out subsets (200-paper subsamples per venue in this work).",
            "benchmark_or_dataset": "Subsamples from test sets of ICLR-2024 and NeurIPS-2024 from the augmented OpenReview dataset.",
            "results_summary": "The simple Title+Abstract content-based prediction models in this study outperformed the Si et al.-style pairwise LLM reviewer on the tested ICLR and NeurIPS subsets; the LLM reviewer performed comparably to prior reported results but below the simple ML predictor.",
            "limitations_or_challenges": "Pairwise LLM reviewers may be unreliable for nuanced, novel research evaluation and perform worse than targeted content-based predictors trained on historical data; results depend on prompt, domain, and scope of comparison.",
            "comparison_to_human_or_traditional": "Pairwise LLM reviewers are less consistent than human reviewers and in this study were outperformed by simple supervised predictors trained on historical data.",
            "recommendations_or_best_practices": "When using pairwise LLM reviewers, calibrate and validate with held-out human-reviewed datasets; consider combining LLM judgments with learned predictors for robustness.",
            "uuid": "e9710.7",
            "source_info": {
                "paper_title": "Automatic Evaluation Metrics for Artificially Generated Scientific Research",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The AI scientist: Towards fully automated open-ended scientific discovery",
            "rating": 2,
            "sanitized_title": "the_ai_scientist_towards_fully_automated_openended_scientific_discovery"
        },
        {
            "paper_title": "Can llms generate novel research ideas? A large-scale human study with 100+ NLP researchers",
            "rating": 2,
            "sanitized_title": "can_llms_generate_novel_research_ideas_a_largescale_human_study_with_100_nlp_researchers"
        },
        {
            "paper_title": "Reviewergpt? an exploratory study on using large language models for paper reviewing",
            "rating": 2,
            "sanitized_title": "reviewergpt_an_exploratory_study_on_using_large_language_models_for_paper_reviewing"
        },
        {
            "paper_title": "Realistic citation count prediction task for newly published papers",
            "rating": 2,
            "sanitized_title": "realistic_citation_count_prediction_task_for_newly_published_papers"
        },
        {
            "paper_title": "Scirepeval: A multi-format benchmark for scientific document representations",
            "rating": 1,
            "sanitized_title": "scirepeval_a_multiformat_benchmark_for_scientific_document_representations"
        },
        {
            "paper_title": "Predicting the citations of scholarly paper",
            "rating": 1,
            "sanitized_title": "predicting_the_citations_of_scholarly_paper"
        }
    ],
    "cost": 0.020801249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Automatic Evaluation Metrics for Artificially Generated Scientific Research</p>
<p>Niklas Hoepner n.r.hopner@uva.nl 
University of Amsterdam</p>
<p>Leon Eshuijs 
Vrije Universiteit</p>
<p>Dimitrios Alivanistos 
Vrije Universiteit</p>
<p>Giacomo Zamprogno 
Vrije Universiteit</p>
<p>Ilaria Tiddi 
Vrije Universiteit</p>
<p>Automatic Evaluation Metrics for Artificially Generated Scientific Research
17E6E966FFD021CDF8B02D56EB62D111
Foundation models are increasingly used in scientific research, but evaluating AI-generated scientific work remains challenging.Expert reviews are costly, while large language models (LLMs) as proxy reviewers have proven to be unreliable.To address this, we investigate two automatic evaluation metrics, specifically citation count prediction and review score prediction.We parse all papers of OpenReview and augment each submission with its citation count, reference, and research hypothesis.Our findings reveal that citation count prediction is more viable than review score prediction, and predicting scores is more difficult purely from the research hypothesis than from the full paper.Furthermore, we show that a simple prediction model based solely on title and abstract outperforms LLM-based reviewers, though it still falls short of human-level consistency 12 .</p>
<p>Introduction</p>
<p>Advances in foundation models (Driess et al., 2023;OpenAI, 2023) have increased interest in their potential to enhance various stages of the research process (Boiko et al., 2023;Lee et al., 2022), including research hypothesis generation (Wang et al., 2024;Baek et al., 2024;Si et al., 2024), writing assistance (Funkquist et al., 2023;Gao et al., 2023), and peer review (Liu and Shah, 2023;Yuan et al., 2022).Their application to literature-based generation of scientific content has been particularly notable, producing research hypotheses that domain experts consider more novel than those generated by humans (Si et al., 2024).While much of the focus has been on generating research hypotheses (Si et al., 2024;Baek et al., 2024;Wang et al., 2024), fewer studies have explored the generation of complete scientific papers or multiple steps of the research process (Lu et al., 2024;Li et al., 2024).</p>
<p>A key challenge in this area is evaluating the quality of generated scientific content (Baek et al., 2024;Lu et al., 2024;Si et al., 2024).Most studies use a mix of domain expert assessments and LLMbased reviews (Baek et al., 2024;Wang et al., 2024;Qi et al., 2023).However, expert evaluations are costly and time-consuming, limiting them to a subset of methodologies, often selected based on LLM reviewer results (Qi et al., 2023).While early studies showed promise for LLMs in reviewing papers (Baek et al., 2024;Lu et al., 2024), recent findings reveal their reliability issues, with some cases showing LLM reviewers performing no better than random guessing compared to human evaluations of novel research hypotheses (Si et al., 2024).</p>
<p>An automatic evaluation metric should be efficient to compute, reliably indicate scientific quality and generalise to unseen future work.The field of scholarly document quality prediction (SDQP) (de Buy Wenniger et al., 2023) has studied the problem of predicting review scores and citation counts (Kang et al., 2018;Hirako et al., 2024) as proxy metrics.Domain experts reviews are the gold standard for evaluating the scientific quality of a paper and citation counts measure a paper's influence on the scientific community.However, limited data availability has constrained review score prediction (Staudinger et al., 2024), while citation prediction typically relies on factors beyond paper content such as paper metadata (Zhang and Wu, 2024).Prior work has not studied the challenge of predicting scientific quality based on research hypotheses alone and assessing generalisation to future work.</p>
<p>To study both prediction problems, we propose data models for scientific papers and reviews to parse all OpenReview submissions 3 into a unified format, further annotating submissions with citation counts and research hypotheses.This allows us to analyse the relationship between review and 3 https:/openreview.netarXiv:2503.05712v1[cs.CY] 14 Feb 2025 citation scores, evaluate their predictability, and compare them with LLM-based review systems.</p>
<p>In summary, our contributions are:</p>
<p>• We propose citation count and review score prediction as automatic evaluation metrics for AI-generated scientific content.</p>
<p>• We parse all OpenReview submissions into a unifying format, augmenting them with additional metadata.</p>
<p>• Demonstrate that a simple score prediction model is more consistent with human reviews than LLM-based reviewers.</p>
<p>2 Related Work</p>
<p>AI generated Science</p>
<p>Advancements in instructable generative models (OpenAI, 2023;Esser et al., 2024) have sparked interest in their use for scientific content generation (Wang et al., 2024;Lu et al., 2024;Si et al., 2024;Bran et al., 2024;Funkquist et al., 2023).Large Language Models (LLMs) have been explored for tasks such as research hypothesis generation (Wang et al., 2024;Baek et al., 2024;Si et al., 2024), paper drafting (Funkquist et al., 2023;Gao et al., 2023), and even experimental design (Bran et al., 2024;Li et al., 2024).Most studies focus on specific subproblems.Notable exceptions include Lu et al. (2024), who apply LLMs to the entire research process, and Li et al. (2024), who focus on machine learning research but exclude report writing.</p>
<p>A key challenge in this domain is evaluating generated outputs, such as the novelty or feasibility of a research hypothesis, which often requires timeconsuming, costly reviews by domain experts (Si et al., 2024).To address this, some researchers have used LLM-based evaluations as substitutes for human judgment (Baek et al., 2024;Qi et al., 2023;Yang et al., 2024), with some even replacing human evaluators in certain cases entirely (Lu et al., 2024).However, recent studies question the reliability of LLMs as evaluators compared to human reviewers (Si et al., 2024).We propose citation count and review score prediction as alternative evaluation metrics and compare them to LLM-based systems.</p>
<p>Automated Peer Reviewing</p>
<p>With the increase in scientific paper submissions, interest in automating aspects of the peer review process has grown (Fernandes and de Melo, 2024), spanning tasks like reviewer assignment (Jecmen et al., 2023) and review generation (Yuan et al., 2022;D'Arcy et al., 2024;Zhou et al., 2024).An overview of the tasks and related datasets is provided in Staudinger et al. (2024).</p>
<p>While, automatically generated textual reviews (D'Arcy et al., 2024) can provide valuable feedback to authors, they are unsuitable as standalone review metrics, as it is difficult to determine which method generates more valuable research papers.To develop automatic evaluation metrics, we focus on the task of Review Score Prediction (RSP) (Kang et al., 2018).Another related task, Paper Decision Prediction (DSP) (Bharti et al., 2021;Fernandes and de Melo, 2024), could also contribute to evaluation metrics but offers less granularity.While Kang et al. (2018) explore review score prediction, their work is limited to a small dataset (PeerRead,&lt;1000 submissions) and does not address temporal generalization.Follow up work focuses on improving prediction accuracy by dealing with the limited data by learning from unlabelled data (Muangkammuen et al., 2022) or adding intermediate tasks for finetuning (Muangkammuen et al., 2023).</p>
<p>As noted by Staudinger et al. (2024), most studies create their own datasets, many of which are derived from OpenReview.Alternative sources of reviews, such as F10004 and PeerJ5 cover a more diverse range of topics but lack an anonymous review process (Dycke et al., 2023).The lack of a standardized, updated dataset with rich metadata limits method comparisons and progress in automating reviews.Our work addresses this by providing data models for scientific papers and reviews to unify all OpenReview submissions.</p>
<p>Citation Count Prediction</p>
<p>Citation count prediction approaches differ based on the information used (Zhang and Wu, 2024).Most models combine citation history (Wang et al., 2021b), metadata (e.g., authors, h-index) (Bai et al., 2019), and paper content (van Dongen et al., 2020).For newly generated content, without citation history or metadata, only content-based models are applicable to build evaluation metrics.Early studies used n-gram and term-frequency features from titles and abstracts, with small datasets ( 3,000 papers) and short time spans (Fu and Aliferis, 2008;Ibáñez et al., 2009).Recent efforts scaled dataset sizes ( 30,000-40,000 papers) and incorporated embeddings from large language models to improve prediction performance (van Dongen et al., 2020;Jr. et al., 2024).Some studies show significant benefits from using full papers, while others report only marginal improvements (Jr. et al., 2024).</p>
<p>These conflicting findings may stem from differences in datasets and target metrics employed by each study.Such variations complicate direct comparisons across works, hindering the overall progress of the field.Furthermore, few approaches explicitly account for generalization over time.Notably, many studies use validation sets with publication dates that are only minimally separated from those in the training set, with a maximum gap of one month (Hirako et al., 2023(Hirako et al., , 2024)).</p>
<p>Few studies have explored the relationship between citation counts and peer-review scores.While prior work shows that reviews can improve citation prediction models (Li et al., 2019;Plank and van Dalen, 2019), our focus is on their correlation.A strong correlation would suggest both metrics capture similar aspects of a paper's quality.Wang et al. (2021a) found a weak positive correlation between citation counts and review scores for ICLR submissions from 2017 to 2019, which we extend to the entire OpenReview dataset.</p>
<p>Score Prediction</p>
<p>To address the challenge of evaluating the quality of scientific content generated by LLMs, we study the problem of predicting quality scores of scientific papers.As proxies for quality, we utilize citation counts and review scores.We consider two primary tasks: (1) pairwise ranking, where the objective is to predict which of two papers has a higher quality score, and (2) regression, where the goal is to predict the quality score of a single pa-per.The pairwise ranker can generate a ranking of generated content by comparing items in sequence, using either a round-robin format where each item is compared against every other item, or a Swiss tournament system where items are matched based on their current rankings (Si et al., 2024).</p>
<p>Formally, the dataset is represented as
D = {(ω, c, s)} N i=1
, where ω denotes the representation of a paper, c refers to paper context such as references and s is the associated quality score.In this work, we investigate representing papers via different part of the paper ranging from title and abstract to their research hypothesis.Given a scientific text ω or c, we compute embeddings using the SPECTER2 model6 with the regression adapter (Singh et al., 2023).SPECTER2 is specifically designed to compute embeddings for tasks requiring high-quality representations of scientific text.When the input text exceeds the context length supported by the embedding model, we segment the text into sentences using the NLTK sentence tokenizer (Bird, 2006).For each chunk, we compute its embedding independently and subsequently average the embeddings to obtain a fixed-size representation of the entire text.We freeze the parameters of the embedding model and denote embedded scientific text via subscript e (ω e ,c e ).</p>
<p>Let d i = (ω e,i , c e,i , s i ) denote the input data for a single paper.The score model f θ is trained to predict the mapping from the target score by minimizing two objectives:</p>
<ol>
<li>
<p>Pairwise Comparison: We minimize the binary cross-entropy loss between score differences:
L(d 1 , d 2 ; θ) = x log y − (1 − x) log(1 − y),(1)
where x = 1 s 1 &gt;s 2 is a binary indicator for the score comparison, and
y = σ(f θ (d 1 ) − f θ (d 2 ))
represents the predicted probability that paper d 1 has a higher score than paper d 2 .</p>
</li>
<li>
<p>Direct Score Prediction: We minimize the L1 distance to learn the direct mapping of the input to the score: L(ω e , c e , s; θ) = |f θ (ω e , c e ) − s|.</p>
</li>
</ol>
<p>(2)</p>
<p>For models without contextual information, f is implemented as an MLP with a single hidden layer.When context is included, it is represented as a sequence of embeddings (e.g., SPECTER2 embeddings derived from the title and abstract of the references).The context embeddings c e are combined with the paper representation ω e to form a sequence of embeddings which is then processed through a one-layer Transformer encoder (Vaswani et al., 2017).The resulting paper representation embedding is then passed through a MLP similarly to predict the target score (see Figure 1).We look at two types of contexts.In the first, the paper is represented by its title and abstract, with the remaining sections serving as the contextual information.In the second approach, to incorporate references, the paper is still represented by its title and abstract, while the context is formed by the title and abstract of the references.We choose the following target scores for the each score type.</p>
<p>Citation Count: For the citation count, we use the average citations per month as the prediction metric.The score is calculated by dividing the total citation count at the time of model training by the number of months since the paper was published.Given that the distribution of citation scores is highly skewed-where many papers have only a few citations while others receive a significantly larger number-we predict the logarithm of the average citations per month (van Dongen et al., 2020;de Buy Wenniger et al., 2023).</p>
<p>Review Scores: For the available data on doubleblind peer-reviews, each paper is associated with multiple reviews, which can vary in format depending on the conference the paper is submitted to.Reviews often include an overall decision, a review score, and evaluations of specific aspects of the paper, such as clarity and impact.We map all the review data to our review data model (Figure 7 in Appendix A).From the available scores, we predict the average overall review score and impact score.</p>
<p>More complex models are possible.However, we found larger models to overfit on the training set and not generalize to the more recent papers of the test set despite applying common regularization techniques (Srivastava et al., 2014).Further training details are described in Appendix B.</p>
<p>Datasets</p>
<p>Our work builds on the widely used ACL-OCL dataset (Rohatgi et al., 2023), enriched with updated citation counts, title and abstract of references and annotated research hypotheses.We created to the best of our knowledge largest dataset of OpenReview submissions that exists and aug- mented submissions with the same metadata as the ACL-OCL dataset7 .</p>
<p>OpenReview: OpenReview8 serves as a valuable resource for linking scientific paper submissions with their corresponding peer-review assessments and has been a key source for datasets used in studies of the peer-review process (Staudinger et al., 2024).However, a significant challenge lies in the lack of a unified format of reviews from different venues.For instance, while many workshops include only an overall score, ICLR-2023 includes five distinct fields, such as clarity and novelty (see Table 8 in Appendix C).To address this, we developed a unified data model for reviews and manually mapped the fields from each venue to our standardized schema (see Table 9 in Appendix C).</p>
<p>For each submission, we processed the PDF documents using GROBID9 .To flexibly condition on different parts of an academic paper we train a section classifier that takes a paragraph of an academic text as input and classifies it as introduction, background, methodology, experiments &amp; results or conclusion.More details on the training of the section classifier can be found in Appendix D. Each submission was annotated with its research hypothesis using prompts to GPT-3.5-turbo.To assess annotation quality, we ask first authors to evaluate the annotations for their respective papers.More details on the annotation process and the results from the survey can be found in Appendix E.</p>
<p>Additionally, we retrieved title and abstract of the references of all submissions and citation counts for accepted submissions from Semantic Scholar10 .Summary statistics of the dataset are provided in Table 10 in Appendix C. While all reviews are licensed under CC BY 4.0, not all submissions are accompanied by a license11 .To ensure compliance, we publicly release the full dataset (excluding submissions) and provide access to submissions that have a CC BY 4.0 license.The complete dataset, including submissions, is accessible via a script from Huggingface.Details on the dataset creation process can be found in Appendix C.</p>
<p>ACL-OCL: For the ACL-OCL dataset (Rohatgi et al., 2023), we update the citation scores, retrieve the title and abstract of the references and label the sections of the already provided GROBID parses and label each sample with its corresponding research hypothesis.</p>
<p>Experiments</p>
<p>First we study the relation between citation counts and review scores to understand how interchangeable these metrics are.Then we discuss the results for both score prediction tasks and compare the performance of simple score predictors with LLM based reviewers.Lastly, we provide a qualitative analysis to interpret the insights gained from the score prediction models.</p>
<p>Citation Count vs. Review Scores</p>
<p>We analyse the correlation between different review score dimensions and the correlation between citation counts and review scores.All review dimensions, except reviewer confidence, exhibit a positive correlation with the final score (see Figure 2).This indicates that reviewers, on average, do not exhibit varying levels of confidence when assigning high or low overall scores.Among the dimensions, impact and clarity show the strongest correlations with the final score.These results align partially with the findings of Kang et al. (2018), who analysed ACL 2017 reviews and identified clarity as a key factor, though their study reported a weaker correlation for impact compared to our findings.</p>
<p>The correlation between review scores and the logarithm of average citations per month across different venues is presented in Table 1.We analyze only venues from before 2024, allowing papers to have been published for at least one year as early citation counts exhibit high variance (Zhang and Wu, 2024).Overall, we observe a weak positive correlation for most venues, with the exception of NeurIPS, where the correlation is negligible.For broad-topic conferences like ICLR and NeurIPS, a potential confounding factor is the varying popularity of different fields, which can influence citation counts regardless of the quality of the work.When separating ICLR-2023 papers by their authorprovided field, we find mixed results: the correlation strengthens for some fields but disappears entirely for others (see Table 18  There only seems to be a weak overlap of the factors influencing citation count and review scores.Neither review scores nor citation counts alone can be considered definitive measures of scientific quality.Both metrics are influenced by external factors, such as author popularity or reviewer-field fit, that are unrelated to the intrinsic quality of the work.However, citation counts have the advantage of being easier to collect on a larger scale, making it easier to train models of larger scale which can be leveraged as an evaluation metric.</p>
<p>Score Prediction</p>
<p>Citation Score Prediction: In this section, we investigate the task of predicting the log average citations per month using various paper representations and contexts.Specifically, we conduct experiments to evaluate the predictive performance of different section types as paper representations, comparing them to representations based on Title + Abstract and Hypothesis.The results on the ACL-OCL dataset are presented in  tion scores achieve similar Spearman correlations with the ground-truth data.Contexts containing result-related information demonstrate higher prediction accuracy, with the most notable difference observed when comparing the Title + Abstract context to the Hypothesis context.Citation counts can still be predicted solely based on a paper's research hypothesis better than random.These results are in line with the intuition that empirical work will be cited more often if the results are more impressive.</p>
<p>Next, we train the context models and rerun the title and abstract baseline with an empty context to isolate the performance change caused by the architectural adjustment.The results show marginal benefits from training on complete paper information or adding the titles and abstracts of all references as context (see Figure 3).A likely reason for the limited improvement is that most content-related variability in citation scores is already captured by the title and abstract.Remaining differences may stem from external factors, such as the authors' networks.Additionally, the small dataset size constrains training to simpler models, as larger models tend to overfit and fail to generalize.Expanding datasets and utilizing full-text models could improve comparison accuracy in future studies.</p>
<p>Review Score Prediction: For review score prediction, our findings indicate that conferenceagnostic prediction is challenging.On the full OpenReview dataset, the model's performance is no better than random guessing (see Table 3).Two factors likely contribute to this difficulty.First, reviewing standards differ across conferences, meaning that even after normalization, the same paper could receive a score of 0.6 in one venue and 0.8 in another.Second, the broader variability in topics between conferences, as opposed to within a single conference, makes comparisons more dif- ficult.Notably, venue-specific review score prediction models achieve a comparison accuracy of approximately 60% for NeurIPS and ICLR.</p>
<p>A consistent trend across all dataset subsets is that predicting citation counts is easier than predicting review scores, resulting in higher comparison accuracies.As observed with the ACL-OCL dataset, predictions based on title and abstract perform better than those based on the research hypothesis.The comparison accuracy for the citation score is higher on the ACL-OCL dataset, one potential reason is the increased amount of data from a less broad range of topics.To check whether distinguishing by topics improves prediction performance on the ICLR subset we ran an latent dirichlet allocation (LDA) to separate all ICLR submissions into different topics.More details on the topic model can be found in Appendix G.We trained pairwise comparison score models for the five most frequent topics.From the results in   evident that training separate prediction models per latent topic does not improve comparison accuracy.</p>
<p>Comparisons with LLM and Human Reviews</p>
<p>To evaluate our review score prediction models against both LLM-based and human reviewers, we subsampled 200 papers from the test set of ICLR 2024 and NeurIPS 2024 respectively.We first tested the LLM-based reviewer from Si et al. (2024), which takes two papers as input and predicts which one is more likely to receive a higher review score.Additionally, we apply our trained review score prediction models that take title and abstract as input to the same subsets.As shown in Table 5, our simple prediction mod-els, outperform the pairwise comparison accuracy of the LLM reviewer for both ICLR and NeurIPS.Two key differences exist compared to the original setup in Si et al. (2024): (1) the LLM in the original work evaluated project proposals rather than full papers, and (2) the reviewed papers in the original setup were restricted to LLM-related topics, potentially simplifying comparisons.Despite these differences, the LLM-based reviewer demonstrates comparable performance to the original study.</p>
<p>Further we compare the review score prediction model with the Sakana reviewer (Lu et al., 2024).In Figure 4, the relation between mean review score and predicted review scores are visualized.To compare against human reviewing performance, we exclude for each submission that has at least three reviews one review, and plot the relation between the single review score and the mean of the other review scores.The review score prediction model has a mean Pearson correlation over five random seeds of 0.330 ± 0.030 which is higher than the correlation of the Sakana reviewer of 0.161, but lower than the human reviewers correlation of 0.412 ± 0.044.Since results depend on which reviewer is sampled as a single reviewer, the human reviewer baseline is ran over five random seeds as well.Additionally, we can repeat the human reviewer consistency analysis for the whole ICLR</p>
<p>Reviewer</p>
<p>Pairwise  and OpenReview dataset.The mean correlation is 0.504±0.004and 0.475±0.002respectively, again showing that human reviews show a higher consistency than LLM reviews with human reviews.</p>
<p>In Table 19 (Appendix F), we qualitatively compare a review generated by the Sakana LLM reviewer with one written by a human reviewer for a ICLR-2024 submission.The LLM review highlights only generic strengths and weaknesses, while the human review provides detailed insights into the proposed methodology and its connections to the related work.This comparison illustrates that, despite a weak correlation in assigned scores, the LLM reviewer does not seem to base the scores on a deep semantic understanding of the paper.</p>
<p>Qualitative Analysis</p>
<p>To gain deeper insights into the features used by the citation and review score prediction models, we compute approximate Shapley values (Lundberg and Lee, 2017) for models trained on the ICLR subset of the dataset.Our analysis includes a comparison between review score and citation score prediction models, as well as an evaluation of mod-els that use research hypothesis versus title and abstract as input.Visualizations of Shapley values for example inputs are provided in Appendix H. Comparing models using titles and abstracts to those based on research hypotheses shows that the former leverage result-oriented information.For example, phrases like "sub to superhuman performance" or "a speedup compared to state-of-the-art systems" consistently have high positive Shapley values.Models based on research hypotheses focus more on individual methods (e.g., NoisyNet, GAN) or topics (e.g., exploration in deep reinforcement learning, generative modeling).When comparing citation score and review score models, some phrases (e.g., "efficient exploration," "adversarial objectives") contribute oppositely to each score.</p>
<p>Conclusion</p>
<p>Our study highlights the potential of citation and review score prediction models as automatic evaluation metrics, demonstrating their alignment with human reviews and their improved performance compared to LLM-based review systems.However, human reviews are still more consistent than review score predictions with human review scores.The lack of a standardized review format across venues complicates comparisons across venues.Further, venues often only make submissions partially (NeurIPS) or not at all (ICML) available.Predicting citation scores is more viable than predicting review scores.Since content based citation prediction has not yet been deeply explored there are still questions to address related to scalability via larger datasets and the most effective target scores and input features.Moreover, excluding tables and figures potentially limits the potential of citation score prediction models (Hirako et al., 2024).</p>
<p>Limitations</p>
<p>While our score prediction models outperform LLM-based review systems, they still have limitations.Current models utilize only a small portion of the full paper's information, leaving room for improvement in incorporating full paper information and references.Limited dataset sizes may hinder current models' ability to leverage more information effectively.Furthermore, both citation count and review scores are imperfect proxies for scientific quality as they are influenced by factors such as the author's network, raising the open question of how accurately they can be predicted from paper content alone.</p>
<p>B Training Details</p>
<p>No-context model: The no-context model consists of an MLP with 256 hidden units taking the SPECTER2 embeddings of size 768 as input.</p>
<p>The training parameters are listed in Table 6.We apply dropout of 0.3 to the SPECTER2 embeddings.We conduct a grid search over learning rates [0.0001, 0.001, 0.0005, 0.00005] and dropout rates [0, 0.1, 0.2, 0.3, 0.4, 0.5], selecting the bestperforming combination based on the validation set.</p>
<p>Training is run for 100 epochs, and the checkpoint with the lowest validation loss is then evaluated on the test set.</p>
<p>Context Model:</p>
<p>The context model consists of a transformer encoder layer (Vaswani et al., 2017) with one head, ReLU activation (Nair and Hinton, 2010), dropout of 0.3 and hidden units of size 1024.The paper representation and context is concatenated and processed via the encoder layer.The processed paper representation embedding is then passed to the same MLP as in the no-context model.Hyperparameter search and training are done in the same way as for the no-context model.</p>
<p>C OpenReview Dataset</p>
<p>In the following, we outline the collection process for the OpenReview dataset and present key dataset statistics.The dataset comprises submissions from OpenReview12 for which reviews and decisions are publicly accessible.The extent of accessibility varies by venue.For instance, ICML does not provide reviews or decisions, NeurIPS includes reviews only for accepted papers, and ICLR offers full access to all submissions.As illustrated in Figure 8, a significant proportion of the dataset originates from ICLR, followed by NeurIPS.For each venue, we manually map the review fields to the proposed review schema (see Table 9) and extract both the decision and the decision text.The PDF document associated with each submission is downloaded and parsed using GROBID 13 .The parsed sections are then classified into categories from the paper schema (see Figure 5) using our section classifier (see Appendix D).Additionally, we annotate each submission with a research hypothesis (see Appendix E).For accepted submissions, we retrieve the citation count and influential citation count of the corresponding published papers from Semantic Scholar14 , along with their references.For rejected submissions, references are extracted from the parsed GROBID output and matched to entries in the Semantic Scholar corpus to obtain additional information.A summary of the dataset statistics is provided in Table 10.OpenReview continues to grow in popularity, and we plan to update the dataset regularly to provide a growing dataset of papers with corresponding reviews in a unified format.</p>
<p>Review attribute</p>
<p>D Section Classification</p>
<p>Academic papers typically follow a standardized structure.To enable flexible use of different sections of a paper as input for our prediction models, we aim to map each of them to one of the section types outlined in the Paper Data Model (Figure 5).This requires training a section classifier.</p>
<p>To construct the necessary dataset, we first define synonyms for section types commonly found as headings in academic papers (see Table 12).We then process the OpenReview dataset and ACL-OCL dataset, identifying sections where the heading matches one of the predefined synonyms.These sections are subsequently added to the section classification dataset along with their corresponding labels15 .An overview of the frequency of the different section types in the two datasets can be found in Table 13.The section classifier takes a paragraph of text as input and assigns one of the five section types from the paper data model to it.First, the paragraph is split into sentences with NLTK's sentence tokenizer (Bird, 2006)  obtained via Specter2 by averaging over all token embeddings for the sentence.The sequence of sentence embeddings are then processed via two transformer encoder layers with dropout 0.3, hidden dimension of 1024 and eight heads.The processed embeddings are then averaged and passed through a linear layer to obtain logits.The model is trained by minimizing the cross entropy loss.The dataset is split into a training, validation and test set where the training set makes up 70% of the dataset and the validation and test set 15% each.In Table 11 one can find relevant training parameters.The accuracy over 4 random seeds of the classifier on the test set is 0.921 ± 0.006 for the ACL-OCL dataset and 0.93 ± 0.1 for the OpenReview dataset.</p>
<p>E Research Hypothesis Annotation</p>
<p>To understand whether it is possible to predict citation and review scores based on the research hypothesis of a paper alone, we annotate the papers in the ACL-OCL and OpenReview dataset using gpt-3.5-turbo.Following Baek et al. (2024)  as part of the research hypothesis.The one-shot prompt used to query the LLM can be seen in Table 14.No more advanced LLM is used for annotation to restrict costs, as the context windows are large due to the content of the scientific papers.The number of annotation examples is restricted to 1 to reduce the context size, as the context window of gpt-3.5-turbo is restricted to 16000 tokens.</p>
<p>To verify that our research hypothesis annotation tool does indeed capture the main ideas of the papers, we asked first authors of academic papers to rate the quality of the annotated research hypothesis.The annotation quality is rated on the dimensions of correctness, precision and completeness for the problem as well as the solution.The 5 point Likert scales for each dimension is displayed in Table 15.The figure 9 shows the Google Form used to conduct the survey.The survey was filled out by 13 first authors (PhD students and assistant professors) rating a total of 32 research hypotheses.The results are displayed in Table 16.The results show that the correctness of the annotated research hypotheses is overall satisfactory, however the completeness can be improved.In Table 17, some examples of the research hypothesis and improved versions written by first authors can be found.It is important to note that a potential source of error comes from mistakes in GROBID PDF parsing step that is used to extract the paper text used to condition the paper text.You are a PhD student tasked to annotate research papers with the hypothesis they investigate.You will be provided with infos about the paper.Your task is to extract the research hypothesis from this provided text.Requirements:</p>
<p>-Clarity: Ensure the hypothesis is clearly stated and understandable without additional context.</p>
<p>-Completeness: The hypothesis should be self-contained, including all necessary components such as the variables involved and the expected relationship or outcome.</p>
<p>-Terminology: Use precise and field-specific terminology that a research scientist in the relevant or adjacent field would understand.</p>
<p>-Conciseness: Keep the hypothesis one to two sentences long, avoiding unnecessary details or jargon.</p>
<p>Examples: Paper 1: {example_paper_text} Hypothesis: {example_hypothesis} Format:</p>
<p>-Problem: The problem that the paper is addressing -Solution: The solution that the paper is proposing User Message Annotate the following paper with its hypothesis: {paper_text} The solution statement has significant errors or misconceptions about how the problem is solved, but some aspects of the approach are correct.3: The solution statement is mostly correct but contains a few incorrect or misleading details about the approach or method used to solve the problem.4: The solution statement is almost entirely correct, with only minor inaccuracies or misinterpretations regarding the proposed solution.5: The solution statement is entirely accurate, with no errors or misrepresentations regarding the approach, methods, or steps taken to solve the problem.</p>
<p>S. Precision 1: The solution statement is vague and lacks specificity, making it difficult to understand the proposed approach or how it addresses the problem.2: The solution statement is somewhat clear but lacks key specifics, leaving room for misinterpretation about how the solution addresses the problem.3: The solution statement is fairly clear but could benefit from more specificity regarding key methods, steps, or conditions that explain how the problem is being solved.4: The solution statement is clear and specific, but there are a few areas where it could be refined for greater precision or clarity regarding the approach.5: The solution statement is very specific and leaves no room for ambiguity, precisely defining the method, scope, and key elements of the proposed solution.</p>
<p>Continued on next page... 1: The solution statement is vague, missing core details, and does not explain how the problem is addressed or resolved.2: The solution statement captures some aspects of the approach but omits critical elements such as the specific methods, techniques, or constraints.3: The solution statement explains the general approach but lacks clarity on important technical aspects, such as specific steps, tools, or techniques needed to solve the problem.4: The solution statement is thorough and covers most key aspects of the approach, including methods, scope, and constraints, with only minor details missing.5: The solution statement is fully articulated, covering all relevant aspects, including the method, tools, constraints, and implications, leaving no critical information out.</p>
<p>F Additional Results</p>
<p>Here, we present additional resutls.In Table 18 one can see the correlation between citation count and review score for different topics of the venue ICLR 2023.Problem: The paper aims to investigate the impact of common ground in social dialogues on the resolution of co-reference, specifically focusing on the distinction between "inner circle" and "outer circle" references in conversations.(Precision:4, Correctness:4, Completeness:3) Solution: The hypothesis is that resolving references to well-known "inner circle" individuals is more challenging compared to lesser-known "outer circle" individuals in social dialogues, and that training models on preceding data may not effectively acquire common ground knowledge for inner circle references but can improve performance for outer circle mentions.(Precision:3, Correctness:2, Completeness:3)</p>
<p>Examples</p>
<p>First Author Correction:</p>
<p>Problem: The paper aims to investigate the impact of common ground in social dialogues on co-reference resolution, specifically focusing on the distinction between "inner circle" and "outer circle" references in conversations.Solution: The hypothesis is that resolving references to well-known "inner circle" individuals is more challenging compared to lesser-known "outer circle" individuals in social dialogues, and that training models on preceding data may help in acquiring common ground knowledge for inner circle references.To test this, a data set of social dialogue is analysed on referring expressions for 'inner circle' and 'outer circle', and a co-reference resolution model is trained on preceding data and its performance analysed.</p>
<p>G Topic Model</p>
<p>Citation prediction has been shown to benefit from classifiers trained for specific research fields (Zhang and Wu, 2024) instead of a single classifier for all data.While the ACL-OCL dataset is already restricted to the field of Computational Linguistics, the OpenReview dataset covers a broader set of topics.Submissions often include a Field of Study attribute.However, the Field of Study attribute varies widely, containing broad categories like "Computer Science" to more narrow research fields (see Table 21).An overview of the distribution of values for the field is presented in Figure 10.Since the ACL-OCL dataset already focuses on a narrow domain ("Computational Linguistics") we do not perform additional topic classification.To test whether topic labels can improve comparison accuracy of pairwise score prediction models, we label all ICLR submissions with a topic label.</p>
<p>Various classification systems for scientific papers exist (Zhang and Wu, 2024), and different models have been explored (Mendoza et al., 2022).Scholarly databases like Semantic Scholar and ArXiv often provide coarse-grained classifications.For example, almost all OpenReview submissions are labeled as Computer Science by Semantic Scholar, while ArXiv categorizes them as cs.AI (Artificial Intelligence).Other systems, such as Thomson Reuters' Web of Science and CSRankings 16 , face similar limitations.Therefore, we adopt an unsupervised classification approach using Latent Dirichlet Allocation (LDA) (Blei et al., 2003).</p>
<p>We combine titles and abstracts as a unique string, and we pre-process the obtained texts using a common pipeline employing the gensim ( Řehůřek and Sojka, 2010) and nltk (Bird, 2006) libraries: we tokenize the texts and lemmatize the results.In addition, we also generate bigrams and trigrams, as they are widely used in scientific style.For the number of topics we choose the same number as in ICLR-2023 which is 13.</p>
<p>H Qualitative Analysis</p>
<p>For the qualitative analysis we approximate Shapley values by determining Owen values (Owen, 1977)   using title and abstracts (see Figure 11) and hypotheses (see Figure 12) as well as the citation prediction models using title and abstracts (see Figure 13) and hypotheses (see Figure 14).</p>
<p>Figure 1 :
1
Figure 1: Architecture of the context model in case of the full paper representation.The paper representation is green (TA=title abstract) and the context representation is blue (RW=Related Work, M=Methodology, E&amp;R=Experiments and Results, C=Conclusion).</p>
<p>Figure 2 :
2
Figure 2: Pearson correlation heat map for the different dimensions of our unified review data model.</p>
<p>Figure 3 :
3
Figure 3: Spearman correlations for context-based models applied to both the regression and pairwise comparison tasks.Results are averaged over five seeds, with error bars representing the standard deviations.</p>
<p>Figure 4 :
4
Figure4: Scatter plots of predicted review scores and groundtruth review scores on a subset of the test set of ICLR-2024 for the Sakana reviewer, the review score prediction model and human reviews.For human reviews, we randomly select a review as the predicted score and average over the rest.</p>
<p>Figure 5 :
5
Figure 5: Schematic overview of the scientific Paper object.The Field of Study is a list of keywords that are part of the OpenReview submission, where the potential values depend on the venue.Number of citations and influential citations are retrieved from Semantic Scholar.</p>
<p>Figure 9 :
9
Figure 9: Example of the Google Form used to collect the survey data.Parts of the form is blacked out to guarantee annonymity.</p>
<p>TextsExample 1 LLM (gpt-3.5.-turbo):Problem:The paper aims to investigate how conventions develop in taskbased interactions, specifically focusing on how common ground influences the formation of conventions in the presence of new information, such as outer circle characters.(Precision:4, Correctness:5, Completeness:4) Solution: The paper proposes the SPOTTER framework, a gameplay framework for task-based interaction, to study how referring expressions to known 'inner circle' (InC) referents and unknown 'outer circle' (OutC) referents evolve over time, aiming to understand how conventions arise and how they are influenced by changing contexts.(Precision:5, Correctness:5, Completeness:5)First Author Correction:Problem: The paper aims to investigate how conventions develop in taskbased interactions in Human-Robot Interaction, specifically focusing on how common ground influences the formation of conventions in the presence of new information, such as outer circle characters.Solution: The paper proposes the SPOTTER framework, a gameplay framework for task-based interaction, to study how referring expressions to known 'inner circle' (InC) referents and unknown 'outer circle' (OutC) referents evolve over time, aiming to understand how conventions arise and how they are influenced by changing contexts.Example 2 LLM (gpt-3.5.-turbo):</p>
<p>Figure 10 :
10
Figure 10: Frequency of the 20 most frequent field of studies in OpenReview.</p>
<p>Figure 11 :
11
Figure 11: Illustrative Shapley values for titles and abstracts in the review score prediction model trained on the ICLR subset of OpenReview.</p>
<p>Figure 12 :
12
Figure 12: Illustrative Shapley values for research hypotheses in the review score prediction model trained on the ICLR subset of OpenReview.</p>
<p>Figure 13 :
13
Figure 13: Illustrative Shapley values for titles and abstracts in the citation score prediction model trained on the ICLR subset of OpenReview.</p>
<p>Figure 14 :
14
Figure 14: Illustrative Shapley values for research hypotheses in the citation score prediction model trained on the ICLR subset of OpenReview.</p>
<p>in Appendix F).
Dataset# SamplesρAll150020.193All-ICLR49200.148ICLR -202315070.168ICLR -202210720.175ICLR -20218370.163ICLR -20206740.120ICLR -20194950.190ICLR -20183350.184NeurIPS -202329630.103NeurIPS -202225530.085NeurIPS -202122850.085
Table1: Pearson correlation between the log average citation per month and mean overall review scores for different subsets of the OpenReview dataset for accepted papers with at least one citation.</p>
<p>Table 2 .
2
Both learning to rank and learning to predict exact cita-
ContextPairwise Comparison Accuracy ρ sRegression L1-Distance ρ sTitle + Abstract0.665(0.010) 0.481(0.026) 0.921(0.001) 0.498(0.002)Hypothesis0.615(0.008)0.339(0.024)1.003(0.002)0.366(0.001)Introduction0.655(0.006)0.460(0.016)0.943(0.002)0.452(0.002)Related Work0.631(0.008)0.386(0.022)0.955(0.001)0.393(0.002)Methodology0.634(0.010)0.397(0.027)0.981(0.001)0.399(0.002)Experiments &amp; Results 0.651(0.004)0.446(0.012)0.946(0.001)0.449(0.001)Conclusion0.647(0.005)0.439(0.014)0.944(0.001)0.431(0.002)</p>
<p>Table 2 :
2
Performance of the citation prediction models for the pairwise comparisons and the prediction of the log average number of citation per month.All models are trained over five random seeds and mean results are presented with standard deviation in brackets.The best performing model is displayed in bold.</p>
<p>Table 4
4, it is</p>
<p>Table 3 :
3
Prediction results on the OpenReview dataset for the different scores and subsets that the dataset contains.Models are run over five random seeds and the mean results are presented with standard deviation in brackets (TA=Title and Abstract, H=Hypothesis, CC=Citation Count).The best performing model based on the Spearman rank correlation for each subset of the dataset is indicated in bold.</p>
<h1>SamplesPairwise ComparisonAccuracyρ s50680.580(0.002) 0.236(0.006)55420.576(0.003) 0.224(0.008)16240.599(0.003) 0.281(0.006)9620.565(0.006) 0.182(0.016)7070.597(0.002) 0.285(0.005)</h1>
<p>Table 4 :
4
Pairwise review score comparison accuracy for different topic subsets of the ICLR dataset, with topic labels for individual submissions generated using LDA.</p>
<p>Table 5 :
5
Comparison accuracy and spearman correlation (ρ</p>
<p>s ) for LLM-based and review-score comparison models on subsets of ICLR and NeurIPS test sets.Review score predictions are averaged over 5 random seeds, with results shown as mean (standard deviation).</p>
<p>Table 6 :
6
Training parameters for the no context models for both citation and review score prediction on the OpenReview and ACL-OCL dataset.
ReferenceTitleAbstractArXiv IDSemantic Scholar Corpus IDIntentIsInfluentialFigure 6: Schematic overview of the Reference object.The Intent indicates the section of the scientific paperwhere the reference appears (e.g., introduction, method-ology), while isInfluential is a boolean value specifyingwhether the reference played a significant role in thecreation of the paper. Both information come from Se-mantic Scholar.ReviewTextReviewScoreConfidenceNoveltyCorrectnessClarityImpactRepoducibilityEthicsFigure 7: Schematic overview of the review object. TheTextReview component concatenates all textual elementsof the review, such as the summary, strengths, weak-nesses, and questions. Except for the Ethics text field,all other attributes of the review are represented as nor-malized floats ranging from 0 to 1.ParameterValueLearning rate0.00005Dropout0.3Epochs100Batch Size256OptimizerAdamHardwareNVIDIA GeForce RTX 3080 (10GB)Training Time max. 20min</p>
<p>Table 7 :
7
Training parameters for the context model.</p>
<p>Table 8 :
8
Review fields for the ICLR-2023 venue.
FieldRecommendationConfidenceCorrectnessEmpirical Novelty and SignificanceTechnical Novelty and SignificanceFlag For Ethics Review:Summary Of The Review:Clarity, Quality, Novelty And ReproducibilityStrength And WeaknessesSummary Of The Paper</p>
<p>Table 10 :
10
Overview of the types of submissions (Sub.) and reviews (Rev.) and the availability of additional metadata.
OpenReview review fieldsoverall ratingratingevaluationQ6 Overall scoreOverall scorerecommended decisionOverall Scorescoreoverall evaluation review ratingresultsscorepreliminary ratingrecommendationworkshop ratingcustom ratingoverall evaluationexperience assessmentReviewer expertiseconfidencereview assessment: thoroughness in paper readingconfidencereviewer's confidence reviewer expertiseConfidenceQ8 Confidence in your scorereview confidenceworkshop confidencetechnical novelty and significanceoriginalitynoveltyempirical novelty and significancenoveltyQ2(1) Originality/Noveltycorrectnesssoundnessreview assessment: checking correctness of experimentsQ2(3) Correctness/Technical qualityreview assessment: checking correctness of derivations and theorycorrectnesstechnical rigorQ2(4) Quality of experiments (Optional)technical quality and correctness ratingscholarshiptechnical qualitylitreviewpresentationclarityclarityclarity of presentationQ2(6) Clarity of writingclarity ratingContinued on next page...</p>
<p>. Sentence embeddings are
Number of submissions0 5000 10000 15000 20000ICLR NeurIPS ICML acmmm auai ACM MIDL IEEE colmweb robot-learning AKBC NoDaLiDa icaps-conference thecvf MICCAI KDD AAAI aclweb EMNLP Interspeech CPAL NLDL automl iscaconf WBIR Conference humanrobotinteraction roboticsfoundation ECMLPKDD swsa SEMANTiCS MLSysFigure 8: Number of submissions per conferenceParameterValueLearning Rate 0.0001Batch Size128# Epochs20HardwareNVIDIA GeForce RTX 3080 (10GB)Training Time 25min</p>
<p>Table 11 :
11
Training parameters for the section classifier training.</p>
<p>Table 12 :
12
we model a research hypothesis h = [p, s] as a problem and a methodology/solution to solve that problem represented via natural language.Unlike Baek et al.Introduction Introduction Section synonyms used to collect a training dataset for the section classifier (E&amp;R = Experiments and Results).
BackgroundBackgroundRelated WorkHistorical ReviewMethodologyMethodologyMethod AlgorithmPropertiesExperimentsResultsExperiments and ResultsE&amp;RExperimental Design Empirical EvaluationExperiments and AnalysisAblation StudiesEvaluationConclusionConclusion &amp; DiscussionDiscussion and ConclusionsConclusionConclusion and OutlookFurther WorkDiscussions andFuture DirectionsSections# OpenReview # ACL-OCLIntroduction3948756223Background2052123412Methodology66564302E&amp;R1880128157Conclusion2281128157</p>
<p>Table 13 :
13
Number of samples in the section classification dataset resulting from matching section headings of the GROBID PDF parses to the section type synonyms for the OpenReview and ACL-OCL dataset (E&amp;R=Experiments and Results).</p>
<p>Table 14 :
14
Few-shot prompt template for research hypothesis annotation.The problem statement is fundamentally incorrect or misrepresents the actual problem or context.2:Theproblem statement has significant errors or misconceptions about the nature of the problem but some aspects are correct 3: The problem statement is mostly correct but contains a few incorrect or misleading details about the nature or scope of the problem.4:Theproblem statement is almost entirely correct, with only minor inaccuracies or misinterpretations.5:Theproblem statement is entirely accurate, with no errors or misrepresentations regarding the nature or scope of the problem.PS Precision 1: The problem statement is vague and lacks specificity, making it difficult to understand the exact nature of the problem.2:Theproblem statement is somewhat clear but lacks key specifics, leaving room for misinterpretation.3:Theproblem statement is fairly clear but could benefit from more specificity regarding key variables, constraints, or conditions.4:The problem statement is clear and specific, but there are a few areas where it could be refined for greater precision.5:The problem statement is very specific and leaves no room for ambiguity, precisely defining the scope, constraints, and core elements of the problem.Continued on next page...The problem statement is vague, missing core details, and does not define the central challenge or its scope.2: The problem statement captures some aspects of the challenge but omits critical elements such as specific requirements, constraints, or the broader
Dimension5-point Likert scalePS Correct.1:</p>
<p>Table 15 :
15
The 5-point Likert scales used to evaluate the quality of each component of the annotated research hypothesis (PS = Problem Statement, S.=Solution, Compl.=Completeness,Correct.=Correctness).
CriteriaRatingProblem Correctness4.125(1.083)Problem Precision3.688(1.158)Problem Completeness 3.563(1.144)Solution Correctness4.125(1.293)Solution Precision3.906(1.042)Solution Completeness 3.625(1.192)</p>
<p>Table 16 :
16
Average ratings of the annotated research hypotheses on the dimensions of correctness, precision, and completeness, measured using a 5-point Likert scale.Standard deviations are provided in parentheses.</p>
<p>Table 17 :
17
Examples of annotated research hypotheses, their ratings and the corrected versions by the corresponding first authors.The highlighted text in red describes the parts that is changed by the first author
Field of Study# SamplesρAll15070.168Applications1860.231Deep Learning and representational learning3420.236General Machine Learning570.190Generative models630.210ML for Sciences59-0.135Reinforcement Learning1380.138Social Aspects of ML930.198Theory930.125Unsupervised and Self-supervised learning71-0.009</p>
<p>Table 18 :
18
Pearson correlation between the log average citation per month and the mean overall review score for accepted papers with at least one citation for the ICLR-2023 venue for the different fields of study.Fields of study with less than 50 entries are left out.
Review FieldSakana ReviewHuman ReviewSummaryThe paper introduces a FairThis paper addresses human-related bias in text-to-Mapping method to mitigateimage diffusion models, and resolves the issue bybias in text-guided diffusionproposing a novel fair mapping module which out-models, focusing on generat-puts a fair text embedding. Such a module can being human-related images. Thetrained on top of frozen pre-trained text encoder, andmethod is model-agnostic andinserting the module during sampling successfullylightweight, using a linear map-mitigates textual bias. Training the fair module in-ping network to address bi-volves two loss terms: (i) text consistency loss, whichases. A novel fairness evalu-preserves semantic coherence, and (ii) fair distanceation metric is proposed, andpenalty, which brings output embeddings within dif-experiments demonstrate theferent sensitive groups close together. Further, themethod's effectiveness.authors propose a novel evaluation metric, FairScore,which also plans to achieve the conditional indepen-dence of the text prompt and sensitive group infor-mation.WeaknessesModerate originality as it buildsAlthough the paper covers a good amount of rele-on existing concepts.vant previous studies, the paper lacks baseline ex-Experimental evaluation is lim-periments. For example, despite [1] focus on fair-ited and lacks comprehensiveguidance while this work focus on pluggable map-comparison with state-of-the-ping module, the authors can calculate FairScore andart methods.compare w.r.t. training time, overhead memory, etc.Insufficient analysis of compu-While the unfairness is largely resolved through thetational complexity and scala-proposed mapping module, such a result may notbility.come at a surprise since FairScore and the employedCertain sections, particularlyfairness loss term are quite similar.the methodology, could beThe authors note that a detector network is employedclearer.to identify predefined sensitive keywords in the inputprompts. There is no additional detailed explanationabout the detector network.This method explicitly needs a labeled dataset tomitigate the demographic bias in diffusion models.However, in real-world scenarios, it may be chal-lenging to identify and address all potential typesof bias comprehensively. Further, there are remain-ing questions regarding whether it is feasible to (i)simultaneously eliminate multiple types of bias or(ii) sequentially address multiple biases without neg-atively impacting performance. If such challengescannot be properly addressed, it would incur a sig-nificant amount of training time to erase all types ofbiases, and heavy memory cost to save all mappingmodules corresponding to each bias type.Continued on next page...</p>
<p>Table 19 :
19
Comparison of human review and review produced by Sakana on ICLR-2024 submission (S.=Soundness,P.=Presentation,C.=Correctness,CF.=Confidence).</p>
<p>via Partition Shap 17 on the training set.We visualise the approximate Shapley values for three examples for the review score prediction models 16 https://csrankings.org/index?all&amp;us 17 https://shap.readthedocs.io/en/latest/generated/shap.PartitionExplainer.html
co m pu te r sc ie nc e m at he m at ic s re in fo rc em en t le ar ni ng de ep le ar ni ng la rg e la ng ua ge m od el s de ep le ar ni ng an d re pr es e re pr es en ta tio n le ar ni ng gr ap h ne ur al ne tw or ks di ff us io n m od el s ge ne ra tiv e m od el s fe de ra te d le ar ni ng se lf-su pe rv is ed le ar ni ng la rg e la ng ua ge m od el tr an sf or m er ge ne ra liz at io n m ac hi ne le ar ni ng in te rp re ta bi lit y op tim iz at io n ro bu ap pl ic at io ns st ne ss (e g,</p>
<h1>Samples Words 6244 datum, distribution, model, sample, training, generalization, use, show, method, prediction 5934 adversarial, model, attack, training, robustness, learning, datum, robust, privacy, client 2753 network, neural, function, deep, show, gradient, linear, learn, use, matrix 2271 graph, model, transformer, attention, task, performance, training, node, propose, layer 1888 causal, graph, variable, fairness, algorithm, game, tree, structure, effect, decision</h1>
<p>Table 20 :
20
The top 10 most important words for the five most frequent topic resulting from the LDA performed on ICLR submissions. .control theory, learning theory, algorithmic game theory) Social Aspects of Machine Learning (eg.AI safety, fairness, privacy, interpretability, human-AI interaction, ethics) Reinforcement Learning (eg.robotics, planning, hierarchical RL,decision and control) Probabilistic Methods (eg.variational inference, causal inference, Gaussian processes) Optimization (eg.convex and non-convex optimization) Neuroscience and Cognitive Science (e.g., neural coding, brain-computer interfaces) Machine Learning for Sciences (eg. biology, physics, health sciences, social sciences, climate sustainability) Infrastructure (eg.datasets, competitions, implementations, libraries) Generative models General Machine Learning (ie.none of the above) Deep Learning and representational learning Applications (eg.speech processing, computer vision, NLP)
Venue Field of StudyUnsupervised and Self-Supervised LearningTheory (egICLR2023</p>
<p>Table 21 :
21
Research fields available for authors to self-select when categorizing their submissions for ICLR 2023.</p>
<p>https://github.com/NikeHop/automatic_ scientific_quality_metrics
https://github.com/NikeHop/OpenReviewParser
https://f1000research.com/
https://peerj.com/
https://huggingface.co/allenai/specter2_base
https://huggingface.co/datasets/nhop/ scientific-quality-score-prediction
https://openreview.net/
https://github.com/kermitt2/grobid
https://www.semanticscholar.org/
https://openreview.net/legal/terms
https://openreview.net/
https://github.com/kermitt2/grobid
https://www.semanticscholar.org/product/api
https://huggingface.co/datasets/nhop/ academic-section-classification
AcknowledgementsThis research was (partially) funded by the Hybrid Intelligence Center, a 10-year programme funded by the Dutch Ministry of Education, Culture and Science through the Netherlands Organisation for Scientific Research, https://hybrid-intelligencecentre.nl.This work used the Dutch national einfrastructure with the support of the SURF Cooperative using grant no.EINF-9756.A Data ModelIn the following we present the full data model for scientific papers, reviews and references.
Researchagent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, 10.48550/ARXIV.2404.07738CoRR, abs/2404.077382024</p>
<p>Predicting the citations of scholarly paper. Xiaomei Bai, Fuli Zhang, Ivan Lee, 10.1016/J.JOI.2019.01.010J. Informetrics. 1312019</p>
<p>Peerassist: Leveraging on paper-review interactions to predict peer review decisions. Prabhat Kumar Bharti, Shashi Ranjan, Tirthankar Ghosal, 10.1007/978-3-030-91669-5_33Towards Open and Trustworthy Digital Societies -23rd International Conference on Asia-Pacific Digital Libraries, ICADL 2021, Virtual Event. Lecture Notes in Computer Science. Springer2021. December 1-3, 202113133Mayank Agrawal, and Asif Ekbal</p>
<p>NLTK: the natural language toolkit. Steven Bird, 10.3115/1225403.1225421ACL 2006, 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference. Sydney, AustraliaThe Association for Computer Linguistics2006. July 2006</p>
<p>Latent dirichlet allocation. David M Blei, Andrew Y Ng, Michael I Jordan, J. Mach. Learn. Res. 32003</p>
<p>Autonomous chemical research with large language models. A Daniil, Robert Boiko, Ben Macknight, Gabe Kline, Gomes, 10.1038/S41586-023-06792-0Nat. 62479922023</p>
<p>Augmenting large language models with chemistry tools. Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, Philippe Schwaller, 10.1038/S42256-024-00832-8Nat. Mac. Intell. 652024</p>
<p>MARG: multi-agent review generation for scientific papers. D' Mike, Tom Arcy, Larry Hope, Doug Birnbaum, Downey, 10.48550/ARXIV.2401.04259CoRR, abs/2401.042592024</p>
<p>Multischubert: Effective multimodal fusion for scholarly document quality prediction. Gideon Maillette De Buy Wenniger, Thomas Van Dongen, Lambert Schomaker, 10.48550/ARXIV.2308.07971CoRR, abs/2308.079712023</p>
<p>Palm-e: An embodied multimodal language model. Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Wenlong Yu, Yevgen Huang, Pierre Chebotar, Daniel Sermanet, Sergey Duckworth, Vincent Levine, Karol Vanhoucke, Marc Hausman, Klaus Toussaint, Andy Greff, Igor Zeng, Pete Mordatch, Florence, International Conference on Machine Learning, ICML 2023. Honolulu, Hawaii, USAPMLR2023. July 2023202</p>
<p>Nlpeer: A unified resource for the computational study of peer review. Nils Dycke, Ilia Kuznetsov, Iryna Gurevych, 10.18653/V1/2023.ACL-LONG.277Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023. July 9-14, 20231ACL 2023</p>
<p>Scaling rectified flow transformers for high-resolution image synthesis. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Forty-first International Conference on Machine Learning, ICML 2024. Vienna, Austria2024. July 21-27, 2024OpenReview.net</p>
<p>Enhancing the examination of obstacles in an automated peer review system. Gustavo , Lúcius Fernandes, Pedro O S Vaz De Melo, 10.1007/S00799-023-00382-1Int. J. Digit. Libr. 2522024</p>
<p>Models for predicting and explaining citation count of biomedical articles. Lawrence D Fu, Constantin F Aliferis, AMIA 2008. Washington, DC, USAAMIA2008. November 8-12, 2008</p>
<p>Citebench: A benchmark for scientific citation text generation. Martin Funkquist, Ilia Kuznetsov, Yufang Hou, Iryna Gurevych, 10.18653/V1/2023.EMNLP-MAIN.455Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Enabling large language models to generate text with citations. Tianyu Gao, Howard Yen, Jiatong Yu, Danqi Chen, 10.18653/V1/2023.EMNLP-MAIN.398Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Realistic citation count prediction task for newly published papers. Jun Hirako, Ryohei Sasano, Koichi Takeda, 10.18653/V1/2023.FINDINGS-EACL.84Findings of the Association for Computational Linguistics: EACL 2023. Dubrovnik, CroatiaAssociation for Computational Linguistics2023. May 2-6, 2023</p>
<p>Cimate: Citation count prediction effectively leveraging the main text. Jun Hirako, Ryohei Sasano, Koichi Takeda, 10.48550/ARXIV.2410.04404CoRR, abs/2410.044042024</p>
<p>Predicting citation count of Bioinformatics papers within four years of publication. Alfonso Ibáñez, Pedro Larrañaga, Concha Bielza, 10.1093/BIOINFORMATICS/BTP585Bioinform. 25242009</p>
<p>A dataset on malicious paper bidding in peer review. Steven Jecmen, Minji Yoon, Vincent Conitzer, B Nihar, Fei Shah, Fang, 10.1145/3543507.3583424Proceedings of the ACM Web Conference 2023, WWW 2023. the ACM Web Conference 2023, WWW 2023Austin, TX, USAACM2023. 30 April 2023 -4 May 2023</p>
<p>Predicting citation impact of research papers using GPT and other text embeddings. Adilson VitalJr, N Filipi, Osvaldo N Silva, Diego R OliveiraJr, Amancio, 10.48550/ARXIV.2407.19942CoRR, abs/2407.199422024</p>
<p>A dataset of peer reviews (peerread): Collection, insights and NLP applications. Dongyeop Kang, Waleed Ammar, Bhavana Dalvi, Madeleine Van Zuylen, Sebastian Kohlmeier, Eduard H Hovy, Roy Schwartz, 10.18653/V1/N18-1149Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018. Long Papers. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018Louisiana, USAAssociation for Computational Linguistics2018. June 1-6, 20181</p>
<p>Coauthor: Designing a human-ai collaborative writing dataset for exploring language model capabilities. Mina Lee, Percy Liang, Qian Yang, 10.1145/3491102.3502030CHI '22: CHI Conference on Human Factors in Computing Systems. New Orleans, LA, USAACM2022. 29 April 2022 -5 May 202238819</p>
<p>Mlr-copilot: Autonomous machine learning research based on large language models agents. Ruochen Li, Teerth Patel, Qingyun Wang, Xinya Du, 10.48550/ARXIV.2408.14033CoRR, abs/2408.140332024</p>
<p>A neural citation count prediction model based on peer review text. Siqing Li, Wayne Xin Zhao, Eddy Jing Yin, Ji-Rong Wen, 10.18653/V1/D19-1497Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019Hong Kong, ChinaAssociation for Computational Linguistics2019. November 3-7, 2019</p>
<p>Reviewergpt? an exploratory study on using large language models for paper reviewing. Ryan Liu, Nihar B Shah, 10.48550/ARXIV.2306.00622CoRR, abs/2306.006222023</p>
<p>The AI scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, 10.48550/ARXIV.2408.06292CoRR, abs/2408.062922024</p>
<p>A unified approach to interpreting model predictions. M Scott, Su-In Lundberg, Lee, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Long Beach, CA, USA2017. 2017. December 4-9, 2017</p>
<p>Benchmark for research theme classification of scholarly documents. E Óscar, Wojciech Mendoza, Alaa Kusa, Ronin El-Ebshihy, David Wu, Petr Pride, Drahomira Knoth, Florina Herrmannova, Gabriella Piroi, Allan Pasi, Hanbury, Proceedings of the Third Workshop on Scholarly Document Processing, SDP@COLING 2022. the Third Workshop on Scholarly Document Processing, SDP@COLING 2022Gyeongju, Republic of KoreaAssociation for Computational Linguistics2022. October 12 -17, 2022</p>
<p>Exploiting labeled and unlabeled data via transformer fine-tuning for peerreview score prediction. Panitan Muangkammuen, Fumiyo Fukumoto, Jiyi Li, Yoshimi Suzuki, 10.18653/V1/2022.FINDINGS-EMNLP.164Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022. December 7-11, 2022</p>
<p>Intermediate-task transfer learning for peer review score prediction. Panitan Muangkammuen, Fumiyo Fukumoto, Jiyi Li, Yoshimi Suzuki, Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics: Student Research Workshop. the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics: Student Research Workshop2023</p>
<p>Rectified linear units improve restricted boltzmann machines. Vinod Nair, Geoffrey E Hinton, Proceedings of the 27th International Conference on Machine Learning (ICML-10). the 27th International Conference on Machine Learning (ICML-10)Haifa, IsraelOmnipress2010. June 21-24, 2010</p>
<p>10.48550/ARXIV.2303.08774CoRR, abs/2303.08774GPT-4 technical report. 2023OpenAI</p>
<p>Values of games with a priori unions. Guilliermo Owen, Mathematical economics and game theory: Essays in honor of Oskar Morgenstern. Springer1977</p>
<p>Citetracked: A longitudinal dataset of peer reviews and citations. Barbara Plank, Reinard Van Dalen, Proceedings of the 4th Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2019) co-located with the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2019). the 4th Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2019) co-located with the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2019)Paris, France2019. July 25, 20192414CEUR Workshop Proceedings</p>
<p>Large language models are zero shot hypothesis proposers. Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Bowen Zhang-Ren Chen, Zhou, 10.48550/ARXIV.2311.05965CoRR, abs/2311.059652023</p>
<p>Software Framework for Topic Modelling with Large Corpora. Radim Řehůřek, Petr Sojka, Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks. the LREC 2010 Workshop on New Challenges for NLP FrameworksValletta, Malta2010ELRA</p>
<p>The ACL OCL corpus: Advancing open science in computational linguistics. Shaurya Rohatgi, Yanxia Qin, Benjamin Aw, Niranjana Unnithan, Min-Yen Kan, 10.18653/V1/2023.EMNLP-MAIN.640Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Can llms generate novel research ideas? A large-scale human study with 100+ NLP researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, 10.48550/ARXIV.2409.04109CoRR, abs/2409.041092024</p>
<p>Scirepeval: A multi-format benchmark for scientific document representations. Amanpreet Singh, Mike D' Arcy, Arman Cohan, Doug Downey, Sergey Feldman, 10.18653/V1/2023.EMNLP-MAIN.338Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Dropout: a simple way to prevent neural networks from overfitting. Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, 10.5555/2627435.2670313J. Mach. Learn. Res. 1512014</p>
<p>An analysis of tasks and datasets in peer reviewing. Moritz Staudinger, Wojciech Kusa, Florina Piroi, Allan Hanbury, Proceedings of the Fourth Workshop on Scholarly Document Processing (SDP 2024). the Fourth Workshop on Scholarly Document Processing (SDP 2024)2024</p>
<p>Schubert: Scholarly document chunks with bert-encoding boost citation count prediction. Gideon Thomas Van Dongen, Maillette De Buy, Lambert Wenniger, Schomaker, 10.18653/V1/2020.SDP-1.17Proceedings of the First Workshop on Scholarly Document Processing. the First Workshop on Scholarly Document Processing2020. November 19. 2020SDP@EMNLP 2020. Association for Computational Linguistics</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017. Long Beach, CA, USA2017. December 4-9, 2017</p>
<p>What have we learned from openreview?. Gang Wang, Qi Peng, Yanfeng Zhang, Mingyang Zhang, 10.1007/978-3-030-85896-4_6Web and Big Data -5th International Joint Conference, APWeb-WAIM 2021. Lecture Notes in Computer Science. Guangzhou, ChinaSpringer2021a. August 23-25, 2021Proceedings, Part I</p>
<p>Prediction and application of article potential citations based on nonlinear citation-forecasting combined model. Kehan Wang, Wenxuan Shi, Junsong Bai, Xiaoping Zhao, Liying Zhang, 10.1007/S11192-021-04026-6Scientometrics. 12682021b</p>
<p>Scimon: Scientific inspiration machines optimized for novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, 10.18653/V1/2024.ACL-LONG.18Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024. August 11-16, 20241ACL 2024</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, 10.18653/V1/2024.FINDINGS-ACL.804Findings of the Association for Computational Linguistics, ACL 2024. Bangkok, ThailandAssociation for Computational Linguistics2024. August 11-16, 2024and virtual meeting</p>
<p>Can we automate scientific reviewing?. Weizhe Yuan, Pengfei Liu, Graham Neubig, 10.1613/JAIR.1.12862J. Artif. Intell. Res. 752022</p>
<p>Predicting citation impact of academic papers across research areas using multiple models and early citations. Fang Zhang, Shengli Wu, 10.1007/S11192-024-05086-0Scientometrics. 12972024</p>
<p>Is LLM a reliable reviewer? A comprehensive evaluation of LLM on automatic paper reviewing tasks. Ruiyang Zhou, Lu Chen, Kai Yu, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024. the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024Torino, ItalyELRA and ICCL2024. 20-25 May, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
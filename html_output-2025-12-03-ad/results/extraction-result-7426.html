<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7426 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7426</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7426</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-266977441</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.06766v3.pdf" target="_blank">Mind Your Format: Towards Consistent Evaluation of In-Context Learning Improvements</a></p>
                <p><strong>Paper Abstract:</strong> Large language models demonstrate a remarkable capability for learning to solve new tasks from a few examples. The prompt template, or the way the input examples are formatted to obtain the prompt, is an important yet often overlooked aspect of in-context learning. In this work, we conduct a comprehensive study of the template format's influence on the in-context learning performance. We evaluate the impact of the prompt template across 21 models (from 770M to 70B parameters) and 4 standard classification datasets. We show that a poor choice of the template can reduce the performance of the strongest models and inference methods to a random guess level. More importantly, the best templates do not transfer between different setups and even between models of the same family. Our findings show that the currently prevalent approach to evaluation, which ignores template selection, may give misleading results due to different templates in different works. As a first step towards mitigating this issue, we propose Template Ensembles that aggregate model predictions across several templates. This simple test-time augmentation boosts average performance while being robust to the choice of random set of templates.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7426.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7426.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Template sensitivity (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt template sensitivity across models and tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper shows that the specific format used to convert demonstrations into a prompt (verbalizers, intra-/inter-separators, ordering) causes large and task‑dependent variation in in‑context learning performance across many LLMs and datasets; a poor template can reduce top models to near random performance and per-model standard deviations across templates can reach tens of percent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple (21 models, e.g., Llama 2, Falcon, OPT, Pythia, BLOOM, LLaMA family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Various transformer-based pretrained large language models from multiple families and sizes evaluated as black-box autoregressive models; some instruction-tuned variants are also evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>770M to 70B (range across evaluated models)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sequence classification (SST-2, DBPedia, AGNews, TREC)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Classify short text inputs into discrete labels (sentiment, ontology classes, news topics, question types) using in‑context few‑shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language few-shot prompts composed from demonstrations formatted by templates (input/output verbalizers, intra- and inter-separators); both zero-shot and few-shot evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / template format</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Templates built from choices of input verbalizer v_I(x), output verbalizer v_O(y), an intra-separator between input and label, and an inter-separator between demonstrations; authors enumerate 168–216 template combinations per dataset and sample random templates (e.g., 10 or 30) per run.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Classification accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Across models and datasets the template choice produces large variance; the strongest models still exhibit per-model standard deviations up to ~35% of their mean accuracy across templates (reported qualitatively and via std across runs).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Template choice can shift performance from high accuracy to near-random; measured std up to ~35% of mean across templates (absolute variability).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot and few-shot (2- and 4-shot) evaluations; DIRECT prediction used for baseline analyses; results averaged over multiple random template samples and demonstration seeds (e.g., 10 templates × 3 seeds).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mind Your Format: Towards Consistent Evaluation of In-Context Learning Improvements', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7426.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7426.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2-70B SST-2 (2-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 70B evaluated on SST-2 in 2-shot setting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Concrete example showing a state-of-the-art model whose mean accuracy and template-induced variance on SST-2 are both substantial; used to illustrate that even large models remain template-sensitive.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 2 70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-access large-scale autoregressive/instruction-tuned transformer model from Meta (Llama 2 family).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SST-2 sentiment classification</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary sentiment classification of short movie-review sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>2-shot few-shot prompts using templates built from combinations of verbalizers and separators; DIRECT prediction (max label probability).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / template format</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Evaluated over 30 random templates for transfer experiments and over 10 random templates for baseline aggregation; templates vary input/output verbalizers and separators (e.g., 'It was {}.', 'input: {}', intra/inter newlines or spaces).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Mean accuracy reported ~0.83 (83%) with standard deviation ~0.14 (14%) across template samples in the 2-shot DIRECT baseline (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Across templates, accuracy varied with std ~14 percentage points (absolute).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>2-shot, DIRECT prediction, random demonstrations fixed across template runs; results aggregated over 10 templates × 3 seeds (or 30 templates for transfer experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mind Your Format: Towards Consistent Evaluation of In-Context Learning Improvements', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7426.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7426.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon-40B SST-2 (2-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon 40B evaluated on SST-2 in 2-shot setting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Another large model example demonstrating notable template sensitivity on SST-2, with mean accuracy and substantial std across templates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon 40B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-access high-quality autoregressive transformer model (Falcon family).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>40B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SST-2 sentiment classification</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary sentiment classification of short movie-review sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>2-shot few-shot prompts with varied templates (verbalizers, separators); DIRECT prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / template format</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Templates sampled randomly from enumerated components (input/output verbalizers, intra/inter separators); evaluations aggregated over multiple templates and seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Mean accuracy reported ~0.79 (79%) with standard deviation ~0.17 (17%) across templates in the 2-shot DIRECT baseline (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Template-induced absolute std ≈ 17 percentage points across sampled templates.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>2-shot, DIRECT prediction, random demonstrations; aggregated over template samples and seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mind Your Format: Towards Consistent Evaluation of In-Context Learning Improvements', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7426.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7426.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prediction method sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity of prediction methods (DIRECT, CHANNEL, CALIBRATION) to template format</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison showing CHANNEL and CALIBRATION often improve mean accuracy relative to DIRECT but that gains overlap with DIRECT across templates; CALIBRATION tends to be more sensitive to template choice than CHANNEL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple (19 base models and instruct models evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Set of transformer LLMs evaluated under different prediction decoding schemes: DIRECT (P(y|x)), CHANNEL (P(x|y) noisy-channel decoding), and CALIBRATION (correction factor to mitigate label biases).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Various (models from ~1B to 70B evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sequence classification (SST-2, DBPedia, AGNews, TREC)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Classify inputs into discrete labels using different prediction scoring strategies applied to prompted hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot prompts formatted by templates; prediction method changes scoring objective (DIRECT, CHANNEL, CALIBRATION).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style and prediction method</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>2-shot setting primarily analyzed; performance aggregated over 10 random templates and multiple demonstration seeds; IoU and ranking comparisons between methods computed over top-10 templates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy; also IoU of top-10 templates across methods</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>CHANNEL and CALIBRATION often exhibit higher average accuracy than DIRECT across templates, but ranges overlap and best DIRECT templates can match advanced methods; CALIBRATION had higher mean but larger sensitivity (higher std) than CHANNEL (paper reports overlap in score ranges in Figure 2 and explicit std observations).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>DIRECT (standard sequence-probability selection)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>No universally consistent absolute gain reported; improvements depend on template choice and can be nullified when accounting for template variance.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>2-shot, evaluations over 10 random templates × 3 demonstration seeds; IoU computed over top-10 templates between prediction methods (Table 4), e.g., Calibration↔Channel IoU ~0.49 (SST-2) with std 0.17 indicating low top-template overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mind Your Format: Towards Consistent Evaluation of In-Context Learning Improvements', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7426.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7426.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Example-selection sensitivity (ITM / Z-ICL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Template dependence of demonstration selection methods (ITM and Z-ICL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper finds that advanced demonstration selection methods (ITM and Z-ICL) are sensitive to prompt templates: ITM often increases average accuracy but has a high standard deviation across templates, while Z-ICL yields more consistent but generally worse performance than RANDOM selection when accounting for templates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple base and instruction-tuned models (evaluated with DIRECT prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models evaluated using different example selection strategies: RANDOM, ITM (Implicit Topic Models), and Z-ICL (pseudo-demonstrations via retrieval and random labels).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Various (evaluations use several model families and sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sequence classification (primarily 4-shot reported for selection methods)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Select N demonstrations to include in few-shot prompts to maximize downstream classification accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>4-shot prompts where the set of demonstrations is chosen by selection method; templates sampled randomly from enumerated template space.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>example selection interacting with prompt format</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>4-shot DIRECT evaluation; used official ITM concept model checkpoints to pick demonstrations and Z-ICL retrieval; evaluated across 10 random templates and multiple seeds; reproduction used CHANNEL prompting in original configurations for comparisons (Appendix F).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (mean and std across templates)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>When aggregating across templates, advanced selection methods frequently perform comparably to or worse than RANDOM baseline: ITM increases average in many cases but shows large template-dependent std; Z-ICL produces more consistent but often lower accuracy (Figures 3, Table 14).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>RANDOM demonstration selection (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>No consistent positive change across templates; gains reported in original works may partially arise from a favorable template choice rather than the selection method alone.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>4-shot, DIRECT prediction (primary comparison); for reproductions some evaluations used CHANNEL prompting to match original papers (Appendix F); results aggregated over 10 templates per seed and multiple seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mind Your Format: Towards Consistent Evaluation of In-Context Learning Improvements', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7426.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7426.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Template transfer (top-10)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Limited transferability of best templates between setups (models/methods/demos)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The best-performing templates for one evaluation setup (model, prediction method, demonstration set) do not reliably transfer to others; the study uses top-10 overlap (IoU) and finds low intersection between setups, though the top-10 usually attains ~90% of the best template score within a single setup.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple (19 base models evaluated for transfer experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base transformer LLMs across families/sizes evaluated for consistency of top templates across models, prediction methods, and demonstration selection.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Various (models from ~1B to 70B included)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sequence classification (SST-2, DBPedia, AGNews, TREC)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assess whether templates that are top-performing for one setup remain top-performing for another.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot prompts sampled across 30 random templates to rank templates by accuracy; top-10 considered 'successful transfer' threshold if present in both top-10 lists for a pair of setups.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style transferability</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>30 random templates used for ranking; IoU (Jaccard) computed between top-10 sets; Spearman rank correlation also analyzed as alternative metric (Appendices H,I,J).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>IoU (Jaccard similarity) of top-10 templates; relative accuracy of top-10 vs best template</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Top-10 templates on average yield ~90% of the best-template score within a setup; however, IoU between top-10 across models/methods is generally low (IoU > 0.5 only for a few model pairs), indicating poor transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Top-10 templates often approximate the best within a setup (~−10% relative or better), but transferring top templates between setups typically results in degraded performance because overlap (IoU) is low.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>2-shot DIRECT baseline used for many transfer experiments; 30 templates sampled per model/setup for ranking comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mind Your Format: Towards Consistent Evaluation of In-Context Learning Improvements', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7426.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7426.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Template Ensembles</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Template Ensembles (averaging predictions across templates)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed mitigation method that averages model label probabilities across multiple prompt templates at test time; ensembles of size ~4–5 typically yield the best trade-off, improving mean accuracy and substantially reducing template-induced variance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple evaluated models (both base and instruction-tuned, e.g., Falcon 40B, Llama 2 etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same pretrained LLMs evaluated earlier; Template Ensembles is a test-time augmentation that runs the model on N different templates and averages label probabilities without additional training.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Various (evaluated broadly across the same model pool)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sequence classification (SST-2, DBPedia, AGNews, TREC)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improve robustness and average accuracy of few-shot classification by aggregating predictions across templates.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Ensembling across multiple few-shot prompt templates (N-way averaging of predicted label probabilities); voting was tested but found inferior.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / test-time augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Authors find ensembles of size 4–5 typically optimal; reported experiments use ensemble size 5 as standard and average over 5 random seeds; increases in mean accuracy and decreases in std reported across models and prediction methods (see Figure 6, Tables 5, 15, 16, 20).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (mean and std across templates)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Template Ensembles increase average accuracy and reduce variance for most evaluated setups; ensembles of size 5 reported to outperform single-template baselines in most cases (quantitative gains vary by model/dataset; example trend shown for Falcon 40B on SST-2 in Figure 6).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Single-template evaluation averaged over templates (or best single template)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Authors report consistent mean accuracy increases and substantial variance reduction (no single uniform numeric improvement given; ensemble size 5 chosen as standard).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Test-time averaging across N templates (N=5 in reported summaries), evaluated across models, datasets, prediction methods; averaged over 5 random seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mind Your Format: Towards Consistent Evaluation of In-Context Learning Improvements', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7426.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7426.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Template parts non-transfer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Non-transferability of template components (verbalizers and separators)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Detailed analysis shows that individual template components (specific input/output verbalizers or separators) have inconsistent effects across models and methods (e.g., an output verbalizer that is best for one model can be among the worst for another).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Examples: Llama 2 70B and Falcon 40B (and others)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large LLMs from different families used to evaluate effect of individual template components.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Examples include 70B (Llama 2) and 40B (Falcon)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SST-2 (template parts analysis reported on SST-2)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assess influence of individual template components (e.g., 'It was {}.' output verbalizer) on classification accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot prompts with one component varied at a time (input/output verbalizers, separators) and others held fixed; DIRECT and other prediction methods evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt component / prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Decompose templates into parts and measure distribution of accuracy for variations of each component; results show high variance per component and model-dependent rankings; example: 'It was {}' output verbalizer ranks highest for Llama 2 70B with DIRECT but is one of the worst for Falcon 40B.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy distribution across templates/components</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Component choice leads to high per-component variance even for top models; no single component consistently improves accuracy across models or prediction methods.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Switching a single component can materially increase or decrease accuracy; combinations matter and components do not transfer reliably between models.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>SST-2, RANDOM 2-shot setup; per-component score distributions plotted and analyzed (Figure 7 and Appendix D).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mind Your Format: Towards Consistent Evaluation of In-Context Learning Improvements', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity <em>(Rating: 2)</em></li>
                <li>Noisy channel language model prompting for few-shot text classification <em>(Rating: 2)</em></li>
                <li>Z-icl: Zeroshot in-context learning with pseudo-demonstrations <em>(Rating: 2)</em></li>
                <li>Batch calibration: Rethinking calibration for in-context learning and prompt engineering <em>(Rating: 2)</em></li>
                <li>The language of prompting: What linguistic properties make a prompt successful? <em>(Rating: 1)</em></li>
                <li>Pythia: A suite for analyzing large language models across training and scaling <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7426",
    "paper_id": "paper-266977441",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "Template sensitivity (baseline)",
            "name_full": "Prompt template sensitivity across models and tasks",
            "brief_description": "The paper shows that the specific format used to convert demonstrations into a prompt (verbalizers, intra-/inter-separators, ordering) causes large and task‑dependent variation in in‑context learning performance across many LLMs and datasets; a poor template can reduce top models to near random performance and per-model standard deviations across templates can reach tens of percent.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple (21 models, e.g., Llama 2, Falcon, OPT, Pythia, BLOOM, LLaMA family)",
            "model_description": "Various transformer-based pretrained large language models from multiple families and sizes evaluated as black-box autoregressive models; some instruction-tuned variants are also evaluated.",
            "model_size": "770M to 70B (range across evaluated models)",
            "task_name": "Sequence classification (SST-2, DBPedia, AGNews, TREC)",
            "task_description": "Classify short text inputs into discrete labels (sentiment, ontology classes, news topics, question types) using in‑context few‑shot prompts.",
            "problem_format": "Natural-language few-shot prompts composed from demonstrations formatted by templates (input/output verbalizers, intra- and inter-separators); both zero-shot and few-shot evaluated.",
            "format_category": "prompt style / template format",
            "format_details": "Templates built from choices of input verbalizer v_I(x), output verbalizer v_O(y), an intra-separator between input and label, and an inter-separator between demonstrations; authors enumerate 168–216 template combinations per dataset and sample random templates (e.g., 10 or 30) per run.",
            "performance_metric": "Classification accuracy",
            "performance_value": "Across models and datasets the template choice produces large variance; the strongest models still exhibit per-model standard deviations up to ~35% of their mean accuracy across templates (reported qualitatively and via std across runs).",
            "baseline_performance": null,
            "performance_change": "Template choice can shift performance from high accuracy to near-random; measured std up to ~35% of mean across templates (absolute variability).",
            "experimental_setting": "Zero-shot and few-shot (2- and 4-shot) evaluations; DIRECT prediction used for baseline analyses; results averaged over multiple random template samples and demonstration seeds (e.g., 10 templates × 3 seeds).",
            "statistical_significance": null,
            "uuid": "e7426.0",
            "source_info": {
                "paper_title": "Mind Your Format: Towards Consistent Evaluation of In-Context Learning Improvements",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Llama2-70B SST-2 (2-shot)",
            "name_full": "Llama 2 70B evaluated on SST-2 in 2-shot setting",
            "brief_description": "Concrete example showing a state-of-the-art model whose mean accuracy and template-induced variance on SST-2 are both substantial; used to illustrate that even large models remain template-sensitive.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 2 70B",
            "model_description": "Open-access large-scale autoregressive/instruction-tuned transformer model from Meta (Llama 2 family).",
            "model_size": "70B",
            "task_name": "SST-2 sentiment classification",
            "task_description": "Binary sentiment classification of short movie-review sentences.",
            "problem_format": "2-shot few-shot prompts using templates built from combinations of verbalizers and separators; DIRECT prediction (max label probability).",
            "format_category": "prompt style / template format",
            "format_details": "Evaluated over 30 random templates for transfer experiments and over 10 random templates for baseline aggregation; templates vary input/output verbalizers and separators (e.g., 'It was {}.', 'input: {}', intra/inter newlines or spaces).",
            "performance_metric": "Accuracy",
            "performance_value": "Mean accuracy reported ~0.83 (83%) with standard deviation ~0.14 (14%) across template samples in the 2-shot DIRECT baseline (Table 3).",
            "baseline_performance": null,
            "performance_change": "Across templates, accuracy varied with std ~14 percentage points (absolute).",
            "experimental_setting": "2-shot, DIRECT prediction, random demonstrations fixed across template runs; results aggregated over 10 templates × 3 seeds (or 30 templates for transfer experiments).",
            "statistical_significance": null,
            "uuid": "e7426.1",
            "source_info": {
                "paper_title": "Mind Your Format: Towards Consistent Evaluation of In-Context Learning Improvements",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Falcon-40B SST-2 (2-shot)",
            "name_full": "Falcon 40B evaluated on SST-2 in 2-shot setting",
            "brief_description": "Another large model example demonstrating notable template sensitivity on SST-2, with mean accuracy and substantial std across templates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Falcon 40B",
            "model_description": "Open-access high-quality autoregressive transformer model (Falcon family).",
            "model_size": "40B",
            "task_name": "SST-2 sentiment classification",
            "task_description": "Binary sentiment classification of short movie-review sentences.",
            "problem_format": "2-shot few-shot prompts with varied templates (verbalizers, separators); DIRECT prediction.",
            "format_category": "prompt style / template format",
            "format_details": "Templates sampled randomly from enumerated components (input/output verbalizers, intra/inter separators); evaluations aggregated over multiple templates and seeds.",
            "performance_metric": "Accuracy",
            "performance_value": "Mean accuracy reported ~0.79 (79%) with standard deviation ~0.17 (17%) across templates in the 2-shot DIRECT baseline (Table 3).",
            "baseline_performance": null,
            "performance_change": "Template-induced absolute std ≈ 17 percentage points across sampled templates.",
            "experimental_setting": "2-shot, DIRECT prediction, random demonstrations; aggregated over template samples and seeds.",
            "statistical_significance": null,
            "uuid": "e7426.2",
            "source_info": {
                "paper_title": "Mind Your Format: Towards Consistent Evaluation of In-Context Learning Improvements",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Prediction method sensitivity",
            "name_full": "Sensitivity of prediction methods (DIRECT, CHANNEL, CALIBRATION) to template format",
            "brief_description": "Comparison showing CHANNEL and CALIBRATION often improve mean accuracy relative to DIRECT but that gains overlap with DIRECT across templates; CALIBRATION tends to be more sensitive to template choice than CHANNEL.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple (19 base models and instruct models evaluated)",
            "model_description": "Set of transformer LLMs evaluated under different prediction decoding schemes: DIRECT (P(y|x)), CHANNEL (P(x|y) noisy-channel decoding), and CALIBRATION (correction factor to mitigate label biases).",
            "model_size": "Various (models from ~1B to 70B evaluated)",
            "task_name": "Sequence classification (SST-2, DBPedia, AGNews, TREC)",
            "task_description": "Classify inputs into discrete labels using different prediction scoring strategies applied to prompted hypotheses.",
            "problem_format": "Few-shot prompts formatted by templates; prediction method changes scoring objective (DIRECT, CHANNEL, CALIBRATION).",
            "format_category": "prompt style and prediction method",
            "format_details": "2-shot setting primarily analyzed; performance aggregated over 10 random templates and multiple demonstration seeds; IoU and ranking comparisons between methods computed over top-10 templates.",
            "performance_metric": "Accuracy; also IoU of top-10 templates across methods",
            "performance_value": "CHANNEL and CALIBRATION often exhibit higher average accuracy than DIRECT across templates, but ranges overlap and best DIRECT templates can match advanced methods; CALIBRATION had higher mean but larger sensitivity (higher std) than CHANNEL (paper reports overlap in score ranges in Figure 2 and explicit std observations).",
            "baseline_performance": "DIRECT (standard sequence-probability selection)",
            "performance_change": "No universally consistent absolute gain reported; improvements depend on template choice and can be nullified when accounting for template variance.",
            "experimental_setting": "2-shot, evaluations over 10 random templates × 3 demonstration seeds; IoU computed over top-10 templates between prediction methods (Table 4), e.g., Calibration↔Channel IoU ~0.49 (SST-2) with std 0.17 indicating low top-template overlap.",
            "statistical_significance": null,
            "uuid": "e7426.3",
            "source_info": {
                "paper_title": "Mind Your Format: Towards Consistent Evaluation of In-Context Learning Improvements",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Example-selection sensitivity (ITM / Z-ICL)",
            "name_full": "Template dependence of demonstration selection methods (ITM and Z-ICL)",
            "brief_description": "The paper finds that advanced demonstration selection methods (ITM and Z-ICL) are sensitive to prompt templates: ITM often increases average accuracy but has a high standard deviation across templates, while Z-ICL yields more consistent but generally worse performance than RANDOM selection when accounting for templates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple base and instruction-tuned models (evaluated with DIRECT prediction)",
            "model_description": "Models evaluated using different example selection strategies: RANDOM, ITM (Implicit Topic Models), and Z-ICL (pseudo-demonstrations via retrieval and random labels).",
            "model_size": "Various (evaluations use several model families and sizes)",
            "task_name": "Sequence classification (primarily 4-shot reported for selection methods)",
            "task_description": "Select N demonstrations to include in few-shot prompts to maximize downstream classification accuracy.",
            "problem_format": "4-shot prompts where the set of demonstrations is chosen by selection method; templates sampled randomly from enumerated template space.",
            "format_category": "example selection interacting with prompt format",
            "format_details": "4-shot DIRECT evaluation; used official ITM concept model checkpoints to pick demonstrations and Z-ICL retrieval; evaluated across 10 random templates and multiple seeds; reproduction used CHANNEL prompting in original configurations for comparisons (Appendix F).",
            "performance_metric": "Accuracy (mean and std across templates)",
            "performance_value": "When aggregating across templates, advanced selection methods frequently perform comparably to or worse than RANDOM baseline: ITM increases average in many cases but shows large template-dependent std; Z-ICL produces more consistent but often lower accuracy (Figures 3, Table 14).",
            "baseline_performance": "RANDOM demonstration selection (baseline)",
            "performance_change": "No consistent positive change across templates; gains reported in original works may partially arise from a favorable template choice rather than the selection method alone.",
            "experimental_setting": "4-shot, DIRECT prediction (primary comparison); for reproductions some evaluations used CHANNEL prompting to match original papers (Appendix F); results aggregated over 10 templates per seed and multiple seeds.",
            "statistical_significance": null,
            "uuid": "e7426.4",
            "source_info": {
                "paper_title": "Mind Your Format: Towards Consistent Evaluation of In-Context Learning Improvements",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Template transfer (top-10)",
            "name_full": "Limited transferability of best templates between setups (models/methods/demos)",
            "brief_description": "The best-performing templates for one evaluation setup (model, prediction method, demonstration set) do not reliably transfer to others; the study uses top-10 overlap (IoU) and finds low intersection between setups, though the top-10 usually attains ~90% of the best template score within a single setup.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple (19 base models evaluated for transfer experiments)",
            "model_description": "Base transformer LLMs across families/sizes evaluated for consistency of top templates across models, prediction methods, and demonstration selection.",
            "model_size": "Various (models from ~1B to 70B included)",
            "task_name": "Sequence classification (SST-2, DBPedia, AGNews, TREC)",
            "task_description": "Assess whether templates that are top-performing for one setup remain top-performing for another.",
            "problem_format": "Few-shot prompts sampled across 30 random templates to rank templates by accuracy; top-10 considered 'successful transfer' threshold if present in both top-10 lists for a pair of setups.",
            "format_category": "prompt style transferability",
            "format_details": "30 random templates used for ranking; IoU (Jaccard) computed between top-10 sets; Spearman rank correlation also analyzed as alternative metric (Appendices H,I,J).",
            "performance_metric": "IoU (Jaccard similarity) of top-10 templates; relative accuracy of top-10 vs best template",
            "performance_value": "Top-10 templates on average yield ~90% of the best-template score within a setup; however, IoU between top-10 across models/methods is generally low (IoU &gt; 0.5 only for a few model pairs), indicating poor transfer.",
            "baseline_performance": null,
            "performance_change": "Top-10 templates often approximate the best within a setup (~−10% relative or better), but transferring top templates between setups typically results in degraded performance because overlap (IoU) is low.",
            "experimental_setting": "2-shot DIRECT baseline used for many transfer experiments; 30 templates sampled per model/setup for ranking comparisons.",
            "statistical_significance": null,
            "uuid": "e7426.5",
            "source_info": {
                "paper_title": "Mind Your Format: Towards Consistent Evaluation of In-Context Learning Improvements",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Template Ensembles",
            "name_full": "Template Ensembles (averaging predictions across templates)",
            "brief_description": "A proposed mitigation method that averages model label probabilities across multiple prompt templates at test time; ensembles of size ~4–5 typically yield the best trade-off, improving mean accuracy and substantially reducing template-induced variance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple evaluated models (both base and instruction-tuned, e.g., Falcon 40B, Llama 2 etc.)",
            "model_description": "Same pretrained LLMs evaluated earlier; Template Ensembles is a test-time augmentation that runs the model on N different templates and averages label probabilities without additional training.",
            "model_size": "Various (evaluated broadly across the same model pool)",
            "task_name": "Sequence classification (SST-2, DBPedia, AGNews, TREC)",
            "task_description": "Improve robustness and average accuracy of few-shot classification by aggregating predictions across templates.",
            "problem_format": "Ensembling across multiple few-shot prompt templates (N-way averaging of predicted label probabilities); voting was tested but found inferior.",
            "format_category": "prompt style / test-time augmentation",
            "format_details": "Authors find ensembles of size 4–5 typically optimal; reported experiments use ensemble size 5 as standard and average over 5 random seeds; increases in mean accuracy and decreases in std reported across models and prediction methods (see Figure 6, Tables 5, 15, 16, 20).",
            "performance_metric": "Accuracy (mean and std across templates)",
            "performance_value": "Template Ensembles increase average accuracy and reduce variance for most evaluated setups; ensembles of size 5 reported to outperform single-template baselines in most cases (quantitative gains vary by model/dataset; example trend shown for Falcon 40B on SST-2 in Figure 6).",
            "baseline_performance": "Single-template evaluation averaged over templates (or best single template)",
            "performance_change": "Authors report consistent mean accuracy increases and substantial variance reduction (no single uniform numeric improvement given; ensemble size 5 chosen as standard).",
            "experimental_setting": "Test-time averaging across N templates (N=5 in reported summaries), evaluated across models, datasets, prediction methods; averaged over 5 random seeds.",
            "statistical_significance": null,
            "uuid": "e7426.6",
            "source_info": {
                "paper_title": "Mind Your Format: Towards Consistent Evaluation of In-Context Learning Improvements",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Template parts non-transfer",
            "name_full": "Non-transferability of template components (verbalizers and separators)",
            "brief_description": "Detailed analysis shows that individual template components (specific input/output verbalizers or separators) have inconsistent effects across models and methods (e.g., an output verbalizer that is best for one model can be among the worst for another).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Examples: Llama 2 70B and Falcon 40B (and others)",
            "model_description": "Large LLMs from different families used to evaluate effect of individual template components.",
            "model_size": "Examples include 70B (Llama 2) and 40B (Falcon)",
            "task_name": "SST-2 (template parts analysis reported on SST-2)",
            "task_description": "Assess influence of individual template components (e.g., 'It was {}.' output verbalizer) on classification accuracy.",
            "problem_format": "Few-shot prompts with one component varied at a time (input/output verbalizers, separators) and others held fixed; DIRECT and other prediction methods evaluated.",
            "format_category": "prompt component / prompt style",
            "format_details": "Decompose templates into parts and measure distribution of accuracy for variations of each component; results show high variance per component and model-dependent rankings; example: 'It was {}' output verbalizer ranks highest for Llama 2 70B with DIRECT but is one of the worst for Falcon 40B.",
            "performance_metric": "Accuracy distribution across templates/components",
            "performance_value": "Component choice leads to high per-component variance even for top models; no single component consistently improves accuracy across models or prediction methods.",
            "baseline_performance": null,
            "performance_change": "Switching a single component can materially increase or decrease accuracy; combinations matter and components do not transfer reliably between models.",
            "experimental_setting": "SST-2, RANDOM 2-shot setup; per-component score distributions plotted and analyzed (Figure 7 and Appendix D).",
            "statistical_significance": null,
            "uuid": "e7426.7",
            "source_info": {
                "paper_title": "Mind Your Format: Towards Consistent Evaluation of In-Context Learning Improvements",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
            "rating": 2,
            "sanitized_title": "fantastically_ordered_prompts_and_where_to_find_them_overcoming_fewshot_prompt_order_sensitivity"
        },
        {
            "paper_title": "Noisy channel language model prompting for few-shot text classification",
            "rating": 2,
            "sanitized_title": "noisy_channel_language_model_prompting_for_fewshot_text_classification"
        },
        {
            "paper_title": "Z-icl: Zeroshot in-context learning with pseudo-demonstrations",
            "rating": 2,
            "sanitized_title": "zicl_zeroshot_incontext_learning_with_pseudodemonstrations"
        },
        {
            "paper_title": "Batch calibration: Rethinking calibration for in-context learning and prompt engineering",
            "rating": 2,
            "sanitized_title": "batch_calibration_rethinking_calibration_for_incontext_learning_and_prompt_engineering"
        },
        {
            "paper_title": "The language of prompting: What linguistic properties make a prompt successful?",
            "rating": 1,
            "sanitized_title": "the_language_of_prompting_what_linguistic_properties_make_a_prompt_successful"
        },
        {
            "paper_title": "Pythia: A suite for analyzing large language models across training and scaling",
            "rating": 1,
            "sanitized_title": "pythia_a_suite_for_analyzing_large_language_models_across_training_and_scaling"
        }
    ],
    "cost": 0.019164749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Mind Your Format: Towards Consistent Evaluation of In-Context Learning Improvements
6 Jun 2024</p>
<p>Anton Voronov voronov.ad@phystech.edu 
Lena Wolf 
Max Ryabinin mryabinin0@gmail.com 
Ebtesam Almazrouei 
Hamza Alobeidli 
Abdulaziz Al- Shamsi 
Alessandro Cappelli 
Ruxandra Cojocaru 
Merouane Debbah 
Etienne Goffinet 
Daniel Hes- Low 
Julien Launay 
Quentin Malartic 
Badreddine Noune 
Baptiste Pannier 
Guilherme Penedo 
Stella Biderman 
Hailey Schoelkopf 
Quentin Anthony 
Herbie Bradley 
Kyle O'brien 
Eric Hallahan 
Mo- Hammad Aflah Khan 
USVSNShivanshu Purohit 
Sai Prashanth 
Edward Raff 
Aviya Skowron 
Lintang 
Sid Black 
Quentin An- Thony 
Leo Gao 
Laurence Golding 
Horace He 
Con- Nor Leahy 
Kyle Mcdonell 
Jason Phang 
Michael Pieler 
Tom B Brown 
Benjamin Mann 
Nick Ryder 
Melanie Subbiah 
Jared Kaplan 
Prafulla Dhariwal 
Arvind Neelakantan 
Pranav Shyam 
Girish Sastry 
Amanda Askell 
Sandhini Agarwal 
Ariel Herbert-Voss 
Gretchen Krueger 
Tom Henighan 
Rewon Child 
Aditya Ramesh 
Daniel M Ziegler 
Jeffrey Wu 
Clemens Winter 
Christopher Hesse 
Mark Chen 
Eric Sigler 
Mateusz Litwin 
Scott Gray 
Benjamin Chess 
Jack Clark 
Christopher Berner 
Sam Mccandlish 
Alec Radford 
Ilya 2019 Sutskever 
Dario Amodei 
Aakanksha Chowdhery 
Sharan Narang 
Jacob Devlin 
Maarten Bosma 
Gaurav Mishra 
Adam Roberts 
HyungPaul Barham 
Won Chung 
Charles Sutton 
Sebastian Gehrmann 
Parker Schuh 
Kensen Shi 
Sasha Tsvyashchenko 
Joshua Maynez 
Abhishek Rao 
Parker Barnes 
Yi Tay 
Noam Shazeer 
Vin- Odkumar Prabhakaran 
Emily Reif 
Nan Du 
Ben Hutchinson 
Reiner Pope 
James Bradbury 
Jacob Austin 
Michael Isard 
Guy Gur-Ari 
Pengcheng Yin 
Toju Duke 
Anselm Levskaya 
Sanjay Ghemawat 
Sunipa Dev 
Henryk Michalewski 
Xavier Garcia 
Vedant Misra 
Kevin Robinson 
Liam Fedus 
Denny Zhou 
Daphne Ippolito 
David Luan 
Hyeontaek Lim 
Barret Zoph 
Alexander Spiridonov 
Ryan Sepassi 
David Dohan 
Shivani Agrawal 
Mark Omernick 
An- Drew M Dai 
Thanumalayan Sankaranarayana 
Marie Pellat 
Aitor Lewkowycz 
Erica Moreira 
Oleksandr Polozov 
Katherine Lee 
Zongwei Zhou 
Xuezhi Wang 
Brennan Saeta 
Mark Diaz 
Orhan Firat 
Michele Catasta 
Jason Wei 
Kathy Meier-Hellstern 
Jordan Hoffmann 
Sebastian Borgeaud 
Arthur Mensch 
Elena Buchatskaya 
Trevor Cai 
Eliza Rutherford 
Diego De 
Las Casas 
Lisa Anne Hendricks 
Johannes Welbl 
Aidan Clark 
Tom Hennigan 
Eric Noland 
Katie Millican 
George Van Den Driessche 
Bogdan Damoc 
Aurelia Guy 
Simon Osindero 
Karen Si- Monyan 
Erich Elsen 
Jack W Rae 
Oriol Vinyals 
Jeff Wu 
Language 
Teven Le Scao 
Angela Fan 
Christopher Akiki 
El- Lie Pavlick 
Suzana Ilić 
Daniel Hesslow 
Roman Castagné 
Alexandra Sasha Luccioni 
François Yvon 
Matthias Gallé 
Jonathan Tow 
Alexander M Rush 
Albert Webson 
Pawan Sasanka 
Thomas Wang 
Benoît Sagot 
Niklas Muennighoff 
Albert Villanova 
Del Moral 
Olatunji Ruwase 
Rachel Bawden 
Stas Bekman 
Angelina Mcmillan-Major 
Iz Beltagy 
Huu Nguyen 
Lucile Saulnier 
Samson Tan 
Pedro Ortiz Suarez 
Vic- Tor Sanh 
Hugo Laurençon 
Yacine Jernite 
Margaret Mitchell 
Colin Raffel 
Aaron Gokaslan 
Adi Simhi 
Aitor Soroa 
Alham Fikri 
Amit Alfassy 
Anna Rogers 
Ariel Kreisberg Nitzav 
Canwen Xu 
Chenghao Mou 
Chris Emezue 
Christopher Klamm 
Colin Leong 
Daniel Van Strien 
De Toni 
Gérard Dupont 
Germán Kruszewski 
Giada Pistilli 
Hady Elsahar 
Hamza Benyamina 
Hieu Tran 
Ian Yu 
Idris Abdulmumin 
Isaac Johnson 
Khalid Bhattacharjee 
Kimbo Almubarak 
Kyle Chen 
Leandro Von Lo 
Leon Werra 
Long Weber 
Loubna Phan 
Ben 
Ludovic Tanguy 
Manan Dey 
Manuel Romero Muñoz 
Maraim Masoud 
María Grandury 
Mario Šaško 
Max Huang 
Max- Imin Coavoux 
MikeMayank Singh 
Tian-Jian Jiang 
Minh Chien Vu 
Mohammad A Jauhar 
Mustafa Ghaleb 
Nishant Subramani 
Nora Kassner 
Nuru- Laqilla Khamis 
Olivier Nguyen 
Omar Espejel 
Ona De Gibert 
Paulo Villegas 
Peter Henderson 
Pierre Colombo 
Priscilla Amuok 
Quentin Lhoest 
Rheza Harliman 
Rishi Bommasani 
Roberto Luis López 
Rui Ribeiro 
Salomey Osei 
Sampo Pyysalo 
Se- Bastian Nagel 
Shamik Bose 
Shamsuddeen Hassan Muhammad 
Shanya Sharma 
Shayne Longpre 
So- Maieh Nikpoor 
Stanislav Silberberg 
Suhas Pai 
Syd- Ney Zink 
Tiago Timponi Torrent 
Timo Schick 
Tris- Tan Thrush 
Valentin Danchev 
Vassilina Nikoulina 
Veronika Laippala 
Violette Lepercq 
Vrinda Prabhu 
Zaid Alyafeai 
Zeerak Talat 
Arun Raja 
Benjamin Heinzerling 
Chenglei Si 
Elizabeth Salesky 
Sab- Rina J Mielke 
Wilson Y Lee 
Abheesht Sharma 
An- Drea Santilli 
Antoine Chaffin 
Arnaud Stiegler 
Deba- Jyoti Datta 
Eliza Szczechla 
Gunjan Chhablani 
Han Wang 
Harshit Pandey 
Hendrik Strobelt 
Jason Alan Fries 
Jos Rozen 
Lintang Sutawika 
M Sai- Ful Bari 
Maged S Al-Shaibani 
Matteo Manica 
Ni- Hal Nayak 
Ryan Teehan 
Samuel Albanie 
Sheng Shen 
Srulik Ben-David 
Stephen H Bach 
Taewoon Kim 
Tali Bers 
Thibault Fevry 
Trishala Neeraj 
Deepak Narayanan 
Hatim Bourfoune 
Jared Casper 
Jeff Rasley 
Mayank Mishra 
Minjia Zhang 
Mohammad Shoeybi 
Myriam Peyrounette 
Nicolas Patry 
Nouamane Tazi 
Omar Sanseviero 
Patrick Von Platen 
Pierre Cornette 
Pierre François Lavallée 
Rémi Lacroix 
Samyam Rajbhandari 
San- Chit Gandhi 
Shaden Smith 
Stéphane Requena 
Suraj Patil 
Tim Dettmers 
Ahmed Baruwa 
Amanpreet Singh 
Anastasia Cheveleva 
Anne-Laure Ligozat 
Arjun Subramonian 
Aurélie Névéol 
Dan Garrette 
Deepak Tunuguntla 
Ehud Reiter 
Ekaterina Taktasheva 
Ekaterina Voloshina 
Eli Bog- Danov, Genta 
Indra Winata 
Jan- Christoph Kalo 
Jekaterina Novikova 
Jessica Zosa Forde 
Jordan Clive 
Jungo Kasai 
Ken Kawamura 
Liam Hazan 
Marine Carpuat 
Miruna Clinciu 
Na- Joung Kim 
Newton Cheng 
Oleg Serikov 
Omer Antverg 
Oskar Van Der Wal 
Rui Zhang 
Ruochen Zhang 
Shani Pais 
Tatiana Shavrina 
Thomas Scialom 
Tian Yun 
Tomasz Lim- Isiewicz 
Verena Rieser 
Vitaly Protasov 
Vladislav Mikhailov 
Yada Pruksachatkun 
Yonatan Belinkov 
Zachary Bamberger 
Zdeněk Kasner 
Alice Rueda 
Amanda Pestana 
Amir Feizpour 
Ammar Khan 
Amy Faranak 
Ana Santos 
Anthony Hevia 
Antigona Unl- Dreaj 
Arash Aghagol 
Arezoo Abdollahi 
Aycha Tam- Mour 
Azadeh Hajihosseini 
Bahareh Behroozi 
Ben- Jamin Ajibade 
Bharat Saxena 
Carlos Muñoz 
Danish Contractor 
David Lansky 
Davis David 
Douwe Kiela 
Duong A Nguyen 
Edward Tan 
Emi Baylor 
Ezinwanne Ozoani 
Fatima Mirza 
Frankline Ononiwu 
Habib Rezanejad 
Hessie Jones 
Indrani Bhattacharya 
Irene Solaiman 
Irina Sedenko 
Isar Nejadgholi 
Jesse Passmore 
Josh Seltzer 
Julio Bo- Nis Sanz 
Karen Fort 
Livia Dutra 
Mairon Sama- Gaio 
Maraim Elbadri 
Margot Mieskes 
Marissa Ger- Chick 
Martha Akinlolu 
Michael Mckenna 
Mike Qiu 
Muhammed Ghauri 
Mykola Burynok 
Nafis Abrar 
Nazneen Rajani 
Nour Elkott 
Nour Fahmy 
Olanrewaju Samuel 
Ran An 
Rasmus Kromann 
Ryan Hao 
Samira Alizadeh 
Sarmad Shubber 
Silas Wang 
Sourav Roy 
Sylvain Viguier 
Thanh Le 
Tobi Oyebade 
Trieu Le 
Yoyo Yang 
Zach Nguyen 
Ramesh Kashyap 
Alfredo Palasciano 
Al- Ison Callahan 
Anima Shukla 
Antonio Miranda- Escalada 
Ayush Singh 
Benjamin Beilharz 
Bo Wang 
Caio Brito 
Chenxi Zhou 
Chirag Jain 
Chuxin Xu 
Clémentine Fourrier 
Daniel León Periñán 
Daniel Molano 
Dian Yu 
Enrique Manjavacas 
Fabio Barth 
Florian Fuhrimann 
Gabriel Altay 
Giyased- Din Bayrak 
Gully Burns 
Helena U Vrabec 
Imane Bello 
Ishani Dash 
Jihyun Kang 
John Giorgi 
Jonas Golde 
Jose David Posada 
Karthik Rangasai 
Lokesh Bulchandani 
Lu Liu 
Luisa Shinzato 
Madeleine Hahn De Bykhovetz 
Maiko Takeuchi 
Marc Pàmies 
Maria A Castillo 
Marianna Nezhurina 
Mario Sänger 
Matthias Samwald 
Michael Cullan 
Michael Weinberg 
Michiel De Wolf 
Mina Mihalj- Cic 
Minna Liu 
Moritz Freidank 
Myungsun Kang 
Natasha Seelam 
Nathan Dahlberg 
Nicholas Michio Broad 
Nikolaus Muellner 
Pascale Fung 
Patrick Haller 
Ramya Chandrasekhar 
Renata Eisenberg 
Robert Martin 
Rodrigo Canalli 
Rosaline Su 
Ruisi Su 
Samuel Cahyawijaya 
Samuele Garda 
S Shlok 
Shubhanshu Deshmukh 
Sid Mishra 
Si- Mon Kiblawi 
Sinee Ott 
Srishti Sang-Aroonsiri 
Ste- Fan Kumar 
Sushil Schweter 
Tanmay Bharati 
Théo Laud 
Tomoya Gigant 
Wojciech Kainuma 
Yanis Kusa 
Labrak 
Shailesh Yash 
Yash Bajaj 
Yifan Venkatraman 
Yingxin Xu 
Yu Xu 
Zhe Xu 
Zhongli Tan 
Melanie Sclar 
Yejin Choi 
Yulia Tsvetkov 
Alane 2023 Suhr 
Taylor Shin 
Yasaman Razeghi 
Robert L Logan 
Eric Wallace 
Sameer 2021 Singh 
Autoprompt 
Karen Simonyan 
Andrew 2015 Zisserman 
Very 
Richard Socher 
Alex Perelygin 
Jean Wu 
Jason Chuang 
Christopher D Manning 
Andrew Ng 
Christopher Potts 
Recursive 
Hugo Touvron 
Thibaut Lavril 
Gautier Izacard 
Xavier Martinet 
Marie-Anne Lachaux 
Timothée Lacroix 
Baptiste Rozière 
Naman Goyal 
Eric Hambro 
Faisal Azhar 
Aurelien Rodriguez 
Armand Joulin 
Louis Martin 
Kevin Stone 
Peter Al- Bert 
Amjad Almahairi 
Yasmine Babaei 
Nikolay Bashlykov 
Soumya Batra 
Prajjwal Bhargava 
Shruti Bhosale 
Dan Bikel 
Lukas Blecher 
Cristian Canton Ferrer 
Moya Chen 
Guillem Cucurull 
David Esiobu 
Jude Fernandes 
Jeremy Fu 
Wenyin Fu 
Brian Fuller 
Cynthia Gao 
Vedanuj Goswami 
An- Thony Hartshorn 
Saghar Hosseini 
Rui Hou 
Hakan Inan 
Marcin Kardas 
Viktor Kerkez 
Madian Khabsa 
Isabel Kloumann 
PunitArtem Korenev 
Singh Koura 
Jenya Lee 
Di- Ana Liskovich 
Yinghai Lu 
Yuning Mao 
Xavier Mar- Tinet 
Todor Mihaylov 
Pushkar Mishra 
Igor Moly- Bog 
Yixin Nie 
Andrew Poulton 
Jeremy Reizen- Stein 
Rashi Rungta 
Kalyan Saladi 
Jerry Wei 
Jason Wei 
Dustin Tran 
Yifeng Lu 
Xinyun Chen 
Hanxiao Liu 
Da Huang 
Tengyu 2022 Ma 
Sang Michael Xie 
Aditi Raghunathan 
Percy Liang 
An 
Susan Zhang 
Stephen Roller 
Mikel Artetxe 
Moya Chen 
Shuohui Chen 
Christopher De- Wan 
Mona Diab 
XiXian Li 
TodorVictoria Lin 
Myle Ott 
Sam Shleifer 
Kurt Shuster 
PunitDaniel Simig 
TianluAnjali Sridhar 
Tony Z Zhao 
Shi Feng 
Dan Klein 
Han Zhou 
Xingchen Wan 
Lev Proleev 
Diana Mincu 
Jilin Chen 
Katherine A Heller </p>
<p>HSE University
Yandex, MIPT</p>
<p>HSE University
Yandex</p>
<p>USVSN Sai Prashanth
Shivanshu Purohit</p>
<p>Laria Reynolds
Jonathan Tow
Ben Wang, and Samuel Weinbach
2022</p>
<p>Douglas Eck
Slav Petrov
Jeff Dean</p>
<p>David Ifeoluwa Adelani
Ed-uardo González Ponferrada
Dragomir Radev, Efrat Levkovizh, Ethan Kim</p>
<p>Eyal Bar Natan
Francesco</p>
<p>Itziar Gonzalez-Dios
Javier de la Rosa, Jenny Chim</p>
<p>Jesse Dodge
Jian Zhu</p>
<p>Jonathan Chang
Jörg Frohberg</p>
<p>Joseph Tobing
Joy</p>
<p>Zheng-Xin Yong
Vikas Raunak, Yallow UriUr-mish Thakker, Xiangru Tang, Zhiqing Sun, Shaked Brody</p>
<p>Hadar Tojarieh
Adam Roberts</p>
<p>Hyung Won Chung
Jaesung Tae, Jason Phang, Ofir PressConglong Li</p>
<p>Thomas Wolf
Zifan Ye, Mathilde Bras, Younes BelkadaXie</p>
<p>Alan Schelten
Ruan Silva</p>
<p>Eric Michael Smith
Ranjan Subrama-nian
Ross Tay-lor, Adina WilliamsXiaoqing Ellen Tan, Binh Tang</p>
<p>Jian Xiang Kuan
Puxin XuZheng Yan</p>
<p>Iliyan Zarov
Yuchen Zhang, Angela Fan</p>
<p>Association for Computational Linguistics
SeattleUnited States</p>
<p>Mind Your Format: Towards Consistent Evaluation of In-Context Learning Improvements
6 Jun 2024482AB0CC1CF610B2523AE9DF89F320D0arXiv:2401.06766v3[cs.CL]0.75 😔 Channel: 0.79 😐 Calibration: 0.80 😊 text: Worst film ever target: negative text: Awesome, I like it target: positive input: Worst film ever It was negative. input: Awesome, I like it It was positive. Sutawika, and Oskar van der Wal. 2023. Pythia: A suite for analyzing large language models across training and scaling. Preprint, arXiv:2304.01373.{C[y]} C = (positivenegative) Intra-sep: " "; inter-sep: "\n" Templates An opensource autoregressive language model. PreprintarXiv:2204.06745 Graveand Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. PreprintarXiv:2302.13971 Improving few-shot performance of language models. CoRRabs/2102.09690 2024. Batch calibration: Rethinking calibration for in-context learning and prompt engineering. In The Twelfth International Conference on Learning Representations
Large language models demonstrate a remarkable capability for learning to solve new tasks from a few examples.The prompt template, or the way the input examples are formatted to obtain the prompt, is an important yet often overlooked aspect of in-context learning.In this work, we conduct a comprehensive study of the template format's influence on the in-context learning performance.We evaluate the impact of the prompt template across 21 models (from 770M to 70B parameters) and 4 standard classification datasets.We show that a poor choice of the template can reduce the performance of the strongest models and inference methods to a random guess level.More importantly, the best templates do not transfer between different setups and even between models of the same family.Our findings show that the currently prevalent approach to evaluation, which ignores template selection, may give misleading results due to different templates in different works.As a first step towards mitigating this issue, we propose Template Ensembles that aggregate model predictions across several templates.This simple test-time augmentation boosts average performance while being robust to the choice of random set of templates.</p>
<p>Introduction</p>
<p>Pretrained language models have emerged as a dominant paradigm for solving many NLP problems in a unified framework (Brown et al., 2020;Chowdhery et al., 2022;Scao et al., 2023;Touvron et al., 2023a).In particular, these models can achieve impressive downstream results with just a few demonstrations given as a part of their input (Liu et al., 2021;Min et al., 2022c), which is often called a prompt in this case.</p>
<p>These few-shot or in-context learning (ICL) abilities (Brown et al., 2020) of large models are a subject of frequent study, as the primary factors behind them are not yet fully understood.For example, one line of work investigates in-context learning within different theoretical frameworks (Xie et al., 2022;Garg et al., 2022;Akyürek et al., 2023).In addition, multiple publications study the importance of different prompt attributes, such as the order of input demonstrations (Lu et al., 2022a) and their labels (Min et al., 2022d).</p>
<p>As shown in Zhao et al. (2021); Min et al. (2022a), the prompt format (i.e., a transformation from a set of examples to a natural language input) is also highly important.However, this aspect is often overlooked in most existing studies.Namely, works proposing modifications of ICL frequently present their results for a specific template without specifying the criteria guiding its selection.Furthermore, even when the results are averaged over a set of templates, they are compared to methods that were evaluated on a different set of templates.We illustrate this common discrepancy in Appendix A. Such inconsistency can lead to a misinterpretation of the reported results: the difference between the performance of two methods may be explained by the variation across prompt formats rather than the methods themselves.</p>
<p>In this work, we evaluate the template sensitivity of 21 models from 8 families, including state-ofthe-art open-access models, such as Llama 2 (Touvron et al., 2023b) and Falcon (Almazrouei et al., 2023), as well as latest instruction-tuned models, such as Mistral (Jiang et al., 2023) and Llama 3 Instruct (AI@Meta, 2024).We show that this issue persists regardless of the model size and the number of demonstrations.Moreover, comparing various in-context learning enhancements while taking the template influence into account renders the superiority of one method over others less apparent.Therefore, it is likely that the gains reported for advanced prompting methods can often be attributed to a luckily chosen template.Figure 1: An example template transformation for two demonstrations.Different prompt formats lead to different rankings both for models and ICL methods, and the best template for one method can be suboptimal for others.</p>
<p>Crucially, there are no universally best templates for a given task.The best performing demonstration format for a fixed evaluation setting (i.e., the dataset, the model, the demonstration set, and the prediction method) does not transfer consistently across models (even within the same family), demonstration sets, or different prediction methods.We find this concerning, as even the best template for a given setting can produce poor results after slight changes, which makes "tuning" the template a very difficult task.</p>
<p>As a first step towards addressing template sensitivity in a practical way, we propose Template Ensembles -a test-time augmentation approach that averages model predictions over several prompt formats.This method is easy to implement and increases the average performance across templates for multiple prompting methods while reducing the sensitivity of these methods to the template choice.</p>
<p>In summary, our contributions are as follows:</p>
<ol>
<li>
<p>We conduct a broad evaluation 1 of prompt template sensitivity across 21 models and 4 datasets, showing that the performance gains similar to using in-context learning improvements can be achieved solely by selecting a proper template.</p>
</li>
<li>
<p>We show that the choice of the best template depends on a combination of factors and that it is not possible to transfer the best template between models or prompting methods without a negative impact on quality.</p>
</li>
<li>
<p>We propose Template Ensembles as a baseline solution for improving the template robustness for in-context learning.</p>
</li>
</ol>
<p>1 Our code and results of all evaluations can be found at github.com/yandex-research/mind-your-format 2 Background and Related Work</p>
<p>In-Context Learning</p>
<p>An important property of LLMs is their ability to learn new tasks from only a few demonstrations (Radford et al., 2019;Brown et al., 2020).This capability, known as in-context learning, forms the focus of our work.We focus on sequence classification, as it is the most widely studied task for understanding and improving ICL performance.</p>
<p>Formally, classifying an input x test with incontext learning can be described as finding the class c in the space of label tokens C that yields a sequence with the highest probability according to a language model.The input sequence consists of demonstration inputs and labels (x i , y i ) and a test input (x test , c); to obtain a natural language input, demonstrations are formatted with a template.</p>
<p>Each template consists of four components: input and output verbalizers v I (x) and v O (y, C) that transform (x i , y i ) into a natural language text, an intra-separator to divide input from output, and an inter-separator to join several demonstrations.Figure 1 shows an example of transforming a set of demonstrations into a context for ICL.</p>
<p>In-Context Learning Analysis</p>
<p>Recent work has shown that ICL can perform at levels comparable to finetuning (Chowdhery et al., 2022;Hoffmann et al., 2022).Still, in-context learning is known to be highly dependent on the way the model input is formed: a prompt is defined by several components, and altering any of them can lead to unpredictable changes in performance.</p>
<p>Template Selection There are multiple ways to construct a template for a task.The most straightforward approach is to use minimal templates (v I = {x}, v O = {C[y]}) or universal verbalizers like "input/output", as done in Wang et al. (2023) and Wei et al. (2023).</p>
<p>Another strategy is to create task-specific templates.Jiang et al. (2020) generate paraphrases of templates for the relation extraction task.Authors show the sensitivity of masked language models to the prompt format and propose to ensemble predictions over the best templates.Compared to this method, our approach is task-agnostic and does not require evaluating all templates in advance.</p>
<p>Several studies aim to find templates that directly optimize in-context learning performance (Shin et al., 2020;Gao et al., 2021).Our work unifies the results of previous research, using the verbalizers proposed by Gao et al. (2021), as well as minimal and universal templates.</p>
<p>Choice and Order of Demonstrations The choice of examples for ICL is highly important, as they enable the model to condition on correct input and label distributions for the task (Wu et al., 2023;Nguyen and Wong, 2023;Min et al., 2022d;Chang and Jia, 2023).Furthermore, the order of examples also significantly affects the results and does not transfer between models even within the same family (Lu et al., 2022b;Zhao et al., 2021).</p>
<p>In this work, we analyze two recent methods for selecting demonstrations.Wang et al. (2023) propose learning latent concept variables for a task and using them to find examples that can best predict the task concept.We refer to this method as Implicit Topic Models or ITM.In turn, Z-ICL (Lyu et al., 2023) generates pseudo-demonstrations by retrieving most similar examples to the test sentence from an unlabeled dataset and assigning random labels to retrieved examples.</p>
<p>Crucially, both methods are evaluated on single templates that differ across two works.Therefore, it is unclear whether the reported performance gains arise from the methods themselves or from a particular combination of the example selection strategy, the model, and the chosen template.</p>
<p>Prediction Methods</p>
<p>The standard approach for classification with LLMs is to compute the sequence probability with each of the possible labels and select the label with the highest probability.We refer to this method as DIRECT further on.</p>
<p>Alternatively, one can use more advanced prediction methods that aim to reduce the variance across prompt formats.The CALIBRATION method (Zhao et al., 2021) computes a correction factor based on the deviation of the model's predictions for a placeholder input from a uniform distribution over labels and applies this factor to test set predictions.</p>
<p>Recent work has proposed multiple improvements of this method (Fei et al., 2023;Han et al., 2023;Zhou et al., 2024); to limit the scope of our study, we focus only on the base CALIBRATION approach in this work.Lastly, the CHANNEL prompting technique, proposed in Min et al. (2022b), maximizes P (x|y) instead of P (y|x).</p>
<p>Both of these methods aim to mitigate the issue of ICL sensitivity to the prompt template choice.However, as we show in Appendix A, these methods are evaluated on their own sets of templates.In this paper, we strive for a more unified view on the robustness of advanced prompting methods and compare their performance across a broader range of templates and models.</p>
<p>Prompt and Template Robustness Although the problem of prompt robustness is relatively wellknown, until recently, the discussion of template robustness has been limited.Notably, Sclar et al. (2023) present a highly relevant study of prompt format sensitivity, reporting a significant performance variation across formats even for large models or minor template changes.While their experiments are conducted in the standard setup (randomly selected examples and default prompting), our work instead focuses on alternative prompting and example selection methods, several of which (Zhao et al., 2021;Min et al., 2022b) were proposed to improve the prompt robustness of ICL.Similarly to papers in other subfields of machine learning arguing for a more consistent methodology (Dacrema et al., 2019;Musgrave et al., 2020;Platonov et al., 2023), the goal of our work is to demonstrate that disparate experiment setups lead to an invalid comparison of competing methods.</p>
<p>Moreover, several works study prompt robustness in a broader sense by considering models that use natural language instructions instead of labeled demonstrations (Webson and Pavlick, 2022;Leidinger et al., 2023;Weber et al., 2023).Recently, Mizrahi et al. (2023) have shown that very similar instructions can lead to drastic differences in task performance for a variety of instruction-tuned models.Although we study a similar issue, the focus of our work is on in-context learning and the transfer of best prompts between evaluation setups.Still, we find that instruction-tuned models lack in-context robustness as well, which confirms previous observations and emphasizes the need for language model evaluation that takes prompt design into account.</p>
<p>Model family</p>
<p>Parameters (B) GPT-J (Wang and Komatsuzaki, 2021) 6 GPT-NeoX (Black et al., 2022) 20 BLOOM (Scao et al., 2023) 1.7, 3, 7.1 OPT (Zhang et al., 2022) 6.7, 30, 66 Pythia (Biderman et al., 2023) 6.9, 12 LLaMA (Touvron et al., 2023a) 7, 13, 30, 65 Llama 2 (Touvron et al., 2023b) 13, 70 Falcon (Almazrouei et al., 2023) 1, 7, 40</p>
<p>Llama 3 Instruct (AI@Meta, 2024) 8 Mistral v0.3 Instruct (Jiang et al., 2023) 7</p>
<p>Table 1: Language models used in our work.</p>
<p>3 Setup &amp; Methodology</p>
<p>Models and Data</p>
<p>We evaluate the robustness of in-context learning to template selection across a wide range of models on classification tasks.All models used in our work are listed in Table 1: we run experiments on model families frequently used in literature (such as OPT and BLOOM), as well as the latest models with the highest quality (such as Llama 2 and Falcon).</p>
<p>In preliminary experiments, we observed that the performance of some models in the few-shot regime lags behind their zero-shot results.Hence, we excluded these models from further investigation.Further details regarding this selection procedure can be found in Appendix B.</p>
<p>We experiment with 4 sequence classification datasets: SST-2 (Socher et al., 2013), DBPedia ontology classification task (Lehmann et al., 2015), AGNews (Zhang et al., 2015), and TREC Question Classification (Li and Roth, 2002).Although these datasets are frequently used in ICL studies, there is no consensus regarding the templates that should be used for each task.</p>
<p>One can construct an input for in-context learning from a set of demonstrations by using a template consisting of four parts, as illustrated in Figure 1.We present all options for verbalizers and separators for each dataset we study in Table 2.</p>
<p>Any combination of these components results in a valid template.This set of options results in 216 possible prompt formats for SST-2 and 168 for DBPedia, AGNews and TREC.A single evaluation run of all models on 10 random templates in one setup takes 17-48 hours on a single NVIDIA A100-80GB GPU, depending on the dataset.</p>
<p>Methods</p>
<p>Along with studying the robustness of standard in-context learning, we consider its improvements proposed in prior work.We focus on two main directions of ICL enhancements mentioned in Section 2: example selection and prediction methods.For each setting, we aggregate the results over 3 random seeds for example selection, with 10 random templates used for each seed and report the mean and standard deviation of classification accuracy, unless specified otherwise.</p>
<p>As a baseline for demonstration selection, we choose the most straightforward approach of selecting N random examples from the training dataset.Intuitively, selecting more relevant examples for ICL should yield better performance.Therefore, we investigate the template sensitivity of two demonstration selection methods described in Section 2.1: ITM and Z-ICL.Specifically, we select N = 4 examples using official implementations of each method.</p>
<p>Importantly, ITM requires training a concept model before choosing the examples.For GPT-2 Large, this procedure takes approximately 30 hours on a single NVIDIA A100-80GB.Repeating it for each model would be infeasible, especially given that the largest model has 86 times more parameters.Therefore, we use the checkpoints of the GPT2 Large concept model provided by the authors to select demonstrations.Also, we reuse the same examples for all models, leveraging authors' observations that demonstrations chosen with ITM can be transferred between models.</p>
<p>Dataset</p>
<p>Input verbalizer Output verbalizer Intra-separator Inter-separator SST-2 "output: {}", "target: {}", "label: {}", "{}" "sentence: {}", "text: {}", "input: {}", "emotion: {}", "sentiment: {}", "A {} one."</p>
<p>"\n" " ", "\n\n" "\n", " ", "It was {}.", "All in all {}.", "A {} piece."</p>
<p>DBPedia</p>
<p>"output: {}", "target: {}", "label: {}", AGNews "Topic: {}.", "Subject: {}.", TREC 'This is about {}.", "It is about {}."As discussed in Section 2, more advanced prediction techniques can improve in-context learning accuracy.Therefore, we compare DIRECT prompting with CHANNEL (Min et al., 2022b) and CALI-BRATION (Zhao et al., 2021) prediction methods.</p>
<p>Evaluation</p>
<p>Baseline Results</p>
<p>We begin with analyzing the robustness of language models to the template choice in the baseline setup.Specifically, we evaluate models in zero-shot and few-shot settings, selecting 2/4 random demonstrations and using the DIRECT prediction method.</p>
<p>Our results in Table 3 show that even the most capable models such as Falcon and Llama 2 are highly sensitive to the prompt format; Appendix C contains the results for the full set of 19 base models, and Appendix L reports the results for instructiontuned models.Although the variance caused by this sensitivity makes it harder to observe the increase in ICL performance with the model size or the number of demonstrations, both trends still persist.However, even the largest models have standard deviations of scores up to 35% of their mean values.</p>
<p>To mitigate this lack of robustness, we could remove consistently underperforming prompt formats from the template pool.We analyze the impact of separate template components in Appendix D and find that there are no specific parts (for example, verbalizers or separators) which could be excluded from evaluation.Furthermore, we observe that a combination of "suboptimal" parts may result in an optimal template.</p>
<p>Prediction Methods</p>
<p>Next, we aim to evaluate the performance of different prediction methods in a unified setting.Ideally, we would like these modifications to reduce the variance across templates, making the model behavior less dependent on the input format.</p>
<p>We evaluate CHANNEL and CALIBRATION methods in the 2-shot setting along with the DI-RECT baseline.As depicted in Figure 2, both CHANNEL and CALIBRATION generally exhibit improved performance in comparison with DI-RECT.Still, for a number of models and datasets, the range of scores for DIRECT substantially overlaps with those of advanced methods.This suggests that there are templates reaching the best performance with the DIRECT prediction method.</p>
<p>Additionally, Table 10 of Appendix E reveals that despite CALIBRATION yielding the highest mean accuracy more often than other methods, it is more sensitive to the template choice than CHAN-NEL.Similar findings for instruction-tuned models are contained in Appendix L. Therefore, the choice of the prediction method should likely rely on the downstream usage scenario and the target evaluation setting.</p>
<p>Example Selection Methods</p>
<p>Another area of ICL improvements that we evaluate on the matter of template sensitivity is the example selection strategy.We compare ITM and Z-ICL methods to the RANDOM baseline in 4-shot setting, since using 4 demonstration was the main evaluation setting in the works proposing these methods.We use DIRECT prediction method to evaluate the gains of advanced example selection strategies independently from other ICL modifications.</p>
<p>Results in Figure 3 and Table 14 illustrate that when taking template sensitivity into account, advanced example selection methods often perform worse than random choice baseline.ITM increases the average performance in most cases but still has a remarkably high standard deviation across templates.Examples selected using the Z-ICL method lead to more consistent but worse performance.Note that our evaluation setup differs from those described in the original works, which might explain the discrepancy between our findings and the results reported by authors.Namely, we use the DIRECT prompting method and sample 10 random templates that may not include the templates used by authors of ITM and Z-ICL.To confirm template instability for prediction methods in their original implementations, we reproduce both methods and report our findings in Appendix F. We observe high sensitivity to the prompt format, which raises a question of how much the reported gains of these methods can be attributed to the methods themselves and not to the template choice.</p>
<p>We conclude that the prompt format should be viewed as important as the example selection or the prediction method for ICL evaluation.However, the search space of possible templates is infinite, which makes exhaustive search for each combination of the dataset, the model and the number of examples impractical.Ideally, the best template for one setting would be optimal for all others or at least for similar settings.However, as we demonstrate in the following section, this is not the case.</p>
<p>Template Transfer Evaluation</p>
<p>Setup</p>
<p>We begin by defining a successful transfer between ICL settings.In order to do so, we evaluate how the quality of model predictions varies across 30 random templates from Table 2.The results described in Appendix H demonstrate that the top-10 template on average yields 90% of the best template score.Therefore, if a prompt format is present in top-10 for both of the two compared setups, we can consider this an instance of successful transfer.</p>
<p>To compare sets of the best templates for a pair of settings, we compute Intersection-over-Union (IoU), also known as the Jaccard similarity coefficient (Jaccard, 1912), for top-10 best templates in each setting.We also consider using the ρ rank correlation coefficient (Spearman, 1904) as another measure of template transfer.However, its value can increase when low-performing templates have similar rankings in different ICL setups, while the transfer of efficient templates remains low.Still, we provide the results for this metric in Appendix I.</p>
<p>Transfer Between Models</p>
<p>Next, we analyze the transfer of the bestperforming templates between models in the baseline setup.Specifically, we collect the results of each model in the 2-shot learning setting with DI-RECT prediction method and RANDOM demonstrations (fixed throughout the experiments) for 30 templates.A heatmap of IoU for the transfer of top-10 best templates between 19 base models on the DB-Pedia dataset is presented in Figure 4; for other datasets, please see Appendix J.</p>
<p>We observe that the IoU values exceed 0.5 only for a few model pairs on all datasets, meaning that the capacity for template transfer between models in the same setup is generally low.This is especially concerning for models within a single family: as these models are trained on the same data and have the same architecture, one would expect them to perform similarly on the same prompt formats.</p>
<p>These observations signify that comparing ICL methods across models with a single template can lead to incorrect conclusions: a template that is effective for one model can easily be one of the worst choices for another model.</p>
<p>Transfer Between Prediction Methods</p>
<p>As discussed in Section 4.2, no prediction method that we evaluate can consistently outperform others across all models and datasets.Therefore, to find an optimal setup for a new ICL improvement, one needs to evaluate every prediction technique in multiple templates.We investigate the possibility of finding a universally optimal prompt for different methods to reduce the total computational cost.To answer this question, we calculate the IoU between top 10 performing templates for each method for a fixed set of demonstrations.Results in Table 4 display that similarly to the models, the transfer between prediction methods is also low.Consequently, the prompt format sensitivity issue creates a burden on authors of new ICL modifications; they must tune templates for every prediction method they want to combine with their own approach.</p>
<p>Transfer Between Demonstration Selection Methods</p>
<p>Having found that the best-performing templates are specific both to the model and the prediction method, we now aim to find whether the best formats would be the same for different demonstration sets in the same setup.Similarly to previous experiments, we calculate IoU for 10 templates that yield the highest scores for each method.</p>
<p>Results in Figure 5 illustrate that simply adding demonstrations, even if they were obtained with the same method, can significantly alter the ranking of the best templates.This justifies the necessity to evaluate example selection methods on a range of templates to avoid misinterpretation of the results.</p>
<p>Discussion</p>
<p>Based on the above findings, we conclude that the results of evaluation of various ICL improvements without consideration of template sensitivity issue are hardly reliable for several reasons.First, as the best templates do not transfer between models even within the same family, scoring a method across several models using the same format will inevitably lead to underestimation of the method for all models except the one for which the format was tuned.Next, as there is little evidence of transfer between setups, the format selection procedure needs to be precisely described and applied in all evaluated settings for a fair comparison.In summary, we find that there are no universally well-performing prompt formats.Therefore, the results of in-context learning evaluation can be reliable only if they are aggregated over several templates or if each setting is evaluated in its best-performing template.The former approach requires accounting for the variance of the scores and makes comparison less apparent, while the latter can be computationally expensive.</p>
<p>Template Ensembles</p>
<p>To reduce the variance in performance caused by the template choice, we propose to ensemble model predictions across multiple templates.This approach is widely used in machine learning (Ho, 1995;Lakshminarayanan et al., 2017) for improving the predictive performance of the model, as well as its robustness, and can be viewed as a form of test-time augmentation (Krizhevsky et al., 2012;Simonyan and Zisserman, 2015).Prior work on prompt ensembling has shown significant gains by training a boosting algorithm on model outputs (Hou et al., 2023); by contrast, our method needs only the pretrained model predictions without additional training.Formally, our method computes label probabilities across predictions for each of N templates, where N is the ensemble size, and outputs the label with the highest average probability.In early experiments, we tried selecting the most common label among the predictions; however, we found this voting strategy to perform poorly on tasks with a large number of classes.It is also important to note that ensembling N predictions involves running the model N times more compared to singleformat evaluation, which makes this approach more computationally expensive.We view template ensembles as a baseline solution for the problem of prompt format sensitivity and leave the exploration of more efficient methods to future work.</p>
<p>We begin with determining the minimal ensemble size that consistently reduces variance while increasing the average performance.We observe that for the majority of models and prediction methods, an ensemble achieves the best accuracy when its size reaches 4 or 5 (see an example in Figure 6), with further expansion being less effective.We also found that smaller ensembles may demonstrate unstable behavior, with the possibility of a drop in performance if a suboptimal template is sampled.Therefore, we report results for ensembles of size 5 and average the results over 5 random seeds.Next, we evaluate the performance gains of Template Ensembles for different prediction methods.Our findings in Tables 5 and 20 and Appendices K and L indicate that ensembles increase the accuracy for all evaluated models and prediction methods.Most importantly, they also significantly reduce the variance caused by the template choice for most setups.Therefore, we conclude that template ensembling allows to preserve the increase in accuracy provided by ICL modifications while mitigating the template sensitivity issue.</p>
<p>Conclusion</p>
<p>In this work, we study the inconsistencies in the evaluation of in-context learning advancements introduced by the template sensitivity of large language models.Specifically, we find that ICL improvements exhibit high variation across template formats and that it is not possible to reuse the same template across different modifications.This aspect is often overlooked in prior work, despite the fact that the impact of template selection on prediction accuracy may be comparable with the choice of demonstrations or prompting methods.</p>
<p>While we propose Template Ensembles as an initial solution to this problem, the general sensitivity of language models to minor prompt variations is yet to be addressed.Consequently, we believe that the research community should take this problem into account when developing new models, evaluation benchmarks, or in-context learning methods.</p>
<p>Limitations</p>
<p>Due to limited computational resources and the high cost for evaluation on a large range of models, we only focus on four classification datasets.Moreover, we only compare two example selection methods to a random baseline, potentially overlooking other effective approaches.</p>
<p>Additionally, the space of templates could be expanded for more comprehensive experimentation.For example, we did not explore label mapping, including random labels, which is an important aspect of the template.</p>
<p>We would like to notice that our study focuses on a template selection impact on a performance and a degree of template transfer between different setups but not on templates themselves.Future work should further analyze not only which templates lead to a change in performance but also on why they affect it.</p>
<p>A Templates from Prior Work</p>
<p>Tables 6 to 9 provide a comparison of all the templates used in the works presenting all methods we evaluate.Noticeably, prompt formats (and the choice of label words for some formats and datasets) used in works proposing investigated methods have no intersection.This is also concerning, since the original papers proposing these methods refer to each other.For instance, CHANNEL prompting outperforming CALIBRATION in Min et al. (2022b) might be explained by selecting a more favorable set of templates for the method proposed in the paper rather than by the advantages of the method itself.</p>
<p>B Model Selection</p>
<p>Our initial evaluation pool consisted of 23 models.We evaluated each of them in 0-shot and 2shot settings with three prediction methods on four datasets, resulting in 12 runs.For each run in both 0-shot and 2-shot setups, we compare the model performance averaged over 10 random templates.</p>
<p>Based on the results presented in Table 10, we restricted the final pool of models for evaluation to those that have a consistent increase in performance in the 2-shot setting, in other words, to those demonstrating a performance boost from ICL.More specifically, we kept the models that had 8 or more wins in 2-shot evaluation against 0-shot.</p>
<p>C Full Baseline Results</p>
<p>Table 11 shows the results of evaluation of all 19 models in the default setting with a varying number of few-shot examples.These results illustrate that the template sensitivity issue is present in all models regardless of their size, and is not efficiently mitigated with the increase in the number of demonstrations.</p>
<p>D Template Parts Analysis</p>
<p>In addition to studying prompt format sensitivity in general, we analyze how each part of a template impacts model performance.For instance, it could be possible that the inclusion of a certain verbalizer in a template consistently leads to a decline in accuracy, irrespective of the other components.</p>
<p>To find that out, we decompose all templates into their parts and measure the distribution of scores for different variations of each component separately.The results presented in Figure 7 illustrate that even for state-of-the-art models, such as Llama 2 70B and Falcon 40B, many components exhibit high variance; also, the variance differs between two models.In other words, even if a certain template yields good performance and low variance for a given setup, it is not guaranteed to work consistently well in other setups, and changing a single component could have detrimental effects.</p>
<p>Along with the non-transferability of whole templates, we notice that individual components also do not transfer both between models and prediction methods.For instance, while "It was {}" ranks highest among output verbalizers for Llama 2 70B with the DIRECT prediction method, it is one of the worst for Falcon 40B.</p>
<p>Moreover, while a combination of best verbalizers is often a well-performing template, it is not necessarily the best one; the same is applicable for "bad" verbalizers too.For example, "input: {}\n sentiment: {}\n\n" is the best template for Falcon 40B with the DIRECT method, even though "sentiment: {}" is one of the "worst" output verbalizers for that model.</p>
<p>In summary, there is a complex interaction between the components of a template and their influence on model performance.We hypothesize that the transfer of both whole prompt templates and their parts is limited and requires further analysis.</p>
<p>E Prediction Methods</p>
<p>We provide the results of advanced prediction methods evaluation for all models in 0-shot and 2-shot setting with random demonstrations in Table 10.We conclude from this comparison that neither of the advanced prediction strategies do not decrease prompt format sensitivity consistently across models and datasets.Moreover, when accounting for the spread in accuracy scores caused by this issue, the advantages of these methods over DIRECT become less apparent.</p>
<p>F Reproduction of Results for Advanced Selection Methods</p>
<p>To evaluate the sensitivity of example selection methods to the template choice, we compare how the results reported in original works on these methods change when evaluated on a set of random templates instead of a predefined single one.For an accurate reproduction of original setups, we evaluate both Z-ICL and ITM using CHANNEL prompting with corresponding templates from Tables 6 and 7.</p>
<p>Method</p>
<p>Input verbalizer Output verbalizer Intra-sep Inter-sep Label words ITM "sentence: {}" "{}" " " " " negative, positive z-ICL "Review: {}" "Sentiment: {}" "\n" "\n\n\n" terrible, great Channel "{}" "A {} one" " " " " terrible, great "{}" "It was {}." " " " " terrible, great "{}"</p>
<p>"All in all {}." " " " " terrible, great "{}" "A {} piece."" " " " terrible, great Calibration "Review: {}" "Answer: {}" "\n" "\n\n" Negative, Positive "Review: {}" "Answer: {}" "\n" "\n\n" bad, good "Review: {}" "Positive review?{}" "\n" "\n\n" No, Yes "Input: {}" "Sentiment: {}" "\n" "\n\n" Negative, Positive "Review: {}" "Positive: {}" "\n" "\n\n" False, True "My review for last night's film: {}" "The critics agreed that this movie was {}" " " "\n\n" bad, good "One of our critics wrote {}" "Her sentiment towards the film was {}" " " "\n\n" Negative, Positive "In a contemporary review, Roger Ebert wrote {}."</p>
<p>"Entertainment Weekly agreed, and the overall critical reception of the film was {}"</p>
<p>" " "\n\n" bad, good "Review: {}" "Question: Is the sentiment of the above review Positive or Negative?\nAnswer: {}" "\n" "\n\n" Negative, Positive "Review: {}" "Question: Did the author think that the movie was good or bad?\nAnswer: {}" "\n" "\n\n" bad, good "Question: Did the author of the following tweet think that the movie was good or bad?\nTweet: {}" "Answer: {}" "\n" "\n\n" bad, good "{}" "My overall feeling was that the movie was {}" " " "\n\n" bad, good "{}" "I {} the movie."" " "\n\n" hated, liked "{}" "My friend asked me if I would give the movie 0 or 5 stars, I said {}" " " "\n\n" 0, 5  "This is about {}." " " " " "{}"</p>
<p>"It is about {} one."" " " " Calibration "Article: {}" "Answer: {}" "\n" "\n\n" Same as above. .From these results, we conclude that both methods are not robust to the template choice, as the mean performance decreases for multiple models while the standard deviation across seeds increases.Therefore, the gains from advanced example selection methods are caused to a certain degree by the choice of a proper prompt format rather than the retrieved demonstrations.</p>
<p>G Example Selection Methods</p>
<p>Full results of evaluation of demonstration selection techniques in 4-shot learning using the DI-RECT prediction method are presented in Table 14.The results highlight that advanced example selection techniques often perform comparably to the random choice baseline when evaluated on multiple templates.One might argue that the prompt format choice is inseparable from the method itself and thus such a comparison is invalid.However, since the best-performing formats do not transfer between models or demonstration sets of different sizes selected with the same method, a proper evaluation would require finding the best template for each setup.This procedure both is computationally expensive and difficult to accomplish, as authors of example selection methods frequently omit the description of their format selection algorithm.</p>
<p>H Accuracy as a Function of Template Rank</p>
<p>We plot the dependence of accuracy on the rank of the template in Figure 8.The results are aggregated across 19 models.Each model was evaluated on 30 random templates with the DIRECT prediction method and the same set of 2 randomly selected demonstrations.We observe that for SST-2 and AG-News datasets, the mean quality of the tenth-best template is within 0.9 of the best template score, which we consider a successful transfer.Despite the more rapid decay for DBPedia and TREC, taking variation across models into account, we still count first 10 formats as performing on par with the best one.</p>
<p>I Transfer Evaluation with Spearman Rank Correlation</p>
<p>One of the possible means to evaluate template transfer is to calculate the Spearman rank correlation between scores of all templates.As can be seen from Figure 9, this method yields higher correlations than IoU over 10 best formats, but the capacity for transfer is still far from perfect (for example, for SST-2 and TREC datasets).</p>
<p>J IoU Transfer For All Datasets</p>
<p>Similarly to Figure 4, we provide Intersection-over-Union of 10 best prompt formats for all 19 models and all datasets explored in our work in Figure 10.These heatmaps illustrate that the transfer of bestperforming templates between models is remarkably low for all datasets.</p>
<p>K Additional Results For Template Ensembles</p>
<p>Tables 15 and 16 show the results of Template Ensembles evaluation on a broader set of models and datasets.For most setups, ensemble of size 5 exhibit better performance than a single template.</p>
<p>L Evaluation of Instruct Models</p>
<p>We validate that our findings hold true even for the latest instruction-tuned models, such as Llama 3 8B Instruct and Mistral v0.3 7B Instruct.First, as we show in Table 17, in the baseline setting, an increase in number of demonstrations generally leads to a better performance of instruct models but does not significantly decrease variance of their final scores, similarly to what we observe in base models.</p>
<p>Second, Table 18 demonstrates that, after adjusting to template robustness, the default DIRECT prediction method performs on par with more advanced methods, e.g.CHANNEL and CALIBRA-TION, while sometimes having a less variance of the model's scores.</p>
<p>Finally, we observe that the examples retrieved by Z-ICL method turn out to be consistently worse than two other methods.This is also in line with our observation for base models.However, in  contrast to our main results, where there was no evident winner between two other methods, the demonstrations selected with ITM method turn out to be slightly more robust to template choice for instruction-tuned models, as we show in Table 19.</p>
<p>L.1 Ensemble Results</p>
<p>Table 20 shows that applying Template Ensembles method to instruct models results in improved mean classification accuracy with significantly reduced variance across different templates in all configurations that we test.</p>
<p>FFigure 2 :
2
Figure 2: Comparison of in-context learning prediction methods in the 2-shot setting.</p>
<p>FFigure 3 :
3
Figure 3: Comparison of the selection methods in the DIRECT 4-shot setting.For the evaluation results of other models and datasets, please refer to Appendix G.</p>
<p>Figure 4 :
4
Figure 4: IoU of top-10 templates for all base models with 2 random demonstrations and the DIRECT prediction method on the DBPedia dataset.</p>
<p>Figure 5 :
5
Figure 5: IoU of 10 best templates for example selection methods on the AG News dataset.METHOD-N indicates that METHOD was used to select N examples.</p>
<p>Figure 6 :
6
Figure 6: Template ensemble accuracy as a function of its size for Falcon 40B on the SST-2 dataset in the 2-shot learning setup.Dashed lines depict the results of baseline methods averaged over 10 templates.</p>
<p>Figure 7 :
7
Figure 7: Accuracy for evaluation of templates with fixed parts on the SST-2 dataset with RANDOM 2-shot for all prediction methods.</p>
<p>Figure 8 :
8
Figure 8: Relative quality of templates sorted by their classification accuracy.The shaded area indicates the standard deviation across 19 models.</p>
<p>Table 2 :
2
Possible choices for all components of templates used in our work..170.77 0.15 0.36 0.25 0.44 0.23 0.52 0.17 0.56 0.19 0.26 0.09 0.31 0.09 Falcon 7B 0.77 0.16 0.83 0.16 0.40 0.21 0.49 0.18 0.51 0.20 0.60 0.19 0.32 0.09 0.39 0.11
ModelSST-2DBPediaAGNewsTREC2-shot4-shot2-shot4-shot2-shot4-shot2-shot4-shotFalcon 1B 0.65 0Falcon 40B 0.79 0.17 0.92 0.07 0.42 0.15 0.54 0.06 0.64 0.23 0.75 0.09 0.36 0.07 0.46 0.10Llama 2 13B 0.79 0.17 0.92 0.07 0.40 0.15 0.51 0.09 0.70 0.15 0.76 0.09 0.32 0.09 0.41 0.14Llama 2 70B 0.83 0.14 0.92 0.09 0.46 0.15 0.60 0.05 0.76 0.14 0.82 0.05 0.41 0.07 0.51 0.06</p>
<p>Table 3 :
3
Classification accuracy in the baseline setting for 2 LLM families.Standard deviation across 30 runs (10 templates for 3 sets of demonstrations) is in underscript.The results for all base models are presented in Appendix C.</p>
<p>Table 4 :
4
Intersection-over-Union for pairs of prompting methods averaged over the results of 19 base models obtained in the RANDOM 2-shot setup.Standard deviations are in subscript.
Calibration Channel CalibrationSST-20.49 0.170.30 0.110.31 0.08DBPedia0.54 0.170.47 0.150.45 0.14AG News 0.36 0.110.25 0.130.35 0.14TREC0.31 0.120.23 0.090.28 0.13
Direct ↔ Direct ↔ Channel ↔</p>
<p>Table 6 :
6
All templates used in methods we evaluate for SST-2 dataset.
MethodInput verbalizerOutput verbalizer Intra-sep Inter-sepLabel wordsCompany,Educational Institution,Artist, Athlete,"{}""Topic: {}."" "" "Office Holder,Channel"{}" "{}""Subject: {}." "This is about {}."" " " "" " " "Building, Natural Place, Village,"{}""It is about {} one."" "" "Animal, Plant,Album, Film,Written Work,Mean of TransportationITM"{}""{}"" "" "Same as aboveCompany, School,"Classify theArtist, Athlete,documents basedPolitician,Calibrationon whether they are about"Answer: {}""\n""\n\n"Building, Nature, Village, Animal,a [Label words]Plant, Album,\n\n Article: {}"Film, Book,Transportation</p>
<p>Table 7 :
7
All templates used in methods we evaluate for DBPedia dataset.
MethodInput verbalizerOutput verbalizer Intra-sep Inter-sepLabel wordsChannel"{}" "{}" "{}" "{}""{}" "Q: {}." "Why {}?" "Answer: {}"" " " " " " " "" " " " " " " "Description, Entity, Expression, Human, Location, Number"Classify theCalibrationquestions based on whether their answer type is a [Label words]\n\n"Answer Type: {}""\n""\n\n"Number, Location, Person, Description, Entity, AbbreviationQuestion: {}"</p>
<p>Table 8 :
8
All templates used in methods we evaluate for TREC dataset.
MethodInput verbalizer Output verbalizer Intra-sep Inter-sepLabel words"{}""Topic: {}."" "" "Channel"{}" "{}""Subject: {}."" "" "World, Sports, Business, Technology</p>
<p>Table 9 :
9
All templates used in methods we evaluate for AG News dataset.
ModelNSST-2 Direct Channel Calib. Direct Channel Calib. Direct Channel Calib. Direct Channel Calib. DBPedia AGNews TREC2-shot wins0 0.650.09 0.720.04 0.700.06 0.340.13 0.400.07 0.490.09 0.480.16 0.560.04 0.640.14 0.250.06 0.280.11 0.300.08 GPT-2 Large 2 0.590.10 0.700.12 0.620.08 0.140.08 0.530.10 0.580.10 0.320.12 0.580.07 0.570.09 0.260.09 0.290.08 0.300.066/12GPT-2 XL0 0.760.04 0.730.05 0.700.09 0.400.05 0.430.08 0.540.09 0.520.09 0.560.05 0.650.08 0.230.03 0.240.07 0.260.05 2 0.580.11 0.710.09 0.630.11 0.150.09 0.540.09 0.500.15 0.400.20 0.610.08 0.560.15 0.260.07 0.340.09 0.330.085/12GPT-J0 0.710.09 0.680.08 0.680.08 0.410.07 0.440.06 0.570.08 0.610.08 0.640.03 0.640.07 0.320.05 0.200.07 0.330.04 9/12 2 0.650.14 0.770.11 0.680.11 0.250.16 0.680.06 0.710.16 0.470.19 0.670.09 0.730.11 0.260.07 0.320.09 0.330.06GPT-NeoX0 0.710.08 0.670.06 0.700.09 0.480.04 0.420.05 0.600.07 0.670.06 0.560.04 0.580.07 0.300.08 0.220.06 0.320.05 9/12 2 0.690.15 0.820.06 0.790.12 0.320.19 0.670.05 0.720.16 0.520.22 0.670.10 0.700.13 0.310.08 0.320.07 0.360.08OPT 1.3B0 0.780.07 0.680.07 0.790.07 0.410.05 0.330.08 0.570.12 0.490.07 0.600.01 0.670.07 0.270.04 0.170.06 0.240.03 2 0.690.15 0.800.06 0.710.16 0.210.11 0.580.08 0.610.12 0.480.24 0.660.06 0.610.11 0.270.08 0.380.09 0.350.086/12OPT 6.7B0 0.790.07 0.670.07 0.800.07 0.460.04 0.490.05 0.610.06 0.590.08 0.610.06 0.640.07 0.240.04 0.270.09 0.330.02 9/12 2 0.670.16 0.810.06 0.720.19 0.270.14 0.690.05 0.710.17 0.450.17 0.690.09 0.700.14 0.270.08 0.340.08 0.340.07OPT 30B0 0.790.06 0.720.05 0.770.08 0.480.04 0.480.07 0.610.08 0.640.05 0.600.06 0.650.11 0.240.03 0.260.05 0.310.01 8/12 2 0.640.17 0.790.09 0.730.17 0.340.21 0.730.05 0.780.14 0.550.19 0.690.09 0.760.11 0.310.06 0.350.09 0.330.06OPT 66B0 0.730.12 0.730.07 0.740.10 0.410.03 0.480.07 0.610.09 0.640.07 0.580.06 0.620.07 0.260.03 0.230.06 0.310.05 8/12 2 0.650.15 0.810.08 0.760.16 0.340.16 0.770.06 0.810.15 0.450.17 0.740.05 0.700.14 0.280.07 0.380.08 0.340.07BLOOM 1.7B0 0.680.11 0.670.06 0.680.11 0.470.03 0.470.06 0.470.07 0.610.08 0.530.04 0.580.06 0.270.04 0.240.08 0.330.03 9/12 2 0.660.12 0.750.06 0.710.10 0.270.19 0.620.08 0.570.13 0.430.19 0.590.08 0.610.11 0.310.09 0.390.07 0.370.08BLOOM 3B0 0.710.10 0.710.06 0.700.08 0.390.06 0.400.07 0.480.05 0.660.02 0.480.06 0.600.08 0.220.06 0.200.07 0.200.06 9/12 2 0.720.14 0.770.09 0.770.10 0.270.21 0.670.06 0.570.14 0.450.19 0.620.07 0.670.13 0.340.09 0.350.08 0.360.09BLOOM 7.1B0 0.720.09 0.710.06 0.680.06 0.440.05 0.450.08 0.510.08 0.640.06 0.560.04 0.640.10 0.350.07 0.220.08 0.320.04 9/12 2 0.690.15 0.760.09 0.760.11 0.260.18 0.700.06 0.670.14 0.430.17 0.690.06 0.680.12 0.330.08 0.340.07 0.360.06Pythia 6.9B0 0.750.08 0.720.05 0.690.11 0.450.05 0.430.04 0.630.09 0.580.14 0.590.04 0.640.08 0.310.07 0.210.07 0.320.03 8/12 2 0.630.12 0.780.09 0.770.11 0.280.16 0.670.08 0.680.14 0.430.17 0.680.09 0.690.14 0.340.09 0.370.07 0.380.06Pythia 12B0 0.730.07 0.710.08 0.690.10 0.430.05 0.430.04 0.510.18 0.610.09 0.570.05 0.650.09 0.330.06 0.230.05 0.320.03 8/12 2 0.630.13 0.790.10 0.740.12 0.290.15 0.680.07 0.710.14 0.530.18 0.680.08 0.700.12 0.290.09 0.350.06 0.330.08LLaMA 7B0 0.770.08 0.700.07 0.740.12 0.460.04 0.530.05 0.550.10 0.720.05 0.650.06 0.660.06 0.340.04 0.250.07 0.300.03 8/12 2 0.720.17 0.830.07 0.830.11 0.380.21 0.760.06 0.730.13 0.610.24 0.750.08 0.720.13 0.290.10 0.380.07 0.380.11LLaMA 13B0 0.810.03 0.690.07 0.770.08 0.420.04 0.520.07 0.650.11 0.740.03 0.620.07 0.730.04 0.340.04 0.180.05 0.340.03 10/12 2 0.750.17 0.830.08 0.820.14 0.380.17 0.750.06 0.800.12 0.680.15 0.750.07 0.800.08 0.350.09 0.360.08 0.420.09LLaMA 30B0 0.760.08 0.710.07 0.760.08 0.510.03 0.470.09 0.670.08 0.750.04 0.660.05 0.740.06 0.330.08 0.210.05 0.300.04 10/12 2 0.780.17 0.810.10 0.830.14 0.430.19 0.760.09 0.800.09 0.650.22 0.740.13 0.780.07 0.340.11 0.410.08 0.430.13LLaMA 65B0 0.780.10 0.710.05 0.750.10 0.450.05 0.490.07 0.620.08 0.740.06 0.610.07 0.740.03 0.310.06 0.190.07 0.310.03 11/12 2 0.820.17 0.840.09 0.870.13 0.450.17 0.780.06 0.800.13 0.680.20 0.780.08 0.820.05 0.380.08 0.380.09 0.450.11Falcon 1B0 0.720.08 0.720.03 0.730.07 0.540.03 0.550.04 0.620.10 0.680.04 0.640.06 0.630.08 0.240.04 0.250.04 0.310.02 9/12 2 0.650.17 0.770.10 0.710.17 0.360.25 0.720.05 0.740.14 0.520.17 0.720.08 0.770.08 0.260.09 0.330.06 0.330.06Falcon 7B0 0.720.09 0.680.05 0.730.08 0.500.06 0.510.13 0.660.06 0.750.06 0.640.03 0.720.06 0.310.04 0.210.07 0.290.03 10/12 2 0.770.16 0.780.09 0.790.15 0.400.21 0.760.06 0.800.17 0.510.20 0.760.07 0.730.12 0.320.09 0.330.08 0.370.11Falcon 40B0 0.760.05 0.680.07 0.740.11 0.450.03 0.570.07 0.690.08 0.750.07 0.620.07 0.720.08 0.310.07 0.270.10 0.270.02 11/12 2 0.790.17 0.810.09 0.870.13 0.420.15 0.830.06 0.850.12 0.640.23 0.790.09 0.800.06 0.360.07 0.410.06 0.450.06Llama 2 7B0 0.700.12 0.590.08 0.620.16 0.350.04 0.290.09 0.210.22 0.680.04 0.450.10 0.410.23 0.300.06 0.130.05 0.150.16 2 0.660.13 0.690.10 0.660.16 0.140.11 0.170.14 0.160.18 0.370.14 0.400.11 0.420.20 0.260.09 0.290.09 0.210.195/12Llama 2 13B0 0.770.09 0.710.04 0.740.10 0.450.03 0.590.05 0.630.10 0.750.07 0.660.05 0.760.05 0.330.03 0.270.07 0.340.03 9/12 2 0.790.17 0.820.10 0.880.09 0.400.15 0.790.06 0.830.11 0.700.15 0.760.09 0.810.05 0.320.09 0.350.11 0.460.11Llama 2 70B0 0.850.05 0.680.08 0.770.10 0.480.04 0.530.06 0.720.13 0.780.06 0.640.05 0.770.06 0.340.03 0.180.07 0.340.04 11/12 2 0.830.14 0.830.08 0.880.13 0.460.15 0.790.10 0.840.12 0.760.14 0.780.09 0.810.09 0.410.07 0.410.10 0.510.06Highest mean, % 37.532.530.05.012.582.532.510.057.525.015.060.0Lowest std, %12.580.07.542.555.02.520.062.517.517.525.057.5</p>
<p>Table 10 :
10
Evaluation of advanced prediction methods for all models on 4 datasets in 0-shot and 2-shot with random demonstrations.Models that were removed from further evaluation are highlighted in gray."Calib."stands for the Calibration prompting method.</p>
<p>Table 14 :
14
ICL Random ITM z-ICL Random ITM z-ICL Random ITM z-ICL GPT-J 0.660.140.730.160.660.050.340.180.460.170.240.100.480.23 0.590.120.450.110.360.110.250.100.210.04GPT-NeoX 0.820.120.730.170.770.090.370.210.450.190.250.110.480.22 0.580.170.460.120.400.100.280.090.250.07OPT 6.7B 0.800.140.770.180.720.090.330.180.460.170.240.110.470.22 0.490.170.450.140.340.100.230.070.220.05OPT 30B 0.790.140.800.160.790.120.390.190.500.160.270.110.610.180.670.140.540.130.340.100.250.08 0.250.06OPT 66B 0.840.130.830.150.750.08 0.400.160.480.140.250.100.530.190.610.150.480.120.330.090.250.090.210.04BLOOM 1.7B 0.670.130.690.140.640.090.310.20 0.390.190.200.070.420.190.460.160.450.08 0.360.110.260.070.190.06BLOOM 3B 0.760.120.720.150.620.070.330.210.430.180.190.08 0.460.22 0.500.160.430.120.390.120.290.100.230.03BLOOM 7.1B 0.740.150.710.150.620.08 0.320.210.440.190.200.08 0.410.210.500.170.420.100.380.100.320.08 0.210.05Pythia 6.9B 0.770.140.720.170.700.100.350.190.470.170.220.100.430.20 0.560.160.460.120.380.130.310.08 0.220.05Pythia 12B 0.810.130.720.180.790.100.350.160.460.140.240.100.460.22 0.570.160.440.130.350.120.300.090.220.04LLaMA 7B 0.850.100.840.160.700.090.460.20 0.510.100.270.08 0.640.180.660.150.520.130.390.150.280.070.270.05LLaMA 13B 0.860.140.850.130.730.110.520.110.520.100.310.060.740.130.770.08 0.550.08 0.420.140.320.130.290.04LLaMA 30B 0.870.160.880.110.670.08 0.530.160.570.150.270.060.710.190.770.08 0.450.070.420.160.300.110.290.07LLaMA 65B 0.920.100.910.08 0.730.100.520.140.510.130.300.060.710.170.840.070.570.08 0.470.090.360.090.340.05Falcon 1B 0.770.150.770.150.710.090.440.23 0.590.180.230.060.560.190.580.150.460.100.310.090.250.100.190.04Falcon 7B 0.830.160.820.160.680.110.490.180.580.090.270.090.600.190.630.150.520.120.390.110.290.090.250.08Falcon 40B 0.920.070.920.090.750.110.540.060.540.070.280.08 0.750.090.800.060.550.110.460.100.370.110.260.08Llama 2 13B 0.920.070.860.140.750.070.510.090.530.090.250.060.760.090.820.050.540.08 0.410.140.340.110.290.04Llama 2 70B 0.920.090.910.070.750.090.600.050.590.090.280.050.820.050.850.050.600.070.510.060.390.100.340.03Evaluation of advanced selection methods for all 19 models using DIRECT prediction method in 4-shot.Results are aggregated over 10 random templates for each of the three demonstrations selection seeds.
ModelSST-2DBPediaAGNewsTRECRandom ITM z-</p>
<p>Table 15 :
15
Comparison of 2-shot learning performance on the SST-2 dataset using ensembles of 5 templates and a single template.Results are averaged over 5 random seeds.</p>
<p>Table13: Mean accuracy and standard deviation of 4-shot learning with Z-ICL demonstrations on SST-2 dataset using the CHANNEL prediction method.
deep models for semantic compositionality over a sentiment treebank.In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631-1642, Seattle, Washington, USA.Association for Computational Linguistics.C. Spearman.1904.The proof and measurement of association between two things.American Journal of Psychology, 15:88-103.In Tables12 and 13, we present the scores for a single template used in original implementations in the column "Reproduced", aggregating results over 5 example selection seeds.The "Random" column shows average scores for fixed demonstrations on a set of 10 random templates.We observe that the results obtained in our code differ from the ones reported in the papers presenting both methods (the "Paper" column).The cause of this discrepancy is presumably the difference in tokenization during preprocessing of the datasets.Both methods use the same codebase with tokenization specific to GPT tokenizers, which results in a significant drop in quality for OPT and LLaMA models.By contrast, our tokenization approach is more general and preserves the ICL performance of these models.Model
A I , Meta , Llama 3 model card. 2024</p>
<p>PromptBoosting: Blackbox text classification with ten forward passes. Bairu Hou, O' Joe, Jacob Connor, Shiyu Andreas, Yang Chang, Zhang, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine LearningPMLR2023202</p>
<p>The distribution of the flora of the alpine zone. Jaccard, In New Phytologist. 111912</p>
<p>Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>How can we know what language models know? Transactions of the. Zhengbao Jiang, Frank F Xu, 10.1162/tacl_a_00324Jun Araki, and Graham Neubig. 2020Association for Computational Linguistics8</p>
<p>Imagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hin, Advances in Neural Information Processing Systems. Curran Associates, Inc201225</p>
<p>Simple and scalable predictive uncertainty estimation using deep ensembles. Alexander Balaji Lakshminarayanan, Charles Pritzel, Blundell, arXiv:1612.014742017Preprint</p>
<p>Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick Van Kleef, Sören Auer, Dbpedia-a large-scale, multilingual knowledge base extracted from wikipedia. Semantic web. 20156</p>
<p>The language of prompting: What linguistic properties make a prompt successful?. Alina Leidinger, Robert Van Rooij, Ekaterina Shutova, 10.18653/v1/2023.findings-emnlp.618Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>Learning question classifiers. Xin Li, Dan Roth, COLING 2002: The 19th International Conference on Computational Linguistics. 2002</p>
<p>What makes good in-context examples for gpt-3?. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen, Workshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning Inside Out. 2021</p>
<p>Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp, 10.18653/v1/2022.acl-long.556Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics2022a1</p>
<p>Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp, arXiv:2104.087862022bPreprint</p>
<p>Z-icl: Zeroshot in-context learning with pseudo-demonstrations. Xinxi Lyu, Sewon Min, Iz Beltagy, Luke Zettlemoyer, Hannaneh Hajishirzi, arXiv:2212.098652023Preprint</p>
<p>Noisy channel language model prompting for few-shot text classification. Sewon Min, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, 10.18653/v1/2022.acl-long.365Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics2022a1</p>
<p>Noisy channel language model prompting for few-shot text classification. Sewon Min, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, arXiv:2108.041062022bPreprint</p>
<p>MetaICL: Learning to learn in context. Sewon Min, Mike Lewis, Luke Zettlemoyer, Hannaneh Hajishirzi, 10.18653/v1/2022.naacl-main.201Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022c</p>
<p>Rethinking the role of demonstrations: What makes in-context learning work?. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, 10.18653/v1/2022.emnlp-main.759Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022d</p>
<p>State of what art? a call for multi-prompt llm evaluation. Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, Gabriel Stanovsky, arXiv:2401.005952023Preprint</p>
<p>A metric learning reality check. Kevin Musgrave, Serge Belongie, Ser-Nam Lim, Computer Vision -ECCV 2020. ChamSpringer International Publishing2020</p>
<p>In-context example selection with influences. Tai Nguyen, Eric Wong, 2023Preprint</p>            </div>
        </div>

    </div>
</body>
</html>
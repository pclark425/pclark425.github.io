<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1096 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1096</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1096</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-8499a250422a3c66357367c8d5fa504de5424c59</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8499a250422a3c66357367c8d5fa504de5424c59" target="_blank">Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work describes a simple scheme that allows an agent to learn about its environment in an unsupervised manner, and focuses on two kinds of environments: (nearly) reversible environments and environments that can be reset.</p>
                <p><strong>Paper Abstract:</strong> We describe a simple scheme that allows an agent to learn about its environment in an unsupervised manner. Our scheme pits two versions of the same agent, Alice and Bob, against one another. Alice proposes a task for Bob to complete; and then Bob attempts to complete the task. In this work we will focus on two kinds of environments: (nearly) reversible environments and environments that can be reset. Alice will "propose" the task by doing a sequence of actions and then Bob must undo or repeat them, respectively. Via an appropriate reward structure, Alice and Bob automatically generate a curriculum of exploration, enabling unsupervised training of the agent. When Bob is deployed on an RL task within the environment, this unsupervised training reduces the number of supervised episodes needed to learn, and in some cases converges to a higher reward.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1096.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1096.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Asymmetric Self-Play</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Asymmetric Self-Play (Alice & Bob)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intrinsically-motivated two-mind RL scheme where one policy (Alice) proposes tasks by acting in the environment and a second policy (Bob) must repeat or reverse those tasks; internal rewards produce an automatic curriculum to improve Bob's ability to transition between states.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Alice & Bob (asymmetric self-play agents)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A single embodied agent with two distinct policy modules: Alice (task proposer) and Bob (task solver). Both are trained with policy-gradient RL (REINFORCE / variants, with value-baseline) using internal rewards R_A = γ max(0, t_B - t_A) and R_B = -γ t_B; policies parameterized tabularly or by neural networks depending on experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (virtual agent with embodied control in simulated environments)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Multiple (Long Hallway, MazeBase, RLLab/MountainCar, RLLab/SwimmerGather, StarCraft sub-task)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Collection of simulated embodied environments ranging from a simple 1-D chain (Long Hallway), discrete procedurally-sampled 2D grid tasks with objects and partial observability (MazeBase Light-Key), continuous control locomotion tasks (MountainCar, SwimmerGather) to a complex RTS macro-management sub-task (StarCraft training Marines). Complexity sources include state-space size, object combinatorics, continuous dynamics, partial observability, multi-unit coordination, and long multi-step action sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Varies by environment: Long Hallway: number of states M (e.g. M=25), discrete actions (3); MazeBase: number of grid cells, number of objects interacted (1-3 shown), partial observability parameter p(Light off); MountainCar: continuous state dim=2 (position, velocity), episode horizon up to 500 steps; SwimmerGather: observation dim ~33 (13 state + 20 sensors), continuous action dims (2 joints discretized to 9 bins each); StarCraft: global state vector (ore/25, N_SCV, N_Barrack, N_SupplyDepot, N_Marines), multi-agent action branching. Complexity is therefore measured by state-space size, number of objects, observation dimensionality, action branching and episode horizon.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Environment-dependent: Long Hallway: low; MazeBase: medium (combinatorial / partially observable); MountainCar: low-to-medium (continuous but low-dim); SwimmerGather: medium-high (continuous, higher-dim sensor inputs); StarCraft Marines: high (multi-unit coordination, long-horizon planning).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Procedural randomness and design choices: Long Hallway: none (fixed chain); MazeBase: per-episode random layouts and p(Light off) controls initial observability distribution; MountainCar / SwimmerGather: randomized initial states; StarCraft: randomized initial placements / deterministic game engine but potentially randomized initial seeds. The paper explicitly manipulates p(Light off) in MazeBase to vary similarity between self-play and target tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>Environment-dependent: Long Hallway: low; MazeBase: medium-high (randomized episodes and tunable p(Light off) between 0.1–0.9); MountainCar & SwimmerGather: low-to-medium (randomized initial states only); StarCraft: medium (stochasticity from gameplay and randomized seeds but fixed task spec).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task-specific: success rate on target task (fraction of episodes where target achieved), cumulative task reward (external reward), time-to-completion (t_B), and learning speed measured as number of target-task episodes to reach performance thresholds (speed-up factors).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative across domains: self-play speeds up learning (reduces required target-task episodes) relative to target-only baselines and in many cases matches or outperforms explicit exploration methods (count-based, VIME, SimHash). No single absolute numeric score is universally reported; examples: Long Hallway M=25 shows much faster early success (Fig.2); MazeBase shows substantially faster attainment of goal for p(Light off)=0.5; MountainCar and SwimmerGather: faster early reward acquisition and comparable final reward to VIME/SimHash (plots in Fig.4); StarCraft: higher final marine count reward and faster learning vs baselines (Fig.5).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper explicitly discusses and experiments with the interaction between environment variation (how self-play is sampled) and similarity to the target task. In MazeBase they vary p(Light off) to change similarity between self-play and target tasks and show that self-play yields significant speedups when self-play tasks are similar to the target tasks; the benefit diminishes or reverses when self-play is biased against the target task. They also discuss trade-offs where Alice may focus on a subset of challenges (low coverage) if allowed, producing faster performance on those but poorer general coverage—highlighting a trade-off between pushing difficulty and breadth of variation.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Automatic curriculum via asymmetric self-play combined with standard RL (policy-gradient). Training mixes self-play episodes (internal reward only) and target-task episodes (external reward); self-play percentage varies by experiment (e.g. MazeBase 20%, MountainCar 1%, SwimmerGather 10%, StarCraft 10%).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Partial/generalization tested as transfer from unsupervised self-play to separate target tasks: Bob trained with self-play required fewer target-task episodes to learn target tasks. MazeBase experiments systematically varied similarity (p(Light off)) and demonstrated that when self-play resembles the target task, transfer speedups are large; when self-play is biased against the target, transfer benefits diminish. SwimmerGather demonstrates transfer when self-play lacks apples/bombs (i.e., different reward structure) yet still accelerates target learning.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Measured as reduction in number of target-task episodes to reach performance thresholds (speed-up). Example hyperparameters: self-play % and t_max reported (see Table 1); some experiments stopped at 5×10^6 episodes for baselines. Exact numeric reductions are shown in plots (e.g. MazeBase speedups vs p(Light off) in Fig.6) but not summarized as single universal numbers in text.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Asymmetric self-play produces an automatic curriculum: Alice proposes tasks just beyond Bob's current capability, improving exploration without external reward. 2) Self-play accelerates learning (reduces required target-task episodes) across discrete and continuous embodied environments and is competitive with state-of-the-art intrinsic-exploration methods (VIME, SimHash). 3) Benefits depend on similarity between self-play distribution and the target task (explicitly shown by p(Light off) sweep): self-play helps when not biased against the target task. 4) In tabular, deterministic reversible settings, equilibrium implies Bob becomes a fast universal transition policy, but with function approximation and local updates Alice may cover only a subset of challenges (coverage vs difficulty trade-off).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1096.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1096.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Long Hallway</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Long Hallway (1-D chain toy environment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple chain-of-states testbed (M states in a line) used to illustrate curriculum formation via asymmetric self-play and to compare against count-based exploration and random-task baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Bob (with Alice proposing tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Tabular policies for Alice and Bob indexed by (start, end) state pairs; trained with policy gradient and constant baseline. Internal self-play reward structure as in main method; target task identical to self-play (move between states).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (virtual)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Long Hallway chain (M=25 in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Discrete 1-D chain of M states {s1...sM}; actions: left, right, stop. Complexity arises from length M and required exploration to reach distant states; reversible dynamics are trivial.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>State space size M (experiment used M=25), action set size = 3, episode max steps t_max = 30.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>low (M=25, 3 discrete actions)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>None/low: environment structure fixed across episodes; target start/goal pairs randomized for target task but environment dynamics constant.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>low</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Fraction of successful target-task episodes (success rate) over training episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: Asymmetric self-play (blue curve, Fig.2 left) achieves faster early success and better exploration than standard policy gradient and random-Alice baselines, comparable to explicit count-based exploration; no absolute numeric success rates reported in text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>No explicit variation dimension to manipulate here; paper uses the simple low-variation environment to show that self-play constructs a progressive curriculum (Alice gives short tasks initially, then longer ones) enabling exploration across increasing complexity (distance).</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Tabular policy-gradient with self-play episodes (self-play only in toy; no mixing with target training in this toy) and internal reward-based curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Demonstrated qualitatively via learning curves: self-play required fewer training episodes to reach non-zero success rates compared to baselines; exact numeric episode counts not provided in text (plots shown).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>In a simple reversible environment, asymmetric self-play yields a natural curriculum that encourages exploration to distant states, giving faster early learning than vanilla policy gradient and random-task baselines, and comparable performance to count-based exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1096.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1096.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MazeBase Light-Key</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MazeBase Light-Key Maze (partially observable grid with objects)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A discrete 2D grid environment with objects (light switch, key, door, goal flag) and partial observability controlled by a light switch; used to evaluate transfer from self-play to a goal-reaching target task and to study the effect of task similarity via p(Light off).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Bob (trained with Alice proposing tasks in MazeBase)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Neural-network policy (two-layer MLP encoder over object-location bag-of-words, then policy head) trained with policy gradient and state-dependent baseline; self-play uses reverse or repeat variants; target task is flag-reaching requiring sequences: turn on light, toggle key to open door, pass through.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (virtual gridworld agent)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>MazeBase Light-Key</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Procedurally-generated 2D grid mazes with a light switch (initial state randomly set with probability p(Light off)), a key and door, and (in target task) a goal flag; partial observability when light is off (agent only sees light switch). Complexity arises from combinatorial object interactions, partial observability, and planning required to manipulate objects in correct order.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Grid combinatorics (number and placement of objects), number of objects interacted (1-3 during Alice episodes observed), partial observability parameter p(Light off) which affects observation set, network input size (object-location bag-of-words) and episode horizon t_max = 80.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium (combinatorial object interactions, partial observability)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>High: per-episode randomized maze layouts and random initial states; explicit tunable variation via p(Light off) ∈ [0.1,0.9] which changes similarity between self-play and target tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-high (procedural randomness and tunable p(Light off))</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate on the target flag-reaching task; speed-up measured as reduction in number of target-task episodes to reach a performance threshold (relative to target-only training).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: With repeat-form self-play at p(Light off)=0.5, self-play agent achieves success much faster than target-only training and random-Alice baseline (Fig.2 right). A sweep over p(Light off) shows significant speedups when self-play is similar to the target task; exact numeric speed-up values are plotted (Fig.6) but not enumerated in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes. The paper explicitly manipulates p(Light off) to vary similarity between self-play and target tasks and reports that self-play yields significant speed-ups when the self-play distribution is not biased against the target task; reverse vs repeat self-play effectiveness depends on p(Light off) (Fig.6). This demonstrates a direct relationship: environment variation/sampling distribution controls transfer effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Mixing self-play episodes (20% of training) with target-task episodes; self-play used both reverse and repeat variants; policies parameterized by neural networks (100 and 50 hidden units), trained with policy gradient and state-dependent baseline; entropy regularization used.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Transfer from unsupervised self-play to the supervised target goal task was demonstrated; varying p(Light off) showed that transfer is strongest when self-play resembles the target task (e.g. reverse self-play helps more when p(Light off) low), and can be harmful when self-play is biased against target task, indicating conditional generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Reported as speed-up (reduction in required target-task episodes) relative to target-only training; Figure 6 shows speed-ups across p(Light off) values for runs that succeed (quantitative curves in figures but not tabulated in text).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Self-play substantially accelerates learning of an object-manipulation target task in a partially observable, procedurally-generated environment. 2) The match between self-play distribution and target-task distribution (controlled by p(Light off)) critically affects transfer: similarity yields large speedups; bias against the target task reduces benefit. 3) Alice and Bob automatically build a curriculum (Alice increases object interactions over training) observable in episode statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1096.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1096.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MountainCar (RLLab)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MountainCar (discretized continuous control in RLLab)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A continuous 1-D valley car task where the agent must alternate thrusts to build momentum and escape; used to compare self-play to exploration baselines VIME and SimHash.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Bob (with Alice proposing tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Neural-network policy with discrete action bins (5 bins for thrust) and an added STOP binary head; trained with policy-gradient (REINFORCE) using reversible/reset self-play (reset variant used due to asymmetry of dynamics), and target external reward +1 on successful hill climb.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (continuous-control virtual agent)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>MountainCar (RLLab)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Continuous 1-D dynamics (position and velocity observations). Complexity arises from continuous dynamics requiring multi-step momentum-building behaviors and asymmetric cost to move uphill vs downhill; episode horizon t_max = 500.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>State dimensionality = 2 (position, velocity); discretized action bins = 5; episode horizon up to 500 steps; success defined by escaping the valley.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>low-to-medium (low-dimensional continuous dynamics but requires temporally extended planning)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Randomized initial states; limited procedural variation beyond initial state randomness.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>low-to-medium</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>External target-task reward (success in climbing hill), learning curves over training iterations, convergence speed.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: Self-play (blue curve in Fig.4 left) learns faster than baselines (VIME, SimHash comparable), while direct policy gradient on target-only failed to solve the problem. Exact numeric rewards per iteration are shown in plots but not enumerated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Implicit: the reset/repeat self-play variant was chosen because dynamics are asymmetric (easier to coast down than climb), illustrating that environment dynamics (a form of complexity) dictate which self-play variant is effective. No explicit high/low variation sweep reported for MountainCar.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Mixing self-play episodes (1% of training) with target-task episodes; reset variant self-play; policies discretized to permit discrete policy-gradient; compared to VIME and SimHash baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Demonstrated improved sample efficiency qualitatively: self-play allowed earlier reward acquisition and successful learning where target-only RL failed; quantitative episode counts and exact speed-up factors shown in plots but not tabulated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Asymmetric self-play (reset variant) accelerates learning on MountainCar, enabling learning where target-only policy gradient fails and matching or exceeding the early learning speed of exploration methods like VIME and SimHash; choice of self-play variant depends on environment dynamics asymmetry.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1096.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1096.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SwimmerGather</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SwimmerGather (RLLab / Mujoco)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A continuous high-dimensional sensorimotor task where a two-joint swimmer must collect positive-reward apples and avoid negative-reward bombs in the target task; self-play lacks apples/bombs, testing transfer when self-play and target tasks differ.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Bob (with Alice proposing tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Neural-network actor trained with policy-gradient (Reinforce and TRPO variants tried) using discretized action bins for joints and a STOP head; self-play objectives encourage matching Alice's final state location (reverse) with success tolerance ||l_b - l_a|| < 0.3; target task uses extrinsic apple/bomb rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (continuous-control virtual agent in Mujoco)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>SwimmerGather (Mujoco)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A worm-like swimmer with two joints in a viscous 2D fluid. Observations: 13-dim state vector + 20-dim nearby-object sensors. Complexity arises from continuous high-dimensional dynamics, sensor inputs, and need to coordinate locomotion to reach/avoid objects; target rewards involve objects not present during self-play.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Observation dimensionality ~33, action dimensionality 2 (discretized to 9 bins each), sensor range for nearby objects, success tolerance for self-play (distance < 0.3), episode horizons (TT: 166, SP: 200 in hyperparams); continuous, non-linear dynamics increase complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Randomized initial conditions and changing locations where Alice hands over (visualized); self-play lacks apples/bombs whereas target includes them, so variation between self-play and target tasks is explicit.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>External cumulative task reward (apple +1, bomb -1) over training iterations; learning curves and early reward acquisition compared to VIME and SimHash.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: Self-play allows earlier attainment of positive target reward compared to VIME and SimHash (plotted in Fig.4 right); both Reinforce+self-play and TRPO+self-play show faster early learning though converge to similar final reward as SimHash. Exact numeric rewards per iteration provided in figures but not enumerated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes: the experiment tests transfer when self-play and target tasks differ (self-play no apples/bombs). Paper shows self-play still accelerates learning, indicating that self-play that teaches efficient state transitions can transfer even when extrinsic rewards differ; however, distributions of handover locations and Alice behavior evolve and can concentrate on subregions (coverage vs difficulty trade-off).</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Mixing self-play episodes (10% of training) with target-task episodes; tried both Reinforce and TRPO with self-play; actor-critic variant for TRPO with L2 regularization on critic. Visualized how Alice's handover distribution changes during training.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Self-play learned policies that transfer to the target task even though apples/bombs were absent during self-play; this produced earlier positive reward acquisition on the target task, demonstrating transfer across differing reward structures, though final performance converged to be similar to strong baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Improved sample efficiency: earlier gains in target-task reward vs baselines, with batch sizes and iteration definitions provided (1 iteration = 50k steps in TRPO experiment; self-play proportion 75% in that TRPO run). Exact episode counts for reaching targets reported in plotted curves but not tabulated.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Self-play that trains efficient transition policies can accelerate learning on target tasks even when target extrinsic rewards and objects are absent during self-play. 2) Alice's behavior evolves and may concentrate on particular regions, indicating potential coverage limitations. 3) Self-play combined with TRPO or Reinforce yields earlier reward than VIME/SimHash, while converging to comparable final performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1096.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1096.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StarCraft Marines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>StarCraft: Marines Training Sub-task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A macro-management sub-task from StarCraft where the agent controls workers and a command center to mine minerals, build barracks/supply depots and train Marines; used to test self-play in a long-horizon multi-unit, multi-step planning domain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Bob (with Alice proposing tasks over global game state)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A multi-unit policy where per-unit local observations and a shared global observation vector are input to a single neural policy; actions include movement, mining, building, and training; self-play (repeat variant) sets a global target vector (unit counts and minerals) and Bob must match or exceed Alice's global final counts to succeed. Policies trained with Reinforce and compared to count-based exploration baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (multi-unit virtual agent in StarCraft simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>StarCraft sub-task (build Marines)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Real-time strategy micro/macro environment starting with 4 worker units and a command center; complexity arises from multi-agent control, combinatorial action choices across units, long horizon (200 steps), resource management, and sequencing of operations (mine → build barracks → train marines, optionally build supply depot and extra workers).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Global observation vector: ⌊N_ore/25⌋, N_SCV, N_Barrack, N_SupplyDepot, N_Marines; per-unit local 64x64 view; action branching across units (7 action IDs in Table 2); episode horizon t_max = 200 (also experiments to 300); build/training actions have resource/time costs. Complexity measured by multi-unit branching, long planning horizon and resource constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Moderate: game engine deterministic rules but initial randomness and different trajectories yield variation; self-play global targets vary because Alice produces different final global vectors. A count-based baseline used the same global state for exploration counts.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>External reward: number of Marines built at episode end (reward +1 per Marine). Learning curves plotted as reward vs #target-task episodes; also compared convergence speed and final reward across methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: Self-play greatly speeds up learning vs target-only Reinforce and outperforms a count-based baseline in both speed and final converged marine count (plots in Fig.5 right). A longer episode (t_max=300) still shows self-play outperforming baselines (Fig.9). Exact numeric marine counts per episode are plotted but not enumerated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Implicit: The environment's high complexity (multi-unit coordination and long sequences) makes matching global states difficult; self-play leverages global counts as success criteria to provide achievable internal goals. The paper notes that matching exact game-state is impossible, so Bob is evaluated on aggregate global features which constrains the variation dimension and shapes the curriculum. They also discuss how self-play coverage limitations (Alice focusing on some challenges) can limit breadth of learned behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Mixing self-play episodes (10% of training) with target-task episodes; repeat self-play variant; Reinforce with per-unit shared policy; count-based exploration baseline implemented over same global state representation for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Improved sample efficiency demonstrated qualitatively: fewer target-task episodes required to reach higher marine counts compared to target-only and count-based baselines; results shown in learning curves (target-task episodes on x-axis) but no single numeric sample-efficiency factor reported in text.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Self-play scales to a complex, long-horizon, multi-agent RTS sub-task, producing faster learning and higher final marine counts than a count-based exploration baseline and target-only Reinforce. 2) Aggregate global-state success criteria enable self-play when exact state matching is infeasible. 3) Extending episode length (200→300) preserves the benefits of self-play.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Active learning of inverse models with intrinsically motivated goal exploration in robots <em>(Rating: 2)</em></li>
                <li>Reverse curriculum generation for reinforcement learning <em>(Rating: 2)</em></li>
                <li>Hindsight experience replay <em>(Rating: 2)</em></li>
                <li>Curiosity-driven exploration in deep reinforcement learning via bayesian neural networks <em>(Rating: 1)</em></li>
                <li>A study of count-based exploration for deep reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1096",
    "paper_id": "paper-8499a250422a3c66357367c8d5fa504de5424c59",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "Asymmetric Self-Play",
            "name_full": "Asymmetric Self-Play (Alice & Bob)",
            "brief_description": "An intrinsically-motivated two-mind RL scheme where one policy (Alice) proposes tasks by acting in the environment and a second policy (Bob) must repeat or reverse those tasks; internal rewards produce an automatic curriculum to improve Bob's ability to transition between states.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Alice & Bob (asymmetric self-play agents)",
            "agent_description": "A single embodied agent with two distinct policy modules: Alice (task proposer) and Bob (task solver). Both are trained with policy-gradient RL (REINFORCE / variants, with value-baseline) using internal rewards R_A = γ max(0, t_B - t_A) and R_B = -γ t_B; policies parameterized tabularly or by neural networks depending on experiment.",
            "agent_type": "simulated agent (virtual agent with embodied control in simulated environments)",
            "environment_name": "Multiple (Long Hallway, MazeBase, RLLab/MountainCar, RLLab/SwimmerGather, StarCraft sub-task)",
            "environment_description": "Collection of simulated embodied environments ranging from a simple 1-D chain (Long Hallway), discrete procedurally-sampled 2D grid tasks with objects and partial observability (MazeBase Light-Key), continuous control locomotion tasks (MountainCar, SwimmerGather) to a complex RTS macro-management sub-task (StarCraft training Marines). Complexity sources include state-space size, object combinatorics, continuous dynamics, partial observability, multi-unit coordination, and long multi-step action sequences.",
            "complexity_measure": "Varies by environment: Long Hallway: number of states M (e.g. M=25), discrete actions (3); MazeBase: number of grid cells, number of objects interacted (1-3 shown), partial observability parameter p(Light off); MountainCar: continuous state dim=2 (position, velocity), episode horizon up to 500 steps; SwimmerGather: observation dim ~33 (13 state + 20 sensors), continuous action dims (2 joints discretized to 9 bins each); StarCraft: global state vector (ore/25, N_SCV, N_Barrack, N_SupplyDepot, N_Marines), multi-agent action branching. Complexity is therefore measured by state-space size, number of objects, observation dimensionality, action branching and episode horizon.",
            "complexity_level": "Environment-dependent: Long Hallway: low; MazeBase: medium (combinatorial / partially observable); MountainCar: low-to-medium (continuous but low-dim); SwimmerGather: medium-high (continuous, higher-dim sensor inputs); StarCraft Marines: high (multi-unit coordination, long-horizon planning).",
            "variation_measure": "Procedural randomness and design choices: Long Hallway: none (fixed chain); MazeBase: per-episode random layouts and p(Light off) controls initial observability distribution; MountainCar / SwimmerGather: randomized initial states; StarCraft: randomized initial placements / deterministic game engine but potentially randomized initial seeds. The paper explicitly manipulates p(Light off) in MazeBase to vary similarity between self-play and target tasks.",
            "variation_level": "Environment-dependent: Long Hallway: low; MazeBase: medium-high (randomized episodes and tunable p(Light off) between 0.1–0.9); MountainCar & SwimmerGather: low-to-medium (randomized initial states only); StarCraft: medium (stochasticity from gameplay and randomized seeds but fixed task spec).",
            "performance_metric": "Task-specific: success rate on target task (fraction of episodes where target achieved), cumulative task reward (external reward), time-to-completion (t_B), and learning speed measured as number of target-task episodes to reach performance thresholds (speed-up factors).",
            "performance_value": "Qualitative across domains: self-play speeds up learning (reduces required target-task episodes) relative to target-only baselines and in many cases matches or outperforms explicit exploration methods (count-based, VIME, SimHash). No single absolute numeric score is universally reported; examples: Long Hallway M=25 shows much faster early success (Fig.2); MazeBase shows substantially faster attainment of goal for p(Light off)=0.5; MountainCar and SwimmerGather: faster early reward acquisition and comparable final reward to VIME/SimHash (plots in Fig.4); StarCraft: higher final marine count reward and faster learning vs baselines (Fig.5).",
            "complexity_variation_relationship": "Yes — the paper explicitly discusses and experiments with the interaction between environment variation (how self-play is sampled) and similarity to the target task. In MazeBase they vary p(Light off) to change similarity between self-play and target tasks and show that self-play yields significant speedups when self-play tasks are similar to the target tasks; the benefit diminishes or reverses when self-play is biased against the target task. They also discuss trade-offs where Alice may focus on a subset of challenges (low coverage) if allowed, producing faster performance on those but poorer general coverage—highlighting a trade-off between pushing difficulty and breadth of variation.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Automatic curriculum via asymmetric self-play combined with standard RL (policy-gradient). Training mixes self-play episodes (internal reward only) and target-task episodes (external reward); self-play percentage varies by experiment (e.g. MazeBase 20%, MountainCar 1%, SwimmerGather 10%, StarCraft 10%).",
            "generalization_tested": true,
            "generalization_results": "Partial/generalization tested as transfer from unsupervised self-play to separate target tasks: Bob trained with self-play required fewer target-task episodes to learn target tasks. MazeBase experiments systematically varied similarity (p(Light off)) and demonstrated that when self-play resembles the target task, transfer speedups are large; when self-play is biased against the target, transfer benefits diminish. SwimmerGather demonstrates transfer when self-play lacks apples/bombs (i.e., different reward structure) yet still accelerates target learning.",
            "sample_efficiency": "Measured as reduction in number of target-task episodes to reach performance thresholds (speed-up). Example hyperparameters: self-play % and t_max reported (see Table 1); some experiments stopped at 5×10^6 episodes for baselines. Exact numeric reductions are shown in plots (e.g. MazeBase speedups vs p(Light off) in Fig.6) but not summarized as single universal numbers in text.",
            "key_findings": "1) Asymmetric self-play produces an automatic curriculum: Alice proposes tasks just beyond Bob's current capability, improving exploration without external reward. 2) Self-play accelerates learning (reduces required target-task episodes) across discrete and continuous embodied environments and is competitive with state-of-the-art intrinsic-exploration methods (VIME, SimHash). 3) Benefits depend on similarity between self-play distribution and the target task (explicitly shown by p(Light off) sweep): self-play helps when not biased against the target task. 4) In tabular, deterministic reversible settings, equilibrium implies Bob becomes a fast universal transition policy, but with function approximation and local updates Alice may cover only a subset of challenges (coverage vs difficulty trade-off).",
            "uuid": "e1096.0",
            "source_info": {
                "paper_title": "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "Long Hallway",
            "name_full": "Long Hallway (1-D chain toy environment)",
            "brief_description": "A simple chain-of-states testbed (M states in a line) used to illustrate curriculum formation via asymmetric self-play and to compare against count-based exploration and random-task baselines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Bob (with Alice proposing tasks)",
            "agent_description": "Tabular policies for Alice and Bob indexed by (start, end) state pairs; trained with policy gradient and constant baseline. Internal self-play reward structure as in main method; target task identical to self-play (move between states).",
            "agent_type": "simulated agent (virtual)",
            "environment_name": "Long Hallway chain (M=25 in experiments)",
            "environment_description": "Discrete 1-D chain of M states {s1...sM}; actions: left, right, stop. Complexity arises from length M and required exploration to reach distant states; reversible dynamics are trivial.",
            "complexity_measure": "State space size M (experiment used M=25), action set size = 3, episode max steps t_max = 30.",
            "complexity_level": "low (M=25, 3 discrete actions)",
            "variation_measure": "None/low: environment structure fixed across episodes; target start/goal pairs randomized for target task but environment dynamics constant.",
            "variation_level": "low",
            "performance_metric": "Fraction of successful target-task episodes (success rate) over training episodes.",
            "performance_value": "Qualitative: Asymmetric self-play (blue curve, Fig.2 left) achieves faster early success and better exploration than standard policy gradient and random-Alice baselines, comparable to explicit count-based exploration; no absolute numeric success rates reported in text.",
            "complexity_variation_relationship": "No explicit variation dimension to manipulate here; paper uses the simple low-variation environment to show that self-play constructs a progressive curriculum (Alice gives short tasks initially, then longer ones) enabling exploration across increasing complexity (distance).",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Tabular policy-gradient with self-play episodes (self-play only in toy; no mixing with target training in this toy) and internal reward-based curriculum.",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": "Demonstrated qualitatively via learning curves: self-play required fewer training episodes to reach non-zero success rates compared to baselines; exact numeric episode counts not provided in text (plots shown).",
            "key_findings": "In a simple reversible environment, asymmetric self-play yields a natural curriculum that encourages exploration to distant states, giving faster early learning than vanilla policy gradient and random-task baselines, and comparable performance to count-based exploration.",
            "uuid": "e1096.1",
            "source_info": {
                "paper_title": "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "MazeBase Light-Key",
            "name_full": "MazeBase Light-Key Maze (partially observable grid with objects)",
            "brief_description": "A discrete 2D grid environment with objects (light switch, key, door, goal flag) and partial observability controlled by a light switch; used to evaluate transfer from self-play to a goal-reaching target task and to study the effect of task similarity via p(Light off).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Bob (trained with Alice proposing tasks in MazeBase)",
            "agent_description": "Neural-network policy (two-layer MLP encoder over object-location bag-of-words, then policy head) trained with policy gradient and state-dependent baseline; self-play uses reverse or repeat variants; target task is flag-reaching requiring sequences: turn on light, toggle key to open door, pass through.",
            "agent_type": "simulated agent (virtual gridworld agent)",
            "environment_name": "MazeBase Light-Key",
            "environment_description": "Procedurally-generated 2D grid mazes with a light switch (initial state randomly set with probability p(Light off)), a key and door, and (in target task) a goal flag; partial observability when light is off (agent only sees light switch). Complexity arises from combinatorial object interactions, partial observability, and planning required to manipulate objects in correct order.",
            "complexity_measure": "Grid combinatorics (number and placement of objects), number of objects interacted (1-3 during Alice episodes observed), partial observability parameter p(Light off) which affects observation set, network input size (object-location bag-of-words) and episode horizon t_max = 80.",
            "complexity_level": "medium (combinatorial object interactions, partial observability)",
            "variation_measure": "High: per-episode randomized maze layouts and random initial states; explicit tunable variation via p(Light off) ∈ [0.1,0.9] which changes similarity between self-play and target tasks.",
            "variation_level": "medium-high (procedural randomness and tunable p(Light off))",
            "performance_metric": "Success rate on the target flag-reaching task; speed-up measured as reduction in number of target-task episodes to reach a performance threshold (relative to target-only training).",
            "performance_value": "Qualitative: With repeat-form self-play at p(Light off)=0.5, self-play agent achieves success much faster than target-only training and random-Alice baseline (Fig.2 right). A sweep over p(Light off) shows significant speedups when self-play is similar to the target task; exact numeric speed-up values are plotted (Fig.6) but not enumerated in the text.",
            "complexity_variation_relationship": "Yes. The paper explicitly manipulates p(Light off) to vary similarity between self-play and target tasks and reports that self-play yields significant speed-ups when the self-play distribution is not biased against the target task; reverse vs repeat self-play effectiveness depends on p(Light off) (Fig.6). This demonstrates a direct relationship: environment variation/sampling distribution controls transfer effectiveness.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Mixing self-play episodes (20% of training) with target-task episodes; self-play used both reverse and repeat variants; policies parameterized by neural networks (100 and 50 hidden units), trained with policy gradient and state-dependent baseline; entropy regularization used.",
            "generalization_tested": true,
            "generalization_results": "Transfer from unsupervised self-play to the supervised target goal task was demonstrated; varying p(Light off) showed that transfer is strongest when self-play resembles the target task (e.g. reverse self-play helps more when p(Light off) low), and can be harmful when self-play is biased against target task, indicating conditional generalization.",
            "sample_efficiency": "Reported as speed-up (reduction in required target-task episodes) relative to target-only training; Figure 6 shows speed-ups across p(Light off) values for runs that succeed (quantitative curves in figures but not tabulated in text).",
            "key_findings": "1) Self-play substantially accelerates learning of an object-manipulation target task in a partially observable, procedurally-generated environment. 2) The match between self-play distribution and target-task distribution (controlled by p(Light off)) critically affects transfer: similarity yields large speedups; bias against the target task reduces benefit. 3) Alice and Bob automatically build a curriculum (Alice increases object interactions over training) observable in episode statistics.",
            "uuid": "e1096.2",
            "source_info": {
                "paper_title": "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "MountainCar (RLLab)",
            "name_full": "MountainCar (discretized continuous control in RLLab)",
            "brief_description": "A continuous 1-D valley car task where the agent must alternate thrusts to build momentum and escape; used to compare self-play to exploration baselines VIME and SimHash.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Bob (with Alice proposing tasks)",
            "agent_description": "Neural-network policy with discrete action bins (5 bins for thrust) and an added STOP binary head; trained with policy-gradient (REINFORCE) using reversible/reset self-play (reset variant used due to asymmetry of dynamics), and target external reward +1 on successful hill climb.",
            "agent_type": "simulated agent (continuous-control virtual agent)",
            "environment_name": "MountainCar (RLLab)",
            "environment_description": "Continuous 1-D dynamics (position and velocity observations). Complexity arises from continuous dynamics requiring multi-step momentum-building behaviors and asymmetric cost to move uphill vs downhill; episode horizon t_max = 500.",
            "complexity_measure": "State dimensionality = 2 (position, velocity); discretized action bins = 5; episode horizon up to 500 steps; success defined by escaping the valley.",
            "complexity_level": "low-to-medium (low-dimensional continuous dynamics but requires temporally extended planning)",
            "variation_measure": "Randomized initial states; limited procedural variation beyond initial state randomness.",
            "variation_level": "low-to-medium",
            "performance_metric": "External target-task reward (success in climbing hill), learning curves over training iterations, convergence speed.",
            "performance_value": "Qualitative: Self-play (blue curve in Fig.4 left) learns faster than baselines (VIME, SimHash comparable), while direct policy gradient on target-only failed to solve the problem. Exact numeric rewards per iteration are shown in plots but not enumerated in text.",
            "complexity_variation_relationship": "Implicit: the reset/repeat self-play variant was chosen because dynamics are asymmetric (easier to coast down than climb), illustrating that environment dynamics (a form of complexity) dictate which self-play variant is effective. No explicit high/low variation sweep reported for MountainCar.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Mixing self-play episodes (1% of training) with target-task episodes; reset variant self-play; policies discretized to permit discrete policy-gradient; compared to VIME and SimHash baselines.",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": "Demonstrated improved sample efficiency qualitatively: self-play allowed earlier reward acquisition and successful learning where target-only RL failed; quantitative episode counts and exact speed-up factors shown in plots but not tabulated in text.",
            "key_findings": "Asymmetric self-play (reset variant) accelerates learning on MountainCar, enabling learning where target-only policy gradient fails and matching or exceeding the early learning speed of exploration methods like VIME and SimHash; choice of self-play variant depends on environment dynamics asymmetry.",
            "uuid": "e1096.3",
            "source_info": {
                "paper_title": "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "SwimmerGather",
            "name_full": "SwimmerGather (RLLab / Mujoco)",
            "brief_description": "A continuous high-dimensional sensorimotor task where a two-joint swimmer must collect positive-reward apples and avoid negative-reward bombs in the target task; self-play lacks apples/bombs, testing transfer when self-play and target tasks differ.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Bob (with Alice proposing tasks)",
            "agent_description": "Neural-network actor trained with policy-gradient (Reinforce and TRPO variants tried) using discretized action bins for joints and a STOP head; self-play objectives encourage matching Alice's final state location (reverse) with success tolerance ||l_b - l_a|| &lt; 0.3; target task uses extrinsic apple/bomb rewards.",
            "agent_type": "simulated agent (continuous-control virtual agent in Mujoco)",
            "environment_name": "SwimmerGather (Mujoco)",
            "environment_description": "A worm-like swimmer with two joints in a viscous 2D fluid. Observations: 13-dim state vector + 20-dim nearby-object sensors. Complexity arises from continuous high-dimensional dynamics, sensor inputs, and need to coordinate locomotion to reach/avoid objects; target rewards involve objects not present during self-play.",
            "complexity_measure": "Observation dimensionality ~33, action dimensionality 2 (discretized to 9 bins each), sensor range for nearby objects, success tolerance for self-play (distance &lt; 0.3), episode horizons (TT: 166, SP: 200 in hyperparams); continuous, non-linear dynamics increase complexity.",
            "complexity_level": "medium-high",
            "variation_measure": "Randomized initial conditions and changing locations where Alice hands over (visualized); self-play lacks apples/bombs whereas target includes them, so variation between self-play and target tasks is explicit.",
            "variation_level": "medium",
            "performance_metric": "External cumulative task reward (apple +1, bomb -1) over training iterations; learning curves and early reward acquisition compared to VIME and SimHash.",
            "performance_value": "Qualitative: Self-play allows earlier attainment of positive target reward compared to VIME and SimHash (plotted in Fig.4 right); both Reinforce+self-play and TRPO+self-play show faster early learning though converge to similar final reward as SimHash. Exact numeric rewards per iteration provided in figures but not enumerated in text.",
            "complexity_variation_relationship": "Yes: the experiment tests transfer when self-play and target tasks differ (self-play no apples/bombs). Paper shows self-play still accelerates learning, indicating that self-play that teaches efficient state transitions can transfer even when extrinsic rewards differ; however, distributions of handover locations and Alice behavior evolve and can concentrate on subregions (coverage vs difficulty trade-off).",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Mixing self-play episodes (10% of training) with target-task episodes; tried both Reinforce and TRPO with self-play; actor-critic variant for TRPO with L2 regularization on critic. Visualized how Alice's handover distribution changes during training.",
            "generalization_tested": true,
            "generalization_results": "Self-play learned policies that transfer to the target task even though apples/bombs were absent during self-play; this produced earlier positive reward acquisition on the target task, demonstrating transfer across differing reward structures, though final performance converged to be similar to strong baselines.",
            "sample_efficiency": "Improved sample efficiency: earlier gains in target-task reward vs baselines, with batch sizes and iteration definitions provided (1 iteration = 50k steps in TRPO experiment; self-play proportion 75% in that TRPO run). Exact episode counts for reaching targets reported in plotted curves but not tabulated.",
            "key_findings": "1) Self-play that trains efficient transition policies can accelerate learning on target tasks even when target extrinsic rewards and objects are absent during self-play. 2) Alice's behavior evolves and may concentrate on particular regions, indicating potential coverage limitations. 3) Self-play combined with TRPO or Reinforce yields earlier reward than VIME/SimHash, while converging to comparable final performance.",
            "uuid": "e1096.4",
            "source_info": {
                "paper_title": "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "StarCraft Marines",
            "name_full": "StarCraft: Marines Training Sub-task",
            "brief_description": "A macro-management sub-task from StarCraft where the agent controls workers and a command center to mine minerals, build barracks/supply depots and train Marines; used to test self-play in a long-horizon multi-unit, multi-step planning domain.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Bob (with Alice proposing tasks over global game state)",
            "agent_description": "A multi-unit policy where per-unit local observations and a shared global observation vector are input to a single neural policy; actions include movement, mining, building, and training; self-play (repeat variant) sets a global target vector (unit counts and minerals) and Bob must match or exceed Alice's global final counts to succeed. Policies trained with Reinforce and compared to count-based exploration baseline.",
            "agent_type": "simulated agent (multi-unit virtual agent in StarCraft simulator)",
            "environment_name": "StarCraft sub-task (build Marines)",
            "environment_description": "Real-time strategy micro/macro environment starting with 4 worker units and a command center; complexity arises from multi-agent control, combinatorial action choices across units, long horizon (200 steps), resource management, and sequencing of operations (mine → build barracks → train marines, optionally build supply depot and extra workers).",
            "complexity_measure": "Global observation vector: ⌊N_ore/25⌋, N_SCV, N_Barrack, N_SupplyDepot, N_Marines; per-unit local 64x64 view; action branching across units (7 action IDs in Table 2); episode horizon t_max = 200 (also experiments to 300); build/training actions have resource/time costs. Complexity measured by multi-unit branching, long planning horizon and resource constraints.",
            "complexity_level": "high",
            "variation_measure": "Moderate: game engine deterministic rules but initial randomness and different trajectories yield variation; self-play global targets vary because Alice produces different final global vectors. A count-based baseline used the same global state for exploration counts.",
            "variation_level": "medium",
            "performance_metric": "External reward: number of Marines built at episode end (reward +1 per Marine). Learning curves plotted as reward vs #target-task episodes; also compared convergence speed and final reward across methods.",
            "performance_value": "Qualitative: Self-play greatly speeds up learning vs target-only Reinforce and outperforms a count-based baseline in both speed and final converged marine count (plots in Fig.5 right). A longer episode (t_max=300) still shows self-play outperforming baselines (Fig.9). Exact numeric marine counts per episode are plotted but not enumerated in text.",
            "complexity_variation_relationship": "Implicit: The environment's high complexity (multi-unit coordination and long sequences) makes matching global states difficult; self-play leverages global counts as success criteria to provide achievable internal goals. The paper notes that matching exact game-state is impossible, so Bob is evaluated on aggregate global features which constrains the variation dimension and shapes the curriculum. They also discuss how self-play coverage limitations (Alice focusing on some challenges) can limit breadth of learned behaviors.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Mixing self-play episodes (10% of training) with target-task episodes; repeat self-play variant; Reinforce with per-unit shared policy; count-based exploration baseline implemented over same global state representation for comparison.",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": "Improved sample efficiency demonstrated qualitatively: fewer target-task episodes required to reach higher marine counts compared to target-only and count-based baselines; results shown in learning curves (target-task episodes on x-axis) but no single numeric sample-efficiency factor reported in text.",
            "key_findings": "1) Self-play scales to a complex, long-horizon, multi-agent RTS sub-task, producing faster learning and higher final marine counts than a count-based exploration baseline and target-only Reinforce. 2) Aggregate global-state success criteria enable self-play when exact state matching is infeasible. 3) Extending episode length (200→300) preserves the benefits of self-play.",
            "uuid": "e1096.5",
            "source_info": {
                "paper_title": "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play",
                "publication_date_yy_mm": "2017-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Active learning of inverse models with intrinsically motivated goal exploration in robots",
            "rating": 2
        },
        {
            "paper_title": "Reverse curriculum generation for reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Hindsight experience replay",
            "rating": 2
        },
        {
            "paper_title": "Curiosity-driven exploration in deep reinforcement learning via bayesian neural networks",
            "rating": 1
        },
        {
            "paper_title": "A study of count-based exploration for deep reinforcement learning",
            "rating": 1
        }
    ],
    "cost": 0.01817175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>INTRINSIC MOTIVATION AND AUTOMATIC Curricula via Asymmetric Self-Play</h1>
<p>Sainbayar Sukhbaatar<br>Dept. of Computer Science<br>New York University<br>sainbar@cs.nyu.edu</p>
<p>Zeming Lin<br>Facebook AI Research<br>New York<br>zlin@fb.com</p>
<p>Ilya Kostrikov<br>Dept. of Computer Science<br>New York University<br>kostrikov@cs.nyu.edu</p>
<p>Gabriel Synnaeve, Arthur Szlam \&amp; Rob Fergus
Facebook AI Research
New York
${$ gab,aszlam, robfergus}@fb.com</p>
<h4>Abstract</h4>
<p>We describe a simple scheme that allows an agent to learn about its environment in an unsupervised manner. Our scheme pits two versions of the same agent, Alice and Bob, against one another. Alice proposes a task for Bob to complete; and then Bob attempts to complete the task. In this work we will focus on two kinds of environments: (nearly) reversible environments and environments that can be reset. Alice will "propose" the task by doing a sequence of actions and then Bob must undo or repeat them, respectively. Via an appropriate reward structure, Alice and Bob automatically generate a curriculum of exploration, enabling unsupervised training of the agent. When Bob is deployed on an RL task within the environment, this unsupervised training reduces the number of supervised episodes needed to learn, and in some cases converges to a higher reward.</p>
<h2>1 INTRODUCTION</h2>
<p>Model-free approaches to reinforcement learning are sample inefficient, typically requiring a huge number of episodes to learn a satisfactory policy. The lack of an explicit environment model means the agent must learn the rules of the environment from scratch at the same time as it tries to understand which trajectories lead to rewards. In environments where reward is sparse, only a small fraction of the agents' experience is directly used to update the policy, contributing to the inefficiency.</p>
<p>In this paper we introduce a novel form of unsupervised training for an agent that enables exploration and learning about the environment without any external reward that incentivizes the agents to learn how to transition between states as efficiently as possible. We demonstrate that this unsupervised training allows the agent to learn new tasks within the environment quickly.</p>
<h2>2 APPROACH</h2>
<p>We consider environments with a single physical agent (or multiple physical units controlled by a single agent), but we allow it to have two separate "minds": Alice and Bob, each with its own objective and parameters. During self-play episodes, Alice's job is to propose a task for Bob to complete, and Bob's job is to complete the task. When presented with a target task episode, Bob is then used to perform it (Alice plays no role). The key idea is that the Bob's play with Alice should help him understand how the environment works and enabling him to learn the target task more quickly.</p>
<p>Our approach is restricted to two classes of environment: (i) those that are (nearly) reversible, or (ii) ones that can be reset to their initial state (at least once). These restrictions allow us to sidestep complications around how to communicate the task and determine its difficulty (see Appendix F. 2</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of the self-play concept in a gridworld setting. Training consists of two types of episode: self-play and target task. In the former, Alice and Bob take turns moving the agent within the environment. Alice sets tasks by altering the state via interaction with its objects (key, door, light) and then hands control over to Bob. He must return the environment to its original state to receive an internal reward. This task is just one of many devised by Alice, who automatically builds a curriculum of increasingly challenging tasks. In the target task, Bob's policy is used to control the agent, with him receiving an external reward if he visits the flag. He is able to learn to do this quickly as he is already familiar with the environment from self-play.
for further discussion). In these two scenarios, Alice starts at some initial state $s_{0}$ and proposes a task by doing it, i.e. executing a sequence of actions that takes the agent to a state $s_{t}$. She then outputs a STOP action, which hands control over to Bob. In reversible environments, Bob's goal is to return the agent back to state $s_{0}$ (or within some margin of it, if the state is continuous), to receive reward. In partially observable environments, the objective is relaxed to Bob finding a state that returns the same observation as Alice's initial state. In environments where resets are permissible, Alice's STOP action also reinitializes the environment, thus Bob starts at $s_{0}$ and now must reach $s_{t}$ to be rewarded, thus repeating Alice's task instead of reversing it. See Fig. 1 for an example, and also Algorithm 1 in Appendix A.</p>
<p>In both cases, this self-play between Alice and Bob only involves internal reward (detailed below), thus the agent can be trained without needing any supervisory signal from the environment. As such, it comprises a form of unsupervised training where Alice and Bob explore the environment and learn how it operates. This exploration can be leveraged for some target task by training Bob on target task episodes in parallel. The idea is that Bob's experience from self-play will help him learn the target task in fewer episodes. The reason behind choosing Bob for the target task is because he learns to transfer from one state to another efficiently from self-play. See Algorithm 2 in Appendix A for detail.</p>
<p>For self-play, we choose the reward structure for Alice and Bob to encourage Alice to push Bob past his comfort zone, but not give him impossible tasks. Denoting Bob's total reward by $R_{B}$ (given at the end of episodes) and Alice's total reward by $R_{A}$, we use</p>
<p>$$
R_{B}=-\gamma t_{B}
$$</p>
<p>where $t_{B}$ is the time taken by Bob to complete his task and</p>
<p>$$
R_{A}=\gamma \max \left(0, t_{B}-t_{A}\right)
$$</p>
<p>where $t_{A}$ is the time until Alice performs the STOP action, and $\gamma$ is a scaling coefficient that balances this internal reward to be of the same scale as external rewards from the target task. The total length of an episode is limited to $t_{\text {Max }}$, so if Bob fails to complete the task in time we set $t_{B}=t_{\text {Max }}-t_{A}$.</p>
<p>Thus Alice is rewarded if Bob takes more time, but the negative term on her own time will encourage Alice not to take too many steps when Bob is failing. For both reversible and resettable environments, Alice must limit her steps to make Bob's task easier, thus Alice's optimal behavior is to the find simplest tasks that Bob cannot complete. This eases learning for Bob since the new task will be only just beyond his current capabilities. The self-regulating feedback between Alice and Bob allows them to automatically construct a curriculum for exploration, a key contribution of our approach.</p>
<h1>2.1 PARAMETERIZING ALICE AND Bob's ACTIONS</h1>
<p>Alice and Bob each have policy functions which take as input two observations of state variables, and output a distribution over actions. In Alice's case, the function will be of the form</p>
<p>$$
a_{\mathrm{A}}=\pi_{A}\left(s_{t}, s_{0}\right)
$$</p>
<p>where $s_{0}$ is the observation of the initial state of the environment and $s_{t}$ is the observation of the current state. In Bob's case, the function will be</p>
<p>$$
a_{\mathrm{B}}=\pi_{B}\left(s_{t}, s^{*}\right)
$$</p>
<p>where $s^{<em>}$ is the target state that Bob has to reach, and set to $s_{0}$ when we have a reversible environment. In a resettable environment $s^{</em>}$ is the state where Alice executed the STOP action.
When a target task is presented, the agent's policy function is $a_{\text {Target }}=\pi_{B}\left(s_{t}, \emptyset\right)$, where the second argument of Bob's policy is simply set to zero ${ }^{1}$. If $s^{<em>}$ is always non-zero, then this is enough to let Bob know whether the current episode is self-play or target task. In some experiments where $s^{</em>}$ can be zero, we give third argument $z \in{0,1}$ that explicitly indicates the episode kind.
In the experiments below, we demonstrate our approach in settings where $\pi_{A}$ and $\pi_{B}$ are tabular; where it is a neural network taking discrete inputs, and where it is a neural network taking in continuous inputs. When using a neural network, we use the same network architecture for both Alice and Bob, except they have different parameters</p>
<p>$$
\pi_{A}\left(s_{t}, s_{0}\right)=f\left(s_{t}, s_{0}, \theta_{A}\right), \quad \pi_{B}\left(s_{t}, s^{<em>}\right)=f\left(s_{t}, s^{</em>}, \theta_{B}\right)
$$</p>
<p>where $f$ is an multi-layered neural network with parameters $\theta_{A}$ or $\theta_{B}$.</p>
<h3>2.2 UNIVERSAL BOB IN THE TABULAR SETTING</h3>
<p>We now present a theoretical argument that shows for environments with finite states, tabular policies, and deterministic, Markovian transitions, we can interpret the self-play as training Bob to find a policy that can get from any state to any other in the least expected number of steps.
Preliminaries: Note that, as discussed above, the policy table for Bob is indexed by $\left(s_{t}, s^{<em>}\right)$, not just by $s_{t}$. In particular, with the assumptions above, this means that there is a fast policy $\pi_{\text {fast }}$ such that $\pi_{\text {fast }}\left(s_{t}, s^{</em>}\right)$ has the smallest expected number of steps to transition from $s_{t}$ to $s^{*}$. It is clear that $\pi_{\text {fast }}$ is a universal policy for Bob, such that $\pi_{B}=\pi_{\text {fast }}$ is optimal with respect to any Alice's policy $\pi_{A}$. In a reset game, $\pi_{\text {fast }}$ nets Alice a return of 0 , and in the reverse game, the return of $\pi_{\text {fast }}$ against an optimal Alice can be considered a measure of the reversibility of the environment. However, in what follows let us assume that either the reset game or the reverse game in a perfectly reversible environment is used. Also, let assume the initial states are randomized and its distribution covers the entire state space.
Claim: If $\pi_{A}$ and $\pi_{B}$ are policies of Alice and Bob that are in equilibrium (i.e., Alice cannot be made better without changing Bob, and vice-versa), then $\pi_{B}$ is a fast policy.
Argument: Let us first show that Alice will always get zero reward in equilibrium. If Alice is getting positive reward on some challenge, that means Bob is taking longer than Alice on that challenge. Then Bob can be improved to use $\pi_{f a s t}$ at that challenge, which contradicts the equilibrium assumption.</p>
<p>Now let us prove $\pi_{B}$ is a fast policy by contradiction. If $\pi_{B}$ is not fast, then there must exist a challenge $\left(s_{t}, s^{*}\right)$ where $\pi_{B}$ will take longer than $\pi_{f a s t}$. Therefore Bob can get more reward by using $\pi_{f a s t}$ if Alice does propose that challenge with non-zero probability. Since we assumed equilibrium and $\pi_{B}$ cannot be improved while $\pi_{A}$ fixed, the only possibility is that Alice is never proposing that challenge. If that is true, Alice can get positive reward by proposing that task using the same actions as $\pi_{f a s t}$, so taking fewer steps than $\pi_{B}$. However this contradicts with the proof that Alice always gets zero reward, making our initial assumption " $\pi_{B}$ is not fast" wrong.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Self-play arises naturally in reinforcement learning, and has been well studied. For example, for playing checkers (Samuel, 1959), backgammon (Tesauro, 1995), and Go, (Silver et al., 2016), and in multi-agent games such as RoboSoccer (Riedmiller et al., 2009). Here, the agents or teams of agents compete for external reward. This differs from our scheme where the reward is purely internal and the self-play is a way of motivating an agent to learn about its environment to augment sparse rewards from separate target tasks.</p>
<p>Our approach has some relationships with generative adversarial networks (GANs) (Goodfellow et al., 2014), which train a generative neural net by having it try to fool a discriminator network which tries to differentiate samples from the training examples. Li et al. (2017) introduce an adversarial approach to dialogue generation, where a generator model is subjected to a form of "Turing test" by a discriminator network. Mescheder et al. (2017) demonstrate how adversarial loss terms can be combined with variational auto-encoders to permit more accurate density modeling. While GAN's are often thought of as methods for training a generator, the generator can be thought of as a method for generating hard negatives for the discriminator. From this viewpoint, in our approach, Alice acts as a "generator", finding "negatives" for Bob. However, Bob's jobs is to complete the generated challenge, not to discriminate it.</p>
<p>There is a large body of work on intrinsic motivation (Barto, 2013; Singh et al., 2004; Klyubin et al., 2005; Schmidhuber, 1991) for self-supervised learning agents. These works propose methods for training an agent to explore and become proficient at manipulating its environment without necessarily having a specific target task, and without a source of extrinsic supervision. One line in this direction is curiosity-driven exploration (Schmidhuber, 1991). These techniques can be applied in encouraging exploration in the context of reinforcement learning, for example (Bellemare et al., 2016; Strehl \&amp; Littman, 2008; Lopes et al., 2012; Tang et al., 2017; Pathak et al., 2017); Roughly, these use some notion of the novelty of a state to give a reward. In the simplest setting, novelty can be just the number of times a state has been visited; in more complex scenarios, the agent can build a model of the world, and the novelty is the difficulty in placing the current state into the model. In our work, there is no explicit notion of novelty. Even if Bob has seen a state many times, if he has trouble getting to it, Alice should force him towards that state. Another line of work on intrinsic motivation is a formalization of the notion of empowerment (Klyubin et al., 2005), or how much control the agent has over its environment. Our work is related in the sense that it is in both Alice's and Bob's interests to have more control over the environment; but we do not explicitly measure that control except in relation to the tasks that Alice sets.</p>
<p>Curriculum learning (Bengio et al., 2009) is widely used in many machine learning approaches. Typically however, the curriculum requires at least some manual specification. A key point about our work is that Alice and Bob devise their own curriculum entirely automatically. Previous automatic approaches, such as Kumar et al. (2010), rely on monitoring training error. But since ours is unsupervised, no training labels are required either.</p>
<p>Our basic paradigm of "Alice proposing a task, and Bob doing it" is related to the Horde architecture (Sutton et al., 2011) and (Schaul et al., 2015). In those works, instead of using a value function $V=V(s)$ that depends on the current state, a value function that explicitly depends on state and goal $V=V(s, g)$ is used. In our experiments, our models will be parameterized in a similar fashion. The novelty in this work is in how Alice defines the goal for Bob.</p>
<p>The closest work to ours is that of Baranes \&amp; Oudeyer (2013), who also have one part of the model that proposes tasks, while another part learns to complete them. As in this work, the policies and cost are parameterized as functions of both state and goal. However, our approach differs in the way tasks are proposed and communicated. In particular, in Baranes \&amp; Oudeyer (2013), the goal space has to be presented in a way that allows explicit partitioning and sampling, whereas in our work, the goals are sampled through Alice's actions. On the other hand, we pay for not having to have such a representation by requiring the environment to be either reversible or resettable.</p>
<p>Several concurrent works are related: Andrychowicz et al. (2017) form an implicit curriculum by using internal states as a target. Florensa et al. (2017) automatically generate a series of increasingly distant start states from a goal. Pinto et al. (2017) use an adversarial framework to perturb the</p>
<p>environment, inducing improved robustness of the agent. Held et al. (2017) propose a scheme related to our "random Alice" strategy ${ }^{2}$.</p>
<h1>4 EXPERIMENTS</h1>
<p>The following experiments explore our self-play approach on a variety of tasks, both continuous and discrete, from the Mazebase (Sukhbaatar et al., 2015), RLLab (Duan et al., 2016), and StarCraft ${ }^{3}$ environments (Synnaeve et al., 2016). The same protocol is used in all settings: self-play and target task episodes are mixed together and used to train the agent via discrete policy gradient. We evaluate both the reverse and repeat versions of self-play. We demonstrate that the self-play episodes help training, in terms of number of target task episodes needed to learn the task. Note that we assume the self-play episodes to be "free", since they make no use of environmental reward. This is consistent with traditional semi-supervised learning, where evaluations typically are based only on the number of labeled points (not unlabeled ones too).</p>
<p>In all the experiments we use policy gradient (Williams, 1992) with a baseline for optimizing the policies. In the tabular task below, we use a constant baseline; in all the other tasks we use a policy parameterized by a neural network, and a baseline that depends on the state. We denote the states in an episode by $s_{1}, \ldots, s_{T}$, and the actions taken at each of those states as $a_{1}, \ldots, a_{T}$, where $T$ is the length of the episode. The baseline is a scalar function of the states $b(s, \theta)$, computed via an extra head on the network producing the action probabilities. Besides maximizing the expected reward with policy gradient, the models are also trained to minimize the distance between the baseline value and actual reward. Thus after finishing an episode, we update the model parameters $\theta$ by</p>
<p>$$
\Delta \theta=\sum_{t=1}^{T}\left[\frac{\partial \log f\left(a_{t} \mid s_{t}, \theta\right)}{\partial \theta}\left(\sum_{i=t}^{T} r_{i}-b\left(s_{t}, \theta\right)\right)-\lambda \frac{\partial}{\partial \theta}\left(\sum_{i=t}^{T} r_{i}-b\left(s_{t}, \theta\right)\right)^{2}\right]
$$</p>
<p>Here $r_{t}$ is reward given at time $t$, and the hyperparameter $\lambda$ is for balancing the reward and the baseline objectives, which is set to 0.1 in all experiments.</p>
<p>For the policy neural networks, we use two-layer fully-connected networks with 50 hidden units in each layer. The training uses RMSProp (Tieleman \&amp; Hinton, 2012). We always do 10 runs with different random initializations and report their mean and standard deviation. See Appendix B for all the hyperparameter values used in the experiments.</p>
<h3>4.1 LONG HALLWAY</h3>
<p>We first describe a simple toy environment designed to illustrate the function of the asymmetric selfplay. The environment consists of $M$ states $\left{s_{1}, \ldots, s_{M}\right}$ arranged in a chain. Both Alice and Bob have three possible actions, "left", "right", or "stop". If the agent is at $s_{i}$ with $i \neq 1$, "left" takes it to $s_{i-1}$; "right" analogously increases the state index, and "stop" transfers control to Bob when Alice runs it and terminates the episode when Bob runs it. We use "return to initial state" as the self-play task (i.e. Reverse in Algorithm 1 in Appendix A ). For the target task, we randomly pick a starting state and target state, and the episode is considered successful if Bob moves to the target state and executes the stop action before a fixed number of maximum steps.</p>
<p>In this case, the target task is essentially the same as the self-play task, and so running it is not unsupervised learning (and in particular, on this toy example unlike the other examples below, we do not mix self-play training with target task training). However, we see that the curriculum afforded by the self-play is efficient at training the agent to do the target task at the beginning of the training, and is effective at forcing exploration of the state space as Bob gets more competent.</p>
<p>In Fig. 2 (left) we plot the number of episodes vs rate of success at the target task with four different methods. We set $M=25$ and the maximum allowed steps for Alice and Bob to be 30 . We use fully tabular controllers; the table is of size $M^{2} \times 3$, with a distribution over the three actions for each possible (start, end pair).</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>The red curve corresponds to policy gradient, with a penalty of -1 given upon failure to complete the task, and a penalty of $-t / t_{\text {Max }}$ for successfully completing the task in $t$ steps. The magenta curve corresponds to taking Alice to have a random policy ( $1 / 2$ probability of moving left or right, and not stopping till the maximum allowed steps). The green curve corresponds to policy gradient with an exploration bonus similar to Strehl \&amp; Littman (2008). That is, we keep count of the number of times $N_{s}$ the agent has been in each state $s$, and the reward for $s$ is adjusted by exploration bonus $\alpha / \sqrt{N_{s}}$, where $\alpha$ is a constant balancing the reward from completing the task with the exploration bonus. We choose the weight $\alpha$ to maximize success at 0.2 M episodes from the set ${0,0.1,0.2, \ldots, 1}$. The blue curve corresponds to the asymmetric self-play training.</p>
<p>We can see that at the very beginning, a random policy for Alice gives some form of curriculum but eventually is harmful, because Bob never gets to see any long treks. On the other hand, policy gradient sees very few successes in the beginning, and so trains slowly. Using the self-play method, Alice gives Bob easy problems at first (she starts from random), and then builds harder and harder problems as the training progresses, finally matching the performance boost of the count based exploration. Although not shown, similar patterns are observed for a wide range of learning rates.</p>
<h1>4.2 MAZEBASE: LIGHT KEY</h1>
<p>We now describe experiments using the MazeBase environment (Sukhbaatar et al., 2015), which have discrete actions and states, but sufficient combinatorial complexity that tabular methods cannot be used. The environment consist of various items placed on a finite 2D grid; and randomly generated for each episode.</p>
<p>We use an environment where the maze contains a light switch (whose initial state is sampled according to a predefined probability, $\mathrm{p}($ Light off $)$ ), a key and a wall with a door (see Fig. 1). An agent can open or close the door by toggling the key switch, and turn on or off light with the light switch. When the light is off, the agent can only see the (glowing) light switch. In the target task, there is also a goal flag item, and the objective of the game is reach to that goal flag.</p>
<p>In self-play, the environment is the same except there is no specific objective. An episode starts with Alice in control, who can navigate through the maze and change the switch states until she outputs the STOP action. Then, Bob takes control and tries to return everything to its original state (restricted to visible items) in the reverse self-play. In the repeat version, the maze resets back to its initial state when Bob takes the control, who tries to reach the final state of Alice.</p>
<p>In Fig. 2 (right), we set $\mathrm{p}($ Light off $)=0.5$ during self-play ${ }^{4}$ and evaluate the repeat form of self-play, alongside two baselines: (i) target task only training (i.e. no self-play) and (ii) self-play with a random policy for Alice. With self-play, the agent succeeds quickly while target task-only training takes much longer ${ }^{5}$. Fig. 3 shows details of a single training run, demonstrating how Alice and Bob automatically build a curriculum between themselves though self-play.</p>
<h3>4.3 RLLAB: MOUNTAIN CAR</h3>
<p>We applied our approach to the Mountain Car task in RLLab. Here the agent controls a car trapped in a 1-D valley. It must learn to build momentum by alternately moving to the left and right, climbing higher up the valley walls until it is able to escape. Although the problem is presented as continuous, we discretize the 1-D action space into 5 bins (uniformly sized) enabling us to use discrete policy gradient, as above. We also added a secondary action head with binary actions to be used as STOP action. An observation of state $s_{t}$ consists of the location and speed of the car.</p>
<p>As in Houthooft et al. (2016); Tang et al. (2017), a reward of +1 is given only when the car succeeds in climbing the hill. In self-play, Bob succeeds if $\left|s_{b}-s_{a}\right|&lt;0.2$, where $s_{a}$ and $s_{b}$ are the final states (location and velocity of the car) of Alice and Bob respectively.</p>
<p>The nature of the environment makes it highly asymmetric from Alice and Bob's point of view, since it is far easier to coast down the hill to the starting point that it is to climb up it. Hence we exclusively use the reset form of self-play. In Fig. 4 (left), we compare this to current state-of-the-art</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Left: The hallway task from section 4.1. The $y$ axis is fraction of successes on the target task, and the $x$ axis is the total number of training examples seen. Standard policy gradient (red) learns slowly. Adding an explicit exploration bonus (Strehl &amp; Littman, 2008) (green) helps significantly. Our self-play approach (blue) gives similar performance however. Using a random policy for Alice (magenta) drastically impairs performance, showing the importance of self-play between Alice and Bob. Right: Mazebase task, illustrated in Fig. 1, for p(Light off) = 0.5. Augmenting with the repeat form of self-play enables significantly faster learning than training on the target task alone and random Alice baselines.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Inspection of a Mazebase learning run, using the environment shown in Fig. 1. (a): rate at which Alice interacts with 1, 2 or 3 objects during an episode, illustrating the automatically generated curriculum. Initially Alice touches no objects, but then starts to interact with one. But this rate drops as Alice devises tasks that involve two and subsequently three objects. (b) by contrast, in the random Alice baseline, she never utilizes more than a single object and even then at a much lower rate. (c) plot of Alice and Bob's reward, which strongly correlates with (a). (d) plot of $t_{\alpha}$ as self-play progresses. Alice takes an increasing amount of time before handing over to Bob, consistent with tasks of increasing difficulty being set.
methods, namely VIME (Houthooft et al., 2016) and SimHash (Tang et al., 2017). Our approach (blue) performs comparably to both of these. We also tried using policy gradient directly on the target task samples, but it was unable to solve the problem.</p>
<h1>4.4 RLLAB: SWIMMERGATHER</h1>
<p>We also applied our approach to the SwimmerGather task in RLLab (which uses the Mujoco (Todorov et al., 2012) simulator), where the agent controls a worm with two flexible joints, swimming in a 2D viscous fluid. In the target task, the agent gets reward +1 for eating green apples and -1 for touching red bombs, which are not present during self-play. Thus the self-play task and target tasks are different: in the former, the worm just swims around but in the latter it must learn to swim towards green apples and away from the red bombs.
The observation state consists of a 13-dimensional vector describing location and joint angles of the worm, and a 20 dimensional vector for sensing nearby objects. The worm takes two real values as an action, each controlling one joint. We add a secondary action head to our models to handle the 2nd joint, and a third binary action head for STOP action. As in the mountain car, we discretize the output space (each joint is given 9 uniformly sized bins) to allow the use of discrete policy gradients.</p>
<p>Bob succeeds in a self-play episode when $|l_{b}-l_{a}|&lt;0.3$ where $l_{a}$ and $l_{b}$ are the final locations of Alice and Bob respectively. Fig. 4 (right) shows the target task reward as a function of training iteration for our approach alongside state-of-the-art exploration methods VIME (Houthooft et al., 2016) and SimHash (Tang et al., 2017). We demonstrate the generality of the self-play approach by applying it to Reinforce and also TRPO (Schulman et al., 2015) (see Appendix D for details). In both cases, it enables them to gain reward significantly earlier than other methods, although both converge to a similar final value to SimHash. A video of our worm performing the test task can be found at https://goo.gl/Vsd8Js.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Evaluation on MountainCar (left) and SwimmerGather (right) target tasks, comparing to VIME <em>Houthooft et al. (2016)</em> and SimHash <em>Tang et al. (2017)</em> (figures adapted from Tang et al. (2017)). With reversible self-play we are able to learn faster than the other approaches, although it converges to a comparable reward. Training directly on the target task using Reinforce without self-play resulted in total failure. Here 1 iteration = 5k (50k) target task steps in Mountain car (SwimmerGather), excluding self-play steps.</p>
<h3>4.5 StarCraft: Training Marines</h3>
<p>Finally, we applied our self-play approach to the same setup as the beginning of a standard StarCraft: Brood War game <em>Synnaeve et al. (2016)</em>, where an agent controls multiple units to mine, construct buildings, and train new units, but without enemies to fight. The environment starts with 4 workers units (Terran SCVs), who can move around, mine nearby minerals and construct new buildings. In addition, the agent controls the command center, which can train new workers. See Fig. 5 (left) for relations between different units and their actions.</p>
<p>The target task is to build Marine units. To do this, an agent must follow a specific sequence of operations: (i) mine minerals with workers; (ii) having accumulated sufficient mineral supply, build a barracks and (iii) once the barracks are complete, train Marine units out of it. Optionally, an agent can train a new worker for faster mining, or build a supply depot to accommodate more units. When the episode ends after 200 steps (little over 3 minutes), the agent gets rewarded +1 for each Marine it has built. Optimizing this task is highly complex due to several factors. First, the agent has to find an optimal mining pattern (concentrating on a single mineral or mining a far away mineral is inefficient). Then, it has to produce the optimal number of workers and barrack at the right timing. In addition, a supply depot needs to be built when the number of units is close to the limit.</p>
<p>During self-play (repeat variant), Alice and Bob control the workers and can try any combination of actions during the episode. Since exactly matching the game state is almost impossible, Bob's success is only based on the global state of the game, which includes the number of units of each type (including buildings), and accumulated mineral resource. So Bob's objective in self-play is to make as many units and mineral as Alice in shortest possible time. Further details are given in Appendix E. Fig. 5 (right) compares the Reinforce algorithm on the target task, with and without self-play. An additional count-based exploration baseline similar to the hallway experiment is also shown. It utilizes the same global game state as self-play.</p>
<h2>5 DISCUSSION</h2>
<p>In this work we described a novel method for intrinsically motivated learning which we call asymmetric self-play. Despite the method's conceptual simplicity, we have seen that it can be effective in both discrete and continuous input settings with function approximation, for encouraging ex-</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Left: Different types of unit in the StarCraft environment. The arrows represent possible actions (excluding movement actions) by the unit, and corresponding numbers shows (blue) amount of minerals and (red) time steps needed to complete. The units under agent's control are outlined by a green border. Right: Plot of reward on the StarCraft sub-task of training marine units vs #target-task episodes (self-play episodes are not included), with and without self-play. A count-based baseline is also shown. Self-play greatly speeds up learning, and also surpasses the count-based approach at convergence.
ploration and automatically generating curriculums. On the challenging benchmarks we consider, our approach is at least as good as state-of-the-art RL methods that incorporate an incentive for exploration, despite being based on very different principles. Furthermore, it is possible show theoretically that in simple environments, using asymmetric self-play with reward functions from (1) and (2), optimal agents can transit between any pair of reachable states as efficiently as possible. The code for our approach can be found at http://cims.nyu.edu/ sainbar/selfplay.</p>
<h1>REFERENCES</h1>
<p>Marcin Andrychowicz, Dwight Crow, Alex K Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In NIPS, 2017.
A. Baranes and P-Y. Oudeyer. Active learning of inverse models with intrinsically motivated goal exploration in robots. Robotics and Autonomous Systems, 61(1):49-73, 2013.</p>
<p>Andrew G. Barto. Intrinsic Motivation and Reinforcement Learning, pp. 17-47. Springer Berlin Heidelberg, 2013.</p>
<p>Marc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Rémi Munos. Unifying count-based exploration and intrinsic motivation. In NIPS, pp. 1471-1479, 2016.</p>
<p>Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In ICML, pp. 41-48, 2009.</p>
<p>Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. In ICML, 2016.</p>
<p>Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, and Pieter Abbeel. Reverse curriculum generation for reinforcement learning. In CoRL, 2017.</p>
<p>Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, pp. 2672-2680, 2014.</p>
<p>David Held, Xinyang Geng, Carlos Florensa, and Pieter Abbeel. Automatic goal generation for reinforcement learning agents. CoRR, abs/1705.06366, 2017.</p>
<p>Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Curiosity-driven exploration in deep reinforcement learning via bayesian neural networks. arXiv 1605.09674, 2016.</p>
<p>Alexander S. Klyubin, Daniel Polani, and Chrystopher L. Nehaniv. Empowerment: a universal agent-centric measure of control. In Proceedings of the IEEE Congress on Evolutionary Computation, CEC, pp. 128-135, 2005.
M. P. Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable models. In NIPS. 2010.</p>
<p>Jiwei Li, Will Monroe, Tianlin Shi, Sébastien Jean, Alan Ritter, and Daniel Jurafsky. Adversarial learning for neural dialogue generation. In EMNLP, 2017.</p>
<p>Manuel Lopes, Tobias Lang, Marc Toussaint, and Pierre-Yves Oudeyer. Exploration in model-based reinforcement learning by empirically estimating learning progress. In NIPS, pp. 206-214, 2012.</p>
<p>Lars M. Mescheder, Sebastian Nowozin, and Andreas Geiger. Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks. In ICML, 2017.</p>
<p>Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In ICML, 2017.</p>
<p>Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial reinforcement learning. In ICML, 2017.</p>
<p>Martin Riedmiller, Thomas Gabel, Roland Hafner, and Sascha Lange. Reinforcement learning for robot soccer. Autonomous Robots, 27(1):55-73, 2009.</p>
<p>Arthur L. Samuel. Some studies in machine learning using the game of checkers. IBM Journal of Research and Development, 3(3):210-229, 1959.</p>
<p>Tom Schaul, Dan Horgan, Karol Gregor, and David Silver. Universal value function approximators. In ICML, pp. 1312-1320, 2015.
J. Schmidhuber. Curious model-building control systems. In Proc. Int. J. Conf. Neural Networks, pp. 1458-1463. IEEE Press, 1991.</p>
<p>John Schulman, Sergey Levine, Pieter Abbeel, Michael I. Jordan, and Philipp Moritz. Trust region policy optimization. In ICML, 2015.</p>
<p>David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484-489, January 2016.</p>
<p>Satinder P. Singh, Andrew G. Barto, and Nuttapong Chentanez. Intrinsically motivated reinforcement learning. In NIPS, pp. 1281-1288, 2004.</p>
<p>Alexander L. Strehl and Michael L. Littman. An analysis of model-based interval estimation for markov decision processes. J. Comput. Syst. Sci., 74(8):1309-1331, 2008.</p>
<p>Sainbayar Sukhbaatar, Arthur Szlam, Gabriel Synnaeve, Soumith Chintala, and Rob Fergus. Mazebase: A sandbox for learning from games. arXiv 1511.07401, 2015.</p>
<p>Richard S. Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M. Pilarski, Adam White, and Doina Precup. Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction. In AAMAS '11, pp. 761-768, 2011.</p>
<p>Gabriel Synnaeve, Nantas Nardelli, Alex Auvolat, Soumith Chintala, Timothée Lacroix, Zeming Lin, Florian Richoux, and Nicolas Usunier. Torchcraft: a library for machine learning research on real-time strategy games. arXiv preprint arXiv:1611.00625, 2016.</p>
<p>Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. #exploration: A study of count-based exploration for deep reinforcement learning. In NIPS, 2017.</p>
<p>Gerald Tesauro. Temporal difference learning and td-gammon. Commun. ACM, 38(3):58-68, 1995.</p>
<p>T. Tieleman and G. Hinton. Lecture 6.5 - rmsprop, coursera: Neural networks for machine learning, 2012.</p>
<p>Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In IROS, pp. 5026-5033. IEEE, 2012.</p>
<p>Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. In Machine Learning, pp. 229-256, 1992.</p>
<h1>A PSEUDO CODE</h1>
<p>Algorithm 1 and 2 are the pseudo codes for training an agent on self-play and target task episodes.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nx">Pseudo</span><span class="w"> </span><span class="nx">code</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">training</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">agent</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="kp">self</span><span class="o">-</span><span class="nx">play</span><span class="w"> </span><span class="nx">episode</span>
<span class="w">    </span><span class="nx">function</span><span class="w"> </span><span class="nx">SelfPlayEPISODE</span><span class="p">(</span><span class="nx">ReVerSE</span><span class="o">/</span><span class="nx">REPEAT</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">t_</span><span class="p">{</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">MAX</span><span class="p">}},</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">A</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">B</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="p">)</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">t_</span><span class="p">{</span><span class="nx">A</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="mi">0</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="mi">0</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">env</span><span class="p">.</span><span class="k">observe</span><span class="p">()</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">s</span><span class="o">^</span><span class="p">{</span><span class="o">*</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">s_</span><span class="p">{</span><span class="mi">0</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">while</span><span class="w"> </span><span class="nx">True</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="err">\#</span><span class="w"> </span><span class="nx">Alice</span><span class="err">&#39;</span><span class="nx">s</span><span class="w"> </span><span class="nx">turn</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="nx">t_</span><span class="p">{</span><span class="nx">A</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">t_</span><span class="p">{</span><span class="nx">A</span><span class="p">}</span><span class="o">+</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="nx">s</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">env</span><span class="p">.</span><span class="k">observe</span><span class="p">()</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="nx">a</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">pi_</span><span class="p">{</span><span class="nx">A</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s</span><span class="p">,</span><span class="w"> </span><span class="nx">s_</span><span class="p">{</span><span class="mi">0</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)=</span><span class="nx">f</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s</span><span class="p">,</span><span class="w"> </span><span class="nx">s_</span><span class="p">{</span><span class="mi">0</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">A</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">a</span><span class="p">=</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">STOP</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">t_</span><span class="p">{</span><span class="nx">A</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">geq</span><span class="w"> </span><span class="nx">t_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">Max</span><span class="w"> </span><span class="p">}}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="nx">s</span><span class="o">^</span><span class="p">{</span><span class="o">*</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">s</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">env</span><span class="p">.</span><span class="nx">reset</span><span class="p">()</span>
<span class="w">            </span><span class="k">break</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">env</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">act</span><span class="p">}(</span><span class="nx">a</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">t_</span><span class="p">{</span><span class="nx">B</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="mi">0</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">while</span><span class="w"> </span><span class="nx">True</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="err">\#</span><span class="w"> </span><span class="nx">Bob</span><span class="err">&#39;</span><span class="nx">s</span><span class="w"> </span><span class="nx">turn</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="nx">s</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">env</span><span class="p">.</span><span class="k">observe</span><span class="p">()</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s</span><span class="p">=</span><span class="nx">s</span><span class="o">^</span><span class="p">{</span><span class="o">*</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">t_</span><span class="p">{</span><span class="nx">A</span><span class="p">}</span><span class="o">+</span><span class="nx">t_</span><span class="p">{</span><span class="nx">B</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">geq</span><span class="w"> </span><span class="nx">t_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">Max</span><span class="w"> </span><span class="p">}}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="w">            </span><span class="k">break</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="nx">t_</span><span class="p">{</span><span class="nx">B</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">t_</span><span class="p">{</span><span class="nx">B</span><span class="p">}</span><span class="o">+</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="nx">a</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">pi_</span><span class="p">{</span><span class="nx">B</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s</span><span class="p">,</span><span class="w"> </span><span class="nx">s</span><span class="o">^</span><span class="p">{</span><span class="o">*</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)=</span><span class="nx">f</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s</span><span class="p">,</span><span class="w"> </span><span class="nx">s</span><span class="o">^</span><span class="p">{</span><span class="o">*</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">B</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">env</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">act</span><span class="p">}(</span><span class="nx">a</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">R_</span><span class="p">{</span><span class="nx">A</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">gamma</span><span class="w"> </span><span class="err">\</span><span class="nx">max</span><span class="w"> </span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="nx">t_</span><span class="p">{</span><span class="nx">B</span><span class="p">}</span><span class="o">-</span><span class="nx">t_</span><span class="p">{</span><span class="nx">A</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">R_</span><span class="p">{</span><span class="nx">B</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="o">-</span><span class="err">\</span><span class="nx">gamma</span><span class="w"> </span><span class="nx">t_</span><span class="p">{</span><span class="nx">B</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">policy</span><span class="p">.</span><span class="nx">update</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">R_</span><span class="p">{</span><span class="nx">A</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">A</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">policy</span><span class="p">.</span><span class="nx">update</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">R_</span><span class="p">{</span><span class="nx">B</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">B</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span>
</code></pre></div>

<p>Algorithm 2 Pseudo code for training an agent on a target task episode</p>
<div class="codehilite"><pre><span></span><code><span class="nx">function</span><span class="w"> </span><span class="nx">TARGETTASKEPISODE</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">t_</span><span class="p">{</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">MAX</span><span class="p">}},</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">B</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">t</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="mi">0</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">R</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="mi">0</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">while</span><span class="w"> </span><span class="nx">True</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="nx">t</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="nx">s</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">env</span><span class="p">.</span><span class="k">observe</span><span class="p">()</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="nx">a</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">pi_</span><span class="p">{</span><span class="nx">B</span><span class="p">}(</span><span class="nx">s</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">emptyset</span><span class="p">)=</span><span class="nx">f</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">emptyset</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">B</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nx">env</span><span class="p">.</span><span class="nx">done</span><span class="p">()</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">t</span><span class="w"> </span><span class="err">\</span><span class="nx">geq</span><span class="w"> </span><span class="nx">t_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">Max</span><span class="w"> </span><span class="p">}}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="w">            </span><span class="k">break</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">env</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">act</span><span class="p">}(</span><span class="nx">a</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="nx">R</span><span class="p">=</span><span class="nx">R</span><span class="o">+</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">env</span><span class="p">.</span><span class="nx">reward</span><span class="p">()</span>
<span class="w">    </span><span class="nx">policy</span><span class="p">.</span><span class="nx">update</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">R</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">B</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span>
</code></pre></div>

<h1>B HYPERPARAMETERS USED IN THE EXPERIMENTS</h1>
<p>For the experiments with neural networks, all parameters are randomly initialized from $\mathcal{N}(0,0.2)$. The Hyperparameters of RMSProp are set to 0.97 and $1 e-6$. The other hyperparameter values used in the experiments are shown in Table 1. In some cases, we used different parameters for self-play and target task episodes. Entropy regularization is implemented as an additional cost maximizing the entropy of the softmax layer. In the StarCraft, skipping 23 frames roughly matches to one action per second.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter <br> name</th>
<th style="text-align: center;">Long <br> Hallway</th>
<th style="text-align: center;">Mazebase</th>
<th style="text-align: center;">Mountain <br> Car</th>
<th style="text-align: center;">Swimmer <br> Gather</th>
<th style="text-align: center;">StarCraft</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Learning rate</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.003</td>
<td style="text-align: center;">0.003</td>
<td style="text-align: center;">0.003</td>
<td style="text-align: center;">0.003</td>
</tr>
<tr>
<td style="text-align: left;">Batch size</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: left;">Max steps of <br> episode $\left(t_{\max }\right)$</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">TT: 166 <br> SP: 200</td>
<td style="text-align: center;">200</td>
</tr>
<tr>
<td style="text-align: left;">Entropy <br> regularization</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.003</td>
<td style="text-align: center;">0.003</td>
<td style="text-align: center;">TT: 0 <br> SP: 0.003</td>
<td style="text-align: center;">TT: 0 <br> SP: 0.003</td>
</tr>
<tr>
<td style="text-align: left;">Self-play reward scale $(\gamma)$</td>
<td style="text-align: center;">0.033</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr>
<td style="text-align: left;">Self-play percentage</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$20 \%$</td>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$10 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Self-play mode</td>
<td style="text-align: center;">Reverse</td>
<td style="text-align: center;">Both</td>
<td style="text-align: center;">Repeat</td>
<td style="text-align: center;">Reverse</td>
<td style="text-align: center;">Repeat</td>
</tr>
<tr>
<td style="text-align: left;">Frame skip</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$150^{6}$</td>
<td style="text-align: center;">23</td>
</tr>
</tbody>
</table>
<p>Table 1: Hyperparameter values used in experiments. TT=target task, SP=self-play</p>
<h2>C MAZEBASE</h2>
<p>The agent has full visibility of the maze when the light is on. If light is off, the agent can only see the light switch. In self-play, Bob does not need to worry about things that are invisible to him. For example, if Alice started with light "off" in reverse self-play, Bob does not need to match the state of the door, because it would be invisible to him when the light is off.</p>
<p>In the target task, the agent and the goal are always placed on opposite sides of the wall. Also, the light and key switches are placed on the same side as the agent, but the light is always off and the door is closed initially. Therefore, in order to succeed, the agent has to turn on the light, toggle the key switch to open the door, pass through it, and reach the goal flag.
Both Alice and Bob's policies are modeled by a fully-connected neural network with two hidden layers each with 100 and 50 units (with tanh non-linearities) respectively. The encoder into each of the networks takes a bag of words over (objects, locations); that is, there is a separate word in the lookup table for each (object, location) pair. Action probabilities are output by a linear layer followed by a softmax.</p>
<h2>C. 1 BIASING FOR OR AGAINST SELF-PLAY</h2>
<p>The effectiveness of our approach depends in part on the similarity between the self-play and target tasks. One way to explore this in our environment is to vary the probability of the light being off initially during self-play episodes ${ }^{7}$. Note that the light is always off in the target task; if the light is usually on at the start of Alice's turn in reverse, for example, she will learn to turn it off, and then Bob will be biased to turn it back on. On the other hand, if the light is usually off at the start of Alice's turn in reverse, Bob is strongly biased against turning the light on, and so the test task becomes especially hard. Thus changing this probability gives us some way to adjust the similarity between the two tasks.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Fig. 6 (left) shows what happens when p (Light off) = 0.3 . Here reverse self-play works well, but repeat self-play does poorly. As discussed above, this flipping, relative to the previous experiment, can be explained as follows: low p (Light off) means that Bob’s task in reverse self-play will typically involve returning the light to the on position (irrespective of how Alice left it), the same function that must be performed in the target task. The opposite situation applies for repeat self-play, where Bob needs to encounter the light typically in the off position to help him with the test task.</p>
<p>In Fig. 6 (right) we systematically vary p (Light off) between 0.1 and 0.9 . The y-axis shows the speed-up (reduction in target task episodes) relative to training purely on the target-task for runs where the reward goes above -2. Unsuccessful runs are given a unity speed-up factor. The curves show that when the self-play task is not biased against the target task it can help significantly.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Left: The performance of self-play when p (Light off) set to 0.3 . Here the reverse form of self-play works well (more details in the text). Right: Reduction in target task episodes relative to training purely on the target-task as the distance between self-play and the target task varies (for runs where the reward goes above -2 on the Mazebase task – unsuccessful runs are given a unity speed-up factor). The y axis is the speedup, and x axis is p (Light off). For reverse self-play, the low p (Light off) corresponds to having self-play and target tasks be similar to one another, while the opposite applies to repeat self-play. For both forms, significant speedups are achieved when self-play is similar to the target tasks, but the effect diminishes when self-play is biased against the target task.</p>
<h3>D SWIMMERGATHER EXPERIMENT</h3>
<p>In Fig. 7 shows details of a single training run. The changes in Alice’s behavior, observed in Fig. 7(c) and (d), correlate with Alice and Bob’s reward (Fig. 7(b)) and, initially at least, to the reward on the test target (Fig. 7(a)). In Fig. 8 we visualize for a single training run the locations where Alice hands over to Bob at different stages of training, showing how the distribution varies.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: A single SwimmerGather training run. (a): Rewards on target task. (b): Rewards from reversible self-play. (c): The number of actions taken by Alice. (d): Distance that Alice travels before switching to Bob.</p>
<p>In the TRPO experiment, we used step size 0.01 and damping coefficient 0.1 . The batch consists of 50,000 steps, of which 25% comes from target task episodes, while the remaining 75% is from self-play. The self-play reward scale γ set to 0.005 . We used two separate network for actor and critic, and the critic network has L2 weight regularization with coefficient of 1e − 5.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Plot of Alice's location at time of STOP action for the SwimmerGather training run shown in Fig. 7, for different stages of training. Note how Alice's distribution changes as Bob learns to solve her tasks.</p>
<h1>E StarCraft Experiment</h1>
<p>We call units that perform action as active unit. This includes worker units (SCVs), the command center, and barrack. The agent controls multiple active units in parallel. At each time step, an action of $i$ 'th unit is output by</p>
<p>$$
a_{t}^{i}=\pi\left(s_{t}^{i}, \hat{s}_{t}\right)
$$</p>
<p>where $s_{t}^{i}$ is a unit specific local observation, and $\hat{s}<em t="t">{t}$ is an global observation. With $s</em>$, a unit can see the 64 x 64 area around it with a resolution of 4 (unit's type is also visible). The global observation contains the number of units and accumulated minerals in the game}^{i</p>
<p>$$
\hat{s}<em _ore="{ore" _text="\text">{t}=\left{\left\lfloor N</em>\right}
$$}} / 25\right\rfloor, N_{\mathrm{SCV}}, N_{\text {Barrack }}, N_{\text {SupplyDepot }}, N_{\text {Marines }</p>
<p>In self-play, Bob perceives only the global observation of his target state</p>
<p>$$
\pi_{B}\left(s_{t}^{i}, \hat{s}_{t}, \hat{s}^{*}\right)
$$</p>
<p>where $\hat{s}^{*}$ is the final global observation of Alice. Bob will succeed only if</p>
<p>$$
\forall i \quad \hat{s}_{t}[i] \geq \hat{s}^{*}[i]
$$</p>
<p>Table 2 shows the action space of different unit types controlled by the agent. The number of possible action is the same for all units since they controlled by a single model (unit type is encoded in the observation), but the meaning of actions differ according to unit type. An empty cell mean that the unit does nothing (nothing is sent to StarCraft, so the previous action persists).</p>
<p>The more complexes actions "mine minerals", "build a barracks", "build a supply depot" have the following semantics, respectively: mine the mineral closest to the current unit, build a barracks at the position of the current unit, build a supply depot on the position of the current unit.</p>
<p>Some actions are ignored under certain conditions: "mining" action is ignored if the distance to the closest mineral is greater than 12; "switch to Bob" is ignored if Bob is already in control; "building" and "training" actions are ignored if there is not enough resources; the actions that create a new SCV or a barracks are ignored if the number of active units is reached the limit of 10. Also "build" actions will be ignored if there is not enough room to build at the unit's location.</p>
<p>For the count-based exploration, we gave an extra reward of $\alpha / \sqrt{N\left(\hat{s}<em t="t">{t}\right)}$ at every step, where $N$ is the visit count function and $\hat{s}</em>$ is a global observation. We found $\alpha=0.1$ to be works the best.</p>
<p>In Fig. 9 we show the result of an additional experiment where we extended the length of the episode from 200 to 300, giving more time to the agent for development. The self-play still outperforms baselines methods. Note that to make more than 6 marines, an agent has to build a supply depot as well as a barracks.</p>
<h2>F Further Discussion</h2>
<h2>F. 1 Meta-exploration for Alice</h2>
<p>We want Alice and Bob to explore the state (or state-action) space, and we would like Bob to be exposed to many different tasks. Because of the form of the standard reinforcement learning</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Action ID</th>
<th style="text-align: left;">SCV</th>
<th style="text-align: left;">Command center</th>
<th style="text-align: left;">Barraks</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: left;">move to right</td>
<td style="text-align: left;">train SCV</td>
<td style="text-align: left;">train a marine</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: left;">move to left</td>
<td style="text-align: left;">switch to Bob</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: left;">move to top</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: left;">move to bottom</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: left;">mine minerals</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: left;">build a barracks</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: left;">build a supply depot</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 2: Action space of different unit types in StarCraft.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Plot of reward on the StarCraft sub-task of training where episode length $t_{\text {Max }}$ is increased to 300 .
objective (expectation over rewards), Alice only wants to find the single hardest thing for Bob, and is not interested in the space of things that are hard for Bob. In the fully tabular setting, with fully reversible dynamics or with resetting, and without the constraints of realistic optimization strategies, we saw in section 2.2 that this ends up forcing Bob and Alice to learn to make any state transition as efficiently as possible. However, with more realistic optimization methods or environments, and with function approximation, Bob and Alice can get stuck in sub-optimal minima.</p>
<p>For example, let us follow the argument in the third paragraph of Sec. 2.2, and assume that Bob and Alice are at an equilibrium (and that we are in the tabular, finite, Markovian setting), but now we can only update Bob's and Alice's policy locally. By this we mean that in our search for a better policy for Bob or Alice, we can only make small perturbations, as in policy gradient algorithms. In this case, we can only guarantee that Bob runs a fast policy on challenges that Alice has non-zero probability of giving; but there is no guarantee that Alice will cover all possible challenges. With function approximation instead of tabular policies, we cannot make any guarantees at all.</p>
<p>Another example with a similar outcome but different mechanism can occur using the reverse game in an environment without fully reversible dynamics. In that case, it could be that the shortest expected number of steps to complete a challenge $\left(s_{0}, s_{T}\right)$ is longer than the reverse, and indeed, so much longer that Alice should concentrate all her energy on this challenge to maximize her rewards. Thus there could be equilibria with Bob matching the fast policy only for a subset of challenges even if we allow non-local optimization.</p>
<p>The result is that Alice can end up in a policy that is not ideal for our purposes. In figure 8 we show the distributions of where Alice cedes control to Bob in the swimmer task. We can see that Alice has a preferred direction. Ideally, in this environment, Alice would be teaching Bob how to get from any state to any other efficiently; but instead, she is mostly teaching him how to move in one direction.</p>
<p>One possible approach to correcting this is to have multiple Alices, regularized so that they do not implement the same policy. More generally, we can investigate objectives for Alice that encourage her to cover a wider distribution of behaviors.</p>
<h1>F. 2 COMMUNICATING VIA ACTIONS</h1>
<p>In this work we have limited Alice to propose tasks for Bob by doing them. This limitation is practical and effective in restricted environments that allow resetting or are (nearly) reversible. It allows a solution to three of the key difficulties of implementing the basic idea of "Alice proposes tasks, Bob does them": parameterizing the sampling of tasks, representing and communicating the tasks, and ensuring the appropriate level of difficulty of the tasks. Each of these is interesting in more general contexts. In this work, the tasks have incentivized efficient transitions. One can imagine other reward functions and task representations that incentivize discovering statistics of the states and state-transitions, for example models of their causality or temporal ordering, cluster structure.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ Experiments in VIME and SimHash papers skip 50 frames, but we matched the total number of frames in an episode by reducing the number of steps.
${ }^{7}$ The initial state of the light should dramatically change the behavior of the agent: if it is on then agent can directly proceed to the key.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4375 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4375</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4375</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-274446281</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.02149v1.pdf" target="_blank">Leveraging Large Language Models for Comparative Literature Summarization with Reflective Incremental Mechanisms</a></p>
                <p><strong>Paper Abstract:</strong> In this paper, we introduce ChatCite, a novel method leveraging large language models (LLMs) for generating comparative literature summaries. The ability to summarize research papers with a focus on key comparisons between studies is an essential task in academic research. Existing summarization models, while effective at generating concise summaries, fail to provide deep comparative insights. ChatCite addresses this limitation by incorporating a multi-step reasoning mechanism that extracts critical elements from papers, incrementally builds a comparative summary, and refines the output through a reflective memory process. We evaluate ChatCite on a custom dataset, CompLit-LongContext, consisting of 1000 research papers with annotated comparative summaries. Experimental results show that ChatCite outperforms several baseline methods, including GPT-4, BART, T5, and CoT, across various automatic evaluation metrics such as ROUGE and the newly proposed G-Score. Human evaluation further confirms that ChatCite generates more coherent, insightful, and fluent summaries compared to these baseline models. Our method provides a significant advancement in automatic literature review generation, offering researchers a powerful tool for efficiently comparing and synthesizing scientific research.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4375.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4375.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatCite (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatCite</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-stage, generative LLM method for comparative literature summarization that combines pretraining on academic text, comparative fine-tuning with a contrastive comparative loss, and a long-context reflective memory mechanism to synthesize comparative summaries across multiple papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatCite</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Generative LLM pipeline composed of (1) pre-training a GPT-style base on a large corpus of academic papers, (2) comparative fine-tuning on grouped paper inputs with reference comparative summaries using a combined generation + comparative (contrastive) loss, and (3) a long-context memory mechanism that chunks documents, applies cross-chunk attention, and updates a memory (Mem) via attention and GRU-style updates to retain and reflect on information across chunks; additionally includes key-element extraction, a comparative incremental mechanism for building summaries stepwise, and a reflective memory mechanism for refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4.0 (used as the base GPT-style model for pretraining/fine-tuning and as a baseline comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Key-element extraction via multi-step reasoning (explicit component), chunking of long documents with attention to select relevant chunk outputs, and retrieval of previous chunk memory (Attn + GRU memory updates); comparative insights are learned via contrastive comparative loss between per-document comparative elements and reference comparative summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Generative synthesis conditioned on concatenated/processed multi-document context with a comparative incremental mechanism that incrementally builds a joint comparative summary across chunks and papers; contrastive comparative loss encourages aggregation of comparative relationships; reflective memory refines output by reusing memory from earlier chunks.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Trained/evaluated on a dataset of 1000 papers (CompLit-LongContext / CiteComp-1000); individual training instances are described as pairs or groups of papers (exact per-instance group sizes not specified).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Computer science research papers (multiple subdomains such as ML, NLP, CV)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Comparative literature summaries / automated literature reviews (textual comparative analyses highlighting strengths, weaknesses, trends and comparisons between papers).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROUGE-1, ROUGE-2, ROUGE-L; G-Score (novel metric introduced to quantify comparative analysis quality); human expert ratings on Coherence, Comparative Insight, and Fluency.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported automatic metrics for ChatCite: ROUGE-1 = 0.50, ROUGE-2 = 0.25, ROUGE-L ≈ 0.45, G-Score = 92 (as reported in Table 1). Human evaluation: higher ratings (Coherence 4.7, Comparative Insight 4.5, Fluency 4.8 on 1-5 scale) relative to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against GPT-4.0 (zero-shot), BART, T5, and CoT (chain-of-thought) as baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Outperforms listed baselines: e.g., vs GPT-4.0 (zero-shot) ChatCite improves ROUGE-1 by +0.05 (0.50 vs 0.45), ROUGE-2 by +0.05 (0.25 vs 0.20), and G-Score by +7 points (92 vs 85); human evaluators prefer ChatCite across coherence, comparative insight and fluency.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Multi-stage fine-tuning targeted at comparative summarization, a long-context reflective memory mechanism, and explicit key-element extraction enable the model to produce more coherent and insightful comparative summaries; ablation shows largest performance drops when key element extraction is removed, indicating its critical role; contrastive comparative loss helps the model focus on inter-document relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Paper notes long-context limitations of standard LLMs and addresses them architecturally; acknowledges broader LLM issues such as potential factual hallucination (citing related work) and scalability/interpretability concerns; exact limitations in cross-paper contradiction handling and computational cost are discussed qualitatively but not exhaustively quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Method is designed to handle long-context via chunking and memory updates; evaluated on a 1000-paper dataset, but the paper does not present a detailed scaling curve showing performance vs number of papers or model size; claims improved long-context handling due to memory mechanism but lacks fine-grained scaling analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Large Language Models for Comparative Literature Summarization with Reflective Incremental Mechanisms', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4375.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4375.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatCite (Li et al., 2024b)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatCite: LLM agent with human workflow guidance for comparative literature summary</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based agent approach that mimics human workflows to organize and compare key aspects of multiple papers to generate comparative literature summaries (presented in related work and cited by the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ChatCite: LLM agent with human workflow guidance for comparative literature summary</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatCite (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described as an LLM agent that emulates human workflows (human-like guidance) to structure comparative summarization tasks, organizing and comparing key dimensions across papers; specifics of architecture are not given in this paper (referenced as prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper (referenced work only).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Reported as using human-workflow guidance to organize information extraction and comparison (details not specified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Organizes and compares key aspects across multiple papers, implying agent-based stepwise synthesis; exact mechanism not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in this paper (referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Comparative literature summarization (general scientific literature; cited in context of academic papers).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Comparative literature summaries / organized comparisons across papers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this paper (referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not reported in this paper (referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Characterized as an important contribution in related work for mimicking human guidance in LLM agents to improve comparative summarization, but this paper does not reproduce or detail findings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not detailed here; cited as related work without full experimental detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Large Language Models for Comparative Literature Summarization with Reflective Incremental Mechanisms', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4375.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4375.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LitLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LitLLM: A toolkit for scientific literature review</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A toolkit designed to assist scientific literature review tasks by facilitating comparative summarization and helping researchers evaluate and synthesize multiple sources (cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Litllm: A toolkit for scientific literature review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LitLLM</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Toolkit aimed at supporting literature review workflows; described as facilitating comparative summarization by helping researchers evaluate and synthesize multiple papers. The paper cites LitLLM as an existing tool but does not detail its internal mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not specified in this paper (toolkit likely includes retrieval and summarization utilities but details are not provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Facilitates comparative summarization and synthesis across sources (implementation details not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific literature review (general across domains).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Tools/outputs for literature reviews and comparative summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as a practical toolkit supporting literature review generation, indicating active interest in tooling around LLM-assisted literature synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Large Language Models for Comparative Literature Summarization with Reflective Incremental Mechanisms', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4375.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4375.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval-augmented methods (RAG / retrieval + long-context LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-augmented generation and retrieval-meets-long-context approaches</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that combine information retrieval (embedding-based or other retrieval) with large language models to focus LLM attention on the most relevant document sections, improving information extraction and generation from long documents or multi-document inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Retrieval-augmented generation / retrieval + long-context LLM</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Hybrid approach that first retrieves relevant chunks or passages (often by embedding-based nearest neighbor search) from large collections of documents, then conditions an LLM on the retrieved context to extract, summarize, or synthesize information; when combined with long-context models this enables processing of long/multi-document inputs more efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper (discussed generally; related references include combinations with long-context LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Embedding-based retrieval of relevant chunks/passages; retrieval of most relevant sections prior to LLM generation.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Generation conditioned on retrieved evidence (retrieval-augmented generation), enabling focused synthesis across multiple documents; can be used with chunking and memory mechanisms for longer contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not concretely specified here; described as applicable to long documents and multi-document inputs (scales with available corpora and retrieval design).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General long-document and multi-document summarization tasks; referenced for long-context literature extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Summaries, extracted evidence, and synthesis across documents (used to improve extraction and factual grounding).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified here in detail; referenced works evaluate retrieval-augmented approaches using standard summarization and factuality metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not reported quantitatively in this paper (cited as an effective approach in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Implied comparison to pure generation from LLM without retrieval; cited works evaluate improvements when retrieval is added.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Paper cites related literature claiming improved efficiency and accuracy when combining retrieval with long-context LLMs, but does not report quantitative comparisons itself.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining retrieval with long-context LLMs helps models focus on relevant sections of long documents and improves extraction accuracy; cited as a promising direction for multi-document synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Implementation-dependent (retrieval quality, latency, and potential for omitted relevant passages); not exhaustively discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Described as improving efficiency for long documents; exact scaling behaviour depends on retrieval system and LLM context handling — no detailed scaling curves provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Large Language Models for Comparative Literature Summarization with Reflective Incremental Mechanisms', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4375.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4375.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain of Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning technique where the model is prompted or trained to produce intermediate reasoning steps (chains of thought) to improve complex multi-step outputs; used here as a baseline for comparative summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Method/approach that elicits intermediate reasoning steps from LLMs to improve multi-step reasoning and complex output generation; in this paper CoT is evaluated as a baseline approach for comparative summarization tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified here beyond being a reasoning method applied with LLM baselines (e.g., GPT-style models).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Implicit multi-step prompted reasoning to extract and combine evidence across input texts (not a formal extraction pipeline in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Intermediate step-based synthesis where the model generates stepwise reasoning before final summary output.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied as a baseline to the same dataset (CompLit-LongContext); exact per-instance paper counts are not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Comparative literature summarization (as a baseline method).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Comparative summaries generated via chain-of-thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROUGE scores and G-Score used in the paper to evaluate CoT baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>CoT baseline reported: ROUGE-1 = 0.37, ROUGE-2 = 0.14, ROUGE-L = 0.33, G-Score = 72 (from Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against ChatCite, GPT-4.0, BART, T5.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Per reported results, CoT underperforms ChatCite and other baselines on ROUGE and G-Score (e.g., ChatCite ROUGE-1 0.50 vs CoT 0.37).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT provides an interpretable multi-step reasoning baseline but is less effective than the proposed multi-stage fine-tuned, memory-equipped ChatCite on comparative summarization in this evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>CoT alone may not sufficiently address long-context or multi-document aggregation needs and underperforms when compared to a tailored multi-stage fine-tuning plus memory approach.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not analyzed in detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Large Language Models for Comparative Literature Summarization with Reflective Incremental Mechanisms', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4375.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4375.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LongLoRA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LongLoRA: Efficient fine-tuning of long-context large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method for efficient fine-tuning of LLMs to handle longer sequences without large computational overhead; cited in the paper as relevant to enabling long-context LLM adaptations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Longlora: Efficient fine-tuning of long-context large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LongLoRA</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Efficient fine-tuning technique tailored to adapt LLMs for longer sequence lengths with reduced computational cost (referenced as a method to address fine-tuning for long-context tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper's description (referenced work likely applies to GPT-style or transformer LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not specifically an extraction technique; enables models to process longer contexts which supports extraction from long documents.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Enables synthesis across longer contexts by allowing the model to accept longer input sequences after fine-tuning; details not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Long-context LLM tasks (general).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Supports generation/synthesis from longer contexts (general).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified here (referenced work contains evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not reported in this paper (referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an important method for efficient long-context fine-tuning relevant to multi-document summarization tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Intended to improve scaling to longer sequence lengths with lower overhead (details are in the cited LongLoRA work, not this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging Large Language Models for Comparative Literature Summarization with Reflective Incremental Mechanisms', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ChatCite: LLM agent with human workflow guidance for comparative literature summary <em>(Rating: 2)</em></li>
                <li>Litllm: A toolkit for scientific literature review <em>(Rating: 2)</em></li>
                <li>Retrieval meets long context large language models <em>(Rating: 2)</em></li>
                <li>Longlora: Efficient fine-tuning of long-context large language models <em>(Rating: 2)</em></li>
                <li>LooGLE: Can long-context language models understand long contexts? <em>(Rating: 1)</em></li>
                <li>Lost in the middle: How language models use long contexts <em>(Rating: 1)</em></li>
                <li>Zero-shot NLG evaluation through pairwise comparisons using large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4375",
    "paper_id": "paper-274446281",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "ChatCite (this paper)",
            "name_full": "ChatCite",
            "brief_description": "A multi-stage, generative LLM method for comparative literature summarization that combines pretraining on academic text, comparative fine-tuning with a contrastive comparative loss, and a long-context reflective memory mechanism to synthesize comparative summaries across multiple papers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ChatCite",
            "system_description": "Generative LLM pipeline composed of (1) pre-training a GPT-style base on a large corpus of academic papers, (2) comparative fine-tuning on grouped paper inputs with reference comparative summaries using a combined generation + comparative (contrastive) loss, and (3) a long-context memory mechanism that chunks documents, applies cross-chunk attention, and updates a memory (Mem) via attention and GRU-style updates to retain and reflect on information across chunks; additionally includes key-element extraction, a comparative incremental mechanism for building summaries stepwise, and a reflective memory mechanism for refinement.",
            "llm_model_used": "GPT-4.0 (used as the base GPT-style model for pretraining/fine-tuning and as a baseline comparison)",
            "extraction_technique": "Key-element extraction via multi-step reasoning (explicit component), chunking of long documents with attention to select relevant chunk outputs, and retrieval of previous chunk memory (Attn + GRU memory updates); comparative insights are learned via contrastive comparative loss between per-document comparative elements and reference comparative summaries.",
            "synthesis_technique": "Generative synthesis conditioned on concatenated/processed multi-document context with a comparative incremental mechanism that incrementally builds a joint comparative summary across chunks and papers; contrastive comparative loss encourages aggregation of comparative relationships; reflective memory refines output by reusing memory from earlier chunks.",
            "number_of_papers": "Trained/evaluated on a dataset of 1000 papers (CompLit-LongContext / CiteComp-1000); individual training instances are described as pairs or groups of papers (exact per-instance group sizes not specified).",
            "domain_or_topic": "Computer science research papers (multiple subdomains such as ML, NLP, CV)",
            "output_type": "Comparative literature summaries / automated literature reviews (textual comparative analyses highlighting strengths, weaknesses, trends and comparisons between papers).",
            "evaluation_metrics": "ROUGE-1, ROUGE-2, ROUGE-L; G-Score (novel metric introduced to quantify comparative analysis quality); human expert ratings on Coherence, Comparative Insight, and Fluency.",
            "performance_results": "Reported automatic metrics for ChatCite: ROUGE-1 = 0.50, ROUGE-2 = 0.25, ROUGE-L ≈ 0.45, G-Score = 92 (as reported in Table 1). Human evaluation: higher ratings (Coherence 4.7, Comparative Insight 4.5, Fluency 4.8 on 1-5 scale) relative to baselines.",
            "comparison_baseline": "Compared against GPT-4.0 (zero-shot), BART, T5, and CoT (chain-of-thought) as baselines.",
            "performance_vs_baseline": "Outperforms listed baselines: e.g., vs GPT-4.0 (zero-shot) ChatCite improves ROUGE-1 by +0.05 (0.50 vs 0.45), ROUGE-2 by +0.05 (0.25 vs 0.20), and G-Score by +7 points (92 vs 85); human evaluators prefer ChatCite across coherence, comparative insight and fluency.",
            "key_findings": "Multi-stage fine-tuning targeted at comparative summarization, a long-context reflective memory mechanism, and explicit key-element extraction enable the model to produce more coherent and insightful comparative summaries; ablation shows largest performance drops when key element extraction is removed, indicating its critical role; contrastive comparative loss helps the model focus on inter-document relationships.",
            "limitations_challenges": "Paper notes long-context limitations of standard LLMs and addresses them architecturally; acknowledges broader LLM issues such as potential factual hallucination (citing related work) and scalability/interpretability concerns; exact limitations in cross-paper contradiction handling and computational cost are discussed qualitatively but not exhaustively quantified.",
            "scaling_behavior": "Method is designed to handle long-context via chunking and memory updates; evaluated on a 1000-paper dataset, but the paper does not present a detailed scaling curve showing performance vs number of papers or model size; claims improved long-context handling due to memory mechanism but lacks fine-grained scaling analysis.",
            "uuid": "e4375.0",
            "source_info": {
                "paper_title": "Leveraging Large Language Models for Comparative Literature Summarization with Reflective Incremental Mechanisms",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "ChatCite (Li et al., 2024b)",
            "name_full": "ChatCite: LLM agent with human workflow guidance for comparative literature summary",
            "brief_description": "An LLM-based agent approach that mimics human workflows to organize and compare key aspects of multiple papers to generate comparative literature summaries (presented in related work and cited by the paper).",
            "citation_title": "ChatCite: LLM agent with human workflow guidance for comparative literature summary",
            "mention_or_use": "mention",
            "system_name": "ChatCite (prior work)",
            "system_description": "Described as an LLM agent that emulates human workflows (human-like guidance) to structure comparative summarization tasks, organizing and comparing key dimensions across papers; specifics of architecture are not given in this paper (referenced as prior work).",
            "llm_model_used": "Not specified in this paper (referenced work only).",
            "extraction_technique": "Reported as using human-workflow guidance to organize information extraction and comparison (details not specified in this paper).",
            "synthesis_technique": "Organizes and compares key aspects across multiple papers, implying agent-based stepwise synthesis; exact mechanism not detailed here.",
            "number_of_papers": "Not specified in this paper (referenced work).",
            "domain_or_topic": "Comparative literature summarization (general scientific literature; cited in context of academic papers).",
            "output_type": "Comparative literature summaries / organized comparisons across papers.",
            "evaluation_metrics": "Not specified in this paper (referenced work).",
            "performance_results": "Not reported in this paper (referenced work).",
            "comparison_baseline": "Not specified in this paper.",
            "performance_vs_baseline": "Not specified in this paper.",
            "key_findings": "Characterized as an important contribution in related work for mimicking human guidance in LLM agents to improve comparative summarization, but this paper does not reproduce or detail findings.",
            "limitations_challenges": "Not detailed here; cited as related work without full experimental detail in this paper.",
            "scaling_behavior": "Not discussed in this paper.",
            "uuid": "e4375.1",
            "source_info": {
                "paper_title": "Leveraging Large Language Models for Comparative Literature Summarization with Reflective Incremental Mechanisms",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "LitLLM",
            "name_full": "LitLLM: A toolkit for scientific literature review",
            "brief_description": "A toolkit designed to assist scientific literature review tasks by facilitating comparative summarization and helping researchers evaluate and synthesize multiple sources (cited in related work).",
            "citation_title": "Litllm: A toolkit for scientific literature review",
            "mention_or_use": "mention",
            "system_name": "LitLLM",
            "system_description": "Toolkit aimed at supporting literature review workflows; described as facilitating comparative summarization by helping researchers evaluate and synthesize multiple papers. The paper cites LitLLM as an existing tool but does not detail its internal mechanisms.",
            "llm_model_used": "Not specified in this paper.",
            "extraction_technique": "Not specified in this paper (toolkit likely includes retrieval and summarization utilities but details are not provided here).",
            "synthesis_technique": "Facilitates comparative summarization and synthesis across sources (implementation details not provided in this paper).",
            "number_of_papers": "Not specified in this paper.",
            "domain_or_topic": "Scientific literature review (general across domains).",
            "output_type": "Tools/outputs for literature reviews and comparative summaries.",
            "evaluation_metrics": "Not specified here.",
            "performance_results": "Not reported in this paper.",
            "comparison_baseline": "Not specified in this paper.",
            "performance_vs_baseline": "Not specified in this paper.",
            "key_findings": "Cited as a practical toolkit supporting literature review generation, indicating active interest in tooling around LLM-assisted literature synthesis.",
            "limitations_challenges": "Not described in this paper.",
            "scaling_behavior": "Not discussed in this paper.",
            "uuid": "e4375.2",
            "source_info": {
                "paper_title": "Leveraging Large Language Models for Comparative Literature Summarization with Reflective Incremental Mechanisms",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Retrieval-augmented methods (RAG / retrieval + long-context LLM)",
            "name_full": "Retrieval-augmented generation and retrieval-meets-long-context approaches",
            "brief_description": "Approaches that combine information retrieval (embedding-based or other retrieval) with large language models to focus LLM attention on the most relevant document sections, improving information extraction and generation from long documents or multi-document inputs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Retrieval-augmented generation / retrieval + long-context LLM",
            "system_description": "Hybrid approach that first retrieves relevant chunks or passages (often by embedding-based nearest neighbor search) from large collections of documents, then conditions an LLM on the retrieved context to extract, summarize, or synthesize information; when combined with long-context models this enables processing of long/multi-document inputs more efficiently.",
            "llm_model_used": "Not specified in this paper (discussed generally; related references include combinations with long-context LLMs).",
            "extraction_technique": "Embedding-based retrieval of relevant chunks/passages; retrieval of most relevant sections prior to LLM generation.",
            "synthesis_technique": "Generation conditioned on retrieved evidence (retrieval-augmented generation), enabling focused synthesis across multiple documents; can be used with chunking and memory mechanisms for longer contexts.",
            "number_of_papers": "Not concretely specified here; described as applicable to long documents and multi-document inputs (scales with available corpora and retrieval design).",
            "domain_or_topic": "General long-document and multi-document summarization tasks; referenced for long-context literature extraction.",
            "output_type": "Summaries, extracted evidence, and synthesis across documents (used to improve extraction and factual grounding).",
            "evaluation_metrics": "Not specified here in detail; referenced works evaluate retrieval-augmented approaches using standard summarization and factuality metrics.",
            "performance_results": "Not reported quantitatively in this paper (cited as an effective approach in related work).",
            "comparison_baseline": "Implied comparison to pure generation from LLM without retrieval; cited works evaluate improvements when retrieval is added.",
            "performance_vs_baseline": "Paper cites related literature claiming improved efficiency and accuracy when combining retrieval with long-context LLMs, but does not report quantitative comparisons itself.",
            "key_findings": "Combining retrieval with long-context LLMs helps models focus on relevant sections of long documents and improves extraction accuracy; cited as a promising direction for multi-document synthesis.",
            "limitations_challenges": "Implementation-dependent (retrieval quality, latency, and potential for omitted relevant passages); not exhaustively discussed in this paper.",
            "scaling_behavior": "Described as improving efficiency for long documents; exact scaling behaviour depends on retrieval system and LLM context handling — no detailed scaling curves provided here.",
            "uuid": "e4375.3",
            "source_info": {
                "paper_title": "Leveraging Large Language Models for Comparative Literature Summarization with Reflective Incremental Mechanisms",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain of Thought (CoT)",
            "brief_description": "A reasoning technique where the model is prompted or trained to produce intermediate reasoning steps (chains of thought) to improve complex multi-step outputs; used here as a baseline for comparative summarization.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Chain-of-Thought (CoT)",
            "system_description": "Method/approach that elicits intermediate reasoning steps from LLMs to improve multi-step reasoning and complex output generation; in this paper CoT is evaluated as a baseline approach for comparative summarization tasks.",
            "llm_model_used": "Not specified here beyond being a reasoning method applied with LLM baselines (e.g., GPT-style models).",
            "extraction_technique": "Implicit multi-step prompted reasoning to extract and combine evidence across input texts (not a formal extraction pipeline in this paper).",
            "synthesis_technique": "Intermediate step-based synthesis where the model generates stepwise reasoning before final summary output.",
            "number_of_papers": "Applied as a baseline to the same dataset (CompLit-LongContext); exact per-instance paper counts are not specified.",
            "domain_or_topic": "Comparative literature summarization (as a baseline method).",
            "output_type": "Comparative summaries generated via chain-of-thought prompting.",
            "evaluation_metrics": "ROUGE scores and G-Score used in the paper to evaluate CoT baseline.",
            "performance_results": "CoT baseline reported: ROUGE-1 = 0.37, ROUGE-2 = 0.14, ROUGE-L = 0.33, G-Score = 72 (from Table 1).",
            "comparison_baseline": "Compared against ChatCite, GPT-4.0, BART, T5.",
            "performance_vs_baseline": "Per reported results, CoT underperforms ChatCite and other baselines on ROUGE and G-Score (e.g., ChatCite ROUGE-1 0.50 vs CoT 0.37).",
            "key_findings": "CoT provides an interpretable multi-step reasoning baseline but is less effective than the proposed multi-stage fine-tuned, memory-equipped ChatCite on comparative summarization in this evaluation.",
            "limitations_challenges": "CoT alone may not sufficiently address long-context or multi-document aggregation needs and underperforms when compared to a tailored multi-stage fine-tuning plus memory approach.",
            "scaling_behavior": "Not analyzed in detail in this paper.",
            "uuid": "e4375.4",
            "source_info": {
                "paper_title": "Leveraging Large Language Models for Comparative Literature Summarization with Reflective Incremental Mechanisms",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "LongLoRA",
            "name_full": "LongLoRA: Efficient fine-tuning of long-context large language models",
            "brief_description": "A method for efficient fine-tuning of LLMs to handle longer sequences without large computational overhead; cited in the paper as relevant to enabling long-context LLM adaptations.",
            "citation_title": "Longlora: Efficient fine-tuning of long-context large language models",
            "mention_or_use": "mention",
            "system_name": "LongLoRA",
            "system_description": "Efficient fine-tuning technique tailored to adapt LLMs for longer sequence lengths with reduced computational cost (referenced as a method to address fine-tuning for long-context tasks).",
            "llm_model_used": "Not specified in this paper's description (referenced work likely applies to GPT-style or transformer LLMs).",
            "extraction_technique": "Not specifically an extraction technique; enables models to process longer contexts which supports extraction from long documents.",
            "synthesis_technique": "Enables synthesis across longer contexts by allowing the model to accept longer input sequences after fine-tuning; details not provided in this paper.",
            "number_of_papers": "Not specified in this paper.",
            "domain_or_topic": "Long-context LLM tasks (general).",
            "output_type": "Supports generation/synthesis from longer contexts (general).",
            "evaluation_metrics": "Not specified here (referenced work contains evaluations).",
            "performance_results": "Not reported in this paper (referenced work).",
            "comparison_baseline": "Not specified in this paper.",
            "performance_vs_baseline": "Not specified in this paper.",
            "key_findings": "Cited as an important method for efficient long-context fine-tuning relevant to multi-document summarization tasks.",
            "limitations_challenges": "Not discussed in this paper.",
            "scaling_behavior": "Intended to improve scaling to longer sequence lengths with lower overhead (details are in the cited LongLoRA work, not this paper).",
            "uuid": "e4375.5",
            "source_info": {
                "paper_title": "Leveraging Large Language Models for Comparative Literature Summarization with Reflective Incremental Mechanisms",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ChatCite: LLM agent with human workflow guidance for comparative literature summary",
            "rating": 2,
            "sanitized_title": "chatcite_llm_agent_with_human_workflow_guidance_for_comparative_literature_summary"
        },
        {
            "paper_title": "Litllm: A toolkit for scientific literature review",
            "rating": 2,
            "sanitized_title": "litllm_a_toolkit_for_scientific_literature_review"
        },
        {
            "paper_title": "Retrieval meets long context large language models",
            "rating": 2,
            "sanitized_title": "retrieval_meets_long_context_large_language_models"
        },
        {
            "paper_title": "Longlora: Efficient fine-tuning of long-context large language models",
            "rating": 2,
            "sanitized_title": "longlora_efficient_finetuning_of_longcontext_large_language_models"
        },
        {
            "paper_title": "LooGLE: Can long-context language models understand long contexts?",
            "rating": 1,
            "sanitized_title": "loogle_can_longcontext_language_models_understand_long_contexts"
        },
        {
            "paper_title": "Lost in the middle: How language models use long contexts",
            "rating": 1,
            "sanitized_title": "lost_in_the_middle_how_language_models_use_long_contexts"
        },
        {
            "paper_title": "Zero-shot NLG evaluation through pairwise comparisons using large language models",
            "rating": 1,
            "sanitized_title": "zeroshot_nlg_evaluation_through_pairwise_comparisons_using_large_language_models"
        }
    ],
    "cost": 0.013974,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Leveraging Large Language Models for Comparative Literature Summarization with Reflective Incremental Mechanisms
3 Dec 2024</p>
<p>Gabriela Fernando 
Autonomous University of Nuevo León</p>
<p>Spencer García 
Autonomous University of Nuevo León</p>
<p>Harrison Burns 
Autonomous University of Nuevo León</p>
<p>Fuller 
Autonomous University of Nuevo León</p>
<p>Leveraging Large Language Models for Comparative Literature Summarization with Reflective Incremental Mechanisms
3 Dec 2024E819EC33610BF942507E007DE98DB1CFarXiv:2412.02149v1[cs.CL]
In this paper, we introduce ChatCite, a novel method leveraging large language models (LLMs) for generating comparative literature summaries.The ability to summarize research papers with a focus on key comparisons between studies is an essential task in academic research.Existing summarization models, while effective at generating concise summaries, fail to provide deep comparative insights.ChatCite addresses this limitation by incorporating a multi-step reasoning mechanism that extracts critical elements from papers, incrementally builds a comparative summary, and refines the output through a reflective memory process.We evaluate ChatCite on a custom dataset, CompLit-LongContext, consisting of 1000 research papers with annotated comparative summaries.Experimental results show that ChatCite outperforms several baseline methods, including GPT-4, BART, T5, and CoT, across various automatic evaluation metrics such as ROUGE and the newly proposed G-Score.Human evaluation further confirms that ChatCite generates more coherent, insightful, and fluent summaries compared to these baseline models.Our method provides a significant advancement in automatic literature review generation, offering researchers a powerful tool for efficiently comparing and synthesizing scientific research.</p>
<p>Introduction</p>
<p>The exponential growth of scientific literature has made the automatic generation of literature reviews an imperative in academic research.A Literature Review is pivotal for grasping the state-of-the-art in any field, aiding researchers in synthesizing previous work, pinpointing research gaps, and positioning their contributions appropriately.Crafting such reviews manually is not only time-consuming but also requires expertise in comparing and contrasting various papers, methodologies, and outcomes.While traditional text summarization methods have been adept at condensing individual research papers, creating comparative summaries that juxtapose multiple works presents a distinct challenge.This task is further complicated when the papers in question contain extensive and complex contextual details, which traditional Large Language Models (LLMs) like GPT-3.5 or GPT-4 often find difficult to process efficiently, particularly when dealing with long documents or maintaining coherence across multiple works [Beltagy et al., 2020, Zaheer et al., 2020, Zhou et al., 2023a].</p>
<p>The primary challenge in this domain is the long-context problem: many state-of-the-art LLMs encounter limitations when handling long documents or multi-document inputs that necessitate synthesizing information across various sources.Moreover, most existing models are predominantly trained on individual document summaries, which hinders their ability to generate comparisons requiring deeper analysis and contextual synthesis across multiple papers [Beasley andManda, 2018, Zhou et al., 2024a].In addition to these technical hurdles, there is a scarcity of effective comparative learning frameworks in the training process of LLMs, which are essential for developing models capable of not only summarizing but also critically comparing research findings [Alomari et al., 2022].Our work aims to bridge these gaps by proposing a method that tailors LLMs for long-context, comparative literature summary generation, training the model to produce not only individual paper summaries but also integrated, comparative insights between multiple research papers, effectively synthesizing common themes, contrasting results, and identifying research trends.</p>
<p>In this paper, we propose an innovative approach for training LLMs to tackle these challenges.Our method includes a multi-stage fine-tuning pipeline designed to manage both long-context documents and the intricate task of comparative summarization.We begin by pre-training a base model on a broad corpus of academic papers to capture a comprehensive understanding of research structures, methodologies, and topics.We then conduct comparative fine-tuning by introducing a new dataset specifically curated for generating comparative summaries, which includes pairs or groups of papers annotated with insights that underscore comparisons, strengths, weaknesses, and gaps.Finally, we implement a long-context memory mechanism, enabling the model to process extensive documents and retain key contextual information across multiple sections.This framework facilitates the generation of coherent and structured summaries that not only summarize individual papers but also highlight comparative insights, making it particularly beneficial for academic literature reviews.</p>
<p>Our experimental evaluation utilizes a custom dataset, CiteComp-1000, comprising 1000 academic papers from the computer science domain.Each paper in the dataset includes the original paper, its related works section, and a list of references.We employ a suite of evaluation metrics to assess the quality of the generated summaries, including ROUGE scores, which measure the overlap between generated and reference summaries, as well as a novel Comparative Quality Score (CQS) that we introduce to evaluate how well the model captures comparative insights across papers.Our results demonstrate that the proposed method significantly outperforms existing baseline models (such as GPT-4.0 and other comparative summarization methods) in terms of both ROUGE metrics and comparative quality, proving our approach to be highly effective at generating high-quality, comparative literature reviews.</p>
<p>In summary, our contributions are as follows:</p>
<p>• We propose a novel multi-stage fine-tuning framework that specifically targets longcontext, comparative literature summary generation with Large Language Models.</p>
<p>• We introduce a new dataset, CiteComp-1000, tailored to the task of generating comparative summaries from academic literature, and a new evaluation metric, Comparative Quality Score (CQS), to assess the model's ability to synthesize and compare research insights.</p>
<p>• Our experimental results show that our method outperforms existing baselines in terms of both ROUGE and comparative qual-ity, making it a valuable tool for automatic literature review generation.</p>
<p>2 Related Work</p>
<p>Large Language Models for Long-Context</p>
<p>The ability of large language models (LLMs) to handle long-context information has become an increasingly important area of research.Early models were limited by their fixed context window, often leading to poor performance when handling longer texts.Recently, several studies have focused on improving the capacity of LLMs to process and understand long contexts, which is crucial for tasks such as document summarization, question answering, and literature review generation.[Liu et al., 2024] investigates how language models use long contexts and highlights the challenges models face when processing long documents, particularly when relevant information appears in the middle of the text.[Li et al., 2024a] introduces LooGLE, a benchmark designed to evaluate the long-context understanding capabilities of LLMs.This work provides valuable insights into the limits of current LLM architectures when confronted with extended context windows.</p>
<p>To address the challenge of fine-tuning large models on long-context data, [Chen et al., 2024] presents LongLoRA, a method for efficiently finetuning LLMs to handle longer sequences without significant computational overhead.This work is essential for improving the scalability of LLMs for long-context tasks.Similarly, [Dong et al., 2024] introduces BAMBOO, a comprehensive benchmark for evaluating long text modeling capacities of LLMs.BAMBOO provides a multi-task, bilingual evaluation framework that assesses models across a variety of long-context tasks.</p>
<p>In the domain of content reduction and information processing, [Ji et al., 2023] explores methods for content reduction, surprisal, and information density estimation in long documents, offering a deeper understanding of how LLMs interact with lengthy texts.Furthermore, [Xu et al., 2024, Zhou et al., 2024b] investigates how retrieval-augmented methods can be combined with long-context LLMs to improve the efficiency and accuracy of information extraction from long documents, allowing models to focus on the most relevant sections.</p>
<p>Lastly, the LooGLE benchmark [Li et al., 2024a] provides a more robust evaluation of long-context LLMs, assessing their ability to retain and utilize context over long passages of text.This expansion of evaluation frameworks is crucial to advancing the development of models capable of handling long-context information effectively.</p>
<p>Comparative Literature Summary</p>
<p>The generation of comparative literature summaries has garnered increasing attention in recent years, particularly with the pretrained language models (PLMs [Zhou et al., 2022a,b]).These models have demonstrated impressive performance in a variety of text generation tasks, but generating high-quality comparative summaries remains a challenging task due to the need for both linguistic fluency and deep comparative analysis [Zhou et al., 2023b[Zhou et al., , 2024c]].</p>
<p>Several recent studies have addressed the task of automatic comparative summarization in the context of academic literature.For instance, [Li et al., 2024b] presents ChatCite, a novel LLM-based agent that mimics human workflows to generate comparative literature summaries.By integrating human-like guidance, ChatCite effectively organizes and compares key aspects of multiple papers, making it an important contribution to the field.[Agarwal et al., 2024] introduces LitLLM, a toolkit designed for scientific literature reviews, which facilitates comparative summarization by assisting researchers in evaluating and synthesizing multiple sources efficiently.</p>
<p>Benchmarking large language models for specific tasks, such as summarization, has also been an area of active research.[Zhang et al., 2024] explores the performance of LLMs in news summarization, providing a valuable evaluation framework that could be extended to academic papers for comparative analysis.Furthermore, [Liusie et al., 2024] presents a framework for zero-shot natural language generation (NLG) evaluation through pairwise comparisons, offering insights into how LLMs can be leveraged to compare different summaries or versions of the same content.</p>
<p>In addition to these advances, techniques like retrieval-augmented generation have been explored in related domains.For example, [Liu et al., 2021, Zhou et al., 2021a,b] demonstrates how hybrid methods combining retrieval and generation can be applied to code summarization, a methodology that can be transferred to the task of generating comparative summaries of academic literature.Moreover, the issue of factual hallucination in LLMs, which could affect the accuracy of comparative summaries, is examined by [Li et al., 2024c], emphasizing the importance of controlling for factual accuracy when using LLMs for literature review tasks.</p>
<p>Method</p>
<p>In this section, we describe the proposed method for long-context comparative literature summary generation using large language models (LLMs).Our approach is primarily generative in nature, as it aims to produce comparative summaries of multiple academic papers by synthesizing relevant content from these papers and generating coherent text.The model is trained to generate not only individual summaries but also comparative insights that connect different studies.</p>
<p>Model Type: Generative Model</p>
<p>Our method is based on a generative architecture, which allows the model to learn to produce novel content, specifically comparative literature summaries, from multiple input papers.Generative models, such as GPT-based architectures, are wellsuited for this task since they can learn to produce fluent, contextually relevant text.Given the complex nature of literature review generation, which requires the synthesis of information from multiple sources, a generative model can integrate insights across documents and produce output that highlights relationships between these works.</p>
<p>Let the input to the model consist of a set of research papers D = {D 1 , D 2 , . . ., D n }, where each D i represents an academic paper.The goal is to produce a comparative summary S, which is a coherent text that captures both individual insights and comparative analysis across the documents.</p>
<p>We define the conditional distribution of the output summary S given the set of input papers D as:
P (S | D) = |S| t=1 P (s t | s 1 , . . . , s t−1 , D) (1)
where s t represents the token at position t in the summary S, and D represents the concatenated or processed context of the input papers.</p>
<p>Pre-training Stage</p>
<p>In the pre-training stage, we start by fine-tuning a base language model (such as GPT-4.0) on a large corpus of academic papers.This stage is designed to teach the model the general structure and language used in academic writing, as well as the different methodologies, findings, and research trends that appear across disciplines.</p>
<p>During this stage, the model learns to predict the next token given a sequence of previous tokens from academic papers.The objective function for pre-training is the traditional maximum likelihood estimation (MLE) loss:
L pretrain = − T t=1 log P (s t | s 1 , . . . , s t−1 ) (2)
where T is the length of the sequence, and s t is the token at position t.</p>
<p>Comparative Fine-tuning Stage</p>
<p>Once the model has learned the general language structure, the next step involves comparative finetuning.In this stage, the model is trained specifically to generate comparative summaries that highlight relationships between different papers.For each training instance, we provide a set of papers D = {D 1 , D 2 , . . ., D n } along with a reference comparative summary S ref that includes not only individual summaries of each paper but also the key comparative insights (e.g., "Paper 1 outperforms Paper 2 in terms of accuracy, but Paper 2 uses a more efficient approach").</p>
<p>The training objective during this phase combines two parts: a standard token prediction loss for generating coherent summaries, and a comparative loss that encourages the model to generate outputs that focus on comparative relationships.</p>
<p>The total loss function for this stage can be expressed as:
L comparative = L generation + λL comparison (3)
where: -L generation is the cross-entropy loss for generating the summary as in the pre-training stage:
L generation = − |S| t=1 log P (s t | s 1 , . . . , s t−1 , D) (4)
-L comparison is the comparative loss, which encourages the model to explicitly learn relationships between documents.One possible approach to this loss is based on contrastive learning.Let C i denote the comparative insight for document D i , and let S ref be the reference comparative summary that contains these insights.The contrastive loss can be formulated as:
L comparison = − n i=1 log exp(sim(C i , S ref )) n j=1 exp(sim(C j , S ref ))(5)
where sim(C i , S ref ) is a similarity function (e.g., cosine similarity) between the comparative insight C i for paper D i and the reference summary S ref .</p>
<p>Long-context Memory Mechanism</p>
<p>One of the core innovations of our method is the long-context memory mechanism, which is necessary for handling the extended context of multiple papers.This mechanism ensures that the model can retain important contextual information from earlier parts of the input documents and generate coherent summaries even when the documents exceed the typical token limits of models like GPT-4.</p>
<p>We implement this mechanism by dividing the input documents into chunks of manageable size C 1 , C 2 , . . ., C m , where each chunk contains a portion of the original documents.The output from each chunk is passed through an attention mechanism that captures dependencies between different chunks.The model's hidden states are then updated using a memory update rule that retains information from all previous chunks.</p>
<p>Let
h (i)
t represent the hidden state of the model at time t in chunk i.The memory update rule is defined as:
h (i) t = GRU(h (i) t−1 , x (i) t + Mem t ) (6)
where
x (i)
t is the input token at time t in chunk i, and Mem t is the memory from previous chunks, which is updated as:
Mem t = Attn(Mem t−1 , h (i−1) t−1 )(7)
This memory mechanism allows the model to consider long-context dependencies between different chunks of the document, ensuring that the generated summary remains coherent and comprehensive across the entire set of papers.</p>
<p>Overall Training Strategy</p>
<p>The overall training strategy involves first pretraining the model on a large corpus of academic papers to learn language and domain knowledge.This is followed by the comparative fine-tuning stage, where the model learns to focus on comparative relationships between papers.The model is then equipped with a long-context memory mechanism to handle large documents.The final loss function combines both generation and comparative losses, ensuring that the model can produce both fluent and insightful comparative summaries.
L total = L pretrain + L comparative (8)
This comprehensive training approach ensures that the model not only generates accurate individual summaries but also excels at synthesizing and comparing the research presented in multiple documents.</p>
<p>Experiments</p>
<p>In this section, we present the experimental setup, including details about the dataset, the comparison with several baseline methods, and the evaluation metrics.We also provide an analysis of the experimental results and human evaluation, demonstrating the effectiveness of our proposed method.</p>
<p>Dataset</p>
<p>For the experiments, we created a new dataset, CompLit-LongContext, specifically designed for the task of comparative literature summary generation.The dataset consists of 1000 research papers from various domains within computer science, such as machine learning, natural language processing, and computer vision.Each paper is annotated with a reference comparative summary that includes both individual summaries of the papers as well as comparisons between them.</p>
<p>The dataset was constructed by collecting papers from publicly available repositories such as ArXiv and Google Scholar.The summaries were manually crafted by domain experts to include relevant comparisons, such as strengths and weaknesses of the methods, experimental setups, and performance metrics.The dataset contains:</p>
<p>• 1000 papers spanning multiple subdomains of computer science.</p>
<p>• For each paper, a comparative summary is provided that highlights key points as well as comparisons to related work.</p>
<p>• The dataset is split into 800 training examples, 100 validation examples, and 100 test examples.</p>
<p>Experimental Setup</p>
<p>We compare our method, ChatCite, with the following baseline methods:</p>
<p>• GPT-4.0:A standard large language model used as a baseline for text generation.</p>
<p>• BART: A model that has shown strong performance on summarization tasks, used here as another baseline for comparative summarization.</p>
<p>• T5: Another transformer-based model for text generation, adapted for the comparative summarization task.</p>
<p>• CoT (Chain of Thought): A method based on reasoning via intermediate steps, tested for comparison with our approach.</p>
<p>We evaluate the models using both automatic metrics and human evaluation.The automatic evaluation uses the following metrics: -ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-L) to evaluate the overlap of n-grams and sentence-level structure between the generated summaries and reference summaries.-G-Score: A novel metric introduced in our work to evaluate the quality of comparative analysis in generated summaries, where higher scores indicate more relevant and detailed comparisons.</p>
<p>The human evaluation is performed by three domain experts who assess the summaries based on the following criteria:</p>
<p>• Coherence: Does the summary logically integrate content from all relevant papers?</p>
<p>• Comparative Insight: Does the summary highlight and compare the strengths and weaknesses of the papers?</p>
<p>• Fluency: Is the generated summary fluent and grammatically correct?</p>
<p>The human evaluation results are presented in a separate table, which shows the preference of human evaluators for the different methods.We present the results of the automatic evaluation of our compared to the baselines.Table 1 summarizes the performance of all models across various evaluation metrics.</p>
<p>From the results, we observe that ChatCite outperforms all baseline methods in terms of ROUGE-1, ROUGE-2, and ROUGE-L, with significant improvements in G-Score, indicating that our model not only generates more fluent and coherent summaries but also provides better comparative analysis.</p>
<p>Ablation Study</p>
<p>To further analyze the effectiveness of the components of ChatCite, we perform an ablation study.We remove certain components of the model and evaluate the performance without those components.The results of this study are summarized in Table 2.</p>
<p>The results demonstrate that removing any of the components-key element extraction, the comparative incremental mechanism, or the reflective memory mechanism-degrades the model's performance, with the largest drop occurring when key element extraction is removed.</p>
<p>Human Evaluation</p>
<p>For human evaluation, we recruited three domain experts to assess the summaries generated by ChatCite and the baselines.The experts were asked to rate each summary based on three criteria: coherence, comparative insight, and fluency.Each criterion was rated on a scale from 1 to 5, with 5 being the best score.</p>
<p>The results of the human evaluation are shown in Table 3.The human evaluation results indicate that ChatCite consistently outperforms the baselines across all evaluation criteria.Our model provides significantly better comparative insights, maintains higher coherence, and produces more fluent summaries.</p>
<p>Analysis and Discussion</p>
<p>The experimental results demonstrate the superiority of ChatCite over the baseline methods.In particular, the use of a generative model with a long-context memory mechanism allows our model to capture both fine-grained details from individual papers and high-level comparisons across papers.The G-Score metric, which emphasizes the quality of comparative analysis, shows that ChatCite excels at drawing insightful comparisons between research papers.</p>
<p>Additionally, the ablation study confirms the importance of the key elements in the proposed model.The comparative incremental mechanism and reflective memory mechanism, which allow for the retention of long-context information, contribute significantly to the model's overall performance.</p>
<p>Human evaluation further validates the quality of the summaries generated by ChatCite, as it outperforms the baselines in terms of coherence, fluency, and comparative insight.This supports the claim that ChatCite not only generates fluent summaries but also excels at capturing and presenting meaningful comparative insights across multiple papers.</p>
<p>Conclusion</p>
<p>In this work, we have presented ChatCite, a novel approach for generating comparative literature summaries using large language models.Unlike traditional summarization methods, which primarily focus on summarizing individual papers, ChatCite integrates a multi-step reasoning mechanism that allows it to compare multiple research papers simultaneously, providing meaningful insights into the relationships and differences between studies.This method was evaluated on our custombuilt dataset, CompLit-LongContext, consisting of 1000 research papers annotated with comparative summaries.The experimental results demonstrate that ChatCite outperforms several state-ofthe-art models, such as GPT-4, BART, T5, and CoT, in both automatic and human evaluation metrics.</p>
<p>Our findings highlight the effectiveness of ChatCite in not only generating fluent and coherent summaries but also in producing valuable comparative insights that are often overlooked by other models.The ablation study further underscores the importance of key components, such as key element extraction and the reflective memory mechanism, in ensuring the success of our method.</p>
<p>In future work, we plan to explore the application of ChatCite to other domains of research and further improve its scalability and generalization.We also aim to investigate how to make the model more interpretable, providing researchers with a better understanding of how ChatCite draws its comparative insights.We believe that our method will be a useful tool for researchers, enabling them to generate high-quality literature reviews efficiently and effectively.</p>
<p>Table 1 :
1
Automatic Evaluation Results
ModelROUGE-1 ROUGE-2 ROUGE-L G-ScoreGPT-4.0 (Zero-Shot)0.450.200.4085BART0.420.180.3880T50.400.160.3675CoT0.370.140.3372ChatCite (Ours)0.500.250.4592</p>
<p>Table 2 :
2
Ablation Study Results
Model VariantROUGE-1 ROUGE-2 ROUGE-L G-ScoreChatCite (Full)0.500.250.4592Without Key Element Extraction0.470.220.4287Without Comparative Incremental Mechanism0.480.240.4490Without Reflective Memory Mechanism0.490.240.4491</p>
<p>Table 3 :
3
Human Evaluation Results
ModelCoherence Comparative Insight FluencyGPT-4.0 (Zero-Shot)4.23.84.5BART4.03.54.2T53.83.23.9CoT3.73.03.8ChatCite (Ours)4.74.54.84.3 Automatic Evaluation Results</p>
<p>Longformer: The long-document transformer. Iz Beltagy, Matthew E Peters, Arman Cohan, CoRR, abs/2004.051502020</p>
<p>Big bird: Transformers for longer sequences. Manzil Zaheer, Guru Guruganesh, Avinava Kumar, Joshua Dubey, Chris Ainslie, Santiago Alberti, Philip Ontanon, Anirudh Pham, Qifan Ravula, Li Wang, Yang, Advances in neural information processing systems. 202033</p>
<p>Yucheng Zhou, Xiubo Geng, Tao Shen, Chongyang Tao, Guodong Long, Jian-Guang Lou, Jianbing Shen, arXiv:2311.08734Thread of thought unraveling chaotic contexts. 2023aarXiv preprint</p>
<p>Comparison of natural language processing tools for automatic gene ontology annotation of scientific literature. Lucas Beasley, Prashanti Manda, PeerJ Preprints. 2018Technical report</p>
<p>Rethinking visual dependency in long-context reasoning for large vision-language models. Yucheng Zhou, Zhi Rao, Jun Wan, Jianbing Shen, arXiv:2410.197322024aarXiv preprint</p>
<p>Aznul Qalid Md Sabri, and Izzat Alsmadi. Deep reinforcement and transfer learning for abstractive text summarization: A review. Ayham Alomari, Norisma Idris, Computer Speech &amp; Language. 711012762022</p>
<p>Lost in the middle: How language models use long contexts. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang, 10.1162/tacl_a_00638Trans. Assoc. Comput. Linguistics. 122024</p>
<p>Loogle: Can long-context language models understand long contexts?. Jiaqi Li, Mengmeng Wang, Zilong Zheng, Muhan Zhang, 10.18653/v1/2024.acl-long.859Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAugust 11-16, 2024. 2024a1ACL 2024</p>
<p>Longlora: Efficient fine-tuning of long-context large language models. Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, Jiaya Jia, ICLR 2024The Twelfth International Conference on Learning Representations. Vienna, AustriaMay 7-11, 2024. 2024OpenReview.net</p>
<p>BAMBOO: A comprehensive benchmark for evaluating long text modeling capacities of large language models. Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024. Nicoletta Calzolari, Min-Yen Kan, Véronique Hoste, Alessandro Lenci, Sakriani Sakti, Nianwen Xue, the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024Torino, ItalyELRA and ICCL20-25 May, 2024. 2024</p>
<p>Content reduction, surprisal and information density estimation for long documents. Shaoxiong Ji, Wei Sun, Pekka Marttinen, 10.48550/arXiv.2309.060092023</p>
<p>Retrieval meets long context large language models. Peng Xu, Wei Ping, Xianchao Wu, Lawrence Mcafee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, Bryan Catanzaro, The Twelfth International Conference on Learning Representations, ICLR 2024. Vienna, AustriaMay 7-11, 2024. 2024OpenReview.net</p>
<p>Visual in-context learning for large visionlanguage models. Yucheng Zhou, Xiang Li, Qianning Wang, Jianbing Shen, Findings of the Association for Computational Linguistics, ACL 2024. Bangkok, ThailandAssociation for Computational LinguisticsAugust 11-16, 2024. 2024band virtual meeting</p>
<p>Claret: Pre-training a correlationaware context-to-event transformer for event-centric generation and classification. Yucheng Zhou, Tao Shen, Xiubo Geng, Guodong Long, Daxin Jiang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics2022a1</p>
<p>Eventbert: A pre-trained model for event correlation reasoning. Yucheng Zhou, Xiubo Geng, Tao Shen, Guodong Long, Daxin Jiang, Proceedings of the ACM Web Conference 2022. the ACM Web Conference 20222022b</p>
<p>Towards robust ranker for text retrieval. Yucheng Zhou, Tao Shen, Xiubo Geng, Chongyang Tao, Can Xu, Guodong Long, Binxing Jiao, Daxin Jiang, Findings of the Association for Computational Linguistics: ACL 2023. 2023b</p>
<p>Fine-grained distillation for long document retrieval. Yucheng Zhou, Tao Shen, Xiubo Geng, Chongyang Tao, Jianbing Shen, Guodong Long, Can Xu, Daxin Jiang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence19732-19740, 2024c38</p>
<p>Chatcite: LLM agent with human workflow guidance for comparative literature summary. Yutong Li, Lu Chen, Aiwei Liu, Kai Yu, Lijie Wen, 10.48550/arXiv.2403.025742024b</p>
<p>Litllm: A toolkit for scientific literature review. Shubham Agarwal, Issam H Laradji, Laurent Charlin, Christopher Pal, 10.48550/arXiv.2402.017882024</p>
<p>Benchmarking large language models for news summarization. Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen R Mckeown, Tatsunori B Hashimoto, 10.1162/tacl_a_00632Trans. Assoc. Comput. Linguistics. 122024</p>
<p>Zero-shot NLG evaluation through pairwise comparisons using large language models. Adian Liusie, Potsawee Manakul, Mark J F Gales, Proceedings of the 18th Conference of the European Chapter. Long Papers. Yvette Graham, Matthew Purver, the 18th Conference of the European ChapterSt. Julian's, MaltaAssociation for Computational LinguisticsMarch 17-22, 2024. 20241EACL 2024</p>
<p>Retrieval-augmented generation for code summarization via hybrid GNN. Shangqing Liu, Yu Chen, Xiaofei Xie, Jing , Kai Siow, Yang Liu, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. AustriaMay 3-7, 2021. OpenReview.net, 2021</p>
<p>Improving zero-shot crosslingual transfer for multilingual question answering over knowledge graph. Yucheng Zhou, Xiubo Geng, Tao Shen, Wenqiang Zhang, Daxin Jiang, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterHuman Language Technologies2021a</p>
<p>Modeling event-pair relations in external knowledge graphs for script reasoning. Yucheng Zhou, Xiubo Geng, Tao Shen, Jian Pei, Wenqiang Zhang, Daxin Jiang, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021b</p>
<p>The dawn after the dark: An empirical study on factuality hallucination in large language models. Junyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, Ji-Rong Wen, 10.18653/v1/2024.acl-long.586Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAugust 11-16, 2024. 2024cACL 2024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6085 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6085</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6085</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-120.html">extraction-schema-120</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-76b99439d524ae1813c30edc4bcad487a30a1f8c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/76b99439d524ae1813c30edc4bcad487a30a1f8c" target="_blank">UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This paper explores targeted distillation with mission-focused instruction tuning to train student models that can excel in a broad application class such as open information extraction, and shows how ChatGPT can be distilled into much smaller UniversalNER models for open NER.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have demonstrated remarkable generalizability, such as understanding arbitrary entities and relations. Instruction tuning has proven effective for distilling LLMs into more cost-efficient models such as Alpaca and Vicuna. Yet such student models still trail the original LLMs by large margins in downstream applications. In this paper, we explore targeted distillation with mission-focused instruction tuning to train student models that can excel in a broad application class such as open information extraction. Using named entity recognition (NER) for case study, we show how ChatGPT can be distilled into much smaller UniversalNER models for open NER. For evaluation, we assemble the largest NER benchmark to date, comprising 43 datasets across 9 diverse domains such as biomedicine, programming, social media, law, finance. Without using any direct supervision, UniversalNER attains remarkable NER accuracy across tens of thousands of entity types, outperforming general instruction-tuned models such as Alpaca and Vicuna by over 30 absolute F1 points in average. With a tiny fraction of parameters, UniversalNER not only acquires ChatGPT's capability in recognizing arbitrary entity types, but also outperforms its NER accuracy by 7-9 absolute F1 points in average. Remarkably, UniversalNER even outperforms by a large margin state-of-the-art multi-task instruction-tuned systems such as InstructUIE, which uses supervised NER examples. We also conduct thorough ablation studies to assess the impact of various components in our distillation approach. We release the distillation recipe, data, and UniversalNER models to facilitate future research on targeted distillation.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6085.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6085.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UniNER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UniversalNER (UniNER)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of small instruction-tuned LLaMA models distilled from ChatGPT to perform open-domain named entity recognition across tens of thousands of entity types and many domains via mission-focused instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>UniversalNER (UniNER)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Targeted distillation system: sample diverse inputs from a large corpus (Pile), use an LLM (ChatGPT) to label entity mentions and types, then instruction-tune smaller LLaMA models in a conversation-style format to output structured JSON lists of entities per queried entity type.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Teacher: ChatGPT (gpt-3.5-turbo-0301) for annotation; Students: LLaMA-based models (LLaMA 7B and 13B; LLaMA2 also tried but no significant diff).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Input passages: 50K passages sampled from the Pile corpus (articles chunked to max 256 tokens). ChatGPT produced annotations for these passages; constructed dataset after filtering: 45,889 input-output pairs, 240,725 entities across 13,020 distinct entity types (also a variant with 353,092 entity types when using definition-based generation). Evaluation inputs: Universal NER benchmark of 43 NER datasets across 9 domains (sampled up to 200K passage-query pairs per dataset for eval).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Mission-focused instruction tuning: (1) sample diverse inputs from raw corpora (Pile), (2) use teacher LLM to generate output annotations (entity mentions + types or definitions), (3) convert each entity type into a natural language query ("What describes <type>?"), (4) fine-tune student LLaMA models to generate structured JSON lists of entities per query, using language modeling objective; augment with negative sampling (sample entity types not present in passage and expect empty JSON list), and optionally supervised fine-tuning with dataset-specific instruction templates.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured JSON lists of entity mentions per queried entity type (one JSON list per type query). Also variants where all types are requested in one response or where types are defined by short sentences (definition-based).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Strict entity-level micro-F1 requiring exact match of both entity type and boundary; also an alternative partial-match evaluation where any word-overlap counts as 0.5 true positive. Evaluated zero-shot and supervised/fine-tuned settings across the Universal NER benchmark (43 datasets, 9 domains).</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Zero-shot average F1: UniNER-7B = 41.7%, UniNER-13B = 43.4%, compared to ChatGPT = 34.9% (average). UniNER variants: 'all-in-one' and 'definition' underperform the main UniNER-type by 3.3% and 11.8% respectively. With supervised continual fine-tuning, UniNER-7B reached average F1 up to 60.0% (out-of-domain) and 84.78% average F1 on 20 supervised datasets (in-domain), outperforming InstructUIE-11B and BERT-base in those settings.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Training inputs: Pile corpus (22 subdatasets). Evaluation benchmark: Universal NER benchmark assembled in this paper (43 NER datasets across general, biomedical, clinical, STEM, programming, social, law, finance, transportation domains). Comparisons use ChatGPT (gpt-3.5-turbo-0301), Vicuna, and InstructUIE (Flan-T5-11B).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Label definition inconsistencies across datasets (causing conflicts in multi-dataset supervised training); sensitivity to entity-type paraphrasing for the type-label variant (improved with definition-based data but that harms standard benchmark alignment); open-world training data mismatch with closed-world evaluation (addressed by negative sampling); heavy-tailed long-tail entity distribution and potential noise in LLM-generated annotations; attention dispersion when asking all types in one query (reduces per-type accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>UniNER substantially outperforms general instruction-tuned models (Alpaca, Vicuna) on zero-shot NER by large margins (e.g., >30 absolute F1 over Vicuna in average on the Universal NER benchmark), and exceeds ChatGPT's average NER F1 by ~7-9 absolute points. UniNER also outperforms InstructUIE (a supervised multi-task instruction-tuned system based on Flan-T5-11B) despite the latter using supervised NER examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6085.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6085.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mission-Focused Instruction Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mission-Focused Instruction Tuning (targeted distillation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A distillation strategy that narrows instruction tuning to a specific broad application class (e.g., open information extraction) by maximizing diversity of inputs and task labels instead of generic instruction diversity, to better transfer LLM capabilities to small students for that mission.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Mission-Focused Instruction Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Instruction-tuning recipe tailored to a single application class: sample diverse real inputs from large corpora, have an LLM produce task-specific outputs, structure prompts as queries per task label (e.g., entity type), and fine-tune a smaller LM on these paired (input, query) -> (structured output) examples. Include negative sampling and dataset-specific prompt fields when supervised data are available.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Teacher LLM for annotation: ChatGPT (gpt-3.5-turbo-0301) used in this work; student LMs: LLaMA 7B and 13B.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Diverse real-world text sampled from Pile (50K 256-token passages in this study) rather than LM-generated inputs; labels are entity types (thousands) or short sentence definitions (in the definition variant).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Generate supervision with teacher LLM on diverse real inputs, reframe each label as a natural-language query, fine-tune student model to answer queries with structured outputs (JSON lists), use negative sampling to present label queries not present in text, and harmonize supervised label differences via dataset-specific prompt fields.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured model responses per query (JSON lists of entities), making the student model a queryable extractor for arbitrary labels in the mission class.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Evaluate downstream task performance on a comprehensive benchmark for the mission (here: Universal NER benchmark) using strict entity-level micro-F1 and partial-match F1, plus ablations on components (negative sampling strategies, prompt templates, data variants).</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Demonstrated that mission-focused tuning yields much stronger zero-shot performance for NER than general instruction-tuned models; negative sampling (frequency-based) significantly improves results; definition-based label generation increases robustness to label paraphrase but reduces alignment to standard benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Pile for input sampling; Universal NER benchmark (43 datasets) for evaluation; supervised datasets for fine-tuning include many standard NER sets (ACE, OntoNotes, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Requires careful design to handle label definition mismatches across datasets; trade-offs between robustness to paraphrases (definitions) and alignment with standard label formats; generation noise from teacher LLM; open-world vs closed-world mismatch that must be mitigated with negative sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Contrasted with generic instruction tuning (Alpaca, Vicuna) which aim for breadth across tasks and with supervised multi-task instruction tuning (InstructUIE). Mission-focused tuning gave much better performance for the targeted application (NER) than generic instruction-tuned models and even surpassed InstructUIE despite the latter using human-labelled supervised data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6085.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6085.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (as teacher)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (teacher annotations via gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deployed LLM used as a label generator/teacher in this work: ChatGPT (gpt-3.5-turbo-0301) produced entity mentions and types (or definitions) for sampled passages from Pile, with temperature=0 to ensure determinism.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatGPT (annotation teacher)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Teacher LLM used to create large-scale instruction-style NER annotations on diverse unlabeled passages sampled from the Pile. Prompts asked ChatGPT to 'extract all entities and identify their entity types' (or to 'define their type using a short sentence' for the definition variant).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>gpt-3.5-turbo-0301 (ChatGPT).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Inputs: 50K passages sampled from the Pile, each up to 256 tokens; ChatGPT generated entity mentions and types for each passage. Temperature set to 0 for stability. After filtering: ~45.9K annotated pairs, 240.7K entities, 13.0K distinct types (353K types in definition variant).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>LLM-as-annotator: use ChatGPT to synthesize labeled examples over broad inputs (open-world entity extraction), and use those synthetic labels to instruction-tune smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>LLM-generated structured outputs listing entity mentions and associated entity types, or entity types plus short natural-language definitions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>The generated labels are used to train students which are then evaluated on the Universal NER benchmark using strict micro-F1 and partial-match F1. The quality of teacher-generated labels is indirectly measured by student performance and ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>ChatGPT served as an effective teacher: students trained on its outputs (UniNER) outperformed ChatGPT itself on the downstream NER benchmark (UniNER-7B/13B average F1 41.7%/43.4% vs ChatGPT 34.9% zero-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Pile corpus as input source; Universal NER benchmark for evaluation of resultant student models.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Teacher-generated annotations may contain noise and domain mismatches; generated entity-type definitions can create mismatch with standard benchmark label formats; reliance on a black-box teacher LLM may have licensing and reproducibility constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Compared to prior label-synthesis approaches that prompt LMs to generate inputs, this work samples inputs from raw corpora then uses LLM to generate outputs, arguing for broader domain coverage. Also contrasted to supervised datasets: models distilled from ChatGPT exceeded supervised multi-task models in zero-shot generalization for this mission.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6085.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6085.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA (student)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA / LLaMA2 (student base models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open foundation LMs used as student models to be instruction-tuned in the distillation pipeline; specifically 7B and 13B parameter variants were fine-tuned to create UniNER models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llama: Open and efficient foundation language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLaMA-based student models</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pretrained autoregressive foundation models (LLaMA) fine-tuned with the mission-focused instruction tuning recipe to produce resource-efficient NER models that mimic and surpass teacher LLM performance on the targeted task.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>LLaMA 7B and 13B (also experiments with LLaMA2 noted with no significant difference).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Fine-tuned on the ChatGPT-annotated Pile dataset (~45.9K examples) and optionally on supervised NER datasets for continual fine-tuning. Evaluation across Universal NER benchmark (43 datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Instruction tuning (conversation-style) where each entity type is posed as a query and the model is trained to generate a JSON list of mentions; negative sampling is used to present absent types.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured JSON lists of predicted entity mentions per queried type.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Strict micro-F1 and partial-match F1 on Universal NER benchmark; ablation studies on negative sampling, prompt templates, and data variants.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>LLaMA-based UniNER models (7B, 13B) attain zero-shot average F1 of 41.7% and 43.4% respectively, outperforming ChatGPT and other instruction-tuned baselines in the NER benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Trained using ChatGPT-annotated Pile; evaluated on Universal NER benchmark (43 datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Model capacity matters (13B > 7B), still sensitive to prompt formulations and entity-type paraphrasing; requires careful negative sampling and dataset-specific templates to reconcile label definition conflicts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Compared directly with Vicuna, Alpaca (generic instruction-tuned models) and InstructUIE (supervised Flan-T5), with UniNER showing substantially better zero-shot NER performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6085.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6085.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Negative sampling (instr-tune)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Negative sampling during instruction tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training strategy where entity-type queries that are not present in a passage are sampled and the model is trained to output empty JSON lists for them, mitigating open-world vs closed-world mismatches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Negative sampling (frequency-based)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>During instruction-tuning, sample negative entity-type queries (types not present in the passage) with probability proportional to type frequency, and train the model to generate an empty JSON list as the correct response, improving closed-world behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Applied during fine-tuning of LLaMA student models (UniNER).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Negative queries sampled from the set of all entity types absent from each passage; sampling probability proportional to global type frequency. Applied across the ChatGPT-annotated Pile training set (~45.9K examples).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Augment positive (type present) queries with negative (type absent) queries so the model learns to produce empty structured outputs when appropriate.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>For negative queries, expected output is an empty JSON list.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Ablation comparing no negative sampling, uniform negative sampling, and frequency-proportional negative sampling; measured by average F1 on Universal NER benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Frequency-based negative sampling achieved best results: average F1 53.4% (vs None 31.5% and Uniform 47.7% in the ablation on UniNER-7B), showing a large positive effect of this strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Ablations performed with instruction-tuning on Pile-derived training data; evaluated on Universal NER benchmark domains.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Requires constructing a representative type frequency distribution; improper negative sampling (none or uniform) substantially harms performance; sampling choices can bias the model toward frequent types.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Demonstrated to be far superior to training without negatives and better than uniform negative sampling in this NER distillation context.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6085.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6085.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Definition-based data</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Definition-based data construction (type definitions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of data construction where ChatGPT is prompted to output entity mentions and short natural-language definitions for their types, increasing robustness to type paraphrasing but reducing alignment with standard NER label formats.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Definition-based data construction</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Prompt ChatGPT to 'extract all entities and concepts, and define their type using a short sentence' to obtain type-definition pairs (many more distinct types), then use these as supervision to tune students that are more robust to label paraphrases.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301) for generation; student LLaMA models fine-tuned on generated definition-based data.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Generated dataset variant containing ~353,092 distinct entity types (much larger and more diverse than label-only variant).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Use definition-based LLM outputs as supervision instead of short-label types; train students to map query definitions to entity mentions.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured outputs associating entity mentions with short natural-language definitions of types.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Evaluated on the Universal NER benchmark (which uses short label phrases) and on paraphrase-sensitivity case studies.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Definition-based UniNER (UniNER-7B-definition) showed increased robustness to entity-type paraphrasing in case studies, but performed worse on standard NER benchmarks (underperformed UniNER-type by ~11.8% average F1).</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Generated definition dataset from Pile; evaluated on Universal NER benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Definition outputs mismatch standard label formats used in evaluation datasets, causing lower benchmark scores despite better paraphrase robustness; much larger label space complicates training and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Compared directly as a variant to UniNER-type and UniNER-all-in-one; shows trade-offs between label-format alignment and paraphrase robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6085.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6085.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UniNER-all-in-one</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UniNER-all-in-one (all-types-in-one-query variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A UniNER variant that asks the model to extract all entity types in a passage in a single query/response rather than decomposing by type; found to underperform the per-type query approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>UniNER-all-in-one</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Instruction-tuning template variant where a single prompt requests extraction of all types and expects a single structured response containing all entities across types.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Fine-tuned on LLaMA 7B/13B with ChatGPT-annotated Pile data.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Same ChatGPT-annotated Pile dataset; training expects a combined response enumerating entities for all types.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Monolithic query-output mapping (multi-type extraction in one pass) rather than per-type queries.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Single structured response listing entities for all types.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Evaluated on Universal NER benchmark with strict micro-F1.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Underperformed the per-type query UniNER variant by ~3.3% average F1; likely due to attention distribution and task complexity when handling multiple types simultaneously.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Trained on ChatGPT-annotated Pile; evaluated on Universal NER benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Model attention may be dispersed across many types leading to reduced per-type accuracy; more complex generation target may increase errors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Compared to UniNER-type per-type query approach and UniNER-definition; per-type queries yielded better performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6085.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6085.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructUIE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructUIE (Multi-task instruction tuning for unified information extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised multi-task instruction-tuned system for unified information extraction based on Flan-T5-11B, used as a strong supervised baseline in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Instructuie: Multi-task instruction tuning for unified information extraction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>InstructUIE</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A supervised multi-task instruction-tuned model for information extraction; trained on diverse annotated IE datasets with a unified natural language generation objective, using Flan-T5 11B as backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Flan-T5 11B (base model for InstructUIE).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Trained on multiple supervised IE/NER datasets (various sizes depending on datasets); in this paper used as a comparison baseline in supervised and out-of-domain evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Supervised multi-task fine-tuning with instruction-style prompts (not LLM label synthesis).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Natural-language generation outputs encoding extracted information (entities, relations) per prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Evaluated (in this paper) on the Universal NER benchmark for out-of-domain and in-domain settings; authors corrected an evaluation bug in InstructUIE's original script before re-evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>UniNER (instruction-tuned from ChatGPT) outperformed InstructUIE-11B in zero-shot generalization and, after supervised fine-tuning, UniNER achieved higher in-domain supervised averages (UniNER-7B in-domain average F1 84.78% vs InstructUIE-11B lower).</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>InstructUIE trained on multiple supervised IE datasets (as per Wang et al., 2023a); evaluated here on Universal NER benchmark comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Supervised multi-dataset training faces label-definition conflicts across datasets; InstructUIE relies on human-annotated data and may generalize less in zero-shot settings compared to the targeted distillation approach.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Compared to UniNER, Vicuna, ChatGPT: UniNER (distilled from ChatGPT) achieved better zero-shot and, with supervised fine-tuning, better in-domain performance than InstructUIE-11B.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6085.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6085.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vicuna</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicuna</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source instruction-tuned chatbot derived from LLaMA and fine-tuned on ShareGPT conversations to imitate ChatGPT quality; used as a baseline comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality, March 2023</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Vicuna (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Open-source conversational model fine-tuned from LLaMA using ShareGPT conversational data to emulate ChatGPT behavior; evaluated as a zero-shot NER baseline in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Vicuna (LLaMA-based fine-tuned model; 7B and 13B variants referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Vicuna is trained on large volumes of ShareGPT conversation data; in this paper Vicuna's zero-shot performance is reported on the Universal NER benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Generic instruction/conversation tuning on real user-ChatGPT conversation pairs (ShareGPT) rather than mission-focused distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Conversational text responses; used here to produce NER outputs via prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Zero-shot NER evaluation on Universal NER benchmark using strict micro-F1 (and partial-match variant).</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Vicuna performed better than base LLaMA/Alpaca in some settings but trailed ChatGPT and substantially trailed UniNER on the Universal NER benchmark (UniNER outperforms Vicuna by large margins; e.g., average F1 differences of over 30 points reported against general instruction-tuned models).</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>ShareGPT for training (origin); evaluated on Universal NER benchmark here.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Generic instruction tuning on conversational data does not produce the mission-specific capabilities required for broad open-type NER; performs poorly on targeted NER tasks compared to UniNER.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Compared as a representative generic instruction-tuned model (alongside Alpaca) and shown to underperform mission-focused UniNER on NER benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6085.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6085.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alpaca</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alpaca</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small instruction-following LLaMA model distilled using synthetic instruction data generated by a teacher LLM; referenced as an example of general instruction distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Stanford alpaca: An instruction-following llama model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Alpaca (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Instruction-tuned LLaMA model using synthetic instruction data generated by a teacher LLM (automated instruction generation), presented as a generic distillation example for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Alpaca is LLaMA-based (the paper references Alpaca as LLaMA fine-tuned on synthetic instructions).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Trained on synthetic instruction-response pairs generated from teacher LMs (data generation and size per Alpaca repo); used as a baseline here for zero-shot NER performance.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Generic instruction distillation via automated generation of instruction-following examples.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Instruction-following responses; used to answer NER prompts in zero-shot evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Compared in zero-shot NER performance on the Universal NER benchmark; Alpaca and base LLaMA perform poorly on this benchmark (close to zero F1 in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Alpaca and base LLaMA perform poorly at zero-shot open NER on the comprehensive benchmark; UniNER greatly outperforms these general instruction-tuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Alpaca training data (per Taori et al.) ; evaluated here on the paper's Universal NER benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Generic instruction data insufficient to capture mission-level capabilities for open NER; yields shallow approximations of teacher LLMs across diverse downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Contrasted with mission-focused distillation (UniNER), which is more effective for targeted application classes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6085.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e6085.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruction Tuning (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruction Tuning (general concept)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of finetuning approaches that adapt pretrained language models to follow natural-language instructions across many tasks, often improving zero-shot/few-shot generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Instruction Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>General paradigm of fine-tuning LMs on instruction-response pairs (crowd-sourced, synthetic, or supervised) to make models follow natural-language instructions; includes methods like FLAN, T0, InstructGPT, Alpaca, Vicuna.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Varies by work; examples cited include Flan-T5, InstructGPT, Alpaca/Vicuna (LLaMA-based), and GPT-4 in other works.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Typically large collections of instructionresponse pairs assembled from existing datasets, crowd-sourced examples, or synthetic outputs from teacher LLMs; sizes vary from thousands to millions of examples depending on the approach.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Fine-tune a pretrained LM with instruction-response pairs so it generalizes to unseen tasks expressed as instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Instruction-following natural-language outputs or structured outputs depending on task templates.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Evaluated on held-out tasks in zero-shot/few-shot setups, benchmarks like Super-NaturalInstructions, or task-specific downstream evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Instruction tuning can enable strong zero-shot generalization for many NLP tasks, but generic instruction-tuned models often underperform teacher LLMs on mission-specific large-scale extraction tasks unless tuned specifically for that mission.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>FLAN, T0, Super-NaturalInstructions, ShareGPT, and synthetic-instruction datasets (e.g., Alpaca).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Generic instruction diversity does not guarantee mission-specific competence; limited compute for student models constrains fidelity to teacher LLMs; evaluation on targeted benchmarks may reveal large gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Compared in the paper with mission-focused instruction tuning; mission-focused tuning specializes instruction-tuned students for one broad application class and yields superior performance for that mission.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Stanford alpaca: An instruction-following llama model <em>(Rating: 2)</em></li>
                <li>Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality, March 2023 <em>(Rating: 2)</em></li>
                <li>Instructuie: Multi-task instruction tuning for unified information extraction <em>(Rating: 2)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 2)</em></li>
                <li>Instruction tuning with gpt-4 <em>(Rating: 2)</em></li>
                <li>Impossible distillation: from low-quality model to high-quality dataset & model for summarization and paraphrasing <em>(Rating: 2)</em></li>
                <li>Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes <em>(Rating: 2)</em></li>
                <li>Distilling large language models for biomedical knowledge extraction: A case study on adverse drug events <em>(Rating: 2)</em></li>
                <li>Self-instruct: Aligning language models with self-generated instructions <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6085",
    "paper_id": "paper-76b99439d524ae1813c30edc4bcad487a30a1f8c",
    "extraction_schema_id": "extraction-schema-120",
    "extracted_data": [
        {
            "name_short": "UniNER",
            "name_full": "UniversalNER (UniNER)",
            "brief_description": "A family of small instruction-tuned LLaMA models distilled from ChatGPT to perform open-domain named entity recognition across tens of thousands of entity types and many domains via mission-focused instruction tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "UniversalNER (UniNER)",
            "system_description": "Targeted distillation system: sample diverse inputs from a large corpus (Pile), use an LLM (ChatGPT) to label entity mentions and types, then instruction-tune smaller LLaMA models in a conversation-style format to output structured JSON lists of entities per queried entity type.",
            "llm_model_used": "Teacher: ChatGPT (gpt-3.5-turbo-0301) for annotation; Students: LLaMA-based models (LLaMA 7B and 13B; LLaMA2 also tried but no significant diff).",
            "input_type_and_size": "Input passages: 50K passages sampled from the Pile corpus (articles chunked to max 256 tokens). ChatGPT produced annotations for these passages; constructed dataset after filtering: 45,889 input-output pairs, 240,725 entities across 13,020 distinct entity types (also a variant with 353,092 entity types when using definition-based generation). Evaluation inputs: Universal NER benchmark of 43 NER datasets across 9 domains (sampled up to 200K passage-query pairs per dataset for eval).",
            "distillation_approach": "Mission-focused instruction tuning: (1) sample diverse inputs from raw corpora (Pile), (2) use teacher LLM to generate output annotations (entity mentions + types or definitions), (3) convert each entity type into a natural language query (\"What describes &lt;type&gt;?\"), (4) fine-tune student LLaMA models to generate structured JSON lists of entities per query, using language modeling objective; augment with negative sampling (sample entity types not present in passage and expect empty JSON list), and optionally supervised fine-tuning with dataset-specific instruction templates.",
            "output_type": "Structured JSON lists of entity mentions per queried entity type (one JSON list per type query). Also variants where all types are requested in one response or where types are defined by short sentences (definition-based).",
            "evaluation_methods": "Strict entity-level micro-F1 requiring exact match of both entity type and boundary; also an alternative partial-match evaluation where any word-overlap counts as 0.5 true positive. Evaluated zero-shot and supervised/fine-tuned settings across the Universal NER benchmark (43 datasets, 9 domains).",
            "results": "Zero-shot average F1: UniNER-7B = 41.7%, UniNER-13B = 43.4%, compared to ChatGPT = 34.9% (average). UniNER variants: 'all-in-one' and 'definition' underperform the main UniNER-type by 3.3% and 11.8% respectively. With supervised continual fine-tuning, UniNER-7B reached average F1 up to 60.0% (out-of-domain) and 84.78% average F1 on 20 supervised datasets (in-domain), outperforming InstructUIE-11B and BERT-base in those settings.",
            "datasets_or_benchmarks": "Training inputs: Pile corpus (22 subdatasets). Evaluation benchmark: Universal NER benchmark assembled in this paper (43 NER datasets across general, biomedical, clinical, STEM, programming, social, law, finance, transportation domains). Comparisons use ChatGPT (gpt-3.5-turbo-0301), Vicuna, and InstructUIE (Flan-T5-11B).",
            "challenges_or_limitations": "Label definition inconsistencies across datasets (causing conflicts in multi-dataset supervised training); sensitivity to entity-type paraphrasing for the type-label variant (improved with definition-based data but that harms standard benchmark alignment); open-world training data mismatch with closed-world evaluation (addressed by negative sampling); heavy-tailed long-tail entity distribution and potential noise in LLM-generated annotations; attention dispersion when asking all types in one query (reduces per-type accuracy).",
            "comparisons_to_other_methods": "UniNER substantially outperforms general instruction-tuned models (Alpaca, Vicuna) on zero-shot NER by large margins (e.g., &gt;30 absolute F1 over Vicuna in average on the Universal NER benchmark), and exceeds ChatGPT's average NER F1 by ~7-9 absolute points. UniNER also outperforms InstructUIE (a supervised multi-task instruction-tuned system based on Flan-T5-11B) despite the latter using supervised NER examples.",
            "uuid": "e6085.0",
            "source_info": {
                "paper_title": "UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Mission-Focused Instruction Tuning",
            "name_full": "Mission-Focused Instruction Tuning (targeted distillation)",
            "brief_description": "A distillation strategy that narrows instruction tuning to a specific broad application class (e.g., open information extraction) by maximizing diversity of inputs and task labels instead of generic instruction diversity, to better transfer LLM capabilities to small students for that mission.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Mission-Focused Instruction Tuning",
            "system_description": "Instruction-tuning recipe tailored to a single application class: sample diverse real inputs from large corpora, have an LLM produce task-specific outputs, structure prompts as queries per task label (e.g., entity type), and fine-tune a smaller LM on these paired (input, query) -&gt; (structured output) examples. Include negative sampling and dataset-specific prompt fields when supervised data are available.",
            "llm_model_used": "Teacher LLM for annotation: ChatGPT (gpt-3.5-turbo-0301) used in this work; student LMs: LLaMA 7B and 13B.",
            "input_type_and_size": "Diverse real-world text sampled from Pile (50K 256-token passages in this study) rather than LM-generated inputs; labels are entity types (thousands) or short sentence definitions (in the definition variant).",
            "distillation_approach": "Generate supervision with teacher LLM on diverse real inputs, reframe each label as a natural-language query, fine-tune student model to answer queries with structured outputs (JSON lists), use negative sampling to present label queries not present in text, and harmonize supervised label differences via dataset-specific prompt fields.",
            "output_type": "Structured model responses per query (JSON lists of entities), making the student model a queryable extractor for arbitrary labels in the mission class.",
            "evaluation_methods": "Evaluate downstream task performance on a comprehensive benchmark for the mission (here: Universal NER benchmark) using strict entity-level micro-F1 and partial-match F1, plus ablations on components (negative sampling strategies, prompt templates, data variants).",
            "results": "Demonstrated that mission-focused tuning yields much stronger zero-shot performance for NER than general instruction-tuned models; negative sampling (frequency-based) significantly improves results; definition-based label generation increases robustness to label paraphrase but reduces alignment to standard benchmarks.",
            "datasets_or_benchmarks": "Pile for input sampling; Universal NER benchmark (43 datasets) for evaluation; supervised datasets for fine-tuning include many standard NER sets (ACE, OntoNotes, etc.).",
            "challenges_or_limitations": "Requires careful design to handle label definition mismatches across datasets; trade-offs between robustness to paraphrases (definitions) and alignment with standard label formats; generation noise from teacher LLM; open-world vs closed-world mismatch that must be mitigated with negative sampling.",
            "comparisons_to_other_methods": "Contrasted with generic instruction tuning (Alpaca, Vicuna) which aim for breadth across tasks and with supervised multi-task instruction tuning (InstructUIE). Mission-focused tuning gave much better performance for the targeted application (NER) than generic instruction-tuned models and even surpassed InstructUIE despite the latter using human-labelled supervised data.",
            "uuid": "e6085.1",
            "source_info": {
                "paper_title": "UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "ChatGPT (as teacher)",
            "name_full": "ChatGPT (teacher annotations via gpt-3.5-turbo-0301)",
            "brief_description": "A deployed LLM used as a label generator/teacher in this work: ChatGPT (gpt-3.5-turbo-0301) produced entity mentions and types (or definitions) for sampled passages from Pile, with temperature=0 to ensure determinism.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "ChatGPT (annotation teacher)",
            "system_description": "Teacher LLM used to create large-scale instruction-style NER annotations on diverse unlabeled passages sampled from the Pile. Prompts asked ChatGPT to 'extract all entities and identify their entity types' (or to 'define their type using a short sentence' for the definition variant).",
            "llm_model_used": "gpt-3.5-turbo-0301 (ChatGPT).",
            "input_type_and_size": "Inputs: 50K passages sampled from the Pile, each up to 256 tokens; ChatGPT generated entity mentions and types for each passage. Temperature set to 0 for stability. After filtering: ~45.9K annotated pairs, 240.7K entities, 13.0K distinct types (353K types in definition variant).",
            "distillation_approach": "LLM-as-annotator: use ChatGPT to synthesize labeled examples over broad inputs (open-world entity extraction), and use those synthetic labels to instruction-tune smaller models.",
            "output_type": "LLM-generated structured outputs listing entity mentions and associated entity types, or entity types plus short natural-language definitions.",
            "evaluation_methods": "The generated labels are used to train students which are then evaluated on the Universal NER benchmark using strict micro-F1 and partial-match F1. The quality of teacher-generated labels is indirectly measured by student performance and ablations.",
            "results": "ChatGPT served as an effective teacher: students trained on its outputs (UniNER) outperformed ChatGPT itself on the downstream NER benchmark (UniNER-7B/13B average F1 41.7%/43.4% vs ChatGPT 34.9% zero-shot).",
            "datasets_or_benchmarks": "Pile corpus as input source; Universal NER benchmark for evaluation of resultant student models.",
            "challenges_or_limitations": "Teacher-generated annotations may contain noise and domain mismatches; generated entity-type definitions can create mismatch with standard benchmark label formats; reliance on a black-box teacher LLM may have licensing and reproducibility constraints.",
            "comparisons_to_other_methods": "Compared to prior label-synthesis approaches that prompt LMs to generate inputs, this work samples inputs from raw corpora then uses LLM to generate outputs, arguing for broader domain coverage. Also contrasted to supervised datasets: models distilled from ChatGPT exceeded supervised multi-task models in zero-shot generalization for this mission.",
            "uuid": "e6085.2",
            "source_info": {
                "paper_title": "UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "LLaMA (student)",
            "name_full": "LLaMA / LLaMA2 (student base models)",
            "brief_description": "Open foundation LMs used as student models to be instruction-tuned in the distillation pipeline; specifically 7B and 13B parameter variants were fine-tuned to create UniNER models.",
            "citation_title": "Llama: Open and efficient foundation language models",
            "mention_or_use": "use",
            "system_name": "LLaMA-based student models",
            "system_description": "Pretrained autoregressive foundation models (LLaMA) fine-tuned with the mission-focused instruction tuning recipe to produce resource-efficient NER models that mimic and surpass teacher LLM performance on the targeted task.",
            "llm_model_used": "LLaMA 7B and 13B (also experiments with LLaMA2 noted with no significant difference).",
            "input_type_and_size": "Fine-tuned on the ChatGPT-annotated Pile dataset (~45.9K examples) and optionally on supervised NER datasets for continual fine-tuning. Evaluation across Universal NER benchmark (43 datasets).",
            "distillation_approach": "Instruction tuning (conversation-style) where each entity type is posed as a query and the model is trained to generate a JSON list of mentions; negative sampling is used to present absent types.",
            "output_type": "Structured JSON lists of predicted entity mentions per queried type.",
            "evaluation_methods": "Strict micro-F1 and partial-match F1 on Universal NER benchmark; ablation studies on negative sampling, prompt templates, and data variants.",
            "results": "LLaMA-based UniNER models (7B, 13B) attain zero-shot average F1 of 41.7% and 43.4% respectively, outperforming ChatGPT and other instruction-tuned baselines in the NER benchmark.",
            "datasets_or_benchmarks": "Trained using ChatGPT-annotated Pile; evaluated on Universal NER benchmark (43 datasets).",
            "challenges_or_limitations": "Model capacity matters (13B &gt; 7B), still sensitive to prompt formulations and entity-type paraphrasing; requires careful negative sampling and dataset-specific templates to reconcile label definition conflicts.",
            "comparisons_to_other_methods": "Compared directly with Vicuna, Alpaca (generic instruction-tuned models) and InstructUIE (supervised Flan-T5), with UniNER showing substantially better zero-shot NER performance.",
            "uuid": "e6085.3",
            "source_info": {
                "paper_title": "UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Negative sampling (instr-tune)",
            "name_full": "Negative sampling during instruction tuning",
            "brief_description": "A training strategy where entity-type queries that are not present in a passage are sampled and the model is trained to output empty JSON lists for them, mitigating open-world vs closed-world mismatches.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Negative sampling (frequency-based)",
            "system_description": "During instruction-tuning, sample negative entity-type queries (types not present in the passage) with probability proportional to type frequency, and train the model to generate an empty JSON list as the correct response, improving closed-world behavior.",
            "llm_model_used": "Applied during fine-tuning of LLaMA student models (UniNER).",
            "input_type_and_size": "Negative queries sampled from the set of all entity types absent from each passage; sampling probability proportional to global type frequency. Applied across the ChatGPT-annotated Pile training set (~45.9K examples).",
            "distillation_approach": "Augment positive (type present) queries with negative (type absent) queries so the model learns to produce empty structured outputs when appropriate.",
            "output_type": "For negative queries, expected output is an empty JSON list.",
            "evaluation_methods": "Ablation comparing no negative sampling, uniform negative sampling, and frequency-proportional negative sampling; measured by average F1 on Universal NER benchmark.",
            "results": "Frequency-based negative sampling achieved best results: average F1 53.4% (vs None 31.5% and Uniform 47.7% in the ablation on UniNER-7B), showing a large positive effect of this strategy.",
            "datasets_or_benchmarks": "Ablations performed with instruction-tuning on Pile-derived training data; evaluated on Universal NER benchmark domains.",
            "challenges_or_limitations": "Requires constructing a representative type frequency distribution; improper negative sampling (none or uniform) substantially harms performance; sampling choices can bias the model toward frequent types.",
            "comparisons_to_other_methods": "Demonstrated to be far superior to training without negatives and better than uniform negative sampling in this NER distillation context.",
            "uuid": "e6085.4",
            "source_info": {
                "paper_title": "UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Definition-based data",
            "name_full": "Definition-based data construction (type definitions)",
            "brief_description": "A variant of data construction where ChatGPT is prompted to output entity mentions and short natural-language definitions for their types, increasing robustness to type paraphrasing but reducing alignment with standard NER label formats.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Definition-based data construction",
            "system_description": "Prompt ChatGPT to 'extract all entities and concepts, and define their type using a short sentence' to obtain type-definition pairs (many more distinct types), then use these as supervision to tune students that are more robust to label paraphrases.",
            "llm_model_used": "ChatGPT (gpt-3.5-turbo-0301) for generation; student LLaMA models fine-tuned on generated definition-based data.",
            "input_type_and_size": "Generated dataset variant containing ~353,092 distinct entity types (much larger and more diverse than label-only variant).",
            "distillation_approach": "Use definition-based LLM outputs as supervision instead of short-label types; train students to map query definitions to entity mentions.",
            "output_type": "Structured outputs associating entity mentions with short natural-language definitions of types.",
            "evaluation_methods": "Evaluated on the Universal NER benchmark (which uses short label phrases) and on paraphrase-sensitivity case studies.",
            "results": "Definition-based UniNER (UniNER-7B-definition) showed increased robustness to entity-type paraphrasing in case studies, but performed worse on standard NER benchmarks (underperformed UniNER-type by ~11.8% average F1).",
            "datasets_or_benchmarks": "Generated definition dataset from Pile; evaluated on Universal NER benchmark.",
            "challenges_or_limitations": "Definition outputs mismatch standard label formats used in evaluation datasets, causing lower benchmark scores despite better paraphrase robustness; much larger label space complicates training and evaluation.",
            "comparisons_to_other_methods": "Compared directly as a variant to UniNER-type and UniNER-all-in-one; shows trade-offs between label-format alignment and paraphrase robustness.",
            "uuid": "e6085.5",
            "source_info": {
                "paper_title": "UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "UniNER-all-in-one",
            "name_full": "UniNER-all-in-one (all-types-in-one-query variant)",
            "brief_description": "A UniNER variant that asks the model to extract all entity types in a passage in a single query/response rather than decomposing by type; found to underperform the per-type query approach.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "UniNER-all-in-one",
            "system_description": "Instruction-tuning template variant where a single prompt requests extraction of all types and expects a single structured response containing all entities across types.",
            "llm_model_used": "Fine-tuned on LLaMA 7B/13B with ChatGPT-annotated Pile data.",
            "input_type_and_size": "Same ChatGPT-annotated Pile dataset; training expects a combined response enumerating entities for all types.",
            "distillation_approach": "Monolithic query-output mapping (multi-type extraction in one pass) rather than per-type queries.",
            "output_type": "Single structured response listing entities for all types.",
            "evaluation_methods": "Evaluated on Universal NER benchmark with strict micro-F1.",
            "results": "Underperformed the per-type query UniNER variant by ~3.3% average F1; likely due to attention distribution and task complexity when handling multiple types simultaneously.",
            "datasets_or_benchmarks": "Trained on ChatGPT-annotated Pile; evaluated on Universal NER benchmark.",
            "challenges_or_limitations": "Model attention may be dispersed across many types leading to reduced per-type accuracy; more complex generation target may increase errors.",
            "comparisons_to_other_methods": "Compared to UniNER-type per-type query approach and UniNER-definition; per-type queries yielded better performance.",
            "uuid": "e6085.6",
            "source_info": {
                "paper_title": "UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "InstructUIE",
            "name_full": "InstructUIE (Multi-task instruction tuning for unified information extraction)",
            "brief_description": "A supervised multi-task instruction-tuned system for unified information extraction based on Flan-T5-11B, used as a strong supervised baseline in this study.",
            "citation_title": "Instructuie: Multi-task instruction tuning for unified information extraction",
            "mention_or_use": "mention",
            "system_name": "InstructUIE",
            "system_description": "A supervised multi-task instruction-tuned model for information extraction; trained on diverse annotated IE datasets with a unified natural language generation objective, using Flan-T5 11B as backbone.",
            "llm_model_used": "Flan-T5 11B (base model for InstructUIE).",
            "input_type_and_size": "Trained on multiple supervised IE/NER datasets (various sizes depending on datasets); in this paper used as a comparison baseline in supervised and out-of-domain evaluations.",
            "distillation_approach": "Supervised multi-task fine-tuning with instruction-style prompts (not LLM label synthesis).",
            "output_type": "Natural-language generation outputs encoding extracted information (entities, relations) per prompt.",
            "evaluation_methods": "Evaluated (in this paper) on the Universal NER benchmark for out-of-domain and in-domain settings; authors corrected an evaluation bug in InstructUIE's original script before re-evaluation.",
            "results": "UniNER (instruction-tuned from ChatGPT) outperformed InstructUIE-11B in zero-shot generalization and, after supervised fine-tuning, UniNER achieved higher in-domain supervised averages (UniNER-7B in-domain average F1 84.78% vs InstructUIE-11B lower).",
            "datasets_or_benchmarks": "InstructUIE trained on multiple supervised IE datasets (as per Wang et al., 2023a); evaluated here on Universal NER benchmark comparisons.",
            "challenges_or_limitations": "Supervised multi-dataset training faces label-definition conflicts across datasets; InstructUIE relies on human-annotated data and may generalize less in zero-shot settings compared to the targeted distillation approach.",
            "comparisons_to_other_methods": "Compared to UniNER, Vicuna, ChatGPT: UniNER (distilled from ChatGPT) achieved better zero-shot and, with supervised fine-tuning, better in-domain performance than InstructUIE-11B.",
            "uuid": "e6085.7",
            "source_info": {
                "paper_title": "UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Vicuna",
            "name_full": "Vicuna",
            "brief_description": "An open-source instruction-tuned chatbot derived from LLaMA and fine-tuned on ShareGPT conversations to imitate ChatGPT quality; used as a baseline comparison.",
            "citation_title": "Vicuna: An open-source chatbot impressing gpt-4 with $90 \\%$ * chatgpt quality, March 2023",
            "mention_or_use": "mention",
            "system_name": "Vicuna (baseline)",
            "system_description": "Open-source conversational model fine-tuned from LLaMA using ShareGPT conversational data to emulate ChatGPT behavior; evaluated as a zero-shot NER baseline in this work.",
            "llm_model_used": "Vicuna (LLaMA-based fine-tuned model; 7B and 13B variants referenced).",
            "input_type_and_size": "Vicuna is trained on large volumes of ShareGPT conversation data; in this paper Vicuna's zero-shot performance is reported on the Universal NER benchmark.",
            "distillation_approach": "Generic instruction/conversation tuning on real user-ChatGPT conversation pairs (ShareGPT) rather than mission-focused distillation.",
            "output_type": "Conversational text responses; used here to produce NER outputs via prompting.",
            "evaluation_methods": "Zero-shot NER evaluation on Universal NER benchmark using strict micro-F1 (and partial-match variant).",
            "results": "Vicuna performed better than base LLaMA/Alpaca in some settings but trailed ChatGPT and substantially trailed UniNER on the Universal NER benchmark (UniNER outperforms Vicuna by large margins; e.g., average F1 differences of over 30 points reported against general instruction-tuned models).",
            "datasets_or_benchmarks": "ShareGPT for training (origin); evaluated on Universal NER benchmark here.",
            "challenges_or_limitations": "Generic instruction tuning on conversational data does not produce the mission-specific capabilities required for broad open-type NER; performs poorly on targeted NER tasks compared to UniNER.",
            "comparisons_to_other_methods": "Compared as a representative generic instruction-tuned model (alongside Alpaca) and shown to underperform mission-focused UniNER on NER benchmarks.",
            "uuid": "e6085.8",
            "source_info": {
                "paper_title": "UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Alpaca",
            "name_full": "Alpaca",
            "brief_description": "A small instruction-following LLaMA model distilled using synthetic instruction data generated by a teacher LLM; referenced as an example of general instruction distillation.",
            "citation_title": "Stanford alpaca: An instruction-following llama model",
            "mention_or_use": "mention",
            "system_name": "Alpaca (baseline)",
            "system_description": "Instruction-tuned LLaMA model using synthetic instruction data generated by a teacher LLM (automated instruction generation), presented as a generic distillation example for comparison.",
            "llm_model_used": "Alpaca is LLaMA-based (the paper references Alpaca as LLaMA fine-tuned on synthetic instructions).",
            "input_type_and_size": "Trained on synthetic instruction-response pairs generated from teacher LMs (data generation and size per Alpaca repo); used as a baseline here for zero-shot NER performance.",
            "distillation_approach": "Generic instruction distillation via automated generation of instruction-following examples.",
            "output_type": "Instruction-following responses; used to answer NER prompts in zero-shot evaluation.",
            "evaluation_methods": "Compared in zero-shot NER performance on the Universal NER benchmark; Alpaca and base LLaMA perform poorly on this benchmark (close to zero F1 in the paper).",
            "results": "Alpaca and base LLaMA perform poorly at zero-shot open NER on the comprehensive benchmark; UniNER greatly outperforms these general instruction-tuned models.",
            "datasets_or_benchmarks": "Alpaca training data (per Taori et al.) ; evaluated here on the paper's Universal NER benchmark.",
            "challenges_or_limitations": "Generic instruction data insufficient to capture mission-level capabilities for open NER; yields shallow approximations of teacher LLMs across diverse downstream tasks.",
            "comparisons_to_other_methods": "Contrasted with mission-focused distillation (UniNER), which is more effective for targeted application classes.",
            "uuid": "e6085.9",
            "source_info": {
                "paper_title": "UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Instruction Tuning (general)",
            "name_full": "Instruction Tuning (general concept)",
            "brief_description": "A family of finetuning approaches that adapt pretrained language models to follow natural-language instructions across many tasks, often improving zero-shot/few-shot generalization.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Instruction Tuning",
            "system_description": "General paradigm of fine-tuning LMs on instruction-response pairs (crowd-sourced, synthetic, or supervised) to make models follow natural-language instructions; includes methods like FLAN, T0, InstructGPT, Alpaca, Vicuna.",
            "llm_model_used": "Varies by work; examples cited include Flan-T5, InstructGPT, Alpaca/Vicuna (LLaMA-based), and GPT-4 in other works.",
            "input_type_and_size": "Typically large collections of instructionresponse pairs assembled from existing datasets, crowd-sourced examples, or synthetic outputs from teacher LLMs; sizes vary from thousands to millions of examples depending on the approach.",
            "distillation_approach": "Fine-tune a pretrained LM with instruction-response pairs so it generalizes to unseen tasks expressed as instructions.",
            "output_type": "Instruction-following natural-language outputs or structured outputs depending on task templates.",
            "evaluation_methods": "Evaluated on held-out tasks in zero-shot/few-shot setups, benchmarks like Super-NaturalInstructions, or task-specific downstream evaluations.",
            "results": "Instruction tuning can enable strong zero-shot generalization for many NLP tasks, but generic instruction-tuned models often underperform teacher LLMs on mission-specific large-scale extraction tasks unless tuned specifically for that mission.",
            "datasets_or_benchmarks": "FLAN, T0, Super-NaturalInstructions, ShareGPT, and synthetic-instruction datasets (e.g., Alpaca).",
            "challenges_or_limitations": "Generic instruction diversity does not guarantee mission-specific competence; limited compute for student models constrains fidelity to teacher LLMs; evaluation on targeted benchmarks may reveal large gaps.",
            "comparisons_to_other_methods": "Compared in the paper with mission-focused instruction tuning; mission-focused tuning specializes instruction-tuned students for one broad application class and yields superior performance for that mission.",
            "uuid": "e6085.10",
            "source_info": {
                "paper_title": "UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Stanford alpaca: An instruction-following llama model",
            "rating": 2
        },
        {
            "paper_title": "Vicuna: An open-source chatbot impressing gpt-4 with $90 \\%$ * chatgpt quality, March 2023",
            "rating": 2
        },
        {
            "paper_title": "Instructuie: Multi-task instruction tuning for unified information extraction",
            "rating": 2
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 2
        },
        {
            "paper_title": "Instruction tuning with gpt-4",
            "rating": 2
        },
        {
            "paper_title": "Impossible distillation: from low-quality model to high-quality dataset & model for summarization and paraphrasing",
            "rating": 2
        },
        {
            "paper_title": "Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes",
            "rating": 2
        },
        {
            "paper_title": "Distilling large language models for biomedical knowledge extraction: A case study on adverse drug events",
            "rating": 2
        },
        {
            "paper_title": "Self-instruct: Aligning language models with self-generated instructions",
            "rating": 1
        }
    ],
    "cost": 0.022643999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>UniversalNER: TARGETED DISTILLATION FROM LARGE LANGUAGE MODELS FOR OPEN NAMED ENTITY RECOGNITION</h1>
<p>Wenxuan Zhou ${ }^{1 <em>}$, Sheng Zhang ${ }^{2 </em>}$, Yu Gu ${ }^{2}$, Muhao Chen ${ }^{1,3}$, Hoifung Poon ${ }^{2}$<br>${ }^{1}$ University of Southern California ${ }^{2}$ Microsoft Research ${ }^{3}$ University of California, Davis<br>${ }^{1}$ {zhouwenx,muhaoche}@usc.edu ${ }^{2}$ {shezhan,yugu1,hoifung}@microsoft.com</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have demonstrated remarkable generalizability, such as understanding arbitrary entities and relations. Instruction tuning has proven effective for distilling LLMs into more cost-efficient models such as Alpaca and Vicuna. Yet such student models still trail the original LLMs by large margins in downstream applications. In this paper, we explore targeted distillation with mission-focused instruction tuning to train student models that can excel in a broad application class such as open information extraction. Using named entity recognition (NER) for case study, we show how ChatGPT can be distilled into much smaller UniversalNER models for open NER. For evaluation, we assemble the largest NER benchmark to date, comprising 43 datasets across 9 diverse domains such as biomedicine, programming, social media, law, finance. Without using any direct supervision, UniversalNER attains remarkable NER accuracy across tens of thousands of entity types, outperforming general instruction-tuned models such as Alpaca and Vicuna by over 30 absolute F1 points in average. With a tiny fraction of parameters, UniversalNER not only acquires ChatGPT's capability in recognizing arbitrary entity types, but also outperforms its NER accuracy by 7-9 absolute F1 points in average. Remarkably, UniversalNER even outperforms by a large margin state-of-the-art multi-task instruction-tuned systems such as InstructUIE, which uses supervised NER examples. We also conduct thorough ablation studies to assess the impact of various components in our distillation approach. We release the distillation recipe, data, and UniversalNER models to facilitate future research on targeted distillation. ${ }^{1}$</p>
<h2>1 INTRODUCTION</h2>
<p>Large language models (LLMs) such as ChatGPT (Ouyang et al., 2022; OpenAI, 2023) have demonstrated remarkable generalization capabilities, but they generally require prohibitive cost in training and inference. Moreover, in mission-critical applications such as biomedicine, white-box access to model weights and inference probabilities are often important for explainability and trust. Consequently, instruction-tuning has become a popular approach for distilling LLMs into more cost-efficient and transparent student models. Such student models, as exemplified by Alpaca (Taori et al., 2023) and Vicuna (Chiang et al., 2023), have demonstrated compelling capabilities in imitating ChatGPT. However, upon close inspection, they still trail the teacher LLM by a large margin, especially in targeted downstream applications (Gudibande et al., 2023). Bounded by limited compute, it is unsurprising that generic distillation can only produce a shallow approximation of the original LLM across all possible applications.</p>
<p>In this paper, we instead explore targeted distillation where we train student models using missionfocused instruction tuning for a broad application class such as open information extraction (Etzioni et al., 2008). We show that this can maximally replicate LLM's capabilities for the given application</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>class, while preserving its generalizability across semantic types and domains. We choose named entity recognition (NER) for our case study, as it is one of the most fundamental tasks in natural language processing (Wu et al., 2017; Perera et al., 2020). Recent studies (Wei et al., 2023; Li et al., 2023) show that when there are abundant annotated examples for an entity type, LLMs still fall behind the state-of-the-art supervised system for that entity type. However, for the vast majority of entity types, there is little annotated data. New entity types constantly emerge, and it is expensive and time-consuming to generate annotated examples, especially in high-value domains such as biomedicine where specialized expertise is required for annotation. Trained on pre-specified entity types and domains, supervised NER models also exhibit limited generalizability for new domains and entity types.</p>
<p>We present a general recipe for targeted distillation from LLMs and demonstrate that for open-domain NER. We show how to use ChatGPT to generate instruction-tuning data for NER from broad-coverage unlabeled web text, and conduct instruction-tuning on LLaMA (Touvron et al., 2023a) to distill the UniversalNER models (UniNER in short).</p>
<p>To facilitate a thorough evaluation, we assemble the largest and most diverse NER benchmark to date (UniversalNER benchmark), comprising 43 datasets across 9 domains such as biomedicine, programming, social media, law, finance. On zero-shot NER, LLaMA and Alpaca perform poorly on this benchmark (close to zero F1). Vicuna performs much better by comparison, but still trails ChatGPT by over 20 absolute points in average F1. By contrast, UniversalNER attains state-of-the-art NER accuracy across tens of thousands of entity types in the UniversalNER benchmark, outperforming Vicuna by over 30 absolute points in average F1. With a tiny fraction of parameters, UniversalNER not only replicates ChatGPT's capability in recognizing arbitrary entity types, but also outperforms its NER accuracy by 7-9 absolute points in average F1. Remarkably, UniversalNER even outperforms by a large margin state-of-the-art multi-task instruction-tuned systems such as InstructUIE (Wang et al., 2023a), which uses supervised NER examples. We also conduct thorough ablation studies to assess the impact of various distillation components, such as the instruction prompts and negative sampling.</p>
<h1>2 Related Work</h1>
<p>Knowledge distillation. While LLMs such as ChatGPT achieve promising results, these models are often black-box and have high computational costs. To address these issues, distilling the task capabilities of LLMs into smaller, more manageable models has emerged as a promising direction. Knowledge distillation (Hinton et al., 2015) often revolves around the transfer of knowledge from larger, more complex models to their smaller counterparts. Recent work (Taori et al., 2023; Chiang et al., 2023; Peng et al., 2023) seeks to distill the general abilities of LLMs with the objective of matching, if not surpassing, the performance of the original LLMs. Particularly, Alpaca (Taori et al., 2023) automates the generation of instructions (Wang et al., 2023c) and distills the knowledge from a teacher LLM. Vicuna (Chiang et al., 2023) adopts the ShareGPT data, which are comprised of real conversations with ChatGPT conducted by users, thereby providing a more authentic context for distillation. Another line of work (Smith et al., 2022; Jung et al., 2023; Hsieh et al., 2023; Gu et al., 2023) focuses on distilling task-level abilities from LLMs. Particularly, Jung et al. (2023) propose an efficient method to distill an order of magnitude smaller model that outperforms GPT-3 on specialized tasks summarization and paraphrasing in certain domains. Hsieh et al. (2022) propose to distill LLMs' reasoning abilities into smaller models by chain-of-the-thought distillation. However, these studies perform distillation either on certain datasets or domains, while our work focuses on a more general formulation that can be applied to diverse domains.</p>
<p>Instruction tuning. As an effective method to adapt LMs to perform a variety of tasks, instruction tuning has attracted an increasing number of community efforts: FLAN (Chung et al., 2022), T0 (Sanh et al., 2021), and Tk-Instruct (Wang et al., 2022) convert a large set of existing supervised learning datasets into instruction-following format, and then fine-tune encoder-decoder models, showing strong zero-shot and few-shot performance on NLP benchmarks. Ouyang et al. (2022) crowd-source high-quality instruction data and fine-tune GPT-3 into InstructGPT, enhancing its ability to understand user intention and follow instructions. Recent advancements (Taori et al., 2023; Chiang et al., 2023; Peng et al., 2023) have also led to smaller models that exhibit task-following capabilities, after being fine-tuned on instruction data generated by LLMs, such as ChatGPT or GPT4. However, these smaller</p>
<p>models often struggle to generate high-quality responses for a diverse range of tasks (Wang et al., 2023b). A closer examination on targeted benchmarks reveals a substantial gap between these models to ChatGPT (Gudibande et al., 2023). Our proposed method, in contrast, focuses on tuning models to excel at a specific type of tasks. The diversity in our instructing-tuning method comes from task labels (e.g., relation types for relation extraction, entity types for NER), rather than instructions. By focusing on task-level capabilities and using NER as a case study, we demonstrate that it is possible to devise a tuning recipe that not only closes the performance gap but also surpasses ChatGPT. Wang et al. (2023a) also explore instruction-tuning for information extraction tasks. However, their method relies solely on supervised datasets and yields subpar performance when compared to ChatGPT.</p>
<h1>3 Mission-Focused Instruction Tuning</h1>
<p>Instruction tuning (Ouyang et al., 2022; Wei et al., 2021) is a method through which pretrained autoregressive language models are finetuned to follow natural language instructions and generate responses. Existing work focuses on tuning models to do diverse tasks (Taori et al., 2023; Chiang et al., 2023). In contrast, we introduce a general recipe for mission-focused instruction tuning, where the pretrained model is tuned for a broad application class such as open information extraction.</p>
<p>In this paper, we conduct a case study on the NER task, as it is one of the fundamental tasks for knowledge extraction from text. The objective is to learn a model $f:(\mathcal{X} \times \mathcal{T}) \rightarrow \mathcal{Y}$, where $\mathcal{X}$ represents the set of inputs, $\mathcal{T}$ denotes a predefined set of entity types, and $\mathcal{Y}$ represents the set of entities of a specific type in the given input.</p>
<h3>3.1 Data Construction</h3>
<p>A typical instruction-tuning example is made of three parts, including instruction, input, and output, where the diversity of instruction causes the models to follow a wide range of task instructions. However, for mission-focused instruction tuning, our goal is to tune the model to maximally generalize across semantic types and domains for the targeted application class. Therefore, we focus on increasing the diversity of input rather than instruction.</p>
<p>While earlier work (Jung et al., 2023) employs language models to generate inputs, these models typically assume that the domains of test data are known and prompt LMs to generate data for each domain. This method falls short when applied to distillation for a broad application class, where the distribution of test data is unknown. Consequently, it is challenging to generate inputs from LMs that provide wide coverage of the test domains.</p>
<p>To address this limitation, we propose an alternative: directly sampling inputs from a large corpus across diverse domains, and then using an LLM to generate outputs. In this paper, we sample inputs from the Pile corpus (Gao et al., 2020), which compiles 22 distinct English subdatasets. We chunk the articles in Pile to passages of a max length of 256 tokens and randomly sample 50 K passages as the inputs. Subsequently, we use ChatGPT (gpt-3.5-turbo-0301) to generate entity mentions and their associated types based on the sampled passages. To ensure stability, we set the generation temperature to 0 . The specific prompt for constructing the data is shown in Fig. 1. In this prompt, we do not specify the set of entity types of interest, allowing the LLM to generate outputs encompassing a broad coverage of entity types.</p>
<p>Data statistics. After filtering out unparseable outputs and inappropriate entities, including nonEnglish entities and those classified under 'ELSE' categories, such as None, NA, MISC, and ELSE, our dataset comprises 45,889 input-output pairs, encompassing 240,725 entities and 13,020 distinct entity types. We divide the entity types according to frequency and show the top 10 entity types in each range in Tab. 1. The distribution of these entity types exhibits a heavy tail, where the top</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Frequency</th>
<th style="text-align: left;">Entity types</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Top 1\%</td>
<td style="text-align: left;">person, organization, location, date, concept, product, event, technology, group, medical</td>
</tr>
<tr>
<td style="text-align: center;">(74\%)</td>
<td style="text-align: left;">condition, ...</td>
</tr>
<tr>
<td style="text-align: center;">$1 \%-10 \%$</td>
<td style="text-align: left;">characteristic, research, county, module, unit, feature, cell, package, anatomical structure,</td>
</tr>
<tr>
<td style="text-align: center;">(19\%)</td>
<td style="text-align: left;">equipment, ...</td>
</tr>
<tr>
<td style="text-align: center;">$10 \%-100 \%$</td>
<td style="text-align: left;">attribute value, pokemon, immune response, physiology, animals, cell feature, FAC, input</td>
</tr>
<tr>
<td style="text-align: center;">(7\%)</td>
<td style="text-align: left;">device, ward, broadcast, ...</td>
</tr>
</tbody>
</table>
<p>Table 1: Examples of entities across different frequency ranges - top 1\%, 1-10\%, and 10-100\%, along with the percentage of total frequencies for each range.
$1 \%$ of entities account for $74 \%$ of total frequencies. We find that the generated data contain entity types from various domains, ranging from the general domain (e.g., PERSON) to the clinical domain (e.g., MEDICAL CONDITION). Moreover, we observe variations in granularity among the entity types. E.g., COUNTY is the subset of LOCATION, and INPUT DEVICE is a subset of PRODUCT. These data characteristics offer extensive coverage of entity types, making them suitable for distilling capabilities from LLMs across various domains.</p>
<p>Definition-based data construction. Besides entity types, we also prompt ChatGPT to generate entity mentions and define their types using short sentences. To do so, we simply change the prompt in Fig. 1 from "extract all entities and identify their entity types" to "extract all entities and concepts, and define their type using a short sentence". This method generates a much more diverse set of 353,092 entity types and leads to a tuned model that is less sensitive to entity type paraphrasing (Section 5.5), but performs worse on standard NER benchmarks (Section 5.2).</p>
<h1>3.2 InStruction Tuning</h1>
<p>After obtaining the data, we apply instruction tuning to smaller models to distill for a broad application class, e.g., diverse entity types in NER. Our template, as shown in Fig. 2, adopts a conversation-style tuning format. In this approach, the language model is presented with a passage $\boldsymbol{X}<em i="i">{\text {passage }}$ as input. Then, for each entity type $\boldsymbol{t}</em>}$ that appears in the output, we transform it into a natural language query "What describes $\boldsymbol{t<em i="i">{i}$ ?" Subsequently, we tune the LM to generate a structured output $\boldsymbol{y}</em>}$ in the form of a JSON list containing all entities of $\boldsymbol{t<em 1="1">{i}$ in the passage. We consider $\boldsymbol{y}</em>$ as gold tokens and apply a language modeling objective on these tokens. Our preliminary experiments show that conversation-style tuning is better than traditional NER-style tuning adopted by Wang et al. (2023a); Sun et al. (2023).}, \ldots, \boldsymbol{y}_{T</p>
<p>Besides one entity type per query, we also consider combining all entity types in a single query, requiring the model to output all entities in a single response. Detailed results and discussions can be found in Section 5.2.</p>
<p>Negative sampling. Our data construction process follows an open-world assumption where we allow the model to generate entity types that have appeared in the passage. However, the generated data do not account for entity types that are not mentioned in the passage, i.e., negative entity types. As a result, it is challenging for us to apply a model trained on this data to a closed-world setting, where one may ask for entity types that do not exist in the passage. To address this potential mismatch, we sample negative entity types from the collection of all entity types that do not appear in the passage as queries and set the expected outputs as empty JSON lists. The sampling of negative entity types</p>
<p>is done with a probability proportional to the frequency of entity types in the entire dataset. This approach greatly improves the instruction tuning results, as shown in Section 5.4.</p>
<p>Supervised finetuning. When we have additional human annotations, model performance can be further improved with supervised data. However, a significant challenge arises when training with multiple datasets, as there might be discrepancies in label definitions among these datasets, resulting in label conflicts. For instance, some datasets like ACE (Walker et al., 2006) consider personal pronouns (e.g., she, he) as PERSON, while other datasets like multiNERD (Tedeschi \&amp; Navigli, 2022) do not include pronouns.</p>
<p>To address this issue, we propose to use datasetspecific instruction tuning templates to harmonize the discrepancies in label definitions, as illustrated in Fig. 3. Specifically, we augment the input with an additional field denoting the dataset name $\boldsymbol{D}$. By doing so, the model can learn the dataset-specific semantics of labels. During inference, we use the respective dataset name in the prompt for the supervised setting, whereas we omit the dataset field from the prompt in the zero-shot setting.</p>
<h2>4 Universal NER Benchmark</h2>
<p>To conduct a comprehensive evaluation of NER models across diverse domains and entity types, we collect the largest NER benchmark to date. This benchmark encompasses 43 NER datasets across 9 domains, including general, biomedical, clinical, STEM, programming, social media, law, finance, and transportation domains. An overview of data distribution is shown in Fig. 4. Detailed dataset statistics are available in Appendix Tab. 6.</p>
<p>Dataset processing. To make the entity types semantically meaningful to LLMs, we conduct a manual inspection of the labels and convert the original labels into natural language formats. For instance, we replace PER with PERSON. While we try to collect a broad coverage of NER datasets, we do not use all entity types. This is because some entity types (e.g., ElSE) are not coming from consistent sources across the different datasets. Their annotations often come from different ontologies for different purposes. The choices of entity types and their annotation guidelines are not optimized for holistic or comprehensive assessments, which renders them suboptimal for use as a "ground truth" to evaluate a universal NER model. Therefore, we remove those labels from the datasets. In addition, some datasets are at the document level and contain very long contexts, which might exceed the input length limit of models. Therefore, we split all instances in document-level datasets into sentence-level ones.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 3: The dataset-specific instruction tuning template. We add the dataset name $\boldsymbol{D}$ (colored in red) as part of the input to resolve conflicts in label definitions.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 4: Distribution of UniNER benchmark.</p>
<h1>5 EXPERIMENTS</h1>
<p>This section presents experimental evaluations of UniversalNER. We start by outlining experimental settings (Section 5.1), followed by presenting the results on both distillation and supervised settings</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" />
(a) Comparisons of zero-shot models on different domains. Our distilled models achieve better results than ChatGPT in all evaluated domains.
<img alt="img-3.jpeg" src="img-3.jpeg" />
(b) Comparisons between UniNER-7B and two variants. UniNER-7B-definition is distilled on Pile data prompted with entity type definitions. UniNER-7B-all-in-one is tuned with the template where all entity types are asked in one query.
(Sections 5.2 and 5.3). Finally, we conduct analysis (Section 5.4) and case study (Section 5.5) to provide deeper insights into the model's performance.</p>
<h1>5.1 EXPERIMENTAL SETTINGS</h1>
<p>Model configurations. We train models based on LLaMA ${ }^{2}$ (Touvron et al., 2023a) following the training schedule of Chiang et al. (2023) for a fair comparison. Considering the large size of certain test datasets, we perform evaluation by sampling up to 200,000 passage-query pairs from each dataset. We use strict entity-level micro- $F_{1}$ in evaluation, requiring both the entity type and boundary to exactly match the ground truth.</p>
<p>Compared models. We compare our model (UniNER) against the following models: (1) ChatGPT (gpt-3.5-turbo-0301). We use the prompting template in Ye et al. (2023) for NER. (2) Vicuna (Chiang et al., 2023) is finetuned with ChatGPT conversations, using LLaMA as the base model. (3) InstructUIE (Wang et al., 2023a) is a supervised model finetuned on diverse information extraction datasets, employing a unified natural language generation objective. It adopts Flan-T5 11B (Chung et al., 2022) as the base model.</p>
<h3>5.2 DISTILLATION</h3>
<p>We first evaluate the models in a zero-shot setting. We compare the performance of ChatGPT, Vicuna, and our model UniNER, which is distilled from ChatGPT NER annotations on Pile without human-labeled datasets in training. Results are shown in Fig. 5a. ${ }^{3}$ We observe that our distilled</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>models, namely UniNER-7B and UniNER-13B, outperform ChatGPT in terms of average $F_{1}$. The average $F_{1}$ scores of UniNER-7B and UniNER-13B are $41.7 \%$ and $43.4 \%$, respectively, compared to $34.9 \%$ for ChatGPT. This demonstrates that our proposed targeted distillation from diverse inputs yields models that have superior performance on a broad application class while maintaining a relatively small model size. Additionally, UniNER-13B exhibits better performance compared to UniNER-7B, indicating that fine-tuning on larger models may lead to improved generalization. In terms of domains, both UniNER-7B and UniNER-13B outperform ChatGPT on all domains, showing that the improvements exist across various domains.</p>
<p>We further compare different variations of UniNER, including (1) UniNER-all-in-one, where the extraction of all entity types are combined into one query and response, and (2) UniNER-definition, where queries in instruction tuning data use entity type definitions generated by ChatGPT instead of entity types. Results are shown in Fig. 5b. We observe that both UniNER-all-in-one and UniNER-definition underperform UniNER-type by $3.3 \%$ and $11.8 \%$ on average, respectively. The UniNER-definition variant's decreased performance could be due to its lower consistency with the evaluation datasets, which all adopt words or short phrases as labels instead of sentences. The performance disparity in the UniNER-all-in-one variant can be potentially attributed to the attention distribution and task complexity. When the model is required to handle multiple entity types within a single query, it might disperse its attention across these varied types, possibly resulting in less accurate identification for each individual type. Conversely, by decomposing the task into several simpler ones, each focusing on one entity type at a time, the model might be better equipped to handle the complexity, thus yielding more accurate results.</p>
<h3>5.3 Supervised Finetuning</h3>
<p>We study whether our models can be further improved using additional human annotations. We compare the performance of ChatGPT, Vicuna, InstructUIE (Wang et al., 2023a) ${ }^{4}$, and UniNER.</p>
<p>Out-of-domain evaluation. We first study whether supervised finetuning leads to better generalization on unseen data. We follow InstructUIE to exclude two datasets CrossNER (Liu et al., 2021) and MIT (Liu et al., 2013) for out-of-domain evaluation, and fine-tune our model using training splits of the remaining datasets in the universal NER benchmark. Results are shown in Tab. 3. Notably, without any fine-tuning, instruction-tuned UniNER 7B and 13B already surpass ChatGPT, Vicuna, and the supervised fine-tuned InstructUIE-11B by a large margin. If we train our model from scratch only using the supervised data, it achieves an average $F_{1}$ of $57.2 \%$. Continual fine-tuning UniNER-7B using the supervised data achieves the best average $F_{1}$ of $60.0 \%$. These findings suggest that the models' generalization can be further improved with additional human-annotated data.</p>
<p>In-domain evaluation. We then study the performance of UniNER in an in-domain supervised setting, where we fine-tune UniNER-7B using the same training data as InstructUIE (Wang et al., 2023a). Results are shown in Tab. 2. Our UniNER-7B achieves an average $F_{1}$ of $84.78 \%$ on the 20 datasets, surpassing both BERT-base and InstructUIE-11B by $4.69 \%$ and $3.62 \%$, respectively. This experiment demonstrates the effectiveness of our model in the supervised setting.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Movie</th>
<th>Restaurant</th>
<th>AI</th>
<th>Literature</th>
<th>Music</th>
<th>Politics</th>
<th>Science</th>
<th>Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td>Zero-shot</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Vicuna-7B</td>
<td>6.0</td>
<td>5.3</td>
<td>12.8</td>
<td>16.1</td>
<td>17.0</td>
<td>20.5</td>
<td>13.0</td>
<td>13.0</td>
</tr>
<tr>
<td>Vicuna-13B</td>
<td>0.9</td>
<td>0.4</td>
<td>22.7</td>
<td>22.7</td>
<td>26.6</td>
<td>27.2</td>
<td>22.0</td>
<td>17.5</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>5.3</td>
<td>32.8</td>
<td>52.4</td>
<td>39.8</td>
<td>$\mathbf{6 6 . 6}$</td>
<td>$\mathbf{6 8 . 5}$</td>
<td>$\mathbf{6 7 . 0}$</td>
<td>47.5</td>
</tr>
<tr>
<td>UniNER-7B</td>
<td>42.4</td>
<td>31.7</td>
<td>53.5</td>
<td>59.4</td>
<td>65.0</td>
<td>60.8</td>
<td>61.1</td>
<td>53.4</td>
</tr>
<tr>
<td>UniNER-13B</td>
<td>$\mathbf{4 8 . 7}$</td>
<td>$\mathbf{3 6 . 2}$</td>
<td>$\mathbf{5 4 . 2}$</td>
<td>$\mathbf{6 0 . 9}$</td>
<td>64.5</td>
<td>61.4</td>
<td>63.5</td>
<td>$\mathbf{5 5 . 6}$</td>
</tr>
<tr>
<td>In-domain supervised</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>InstructUIE-11B</td>
<td>-</td>
<td>-</td>
<td>48.4</td>
<td>48.8</td>
<td>54.4</td>
<td>49.9</td>
<td>49.4</td>
<td>-</td>
</tr>
<tr>
<td>UniNER-7B (sup. only)</td>
<td>54.2</td>
<td>16.0</td>
<td>62.3</td>
<td>$\mathbf{6 7 . 4}$</td>
<td>69.0</td>
<td>64.5</td>
<td>66.9</td>
<td>57.2</td>
</tr>
<tr>
<td>UniNER-7B (inst-tuned + sup.)</td>
<td>$\mathbf{6 1 . 2}$</td>
<td>$\mathbf{3 5 . 2}$</td>
<td>$\mathbf{6 2 . 9}$</td>
<td>64.9</td>
<td>$\mathbf{7 0 . 6}$</td>
<td>$\mathbf{6 6 . 9}$</td>
<td>$\mathbf{7 0 . 8}$</td>
<td>$\mathbf{6 1 . 8}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Out-of-domain evaluation on datasets from <em>Wang et al. (2023a)</em>. sup. only denotes a variant of UniNER-7B, trained from scratch using in-domain supervised data only and evaluated on out-of-domain datasets.</p>
<h3>5.4 Analysis</h3>
<table>
<thead>
<tr>
<th>Strategy</th>
<th>Movie</th>
<th>Restaurant</th>
<th>AI</th>
<th>Literature</th>
<th>Music</th>
<th>Politics</th>
<th>Science</th>
<th>Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>19.1</td>
<td>19.1</td>
<td>25.1</td>
<td>39.5</td>
<td>42.7</td>
<td>48.9</td>
<td>26.2</td>
<td>31.5</td>
</tr>
<tr>
<td>Uniform</td>
<td>42.5</td>
<td>29.0</td>
<td>42.5</td>
<td>53.3</td>
<td>57.4</td>
<td>56.8</td>
<td>52.6</td>
<td>47.7</td>
</tr>
<tr>
<td>Frequency</td>
<td>42.4</td>
<td>31.7</td>
<td>53.5</td>
<td>59.4</td>
<td>65.0</td>
<td>60.8</td>
<td>61.1</td>
<td>53.4</td>
</tr>
</tbody>
</table>
<p>Table 4: Ablation study on negative sampling strategies for UniNER-7B. All models are instruction-tuned on Pile.</p>
<p>Negative sampling strategies. We experiment with different negative sampling strategies in instruction tuning, including (1) no negative sampling, (2) uniform sampling where entity types are randomly sampled with equal probability for each one, and (3) frequency-based sampling where we sample entity types with probabilities proportional to their frequency in the constructed dataset. Results are shown in Tab. 4. Among the approaches tested, frequency-based sampling yielded the best results, outperforming no sampling and uniform sampling by 21.9% and 5.7%, respectively. These findings highlight the crucial role of negative sampling in instruction tuning, with frequency-based sampling emerging as the most effective method for enhancing model performance in our study.</p>
<p>Dataset-specific template. We compare the results of our dataset-specific instruction tuning template and the original template in the supervised setting. As shown in Fig. 6, we find that the data-specific template outperforms the original template on most datasets. To gain deeper insights into the improvements achieved, we further divide the datasets into two categories: those with label (entity type) overlap with other datasets and those without overlap. Our analysis reveals that datasets with label overlap demonstrate more substantial improvements.</p>
<p>To explore this further, we measure $F_{1}$ score across all evaluation datasets and calculate the difference. Apart from the long-tail entity types that manifest a high variance in results, we identify two entity types where the dataset-specific template outperforms the original template by over 10%: FACILITY.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Different in $F_{1}$ between data-specific and original templates in the supervised setting. Orange and Blue mark datasets with/without label overlap with other datasets, respectively.</p>
<table>
<thead>
<tr>
<th>Partial match</th>
<th>Model</th>
<th>Movie</th>
<th>Restaurant</th>
<th>AI</th>
<th>Literature</th>
<th>Music</th>
<th>Politics</th>
<th>Science</th>
<th>Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td>No</td>
<td>ChatGPT</td>
<td>5.3</td>
<td>32.8</td>
<td>52.4</td>
<td>39.8</td>
<td>66.6</td>
<td>68.5</td>
<td>67.0</td>
<td>47.5</td>
</tr>
<tr>
<td></td>
<td>UniNER-7B</td>
<td>42.4</td>
<td>31.7</td>
<td>53.5</td>
<td>59.4</td>
<td>65.0</td>
<td>60.8</td>
<td>61.1</td>
<td>53.4</td>
</tr>
<tr>
<td></td>
<td>UniNER-7B w/ sup</td>
<td>61.2</td>
<td>35.2</td>
<td>62.9</td>
<td>64.9</td>
<td>70.6</td>
<td>66.9</td>
<td>70.8</td>
<td>61.8</td>
</tr>
<tr>
<td>Yes</td>
<td>ChatGPT</td>
<td>5.9</td>
<td>40.1</td>
<td>55.7</td>
<td>42.8</td>
<td>70.2</td>
<td>71.7</td>
<td>70.1</td>
<td>50.9</td>
</tr>
<tr>
<td></td>
<td>UniNER-7B</td>
<td>46.9</td>
<td>40.3</td>
<td>57.7</td>
<td>62.7</td>
<td>62.9</td>
<td>63.2</td>
<td>63.3</td>
<td>56.7</td>
</tr>
<tr>
<td></td>
<td>UniNER-7B w/ sup</td>
<td>65.5</td>
<td>39.4</td>
<td>66.2</td>
<td>67.2</td>
<td>72.7</td>
<td>68.9</td>
<td>73.4</td>
<td>64.8</td>
</tr>
</tbody>
</table>
<p>Table 5: Allowing partial match between the prediction and the gold that has overlap increases the results. When it is allowed, any partial match is regarded as half correct (counted as 0.5 in true positive) when computing $F_{1}$.
(22.0%) and TIME (12.4%). Intriguingly, both labels exhibit inconsistencies in their definitions across various datasets. The FACILITY label has been annotated on pronouns (e.g., it, which) as entities in ACE datasets but are excluded in OntoNotes. The TIME label denotes well-defined time intervals (e.g., Christmas) in MultiNERD, but may encompass any general time expressions (e.g., 3 pm) in OntoNotes. This finding suggests that the improvements provided by the data-specific template are particularly effective in resolving label conflicts.</p>
<p>Evaluation with partial match. While using strict $F_{1}$ as an evaluation metric, we notice that it may underestimate the zero-shot learning capabilities of NER models. In particular, strict $F_{1}$ penalizes slight misalignments in the boundaries of the extracted entities, which may not necessarily indicate an incorrect understanding of the text. For instance, given the sentence any asian cuisine around and the entity type CUISINE, UniNER extracts asian cuisine as the named entity, while the ground truth only labels asian as the correct entity. However, the models prediction can still be viewed as correct, even though it is deemed incorrect by strict $F_{1}$. To better estimate the zero-shot abilities, we also consider partial match (Segura-Bedmar et al., 2013) in evaluation. In this context, a prediction that exhibits word overlap with the ground truth is regarded as half correct (counted as 0.5 in true positive) when computing $F_{1}$. Results are shown in Tab. 5. We find that allowing partial match consistently improves the results. Besides, our models is still the best-performing model on average.</p>
<h1>5.5 CASE STUDY</h1>
<p>Sensitivity to entity type paraphrasing. One type of entity can be expressed in multiple ways, so it is essential for our model to give consistent predictions given entity types with similar meanings. An example of sensitivity analysis is present in Fig. 7. We observe that UniNER-7B-type sometimes fails to recognize entities with similar semantic meanings. On the other hand, UniNER-7B-definition, despite performing worse on our Universal NER benchmark, exhibits robustness to entity type paraphrasing. It demonstrates that although using definitions may result in lower performance on standard NER benchmarks, it could yield improved performance for less populous entity types.</p>
<p>Recognition of diverse entity types. We present an example in Fig. 8 showcasing the capabilities of UniNER in recognizing various entities. Particularly, we focus on a novel domain of code and assess UniNER's ability to extract diverse types of entities within the code. Despite minor mistakes (e.g., from_pretrained is not identified as a method), this case study effectively demonstrates our model's capacity to capture entities of various types.</p>
<h2>6 CONCLUSION</h2>
<p>We present a targeted distillation approach with mission-focused instruction tuning. Using NER as a case study, we train smaller and more efficient models for open-domain NER. The proposed method successfully distills ChatGPT into a smaller model UniversalNER, achieving remarkable NER accuracy across a wide range of domains and entity types without direct supervision. These models not only retain ChatGPT's capabilities but also surpass it and other state-of-the-art systems in NER performance.</p>
<h1>ACKNOWLEDGEMENT</h1>
<p>Wenxuan Zhou and Muhao Chen were supported by the NSF Grants IIS 2105329 and ITE 2333736.</p>
<h2>REFERENCES</h2>
<p>Rami Al-Rfou, Vivek Kulkarni, Bryan Perozzi, and Steven Skiena. Polyglot-ner: Massive multilingual named entity recognition. In Proceedings of the 2015 SIAM International Conference on Data Mining, pp. 586-594. SIAM, 2015.</p>
<p>Victoria Arranz, Khalid Choukri, Montse Cuadros, Aitor Garca Pablos, Lucie Gianola, Cyril Grouin, Manuel Herranz, Patrick Paroubek, and Pierre Zweigenbaum. MAPA project: Ready-to-go open-source datasets and deep learning technology to remove identifying information from text documents. In Proceedings of the Workshop on Ethical and Legal Issues in Human Language Technologies and Multilingual De-Identification of Sensitive Data In Language Resources within the 13th Language Resources and Evaluation Conference, pp. 64-72, Marseille, France, June 2022. European Language Resources Association. URL https://aclanthology.org/2022.legal-1. 12 .</p>
<p>Ting Wai Terence Au, Vasileios Lampos, and Ingemar Cox. E-NER  an annotated named entity recognition corpus of legal text. In Nikolaos Aletras, Ilias Chalkidis, Leslie Barrett, Ctlina Goant, and Daniel Preotiuc-Pietro (eds.), Proceedings of the Natural Legal Language Processing Workshop 2022, pp. 246-255, Abu Dhabi, United Arab Emirates (Hybrid), December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.nllp-1.22. URL https: //aclanthology.org/2022.nllp-1.22.</p>
<p>Pei Chen, Haotian Xu, Cheng Zhang, and Ruihong Huang. Crossroads, buildings and neighborhoods: A dataset for fine-grained location recognition. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 3329-3339, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.243. URL https://aclanthology.org/2022. naacl-main. 243 .</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality, March 2023. URL https: //vicuna.lmsys.org.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.</p>
<p>Nigel Collier and Jin-Dong Kim. Introduction to the bio-entity recognition task at jnlpba. In Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP), pp. 73-78, 2004.</p>
<p>Leon Derczynski, Kalina Bontcheva, and Ian Roberts. Broad Twitter corpus: A diverse named entity recognition resource. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pp. 1169-1179, Osaka, Japan, December 2016. The COLING 2016 Organizing Committee. URL https://aclanthology.org/C16-1111.</p>
<p>Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han, Pengjun Xie, Haitao Zheng, and Zhiyuan Liu. Few-NERD: A few-shot named entity recognition dataset. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 3198-3213, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.248. URL https://aclanthology.org/2021.acl-long. 248.</p>
<p>Rezarta Islamaj Doan, Robert Leaman, and Zhiyong Lu. Ncbi disease corpus: a resource for disease name recognition and concept normalization. Journal of biomedical informatics, 47:1-10, 2014.</p>
<p>Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S Weld. Open information extraction from the web. Communications of the ACM, 51(12):68-74, 2008.</p>
<p>Annemarie Friedrich, Heike Adel, Federico Tomazic, Johannes Hingerl, Renou Benteau, Anika Maruscyk, and Lukas Lange. The sofc-exp corpus and neural approaches to information extraction in the materials science domain, 2020.</p>
<p>Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.</p>
<p>Felix Grezes, Sergi Blanco-Cuaresma, Thomas Allen, and Tirthankar Ghosal. Overview of the first shared task on detecting entities in the astrophysics literature (DEAL). In Proceedings of the first Workshop on Information Extraction from Scientific Publications, pp. 1-7, Online, November 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.wiesp-1. 1 .</p>
<p>Yu Gu, Sheng Zhang, Naoto Usuyama, Yonas Woldesenbet, Cliff Wong, Praneeth Sanapathi, Mu Wei, Naveen Valluri, Erika Strandberg, Tristan Naumann, and Hoifung Poon. Distilling large language models for biomedical knowledge extraction: A case study on adverse drug events, 2023.</p>
<p>Runwei Guan, Ka Lok Man, Feifan Chen, Shanliang Yao, Rongsheng Hu, Xiaohui Zhu, Jeremy Smith, Eng Gee Lim, and Yutao Yue. Findvehicle and vehiclefinder: A ner dataset for natural language-based vehicle retrieval and a keyword-based cross-modal vehicle retrieval system. arXiv preprint arXiv:2304.10893, 2023.</p>
<p>Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. The false promise of imitating proprietary llms, 2023.</p>
<p>Sam Henry, Kevin Buchan, Michele Filannino, Amber Stubbs, and Ozlem Uzuner. 2018 n2c2 shared task on adverse drug events and medication extraction in electronic health records. Journal of the American Medical Informatics Association, 27(1):3-12, 2020.</p>
<p>Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.</p>
<p>Cheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 8003-8017, Toronto, Canada, July 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.findings-acl.507.</p>
<p>Yu-Ming Hsieh, Yueh-Yin Shih, and Wei-Yun Ma. Converting the Sinica Treebank of Mandarin Chinese to Universal Dependencies. In Proceedings of the 16th Linguistic Annotation Workshop (LAW-XVI) within LREC2022, pp. 23-30, Marseille, France, June 2022. European Language Resources Association. URL https://aclanthology.org/2022.law-1.4.</p>
<p>Sarthak Jain, Madeleine van Zuylen, Hannaneh Hajishirzi, and Iz Beltagy. SciREX: A challenge dataset for document-level information extraction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7506-7516, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.670. URL https://aclanthology. org/2020.acl-main. 670 .</p>
<p>Jaehun Jung, Peter West, Liwei Jiang, Faeze Brahman, Ximing Lu, Jillian Fisher, Taylor Sorensen, and Yejin Choi. Impossible distillation: from low-quality model to high-quality dataset \&amp; model for summarization and paraphrasing, 2023.</p>
<p>J-D Kim, Tomoko Ohta, Yuka Tateisi, and Jun'ichi Tsujii. Genia corpus-a semantically annotated corpus for bio-textmining. Bioinformatics, 19(suppl_1):i180-i182, 2003.</p>
<p>Martin Krallinger, Obdulia Rabal, Florian Leitner, Miguel Vazquez, David Salgado, Zhiyong Lu, Robert Leaman, Yanan Lu, Donghong Ji, Daniel M Lowe, et al. The chemdner corpus of chemicals and drugs and its annotation principles. Journal of cheminformatics, 7(1):1-17, 2015.</p>
<p>Chaitanya Kulkarni, Wei Xu, Alan Ritter, and Raghu Machiraju. An annotated corpus for machine reading of instructions in wet lab protocols. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp. 97-106, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2016. URL https://aclanthology.org/N18-2016.</p>
<p>Aman Kumar and Binil Starly. "fabner": information extraction from manufacturing process science domain literature using named entity recognition. Journal of Intelligent Manufacturing, 33(8): 2393-2407, 2022.</p>
<p>Bo Li, Gexiang Fang, Yang Yang, Quansen Wang, Wei Ye, Wen Zhao, and Shikun Zhang. Evaluating chatgpt's information extraction capabilities: An assessment of performance, explainability, calibration, and faithfulness. arXiv preprint arXiv:2304.11633, 2023.</p>
<p>Jiao Li, Yueping Sun, Robin J Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn J Mattingly, Thomas C Wiegers, and Zhiyong Lu. Biocreative v cdr task corpus: a resource for chemical disease relation extraction. Database, 2016, 2016.</p>
<p>Jingjing Liu, Panupong Pasupat, Scott Cyphers, and Jim Glass. Asgard: A portable architecture for multilingual dialogue systems. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 8386-8390. IEEE, 2013.</p>
<p>Zihan Liu, Yan Xu, Tiezheng Yu, Wenliang Dai, Ziwei Ji, Samuel Cahyawijaya, Andrea Madotto, and Pascale Fung. Crossner: Evaluating cross-domain named entity recognition. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 13452-13460, 2021.</p>
<p>Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 3219-3232, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/ D18-1360. URL https://aclanthology.org/018-1360.</p>
<p>Ling Luo, Po-Ting Lai, Chih-Hsuan Wei, Cecilia N Arighi, and Zhiyong Lu. Biored: a rich biomedical relation extraction dataset. Briefings in Bioinformatics, 23(5):bbac282, 2022.</p>
<p>Alexis Mitchell, Stephanie Strassel, Shudong Huang, and Ramez Zakhary. Ace 2004 multilingual training corpus. Linguistic Data Consortium, Philadelphia, 1:1-1, 2005.</p>
<p>Danielle L Mowery, Sumithra Velupillai, Brett R South, Lee Christensen, David Martinez, Liadh Kelly, Lorraine Goeuriot, Noemie Elhadad, Sameer Pradhan, Guergana Savova, et al. Task 2: Share/clef ehealth evaluation lab 2014. In Proceedings of CLEF 2014, 2014.</p>
<p>Benjamin Nye, Junyi Jessy Li, Roma Patel, Yinfei Yang, Iain J Marshall, Ani Nenkova, and Byron C Wallace. A corpus with multi-level annotations of patients, interventions and outcomes to support language processing for medical literature. In Proceedings of the conference. Association for Computational Linguistics. Meeting, volume 2018, pp. 197. NIH Public Access, 2018.</p>
<p>OpenAI. Gpt-4 technical report, 2023.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, et al. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, 2022.</p>
<p>Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng Ji. Cross-lingual name tagging and linking for 282 languages. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1946-1958, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1178. URL https://aclanthology.org/P17-1178.</p>
<p>Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.</p>
<p>Nadeesha Perera, Matthias Dehmer, and Frank Emmert-Streib. Named entity recognition and relation detection for biomedical information extraction. Frontiers in cell and developmental biology, pp. 673, 2020.</p>
<p>Sampo Pyysalo and Sophia Ananiadou. Anatomical entity mention recognition at literature scale. Bioinformatics, 30(6):868-875, 2014.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations, 2021.</p>
<p>David Schindler, Felix Bensmann, Stefan Dietze, and Frank Krger. Somesci-a 5 star open data gold standard knowledge graph of software mentions in scientific articles. In Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management, pp. 4574-4583, 2021.</p>
<p>Isabel Segura-Bedmar, Paloma Martnez Fernndez, and Mara Herrero Zazo. Semeval-2013 task 9: Extraction of drug-drug interactions from biomedical texts (ddiextraction 2013). Association for Computational Linguistics, 2013.</p>
<p>Agam Shah, Ruchit Vithani, Abhinav Gullapalli, and Sudheer Chava. Finer: Financial named entity recognition dataset and weak-supervision model. arXiv preprint arXiv:2302.11157, 2023.</p>
<p>Larry Smith, Lorraine K Tanabe, Cheng-Ju Kuo, I Chung, Chun-Nan Hsu, Yu-Shi Lin, Roman Klinger, Christoph M Friedrich, Kuzman Ganchev, Manabu Torii, et al. Overview of biocreative ii gene mention recognition. Genome biology, 9(2):1-19, 2008.</p>
<p>Ryan Smith, Jason A Fries, Braden Hancock, and Stephen H Bach. Language models in the loop: Incorporating prompting into weak supervision. arXiv preprint arXiv:2205.02318, 2022.</p>
<p>Amber Stubbs, Christopher Kotfila, and zlem Uzuner. Automated systems for the de-identification of longitudinal clinical narratives: Overview of 2014 i2b2/uthealth shared task track 1. Journal of biomedical informatics, 58:S11-S19, 2015.</p>
<p>Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner. Evaluating temporal relations in clinical text: 2012 i2b2 challenge. Journal of the American Medical Informatics Association, 20(5):806-813, 2013.</p>
<p>Xiaofei Sun, Linfeng Dong, Xiaoya Li, Zhen Wan, Shuhe Wang, Tianwei Zhang, Jiwei Li, Fei Cheng, Lingjuan Lyu, Fei Wu, and Guoyin Wang. Pushing the limits of chatgpt on nlp tasks, 2023.</p>
<p>Jeniya Tabassum, Mounica Maddela, Wei Xu, and Alan Ritter. Code and named entity recognition in StackOverflow. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4913-4926, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.443. URL https://aclanthology.org/2020.acl-main. 443.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.</p>
<p>Simone Tedeschi and Roberto Navigli. MultiNERD: A multilingual, multi-genre and fine-grained dataset for named entity recognition (and disambiguation). In Findings of the Association for Computational Linguistics: NAACL 2022, pp. 801-812, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-naacl.60. URL https://aclanthology.org/2022.findings-naacl.60.</p>
<p>Simone Tedeschi, Valentino Maiorca, Niccol Campolungo, Francesco Cecconi, and Roberto Navigli. WikiNEuRal: Combined neural and knowledge-based silver data creation for multilingual NER. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 2521-2533, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10. 18653/v1/2021.findings-emnlp.215. URL https://aclanthology.org/2021.findings-emnlp. 215 .</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023a.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023b.</p>
<p>Asahi Ushio, Leonardo Neves, Vitor Silva, Francesco. Barbieri, and Jose Camacho-Collados. Named Entity Recognition in Twitter: A Dataset and Analysis on Short-Term Temporal Shifts. In The 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing, Online, November 2022. Association for Computational Linguistics.</p>
<p>zlem Uzuner, Yuan Luo, and Peter Szolovits. Evaluating the state-of-the-art in automatic deidentification. Journal of the American Medical Informatics Association, 14(5):550-563, 2007.</p>
<p>zlem Uzuner, Brett R South, Shuying Shen, and Scott L DuVall. 2010 i2b2/va challenge on concepts, assertions, and relations in clinical text. Journal of the American Medical Informatics Association, 18(5):552-556, 2011.</p>
<p>Christopher Walker, Stephanie Strassel, Julie Medero, and Kazuaki Maeda. Ace 2005 multilingual training corpus. Linguistic Data Consortium, Philadelphia, 57:45, 2006.</p>
<p>Xiao Wang, Weikang Zhou, Can Zu, Han Xia, Tianze Chen, Yuansen Zhang, Rui Zheng, Junjie Ye, Qi Zhang, Tao Gui, Jihua Kang, Jingsheng Yang, Siyuan Li, and Chunsai Du. Instructuie: Multi-task instruction tuning for unified information extraction, 2023a.</p>
<p>Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 5085-5109, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://aclanthology.org/ 2022.emnlp-main. 340 .</p>
<p>Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. How far can camels go? exploring the state of instruction tuning on open resources, 2023b.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1348413508, Toronto, Canada, July 2023c. Association for Computational Linguistics. doi: 10.18653/ v1/2023.acl-long.754. URL https://aclanthology.org/2023.acl-long. 754.</p>
<p>Zihan Wang, Jingbo Shang, Liyuan Liu, Lihao Lu, Jiacheng Liu, and Jiawei Han. Crossweigh: Training named entity tagger from imperfect annotations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 5157-5166, 2019.</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2021.</p>
<p>Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, et al. Zero-shot information extraction via chatting with chatgpt. arXiv preprint arXiv:2302.10205, 2023.</p>
<p>Ralph Weischedel, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Pradhan, Lance Ramshaw, Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, et al. Ontonotes release 5.0 ldc2013t19. Linguistic Data Consortium, Philadelphia, PA, 23:170, 2013.</p>
<p>Ania Wrblewska, Agnieszka Kaliska, Maciej Pawowski, Dawid Winiewski, Witold Sosnowski, and Agnieszka awrynowicz. Tasteset-recipe dataset and food entities recognition benchmark. arXiv preprint arXiv:2204.07775, 2022.</p>
<p>Yonghui Wu, Min Jiang, Jun Xu, Degui Zhi, and Hua Xu. Clinical named entity recognition using deep learning models. In AMIA annual symposium proceedings, volume 2017, pp. 1812. American Medical Informatics Association, 2017.</p>
<p>Junjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, et al. A comprehensive capability analysis of gpt-3 and gpt-3.5 series models. arXiv preprint arXiv:2303.10420, 2023.</p>
<h1>A APPENDIX</h1>
<h2>A. 1 CASE Study</h2>
<p>Sensitivity to entity type paraphrasing. One type of entity can be expressed in multiple different ways. In this scenario, it is essential for our model to give consistent predictions given entity types with similar meanings. An example of sensitivity analysis is present in Fig. 7. We observe that UniNER-7B-type sometimes fails to recognize entities with similar semantic meanings. On the other hand, UniNER-7B-definition, despite performing worse on our Universal NER benchmark, exhibits robustness to entity type paraphrasing. It demonstrates that although using definitions may result in lower performance on standard NER benchmarks, it could yield improved performance for less populous entity types.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Sensitivity Analysis of Entity Types</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Text: I'm visiting Los Angeles next week.</td>
</tr>
<tr>
<td style="text-align: left;">User: What describes city in the text?</td>
</tr>
<tr>
<td style="text-align: left;">UniNER-7B-type: ["Los Angeles"]</td>
</tr>
<tr>
<td style="text-align: left;">UniNER-7B-definition: ["Los Angeles"]</td>
</tr>
<tr>
<td style="text-align: left;">User: What describes place in the text?</td>
</tr>
<tr>
<td style="text-align: left;">UniNER-7B-type: []</td>
</tr>
<tr>
<td style="text-align: left;">UniNER-7B-definition: ["Los Angeles"]</td>
</tr>
<tr>
<td style="text-align: left;">User: What describes metropolis in the</td>
</tr>
<tr>
<td style="text-align: left;">text?</td>
</tr>
<tr>
<td style="text-align: left;">UniNER-7B-type: []</td>
</tr>
<tr>
<td style="text-align: left;">UniNER-7B-definition: ["Los Angeles"]</td>
</tr>
<tr>
<td style="text-align: left;">User: What describes urban area in the text?</td>
</tr>
<tr>
<td style="text-align: left;">UniNER-7B-type: []</td>
</tr>
<tr>
<td style="text-align: left;">UniNER-7B-definition: ["Los Angeles"]</td>
</tr>
<tr>
<td style="text-align: left;">User: What describes human settlement in</td>
</tr>
<tr>
<td style="text-align: left;">the text?</td>
</tr>
<tr>
<td style="text-align: left;">UniNER-7B-type: []</td>
</tr>
<tr>
<td style="text-align: left;">UniNER-7B-definition: ["Los Angeles"]</td>
</tr>
</tbody>
</table>
<p>Figure 7: Sensitivity of UniNER-7B on different entity types. Green and red mark correct and wrong predictions, respectively.</p>
<p>Recognition of diverse entity types The code snippet provided is for text generation using the Huggingface library:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPT2Tokenizer</span><span class="p">,</span> <span class="n">GPT2LMHeadModel</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>
<span class="n">input_text</span> <span class="o">=</span> <span class="s2">&quot;This,is,assimple,program&quot;</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">generated_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>

<p>Figure 8: A code snippet to assess UniNER's ability to extract diverse types of entities within the code.</p>
<p>In the ensuing conversations with UniNER, we present the queries and corresponding results:</p>
<div class="codehilite"><pre><span></span><code><span class="n">User</span><span class="o">:</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="n">describes</span><span class="w"> </span><span class="n">library</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">text</span><span class="o">?</span>
<span class="n">Assistant</span><span class="o">:</span><span class="w"> </span><span class="o">[</span><span class="s2">&quot;transformers&quot;</span><span class="o">]</span>
<span class="n">User</span><span class="o">:</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="n">describes</span><span class="w"> </span><span class="n">local</span><span class="w"> </span><span class="n">variable</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">text</span><span class="o">?</span>
<span class="n">Assistant</span><span class="o">:</span><span class="w"> </span><span class="o">[</span><span class="s2">&quot;input_text&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;input_ids&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;output&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;generated_text&quot;</span><span class="o">]</span>
<span class="n">User</span><span class="o">:</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="n">describes</span><span class="w"> </span><span class="n">global</span><span class="w"> </span><span class="n">variable</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">text</span><span class="o">?</span>
<span class="n">Assistant</span><span class="o">:</span><span class="w"> </span><span class="o">[]</span>
<span class="n">User</span><span class="o">:</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="n">describes</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">text</span><span class="o">?</span>
<span class="n">Assistant</span><span class="o">:</span><span class="w"> </span><span class="o">[</span><span class="s2">&quot;encode&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;generate&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;decode&quot;</span><span class="o">]</span>
<span class="n">User</span><span class="o">:</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="n">describes</span><span class="w"> </span><span class="kd">class</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">text</span><span class="o">?</span>
<span class="n">Assistant</span><span class="o">:[</span><span class="s2">&quot;GPT2Tokenizer&quot;</span><span class="o">,</span><span class="s2">&quot;GPT2LMHeadModel&quot;</span><span class="o">]</span>
</code></pre></div>

<p>Despite minor mistakes (e.g., from_pretrained is not identified as a method), this case study effectively demonstrates our model's capacity to capture entities of various types.</p>
<h1>B Full Evaluation Results</h1>
<p>Full results on ChatGPT, UniNER-7B-type, and UniNER-7B-sup+type are shown in Fig. 9.</p>
<h2>C Data Statistics</h2>
<p>We show the full dataset statistics in Universal NER in Tab. 6, including the number of instances in train/dev/test data, number of entity types, average number of tokens in input text, and the average number of entities in each instance.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 9: Full evaluation results of ChatGPT, UniNER-7B, and UniNER-7B w/ sup (joint training on both supervised and Pile-type data, MIT and CrossNER data are excluded in training).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Domain</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;"># train</th>
<th style="text-align: center;"># dev</th>
<th style="text-align: center;"># test</th>
<th style="text-align: center;"># types</th>
<th style="text-align: center;">Avg. tokens</th>
<th style="text-align: center;">Avg. entities</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">General</td>
<td style="text-align: center;">ACE04 (Mitchell et al., 2005)</td>
<td style="text-align: center;">6202</td>
<td style="text-align: center;">745</td>
<td style="text-align: center;">812</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">4.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ACE05 (Walker et al., 2006)</td>
<td style="text-align: center;">7299</td>
<td style="text-align: center;">971</td>
<td style="text-align: center;">1060</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">conllpp (Wang et al., 2019)</td>
<td style="text-align: center;">14041</td>
<td style="text-align: center;">3250</td>
<td style="text-align: center;">3453</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">1.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CrossNER AI (Liu et al., 2021)</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">431</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">5.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CrossNER literature (Liu et al., 2021)</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">416</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">5.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CrossNER music (Liu et al., 2021)</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">380</td>
<td style="text-align: center;">465</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">6.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CrossNER politics (Liu et al., 2021)</td>
<td style="text-align: center;">199</td>
<td style="text-align: center;">540</td>
<td style="text-align: center;">650</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">6.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CrossNER science (Liu et al., 2021)</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">450</td>
<td style="text-align: center;">543</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">5.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FewNERD-coarse (Ding et al., 2021)</td>
<td style="text-align: center;">131767</td>
<td style="text-align: center;">18824</td>
<td style="text-align: center;">37648</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FewNERD-fine (Ding et al., 2021)</td>
<td style="text-align: center;">131767</td>
<td style="text-align: center;">18824</td>
<td style="text-align: center;">37648</td>
<td style="text-align: center;">59</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MultiNERD (Tedeschi \&amp; Navigli, 2022)</td>
<td style="text-align: center;">134144</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">1.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ontonotes (Weischedel et al., 2013)</td>
<td style="text-align: center;">59924</td>
<td style="text-align: center;">8528</td>
<td style="text-align: center;">8262</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">0.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PolyglotNER (Al-Rfou et al., 2015)</td>
<td style="text-align: center;">393982</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TASTEset (Wrblewska et al., 2022)</td>
<td style="text-align: center;">556</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">19.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WikiANN en (Pan et al., 2017)</td>
<td style="text-align: center;">20000</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">1.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WikiNeural (Tedeschi et al., 2021)</td>
<td style="text-align: center;">92720</td>
<td style="text-align: center;">11590</td>
<td style="text-align: center;">11597</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">1.4</td>
</tr>
<tr>
<td style="text-align: center;">Biomed</td>
<td style="text-align: center;">AnatEM (Pyysalo \&amp; Ananiadou, 2014)</td>
<td style="text-align: center;">5861</td>
<td style="text-align: center;">2118</td>
<td style="text-align: center;">3830</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">0.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BioRED (Luo et al., 2022)</td>
<td style="text-align: center;">4373</td>
<td style="text-align: center;">1131</td>
<td style="text-align: center;">1106</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">3.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GENIA (Kim et al., 2003)</td>
<td style="text-align: center;">15023</td>
<td style="text-align: center;">1669</td>
<td style="text-align: center;">1854</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">3.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">JNLPBA (Collier \&amp; Kim, 2004)</td>
<td style="text-align: center;">18608</td>
<td style="text-align: center;">1940</td>
<td style="text-align: center;">4261</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">2.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">bc2gm (Smith et al., 2008)</td>
<td style="text-align: center;">12500</td>
<td style="text-align: center;">2500</td>
<td style="text-align: center;">5000</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">0.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">bc4chemd (Krallinger et al., 2015)</td>
<td style="text-align: center;">30682</td>
<td style="text-align: center;">30639</td>
<td style="text-align: center;">26364</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">0.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">bc5cdr (Li et al., 2016)</td>
<td style="text-align: center;">4560</td>
<td style="text-align: center;">4581</td>
<td style="text-align: center;">4797</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">41</td>
<td style="text-align: center;">2.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ncbi (Doan et al., 2014)</td>
<td style="text-align: center;">5432</td>
<td style="text-align: center;">923</td>
<td style="text-align: center;">940</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: center;">Clinics</td>
<td style="text-align: center;">ebmnlp (Nye et al., 2018)</td>
<td style="text-align: center;">40713</td>
<td style="text-align: center;">10608</td>
<td style="text-align: center;">2076</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">1.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">i2b2 2006 deid 1B (Uzuner et al., 2007)</td>
<td style="text-align: center;">34958</td>
<td style="text-align: center;">14983</td>
<td style="text-align: center;">18095</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">0.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">i2b2 2010 concepts (Uzuner et al., 2011)</td>
<td style="text-align: center;">14553</td>
<td style="text-align: center;">1762</td>
<td style="text-align: center;">27625</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">i2b2 2012 temporal (Sun et al., 2013)</td>
<td style="text-align: center;">6235</td>
<td style="text-align: center;">787</td>
<td style="text-align: center;">5282</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">i2b2 2014 deid (Stubbs et al., 2015)</td>
<td style="text-align: center;">46272</td>
<td style="text-align: center;">4610</td>
<td style="text-align: center;">32587</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">0.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">n2c2 2018 task2 (Henry et al., 2020)</td>
<td style="text-align: center;">84351</td>
<td style="text-align: center;">9252</td>
<td style="text-align: center;">60228</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">0.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ShAEeCLEF (Mowery et al., 2014)</td>
<td style="text-align: center;">12494</td>
<td style="text-align: center;">2459</td>
<td style="text-align: center;">14143</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">0.3</td>
</tr>
<tr>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">DEAL (Grezes et al., 2022)</td>
<td style="text-align: center;">26906</td>
<td style="text-align: center;">20800</td>
<td style="text-align: center;">36665</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">1.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FabNER (Kumar \&amp; Starly, 2022)</td>
<td style="text-align: center;">9435</td>
<td style="text-align: center;">2182</td>
<td style="text-align: center;">2064</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">5.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SOFC (Friedrich et al., 2020)</td>
<td style="text-align: center;">568</td>
<td style="text-align: center;">135</td>
<td style="text-align: center;">173</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">5.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SciERC (Luan et al., 2018)</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">163</td>
<td style="text-align: center;">16.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SciREX (Jain et al., 2020)</td>
<td style="text-align: center;">71511</td>
<td style="text-align: center;">15182</td>
<td style="text-align: center;">16599</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">29</td>
<td style="text-align: center;">1.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SoMeSci (Schindler et al., 2021)</td>
<td style="text-align: center;">31055</td>
<td style="text-align: center;">159</td>
<td style="text-align: center;">16427</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">41</td>
<td style="text-align: center;">2.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WLP (Kulkarni et al., 2018)</td>
<td style="text-align: center;">8177</td>
<td style="text-align: center;">2717</td>
<td style="text-align: center;">2726</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">4.5</td>
</tr>
<tr>
<td style="text-align: center;">Programming</td>
<td style="text-align: center;">Stackoverflow-NER (Tabassum et al., 2020)</td>
<td style="text-align: center;">9263</td>
<td style="text-align: center;">2936</td>
<td style="text-align: center;">3108</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">1.2</td>
</tr>
<tr>
<td style="text-align: center;">Social media</td>
<td style="text-align: center;">HarveyNER (Chen et al., 2022)</td>
<td style="text-align: center;">3967</td>
<td style="text-align: center;">1301</td>
<td style="text-align: center;">1303</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">0.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Broad Tweet Corpus (Derczynski et al., 2016)</td>
<td style="text-align: center;">5334</td>
<td style="text-align: center;">2001</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TweetNER7 (Ushio et al., 2022)</td>
<td style="text-align: center;">7111</td>
<td style="text-align: center;">886</td>
<td style="text-align: center;">576</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">3.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">mit-movie (Liu et al., 2013)</td>
<td style="text-align: center;">9774</td>
<td style="text-align: center;">2442</td>
<td style="text-align: center;">2442</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">1.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">mit-restaurant (Liu et al., 2013)</td>
<td style="text-align: center;">7659</td>
<td style="text-align: center;">1520</td>
<td style="text-align: center;">1520</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2.2</td>
</tr>
<tr>
<td style="text-align: center;">Law</td>
<td style="text-align: center;">E-NER (Au et al., 2022)</td>
<td style="text-align: center;">8072</td>
<td style="text-align: center;">1009</td>
<td style="text-align: center;">1010</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">0.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MAPA-coarse (Arranz et al., 2022)</td>
<td style="text-align: center;">893</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">408</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">0.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MAPA-fine (Arranz et al., 2022)</td>
<td style="text-align: center;">893</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">408</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">1.3</td>
</tr>
<tr>
<td style="text-align: center;">Finance</td>
<td style="text-align: center;">FiNER-ord (Shah et al., 2023)</td>
<td style="text-align: center;">3262</td>
<td style="text-align: center;">403</td>
<td style="text-align: center;">1075</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">1.1</td>
</tr>
<tr>
<td style="text-align: center;">Transportation</td>
<td style="text-align: center;">FindVehicle (Guan et al., 2023)</td>
<td style="text-align: center;">21565</td>
<td style="text-align: center;">20777</td>
<td style="text-align: center;">20777</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">5.5</td>
</tr>
</tbody>
</table>
<p>Table 6: Statistics of datasets in our benchmark.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ Please note that the original evaluation script in InstructUIE contains a critical bug. For passages that do not contain any entities, the script adds NONE as a placeholder entity and takes it into account when calculating $F_{1}$. To rectify this error, we re-evaluated InstructUIE using their released checkpoint.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
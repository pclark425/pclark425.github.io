<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1140 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1140</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1140</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-204788663</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1910.08348v2.pdf" target="_blank">VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning</a></p>
                <p><strong>Paper Abstract:</strong> Trading off exploration and exploitation in an unknown environment is key to maximising expected return during learning. A Bayes-optimal policy, which does so optimally, conditions its actions not only on the environment state but on the agent's uncertainty about the environment. Computing a Bayes-optimal policy is however intractable for all but the smallest tasks. In this paper, we introduce variational Bayes-Adaptive Deep RL (variBAD), a way to meta-learn to perform approximate inference in an unknown environment, and incorporate task uncertainty directly during action selection. In a grid-world domain, we illustrate how variBAD performs structured online exploration as a function of task uncertainty. We further evaluate variBAD on MuJoCo domains widely used in meta-RL and show that it achieves higher online return than existing methods.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1140.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1140.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>variBAD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>variational Bayes-Adaptive Deep Reinforcement Learning (variBAD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A meta-learned approach that performs amortised variational inference over a low-dimensional latent embedding of the task and conditions a policy on the posterior belief to approximate Bayes-optimal exploration and action selection in unknown MDPs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>variBAD</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Components: (1) an RNN-based encoder q_phi(m | tau:t) (GRU) that produces an amortised variational posterior over a low-dimensional stochastic task embedding m (latent dim = 5); (2) decoder(s) p_theta that predict rewards (and optionally next states) given m (used only in training) trained with an ELBO objective; (3) a policy pi_psi(a_t | s_t, q_phi(m | tau:t)) (PPO-trained) that conditions on the posterior parameters (mean/variance) to select actions. The encoder and model are trained via a VAE-style ELBO (reconstruction + KL), and the policy is trained to maximise online return while using the posterior as input. At test time only the encoder and policy are used (decoder unused).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian adaptive exploration via amortised variational inference (approximate Bayes-adaptive RL); information-seeking actions driven by posterior uncertainty (not Thompson sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each time step the agent encodes past trajectory tau:t into q_phi(m|tau:t), obtaining a posterior over task embeddings; the policy conditions on this posterior (its parameters or samples) and chooses actions that trade off exploration vs exploitation to maximise expected cumulative return within the BAMDP horizon. The encoder is trained to predict past and future transitions/rewards (decoder) so the posterior reflects uncertainty about unobserved task-relevant quantities; the policy thereby selects experimentally informative actions when uncertainty is high and exploitative actions when the posterior concentrates.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Gridworld (5x5 unknown goal), MuJoCo continuous-control (AntDir, HalfCheetahDir, HalfCheetahVel, Walker)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable in the sense of unknown task identity (hidden goal location or hidden dynamics/reward parameters); gridworld: discrete 5x5 state space, sparse reward; MuJoCo: continuous state and action spaces, variable/hidden dynamics or reward targets across tasks (e.g., direction or target velocity), stochasticity due to physics.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Gridworld: 5x5 discrete grid, 5 actions (up,right,down,left,stay), MDP horizon H=15, BAMDP horizon H+ = 45 (4 episodes). MuJoCo: standard locomotion tasks with continuous states and actions, episode length H=200 (reported), latent embedding size 5; tasks vary (2-way direction, range of velocities, randomized system parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Gridworld: average online return closely matches Bayes-optimal; variBAD reaches near-Bayes-optimal behaviour by the 3rd episode (figure captions: closely approximates Bayes-optimal and matches optimal performance from 3rd rollout; Bayes-optimal matches from 2nd); outperforms posterior-sampling baseline which requires ~6 rollouts to reach comparable returns. MuJoCo: adapts within the first rollout (H=200) and achieves higher online return during the first episode than competing meta-RL baselines (RL^2, PEARL, E-MAML, ProMP) in test-time adaptation metrics (Figure 4 shows average test performance across first 5 rollouts). Exact numerical cumulative rewards are reported in figures but not enumerated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Posterior sampling baseline (PEARL-like behaviour): performs poorly during first episode and only starts to perform well after multiple episodes (PEARL typically improves from the 3rd episode onward). Oracle (policy with privileged task info) attains upper-bound performance; variBAD approaches this upper bound during early rollouts. Precise baseline numeric values are shown in plots but not listed in-text.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>At meta-test time variBAD adapts within a single episode: in MuJoCo H=200 timesteps the policy utilises posterior updates online to perform near-optimal actions within that episode; gridworld: achieves near-optimal within 3 episodes (BAMDP horizon H+ = 45). During meta-training variBAD is on-policy (PPO) and thus less sample efficient than off-policy baselines like PEARL for meta-training, but more sample-efficient at meta-test adaptation (per-episode performance).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Trade-off is handled implicitly by conditioning the policy on the learned posterior over m: when posterior uncertainty is high the policy selects information-gathering actions (structured exploration), and when the posterior concentrates it selects exploitative actions; this behaviour is learned end-to-end to maximise expected return in the BAMDP objective. The ELBO-based encoder encourages informative posteriors by predicting future observations during training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against: Bayes-optimal (hard-coded) policy, posterior sampling (hand-coded), RL^2 (recurrent meta-RL), PEARL (probabilistic context variables / posterior-sampling style meta-RL), E-MAML, ProMP, and an Oracle policy with privileged task info.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>variBAD meta-learns an amortised inference procedure and a posterior-conditioned policy that approximates Bayes-optimal exploration in practice: in the 5x5 gridworld it closely matches Bayes-optimal exploration and outperforms posterior sampling (requires fewer rollouts to find goal); in MuJoCo locomotion tasks variBAD adapts within the first rollout and achieves higher online return during early interaction than several state-of-the-art meta-RL baselines. The approach yields interpretable posterior beliefs (decoder predictions, latent mean/variance trajectories) and concentrates posterior uncertainty rapidly when informative observations are gathered.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires a distribution p(M) of related tasks for meta-training — performance may degrade for out-of-distribution test tasks (the learned inference and policy may be wrong). Lacks formal Bayes-optimality guarantees due to amortised approximations and deep nets. Meta-training is on-policy (PPO), making meta-training sample-inefficient compared to off-policy methods (PEARL). The decoder is unused at test time (possible missed opportunities for planning); reported instability in some baselines (e.g., RL^2) but variBAD can still suffer from variability across random seeds (only learned Bayes-optimal exactly for some seeds).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1140.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1140.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RL^2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RL-squared (RL^2): Fast reinforcement learning via slow reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory-based/meta-RL approach that uses a recurrent policy (RNN) which receives past actions and rewards as inputs so the recurrent hidden state implements within-task learning dynamics; treated here as a baseline that can adapt online within an episode.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RL^2: Fast reinforcement learning via slow reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RL^2 (recurrent meta-RL baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Recurrent neural policy (GRU/LSTM) that ingests previous actions, rewards and observations to implement online adaptation inside the RNN hidden state; trained via RL (PPO in this work) to learn to learn across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Memory-based meta-learning that enables online adaptation (implicit information-seeking behaviour via recurrent dynamics), i.e., learned exploration via RNN hidden-state updates (not explicit Bayesian posterior).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>The RNN updates its hidden state from the stream of past actions/rewards/observations and the policy's behaviour changes as hidden state evolves; exploration emerges from the learned recurrent dynamics rather than from an explicit posterior or explicit information-gain objective.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Gridworld and MuJoCo tasks (same benchmarks used for variBAD comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Same partially observable/hidden-task settings (unknown goal or hidden dynamics/targets) as variBAD experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same as corresponding experiments (gridworld 5x5; MuJoCo tasks with continuous states/actions, H up to 200).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Can adapt within a single episode; in experiments RL^2 adapts online but underperforms variBAD on some tasks (e.g., HalfCheetahDir) and shows less stable / slower learning; in gridworld it closely approximates Bayes-optimal but less reliably across seeds (learning curves show variance). Exact reward numbers are given in figures rather than text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Not explicitly provided in text; baseline comparisons show RL^2 performs better than non-adaptive methods during first episode but often worse than variBAD.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Can adapt online within a single episode; however RL^2 was observed to be less stable across multiple rollouts and sometimes degrades after resets (due to high-dimensional hidden state). Meta-training sample efficiency comparable to variBAD when both are on-policy.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Implicitly encoded in learned recurrent dynamics; trade-off emerges from training objective that maximises expected return over tasks, not from explicit Bayesian uncertainty estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to variBAD, PEARL, E-MAML, ProMP, Oracle, Bayes-optimal, posterior sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>RL^2 is capable of online adaptation in a single episode but is generally less stable and sometimes lower-performing than variBAD in the MuJoCo tasks (e.g., HalfCheetahDir), and shows sensitivity to resets due to recurrent hidden-state handling.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Instability in maintaining performance across environment resets and across multiple rollouts; hidden-state dimensionality (128) can cause instability; lacks explicit uncertainty representation (no posterior), which can make exploration less principled than variBAD.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1140.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1140.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PEARL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PEARL: Probabilistic Embeddings for Actor-critic Reinforcement Learning (probabilistic context variables)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An off-policy meta-RL method that learns a probabilistic context encoder to infer a latent context variable for each task and conditions a policy on a sample from that posterior; behaves similarly to posterior sampling when used with certain encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Efficient off-policy meta-reinforcement learning via probabilistic context variables</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PEARL</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Encodes context (collected transitions) into a permutation-invariant probabilistic posterior over task context variables and conditions an off-policy actor-critic (SAC) policy on samples from that posterior; designed for sample-efficient meta-training and later adaptation via posterior sampling-like behaviour.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Posterior-sampling-style adaptation via probabilistic context variables; adaptation is accomplished by sampling/inferencing a context posterior and acting according to a policy conditioned on that sample (episodic posterior sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At test time, PEARL infers a posterior over the task context from gathered transitions; adaptation occurs episodically as the encoder aggregates context data; policy uses samples from the context posterior to act. Exploration is largely driven by the sampling of contexts from the posterior (similar to Thompson/posterior sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>MuJoCo tasks (AntDir, HalfCheetahDir, HalfCheetahVel, Walker) and other meta-RL benchmarks used in comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable insofar as tasks differ by hidden parameters (direction, target velocity, dynamics); continuous state and action spaces; diverse task distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Standard MuJoCo locomotion tasks with continuous dynamics; episode length and task specifics match those used in the paper (e.g., H=200).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>PEARL is more sample-efficient during meta-training (off-policy) but at meta-test time behaves like posterior sampling and therefore typically underperforms Bayes-optimal-style approaches during the first episode; in the experiments PEARL starts performing well from around the 3rd episode and is outperformed by variBAD during the first-episode online return metric. PEARL sometimes slightly outperforms the Oracle policy in later episodes due to algorithmic differences (SAC vs PPO).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>When acting according to a single sampled context for the first episode, PEARL can perform poorly if the sampled hypothesis is incorrect; its performance improves over multiple episodes as context is accumulated.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Very sample-efficient during meta-training due to off-policy SAC training; but meta-test per-episode adaptation (first episode performance) is weaker compared to variBAD which is trained to optimise Bayes-adaptive online return.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Exploration emerges from sampling different context hypotheses from the posterior (episodic posterior sampling); there is no explicit Bayes-adaptive planning, so the exploration strategy may be less targeted than variBAD's learned posterior-conditioned policy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against variBAD, RL^2, E-MAML, ProMP, Oracle, Bayes-optimal and posterior-sampling baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>PEARL is an efficient off-policy meta-training approach and performs well after several episodes; however, because it resembles posterior sampling, it is less effective at maximising return during the first episode compared to variBAD which is explicitly trained for Bayes-adaptive behaviour.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Acts like posterior sampling and can require multiple episodes to reach good performance in a new task; its encoder must be permutation-invariant for off-policy training which constrains encoder design; less suited to maximising single-episode online return.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1140.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1140.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Posterior sampling (Thompson-style)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Posterior sampling for reinforcement learning (Thompson sampling extension to MDPs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An exploration strategy that samples a single hypothesis MDP from the agent's posterior and follows the optimal policy for that sampled model until the next resampling; used as a baseline (hand-coded) and contrasted with Bayes-optimal behaviour.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>More efficient reinforcement learning via posterior sampling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Posterior sampling (episode-wise)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Maintains a posterior over MDPs and at episode boundaries samples one MDP hypothesis, follows the policy optimal for that sampled MDP for the episode (or until next resample).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Thompson/posterior sampling (episodic hypothesis sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adaptation occurs only when the agent resamples a new hypothesis from its posterior (e.g., at episode boundaries); it follows the sampled-model-optimal policy thereby exploring according to the variability in the posterior but not explicitly maximising information gain.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Referenced as baseline for gridworld and MuJoCo experiments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Same hidden-task settings (unknown goal position in gridworld; hidden dynamics/targets in MuJoCo), discrete or continuous state/action depending on task.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Gridworld: 5x5 grid with horizon H=15; MuJoCo: continuous locomotion tasks with H typically 200 per episode.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>In the gridworld baseline, posterior sampling requires ~6 rollouts to match performance that Bayes-optimal policy achieves earlier; performance during early rollouts is substantially worse than Bayes-optimal and worse than variBAD which learns more structured exploration. In MuJoCo, PEARL (a posterior-sampling-like method) improves over multiple episodes but lags in first-episode online return.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Non-adaptive or random exploration baselines perform worse; exact numeric comparisons are provided in figures but not enumerated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Inefficient in terms of early online return because sampling one hypothesis per episode can lead to wasteful behaviour (e.g., revisiting states) and slow posterior concentration; requires multiple episodes for effective learning in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Exploration is implemented by acting optimally under sampled hypotheses; there is no explicit planning over information gain, so actions may not be chosen to maximally reduce uncertainty in an expected-return-aware manner.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared conceptually and empirically to Bayes-optimal policy and variBAD (which aims to approximate Bayes-optimal), as well as to PEARL which implements a learned posterior-sampling style approach.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Posterior sampling is simpler and more tractable but can be highly inefficient (wasteful revisits and slow uncertainty reduction) compared to Bayes-optimal strategies; variBAD demonstrates that a learned posterior-conditioned policy can substantially outperform episodic posterior sampling in early online return.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Inefficient early exploration; can waste episodes following wrong hypotheses and thus has lower cumulative return during initial learning episodes.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1140.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1140.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayes-optimal policy (BAMDP solution)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayes-optimal policy (solution of the Bayes-Adaptive MDP / BAMDP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The theoretical optimal policy for maximising expected return when the MDP is uncertain, obtained by treating the agent's belief as part of the state (the BAMDP); used as an upper-bound baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Bayes-optimal policy (BAMDP solution)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A policy that conditions on the full posterior over reward and transition functions (belief state) and optimally trades off exploration and exploitation by planning in belief (hyper-)state space; intractable except for small problems but used as a hard-coded optimal reference in the gridworld experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Belief-space planning in BAMDP: explicit Bayes-adaptive exploration that plans information-gathering actions based on expected-value-of-information within remaining horizon.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>The Bayes-optimal agent explicitly updates its posterior belief (exact Bayes updates) and plans in the augmented belief-state (S x B) to choose actions that optimally reduce uncertainty insofar as it increases expected return within the allowed horizon.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Gridworld (used as a hard-coded Bayes-optimal baseline) and conceptually relevant to all partially observable/unknown-task settings.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Same as experiment settings; assumes tractable belief updates and planning (only available for small problems).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Tractable only for small discrete environments (the paper uses it as a reference for the 5x5 gridworld BAMDP with small state/action/horizon).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Upper-bound reference: in the gridworld the Bayes-optimal policy matches optimal behaviour from the 2nd episode and yields the highest possible expected return; variBAD closely approximates this behaviour.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Bayes-optimal is maximally sample-efficient in expected-return terms (by definition) but computationally intractable for large or continuous problems.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Exact planning trades off exploration and exploitation optimally given the belief and remaining horizon; selects information-seeking actions only when they are expected to increase cumulative return.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Used as an upper-bound reference against variBAD, posterior sampling and other meta-RL baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Provides a principled target: Bayes-optimal behaviour is superior to naive posterior sampling; variBAD aims to approximate this behaviour in scalable deep RL settings and succeeds closely in the gridworld and well in first-episode adaptation in MuJoCo.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Intractable to compute or plan exactly in realistic/large/continuous RL problems; requires exact posterior and belief-space planning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Efficient off-policy meta-reinforcement learning via probabilistic context variables <em>(Rating: 2)</em></li>
                <li>RL^2: Fast reinforcement learning via slow reinforcement learning <em>(Rating: 2)</em></li>
                <li>More efficient reinforcement learning via posterior sampling <em>(Rating: 2)</em></li>
                <li>Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes <em>(Rating: 2)</em></li>
                <li>Hidden parameter Markov decision processes: A semiparametric regression approach for discovering latent task parametrizations <em>(Rating: 2)</em></li>
                <li>A bayesian framework for reinforcement learning <em>(Rating: 1)</em></li>
                <li>Variational inference for data-efficient model learning in POMDPs <em>(Rating: 1)</em></li>
                <li>Randomized prior functions for deep reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1140",
    "paper_id": "paper-204788663",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "variBAD",
            "name_full": "variational Bayes-Adaptive Deep Reinforcement Learning (variBAD)",
            "brief_description": "A meta-learned approach that performs amortised variational inference over a low-dimensional latent embedding of the task and conditions a policy on the posterior belief to approximate Bayes-optimal exploration and action selection in unknown MDPs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "variBAD",
            "agent_description": "Components: (1) an RNN-based encoder q_phi(m | tau:t) (GRU) that produces an amortised variational posterior over a low-dimensional stochastic task embedding m (latent dim = 5); (2) decoder(s) p_theta that predict rewards (and optionally next states) given m (used only in training) trained with an ELBO objective; (3) a policy pi_psi(a_t | s_t, q_phi(m | tau:t)) (PPO-trained) that conditions on the posterior parameters (mean/variance) to select actions. The encoder and model are trained via a VAE-style ELBO (reconstruction + KL), and the policy is trained to maximise online return while using the posterior as input. At test time only the encoder and policy are used (decoder unused).",
            "adaptive_design_method": "Bayesian adaptive exploration via amortised variational inference (approximate Bayes-adaptive RL); information-seeking actions driven by posterior uncertainty (not Thompson sampling).",
            "adaptation_strategy_description": "At each time step the agent encodes past trajectory tau:t into q_phi(m|tau:t), obtaining a posterior over task embeddings; the policy conditions on this posterior (its parameters or samples) and chooses actions that trade off exploration vs exploitation to maximise expected cumulative return within the BAMDP horizon. The encoder is trained to predict past and future transitions/rewards (decoder) so the posterior reflects uncertainty about unobserved task-relevant quantities; the policy thereby selects experimentally informative actions when uncertainty is high and exploitative actions when the posterior concentrates.",
            "environment_name": "Gridworld (5x5 unknown goal), MuJoCo continuous-control (AntDir, HalfCheetahDir, HalfCheetahVel, Walker)",
            "environment_characteristics": "Partially observable in the sense of unknown task identity (hidden goal location or hidden dynamics/reward parameters); gridworld: discrete 5x5 state space, sparse reward; MuJoCo: continuous state and action spaces, variable/hidden dynamics or reward targets across tasks (e.g., direction or target velocity), stochasticity due to physics.",
            "environment_complexity": "Gridworld: 5x5 discrete grid, 5 actions (up,right,down,left,stay), MDP horizon H=15, BAMDP horizon H+ = 45 (4 episodes). MuJoCo: standard locomotion tasks with continuous states and actions, episode length H=200 (reported), latent embedding size 5; tasks vary (2-way direction, range of velocities, randomized system parameters).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Gridworld: average online return closely matches Bayes-optimal; variBAD reaches near-Bayes-optimal behaviour by the 3rd episode (figure captions: closely approximates Bayes-optimal and matches optimal performance from 3rd rollout; Bayes-optimal matches from 2nd); outperforms posterior-sampling baseline which requires ~6 rollouts to reach comparable returns. MuJoCo: adapts within the first rollout (H=200) and achieves higher online return during the first episode than competing meta-RL baselines (RL^2, PEARL, E-MAML, ProMP) in test-time adaptation metrics (Figure 4 shows average test performance across first 5 rollouts). Exact numerical cumulative rewards are reported in figures but not enumerated in text.",
            "performance_without_adaptation": "Posterior sampling baseline (PEARL-like behaviour): performs poorly during first episode and only starts to perform well after multiple episodes (PEARL typically improves from the 3rd episode onward). Oracle (policy with privileged task info) attains upper-bound performance; variBAD approaches this upper bound during early rollouts. Precise baseline numeric values are shown in plots but not listed in-text.",
            "sample_efficiency": "At meta-test time variBAD adapts within a single episode: in MuJoCo H=200 timesteps the policy utilises posterior updates online to perform near-optimal actions within that episode; gridworld: achieves near-optimal within 3 episodes (BAMDP horizon H+ = 45). During meta-training variBAD is on-policy (PPO) and thus less sample efficient than off-policy baselines like PEARL for meta-training, but more sample-efficient at meta-test adaptation (per-episode performance).",
            "exploration_exploitation_tradeoff": "Trade-off is handled implicitly by conditioning the policy on the learned posterior over m: when posterior uncertainty is high the policy selects information-gathering actions (structured exploration), and when the posterior concentrates it selects exploitative actions; this behaviour is learned end-to-end to maximise expected return in the BAMDP objective. The ELBO-based encoder encourages informative posteriors by predicting future observations during training.",
            "comparison_methods": "Compared against: Bayes-optimal (hard-coded) policy, posterior sampling (hand-coded), RL^2 (recurrent meta-RL), PEARL (probabilistic context variables / posterior-sampling style meta-RL), E-MAML, ProMP, and an Oracle policy with privileged task info.",
            "key_results": "variBAD meta-learns an amortised inference procedure and a posterior-conditioned policy that approximates Bayes-optimal exploration in practice: in the 5x5 gridworld it closely matches Bayes-optimal exploration and outperforms posterior sampling (requires fewer rollouts to find goal); in MuJoCo locomotion tasks variBAD adapts within the first rollout and achieves higher online return during early interaction than several state-of-the-art meta-RL baselines. The approach yields interpretable posterior beliefs (decoder predictions, latent mean/variance trajectories) and concentrates posterior uncertainty rapidly when informative observations are gathered.",
            "limitations_or_failures": "Requires a distribution p(M) of related tasks for meta-training — performance may degrade for out-of-distribution test tasks (the learned inference and policy may be wrong). Lacks formal Bayes-optimality guarantees due to amortised approximations and deep nets. Meta-training is on-policy (PPO), making meta-training sample-inefficient compared to off-policy methods (PEARL). The decoder is unused at test time (possible missed opportunities for planning); reported instability in some baselines (e.g., RL^2) but variBAD can still suffer from variability across random seeds (only learned Bayes-optimal exactly for some seeds).",
            "uuid": "e1140.0"
        },
        {
            "name_short": "RL^2",
            "name_full": "RL-squared (RL^2): Fast reinforcement learning via slow reinforcement learning",
            "brief_description": "A memory-based/meta-RL approach that uses a recurrent policy (RNN) which receives past actions and rewards as inputs so the recurrent hidden state implements within-task learning dynamics; treated here as a baseline that can adapt online within an episode.",
            "citation_title": "RL^2: Fast reinforcement learning via slow reinforcement learning",
            "mention_or_use": "use",
            "agent_name": "RL^2 (recurrent meta-RL baseline)",
            "agent_description": "Recurrent neural policy (GRU/LSTM) that ingests previous actions, rewards and observations to implement online adaptation inside the RNN hidden state; trained via RL (PPO in this work) to learn to learn across tasks.",
            "adaptive_design_method": "Memory-based meta-learning that enables online adaptation (implicit information-seeking behaviour via recurrent dynamics), i.e., learned exploration via RNN hidden-state updates (not explicit Bayesian posterior).",
            "adaptation_strategy_description": "The RNN updates its hidden state from the stream of past actions/rewards/observations and the policy's behaviour changes as hidden state evolves; exploration emerges from the learned recurrent dynamics rather than from an explicit posterior or explicit information-gain objective.",
            "environment_name": "Gridworld and MuJoCo tasks (same benchmarks used for variBAD comparisons)",
            "environment_characteristics": "Same partially observable/hidden-task settings (unknown goal or hidden dynamics/targets) as variBAD experiments.",
            "environment_complexity": "Same as corresponding experiments (gridworld 5x5; MuJoCo tasks with continuous states/actions, H up to 200).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Can adapt within a single episode; in experiments RL^2 adapts online but underperforms variBAD on some tasks (e.g., HalfCheetahDir) and shows less stable / slower learning; in gridworld it closely approximates Bayes-optimal but less reliably across seeds (learning curves show variance). Exact reward numbers are given in figures rather than text.",
            "performance_without_adaptation": "Not explicitly provided in text; baseline comparisons show RL^2 performs better than non-adaptive methods during first episode but often worse than variBAD.",
            "sample_efficiency": "Can adapt online within a single episode; however RL^2 was observed to be less stable across multiple rollouts and sometimes degrades after resets (due to high-dimensional hidden state). Meta-training sample efficiency comparable to variBAD when both are on-policy.",
            "exploration_exploitation_tradeoff": "Implicitly encoded in learned recurrent dynamics; trade-off emerges from training objective that maximises expected return over tasks, not from explicit Bayesian uncertainty estimates.",
            "comparison_methods": "Compared to variBAD, PEARL, E-MAML, ProMP, Oracle, Bayes-optimal, posterior sampling.",
            "key_results": "RL^2 is capable of online adaptation in a single episode but is generally less stable and sometimes lower-performing than variBAD in the MuJoCo tasks (e.g., HalfCheetahDir), and shows sensitivity to resets due to recurrent hidden-state handling.",
            "limitations_or_failures": "Instability in maintaining performance across environment resets and across multiple rollouts; hidden-state dimensionality (128) can cause instability; lacks explicit uncertainty representation (no posterior), which can make exploration less principled than variBAD.",
            "uuid": "e1140.1"
        },
        {
            "name_short": "PEARL",
            "name_full": "PEARL: Probabilistic Embeddings for Actor-critic Reinforcement Learning (probabilistic context variables)",
            "brief_description": "An off-policy meta-RL method that learns a probabilistic context encoder to infer a latent context variable for each task and conditions a policy on a sample from that posterior; behaves similarly to posterior sampling when used with certain encoders.",
            "citation_title": "Efficient off-policy meta-reinforcement learning via probabilistic context variables",
            "mention_or_use": "use",
            "agent_name": "PEARL",
            "agent_description": "Encodes context (collected transitions) into a permutation-invariant probabilistic posterior over task context variables and conditions an off-policy actor-critic (SAC) policy on samples from that posterior; designed for sample-efficient meta-training and later adaptation via posterior sampling-like behaviour.",
            "adaptive_design_method": "Posterior-sampling-style adaptation via probabilistic context variables; adaptation is accomplished by sampling/inferencing a context posterior and acting according to a policy conditioned on that sample (episodic posterior sampling).",
            "adaptation_strategy_description": "At test time, PEARL infers a posterior over the task context from gathered transitions; adaptation occurs episodically as the encoder aggregates context data; policy uses samples from the context posterior to act. Exploration is largely driven by the sampling of contexts from the posterior (similar to Thompson/posterior sampling).",
            "environment_name": "MuJoCo tasks (AntDir, HalfCheetahDir, HalfCheetahVel, Walker) and other meta-RL benchmarks used in comparisons",
            "environment_characteristics": "Partially observable insofar as tasks differ by hidden parameters (direction, target velocity, dynamics); continuous state and action spaces; diverse task distribution.",
            "environment_complexity": "Standard MuJoCo locomotion tasks with continuous dynamics; episode length and task specifics match those used in the paper (e.g., H=200).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "PEARL is more sample-efficient during meta-training (off-policy) but at meta-test time behaves like posterior sampling and therefore typically underperforms Bayes-optimal-style approaches during the first episode; in the experiments PEARL starts performing well from around the 3rd episode and is outperformed by variBAD during the first-episode online return metric. PEARL sometimes slightly outperforms the Oracle policy in later episodes due to algorithmic differences (SAC vs PPO).",
            "performance_without_adaptation": "When acting according to a single sampled context for the first episode, PEARL can perform poorly if the sampled hypothesis is incorrect; its performance improves over multiple episodes as context is accumulated.",
            "sample_efficiency": "Very sample-efficient during meta-training due to off-policy SAC training; but meta-test per-episode adaptation (first episode performance) is weaker compared to variBAD which is trained to optimise Bayes-adaptive online return.",
            "exploration_exploitation_tradeoff": "Exploration emerges from sampling different context hypotheses from the posterior (episodic posterior sampling); there is no explicit Bayes-adaptive planning, so the exploration strategy may be less targeted than variBAD's learned posterior-conditioned policy.",
            "comparison_methods": "Compared against variBAD, RL^2, E-MAML, ProMP, Oracle, Bayes-optimal and posterior-sampling baselines.",
            "key_results": "PEARL is an efficient off-policy meta-training approach and performs well after several episodes; however, because it resembles posterior sampling, it is less effective at maximising return during the first episode compared to variBAD which is explicitly trained for Bayes-adaptive behaviour.",
            "limitations_or_failures": "Acts like posterior sampling and can require multiple episodes to reach good performance in a new task; its encoder must be permutation-invariant for off-policy training which constrains encoder design; less suited to maximising single-episode online return.",
            "uuid": "e1140.2"
        },
        {
            "name_short": "Posterior sampling (Thompson-style)",
            "name_full": "Posterior sampling for reinforcement learning (Thompson sampling extension to MDPs)",
            "brief_description": "An exploration strategy that samples a single hypothesis MDP from the agent's posterior and follows the optimal policy for that sampled model until the next resampling; used as a baseline (hand-coded) and contrasted with Bayes-optimal behaviour.",
            "citation_title": "More efficient reinforcement learning via posterior sampling",
            "mention_or_use": "mention",
            "agent_name": "Posterior sampling (episode-wise)",
            "agent_description": "Maintains a posterior over MDPs and at episode boundaries samples one MDP hypothesis, follows the policy optimal for that sampled MDP for the episode (or until next resample).",
            "adaptive_design_method": "Thompson/posterior sampling (episodic hypothesis sampling).",
            "adaptation_strategy_description": "Adaptation occurs only when the agent resamples a new hypothesis from its posterior (e.g., at episode boundaries); it follows the sampled-model-optimal policy thereby exploring according to the variability in the posterior but not explicitly maximising information gain.",
            "environment_name": "Referenced as baseline for gridworld and MuJoCo experiments",
            "environment_characteristics": "Same hidden-task settings (unknown goal position in gridworld; hidden dynamics/targets in MuJoCo), discrete or continuous state/action depending on task.",
            "environment_complexity": "Gridworld: 5x5 grid with horizon H=15; MuJoCo: continuous locomotion tasks with H typically 200 per episode.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "In the gridworld baseline, posterior sampling requires ~6 rollouts to match performance that Bayes-optimal policy achieves earlier; performance during early rollouts is substantially worse than Bayes-optimal and worse than variBAD which learns more structured exploration. In MuJoCo, PEARL (a posterior-sampling-like method) improves over multiple episodes but lags in first-episode online return.",
            "performance_without_adaptation": "Non-adaptive or random exploration baselines perform worse; exact numeric comparisons are provided in figures but not enumerated in text.",
            "sample_efficiency": "Inefficient in terms of early online return because sampling one hypothesis per episode can lead to wasteful behaviour (e.g., revisiting states) and slow posterior concentration; requires multiple episodes for effective learning in many settings.",
            "exploration_exploitation_tradeoff": "Exploration is implemented by acting optimally under sampled hypotheses; there is no explicit planning over information gain, so actions may not be chosen to maximally reduce uncertainty in an expected-return-aware manner.",
            "comparison_methods": "Compared conceptually and empirically to Bayes-optimal policy and variBAD (which aims to approximate Bayes-optimal), as well as to PEARL which implements a learned posterior-sampling style approach.",
            "key_results": "Posterior sampling is simpler and more tractable but can be highly inefficient (wasteful revisits and slow uncertainty reduction) compared to Bayes-optimal strategies; variBAD demonstrates that a learned posterior-conditioned policy can substantially outperform episodic posterior sampling in early online return.",
            "limitations_or_failures": "Inefficient early exploration; can waste episodes following wrong hypotheses and thus has lower cumulative return during initial learning episodes.",
            "uuid": "e1140.3"
        },
        {
            "name_short": "Bayes-optimal policy (BAMDP solution)",
            "name_full": "Bayes-optimal policy (solution of the Bayes-Adaptive MDP / BAMDP)",
            "brief_description": "The theoretical optimal policy for maximising expected return when the MDP is uncertain, obtained by treating the agent's belief as part of the state (the BAMDP); used as an upper-bound baseline in experiments.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Bayes-optimal policy (BAMDP solution)",
            "agent_description": "A policy that conditions on the full posterior over reward and transition functions (belief state) and optimally trades off exploration and exploitation by planning in belief (hyper-)state space; intractable except for small problems but used as a hard-coded optimal reference in the gridworld experiment.",
            "adaptive_design_method": "Belief-space planning in BAMDP: explicit Bayes-adaptive exploration that plans information-gathering actions based on expected-value-of-information within remaining horizon.",
            "adaptation_strategy_description": "The Bayes-optimal agent explicitly updates its posterior belief (exact Bayes updates) and plans in the augmented belief-state (S x B) to choose actions that optimally reduce uncertainty insofar as it increases expected return within the allowed horizon.",
            "environment_name": "Gridworld (used as a hard-coded Bayes-optimal baseline) and conceptually relevant to all partially observable/unknown-task settings.",
            "environment_characteristics": "Same as experiment settings; assumes tractable belief updates and planning (only available for small problems).",
            "environment_complexity": "Tractable only for small discrete environments (the paper uses it as a reference for the 5x5 gridworld BAMDP with small state/action/horizon).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Upper-bound reference: in the gridworld the Bayes-optimal policy matches optimal behaviour from the 2nd episode and yields the highest possible expected return; variBAD closely approximates this behaviour.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Bayes-optimal is maximally sample-efficient in expected-return terms (by definition) but computationally intractable for large or continuous problems.",
            "exploration_exploitation_tradeoff": "Exact planning trades off exploration and exploitation optimally given the belief and remaining horizon; selects information-seeking actions only when they are expected to increase cumulative return.",
            "comparison_methods": "Used as an upper-bound reference against variBAD, posterior sampling and other meta-RL baselines.",
            "key_results": "Provides a principled target: Bayes-optimal behaviour is superior to naive posterior sampling; variBAD aims to approximate this behaviour in scalable deep RL settings and succeeds closely in the gridworld and well in first-episode adaptation in MuJoCo.",
            "limitations_or_failures": "Intractable to compute or plan exactly in realistic/large/continuous RL problems; requires exact posterior and belief-space planning.",
            "uuid": "e1140.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Efficient off-policy meta-reinforcement learning via probabilistic context variables",
            "rating": 2,
            "sanitized_title": "efficient_offpolicy_metareinforcement_learning_via_probabilistic_context_variables"
        },
        {
            "paper_title": "RL^2: Fast reinforcement learning via slow reinforcement learning",
            "rating": 2,
            "sanitized_title": "rl2_fast_reinforcement_learning_via_slow_reinforcement_learning"
        },
        {
            "paper_title": "More efficient reinforcement learning via posterior sampling",
            "rating": 2,
            "sanitized_title": "more_efficient_reinforcement_learning_via_posterior_sampling"
        },
        {
            "paper_title": "Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes",
            "rating": 2,
            "sanitized_title": "optimal_learning_computational_procedures_for_bayesadaptive_markov_decision_processes"
        },
        {
            "paper_title": "Hidden parameter Markov decision processes: A semiparametric regression approach for discovering latent task parametrizations",
            "rating": 2,
            "sanitized_title": "hidden_parameter_markov_decision_processes_a_semiparametric_regression_approach_for_discovering_latent_task_parametrizations"
        },
        {
            "paper_title": "A bayesian framework for reinforcement learning",
            "rating": 1,
            "sanitized_title": "a_bayesian_framework_for_reinforcement_learning"
        },
        {
            "paper_title": "Variational inference for data-efficient model learning in POMDPs",
            "rating": 1,
            "sanitized_title": "variational_inference_for_dataefficient_model_learning_in_pomdps"
        },
        {
            "paper_title": "Randomized prior functions for deep reinforcement learning",
            "rating": 1,
            "sanitized_title": "randomized_prior_functions_for_deep_reinforcement_learning"
        }
    ],
    "cost": 0.01777825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>VARIBAD: A VERY GOOD METHOD FOR BAYES-ADAPTIVE DEEP RL VIA META-LEARNING</p>
<p>Luisa Zintgraf 
Latent Logic
University of Oxford
University of Oxford
University of Oxford
University of Oxford
Microsoft Research
University of Oxford</p>
<p>Kyriacos Shiarlis 
Latent Logic
University of Oxford
University of Oxford
University of Oxford
University of Oxford
Microsoft Research
University of Oxford</p>
<p>Maximilian Igl 
Latent Logic
University of Oxford
University of Oxford
University of Oxford
University of Oxford
Microsoft Research
University of Oxford</p>
<p>Sebastian Schulze 
Latent Logic
University of Oxford
University of Oxford
University of Oxford
University of Oxford
Microsoft Research
University of Oxford</p>
<p>Yarin Gal 
Latent Logic
University of Oxford
University of Oxford
University of Oxford
University of Oxford
Microsoft Research
University of Oxford</p>
<p>Katja Hofmann 
Latent Logic
University of Oxford
University of Oxford
University of Oxford
University of Oxford
Microsoft Research
University of Oxford</p>
<p>Shimon Whiteson 
Latent Logic
University of Oxford
University of Oxford
University of Oxford
University of Oxford
Microsoft Research
University of Oxford</p>
<p>VARIBAD: A VERY GOOD METHOD FOR BAYES-ADAPTIVE DEEP RL VIA META-LEARNING
Published as a conference paper at ICLR 2020
Trading off exploration and exploitation in an unknown environment is key to maximising expected return during learning. A Bayes-optimal policy, which does so optimally, conditions its actions not only on the environment state but on the agent's uncertainty about the environment. Computing a Bayes-optimal policy is however intractable for all but the smallest tasks. In this paper, we introduce variational Bayes-Adaptive Deep RL (variBAD), a way to meta-learn to perform approximate inference in an unknown environment, and incorporate task uncertainty directly during action selection. In a grid-world domain, we illustrate how variBAD performs structured online exploration as a function of task uncertainty. We further evaluate variBAD on MuJoCo domains widely used in meta-RL and show that it achieves higher online return than existing methods. * . Robust and efficient transfer learning with hidden parameter markov decision processes. In Advances in neural information processing systems, 2017.</p>
<p>INTRODUCTION</p>
<p>Reinforcement learning (RL) is typically concerned with finding an optimal policy that maximises expected return for a given Markov decision process (MDP) with an unknown reward and transition function. If these were known, the optimal policy could in theory be computed without environment interactions. By contrast, learning in an unknown environment usually requires trading off exploration (learning about the environment) and exploitation (taking promising actions). Balancing this trade-off is key to maximising expected return during learning, which is desirable in many settings, particularly in high-stakes real-world applications like healthcare and education (Liu et al., 2014;Yauney &amp; Shah, 2018). A Bayes-optimal policy, which does this trade-off optimally, conditions actions not only on the environment state but on the agent's own uncertainty about the current MDP.</p>
<p>In principle, a Bayes-optimal policy can be computed using the framework of Bayes-adaptive Markov decision processes (BAMDPs) (Martin, 1967;Duff &amp; Barto, 2002), in which the agent maintains a belief distribution over possible environments. Augmenting the state space of the underlying MDP with this belief yields a BAMDP, a special case of a belief MDP (Kaelbling et al., 1998). A Bayes-optimal agent maximises expected return in the BAMDP by systematically seeking out the data needed to quickly reduce uncertainty, but only insofar as doing so helps maximise expected return. Its performance is bounded from above by the optimal policy for the given MDP, which does not need to take exploratory actions but requires prior knowledge about the MDP to compute.</p>
<p>Unfortunately, planning in a BAMDP, i.e., computing a Bayes-optimal policy that conditions on the augmented state, is intractable for all but the smallest tasks. A common shortcut is to rely instead on posterior sampling (Thompson, 1933;Strens, 2000;Osband et al., 2013). Here, the agent periodically samples a single hypothesis MDP (e.g., at the beginning of an episode) from its posterior, and the policy that is optimal for the sampled MDP is followed until the next sample is drawn. Planning is far more tractable since it is done on a regular MDP, not a BAMDP. However, posterior sampling's exploration can be highly inefficient and far from Bayes-optimal. The agent starts at the bottom left and has to navigate to an unknown goal, located in the grey area. (b) A Bayes-optimal exploration strategy that systematically searches possible grid cells to find the goal, shown in solid (past actions) and dashed (future actions) blue lines. A simplified posterior is shown in the background in grey (p = 1/(number of possible goal positions left) of containing the goal) and white (p = 0). (c) Posterior sampling, which repeatedly samples a possible goal position (red squares) from the current posterior, takes the shortest route there, and updates its posterior. (d) Exploration strategy learned by variBAD. The grey background represents the approximate posterior the agent has learned. (e) Average return over all possible environments, over six episodes with 15 steps each (after which the agent is reset to the starting position). VariBAD results are averaged across 20 random seeds. The performance of any exploration strategy is bounded above by the optimal behaviour (of a policy with access to the true goal position). The Bayes-optimal agent matches this behaviour from the second episode, whereas posterior sampling needs six rollouts. VariBAD closely approximates Bayes-optimal behaviour in this environment.</p>
<p>Consider the example of a gridworld in Figure 1, where the agent must navigate to an unknown goal located in the grey area (1a). To maintain a posterior, the agent can uniformly assign non-zero probability to cells where the goal could be, and zero to all other cells. A Bayes-optimal strategy strategically searches the set of goal positions that the posterior considers possible, until the goal is found (1b). Posterior sampling by contrast samples a possible goal position, takes the shortest route there, and then resamples a different goal position from the updated posterior (1c). Doing so is much less efficient since the agent's uncertainty is not reduced optimally (e.g., states are revisited).</p>
<p>As this example illustrates, Bayes-optimal policies can explore much more efficiently than posterior sampling. A key challenge is to learn approximately Bayes-optimal policies while retaining the tractability of posterior sampling. In addition, the inference involved in maintaining a posterior belief, needed even for posterior sampling, may itself be intractable.</p>
<p>In this paper, we combine ideas from Bayesian RL, approximate variational inference, and metalearning to tackle these challenges, and equip an agent with the ability to strategically explore unseen (but related) environments for a given distribution, in order to maximise its expected online return.</p>
<p>More specifically, we propose variational Bayes-Adaptive Deep RL (variBAD), a way to meta-learn to perform approximate inference on an unknown task, 1 and incorporate task uncertainty directly during action selection. Given a distribution over MDPs p(M ), we represent a single MDP M using a learned, low-dimensional stochastic latent variable m and jointly meta-train: 1. A variational auto-encoder that can infer the posterior distribution over m in a new task, given the agent's experience, while interacting with the environment, and 2. A policy that conditions on this posterior belief over MDP embeddings, and thus learns how to trade off exploration and exploitation when selecting actions under task uncertainty. Figure 1e shows the performance of our method versus the hard-coded optimal (with privileged goal information), Bayes-optimal, and posterior sampling exploration strategies. VariBAD's performance closely matches the Bayes-optimal one, matching optimal performance from the third rollout.</p>
<p>Previous approaches to BAMDPs are only tractable in environments with small action and state spaces or rely on privileged information about the task during training. VariBAD offers a tractable and flexible approach for learning Bayes-adaptive policies tailored to the training task distribution, with the only assumption that such a distribution is available for meta-training. We evaluate our approach on the gridworld shown above and on MuJoCo domains that are widely used in meta-RL, and show that variBAD exhibits superior exploratory behaviour at test time compared to existing meta-learning methods, achieving higher returns during learning. As such, variBAD opens a path to tractable approximate Bayes-optimal exploration for deep reinforcement learning.</p>
<p>BACKGROUND</p>
<p>We define a Markov decision process (MDP) as a tuple M = (S, A, R, T, T 0 , γ, H) with S a set of states, A a set of actions, R(r t+1 |s t , a t , s t+1 ) a reward function, T (s t+1 |s t , a t ) a transition function, T 0 (s 0 ) an initial state distribution, γ a discount factor, and H the horizon. In the standard RL setting, we want to learn a policy π that maximises J (π) = E T0,T,π H−1 t=0 γ t R(r t+1 |s t , a t , s t+1 ) , the expected return. Here, we consider a multi-task meta-learning setting, which we introduce next.</p>
<p>TRAINING SETUP</p>
<p>We adopt the standard meta-learning setting where we have a distribution p(M ) over MDPs from which we can sample during meta-training, with an MDP M i ∼ p(M ) defined by a tuple M i = (S, A, R i , T i , T i,0 , γ, H). Across tasks, the reward and transition functions vary but share some structure. The index i represents an unknown task description (e.g., a goal position or natural language instruction) or task ID. Sampling an MDP from p(M ) is typically done by sampling a reward and transition function from a distribution p(R, T ). During meta-training, batches of tasks are repeatedly sampled, and a small training procedure is performed on each of them, with the goal of learning to learn (for an overview of existing methods see Sec 4). At meta-test time, the agent is evaluated based on the average return it achieves during learning, for tasks drawn from p. Doing this well requires at least two things: (1) incorporating prior knowledge obtained in related tasks, and (2) reasoning about task uncertainty when selecting actions to trade off exploration and exploitation. In the following, we combine ideas from meta-learning and Bayesian RL to tackle these challenges.</p>
<p>BAYESIAN REINFORCEMENT LEARNING</p>
<p>When the MDP is unknown, optimal decision making has to trade off exploration and exploitation when selecting actions. In principle, this can be done by taking a Bayesian approach to reinforcement learning formalised as a Bayes-Adaptive MDP (BAMDP), the solution to which is a Bayesoptimal policy (Bellman, 1956;Duff &amp; Barto, 2002;Ghavamzadeh et al., 2015).</p>
<p>In the Bayesian formulation of RL, we assume that the transition and reward functions are distributed according to a prior b 0 = p(R, T ). Since the agent does not have access to the true reward and transition function, it can maintain a belief b t (R, T ) = p(R, T |τ :t ), which is the posterior over the MDP given the agent's experience τ :t = {s 0 , a 0 , r 1 , s 1 , a 1 , . . . , s t } up until the current timestep. This is often done by maintaining a distribution over the model parameters.</p>
<p>To allow the agent to incorporate the task uncertainty into its decision-making, this belief can be augmented to the state, resulting in hyper-states s + t ∈ S + = S × B, where B is the belief space. These transition according to
T + (s + t+1 |s + t , a t , r t ) = T + (s t+1 , b t+1 |s t , a t , r t , b t ) = T + (s t+1 |s t , a t , b t ) T + (b t+1 |s t , a t , r t , b t , s t+1 ) = E bt [T (s t+1 |s t , a t )] δ(b t+1 = p(R, T |τ :t+1 ))(1)
i.e., the new environment state s t is the expected new state w.r.t. the current posterior distribution of the transition function, and the belief is updated deterministically according to Bayes rule. The reward function on hyper-states is defined as the expected reward under the current posterior (after the state transition) over reward functions,
R + (s + t , a t , s + t+1 ) = R + (s t , b t , a t , s t+1 , b t+1 ) = E bt+1 [R(s t , a t , s t+1 )] .(2)
This results in a BAMDP M + = (S + , A, R + , T + , T + 0 , γ, H + ) (Duff &amp; Barto, 2002), which is a special case of a belief MDP, i.e, the MDP formed by taking the posterior beliefs maintained by an agent in a partially observable MDP and reinterpreting them as Markov states (Cassandra et al., 1994). In an arbitrary belief MDP, the belief is over a hidden state that can change over time. In a BAMDP, the belief is over the transition and reward functions, which are constant for a given task. The agent's objective is now to maximise the expected return in the BAMDP,
J + (π) = E b0,T + 0 ,T + ,π   H + −1 t=0 γ t R + (r t+1 |s + t , a t , s + t+1 )   ,(3)
i.e., maximise the expected return in an initially unknown environment, while learning, within the horizon H + . Note the distinction between the MDP horizon H and BAMDP horizon H + . Although they often coincide, we might instead want the agent to act Bayes-optimal within the first N MDP episodes, so H + =N × H. Trading off exploration and exploitation optimally depends heavily on how much time the agent has left (e.g., to decide whether information-seeking actions are worth it).</p>
<p>The objective in (3) is maximised by the Bayes-optimal policy, which automatically trades off exploration and exploitation: it takes exploratory actions to reduce its task uncertainty only insofar as it helps to maximise the expected return within the horizon. The BAMDP framework is powerful because it provides a principled way of formulating Bayes-optimal behaviour. However, solving the BAMDP is hopelessly intractable for most interesting problems.</p>
<p>The main challenges are as follows.</p>
<p>• We typically do not know the parameterisation of the true reward and/or transition model, • The belief update (computing the posterior p(R, T |τ :t )) is often intractable, and • Even with the correct posterior, planning in belief space is typically intractable.</p>
<p>In the following, we propose a method that simultaneously meta-learns the reward and transition functions, how to perform inference in an unknown MDP, and how to use the belief to maximise expected online return. Since the Bayes-adaptive policy is learned end-to-end with the inference framework, no planning is necessary at test time. We make minimal assumptions (no privileged task information is required during training), resulting in a highly flexible and scalable approach to Bayes-adaptive Deep RL.</p>
<p>BAYES-ADAPTIVE DEEP RL VIA META-LEARNING</p>
<p>In this section, we present variBAD, and describe how we tackle the challenges outlined above. We start by describing how to represent reward and transition functions, and (posterior) distributions over these. We then consider how to meta-learn to perform approximate variational inference in a given task, and finally put all the pieces together to form our training objective.</p>
<p>In the typical meta-learning setting, the reward and transition functions that are unique to each MDP are unknown, but also share some structure across the MDPs M i in p(M ). We know that there exists a true i which represents either a task description or task ID, but we do not have access to this information. We therefore represent this value using a learned stochastic latent variable m i . For a given MDP M i we can then write
R i (r t+1 |s t , a t , s t+1 ) ≈ R(r t+1 |s t , a t , s t+1 ; m i ), (4) T i (s t+1 |s t , a t ) ≈ T (s t+1 |s t , a t ; m i ),(5)
where R and T are shared across tasks. Since we do not have access to the true task description or ID, we need to infer m i given the agent's experience up to time step t collected in M i , τ</p>
<p>:t = (s 0 , a 0 , r 1 , s 1 , a 1 , r 2 , . . . , s t−1 , a t−1 , r t , s t ),</p>
<p>i.e., we want to infer the posterior distribution p(m i |τ</p>
<p>:t ) over m i given τ</p>
<p>:t (from now on, we drop the sub-and superscript i for ease of notation).</p>
<p>Recall that our goal is to learn a distribution over the MDPs, and given a posteriori knowledge of the environment compute the optimal action. Given the above reformulation, it is now sufficient to Figure 2: VariBAD architecture: A trajectory of states, actions and rewards is processed online using an RNN to produce the posterior over task embeddings, q φ (m|τ :t ). The posterior is trained using a decoder which attempts to predict past and future states and rewards from current states and actions. The policy conditions on the posterior in order to act in the environment and is trained using RL.</p>
<p>reason about the embedding m, instead of the transition and reward dynamics. This is particularly useful when deploying deep learning strategies, where the reward and transition function can consist of millions of parameters, but the embedding m can be a small vector.</p>
<p>APPROXIMATE INFERENCE</p>
<p>Computing the exact posterior is typically not possible: we do not have access to the MDP (and hence the transition and reward function), and marginalising over tasks is computationally infeasible. Consequently, we need to learn a model of the environment p θ (τ :H + |a :H + −1 ), parameterised by θ, together with an amortised inference network q φ (m|τ :t ), parameterised by φ, which allows fast inference at runtime at each timestep t. The action-selection policy is not part of the MDP, so an environmental model can only give rise to a distribution of trajectories when conditioned on actions, which we typically draw from our current policy, a ∼ π. At any given time step t, our model learning objective is thus to maximise
E ρ(M,τ :H + ) [log p θ (τ :H + |a :H + −1 )] ,(7)
where ρ(M, τ :H + ) is the trajectory distribution induced by our policy and we slightly abuse notation by denoting by τ the state-reward trajectories, excluding the actions. In the following, we drop the conditioning on a :H + −1 to simplify notation.</p>
<p>Instead of optimising (7), which is intractable, we can optimise a tractable lower bound, defined with a learned approximate posterior q φ (m|τ :t ) which can be estimated by Monte Carlo sampling (for the full derivation see AppendixA):
E ρ(M,τ :H + ) [log p θ (τ :H + )] ≥ E ρ E q φ (m|τ:t) [log p θ (τ :H + |m)] − KL(q φ (m|τ :t )||p θ (m)) (8) = ELBO t .
The term E q [log p(τ :H + |m)] is often referred to as the reconstruction loss, and p(τ :t |m) as the decoder. The term KL(q(m|τ :t )||p θ (m)) is the KL-divergence between our variational posterior q φ and the prior over the embeddings p θ (m). We set the prior to our previous posterior, q φ (m|τ :t−1 ), with initial prior q φ (m) = N (0, I).</p>
<p>As can be seen in Equation (8) and Figure 2, when the agent is at timestep t, we encode the past trajectory τ :t to get the current posterior q(m|τ :t ) since this is all the information available to perform inference about the current task. We then decode the entire trajectory τ :H + including the future, i.e., model E q [p(τ :H + |m)]. This is different than the conventional VAE setup (and possible since we have access to this information during training). Decoding not only the past but also the future is important because this way, variBAD learns to perform inference about unseen states given the past.</p>
<p>The reconstruction term log p(τ :H + |m) factorises as
log p(τ :H + |m, a :H + −1 ) = log p((s 0 , r 0 , . . . , s t−1 , r t−1 , s t )|m, a :H + −1 ) (9) = log p(s 0 |m) + H + −1 i=0 [log p(s i+1 |s i , a i , m) + log p(r i+1 |s i , a i , s i+1 , m)] .
Here, p(s 0 |m) is the initial state distribution T 0 , p(s i+1 |s i , a i ; m) the transition function T , and p(r i+1 |s t , a t , s i+1 ; m) the reward function R . From now, we include T 0 in T for ease of notation.</p>
<p>TRAINING OBJECTIVE</p>
<p>We can now formulate a training objective for learning the approximate posterior distribution over task embeddings, the policy, and the generalised reward and transition functions R and T . We use deep neural networks to represent the individual components. These are:</p>
<ol>
<li>
<p>The encoder q φ (m|τ :t ), parameterised by φ;</p>
</li>
<li>
<p>An approximate transition function T = p T θ (s i+1 |s i , a i ; m) and an approximate reward function R = p R θ (r i+1 |s t , a t , s i+1 ; m) which are jointly parameterised by θ; and 3. A policy π ψ (a t |s t , q φ (m|τ :t )) parameterised by ψ and dependent on φ.</p>
</li>
</ol>
<p>The policy is conditioned on both the environment state and the posterior over m, π(a t |s t , q(m|τ :t )). This is similar to the formulation of BAMDPs introduced in 2.2, with the difference that we learn a unifying distribution over MDP embeddings, instead of the transition/reward function directly. This makes learning easier since there are fewer parameters to perform inference over, and we can use data from all tasks to learn the shared reward and transition function. The posterior can be represented by the distribution's parameters (e.g., mean and standard deviation if q is Gaussian).</p>
<p>Our overall objective is to maximise In Equation (10), we see that the ELBO appears for all possible context lengths t. This way, variBAD can learn how to perform inference online (while the agent is interacting with an environment), and decrease its uncertainty over time given more data. In practice, we may subsample a fixed number of ELBO terms (for random time steps t) for computational efficiency if H + is large.
L(φ, θ, ψ) = E p(M )   J (ψ, φ) + λ H + t=0 ELBO t (φ, θ)   .(10)
Equation (10) is trained end-to-end, and λ weights the supervised model learning objective against the RL loss. This is necessary since parameters φ are shared between the model and the policy. However, we found that backpropagating the RL loss through the encoder is typically unnecessary in practice. Not doing so also speeds up training considerably, avoids the need to trade off these losses, and prevents interference between gradients of opposing losses. In our experiments, we therefore optimise the policy and the VAE using different optimisers and learning rates. We train the RL agent and the VAE using different data buffers: the policy is only trained with the most recent data since we use on-policy algorithms in our experiments; and for the VAE we maintain a separate, larger buffer of observed trajectories.</p>
<p>At meta-test time, we roll out the policy in randomly sampled test tasks (via forward passes through the encoder and policy) to evaluate performance. The decoder is not used at test time, and no gradient adaptation is done: the policy has learned to act approximately Bayes-optimal during meta-training.</p>
<p>RELATED WORK</p>
<p>Meta Reinforcement Learning. A prominent model-free meta-RL approach is to utilise the dynamics of recurrent networks for fast adaptation (RL 2 , Wang et al. (2016); Duan et al. (2016)). At every time step, the network gets an auxiliary comprised of the preceding action and reward. This allows learning within a task to happen online, entirely in the dynamics of the recurrent network. If we remove the decoder (Fig 2) and the VAE objective (Eq (7)), variBAD reduces to this setting, i.e., the main differences are that we use a stochastic latent variable (an inductive bias for representing uncertainty) together with a decoder to reconstruct previous and future transitions / rewards (which acts as an auxiliary loss (Jaderberg et al., 2017) to encode the task in latent space and deduce information about unseen states). Ortega et al. (2019) provide an in-depth discussion of meta-learning sequential strategies and how to recast memory-based meta-learning within a Bayesian framework.</p>
<p>Another popular approach to meta RL is to learn an initialisation of the model, such that at test time, only a few gradient steps are necessary to achieve good performance (Finn et al., 2017;Nichol &amp; Schulman, 2018). These methods do not directly account for the fact that the initial policy needs to explore, a problem addressed, a.o., by Stadie et al. (2018) (2017) who learn a meta-critic that can criticise any actor for any task, and is used at test time to train a policy. Compared to variBAD, these methods usually separate exploration (before gradient adaptation) and exploitation (after gradient adaptation) at test time by design, making them less sample efficient. VariBAD differs from the above methods mainly in what the embedding represents (i.e., task uncertainty) and how it is used: the policy conditions on the posterior distribution over MDPs, allowing it to reason about task uncertainty and trade off exploration and exploitation online. Our objective (8) explicitly optimises for Bayes-optimal behaviour. Unlike some of the above methods, we do not use the model at test time, but model-based planning is a natural extension for future work.</p>
<p>Bayesian Reinforcement Learning. Bayesian methods for RL can be used to quantify uncertainty to support action-selection, and provide a way to incorporate prior knowledge into the algorithms (see Ghavamzadeh et al. (2015) for a review). A Bayes-optimal policy is one that optimally trades off exploration and exploitation, and thus maximises expected return during learning. While such a policy can in principle be computed using the BAMDP framework, it is hopelessly intractable for all but the smallest tasks. Existing methods are therefore restricted to small and discrete state / action spaces (Asmuth &amp; Littman, 2011;Guez et al., 2012;, or a discrete set of tasks (Brunskill, 2012;Poupart et al., 2006). VariBAD opens a path to tractable approximate Bayes-optimal exploration for deep RL by leveraging ideas from meta-learning and approximate variational inference, with the only assumption that we can meta-train on a set of related tasks. Existing approximate Bayesian RL methods often require us to define a prior / belief update on the reward / transition function, and rely on (possibly expensive) sample-based planning procedures. Due to the use of deep neural networks however, variBAD lacks the formal guarantees enjoyed by some of the methods mentioned above.</p>
<p>Closely related to our approach is the recent work of Humplik et al. (2019). Like variBAD, they condition the policy on a posterior distribution over the MDP, which is meta-trained using privileged information such as a task description. In comparison, variBAD meta-learns to represent the belief in an unsupervised way, and does not rely on privileged task information during training.</p>
<p>Posterior sampling (Strens, 2000;Osband et al., 2013), which extends Thompson sampling (Thompson, 1933) from bandits to MDPs, estimates a posterior distribution over MDPs (i.e., model and reward functions), in the same spirit as variBAD. This posterior is used to periodically sample a single hypothesis MDP (e.g., at the beginning of an episode), and the policy that is optimal for the sampled MDP is followed subsequently. This approach is less efficient than Bayes-optimal behaviour and therefore typically has lower expected return during learning. Research in this area is often concerned with developing tight bounds by putting assumptions on the context, such as having a small known number of contexts, or that there is a linear relationship between the contexts and dynamics/rewards. Similarly, the framework of hidden parameter (HiP-) MDPs assumes that there is a set of low-dimensional latent factors which define a family of related dynamical systems (with shared reward structure), similar to the assumption we make in Equation (5) (Doshi-Velez &amp; Konidaris, 2016;Killian et al., 2017;Yao et al., 2018). These methods however dont directly learn Bayes-optimal behaviour but allow for a longer training period in new environments to infer the latents and train the policy.</p>
<p>Variational Inference and Meta-Learning. A main difference of variBAD to many existing Bayesian RL methods is that we meta-learn the inference procedure, i.e., how to do a posterior update. Apart from (RL) methods mentioned above, related work in this direction can be found, a.o., in Garnelo et al. (2018); Gordon et al. (2019); Choi et al. (2019). By comparison, variBAD has an inference procedure tailored to the setting of Bayes-optimal RL.</p>
<p>POMDPs. Several deep learning approaches to model-free reinforcement learning (Igl et al., 2019) and model learning for planning (Tschiatschek et al., 2018) in partially observable Markov decision processes have recently been proposed and utilise approximate variational inference methods. VariBAD by contrast focuses on BAMDPs (Martin, 1967;Duff &amp; Barto, 2002;Ghavamzadeh et al., 2015), a special case of POMDPs where the transition and reward functions constitute the hidden state and the agent must maintain a belief over them. While in general the hidden state in a POMDP can change at each time-step, in a BAMDP the underlying task, and therefore the hidden state, is fixed per task. We exploit this property by learning an embedding that is fixed over time, unlike approaches like Igl et al. (2019) </p>
<p>EXPERIMENTS</p>
<p>In this section we first investigate the properties of variBAD on a didactic gridworld domain. We show that variBAD performs structured and online exploration as it infers the task at hand. Then we consider more complex meta-learning settings by employing on four MuJoCo continuous control tasks commonly used in the meta-RL literature. We show that variBAD learns to adapt to the task during the first rollout, unlike many existing meta-learning methods. Details and hyperparameters can be found in the appendix, and at https://github.com/lmzintgraf/varibad.</p>
<p>GRIDWORLD</p>
<p>To gain insight into variBAD's properties, we start with a didactic gridworld environment. The task is to reach a goal (selected uniformly at random) in a 5 × 5 gridworld. The goal is unobserved by the agent, inducing task uncertainty and necessitating exploration. The goal can be anywhere except around the starting cell, which is at the bottom left. Actions are: up, right, down, left, stay (executed deterministically), and after 15 steps the agent is reset. The horizon within the MDP is H = 15, but we choose a horizon of H + = 4 × H = 45 for the BAMDP. I.e., we train our agent to maximise performance for 4 MDP episodes. The agent gets a sparse reward signal: −0.1 on non-goal cells, and +1 on the goal cell. The best strategy is to explore until the goal is found, and stay at the goal or return to it when reset to the initial position. We use a latent dimensionality of 5. Figure 3 illustrates how variBAD behaves at test time with deterministic actions (i.e., all exploration is done by the policy). In 3a we see how the agent interacts with the environment, with the blue background visualising the posterior belief by using the learned reward function. VariBAD learns the correct prior and adjusts its belief correctly over time. It predicts no reward for cells it has visited, and explores the remaining cells until it finds the goal.</p>
<p>A nice property of variBAD is that we can gain insight into the agent's belief about the environment by analysing what the decoder predicts, and how the latent space changes while the agent interacts with the environment. Figure 3b show the reward predictions: each line represents a grid cell and its value the probability of receiving a reward at that cell. As the agent gathers more data, more and more cells are excluded (p(rew = 1) = 0), until eventually the agent finds the goal. In Figure 3c we visualise the 5-dimensional latent space. We see that once the agent finds the goal, the posterior concentrates: the variance drops close to zero, and the mean settles on a value.</p>
<p>As we showed in Figure 1e, the behaviour of variBAD closely matches that of the Bayes-optimal policy. Recall that the Bayes-optimal policy is the one which optimally trades off exploration and exploitation in an unknown environment, and outperforms posterior sampling. Our results on this gridworld indicate that variBAD is an effective way to approximate Bayes-optimal control, and has the additional benefit of giving insight into the task belief of the policy.</p>
<p>Published as a conference paper at ICLR 2020 Figure 4: Average test performance for the first 5 rollouts of MuJoCo environments (using 5 seeds).</p>
<p>MUJOCO CONTINUOUS CONTROL META-LEARNING TASKS</p>
<p>We show that variBAD can scale to more complex meta learning settings by employing it on MuJoCo (Todorov et al., 2012) locomotion tasks commonly used in the meta-RL literature. 2 We consider the AntDir and HalfCheetahDir environment where the agent has to run either forwards or backwards (i.e., there are only two tasks), the HalfCheetahVel environment where the agent has to run at different velocities, and the Walker environment where the system parameters are randomised. Figure 4 shows the performance at test time compared to existing methods. While we show performance for multiple rollouts for the sake of completeness, anything beyond the first rollout is not directly relevant to our goal, which is to maximise performance on a new task, while learning, within a single episode. Only variBAD and RL 2 are able to adapt to the task at hand within a single episode. RL 2 underperforms variBAD on the HalfCheetahDir environment, and learning is slower and less stable (see learning curves and runtime comparisons in Appendix C). Even though the first rollout includes exploratory steps, this matches the optimal oracle policy (which is conditioned on the true task description) up to a small margin. The other methods ( 2019)) are not designed to maximise reward during a single rollout, and perform poorly in this case. They all require substantially more environment interactions in each new task to achieve good performance. PEARL, which is akin to posterior sampling, only starts performing well starting from the third episode (Note: PEARL outperforms our oracle slightly, likely since our oracle is based on PPO, and PEARL is based on SAC).</p>
<p>Overall, our empirical results confirm that variBAD can scale up to current benchmarks and maximise expected reward within a single episode.</p>
<p>CONCLUSION &amp; FUTURE WORK</p>
<p>We presented variBAD, a novel deep RL method to approximate Bayes-optimal behaviour, which uses meta-learning to utilise knowledge obtained in related tasks and perform approximate inference in unknown environments. In a didactic gridworld environment, our agent closely matches Bayesoptimal behaviour, and in more challenging MuJoCo tasks, variBAD outperforms existing methods in terms of achieved reward during a single episode. In summary, we believe variBAD opens a path to tractable approximate Bayes-optimal exploration for deep reinforcement learning.</p>
<p>There are several interesting directions of future work based on variBAD. For example, we currently do not use the decoder at test time. One could instead use the decoder for model-predictive planning, or to get a sense for how wrong the predictions are (which might indicate we are out of distribution, and further training is necessary). Another exciting direction for future research is considering settings where the training and test distribution of environments are not the same. Generalising to out-of-distribution tasks poses additional challenges and in particular for variBAD two problems are likely to arise: the inference procedure will be wrong (the prior and/or posterior update) and the policy will not be able to interpret a changed posterior. In this case, further training of both the encoder/decoder might be necessary, together with updates to the policy and/or explicit planning. 
E ρ(M,τ :H ) [log p θ (τ :H )] = E ρ log p θ (τ :H , m) q φ (m|τ :t ) q φ (m|τ :t ) dm = E ρ log E q φ (m|τ:t) p θ (τ :H , m) q φ (m|τ :t ) ≥ E ρ, q φ (m|τ:t) log p θ (τ :H , m) q φ (m|τ :t ) = E ρ, q φ (m|τ:t) [log p θ (τ :H |m) + log p θ (m) − log q φ (m|τ :t )] = E ρ E q φ (m|τ:t) [log p θ (τ :H |m)] − KL(q φ (m|τ :t )||p θ (m)) (11) = ELBO t .
B EXPERIMENTS: GRIDWORLD B.1 ADDITIONAL REMARKS Figure 3c visualises how the latent space changes as the agent interacts with the environment. As we can see, the value of the latent dimensions starts around mean 1 and variance 0, which is the prior we chose for the beginning of an episode. Given that the variance increases for a little bit before the agent finds the goal, this prior might not be optimal. A natural extension of variBAD is therefore to also learn the prior to match the task at hand.</p>
<p>B.2 HYPERPARAMETERS</p>
<p>We used the PyTorch framework for our experiments. Hyperparameters are listed below, and the source code can be found at https://github.com/lmzintgraf/varibad.  Figure 5: Results for the gridworld toy environment. Results are averages over 20 seeds (with 95% confidence intervals for the learning curve).</p>
<p>Hyperparameters for RL2 the same as above, with the following changes:</p>
<p>Policy architecture States are embedded using a fc linear layer, output size 32.</p>
<p>Rewards are embedding using a fc layer, output size 8.</p>
<p>Results are concatenate and passed to a GRU, hidden size 128, output size 32. After an additional fc layer with hidden size 32, the network outputs the actions. We used TanH activations throughout. Figure 5a shows the learning curves for variBAD and RL2, in comparison to an oracle policy (which has access to the goal position). We trained these policies on a horizon of H + = 4 × H = 60, i.e., on a BAMDP in which the agent has to maximise online return within four episodes. We indicate the values of a hard-coded Bayes-optimal policy, and a hard-coded posterior sampling policy using dashed lines. Figure 5b shows the end-performance of variBAD and RL2, compared to the hard-coded optimal policy (which has access to the goal position), Bayes-optimal policy, and posterior sampling policy. VariBAD and RL2 both closely approximate the Bayes-optimal solution. By inspecting the individual runs, we found that VariBAD learned the Bayes-optimal solution for 4 out of 20 seeds, RL2 zero times. Both otherwise find solutions that are very close to Bayes-optimal, with the difference that during the second rollout, the cells left to search are not all on the shortest path from the starting point.</p>
<p>B.3 COMPARISON TO RL2</p>
<p>Note that both variBAD and RL2 were trained on only four episodes, but we evaluate them on six episodes here. After the fourth rollout, we do not fix the latent / hidden state, but continue rolling out the policy as before. As we can see, the performance of RL2 drops again after the fourth episode: this is likely due to instabilities in the 128-dimensional hidden state. VariBAD's latent representation, the approximate task posterior, is concentrated and does not change with more data.</p>
<p>C EXPERIMENTS: MUJOCO C.1 LEARNING CURVES Figure 6 shows the learning curves for the MuJoCo environments for all approaches. The oracle policy was trained using PPO. PEARL (Rakelly et al., 2019) was trained using the reference implementation provided by the authors. The environments we used are also taken from this implementation. E-MAML (Stadie et al., 2018) and ProMP (Rothfuss et al., 2019) were trained using the reference implementation provided by Rothfuss et al. (2019). As we can see, PEARL is much more sample efficient in terms of number of frames than the other methods (Fig 6), which is because it is an off-policy method. On-policy vs off-policy training is an orthogonal issue to our contribution, but an extension of variBAD to off-policy methods is an interesting direction for future work. Doing posterior sampling using off-policy methods also requires PEARL to use a different encoder (to maintain order invariance of the sampled trajectories) which is non-recurrent (and hence faster to train, see next section) but restrictive since it assumes independence between individual transitions.</p>
<p>Note than in Figure 4, for the Walker environment evaluation, we used the models obtained after half the training time (5e + 7 frames) for variBAD and the Oracle, since performance declined again after that.</p>
<p>For all MuJoCo environments, we trained variBAD with a reward decoder only (even for Walker, where the dynamics change, we found that this has superior performance).</p>
<p>C.2 TRAINING DETAILS AND COMPARISON TO RL2</p>
<p>We are interested in maximising performance within a single rollout (H = 200). However in order to compare better to existing methods, we trained variBAD and the RL2 baseline to maximise performance within two rollouts (H + = 400) . We implemented task resets by adding a 'done' flag to the states, so that the agent knows when it gets reset in-between episodes. This allows us to evaluate on multiple rollouts (without re-setting the hidden states of the RNN) because the agents have learned to handle re-sets to the starting position.</p>
<p>We observe that RL2 is sometimes unstable when it comes to maintaining its performance over multiple rollouts, e.g., in the CheetahVel task ( Figure 6). We hypothesise that the drop of RL2's performance in CheetahVel occurs because it has not properly learned to deal with environment resets. The sudden change in state space (with includes joint positions and velocities) could lead to a dramatic shift in the hidden state, which then might not represent the task at hand properly. In addition, once the Cheetah is running at the correct velocity, it can infer which task it is in from its own velocity (which is part of the environment state) and stop doing inference, which might be another reason we observe this drop when the environment resets and the state suddenly has a different (very low) velocity. For variBAD this is less of a problem, since we train the latent embedding to represent the task, and only the task. Therefore, the agent does not have to do the inference procedure again when reset to the starting position, but can rely on the latent task description that is given by the approximate posterior. It might also just be due to implementation details, and, e.g., Mishra et al. (2017) do not observe this problem (see their Fig 4). </p>
<p>C.3 CHEETAHDIR TEST TIME BEHAVIOUR</p>
<p>To get a sense for where these differences between the different approaches might stem from, consider Figure 7 which shows example behaviour of the policies during the first three rollouts in the HalfCheetahDir environment, when the task is "go left". Both variBAD and RL 2 adapt to the task online, whereas PEARL acts according to the current sample, which in the first two rollouts can mean walking in the wrong direction. For a visualisation of the variBAD latent space at test time for this environment see Appendix C.5. While we outperform at meta-test time, PEARL is more sample efficient during meta-training (see Fig 6), since it is an off-policy method. Extending variBAD to off-policy methods is an interesting but orthogonal direction for future work.</p>
<p>C.4 RUNTIME COMPARISON</p>
<p>The following are rough estimates of average run-times for the HalfCheetah-Dir environment (from what we have experienced; we often ran multiple experiments per machine, so some of these might be overestimated and should be mostly understood as giving a relative sense of ordering).</p>
<p>• ProMP, E-MAML: 5-8 hours Even though both variBAD and RL 2 use recurrent modules, we observed that variBAD is faster when training the policy with PPO. This is because we do not backpropagate the RL-loss through the recurrent part, which allows us to make the PPO mini-batch updates without having to re-compute the embeddings (so it saves us a lot of forward/backward passes through the recurrent model). This difference is less pronounced with other RL methods that do not rely on this many forward/backward passes per policy update. </p>
<p>C.5 LATENT SPACE VISUALISATION</p>
<p>A nice feature of variBAD is that it can give us insight into the uncertainty of the agent about what task it is in. Figure 8 shows the latent space for the HalfCheetahDir tasks "go right" (top row) and "go left" (bottom row). We observe that the latent mean and log-variance adapt rapidly, within just a few environment steps (left and middle figures). This is also how fast the agent adapts to the current task (right figures). As expected, the variance decreases over time as the agent gets more certain. It is interesting to note that the values of the latent dimensions swap signs between the two tasks.</p>
<p>Visualising the belief in the reward/state space directly, as we have done in the gridworld example, is more difficult for MuJoCo tasks, since we now have continuous states and actions. What we could do instead, is to additionally train a model that predicts a ground-truth task description (separate from the main objective and just for further analysis, since we do not want to use this privileged information for meta-training). This would give us a more direct sense of what task it thinks it is in.</p>
<p>C.6 HYPERPARAMETERS</p>
<p>We used the PyTorch framework (Paszke et al., 2017) </p>
<p>Figure 1 :
1Illustration of different exploration strategies. (a) Environment:</p>
<p>Expectations are approximated by Monte Carlo samples, and the ELBO can be optimised using the reparameterisation trick(Kingma &amp; Welling, 2014). For t = 0, we use the prior q φ (m) = N (0, I). We encode past trajectories using a recurrent network as inDuan et al. (2016);Wang et al. (2016), but other types of encoders could be considered like the ones used inZaheer et al. (2017);Garnelo et al. (2018);Rakelly et al. (2019). The network architecture is shown inFigure 2.</p>
<p>Skill / Task Embeddings. Learning (variational) task or skill embeddings for meta / transfer reinforcement learning is used in a variety of approaches.Hausman et al. (2018)  use approximate variational inference learn an embedding space of skills (with a different lower bound than variBAD). At test time the policy is fixed, and a new embedder is learned that interpolates between already learned skills.Arnekvist et al. (2019) learn a stochastic embedding of optimal Q-functions for different skills, and condition the policy on (samples of) this embedding. Adaptation at test time is done in latent space. Co-Reyes et al. (2018) learn a latent space of low-level skills that can be controlled by a higher-level policy, framed within the setting of hierarchical RL. This embedding is learned using a VAE to encode state trajectories and decode states and actions. Zintgraf et al. (2019) learn a deterministic task embedding trained similarly to MAML(Finn et al., 2017). Similar to variBAD,Zhang et al. (2018) use learned dynamics and reward modules to learn a latent representation which the policy conditions on and show that transferring the (fixed) encoder to new environments helps learning.Perez et al. (2018) learn dynamic models with auxiliary latent variables, and use them for model-predictive control. Lan et al. (2019) learn a task embedding with an optimisation procedure similar to MAML, where the encoder is updated at test time, and the policy is fixed.Saemundsson et al. (2018) explicitly learn an embedding of the environment model, which is subsequently used for model predictive control (and not, like in variBAD, for exploration). In the field of imitation learning, some approaches embed expert demonstrations to represent the task; e.g.,Wang et al. (2017) use variational methods andDuan et al. (2017) learn deterministic embeddings.</p>
<p>A
related approach for inter-task transfer of abstract knowledge is to pose policy search with priors as Markov Chain Monte Carlo inference(Wingate et al., 2011). SimilarlyGuez et al. (2013)   propose a Monte Carlo Tree Search based method for Bayesian planning to get a tractable, samplebased method for obtaining approximate Bayes-optimal behaviour.Osband et al. (2018) note that non-Bayesian treatment for decision making can be arbitrarily suboptimal and propose a simple randomised prior based approach for structured exploration. Some recent deep RL methods use stochastic latent variables for structured exploration(Gupta et al., 2018;Rakelly et al., 2019), which gives rise to behaviour similar to posterior sampling. Other ways to use the posterior for exploration are, e.g., certain reward bonuses Kolter &amp; Ng (2009);Sorg et al. (2012) and methods based on optimism in the face of uncertainty(Kearns &amp; Singh, 2002;Brafman &amp; Tennenholtz, 2002). Non-Bayesian methods for exploration are often used in practice, such as other exploration bonuses (e.g., via state-visitation counts) or using uninformed sampling of actions (e.g., -greedy action selection). Such methods are prone to wasteful exploration that does not help maximise expected reward.Related to BAMDPs are contextual MDPs, where the task description is referred to as a context, on which the environment dynamics and rewards depend(Hallak et al., 2015; Jiang et al., 2017;Dann et al., 2018;Modi &amp; Tewari, 2019).</p>
<p>Figure 3 :
3Behaviour of variBAD in the gridworld environment. (a) Hand-picked but representative example test rollout. The blue background indicates the posterior probability of receiving a reward at that cell. (b) Probability of receiving a reward for each cell, as predicted by the decoder, over the course of interacting with the environment (average in black, goal state in green). (c) Visualisation of the latent space; each line is one latent dimension, the black line is the average.</p>
<p>PEARL Rakelly et al. (2019), E-MAML Stadie et al. (2018) and ProMP Rothfuss et al. (</p>
<p>Figure 6 :
6Learning curves for the MuJoCo results presented in Section 5.2. The top row shows performance evaluated at the first rollout, and the second row shows the performance at the N -th rollout. For variBAD and RL2, N = 2. For ProMP and E-MAML, N = 20. For PEARL, N = 10.</p>
<p>Figure 7 :
7Behaviour at test time for the for the task "walk left" in HalfCheetahDir. The x-axis reflects the position of the agent; the y-axis the steps in the environment (to be read from bottom to top). Rows are separate examples, columns the number of rollouts.</p>
<p>Figure 8 :
8Visualisation of the latent space at meta-test time, for the HalfCheetahDir environment and the tasks "go right" (top) and the task "go left" (bottom). Left: value of the posterior mean during a single rollout (200 environment steps). The black line is the average value. Middle: value of the posterior log-variance during a single rollout. Right: Behaviour of the policy during a single rollout. The x-axis show the position of the Cheetah, and the y-axis the step (should be read from bottom to top).</p>
<p>• variBAD: 48 hours • RL 2 : 60 hours • PEARL: 24 hours E-MAML and ProMP have the advantage that they do not have a recurrent part such as variBAD or RL 2 . Forward and backward passes through recurrent networks can be slow, especially with large horizons.</p>
<p>for our experiments. The default arguments for our MuJoCo experiments can be found below, for details see our reference implementation at https://github.com/lmzintgraf/varibad. hidden layers, 128 nodes each, TanH activations Encoder architecture States, actions, rewards encoder: FC layer (32/16/16-dim), GRU with hidden size 128, output layer with 5 outputs, ReLu activations Reward decoder architecture 2 hidden layers, 64 and 32 nodes, ReLu activations Reward decoder loss function Mean squared errorRL Algorithm 
PPO 
Batch size 
3200 
Epochs 
2 
Minibatches 
4 
Max grad norm 
0.5 
Clip parameter 
0.1 
Value loss coefficient 
0.5 
Entropy coefficient 
0.01 
Notes 
We use a Huber loss in the RL loss 
Weight of KL term in ELBO 
0.1 
Policy LR 
0.0007 
Policy VAE 
0.001 
Task embedding size 
5 
Policy architecture 
2 
We use the terms environment, task, and MDP, interchangeably.
Environments taken from https://github.com/katerakelly/oyster.
ACKNOWLEDGMENTSWe thank Anuj Mahajan who contributed to early work on this topic. We thank Joost van Amersfoort, Andrei Rusu and Dushyant Rao for useful discussions and feedback. Luisa Zintgraf is supported by the Microsoft Research PhD Scholarship Program. Maximilian Igl is supported by the UK EPSRC CDT in Autonomous Intelligent Machines and Systems. Sebastian Schulze is supported by Dyson. This work was supported by a generous equipment grant and a donated DGX-1 from NVIDIA. This project has received funding from the European Research Council under the European Union's Horizon 2020 research and innovation programme (grant agreement number 637713).
Vpe: Variational policy embedding for transfer reinforcement learning. Isac Arnekvist, Danica Kragic, Johannes A Stork, International Conference on Robotics and Automation. Isac Arnekvist, Danica Kragic, and Johannes A Stork. Vpe: Variational policy embedding for transfer reinforcement learning. In International Conference on Robotics and Automation, 2019.</p>
<p>Learning is planning: near bayes-optimal reinforcement learning via monte-carlo tree search. John Asmuth, Michael L Littman, Conf on Uncertainty in Artificial Intelligence. John Asmuth and Michael L Littman. Learning is planning: near bayes-optimal reinforcement learning via monte-carlo tree search. In Conf on Uncertainty in Artificial Intelligence, 2011.</p>
<p>A problem in the sequential design of experiments. Richard Bellman, Sankhyā: The Indian Journal of Statistics. 163/4Richard Bellman. A problem in the sequential design of experiments. Sankhyā: The Indian Journal of Statistics (1933-1960), 16(3/4):221-229, 1956.</p>
<p>R-max-a general polynomial time algorithm for nearoptimal reinforcement learning. I Ronen, Moshe Brafman, Tennenholtz, Journal of Machine Learning Research. 3Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for near- optimal reinforcement learning. Journal of Machine Learning Research, pp. 3:213-231, 2002.</p>
<p>Bayes-optimal reinforcement learning for discrete uncertainty domains. Emma Brunskill, International Conference on Autonomous Agents and Multiagent Systems. Emma Brunskill. Bayes-optimal reinforcement learning for discrete uncertainty domains. In Inter- national Conference on Autonomous Agents and Multiagent Systems, 2012.</p>
<p>Acting optimally in partially observable stochastic domains. Anthony R Cassandra, Leslie Pack Kaelbling, Michael L Littman, Twelfth National Conference on Artificial Intelligence. AAAI Classic Paper AwardAnthony R. Cassandra, Leslie Pack Kaelbling, and Michael L. Littman. Acting optimally in partially observable stochastic domains. In Twelfth National Conference on Artificial Intelligence, 1994. AAAI Classic Paper Award, 2013.</p>
<p>Meta-amortized variational inference and learning. Kristy Choi, Mike Wu, Noah Goodman, Stefano Ermon, International Conference on Learning Representation. Kristy Choi, Mike Wu, Noah Goodman, and Stefano Ermon. Meta-amortized variational inference and learning. In International Conference on Learning Representation, 2019.</p>
<p>Self-consistent trajectory autoencoder: Hierarchical reinforcement learning with trajectory embeddings. D John, Yuxuan Co-Reyes, Abhishek Liu, Benjamin Gupta, Pieter Eysenbach, Sergey Abbeel, Levine, International Conference on Machine Learning. John D Co-Reyes, YuXuan Liu, Abhishek Gupta, Benjamin Eysenbach, Pieter Abbeel, and Sergey Levine. Self-consistent trajectory autoencoder: Hierarchical reinforcement learning with trajec- tory embeddings. In International Conference on Machine Learning, 2018.</p>
<p>Christoph Dann, Lihong Li, Wei Wei, Emma Brunskill, arXiv:1811.03056Policy certificates: Towards accountable reinforcement learning. arXiv preprintChristoph Dann, Lihong Li, Wei Wei, and Emma Brunskill. Policy certificates: Towards accountable reinforcement learning. arXiv preprint arXiv:1811.03056, 2018.</p>
<p>Hidden parameter markov decision processes: A semiparametric regression approach for discovering latent task parametrizations. Finale Doshi, - Velez, George Konidaris, International Joint Conference on Artificial Intelligence. Finale Doshi-Velez and George Konidaris. Hidden parameter markov decision processes: A semi- parametric regression approach for discovering latent task parametrizations. In International Joint Conference on Artificial Intelligence, 2016.</p>
<p>RL 2 : Fast reinforcement learning via slow reinforcement learning. Yan Duan, John Schulman, Xi Chen, L Peter, Ilya Bartlett, Pieter Sutskever, Abbeel, arXiv:1611.02779Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. RL 2 : Fast reinforcement learning via slow reinforcement learning. arXiv:1611.02779, 2016.</p>
<p>One-shot imitation learning. Yan Duan, Marcin Andrychowicz, Bradly Stadie, Jonathan Openai, Jonas Ho, Ilya Schneider, Pieter Sutskever, Wojciech Abbeel, Zaremba, Advances in Neural Information Processing Systems. Yan Duan, Marcin Andrychowicz, Bradly Stadie, OpenAI Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter Abbeel, and Wojciech Zaremba. One-shot imitation learning. In Advances in Neural Information Processing Systems, 2017.</p>
<p>Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes. O&apos;gordon Michael, Andrew Duff, Barto, Univ of Massachusetts at AmherstPhD thesisMichael O'Gordon Duff and Andrew Barto. Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes. PhD thesis, Univ of Massachusetts at Amherst, 2002.</p>
<p>Model-agnostic meta-learning for fast adaptation of deep networks. Chelsea Finn, Pieter Abbeel, Sergey Levine, International Conference on Machine Learning. Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning, 2017.</p>
<p>Neural processes. Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, J Danilo, Rezende, Yee Whye Eslami, Teh, ICML 2018 Workshop on Theoretical Foundations and Applications of Deep Generative Models. Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J Rezende, SM Eslami, and Yee Whye Teh. Neural processes. In ICML 2018 Workshop on Theoretical Foundations and Applications of Deep Generative Models, 2018.</p>
<p>Bayesian reinforcement learning: A survey. Foundations and Trends R in Machine Learning. Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, Aviv Tamar, 8Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, Aviv Tamar, et al. Bayesian reinforcement learning: A survey. Foundations and Trends R in Machine Learning, 8(5-6):359-483, 2015.</p>
<p>Trading off scientific knowledge and user learning with multi-armed bandits. Yun-En Liu, Travis Mandel, Emma Brunskill, Zoran Popovic, EDM. Yun-En Liu, Travis Mandel, Emma Brunskill, and Zoran Popovic. Trading off scientific knowledge and user learning with multi-armed bandits. In EDM, pp. 161-168, 2014.</p>
<p>Bayesian decision problems and Markov chains. James John Martin, WileyJames John Martin. Bayesian decision problems and Markov chains. Wiley, 1967.</p>
<p>A simple neural attentive metalearner. Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, Pieter Abbeel, arXiv:1707.03141Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta- learner. arXiv:1707.03141, 2017.</p>
<p>Contextual markov decision processes using generalized linear models. Aditya Modi, Ambuj Tewari, Reinforcement Learning for Real Life (RL4RealLife) Workshop at the International Conference on Machine Learning. Aditya Modi and Ambuj Tewari. Contextual markov decision processes using generalized linear models. In Reinforcement Learning for Real Life (RL4RealLife) Workshop at the International Conference on Machine Learning, 2019.</p>
<p>Alex Nichol, John Schulman, arXiv:1803.02999Reptile: a scalable metalearning algorithm. Alex Nichol and John Schulman. Reptile: a scalable metalearning algorithm. arXiv:1803.02999, 2018.</p>
<p>A Pedro, Jane X Ortega, Mark Wang, Tim Rowland, Zeb Genewein, Razvan Kurth-Nelson, Nicolas Pascanu, Joel Heess, Alex Veness, Pablo Pritzel, Sprechmann, arXiv:1905.03030Meta-learning of sequential strategies. Pedro A Ortega, Jane X Wang, Mark Rowland, Tim Genewein, Zeb Kurth-Nelson, Razvan Pascanu, Nicolas Heess, Joel Veness, Alex Pritzel, Pablo Sprechmann, et al. Meta-learning of sequential strategies. arXiv:1905.03030, 2019.</p>
<p>(more) efficient reinforcement learning via posterior sampling. Ian Osband, Daniel Russo, Benjamin Van Roy, Advances in Neural Information Processing Systems. Ian Osband, Daniel Russo, and Benjamin Van Roy. (more) efficient reinforcement learning via posterior sampling. In Advances in Neural Information Processing Systems, 2013.</p>
<p>Randomized prior functions for deep reinforcement learning. Ian Osband, John Aslanides, Albin Cassirer, Advances in Neural Information Processing Systems. Ian Osband, John Aslanides, and Albin Cassirer. Randomized prior functions for deep reinforcement learning. In Advances in Neural Information Processing Systems, 2018.</p>
<p>Automatic differentiation in pytorch. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer, Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.</p>
<p>Felipe Petroski Such, and Theofanis Karaletsos. Efficient transfer learning and online adaptation with latent variable models for continuous control. F Christian, Perez, Continual Learning Workshop. NeurIPSChristian F Perez, Felipe Petroski Such, and Theofanis Karaletsos. Efficient transfer learning and online adaptation with latent variable models for continuous control. In Continual Learning Work- shop, NeurIPS 2018, 2018.</p>
<p>An analytic solution to discrete bayesian reinforcement learning. Pascal Poupart, Nikos Vlassis, Jesse Hoey, Kevin Regan, International Conference on Machine Learning. Pascal Poupart, Nikos Vlassis, Jesse Hoey, and Kevin Regan. An analytic solution to discrete bayesian reinforcement learning. In International Conference on Machine Learning, 2006.</p>
<p>Efficient off-policy meta-reinforcement learning via probabilistic context variables. Kate Rakelly, Aurick Zhou, Deirdre Quillen, Chelsea Finn, Sergey Levine, International Conference on Machine Learning. Kate Rakelly, Aurick Zhou, Deirdre Quillen, Chelsea Finn, and Sergey Levine. Efficient off-policy meta-reinforcement learning via probabilistic context variables. In International Conference on Machine Learning, 2019.</p>
<p>Promp: Proximal meta-policy search. Jonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, Pieter Abbeel, International Conference on Learning Representation. Jonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, and Pieter Abbeel. Promp: Proximal meta-policy search. In International Conference on Learning Representation, 2019.</p>
<p>Meta reinforcement learning with latent variable gaussian processes. Steindór Saemundsson, Katja Hofmann, Marc Peter Deisenroth, Conference on Uncertainty in Artificial Intelligence. Steindór Saemundsson, Katja Hofmann, and Marc Peter Deisenroth. Meta reinforcement learning with latent variable gaussian processes. In Conference on Uncertainty in Artificial Intelligence, 2018.</p>
<p>Variance-based rewards for approximate bayesian reinforcement learning. Jonathan Sorg, Satinder Singh, Richard L Lewis, Conference on Uncertainty in Artificial Intelligence. Jonathan Sorg, Satinder Singh, and Richard L Lewis. Variance-based rewards for approximate bayesian reinforcement learning. In Conference on Uncertainty in Artificial Intelligence, 2012.</p>
<p>Some considerations on learning to explore via meta-reinforcement learning. C Bradly, Ge Stadie, Rein Yang, Xi Houthooft, Yan Chen, Yuhuai Duan, Pieter Wu, Ilya Abbeel, Sutskever, Advances in Neural Processing Systems. Bradly C Stadie, Ge Yang, Rein Houthooft, Xi Chen, Yan Duan, Yuhuai Wu, Pieter Abbeel, and Ilya Sutskever. Some considerations on learning to explore via meta-reinforcement learning. In Advances in Neural Processing Systems, 2018.</p>
<p>A bayesian framework for reinforcement learning. Malcolm Strens, International Conference on Machine Learning. Malcolm Strens. A bayesian framework for reinforcement learning. In International Conference on Machine Learning, 2000.</p>
<p>Learning to learn: Meta-critic networks for sample efficient learning. Flood Sung, Li Zhang, Tao Xiang, Timothy Hospedales, Yongxin Yang, arXiv:1706.09529Flood Sung, Li Zhang, Tao Xiang, Timothy Hospedales, and Yongxin Yang. Learning to learn: Meta-critic networks for sample efficient learning. arXiv:1706.09529, 2017.</p>
<p>On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. William R Thompson, Biometrika. 253/4William R Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4):285-294, 1933.</p>
<p>Mujoco: A physics engine for model-based control. Emanuel Todorov, Tom Erez, Yuval Tassa, 978-1-4673-1737-5IROS. IEEEEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In IROS, pp. 5026-5033. IEEE, 2012. ISBN 978-1-4673-1737-5.</p>
<p>Variational inference for data-efficient model learning in pomdps. Sebastian Tschiatschek, Kai Arulkumaran, Jan Stühmer, Katja Hofmann, arXiv:1805.09281Sebastian Tschiatschek, Kai Arulkumaran, Jan Stühmer, and Katja Hofmann. Variational inference for data-efficient model learning in pomdps. arXiv:1805.09281, 2018.</p>
<p>Learning to reinforcement learn. X Jane, Zeb Wang, Dhruva Kurth-Nelson, Hubert Tirumala, Joel Z Soyer, Remi Leibo, Charles Munos, Dharshan Blundell, Matt Kumaran, Botvinick, Annual Meeting of the Cognitive Science Community (CogSci). Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. In Annual Meeting of the Cognitive Science Community (CogSci), 2016.</p>
<p>Robust imitation of diverse behaviors. Ziyu Wang, Josh S Merel, Scott E Reed, Gregory Nando De Freitas, Nicolas Wayne, Heess, Advances in Neural Information Processing Systems. Ziyu Wang, Josh S Merel, Scott E Reed, Nando de Freitas, Gregory Wayne, and Nicolas Heess. Robust imitation of diverse behaviors. In Advances in Neural Information Processing Systems, 2017.</p>
<p>Bayesian policy search with policy priors. David Wingate, D Noah, Goodman, M Daniel, Leslie P Roy, Joshua B Kaelbling, Tenenbaum, International Joint Conference on Artificial Intelligence. David Wingate, Noah D Goodman, Daniel M Roy, Leslie P Kaelbling, and Joshua B Tenenbaum. Bayesian policy search with policy priors. In International Joint Conference on Artificial Intelli- gence, 2011.</p>
<p>Direct policy transfer via hidden parameter markov decision processes. Jiayu Yao, Taylor Killian, George Konidaris, Finale Doshi-Velez, LLARLA Workshop, FAIM. Jiayu Yao, Taylor Killian, George Konidaris, and Finale Doshi-Velez. Direct policy transfer via hidden parameter markov decision processes. In LLARLA Workshop, FAIM, 2018.</p>
<p>Reinforcement learning with action-derived rewards for chemotherapy and clinical trial dosing regimen selection. Gregory Yauney, Pratik Shah, Machine Learning for Healthcare Conference. Gregory Yauney and Pratik Shah. Reinforcement learning with action-derived rewards for chemotherapy and clinical trial dosing regimen selection. In Machine Learning for Healthcare Conference, pp. 161-226, 2018.</p>
<p>Deep sets. Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, R Ruslan, Alexander J Salakhutdinov, Smola, Advances in Neural Processing Systems. Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov, and Alexander J Smola. Deep sets. In Advances in Neural Processing Systems, 2017.</p>
<p>Decoupling dynamics and reward for transfer learning. Amy Zhang, Harsh Satija, Joelle Pineau, ICLR workshop track. Amy Zhang, Harsh Satija, and Joelle Pineau. Decoupling dynamics and reward for transfer learning. In ICLR workshop track, 2018.</p>
<p>Fast context adaptation via meta-learning. Kyriacos Luisa M Zintgraf, Vitaly Shiarlis, Katja Kurin, Shimon Hofmann, Whiteson, International Conference on Machine Learning. Luisa M Zintgraf, Kyriacos Shiarlis, Vitaly Kurin, Katja Hofmann, and Shimon Whiteson. Fast context adaptation via meta-learning. In International Conference on Machine Learning, 2019.</p>            </div>
        </div>

    </div>
</body>
</html>
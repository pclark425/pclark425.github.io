<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7135 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7135</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7135</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-134.html">extraction-schema-134</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-d36e2b1f3d06a732773ace7b9cd786247d563fa7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d36e2b1f3d06a732773ace7b9cd786247d563fa7" target="_blank">Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work pushes the performance boundary of zero-shot NER with LLMs by proposing a training-free self-improving framework, which utilizes an unlabeled corpus to stimulate the self-learning ability of LLMs.</p>
                <p><strong>Paper Abstract:</strong> Exploring the application of powerful large language models (LLMs) on the named entity recognition (NER) task has drawn much attention recently. This work pushes the performance boundary of zero-shot NER with LLMs by proposing a training-free self-improving framework, which utilizes an unlabeled corpus to stimulate the self-learning ability of LLMs. First, we use the LLM to make predictions on the unlabeled corpus using self-consistency and obtain a self-annotated dataset. Second, we explore various strategies to select reliable annotations to form a reliable self-annotated dataset. Finally, for each test input, we retrieve demonstrations from the reliable self-annotated dataset and perform inference via in-context learning. Experiments on four benchmarks show substantial performance improvements achieved by our framework. Through comprehensive experimental analysis, we find that increasing the size of unlabeled corpus or iterations of self-improving does not guarantee further improvement, but the performance might be boosted via more advanced strategies for reliable annotation selection.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7135.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7135.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sampling-and-voting confidence method: sample multiple outputs from an LLM for the same prompt, compute per-entity vote counts and average them to produce entity- and sample-level reliability scores used to filter self-annotations and rank demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI chat model used via official API as the backbone LLM for zero-shot NER and self-annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Consistency (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate multiple sampled answers from the LLM for each unlabeled input (temperature=0.7, 5 samples in experiments), count votes for each predicted entity to obtain an entity-level SC score, average entity scores to obtain a sample-level SC score; use these scores to (a) filter/select reliable self-annotations and (b) rank demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>voting over multiple samples (sample-and-vote)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot Named Entity Recognition (CoNLL03, ACE05, WikiGold, GENIA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Token-level named entity recognition in the strict zero-shot setting (no labeled data), evaluated on four standard NER benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>No-demos (naive zero-shot prompting, GPT-3.5): CoNLL03 68.97 F1, ACE05 27.29 F1, WikiGold 70.80 F1, GENIA 47.41 F1 (reported as baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Best SC-based results reported (two-stage majority voting or entity-level filtering combined with Diverse Nearest + SC ranking): CoNLL03 74.51 F1 (or up to 74.99 F1 for some SC threshold variants), ACE05 32.27 F1, WikiGold 73.98 F1, GENIA 52.06 F1; average improved from 53.62 to ~58.20 F1 (best reported avg).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>SC improves reliability of self-annotations but is not perfect: increasing unlabeled data or more iterations of self-annotation did not guarantee further gains; SC relies on sampling (they used 5 samples, temp=0.7) and can still include noisy annotations that contaminate demonstrations; selection strategy matters (best gains require careful annotation-selection and demo-retrieval strategies). Upper-bound experiments show room for better reliable-selection methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7135.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7135.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Verification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-confidence scoring method where the LLM is asked to assign an explicit 0–5 confidence score to each entity it produced; scores are used to filter/select self-annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI chat model used via official API for generating answers and for answering the confidence query.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Verification (SV)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>After producing recognized entities, ask the LLM 'How confident are you... ? Please give each named entity a confidence score 0-5.' Use these self-assigned scores to measure annotation quality and perform selection/ranking of demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-and-self-score (single-pass self-reported confidence)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot Named Entity Recognition (CoNLL03 reported comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same zero-shot NER task; SV was compared to SC primarily on CoNLL03.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>No-demos (GPT-3.5 baseline) CoNLL03 68.97 F1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>SV-based selection produced modest improvements but lagged behind SC on CoNLL03; best reported SV result on CoNLL03 in table: around 71.44 F1 (e.g., Diverse Nearest, random with SV gave 71.44 F1), which is an improvement over baseline but below SC results (~74+ F1).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The LLM displayed overconfidence under SV (authors observed no sample with SV < 3 on CoNLL03), making SV less reliable than SC; SV-based filtering yielded smaller gains than SC. Overconfidence of self-reported scores is a noted failure mode.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7135.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7135.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Iterative SI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative Self-Improving (Bootstrapping) Process</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bootstrapping loop: use self-annotated demonstrations from one round to guide in-context learning for the next round of self-annotation, repeating for multiple iterations to (attempt to) improve annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI chat model used for iterative self-annotation and ICL with self-generated demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Iterative self-annotation / bootstrapping</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>After an initial round of self-annotation and reliable-selection, retrieve those self-annotated demonstrations as in-context examples to annotate the unlabeled corpus again; repeat the self-annotate → select → use-as-demos loop for multiple iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>recursive/bootstrapping iterative self-annotation</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>8</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot Named Entity Recognition (CoNLL03, ACE05, WikiGold, GENIA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same zero-shot NER benchmarks; authors tested iterative bootstrapping up to 8 iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>0-th iteration equals No-demos baseline (CoNLL03 68.97 F1, ACE05 27.29 F1, WikiGold 70.80 F1, GENIA 47.41 F1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Authors report that increasing iterations up to 8 did not guarantee improvement and in many cases did not improve performance; no consistent across-the-board gains reported (figures show fluctuations but no reliable monotonic improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Error accumulation across iterations is a key failure mode in this training-free process; iterative self-improving often did not improve or even degraded performance (no consistent benefit), indicating that bootstrapping without training or stronger denoising/selection can propagate mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7135.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7135.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Weaker-LLM Failure</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Negative Effects on Weaker LLMs (Llama2-13B example)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applying the same self-improving framework to a weaker LLM (Llama2-13B-chat) produced negative results: self-annotation and demo-based ICL substantially degraded performance compared to the no-demos baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-13B-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Meta's Llama 2 chat model with 13 billion parameters (chat-finetuned variant) used off-the-shelf.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-annotation + two-stage majority voting + nearest-demo retrieval (framework applied as for GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Same pipeline (self-annotate with SC or voting, apply two-stage majority voting selection, retrieve nearest demonstrations) applied to Llama2-13B-chat.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>single-round self-annotation with demonstration retrieval (no benefit from iterative SI reported here)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot NER (CoNLL03, WikiGold) on Llama2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate the training-free self-improving framework on a smaller/weaker LLM to test generality.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Llama2-13B No-demos baseline: CoNLL03 42.24 F1, WikiGold 28.57 F1 (reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Applying the framework produced worse results: CoNLL03 23.55 F1 (Nearest retrieval case), WikiGold 8.94 F1 — i.e., substantial degradation compared to no-demos.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Framework appears suitable only for LLMs with strong zero-shot capability; when applied to weaker models (Llama2-13B) self-annotation + ICL can strongly hurt performance (negative effect observed). Authors recommend focusing on prompt design for weaker models rather than this training-free self-improving pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Z-ICL: Zero-shot in-context learning with pseudo-demonstrations <em>(Rating: 2)</em></li>
                <li>Evaluating chatgpt's information extraction capabilities: An assessment of performance, explainability, calibration, and faithfulness <em>(Rating: 2)</em></li>
                <li>Llmaaa: Making large language models as active annotators <em>(Rating: 1)</em></li>
                <li>Is information extraction solved by chatgpt? an analysis of performance, evaluation criteria, robustness and errors <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7135",
    "paper_id": "paper-d36e2b1f3d06a732773ace7b9cd786247d563fa7",
    "extraction_schema_id": "extraction-schema-134",
    "extracted_data": [
        {
            "name_short": "SC",
            "name_full": "Self-Consistency",
            "brief_description": "A sampling-and-voting confidence method: sample multiple outputs from an LLM for the same prompt, compute per-entity vote counts and average them to produce entity- and sample-level reliability scores used to filter self-annotations and rank demonstrations.",
            "citation_title": "Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo)",
            "model_description": "OpenAI chat model used via official API as the backbone LLM for zero-shot NER and self-annotation.",
            "model_size": "",
            "reflection_method_name": "Self-Consistency (SC)",
            "reflection_method_description": "Generate multiple sampled answers from the LLM for each unlabeled input (temperature=0.7, 5 samples in experiments), count votes for each predicted entity to obtain an entity-level SC score, average entity scores to obtain a sample-level SC score; use these scores to (a) filter/select reliable self-annotations and (b) rank demonstrations.",
            "iteration_type": "voting over multiple samples (sample-and-vote)",
            "num_iterations": null,
            "task_name": "Zero-shot Named Entity Recognition (CoNLL03, ACE05, WikiGold, GENIA)",
            "task_description": "Token-level named entity recognition in the strict zero-shot setting (no labeled data), evaluated on four standard NER benchmarks.",
            "evaluation_metric": "F1",
            "performance_before_reflection": "No-demos (naive zero-shot prompting, GPT-3.5): CoNLL03 68.97 F1, ACE05 27.29 F1, WikiGold 70.80 F1, GENIA 47.41 F1 (reported as baseline).",
            "performance_after_reflection": "Best SC-based results reported (two-stage majority voting or entity-level filtering combined with Diverse Nearest + SC ranking): CoNLL03 74.51 F1 (or up to 74.99 F1 for some SC threshold variants), ACE05 32.27 F1, WikiGold 73.98 F1, GENIA 52.06 F1; average improved from 53.62 to ~58.20 F1 (best reported avg).",
            "improvement_observed": true,
            "limitations_or_failure_cases": "SC improves reliability of self-annotations but is not perfect: increasing unlabeled data or more iterations of self-annotation did not guarantee further gains; SC relies on sampling (they used 5 samples, temp=0.7) and can still include noisy annotations that contaminate demonstrations; selection strategy matters (best gains require careful annotation-selection and demo-retrieval strategies). Upper-bound experiments show room for better reliable-selection methods.",
            "uuid": "e7135.0",
            "source_info": {
                "paper_title": "Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "SV",
            "name_full": "Self-Verification",
            "brief_description": "A self-confidence scoring method where the LLM is asked to assign an explicit 0–5 confidence score to each entity it produced; scores are used to filter/select self-annotations.",
            "citation_title": "Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo)",
            "model_description": "OpenAI chat model used via official API for generating answers and for answering the confidence query.",
            "model_size": "",
            "reflection_method_name": "Self-Verification (SV)",
            "reflection_method_description": "After producing recognized entities, ask the LLM 'How confident are you... ? Please give each named entity a confidence score 0-5.' Use these self-assigned scores to measure annotation quality and perform selection/ranking of demonstrations.",
            "iteration_type": "generate-and-self-score (single-pass self-reported confidence)",
            "num_iterations": null,
            "task_name": "Zero-shot Named Entity Recognition (CoNLL03 reported comparison)",
            "task_description": "Same zero-shot NER task; SV was compared to SC primarily on CoNLL03.",
            "evaluation_metric": "F1",
            "performance_before_reflection": "No-demos (GPT-3.5 baseline) CoNLL03 68.97 F1.",
            "performance_after_reflection": "SV-based selection produced modest improvements but lagged behind SC on CoNLL03; best reported SV result on CoNLL03 in table: around 71.44 F1 (e.g., Diverse Nearest, random with SV gave 71.44 F1), which is an improvement over baseline but below SC results (~74+ F1).",
            "improvement_observed": true,
            "limitations_or_failure_cases": "The LLM displayed overconfidence under SV (authors observed no sample with SV &lt; 3 on CoNLL03), making SV less reliable than SC; SV-based filtering yielded smaller gains than SC. Overconfidence of self-reported scores is a noted failure mode.",
            "uuid": "e7135.1",
            "source_info": {
                "paper_title": "Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Iterative SI",
            "name_full": "Iterative Self-Improving (Bootstrapping) Process",
            "brief_description": "A bootstrapping loop: use self-annotated demonstrations from one round to guide in-context learning for the next round of self-annotation, repeating for multiple iterations to (attempt to) improve annotations.",
            "citation_title": "Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo)",
            "model_description": "OpenAI chat model used for iterative self-annotation and ICL with self-generated demonstrations.",
            "model_size": "",
            "reflection_method_name": "Iterative self-annotation / bootstrapping",
            "reflection_method_description": "After an initial round of self-annotation and reliable-selection, retrieve those self-annotated demonstrations as in-context examples to annotate the unlabeled corpus again; repeat the self-annotate → select → use-as-demos loop for multiple iterations.",
            "iteration_type": "recursive/bootstrapping iterative self-annotation",
            "num_iterations": 8,
            "task_name": "Zero-shot Named Entity Recognition (CoNLL03, ACE05, WikiGold, GENIA)",
            "task_description": "Same zero-shot NER benchmarks; authors tested iterative bootstrapping up to 8 iterations.",
            "evaluation_metric": "F1",
            "performance_before_reflection": "0-th iteration equals No-demos baseline (CoNLL03 68.97 F1, ACE05 27.29 F1, WikiGold 70.80 F1, GENIA 47.41 F1).",
            "performance_after_reflection": "Authors report that increasing iterations up to 8 did not guarantee improvement and in many cases did not improve performance; no consistent across-the-board gains reported (figures show fluctuations but no reliable monotonic improvement).",
            "improvement_observed": false,
            "limitations_or_failure_cases": "Error accumulation across iterations is a key failure mode in this training-free process; iterative self-improving often did not improve or even degraded performance (no consistent benefit), indicating that bootstrapping without training or stronger denoising/selection can propagate mistakes.",
            "uuid": "e7135.2",
            "source_info": {
                "paper_title": "Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Weaker-LLM Failure",
            "name_full": "Negative Effects on Weaker LLMs (Llama2-13B example)",
            "brief_description": "Applying the same self-improving framework to a weaker LLM (Llama2-13B-chat) produced negative results: self-annotation and demo-based ICL substantially degraded performance compared to the no-demos baseline.",
            "citation_title": "Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models",
            "mention_or_use": "use",
            "model_name": "Llama2-13B-chat",
            "model_description": "Meta's Llama 2 chat model with 13 billion parameters (chat-finetuned variant) used off-the-shelf.",
            "model_size": "13B",
            "reflection_method_name": "Self-annotation + two-stage majority voting + nearest-demo retrieval (framework applied as for GPT-3.5)",
            "reflection_method_description": "Same pipeline (self-annotate with SC or voting, apply two-stage majority voting selection, retrieve nearest demonstrations) applied to Llama2-13B-chat.",
            "iteration_type": "single-round self-annotation with demonstration retrieval (no benefit from iterative SI reported here)",
            "num_iterations": null,
            "task_name": "Zero-shot NER (CoNLL03, WikiGold) on Llama2-13B",
            "task_description": "Evaluate the training-free self-improving framework on a smaller/weaker LLM to test generality.",
            "evaluation_metric": "F1",
            "performance_before_reflection": "Llama2-13B No-demos baseline: CoNLL03 42.24 F1, WikiGold 28.57 F1 (reported).",
            "performance_after_reflection": "Applying the framework produced worse results: CoNLL03 23.55 F1 (Nearest retrieval case), WikiGold 8.94 F1 — i.e., substantial degradation compared to no-demos.",
            "improvement_observed": false,
            "limitations_or_failure_cases": "Framework appears suitable only for LLMs with strong zero-shot capability; when applied to weaker models (Llama2-13B) self-annotation + ICL can strongly hurt performance (negative effect observed). Authors recommend focusing on prompt design for weaker models rather than this training-free self-improving pipeline.",
            "uuid": "e7135.3",
            "source_info": {
                "paper_title": "Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Z-ICL: Zero-shot in-context learning with pseudo-demonstrations",
            "rating": 2
        },
        {
            "paper_title": "Evaluating chatgpt's information extraction capabilities: An assessment of performance, explainability, calibration, and faithfulness",
            "rating": 2
        },
        {
            "paper_title": "Llmaaa: Making large language models as active annotators",
            "rating": 1
        },
        {
            "paper_title": "Is information extraction solved by chatgpt? an analysis of performance, evaluation criteria, robustness and errors",
            "rating": 1
        }
    ],
    "cost": 0.01267575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models</h1>
<p>Tingyu Xie ${ }^{1,2}$, Qi Li ${ }^{1,2}$, Yan Zhang ${ }^{3 <em>}$, Zuozhu Liu ${ }^{2}$, Hongwei Wang ${ }^{1,2 </em>}$<br>${ }^{1}$ College of Computer Science and Technology, Zhejiang University, China<br>${ }^{2}$ ZJU-UIUC Institute, Zhejiang University, China<br>${ }^{3}$ National University of Singapore, Singapore<br>{tingyuxie, hongweiwang}@zju.edu.cn, yanzhang.jlu@gmail.com</p>
<h4>Abstract</h4>
<p>Exploring the application of powerful large language models (LLMs) on the named entity recognition (NER) task has drawn much attention recently. This work pushes the performance boundary of zero-shot NER with LLMs by proposing a training-free self-improving framework, which utilizes an unlabeled corpus to stimulate the self-learning ability of LLMs. First, we use the LLM to make predictions on the unlabeled corpus using self-consistency and obtain a self-annotated dataset. Second, we explore various strategies to select reliable annotations to form a reliable self-annotated dataset. Finally, for each test input, we retrieve demonstrations from the reliable self-annotated dataset and perform inference via in-context learning. Experiments on four benchmarks show substantial performance improvements achieved by our framework. Through comprehensive experimental analysis, we find that increasing the size of unlabeled corpus or iterations of self-improving does not guarantee further improvement, but the performance might be boosted via more advanced strategies for reliable annotation selection. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>There have been many works exploring new possibilities of the named entity recognition (NER) task in the era of large language models (LLMs) (OpenAI, 2022; Touvron et al., 2023; Chowdhery et al., 2022) recently. These studies include designing advanced prompting methods for zero-shot prediction or few-shot in-context learning (ICL) (Wei et al., 2023b; Wang et al., 2023; Xie et al., 2023; Li et al., 2023b), training task-specific LLMs for NER (Zhou et al., 2023; Sainz et al., 2023), and generating data with LLMs to train small specific models (Zhang et al., 2023; Ma et al., 2023; Josifoski et al., 2023).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>In this work, we explore the possibility of pushing the performance boundary of zero-shot NER with LLMs via self-improving. We focus on the strict zero-shot scenarios where no annotated data is available but only an unlabeled corpus is accessible, and no training resource or auxiliary models are available. We propose a totally training-free self-improving framework for NER, which utilizes an unlabeled corpus to stimulate the self-learning ability of LLMs. The framework consists of the following three steps. (1) Step 1: we use LLMs to self-annotate the unlabeled corpus using selfconsistency (SC, Wang et al., 2022). Each annotated entity is associated with a SC score, which is used as the measure of the reliability of this annotation. (2) Step 2: we select reliable annotation to form a reliable self-annotated dataset, during which diverse annotation selection strategies are explored, including entity-level threshold filtering, sample-level threshold filtering and two-stage majority voting. (3) Step 3: for each arrived test input, we perform inference via ICL with demonstrations from the reliable self-annotated dataset. Various strategies for demonstration retrieval are explored.</p>
<p>Our contributions include: (1) We proposed a training-free self-improving framework for zeroshot NER with LLMs. (2) This framework achieved significant performance improvements on four benchmarks. (3) We conduct comprehensive experimental analysis, finding that increasing the size of unlabeled corpus or iterations of self-annotating does not guarantee gains, but there might be room for improvements with more advanced strategies for reliable annotation selection.</p>
<h2>2 Zero-Shot NER with Self-Improving</h2>
<p>Motivation. To push the performance boundary of zero-shot NER with LLMs, we propose a selfimproving framework under a strict zero-shot and low-resource setting: No annotated data but only an</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The overview of the proposed self-improving framework for zero-shot NER with LLM.
unlabeled corpus is available; No auxiliary model or training step is required. This study is orthogonal to previous prompt designing works, as any advanced prompting method can be applied to this framework. Fig. 1 shows the framework overview.
Task Formulation. Given an input sentence $x$, the NER task is to recognize the structure output $y$ from $x$, which consists of a set of $(e, t)$ pairs. $e$ is an entity span, which is a sequence of tokens form $x ; t$ is the corresponding entity type, which belongs to a predefined entity type set.</p>
<h3>2.1 Step 1: Zero-Shot Self-Annotating</h3>
<p>We assume an unlabeled corpus $\mathcal{U}=\left{x_{i}\right}<em i="i">{i=1}^{n}$ is available. We use the training set without labels as the unlabeled dataset in this work. For unlabeled sample $x</em>=\arg \max }$, we generate predictions with LLMs via zero-shot prompting, as shown in upper part of Fig. 1. This process is formulated as $y_{i<em i="i">{y} P\left(y \mid T, x</em>\right)\right}}\right)$, where $T$ is the task instruction of NER, and $y_{i}=\left{\left(e_{i}^{j}, t_{i}^{j<em i="i">{j=1}^{m}$. We apply self-consistency (SC) (Wang et al., 2022) to obtain a SC score for each prediction, which will be used in step 2 for reliable annotation selection. We sample multiple answers from the model, and the vote for each predicted entity $\left(e</em>\right)\right}}^{j}, t_{i}^{j}\right)$ is the times it appeared in all the sampled answers, which we denoted as entity-level SC score $c_{i}^{j}$. Then we get the sample-level SC score $c_{i}$ for each input sentence $x_{i}$ by taking the average SC score over all predicted entities in this sentence, i.e., $c_{i}=\frac{1}{m} \sum_{j} c_{i}^{j}$. For each self-annotated sample with SC scores, we can denote it as $\left(x_{i},\left{\left(e_{i}^{j}, t_{i}^{j}, c_{i}^{j<em i="i">{j=1}^{m}, c</em>\right)$.</p>
<h3>2.2 Step 2: Reliable Annotation Selection</h3>
<p>We assume that a higher SC score indicates a higher reliablity. Thus, we investigate the three following strategies for reliable annotation selection. (1) Entity-level threshold filtering, which drops the predicted entity $e_{i}^{j}$ if $c_{i}^{j}&lt;T h _e n t i t y$, where $T h _e n t i t y$ is the threshold for entity-level SC score. (2) Sample-level threshold filtering, which drops the sample $x_{i}$ if $c_{i}&lt;T h _$sample, where Th_sample is the threshold for sample-level SC score. (3) Two-stage majority voting (Xie et al., 2023), is an entity-level selection method, which first votes for the most consistent entity spans, then the most consistent types based on the voted spans.</p>
<h3>2.3 Step 3: Inference with Self-Annotated Demonstration</h3>
<p>When a test input $x^{q}$ arrives, we retrieve $k$ demonstrations from the reliable self-annotated dataset to help the inference. ${ }^{2}$ We investigate the following four methods for demonstration retrieval. (1) Random retrieval, which randomly select $k$ demonstrations. (2) Nearest retrieval, which select the $k$ nearest neighbors of $x^{q}$. The distance of samples is measured by the cosine similarity in the representation space. (3) Diverse nearest retrieval, which first retrieve $K$ nearest neighbors, where $K&gt;k$, then uniformly samples a random set of $k$ samples from the $K$ neighbors. (4) Diverse nearest with SC</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">CoNLL03</th>
<th style="text-align: center;">ACE05</th>
<th style="text-align: center;">WikiGold</th>
<th style="text-align: center;">GENIA</th>
<th style="text-align: center;">Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">No-demos</td>
<td style="text-align: center;">$68.97_{0.22}$</td>
<td style="text-align: center;">$27.29_{0.58}$</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">$47.41_{0.29}$</td>
<td style="text-align: center;">53.62</td>
</tr>
<tr>
<td style="text-align: left;">ICL with self-annotated demonstrations (Zero-shot)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Without annotation selection</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">$71.45_{0.10}$</td>
<td style="text-align: center;">$30.38_{0.93}$</td>
<td style="text-align: center;">70.51</td>
<td style="text-align: center;">$48.78_{0.06}$</td>
<td style="text-align: center;">55.28</td>
</tr>
<tr>
<td style="text-align: left;">Nearest</td>
<td style="text-align: center;">$72.07_{0.11}$</td>
<td style="text-align: center;">$\mathbf{3 2 . 2 0}_{0.92}$</td>
<td style="text-align: center;">$\mathbf{7 1 . 8 1}$</td>
<td style="text-align: center;">$49.54_{1.88}$</td>
<td style="text-align: center;">$\mathbf{5 6 . 4 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Diverse Nearest, random</td>
<td style="text-align: center;">$\mathbf{7 2 . 1 5}_{0.65}$</td>
<td style="text-align: center;">$31.07_{1.45}$</td>
<td style="text-align: center;">70.72</td>
<td style="text-align: center;">$\mathbf{5 0 . 0 1}_{1.20}$</td>
<td style="text-align: center;">55.99</td>
</tr>
<tr>
<td style="text-align: left;">Entity-level threshold filtering</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">$70.91_{0.55}$</td>
<td style="text-align: center;">$30.41_{0.95}$</td>
<td style="text-align: center;">72.33</td>
<td style="text-align: center;">$50.70_{1.53}$</td>
<td style="text-align: center;">56.09</td>
</tr>
<tr>
<td style="text-align: left;">Nearest</td>
<td style="text-align: center;">$73.24_{0.53}$</td>
<td style="text-align: center;">$32.22_{0.38}$</td>
<td style="text-align: center;">72.53</td>
<td style="text-align: center;">$49.85_{1.20}$</td>
<td style="text-align: center;">56.96</td>
</tr>
<tr>
<td style="text-align: left;">Diverse Nearest, random</td>
<td style="text-align: center;">$74.11_{0.12}$</td>
<td style="text-align: center;">$\mathbf{3 2 . 2 9}_{0.31}$</td>
<td style="text-align: center;">72.01</td>
<td style="text-align: center;">$50.68_{0.14}$</td>
<td style="text-align: center;">57.27</td>
</tr>
<tr>
<td style="text-align: left;">Diverse Nearest, SC ranking</td>
<td style="text-align: center;">$\mathbf{7 4 . 9 9}_{0.20}$</td>
<td style="text-align: center;">$31.65_{0.97}$</td>
<td style="text-align: center;">$\mathbf{7 3 . 5 3}$</td>
<td style="text-align: center;">$\mathbf{5 1 . 1 1}_{0.28}$</td>
<td style="text-align: center;">$\mathbf{5 7 . 8 2}$</td>
</tr>
<tr>
<td style="text-align: left;">Sample-level threshold filtering</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">$72.41_{1.28}$</td>
<td style="text-align: center;">$30.00_{1.26}$</td>
<td style="text-align: center;">$\mathbf{7 3 . 3 8}$</td>
<td style="text-align: center;">$51.61_{1.21}$</td>
<td style="text-align: center;">56.86</td>
</tr>
<tr>
<td style="text-align: left;">Nearest</td>
<td style="text-align: center;">$72.28_{0.14}$</td>
<td style="text-align: center;">$\mathbf{3 2 . 0 0}_{0.08}$</td>
<td style="text-align: center;">73.27</td>
<td style="text-align: center;">$\mathbf{5 2 . 7 2}_{0.80}$</td>
<td style="text-align: center;">$\mathbf{5 7 . 5 7}$</td>
</tr>
<tr>
<td style="text-align: left;">Diverse Nearest, random</td>
<td style="text-align: center;">$72.32_{0.68}$</td>
<td style="text-align: center;">$30.74_{0.06}$</td>
<td style="text-align: center;">72.09</td>
<td style="text-align: center;">$52.50_{0.50}$</td>
<td style="text-align: center;">56.91</td>
</tr>
<tr>
<td style="text-align: left;">Diverse Nearest, SC ranking</td>
<td style="text-align: center;">$\mathbf{7 3 . 9 7}_{0.12}$</td>
<td style="text-align: center;">$31.08_{0.54}$</td>
<td style="text-align: center;">72.80</td>
<td style="text-align: center;">$51.67_{0.93}$</td>
<td style="text-align: center;">57.38</td>
</tr>
<tr>
<td style="text-align: left;">Two-stage majority voting</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">$72.12_{0.59}$</td>
<td style="text-align: center;">$31.18_{0.38}$</td>
<td style="text-align: center;">72.32</td>
<td style="text-align: center;">$50.17_{0.93}$</td>
<td style="text-align: center;">56.45</td>
</tr>
<tr>
<td style="text-align: left;">Nearest</td>
<td style="text-align: center;">$71.66_{0.37}$</td>
<td style="text-align: center;">$31.45_{1.32}$</td>
<td style="text-align: center;">72.84</td>
<td style="text-align: center;">$50.19_{1.59}$</td>
<td style="text-align: center;">56.53</td>
</tr>
<tr>
<td style="text-align: left;">Diverse Nearest, random</td>
<td style="text-align: center;">$72.45_{0.41}$</td>
<td style="text-align: center;">$30.84_{0.56}$</td>
<td style="text-align: center;">70.83</td>
<td style="text-align: center;">$51.03_{0.73}$</td>
<td style="text-align: center;">56.28</td>
</tr>
<tr>
<td style="text-align: left;">Diverse Nearest, SC ranking</td>
<td style="text-align: center;">$\mathbf{7 4 . 5 1}_{0.03}$</td>
<td style="text-align: center;">$\mathbf{3 2 . 2 7}_{0.25}$</td>
<td style="text-align: center;">$\mathbf{7 3 . 9 8}$</td>
<td style="text-align: center;">$\mathbf{5 2 . 0 6}_{0.09}$</td>
<td style="text-align: center;">$\mathbf{5 8 . 2 0}$</td>
</tr>
<tr>
<td style="text-align: left;">ICL with gold labeled demonstrations</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Random (Gold)</td>
<td style="text-align: center;">$78.36_{0.31}$</td>
<td style="text-align: center;">$42.12_{0.30}$</td>
<td style="text-align: center;">74.27</td>
<td style="text-align: center;">$54.50_{1.14}$</td>
<td style="text-align: center;">62.31</td>
</tr>
<tr>
<td style="text-align: left;">Nearest (Gold)</td>
<td style="text-align: center;">$84.30_{0.39}$</td>
<td style="text-align: center;">$52.72_{0.44}$</td>
<td style="text-align: center;">78.20</td>
<td style="text-align: center;">$54.78_{0.94}$</td>
<td style="text-align: center;">67.50</td>
</tr>
<tr>
<td style="text-align: left;">Random (Gold), full data</td>
<td style="text-align: center;">$78.35_{1.44}$</td>
<td style="text-align: center;">$41.33_{0.79}$</td>
<td style="text-align: center;">78.47</td>
<td style="text-align: center;">$52.77_{0.93}$</td>
<td style="text-align: center;">62.73</td>
</tr>
<tr>
<td style="text-align: left;">Nearest (Gold), full data</td>
<td style="text-align: center;">$83.51_{0.02}$</td>
<td style="text-align: center;">$55.54_{0.61}$</td>
<td style="text-align: center;">79.73</td>
<td style="text-align: center;">$58.72_{1.52}$</td>
<td style="text-align: center;">69.37</td>
</tr>
</tbody>
</table>
<p>Table 1: Main results. The right subscript number are standard deviations. Gold indicates the method has access to the gold labeled data, thus is not comparable with the rest of methods. Full data indicates the method has access to the full training set. Results of $T h _e n t i t y=4.0$ and $T h _s a m p l e=4.0$ is shown here. Texts in bold are the best results in each category; Text underlined are the best results among all methods. The proposed framework significantly improves the zero-shot performances. On average, two-stage majority voting combined with the proposed diverse nearest with SC ranking achieves the best results.
ranking, proposed by this work to achieve a better trade-off between the similarity, diversity and reliability of self-annotated demonstrations. After retrieving $K$ nearest neighbors, we select samples with the top- $k$ sample-level SC scores.</p>
<p>Let $S=\left{x_{i}, y_{i}\right}<em y="y">{i=1}^{k}$ denotes the self-annotated demonstrations retrieved for the test input $x^{q}$. Finally, our framework conduct ICL by concatenating these $k$ samples as well as the test input sentence $x^{q}$, as shown in the below part in Fig. 1. The prediction is obtained via $y^{q}=\arg \max </em>\right)$.} P\left(y \mid T, S, x^{q</p>
<h2>3 Experiment</h2>
<h3>3.1 Setup</h3>
<p>We experiment on four widely-used NER datasets, CoNLL03 (Sang and De Meulder, 2003), ACE05 (Walker et al., 2006), WikiGold (Balasuriya et al., 2009) and GENIA (Ohta et al., 2002). We use GPT3.5 (gpt-3.5-turbo) as the LLM backbone and text-embedding-ada-002 model to get sentence representations. ${ }^{3}$ We set $k=16$ and $K=50$. For</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>SC, we set temperature to 0.7 and sample 5 answers. For cost saving, we randomly sample 300 test samples twice then report the means and standard deviations, and we randomly sample 500 training samples without labels to form the unlabeled corpus $\mathcal{U}$. The naive zero-shot prompting is our baseline, which we denote as No-demos. We report F1 scores throughout this paper.</p>
<h3>3.2 Results</h3>
<p>The main results are shown in Table 1. Results of other values for thresholds $T h _e n t i t y$ and Th_sample can be found in Appendix E. (1) Without annotation selection, we only generate one answer for each unlabeled sample. The results show improvements over No-demos, revealing that our framework is helpful even without any carefully designed annotation selection step. (2) The performance is further improved under three annotation selection strategies respectively. (3) The proposed diverse nearest with SC ranking shows consistent improvements under various settings and achieves the best results when combined with two-stage majority voting. This confirms that this strategy</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Results of increasing the size of unlabeled dataset. Vertical axes represent F1 scores. <em>Ours</em> refers to the combination of two-stage majority voting and diverse nearest with SC ranking. Increasing unlabeled data does not guarantee performance gains.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Increasing the iterations of self-improving does not guarantee performance improvements.</p>
<p>achieves a better trade-off between similarity, diversity and reliability of the demonstrations. (4) Random retrieval lags behind nearest retrieval in self-improving scenario but is not as much as in the gold label scenario, likely because of the noise contained in self-annotated labels. The model may directly copy the wrong answers in the most similar self-annotated demonstrations due to the copy mechanism of ICL (Lyu et al., 2023).</p>
<h3>3.3 Analysis</h3>
<p><strong>Increasing unlabeled data.</strong> We expanded the size of <em>U</em> by 10 times and randomly sampled 5000 samples from the original training set. Results are shown in Fig. 2. Increasing the size of the unlabeled corpus does not guarantee performance improvements under the self-improving scenario. Meanwhile, increasing the size of the demonstration pool only brings marginal improvement, even under the gold label scenario. The reason may be that the small dataset already approximately captures the data distribution.</p>
<p><strong>Iterative self-improving.</strong> We use the self-annotated data as demonstrations to guide the next iteration of self-annotating, forming a bootstrapping process. The illustration of iterative self-improving process can be found in Appendix G.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Kernel density estimation for SC scores. Vertical axes represent density, horizontal axes represent SC scores.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>CoNLL03</th>
<th>ACE05</th>
<th>WikiGold</th>
<th>GENIA</th>
<th>Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td>No-demos</td>
<td>68.97</td>
<td>27.29</td>
<td>70.8</td>
<td>47.41</td>
<td>53.62</td>
</tr>
<tr>
<td>TSMV</td>
<td>74.51</td>
<td>32.27</td>
<td>73.98</td>
<td>52.06</td>
<td>58.20</td>
</tr>
<tr>
<td>Upper bound</td>
<td>81.65</td>
<td>37.82</td>
<td>76.57</td>
<td>56.24</td>
<td>63.07</td>
</tr>
<tr>
<td>Gold label</td>
<td>84.30</td>
<td>52.72</td>
<td>78.20</td>
<td>54.78</td>
<td>67.50</td>
</tr>
</tbody>
</table>
<p>Table 2: Results of the upper bound of reliable annotation selection. <em>TSMV</em> represents two-stage majority voting. We display the best results for each strategy. The setting of <em>Upper bound</em> performs on par with the setting of <em>Gold label</em>, showing that there might be space to be improved for reliable annotation selection.</p>
<p>We experiment up to 8 iterations. The 0-th iteration indicates the <em>No-demos</em> setting. Results are shown in Fig. 3. Increasing iterations of self-improving cannot guarantee improvements on most datasets. This may be due to the fact that error accumulation in self-annotating is difficult to be eliminated in this training-free process.</p>
<p><strong>Upper bound of reliable annotation selection.</strong> We keep only the true predictions and discard the false predictions in all the sampled answers to evaluate the upper bound of reliable annotation selection. Results are shown in Table 2. More detailed results can be found in Appendix F. <em>Upper bound</em> setting performs on par with the <em>Gold label</em> setting, indicating that there might still be space to be improved for reliable annotation selection.</p>
<p><strong>SC score analysis.</strong> We plot the kernel density estimation for entity-level SC scores in Fig. 4. Most true predictions gather in the interval of high SC scores, while most false predictions have low SC scores. This shows that SC scores effectively reflect the reliability of annotations.</p>
<p><strong>Self-verification.</strong> Besides SC, we also explore self-verification (SV) to measure the confidence of self-annotation by asking the LLM to score its own answer about its own confidence. After the LLM outputs the recognized entities, we obtain the</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: right;">SC</th>
<th style="text-align: right;">SV</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">No-demos</td>
<td style="text-align: right;">$68.97{ }_{0.22}$</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Entity-level threshold filtering</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: right;">$70.91{ }_{0.55}$</td>
<td style="text-align: right;">$70.91{ }_{0.56}$</td>
</tr>
<tr>
<td style="text-align: left;">Nearest</td>
<td style="text-align: right;">$73.24{ }_{0.53}$</td>
<td style="text-align: right;">$71.23{ }_{0.01}$</td>
</tr>
<tr>
<td style="text-align: left;">Diverse Nearest, random</td>
<td style="text-align: right;">$74.11{ }_{0.12}$</td>
<td style="text-align: right;">$\mathbf{7 1 . 4 4}{ }_{0.93}$</td>
</tr>
<tr>
<td style="text-align: left;">Diverse Nearest, score ranking</td>
<td style="text-align: right;">$\mathbf{7 4 . 9 9}{ }_{0.20}$</td>
<td style="text-align: right;">$68.09{ }_{0.60}$</td>
</tr>
<tr>
<td style="text-align: left;">Sample-level threshold filtering</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: right;">$72.41{ }_{1.28}$</td>
<td style="text-align: right;">$\mathbf{7 1 . 0 0}{ }_{0.32}$</td>
</tr>
<tr>
<td style="text-align: left;">Nearest</td>
<td style="text-align: right;">$72.28{ }_{0.14}$</td>
<td style="text-align: right;">$70.45{ }_{0.46}$</td>
</tr>
<tr>
<td style="text-align: left;">Diverse Nearest, random</td>
<td style="text-align: right;">$72.32{ }_{0.08}$</td>
<td style="text-align: right;">$70.06{ }_{1.29}$</td>
</tr>
<tr>
<td style="text-align: left;">Diverse Nearest, score ranking</td>
<td style="text-align: right;">$\mathbf{7 3 . 9 7}{ }_{0.12}$</td>
<td style="text-align: right;">$68.95{ }_{0.35}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparison between SC and SV on CoNLL03 dataset. Th_entity $=4.0$ and Th_sample $=4.0$ is used. Right subscript number are standard deviations. Texts in bold are the best results in each category; Text underlined are the best results among all methods.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">CoNLL03</th>
<th style="text-align: center;">WikiGold</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">No-demos</td>
<td style="text-align: center;">42.24</td>
<td style="text-align: center;">28.57</td>
</tr>
<tr>
<td style="text-align: left;">Nearest</td>
<td style="text-align: center;">23.55</td>
<td style="text-align: center;">8.94</td>
</tr>
</tbody>
</table>
<p>Table 4: Results on the Llama2 chat 13B. Two-stage majority voting is used here. The negative results show that the proposed framework is more suitable for models with a strong zero-shot capability. The negative effect is obvious on the first sampled test set, thus we do not continue to test on other seeds.</p>
<p>SV score by asking the LLM: "How confident are you in providing the above answers? Please give each named entity in your answer a confidence score of 0-5." The comparison results between SC and SV are in Table 3. As shown in the table, SV also achieves some improvements compared with the No-demos baseline. However, it lags behind the SC measurement. This is presumably because the LLM tends to be over-confident about its own answer, since we found that no sample gets a confidence score lower than 3 under the SV measurement in CoNLL03 benchmark. The overconfidence problem is also mentioned in Li et al. (2023a).
Evaluation on weaker LLMs. To explore the performance of the proposed self-improving framework on weaker LLMs, we conduct experiments on the Llama2 chat 13B model (Touvron et al., 2023), ${ }^{4}$ the results are shown in Table 4. Two-stage majority voting selection strategy and the nearest neighbor retrieval method are used in this experiment. With a much weaker ability in zero-shot scenarios, Llama2 13B model shows negative results under the self-improving framework. This indicates that the proposed framework is more suit-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>able for models with a strong zero-shot capability. For the models with a relatively weaker zero-shot ability, improving the prompt designing might be a more effective strategy to boost performance.</p>
<h2>4 Related Work</h2>
<p>Information extraction with LLM. The research of information extraction (IE) with LLMs includes prompt designing (Wei et al., 2023b; Wang et al., 2023; Xie et al., 2023; Li et al., 2023b), task-specific LLMs instruction-tuning (Zhou et al., 2023; Sainz et al., 2023) and data augmentation (Zhang et al., 2023; Ma et al., 2023; Josifoski et al., 2023). Zhang et al. (2023) use LLM to annotate data, which is used to fine-tune a specific IE model, then the fine-tuned model is used to help select the data to be annotated in the next iteration. Unlike previous works, this work propose a training-free self-improving framework to push the zero-shot boundary of LLM on NER. Different from Zhang et al. (2023), no seed labeled data, expert small model nor training resources are required in our framework. In addition, our work is orthogonal to previous prompt designing works. They explored various advanced prompt formats to boost performance, and did not utilize unlabeled corpus. Unlike them, this work improves zero-shot NER by using unlabeled corpus without designing any complex prompt format.
Demonstrations in ICL. Some works explored factors that have impacts on ICL (Lyu et al., 2023; Min et al., 2022; Wei et al., 2023a). Lyu et al. (2023) investigate the impact of randomly assigning labels to demonstrations in ICL. However, this random labeling method is not suitable for tasks like NER, which requires label information on the token-level instead of sentence-level. Different from them, we first use LLM to make predictions on the unlabeled corpus, then select reliable selfannotated data as demonstrations.</p>
<h2>5 Conclusion</h2>
<p>We propose a training-free self-improving framework for zero-shot NER with LLMs, which achieves significant performance improvements on four benchmarks. Comprehensive experimental analysis shows that, simply increasing the size of unlabeled corpus or the iterations of self-annotation do not guarantee further improvement, but there might still be room for improvement with more advanced strategies for reliable annotation selection.</p>
<h2>Limitations</h2>
<p>We acknowledge the following limitations of this study.</p>
<ul>
<li>This work focus on exploring the zero-shot self-improving framework on NER task. The investigation of this paradigm on other IE tasks are not studied yet.</li>
<li>We explored the commonly-used selfconsistency and the self-verification method to obtain the confidence score for measuring the quality of self-annotated data. There might be other approaches to measure the quality of self-annotation.</li>
<li>The zero-shot performance still lag behind previous state-of-the-art of fully-supervised methods.</li>
<li>Although this framework achieves significant improvement on the strong LLM, GPT-3.5, it gets negative results on a much weaker LLM, Llama2 13B. Improving the zero-shot NER on the weaker and smaller LLMs remains to be explored.</li>
</ul>
<h2>Acknowledgements</h2>
<p>This research is supported by Zhejiang Provincial Natural Science Foundation of China (LDT23F02023F02). We would like to thank the anonymous reviewers for their insightful comments and constructive suggestions. We would also like to thank Chen Wang and Xinlong Qiao for their help at the visualization.</p>
<h2>References</h2>
<p>Dominic Balasuriya, Nicky Ringland, Joel Nothman, Tara Murphy, and James R Curran. 2009. Named entity recognition in wikipedia. In Proceedings of the 2009 workshop on the people's web meets NLP: Collaboratively constructed semantic resources (People's Web), pages 10-18.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny</p>
<p>Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways.</p>
<p>Ridong Han, Tao Peng, Chaohao Yang, Benyou Wang, Lu Liu, and Xiang Wan. 2023. Is information extraction solved by chatgpt? an analysis of performance, evaluation criteria, robustness and errors. arXiv preprint arXiv:2305.14450.</p>
<p>Martin Josifoski, Marija Sakota, Maxime Peyrard, and Robert West. 2023. Exploiting asymmetry for synthetic training data generation: Synthie and the case of information extraction.</p>
<p>Bo Li, Gexiang Fang, Yang Yang, Quansen Wang, Wei Ye, Wen Zhao, and Shikun Zhang. 2023a. Evaluating chatgpt's information extraction capabilities: An assessment of performance, explainability, calibration, and faithfulness. arXiv preprint arXiv:2304.11633.</p>
<p>Peng Li, Tianxiang Sun, Qiong Tang, Hang Yan, Yuanbin Wu, Xuanjing Huang, and Xipeng Qiu. 2023b. CodeIE: Large code generation models are better few-shot information extractors. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15339-15353, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. 2023c. Towards general text embeddings with multi-stage contrastive learning. arXiv preprint arXiv:2308.03281.</p>
<p>Yi Luan, Dave Wadden, Luheng He, Amy Shah, Mari Ostendorf, and Hannaneh Hajishirzi. 2019. A general framework for information extraction using dynamic span graphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3036-3046, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Xinxi Lyu, Sewon Min, Iz Beltagy, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. Z-ICL: Zero-shot in-context learning with pseudo-demonstrations. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2304-2317, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Mingyu Derek Ma, Xiaoxuan Wang, Po-Nien Kung, P. Jeffrey Brantingham, Nanyun Peng, and Wei Wang. 2023. Star: Improving low-resource information extraction by structure-to-text data generation with large language models.</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11048-11064, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Tomoko Ohta, Yuka Tateisi, Jin-Dong Kim, Hideki Mima, and Junichi Tsujii. 2002. The genia corpus: An annotated research abstract corpus in molecular biology domain. In Proceedings of the human language technology conference, pages 73-77. Citeseer.</p>
<p>OpenAI. 2022. Introducing chatgpt.
Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982-3992, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Oscar Sainz, Iker García-Ferrero, Rodrigo Agerri, Oier Lopez de Lacalle, German Rigau, and Eneko Agirre. 2023. Gollie: Annotation guidelines improve zero-shot information-extraction.</p>
<p>Erik F Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. arXiv preprint cs/0306050.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.</p>
<p>Christopher Walker, Stephanie Strassel, Julie Medero, and Kazuaki Maeda. 2006. Ace 2005 multilingual training corpus ldc2006t06, 2006. URL https://catalog. ldc. upenn. edu/LDC2006T06.</p>
<p>Shuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang, Fei Wu, Tianwei Zhang, Jiwei Li, and Guoyin Wang. 2023. Gpt-ner: Named entity recognition via large language models.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</p>
<p>Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, and Tengyu Ma. 2023a. Larger language models do in-context learning differently.</p>
<p>Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, Yong Jiang, and Wenjuan Han. 2023b. Zero-shot information extraction via chatting with chatgpt.</p>
<p>Tingyu Xie, Qi Li, Jian Zhang, Yan Zhang, Zuozhu Liu, and Hongwei Wang. 2023. Empirical study of zero-shot ner with chatgpt.</p>
<p>Ruoyu Zhang, Yanzeng Li, Yongliang Ma, Ming Zhou, and Lei Zou. 2023. Llmaaa: Making large language models as active annotators.</p>
<p>Suxiang Zhang, Ying Qin, Juan Wen, and Xiaojie Wang. 2006. Word segmentation and named entity recognition for SIGHAN bakeoff3. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 158-161, Sydney, Australia. Association for Computational Linguistics.</p>
<p>Wenxuan Zhou, Sheng Zhang, Yu Gu, Muhao Chen, and Hoifung Poon. 2023. Universalner: Targeted distillation from large language models for open named entity recognition.</p>
<h2>A Dataset Statistics</h2>
<p>We evaluate on four commonly-used NER English datasets, CoNLL03 (Sang and De Meulder, 2003), ACE05 (Walker et al., 2006), WikiGold (Balasuriya et al., 2009) and GENIA (Ohta et al., 2002), among which CoNLL03, WikiGold and GENIA are public datasets, and ACE05 ${ }^{5}$ can be accessed on Linguistic Data Consortium (LDC) platform with specific license. In addition, we also evaluate on two Chinese datasets, Ontonotes $4^{6}$ and MSRA (Zhang et al., 2006), in Appendix B. Table 5 and 6 shows the statistics of the processed datasets used in this work. For CoNLL03, we use the processed version shared by Han et al. (2023). For ACE05, we follow Luan et al. (2019)'s processing steps.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">CoNLL03</th>
<th style="text-align: center;">ACE05</th>
<th style="text-align: center;">WikiGold</th>
<th style="text-align: center;">GENIA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">#Train</td>
<td style="text-align: center;">14382</td>
<td style="text-align: center;">12475</td>
<td style="text-align: center;">1422</td>
<td style="text-align: center;">16692</td>
</tr>
<tr>
<td style="text-align: left;">#Test</td>
<td style="text-align: center;">3453</td>
<td style="text-align: center;">2050</td>
<td style="text-align: center;">274</td>
<td style="text-align: center;">1854</td>
</tr>
</tbody>
</table>
<p>Table 5: Statistics of the processed English datasets used in this work. The training set is formed by combining the original training split and development split.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Ontonotes 4</th>
<th style="text-align: center;">MSRA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">#Train</td>
<td style="text-align: center;">20025</td>
<td style="text-align: center;">46364</td>
</tr>
<tr>
<td style="text-align: left;">#Test</td>
<td style="text-align: center;">4346</td>
<td style="text-align: center;">4365</td>
</tr>
</tbody>
</table>
<p>Table 6: Statistics of the processed Chinese datasets used in this work. The training set is formed by combining the original training split and development split.</p>
<h2>B Results on Additional Benchmarks</h2>
<p>We additionally evaluate on two widely-used Chinese benchmarks, the results are in Table 7.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: right;">Ontonotes 4</th>
<th style="text-align: right;">MSRA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">No-demos</td>
<td style="text-align: right;">$31.71_{1.14}$</td>
<td style="text-align: right;">$39.21_{0.93}$</td>
</tr>
<tr>
<td style="text-align: left;">ICL with self-annotated demonstrations</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: right;">$32.45_{0.19}$</td>
<td style="text-align: right;">$39.55_{0.75}$</td>
</tr>
<tr>
<td style="text-align: left;">Nearest</td>
<td style="text-align: right;">$31.54_{1.60}$</td>
<td style="text-align: right;">$36.31_{1.76}$</td>
</tr>
<tr>
<td style="text-align: left;">Diverse Nearest, SC ranking</td>
<td style="text-align: right;">$\mathbf{3 5 . 5 7}_{1.22}$</td>
<td style="text-align: right;">$\mathbf{4 0 . 8 4}_{2.83}$</td>
</tr>
<tr>
<td style="text-align: left;">ICL with gold labeled demonstrations</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Random (Gold)</td>
<td style="text-align: right;">$49.42_{0.22}$</td>
<td style="text-align: right;">$53.51_{1.38}$</td>
</tr>
<tr>
<td style="text-align: left;">Nearest (Gold)</td>
<td style="text-align: right;">$64.16_{1.08}$</td>
<td style="text-align: right;">$61.58_{1.58}$</td>
</tr>
</tbody>
</table>
<p>Table 7: Results on Chinese benchmarks. Right subscript numbers are standard deviations. Gold indicates access to the gold labeled data, thus is not comparable with the rest of methods. Two-stage majority voting is used here. Texts in bold are the best results.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>C Results on Other Embedding Models</h2>
<p>We explore the effect of using other embedding models for retrieval, SBERT (Reimers and Gurevych, 2019) ${ }^{7}$ and GTE (Li et al., 2023c) ${ }^{8}$. Results are in Table 8.</p>
<h2>D Results on Various Number of Demonstrations</h2>
<p>We investigate the performance on various number of demonstrations in the input context, the results are in Table 9. As shown in the table, the quantity of examples is not always proportional to the final performance. Similar findings have also been mentioned in Min et al. (2022). We hypothesize that after the LLM learns the mapping between the input-output examples, new information gained from more examples is marginal and might be offset by the more noise introduced.</p>
<h2>E More Results on Threshold Filtering</h2>
<p>Table 10 shows the results of various values of entity-level and sample-level SC thresholds.</p>
<h2>F Upper Bound of Reliable Annotation Selection</h2>
<p>Table 11 summarizes the complete results of the upper bound of reliable annotation selection.</p>
<h2>G Illustration of Iterative Self-improving</h2>
<p>The bootstrapping process of iterative selfimproving is shown in Fig. 5.</p>
<h2>H Case Study</h2>
<p>We take a closer look at the cases where the errors in predictions are corrected with self-annotated demonstrations, as shown in Fig. 6. The proposed framework makes the model reuse its own knowledge and correct its own errors, forming a process of self-improving.</p>
<h2>I Prompts</h2>
<p>We show the prompts use in this work in Table 12. We take samples from ACE05 for demonstrations.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Datasets</th>
<th style="text-align: center;">CoNLL2003</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">WikiGold</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Embedding Models</td>
<td style="text-align: center;">embed-ada</td>
<td style="text-align: center;">SBERT</td>
<td style="text-align: center;">GTE</td>
<td style="text-align: center;">embed-ada</td>
<td style="text-align: center;">SBERT</td>
<td style="text-align: center;">GTE</td>
</tr>
<tr>
<td style="text-align: center;">No-demos</td>
<td style="text-align: center;">$68.97_{0.22}$</td>
<td style="text-align: center;">$68.97_{0.22}$</td>
<td style="text-align: center;">$68.97_{0.22}$</td>
<td style="text-align: center;">70.80</td>
<td style="text-align: center;">70.80</td>
<td style="text-align: center;">70.80</td>
</tr>
<tr>
<td style="text-align: center;">ICL with self-annotated demonstrations (Zero-shot)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">$72.12_{0.59}$</td>
<td style="text-align: center;">$72.12_{0.59}$</td>
<td style="text-align: center;">$72.12_{0.59}$</td>
<td style="text-align: center;">72.32</td>
<td style="text-align: center;">72.32</td>
<td style="text-align: center;">72.32</td>
</tr>
<tr>
<td style="text-align: center;">Nearest</td>
<td style="text-align: center;">$71.66_{0.37}$</td>
<td style="text-align: center;">$72.07_{0.22}$</td>
<td style="text-align: center;">$72.37_{1.17}$</td>
<td style="text-align: center;">72.84</td>
<td style="text-align: center;">72.39</td>
<td style="text-align: center;">72.24</td>
</tr>
<tr>
<td style="text-align: center;">Diverse Nearest, SC ranking</td>
<td style="text-align: center;">$74.51_{0.03}$</td>
<td style="text-align: center;">$72.67_{0.37}$</td>
<td style="text-align: center;">$72.53_{0.96}$</td>
<td style="text-align: center;">73.98</td>
<td style="text-align: center;">76.08</td>
<td style="text-align: center;">73.60</td>
</tr>
<tr>
<td style="text-align: center;">ICL with gold labeled demonstrations</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Random (Gold)</td>
<td style="text-align: center;">$77.25_{1.39}$</td>
<td style="text-align: center;">$77.25_{1.39}$</td>
<td style="text-align: center;">$77.25_{1.39}$</td>
<td style="text-align: center;">75.82</td>
<td style="text-align: center;">75.82</td>
<td style="text-align: center;">75.82</td>
</tr>
<tr>
<td style="text-align: center;">Nearest (Gold)</td>
<td style="text-align: center;">$84.71_{0.39}$</td>
<td style="text-align: center;">$83.28_{1.34}$</td>
<td style="text-align: center;">$83.59_{0.09}$</td>
<td style="text-align: center;">79.40</td>
<td style="text-align: center;">78.18</td>
<td style="text-align: center;">79.03</td>
</tr>
</tbody>
</table>
<p>Table 8: Results on various embedding models. Right subscript numbers are standard deviations. embed-ada refers to text-embedding-ada. Gold indicates access to the gold labeled data, thus is not comparable with the rest of methods. Two-stage majority voting is used here. Texts in bold are the best results.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Num</th>
<th style="text-align: center;">0</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">8</th>
<th style="text-align: center;">16</th>
<th style="text-align: center;">32</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">WikiGold</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">70.80</td>
<td style="text-align: center;">70.25</td>
<td style="text-align: center;">70.86</td>
<td style="text-align: center;">$\mathbf{7 1 . 7 4}$</td>
<td style="text-align: center;">71.39</td>
<td style="text-align: center;">70.35</td>
</tr>
<tr>
<td style="text-align: left;">Nearest</td>
<td style="text-align: center;">70.80</td>
<td style="text-align: center;">70.41</td>
<td style="text-align: center;">71.32</td>
<td style="text-align: center;">70.47</td>
<td style="text-align: center;">$\mathbf{7 2 . 5 7}$</td>
<td style="text-align: center;">71.81</td>
</tr>
<tr>
<td style="text-align: left;">Random (Gold)</td>
<td style="text-align: center;">70.80</td>
<td style="text-align: center;">71.75</td>
<td style="text-align: center;">71.54</td>
<td style="text-align: center;">$\mathbf{7 5 . 7 9}$</td>
<td style="text-align: center;">73.95</td>
<td style="text-align: center;">74.43</td>
</tr>
<tr>
<td style="text-align: left;">Nearest (Gold)</td>
<td style="text-align: center;">70.80</td>
<td style="text-align: center;">76.14</td>
<td style="text-align: center;">77.66</td>
<td style="text-align: center;">$\mathbf{7 8 . 9 7}$</td>
<td style="text-align: center;">78.34</td>
<td style="text-align: center;">77.05</td>
</tr>
<tr>
<td style="text-align: left;">CoNLL03</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">68.97</td>
<td style="text-align: center;">69.54</td>
<td style="text-align: center;">70.84</td>
<td style="text-align: center;">70.53</td>
<td style="text-align: center;">70.72</td>
<td style="text-align: center;">$\mathbf{7 1 . 9 5}$</td>
</tr>
<tr>
<td style="text-align: left;">Nearest</td>
<td style="text-align: center;">68.97</td>
<td style="text-align: center;">70.12</td>
<td style="text-align: center;">69.15</td>
<td style="text-align: center;">70.90</td>
<td style="text-align: center;">71.81</td>
<td style="text-align: center;">$\mathbf{7 2 . 4 4}$</td>
</tr>
<tr>
<td style="text-align: left;">Random (Gold)</td>
<td style="text-align: center;">68.97</td>
<td style="text-align: center;">71.94</td>
<td style="text-align: center;">72.76</td>
<td style="text-align: center;">75.12</td>
<td style="text-align: center;">77.81</td>
<td style="text-align: center;">$\mathbf{8 0 . 4 3}$</td>
</tr>
<tr>
<td style="text-align: left;">Nearest (Gold)</td>
<td style="text-align: center;">68.97</td>
<td style="text-align: center;">79.07</td>
<td style="text-align: center;">80.81</td>
<td style="text-align: center;">83.20</td>
<td style="text-align: center;">$\mathbf{8 4 . 1 2}$</td>
<td style="text-align: center;">83.94</td>
</tr>
</tbody>
</table>
<p>Table 9: Results on various number of demonstrations in the input context. Gold indicates access to the gold labeled data, thus is not comparable with the rest of methods. Two-stage majority voting is used here. Texts in bold are the best results. Since the standard deviation values of CoNLL03 are around the same level as in Table 1, we omit them here.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The pipeline of iterative self-improving.</p>
<p>Input Sentence: Angelo has reached out to corporate America, the young and successful , the trendy . $\cdots$
Gold Label: [['America': 'Location', 'Angelo': 'Person']].
Self-annotated demonstrations:
Text: The Anguilla United Front is an alliance of political parties in Anguilla .
Answer: [['Anguilla United Front': 'Organization'], {'Anguilla': 'Location'}]
No-demos pred.: [['Angelo': 'Person']].
Ours pred.: [['America': 'Location', 'Angelo': 'Person']].
Input Sentence: Ben now also helps run Movement Bodyboarding MagAzine.
Gold Label: [['Movement Bodyboarding Magazine': 'Organization', 'Ben': 'Person']].
Self-annotated demonstrations:
Text: Bobick had now improved enough as a boxer to be a legitimate title threat .
Answer: [['Bobick': 'Person']]
No-demos pred.: [['Movement Bodyboarding Magazine': 'Organization']].
Ours pred.: [['Ben': 'Person', 'Movement Bodyboarding Magazine': 'Organization']].</p>
<p>Figure 6: Case study of self-improving. Examples from WikiGold are illustrated. The errors in predictions of No-demos are corrected by our framework with self-annotated demonstrations. Texts in green are entities corrected by our method. Texts in blue are entities in demonstrations that potentially help with the error correction.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">CoNLL03</th>
<th style="text-align: center;">ACE05</th>
<th style="text-align: center;">WikiGold</th>
<th style="text-align: center;">GENIA</th>
<th style="text-align: center;">Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">No-demos</td>
<td style="text-align: center;">$68.97_{0.22}$</td>
<td style="text-align: center;">$27.29_{0.58}$</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">$47.41_{0.29}$</td>
<td style="text-align: center;">53.62</td>
</tr>
<tr>
<td style="text-align: center;">Entity-level SC threshold $=3.0$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">$71.17_{0.13}$</td>
<td style="text-align: center;">$30.16_{0.66}$</td>
<td style="text-align: center;">71.79</td>
<td style="text-align: center;">$50.41_{0.00}$</td>
<td style="text-align: center;">55.88</td>
</tr>
<tr>
<td style="text-align: center;">Nearest</td>
<td style="text-align: center;">$71.41_{0.66}$</td>
<td style="text-align: center;">$31.58_{0.76}$</td>
<td style="text-align: center;">73.16</td>
<td style="text-align: center;">$51.24_{1.79}$</td>
<td style="text-align: center;">56.85</td>
</tr>
<tr>
<td style="text-align: center;">Diverse Nearest, random</td>
<td style="text-align: center;">$72.68_{1.31}$</td>
<td style="text-align: center;">$31.39_{1.62}$</td>
<td style="text-align: center;">72.01</td>
<td style="text-align: center;">$50.65_{0.11}$</td>
<td style="text-align: center;">56.68</td>
</tr>
<tr>
<td style="text-align: center;">Diverse Nearest, SC ranking</td>
<td style="text-align: center;">$73.68_{0.03}$</td>
<td style="text-align: center;">$31.86_{0.13}$</td>
<td style="text-align: center;">73.36</td>
<td style="text-align: center;">$51.15_{0.69}$</td>
<td style="text-align: center;">57.51</td>
</tr>
<tr>
<td style="text-align: center;">Entity-level SC threshold $=4.0$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">$70.91_{0.55}$</td>
<td style="text-align: center;">$30.41_{0.95}$</td>
<td style="text-align: center;">72.33</td>
<td style="text-align: center;">$50.70_{1.53}$</td>
<td style="text-align: center;">56.09</td>
</tr>
<tr>
<td style="text-align: center;">Nearest</td>
<td style="text-align: center;">$73.24_{0.53}$</td>
<td style="text-align: center;">$32.22_{0.38}$</td>
<td style="text-align: center;">72.53</td>
<td style="text-align: center;">$49.85_{1.20}$</td>
<td style="text-align: center;">56.96</td>
</tr>
<tr>
<td style="text-align: center;">Diverse Nearest, random</td>
<td style="text-align: center;">$74.11_{0.12}$</td>
<td style="text-align: center;">$32.29_{0.31}$</td>
<td style="text-align: center;">72.01</td>
<td style="text-align: center;">$50.68_{0.14}$</td>
<td style="text-align: center;">57.27</td>
</tr>
<tr>
<td style="text-align: center;">Diverse Nearest, SC ranking</td>
<td style="text-align: center;">$74.99_{0.20}$</td>
<td style="text-align: center;">$31.65_{0.97}$</td>
<td style="text-align: center;">73.53</td>
<td style="text-align: center;">$51.11_{0.28}$</td>
<td style="text-align: center;">57.82</td>
</tr>
<tr>
<td style="text-align: center;">Entity-level SC threshold $=5.0$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">$72.53_{0.07}$</td>
<td style="text-align: center;">$29.44_{0.73}$</td>
<td style="text-align: center;">72.13</td>
<td style="text-align: center;">$50.65_{0.57}$</td>
<td style="text-align: center;">56.18</td>
</tr>
<tr>
<td style="text-align: center;">Nearest</td>
<td style="text-align: center;">$74.24_{0.03}$</td>
<td style="text-align: center;">$29.65_{1.30}$</td>
<td style="text-align: center;">72.45</td>
<td style="text-align: center;">$48.12_{0.45}$</td>
<td style="text-align: center;">56.11</td>
</tr>
<tr>
<td style="text-align: center;">Diverse Nearest, random</td>
<td style="text-align: center;">$73.50_{0.14}$</td>
<td style="text-align: center;">$30.55_{0.27}$</td>
<td style="text-align: center;">71.34</td>
<td style="text-align: center;">$49.34_{0.27}$</td>
<td style="text-align: center;">56.18</td>
</tr>
<tr>
<td style="text-align: center;">Diverse Nearest, SC ranking</td>
<td style="text-align: center;">$72.50_{0.66}$</td>
<td style="text-align: center;">$30.14_{0.35}$</td>
<td style="text-align: center;">74.01</td>
<td style="text-align: center;">$49.57_{0.61}$</td>
<td style="text-align: center;">56.55</td>
</tr>
<tr>
<td style="text-align: center;">Sample-level SC threshold $=3.0$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">$70.17_{0.00}$</td>
<td style="text-align: center;">$28.78_{1.71}$</td>
<td style="text-align: center;">71.81</td>
<td style="text-align: center;">$50.45_{0.34}$</td>
<td style="text-align: center;">55.30</td>
</tr>
<tr>
<td style="text-align: center;">Nearest</td>
<td style="text-align: center;">$69.48_{0.90}$</td>
<td style="text-align: center;">$30.39_{0.17}$</td>
<td style="text-align: center;">70.33</td>
<td style="text-align: center;">$51.76_{0.29}$</td>
<td style="text-align: center;">55.49</td>
</tr>
<tr>
<td style="text-align: center;">Diverse Nearest, random</td>
<td style="text-align: center;">$68.98_{0.86}$</td>
<td style="text-align: center;">$30.04_{0.34}$</td>
<td style="text-align: center;">69.71</td>
<td style="text-align: center;">$51.71_{1.41}$</td>
<td style="text-align: center;">55.11</td>
</tr>
<tr>
<td style="text-align: center;">Diverse Nearest, SC ranking</td>
<td style="text-align: center;">$74.32_{1.37}$</td>
<td style="text-align: center;">$30.73_{0.04}$</td>
<td style="text-align: center;">74.44</td>
<td style="text-align: center;">$52.31_{0.34}$</td>
<td style="text-align: center;">57.95</td>
</tr>
<tr>
<td style="text-align: center;">Sample-level SC threshold $=4.0$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">$72.41_{1.28}$</td>
<td style="text-align: center;">$30.05_{1.26}$</td>
<td style="text-align: center;">73.38</td>
<td style="text-align: center;">$51.61_{1.21}$</td>
<td style="text-align: center;">56.86</td>
</tr>
<tr>
<td style="text-align: center;">Nearest</td>
<td style="text-align: center;">$72.28_{0.14}$</td>
<td style="text-align: center;">$32.00_{0.08}$</td>
<td style="text-align: center;">73.27</td>
<td style="text-align: center;">$52.72_{0.80}$</td>
<td style="text-align: center;">57.57</td>
</tr>
<tr>
<td style="text-align: center;">Diverse Nearest, random</td>
<td style="text-align: center;">$72.32_{0.08}$</td>
<td style="text-align: center;">$30.74_{0.06}$</td>
<td style="text-align: center;">72.09</td>
<td style="text-align: center;">$52.50_{0.50}$</td>
<td style="text-align: center;">56.91</td>
</tr>
<tr>
<td style="text-align: center;">Diverse Nearest, SC ranking</td>
<td style="text-align: center;">$73.97_{0.12}$</td>
<td style="text-align: center;">$31.08_{0.54}$</td>
<td style="text-align: center;">72.80</td>
<td style="text-align: center;">$51.67_{0.93}$</td>
<td style="text-align: center;">57.38</td>
</tr>
<tr>
<td style="text-align: center;">Sample-level SC threshold $=5.0$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">$73.66_{0.69}$</td>
<td style="text-align: center;">$29.19_{0.26}$</td>
<td style="text-align: center;">71.92</td>
<td style="text-align: center;">$51.34_{0.97}$</td>
<td style="text-align: center;">56.52</td>
</tr>
<tr>
<td style="text-align: center;">Nearest</td>
<td style="text-align: center;">$74.19_{0.30}$</td>
<td style="text-align: center;">$30.94_{0.11}$</td>
<td style="text-align: center;">74.96</td>
<td style="text-align: center;">$52.01_{0.23}$</td>
<td style="text-align: center;">58.02</td>
</tr>
<tr>
<td style="text-align: center;">Diverse Nearest, random</td>
<td style="text-align: center;">$73.16_{0.66}$</td>
<td style="text-align: center;">$27.98_{0.08}$</td>
<td style="text-align: center;">74.55</td>
<td style="text-align: center;">$50.64_{0.18}$</td>
<td style="text-align: center;">56.58</td>
</tr>
<tr>
<td style="text-align: center;">Diverse Nearest, SC ranking</td>
<td style="text-align: center;">$74.53_{0.51}$</td>
<td style="text-align: center;">$30.00_{0.73}$</td>
<td style="text-align: center;">73.60</td>
<td style="text-align: center;">$51.02_{0.98}$</td>
<td style="text-align: center;">57.28</td>
</tr>
</tbody>
</table>
<p>Table 10: Results of various entity-level SC thresholds and sample-level SC thresholds. Right subscript numbers are standard deviations.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">CoNLL03</th>
<th style="text-align: center;">ACE05</th>
<th style="text-align: center;">WikiGold</th>
<th style="text-align: center;">GENIA</th>
<th style="text-align: center;">Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">No-demos</td>
<td style="text-align: center;">$68.97_{0.22}$</td>
<td style="text-align: center;">$27.29_{0.58}$</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">$47.41_{0.29}$</td>
<td style="text-align: center;">53.62</td>
</tr>
<tr>
<td style="text-align: left;">Two-stage majority voting</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">$72.12_{0.59}$</td>
<td style="text-align: center;">$31.18_{0.38}$</td>
<td style="text-align: center;">72.32</td>
<td style="text-align: center;">$50.17_{0.93}$</td>
<td style="text-align: center;">56.45</td>
</tr>
<tr>
<td style="text-align: left;">Nearest</td>
<td style="text-align: center;">$71.66_{0.37}$</td>
<td style="text-align: center;">$31.45_{1.32}$</td>
<td style="text-align: center;">72.84</td>
<td style="text-align: center;">$50.19_{1.59}$</td>
<td style="text-align: center;">56.53</td>
</tr>
<tr>
<td style="text-align: left;">Diverse Nearest, random</td>
<td style="text-align: center;">$72.45_{0.41}$</td>
<td style="text-align: center;">$30.84_{10.56}$</td>
<td style="text-align: center;">70.83</td>
<td style="text-align: center;">$51.03_{0.73}$</td>
<td style="text-align: center;">56.28</td>
</tr>
<tr>
<td style="text-align: left;">Diverse Nearest, SC ranking</td>
<td style="text-align: center;">$74.51_{0.03}$</td>
<td style="text-align: center;">$32.27_{0.25}$</td>
<td style="text-align: center;">73.98</td>
<td style="text-align: center;">$52.06_{0.09}$</td>
<td style="text-align: center;">58.20</td>
</tr>
<tr>
<td style="text-align: left;">Upper bound</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">$73.72_{0.41}$</td>
<td style="text-align: center;">$32.71_{0.56}$</td>
<td style="text-align: center;">73.83</td>
<td style="text-align: center;">$52.67_{0.09}$</td>
<td style="text-align: center;">58.23</td>
</tr>
<tr>
<td style="text-align: left;">Nearest</td>
<td style="text-align: center;">$81.65_{0.17}$</td>
<td style="text-align: center;">$37.82_{0.59}$</td>
<td style="text-align: center;">76.57</td>
<td style="text-align: center;">$56.24_{0.44}$</td>
<td style="text-align: center;">63.07</td>
</tr>
<tr>
<td style="text-align: left;">Diverse Nearest, random</td>
<td style="text-align: center;">$78.84_{1.43}$</td>
<td style="text-align: center;">$35.79_{0.26}$</td>
<td style="text-align: center;">76.20</td>
<td style="text-align: center;">$54.46_{0.98}$</td>
<td style="text-align: center;">61.32</td>
</tr>
<tr>
<td style="text-align: left;">Diverse Nearest, SC ranking</td>
<td style="text-align: center;">$80.12_{0.02}$</td>
<td style="text-align: center;">$35.23_{0.63}$</td>
<td style="text-align: center;">76.64</td>
<td style="text-align: center;">$54.58_{0.57}$</td>
<td style="text-align: center;">61.64</td>
</tr>
</tbody>
</table>
<p>Table 11: Complete results of the upper bound of reliable annotation selection. Right subscript numbers are standard deviations.</p>
<h1>Prompts of zero-shot setting</h1>
<p>Given entity label set: {'Person', 'Organization', 'Location', 'Facility', 'Weapon', 'Vehicle', 'Geo-Political Entity'].
Please recognize the named entities in the given text. Based on the given entity label set, provide answer in the following JSON format: [ {'Entity Name': 'Entity Label'}]. If there is no entity in the text, return the following empty list: [].</p>
<p>Text: right now we 're also waiting to hear from the president at the white house .
Answer:
Prompts of ICL
Given entity label set: {'Person', 'Organization', 'Location', 'Facility', 'Weapon', 'Vehicle', 'Geo-Political Entity'].
Please recognize the named entities in the given text. Based on the given entity label set, provide answer in the following JSON format: [ {'Entity Name': 'Entity Label'}]. If there is no entity in the text, return the following empty list: [].</p>
<p>Text: right now we 're also waiting to hear from the president at the white house .
Answer: [ {'white house': 'Location'}, {'president': 'Person'}]
Text: At the Pentagon, Barbara Starr reports officials say today begins a new strategy in the skies over Baghdad .
Answer: [ {'Barbara Starr': 'Person'}, {'Pentagon': 'Facility'}, {'officials': 'Person'}, {'skies':'Location'}, {'Baghdad': 'Geo-Political Entity'}]</p>
<p>Text: John Irvine , ITV News , Baghdad .
Answer: [ {'John Irvine': 'Person'}, {'ITV News': 'Organization'},
{'Baghdad': Geo-Political Entity'}]
... ...</p>
<p>Text: right now we 're also waiting to hear from the president at the white house .
Answer:</p>
<p>Table 12: Prompts used in this work. A few samples from ACE05 are displayed for demonstrations.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ https://catalog.ldc.upenn.edu/LDC2006T06
${ }^{6}$ https://catalog.ldc.upenn.edu/LDC2011T03&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{7}$ https://huggingface.co/sentence-transformers/ all-mpnet-base-v2
${ }^{8}$ https://huggingface.co/thenlper/gte-large&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
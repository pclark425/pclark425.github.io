<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5364 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5364</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5364</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-a4c40532e68728fbeab5d9415f6ad8e9530db360</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a4c40532e68728fbeab5d9415f6ad8e9530db360" target="_blank">The WebNLG Challenge: Generating Text from RDF Data</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Natural Language Generation</p>
                <p><strong>Paper TL;DR:</strong> The microplanning task is introduced, data preparation, evaluation methodology, participant results and a brief description of the participating systems are provided.</p>
                <p><strong>Paper Abstract:</strong> The WebNLG challenge consists in mapping sets of RDF triples to text. It provides a common benchmark on which to train, evaluate and compare “microplanners”, i.e. generation systems that verbalise a given content by making a range of complex interacting choices including referring expression generation, aggregation, lexicalisation, surface realisation and sentence segmentation. In this paper, we introduce the microplanning task, describe data preparation, introduce our evaluation methodology, analyse participant results and provide a brief description of the participating systems.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5364.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5364.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baseline-Delex-Linear</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Baseline delexicalised linearisation + seq2seq (OpenNMT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline system that linearises sets of RDF triples, delexicalises entity surface forms to abstract placeholders, trains an attention-based sequence-to-sequence LSTM model (OpenNMT), and re-lexicalises outputs using entity mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>delexicalised linearisation (sequence-to-sequence)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Input: RDF triples are linearised into a sequence (order chosen) and tokenised. Exact-match delexicalisation replaces subject entities by their category (e.g., COUNTRY, FOOD) and replaces object tokens that match properties by property placeholders (e.g., COUNTRY). Training on delexicalised data uses a two-layer bidirectional LSTM encoder-decoder with attention (OpenNMT) and standard decoding; generated delexicalised text is then re-lexicalised by substituting placeholders with entities present in the predicted output.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF triples / knowledge-graph triples</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Compact linear string representation of graph; interpretable mapping between triple slots and placeholders; reduces surface sparsity via delexicalisation; may lose some entity-specific lexical signals; depends on exact-match delexicalisation (incomplete replacement possible).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Data-to-text generation (microplanning) on the WebNLG dataset (seen/unseen categories split).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Global: BLEU 33.24, TER 0.61, METEOR 0.23 (reported in paper Table 5). Seen categories: BLEU 52.39, TER 0.44, METEOR 0.37 (Table 6). Unseen categories: BLEU 6.13, TER 0.80, METEOR 0.07 (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Outperformed by several participant systems (both NMT and SMT) on global metrics; performs competitively on seen data (middle ranks) but fails on unseen categories where delexicalisation and exact matching were insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relies on exact-match delexicalisation, causing incomplete substitution and errors; linearisation loses explicit graph structure and may obscure relations; poor generalisation to unseen properties/domains; limited morphological handling (target relexicalisation may be brittle).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The WebNLG Challenge: Generating Text from RDF Data', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5364.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5364.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UTILBURG-SMT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UTilburg statistical machine translation (Moses) over linearised delexicalised triples</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An SMT approach that treats linearised, delexicalised RDF triple sets as 'source' sentences and delexicalised verbalizations as 'target' sentences; trained with MGIZA alignments and Moses phrase-based SMT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>delexicalised linearisation (SMT)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Input triples are linearised and delexicalised (entities replaced with placeholders and annotated with Wikipedia IDs). Training uses MGIZA to obtain alignments and Moses to learn phrase translation and language models; a 6-gram KenLM language model trained on Gigaword is used for final ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF triples / knowledge-graph triples</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>String serialisation amenable to SMT alignment; delexicalisation reduces data sparsity; allows use of strong n-gram LM for surface fluency; preserves some ordering via linearisation but discards explicit graph topology.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Data-to-text generation (WebNLG microplanning task), evaluated on seen/unseen splits.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Global: BLEU 44.28, TER 0.53, METEOR 0.38 (Table 5). Seen categories: BLEU 54.29, TER 0.47, METEOR 0.42 (Table 6). Unseen categories: BLEU 29.88, TER 0.61, METEOR 0.33 (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>One of the top-performing systems globally (BLEU second place); outperforms many NMT and pipeline systems on seen data and overall; however UPF-FORGE outperforms it on certain metrics (e.g., METEOR global ranking close).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Depends on good delexicalisation and alignment; linearisation hides graph structure; adaptation to unseen properties limited (lower scores on unseen categories than best pipeline UPF-FORGE).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The WebNLG Challenge: Generating Text from RDF Data', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5364.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5364.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ADAPTCentre-NMT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ADAPTCentre NMT with subword linearisation (Nematus)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural encoder-decoder system (Nematus) that linearises RDF triples and uses subword (BPE-like) representations instead of delexicalisation, inserting tuple-separation special tokens to mark triple boundaries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>linearisation with tuple separation + subword encoding (NMT)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Triples are linearised into a sequence where tuple separation tokens mark boundaries between triples/tuples. Instead of delexicalisation, the system applies subword segmentation to handle rare words. The linearised subword token sequence is used as input to an attention-based encoder-decoder (Nematus).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF triples / knowledge-graph triples</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Preserves triple segmentation via separator tokens; subword encoding mitigates rare-word sparsity and reduces need for delexicalisation; still flattens graph structure into sequence, potentially losing explicit relation topology.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Data-to-text (WebNLG), seen/unseen categories.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Global: BLEU 31.06, TER 0.84, METEOR 0.31 (Table 5). Seen categories: ranked 1 for BLEU on seen test (ADAPT BLEU unspecified in Table 6 top line indicates ADAPT best on seen), Seen TER 0.37, METEOR 0.44 (Table 6 entries). Unseen: BLEU 10.53, TER 1.4, METEOR 0.19 (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>On seen categories ADAPTCentre ranked well (benefit from subword modeling), but overall global scores lower than top NMT/SMT competitors; performs poorly on unseen categories, suggesting subword handling does not fully address unseen property lexicalisation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Sequence linearisation still removes explicit graph connectivity; subword approach helps rare lexical items but does not guarantee correct lexicalisation of unseen properties; poor generalisation to unseen domains observed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The WebNLG Challenge: Generating Text from RDF Data', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5364.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5364.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UMelbourne-NMT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UMelbourne delexicalisation + type-enriched linearisation (NMT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neural encoder-decoder model where entities are delexicalised to ENTITY-ID placeholders, optionally enriched with DBPedia types appended to entity tokens; n-gram search ensures accurate delexicalisation mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>delexicalised linearisation with entity-type enrichment (NMT)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Triples are linearised and entities are delexicalised using ENTITY-ID placeholders; when available, DBPedia types are appended to entity placeholders to provide type signals (e.g., ENTITY-ID|Astronaut). An n-gram match procedure aligns textual mentions in targets with input entities. This sequence is given to a standard attention-based encoder-decoder model.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF triples / knowledge-graph triples</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Combines delexicalisation (reducing sparsity) with type information (more semantic signal than bare placeholders); retains triple order via linearisation; still flattens graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>WebNLG data-to-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Global: Melbourne (UMelbourne) BLEU 45.13, TER 0.47, METEOR 0.37 (Table 5). Seen: BLEU 54.52, TER 0.40, METEOR 0.41 (Table 6). Unseen: BLEU 33.27, TER 0.55, METEOR 0.33 (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Top global performer (BLEU) in the challenge; outperforms baseline and many other NMT systems, indicating that delexicalisation plus type signals are effective. On unseen data it ranks high but UPF-FORGE still outperforms certain metrics on unseen.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires accurate delexicalisation (n-gram search); type annotations must be available; linearisation still discards explicit graph topology; potential mismatch when entity mentions in surface text do not exactly align with input triplets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The WebNLG Challenge: Generating Text from RDF Data', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5364.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5364.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UTilburg-Pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UTILBURG pipeline: extracted templates+ordering+6-gram LM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline system that extracts delexicalised templates/rules mapping triples or triple sets to delexicalised text, orders triples for discourse, uses a referring expression module to fill missing entities, and ranks outputs with a 6-gram language model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>template/rule extraction with triple ordering (pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Rules/templates are mined from training data mapping a (delexicalised) triple or triple-set to observed delexicalised text. At generation time, triples are ordered (to maintain discourse order), matching templates are applied to produce delexicalised text, referring expressions inserted, and final outputs ranked using a 6-gram LM trained on Gigaword.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF triples / knowledge-graph triples</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Preserves explicit mapping from triples to sentence fragments; interpretable and adaptable; can capture aggregation and sentence segmentation patterns seen in training; may be brittle for unseen property combinations and requires template coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>WebNLG data-to-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Global: UTILBURG-PIPELINE BLEU 35.29, TER 0.56, METEOR 0.30 (Table 5). Seen/Unseen splits: Seen BLEU 44.34 (Table 6), Unseen BLEU 20.65 (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Pipeline systems generally underperform ML-based systems on seen categories, but template approaches can adapt to unseen properties if templates/grammars are definable; overall lower global scores than top SMT/NMT but competitive when high-quality templates exist.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires extraction/coverage of templates from training data; less robust to unseen properties/patterns; delexicalisation dependency; pipeline complexity (ordering, REG) adds engineering overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The WebNLG Challenge: Generating Text from RDF Data', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5364.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5364.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UIT-VNU-HCM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UIT-VNU-HCM typed-dependency template extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline approach that extracts generation rules using typed-dependency structures from training texts (no delexicalisation), and uses WordNet to estimate predicate similarity at run time for test-train generalisation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>typed-dependency-based templates</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Instead of extracting surface templates, the system uses parsed typed-dependency structures of training texts to derive template rules. At generation time, for unseen predicates, WordNet-based similarity is used to map test predicates to similar training predicates to select templates. No delexicalisation is performed during rule extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF triples / knowledge-graph triples (paired with dependency templates)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Leverages syntactic dependency patterns to capture structural realization choices; can generalise across predicates via lexical similarity (WordNet); keeps more syntactic information than flat templates.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>WebNLG data-to-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Global: BLEU 7.07, TER 0.82, METEOR 0.09 (Table 5). Seen: BLEU 19.87, TER 0.78, METEOR 0.15 (Table 6). Unseen: BLEU 0.11, TER 0.87, METEOR 0.03 (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Ranks low compared to other pipeline and ML systems, indicating difficulties scaling template extraction from dependency structures and generalising to unseen categories despite WordNet similarity heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No delexicalisation causes heavy sparsity; dependency-based templates may not match diverse lexicalisations; WordNet-based mapping insufficient to cover unseen property lexicalisation variability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The WebNLG Challenge: Generating Text from RDF Data', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5364.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5364.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UPF-FORGe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UPF-FORGe: PredArg templates -> PredArg graph -> FORGe generator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A grammar-based sentence planning system that maps DBpedia properties to manually defined predicate-argument (PredArg) templates, builds PredArg graph structures for triple sets, and uses the FORGe generator for surface realisation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>PredArg template conversion to PredArg graph + FORGe generation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For each DBpedia property, human-authored PredArg templates encode linguistic and DBpedia-specific features; given a triple set, PredArg templates produce PredArg structures which are aggregated into a PredArg graph capturing inter-triple relations and aggregation opportunities; the FORGe grammar-based generator consumes this PredArg graph to produce fluent text.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF triples -> PredArg graph (linguistic graph representation)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Retains explicit structured linguistic representation (PredArg graph) enabling aggregation, sentence segmentation, and controlled lexicalisation; high interpretability and direct mapping between data properties and linguistic realisation; requires manual template authoring for properties.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>WebNLG data-to-text generation, including generalisation to unseen categories.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Global: BLEU 38.65, TER 0.55, METEOR 0.39 (Table 5). Seen: BLEU 40.88, TER 0.55, METEOR 0.40 (Table 6). Unseen: BLEU 35.70, TER 0.55, METEOR 0.37 (Table 7). UPF-FORGe ranked first on METEOR globally and first on unseen categories.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>UPF-FORGe outperformed many ML systems on unseen categories (BLEU and METEOR), showing better adaptability to unseen properties due to manual PredArg templates; ranked top in METEOR globally despite lower BLEU than the very best SMT/NMT on seen data.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires manual effort to define PredArg templates for properties (scalability issue across large property sets); may be less flexible to diverse lexicalisations not covered by templates; development cost higher than data-driven approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The WebNLG Challenge: Generating Text from RDF Data', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5364.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5364.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PKUWriter-Ensemble-Ranker-RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PKUWRITER: ensemble of attention-based encoder-decoder models + supervised ranker + reinforcement learning objective</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid approach combining an ensemble of seq2seq generation models, a supervised ranker trained on BLEU-scored outputs to select best verbalisation, and an additional RL objective encouraging generation to include input subjects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>linearised delexicalised input + ensemble + output ranking + RL objective</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Input triple sets are linearised (and delexicalised) and fed to multiple attention-based encoder-decoder models (ensemble). Each model produces candidate verbalizations; a supervised ranker (trained on features and BLEU scores computed against references) selects the best output. The generation models are additionally trained with a reinforcement learning objective that rewards inclusion of input subjects in outputs. Hand-crafted rules post-process cases where models fail.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF triples / knowledge-graph triples</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Combines data-driven generation with discriminative reranking to improve output selection; RL component injects task-specific desiderata (inclusion of subjects); ensemble increases robustness to individual model errors.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>WebNLG data-to-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Global: BLEU 39.88, TER 0.55, METEOR 0.31 (Table 5). Seen: BLEU 51.23, TER 0.45, METEOR 0.37 (Table 6). Unseen: BLEU 25.36, TER 0.67, METEOR 0.24 (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Ranks among the top systems globally; outperforming baseline and many single-model NMTs. The ranker and RL objective improved selection of higher-BLEU outputs relative to single-model decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Ensemble and ranker increase system complexity and training data partitioning requirements; RL objective focused on subject inclusion may not capture other quality aspects; still reliant on linearisation and delexicalisation that can impede unseen property handling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The WebNLG Challenge: Generating Text from RDF Data', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>FORGe at SemEval-2017 task 9: Deep sentence generation based on a sequence of graph transducers <em>(Rating: 2)</em></li>
                <li>Nematus: A toolkit for neural machine translation <em>(Rating: 2)</em></li>
                <li>Moses: Open source toolkit for statistical machine translation <em>(Rating: 2)</em></li>
                <li>OpenNMT: Open-Source Toolkit for Neural Machine Translation <em>(Rating: 2)</em></li>
                <li>Neural machine translation by jointly learning to align and translate <em>(Rating: 1)</em></li>
                <li>Creating training corpora for nlg micro-planners <em>(Rating: 2)</em></li>
                <li>Split and rephrase <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5364",
    "paper_id": "paper-a4c40532e68728fbeab5d9415f6ad8e9530db360",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "Baseline-Delex-Linear",
            "name_full": "Baseline delexicalised linearisation + seq2seq (OpenNMT)",
            "brief_description": "A baseline system that linearises sets of RDF triples, delexicalises entity surface forms to abstract placeholders, trains an attention-based sequence-to-sequence LSTM model (OpenNMT), and re-lexicalises outputs using entity mappings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "delexicalised linearisation (sequence-to-sequence)",
            "representation_description": "Input: RDF triples are linearised into a sequence (order chosen) and tokenised. Exact-match delexicalisation replaces subject entities by their category (e.g., COUNTRY, FOOD) and replaces object tokens that match properties by property placeholders (e.g., COUNTRY). Training on delexicalised data uses a two-layer bidirectional LSTM encoder-decoder with attention (OpenNMT) and standard decoding; generated delexicalised text is then re-lexicalised by substituting placeholders with entities present in the predicted output.",
            "graph_type": "RDF triples / knowledge-graph triples",
            "representation_properties": "Compact linear string representation of graph; interpretable mapping between triple slots and placeholders; reduces surface sparsity via delexicalisation; may lose some entity-specific lexical signals; depends on exact-match delexicalisation (incomplete replacement possible).",
            "evaluation_task": "Data-to-text generation (microplanning) on the WebNLG dataset (seen/unseen categories split).",
            "performance_metrics": "Global: BLEU 33.24, TER 0.61, METEOR 0.23 (reported in paper Table 5). Seen categories: BLEU 52.39, TER 0.44, METEOR 0.37 (Table 6). Unseen categories: BLEU 6.13, TER 0.80, METEOR 0.07 (Table 7).",
            "comparison_to_other_representations": "Outperformed by several participant systems (both NMT and SMT) on global metrics; performs competitively on seen data (middle ranks) but fails on unseen categories where delexicalisation and exact matching were insufficient.",
            "limitations_or_challenges": "Relies on exact-match delexicalisation, causing incomplete substitution and errors; linearisation loses explicit graph structure and may obscure relations; poor generalisation to unseen properties/domains; limited morphological handling (target relexicalisation may be brittle).",
            "uuid": "e5364.0",
            "source_info": {
                "paper_title": "The WebNLG Challenge: Generating Text from RDF Data",
                "publication_date_yy_mm": "2017-09"
            }
        },
        {
            "name_short": "UTILBURG-SMT",
            "name_full": "UTilburg statistical machine translation (Moses) over linearised delexicalised triples",
            "brief_description": "An SMT approach that treats linearised, delexicalised RDF triple sets as 'source' sentences and delexicalised verbalizations as 'target' sentences; trained with MGIZA alignments and Moses phrase-based SMT.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "delexicalised linearisation (SMT)",
            "representation_description": "Input triples are linearised and delexicalised (entities replaced with placeholders and annotated with Wikipedia IDs). Training uses MGIZA to obtain alignments and Moses to learn phrase translation and language models; a 6-gram KenLM language model trained on Gigaword is used for final ranking.",
            "graph_type": "RDF triples / knowledge-graph triples",
            "representation_properties": "String serialisation amenable to SMT alignment; delexicalisation reduces data sparsity; allows use of strong n-gram LM for surface fluency; preserves some ordering via linearisation but discards explicit graph topology.",
            "evaluation_task": "Data-to-text generation (WebNLG microplanning task), evaluated on seen/unseen splits.",
            "performance_metrics": "Global: BLEU 44.28, TER 0.53, METEOR 0.38 (Table 5). Seen categories: BLEU 54.29, TER 0.47, METEOR 0.42 (Table 6). Unseen categories: BLEU 29.88, TER 0.61, METEOR 0.33 (Table 7).",
            "comparison_to_other_representations": "One of the top-performing systems globally (BLEU second place); outperforms many NMT and pipeline systems on seen data and overall; however UPF-FORGE outperforms it on certain metrics (e.g., METEOR global ranking close).",
            "limitations_or_challenges": "Depends on good delexicalisation and alignment; linearisation hides graph structure; adaptation to unseen properties limited (lower scores on unseen categories than best pipeline UPF-FORGE).",
            "uuid": "e5364.1",
            "source_info": {
                "paper_title": "The WebNLG Challenge: Generating Text from RDF Data",
                "publication_date_yy_mm": "2017-09"
            }
        },
        {
            "name_short": "ADAPTCentre-NMT",
            "name_full": "ADAPTCentre NMT with subword linearisation (Nematus)",
            "brief_description": "A neural encoder-decoder system (Nematus) that linearises RDF triples and uses subword (BPE-like) representations instead of delexicalisation, inserting tuple-separation special tokens to mark triple boundaries.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "linearisation with tuple separation + subword encoding (NMT)",
            "representation_description": "Triples are linearised into a sequence where tuple separation tokens mark boundaries between triples/tuples. Instead of delexicalisation, the system applies subword segmentation to handle rare words. The linearised subword token sequence is used as input to an attention-based encoder-decoder (Nematus).",
            "graph_type": "RDF triples / knowledge-graph triples",
            "representation_properties": "Preserves triple segmentation via separator tokens; subword encoding mitigates rare-word sparsity and reduces need for delexicalisation; still flattens graph structure into sequence, potentially losing explicit relation topology.",
            "evaluation_task": "Data-to-text (WebNLG), seen/unseen categories.",
            "performance_metrics": "Global: BLEU 31.06, TER 0.84, METEOR 0.31 (Table 5). Seen categories: ranked 1 for BLEU on seen test (ADAPT BLEU unspecified in Table 6 top line indicates ADAPT best on seen), Seen TER 0.37, METEOR 0.44 (Table 6 entries). Unseen: BLEU 10.53, TER 1.4, METEOR 0.19 (Table 7).",
            "comparison_to_other_representations": "On seen categories ADAPTCentre ranked well (benefit from subword modeling), but overall global scores lower than top NMT/SMT competitors; performs poorly on unseen categories, suggesting subword handling does not fully address unseen property lexicalisation.",
            "limitations_or_challenges": "Sequence linearisation still removes explicit graph connectivity; subword approach helps rare lexical items but does not guarantee correct lexicalisation of unseen properties; poor generalisation to unseen domains observed.",
            "uuid": "e5364.2",
            "source_info": {
                "paper_title": "The WebNLG Challenge: Generating Text from RDF Data",
                "publication_date_yy_mm": "2017-09"
            }
        },
        {
            "name_short": "UMelbourne-NMT",
            "name_full": "UMelbourne delexicalisation + type-enriched linearisation (NMT)",
            "brief_description": "Neural encoder-decoder model where entities are delexicalised to ENTITY-ID placeholders, optionally enriched with DBPedia types appended to entity tokens; n-gram search ensures accurate delexicalisation mapping.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "delexicalised linearisation with entity-type enrichment (NMT)",
            "representation_description": "Triples are linearised and entities are delexicalised using ENTITY-ID placeholders; when available, DBPedia types are appended to entity placeholders to provide type signals (e.g., ENTITY-ID|Astronaut). An n-gram match procedure aligns textual mentions in targets with input entities. This sequence is given to a standard attention-based encoder-decoder model.",
            "graph_type": "RDF triples / knowledge-graph triples",
            "representation_properties": "Combines delexicalisation (reducing sparsity) with type information (more semantic signal than bare placeholders); retains triple order via linearisation; still flattens graph structure.",
            "evaluation_task": "WebNLG data-to-text generation.",
            "performance_metrics": "Global: Melbourne (UMelbourne) BLEU 45.13, TER 0.47, METEOR 0.37 (Table 5). Seen: BLEU 54.52, TER 0.40, METEOR 0.41 (Table 6). Unseen: BLEU 33.27, TER 0.55, METEOR 0.33 (Table 7).",
            "comparison_to_other_representations": "Top global performer (BLEU) in the challenge; outperforms baseline and many other NMT systems, indicating that delexicalisation plus type signals are effective. On unseen data it ranks high but UPF-FORGE still outperforms certain metrics on unseen.",
            "limitations_or_challenges": "Requires accurate delexicalisation (n-gram search); type annotations must be available; linearisation still discards explicit graph topology; potential mismatch when entity mentions in surface text do not exactly align with input triplets.",
            "uuid": "e5364.3",
            "source_info": {
                "paper_title": "The WebNLG Challenge: Generating Text from RDF Data",
                "publication_date_yy_mm": "2017-09"
            }
        },
        {
            "name_short": "UTilburg-Pipeline",
            "name_full": "UTILBURG pipeline: extracted templates+ordering+6-gram LM",
            "brief_description": "A pipeline system that extracts delexicalised templates/rules mapping triples or triple sets to delexicalised text, orders triples for discourse, uses a referring expression module to fill missing entities, and ranks outputs with a 6-gram language model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "template/rule extraction with triple ordering (pipeline)",
            "representation_description": "Rules/templates are mined from training data mapping a (delexicalised) triple or triple-set to observed delexicalised text. At generation time, triples are ordered (to maintain discourse order), matching templates are applied to produce delexicalised text, referring expressions inserted, and final outputs ranked using a 6-gram LM trained on Gigaword.",
            "graph_type": "RDF triples / knowledge-graph triples",
            "representation_properties": "Preserves explicit mapping from triples to sentence fragments; interpretable and adaptable; can capture aggregation and sentence segmentation patterns seen in training; may be brittle for unseen property combinations and requires template coverage.",
            "evaluation_task": "WebNLG data-to-text generation.",
            "performance_metrics": "Global: UTILBURG-PIPELINE BLEU 35.29, TER 0.56, METEOR 0.30 (Table 5). Seen/Unseen splits: Seen BLEU 44.34 (Table 6), Unseen BLEU 20.65 (Table 7).",
            "comparison_to_other_representations": "Pipeline systems generally underperform ML-based systems on seen categories, but template approaches can adapt to unseen properties if templates/grammars are definable; overall lower global scores than top SMT/NMT but competitive when high-quality templates exist.",
            "limitations_or_challenges": "Requires extraction/coverage of templates from training data; less robust to unseen properties/patterns; delexicalisation dependency; pipeline complexity (ordering, REG) adds engineering overhead.",
            "uuid": "e5364.4",
            "source_info": {
                "paper_title": "The WebNLG Challenge: Generating Text from RDF Data",
                "publication_date_yy_mm": "2017-09"
            }
        },
        {
            "name_short": "UIT-VNU-HCM",
            "name_full": "UIT-VNU-HCM typed-dependency template extraction",
            "brief_description": "A pipeline approach that extracts generation rules using typed-dependency structures from training texts (no delexicalisation), and uses WordNet to estimate predicate similarity at run time for test-train generalisation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "typed-dependency-based templates",
            "representation_description": "Instead of extracting surface templates, the system uses parsed typed-dependency structures of training texts to derive template rules. At generation time, for unseen predicates, WordNet-based similarity is used to map test predicates to similar training predicates to select templates. No delexicalisation is performed during rule extraction.",
            "graph_type": "RDF triples / knowledge-graph triples (paired with dependency templates)",
            "representation_properties": "Leverages syntactic dependency patterns to capture structural realization choices; can generalise across predicates via lexical similarity (WordNet); keeps more syntactic information than flat templates.",
            "evaluation_task": "WebNLG data-to-text generation.",
            "performance_metrics": "Global: BLEU 7.07, TER 0.82, METEOR 0.09 (Table 5). Seen: BLEU 19.87, TER 0.78, METEOR 0.15 (Table 6). Unseen: BLEU 0.11, TER 0.87, METEOR 0.03 (Table 7).",
            "comparison_to_other_representations": "Ranks low compared to other pipeline and ML systems, indicating difficulties scaling template extraction from dependency structures and generalising to unseen categories despite WordNet similarity heuristics.",
            "limitations_or_challenges": "No delexicalisation causes heavy sparsity; dependency-based templates may not match diverse lexicalisations; WordNet-based mapping insufficient to cover unseen property lexicalisation variability.",
            "uuid": "e5364.5",
            "source_info": {
                "paper_title": "The WebNLG Challenge: Generating Text from RDF Data",
                "publication_date_yy_mm": "2017-09"
            }
        },
        {
            "name_short": "UPF-FORGe",
            "name_full": "UPF-FORGe: PredArg templates -&gt; PredArg graph -&gt; FORGe generator",
            "brief_description": "A grammar-based sentence planning system that maps DBpedia properties to manually defined predicate-argument (PredArg) templates, builds PredArg graph structures for triple sets, and uses the FORGe generator for surface realisation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "PredArg template conversion to PredArg graph + FORGe generation",
            "representation_description": "For each DBpedia property, human-authored PredArg templates encode linguistic and DBpedia-specific features; given a triple set, PredArg templates produce PredArg structures which are aggregated into a PredArg graph capturing inter-triple relations and aggregation opportunities; the FORGe grammar-based generator consumes this PredArg graph to produce fluent text.",
            "graph_type": "RDF triples -&gt; PredArg graph (linguistic graph representation)",
            "representation_properties": "Retains explicit structured linguistic representation (PredArg graph) enabling aggregation, sentence segmentation, and controlled lexicalisation; high interpretability and direct mapping between data properties and linguistic realisation; requires manual template authoring for properties.",
            "evaluation_task": "WebNLG data-to-text generation, including generalisation to unseen categories.",
            "performance_metrics": "Global: BLEU 38.65, TER 0.55, METEOR 0.39 (Table 5). Seen: BLEU 40.88, TER 0.55, METEOR 0.40 (Table 6). Unseen: BLEU 35.70, TER 0.55, METEOR 0.37 (Table 7). UPF-FORGe ranked first on METEOR globally and first on unseen categories.",
            "comparison_to_other_representations": "UPF-FORGe outperformed many ML systems on unseen categories (BLEU and METEOR), showing better adaptability to unseen properties due to manual PredArg templates; ranked top in METEOR globally despite lower BLEU than the very best SMT/NMT on seen data.",
            "limitations_or_challenges": "Requires manual effort to define PredArg templates for properties (scalability issue across large property sets); may be less flexible to diverse lexicalisations not covered by templates; development cost higher than data-driven approaches.",
            "uuid": "e5364.6",
            "source_info": {
                "paper_title": "The WebNLG Challenge: Generating Text from RDF Data",
                "publication_date_yy_mm": "2017-09"
            }
        },
        {
            "name_short": "PKUWriter-Ensemble-Ranker-RL",
            "name_full": "PKUWRITER: ensemble of attention-based encoder-decoder models + supervised ranker + reinforcement learning objective",
            "brief_description": "A hybrid approach combining an ensemble of seq2seq generation models, a supervised ranker trained on BLEU-scored outputs to select best verbalisation, and an additional RL objective encouraging generation to include input subjects.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "linearised delexicalised input + ensemble + output ranking + RL objective",
            "representation_description": "Input triple sets are linearised (and delexicalised) and fed to multiple attention-based encoder-decoder models (ensemble). Each model produces candidate verbalizations; a supervised ranker (trained on features and BLEU scores computed against references) selects the best output. The generation models are additionally trained with a reinforcement learning objective that rewards inclusion of input subjects in outputs. Hand-crafted rules post-process cases where models fail.",
            "graph_type": "RDF triples / knowledge-graph triples",
            "representation_properties": "Combines data-driven generation with discriminative reranking to improve output selection; RL component injects task-specific desiderata (inclusion of subjects); ensemble increases robustness to individual model errors.",
            "evaluation_task": "WebNLG data-to-text generation.",
            "performance_metrics": "Global: BLEU 39.88, TER 0.55, METEOR 0.31 (Table 5). Seen: BLEU 51.23, TER 0.45, METEOR 0.37 (Table 6). Unseen: BLEU 25.36, TER 0.67, METEOR 0.24 (Table 7).",
            "comparison_to_other_representations": "Ranks among the top systems globally; outperforming baseline and many single-model NMTs. The ranker and RL objective improved selection of higher-BLEU outputs relative to single-model decoders.",
            "limitations_or_challenges": "Ensemble and ranker increase system complexity and training data partitioning requirements; RL objective focused on subject inclusion may not capture other quality aspects; still reliant on linearisation and delexicalisation that can impede unseen property handling.",
            "uuid": "e5364.7",
            "source_info": {
                "paper_title": "The WebNLG Challenge: Generating Text from RDF Data",
                "publication_date_yy_mm": "2017-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "FORGe at SemEval-2017 task 9: Deep sentence generation based on a sequence of graph transducers",
            "rating": 2
        },
        {
            "paper_title": "Nematus: A toolkit for neural machine translation",
            "rating": 2
        },
        {
            "paper_title": "Moses: Open source toolkit for statistical machine translation",
            "rating": 2
        },
        {
            "paper_title": "OpenNMT: Open-Source Toolkit for Neural Machine Translation",
            "rating": 2
        },
        {
            "paper_title": "Neural machine translation by jointly learning to align and translate",
            "rating": 1
        },
        {
            "paper_title": "Creating training corpora for nlg micro-planners",
            "rating": 2
        },
        {
            "paper_title": "Split and rephrase",
            "rating": 2
        }
    ],
    "cost": 0.0140665,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The WebNLG Challenge: Generating Text from RDF Data</h1>
<p>Claire Gardent Anastasia Shimorina<br>CNRS, LORIA, UMR 7503<br>Vandoeuvre-lès-Nancy, F-54500, France<br>claire.gardent@loria.fr<br>anastasia.shimorina@loria.fr</p>
<p>Shashi Narayan Laura Perez-Beltrachini School of Informatics, University of Edinburgh 10 Crichton Street, Edinburgh, EH8 9AB, UK shashi.narayan@ed.ac.uk lperez@ed.ac.uk</p>
<h4>Abstract</h4>
<p>The WebNLG challenge consists in mapping sets of RDF triples to text. It provides a common benchmark on which to train, evaluate and compare "microplanners", i.e. generation systems that verbalise a given content by making a range of complex interacting choices including referring expression generation, aggregation, lexicalisation, surface realisation and sentence segmentation. In this paper, we introduce the microplanning task, describe data preparation, introduce our evaluation methodology, analyse participant results and provide a brief description of the participating systems.</p>
<h2>1 Introduction</h2>
<p>Previous Natural Language Generation (NLG) challenges have focused on surface realisation (Banik et al., 2013; Belz et al., 2011), referring expression generation (Belz and Gatt, 2007; Gatt et al., 2008; Gatt et al., 2009; Belz et al., 2008; Belz et al., 2009; Belz et al., 2010) and content selection (BouayadAgha et al., 2013).</p>
<p>In contrast, the WebNLG challenge focuses on microplanning, that subtask of NLG which consists in mapping a given content to a text verbalising this content. Microplanning is a complex choice problem involving several subtasks referred to in the literature as referring expression generation, aggregation, lexicalisation, surface realisation and sentence segmentation. For instance, given the WebNLG data unit shown in (1a), generating the text in (1b) involves choosing to lexicalise the JOHN.E.BLAHA
entity only once (referring expression generation), lexicalising the occupation property as the phrase worked as (lexicalisation), using PP coordination to avoid repeating the word born (aggregation) and verbalising the three triples by a single complex sentence including an apposition, a PP coordination and a transitive verb construction (sentence segmentation and surface realisation).
(1) a. Data: (John.E.BlaHa birthDate 1942_08_26) (John.E.BlaHa birthPlace San_Antonio) (John.E.BlaHa occupation Fighter_pilot)
b. Text: John E Blaha, born in San Antonio on 1942-08-26, worked as a fighter pilot</p>
<h2>2 Data</h2>
<p>As illustrated by the above example, the WebNLG dataset was designed to exercise the ability of NLG systems to handle the whole range of microplanning operations and their interactions. It was created using a content selection procedure specifically designed to enhance data and text variety (Perez-Beltrachini et al., 2016). In (Gardent et al., 2017), we compared a dataset created using the WebNLG process with existing benchmarks in particular, (Wen et al., 2016)'s dataset (RNNLG) which was produced using a similar process. In what follows, we give various statistics about the WebNLG dataset using the RNNLG dataset as a reference point.</p>
<p>Size. The WebNLG dataset consists of 25,298 (data,text) pairs and 9,674 distinct data units. The data units are sets of RDF triples extracted from DBPedia and the texts are sequences of one or more sentences verbalising these data units.</p>
<p>Lexicalisation. As illustrated by the examples in (2), different properties can induce different lexical forms (a property might be lexicalised as a verb, a relational noun, a preposition or an adjective). Therefore, the larger the number of properties, the more likely the data is to allow for a wider range of lexicalisation patterns.
(2) X title $\mathrm{Y} \Rightarrow X$ served as $Y$</p>
<p>X Nationality $\mathrm{Y} \Rightarrow X$ 's nationality is $Y$
Relational noun
X country $\mathrm{Y} \Rightarrow X$ is in $Y \quad$ Preposition
X nationality USA $\Rightarrow X$ is American Adjective
To promote diverse lexicalisation patterns, we extracted data from 15 DBPedia categories (Astronaut, University, Monument, Building, ComicsCharacter, Food, Airport, SportsTeam, WrittenWork, Athlete, Artist, City, MeanOfTransportation, CelestialBody, Politician) resulting in a set of 373 distinct RDF properties (more than three times the number of properties contained in the RNNLG dataset). The corrected type token ratio $\left(\right.$ CTTR $^{1}$ ) and the number of word types is roughly twice as large in theWebNLG dataset than in RNNLG.</p>
<p>Surface Realisation. To increase syntactic variety, we use a content selection procedure which extracts data units of various shapes. The intuition is that different input shapes may induce distinct linguistic constructions. This is illustrated in Figure 2. Typically, while triples sharing a subject (SIBLING configuration) are likely to induce a VP or a sentence coordination, a CHAIN configuration (where the object of one triple is the subject of the other) will more naturally give rise to object relative clauses or participials.</p>
<p>Another factor impacting syntactic variation is the set of properties (input patterns) cooccuring in a given input. This is illustrated by the examples in (3) where two inputs of the same length ( 3 triples hence 3 properties) result in text with different syntax. That is, a larger number of input patterns is more likely to induce texts with greater syntactic variety. By extracting data units from a large number of distinct domains (DBPedia categories), we seeked to produce a large number of distinct input patterns.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>A was born in E. She worked as a D. A was born in $E$ and worked as a $D$.
(3) a. location-country-startDate
$\Rightarrow$ Passive-Apposition-Active
108 St. Georges Terrace is located in Perth, Australia. Its construction began in 1981.
b. BIRTHPLACE-ALMA MATER-SELECTION
$\Rightarrow$ Passive-VP coordination
William Anders was born in British Hong Kong, graduated from AFIT in 1962, and joined NASA in 1963.</p>
<p>As shown in Table 3, the WebNLG dataset contains twice as many distinct input patterns and ten times more input shapes than the RNNLG dataset. It is also less redundant with a ratio between number of inputs and number of input patterns of 2.34 against 10.31 for RNNLG.</p>
<p>Aggregation, Sentence Segmentation and Referring Expression Generation. Finally, the need for aggregation, sentence segmentation and referring expression generation mainly arise when texts contains more than one sentence. As Table 3 shows, although data units are overall smaller in the WebNLG dataset than in RNNLG, the WebNLG dataset has a higher number of texts containing more than one sentence and contains texts of longer length.</p>
<h2>3 Participating Systems</h2>
<p>The WebNLG challenge received eight submissions from six participating teams: the ADAPT Centre, Ireland (ADAPTCentre), the University of Melbourne, Australia (UMelbourne), Peking University, China (PKUWriter), Tilburg University, The Netherlands (UTilburg), University of Information Technology, VNU-HCM, Vietnam (UIT-VNU-</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">WebNLG</th>
<th style="text-align: right;">RNNLG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Size</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;"># data-text pairs</td>
<td style="text-align: right;">25,298</td>
<td style="text-align: right;">30,842</td>
</tr>
<tr>
<td style="text-align: left;"># distinct inputs</td>
<td style="text-align: right;">9,674</td>
<td style="text-align: right;">22,225</td>
</tr>
<tr>
<td style="text-align: left;">Lexicalisation</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;"># properties</td>
<td style="text-align: right;">373</td>
<td style="text-align: right;">108</td>
</tr>
<tr>
<td style="text-align: left;"># domains</td>
<td style="text-align: right;">15</td>
<td style="text-align: right;">4</td>
</tr>
<tr>
<td style="text-align: left;"># CTTR</td>
<td style="text-align: right;">6.51</td>
<td style="text-align: right;">3.42</td>
</tr>
<tr>
<td style="text-align: left;"># Words (Type)</td>
<td style="text-align: right;">6,547</td>
<td style="text-align: right;">3,524</td>
</tr>
<tr>
<td style="text-align: left;">Syntactic Variety</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;"># input patterns</td>
<td style="text-align: right;">4,129</td>
<td style="text-align: right;">2,155</td>
</tr>
<tr>
<td style="text-align: left;"># input / # input patterns</td>
<td style="text-align: right;">2.34</td>
<td style="text-align: right;">10.31</td>
</tr>
<tr>
<td style="text-align: left;"># input shapes</td>
<td style="text-align: right;">62</td>
<td style="text-align: right;">6</td>
</tr>
<tr>
<td style="text-align: left;">Aggregation, GRE, Segmentation</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;"># input with 1 or 2 triples</td>
<td style="text-align: right;">11,111</td>
<td style="text-align: right;">4,087</td>
</tr>
<tr>
<td style="text-align: left;"># input with 3 or 4 triples</td>
<td style="text-align: right;">8,172</td>
<td style="text-align: right;">6,690</td>
</tr>
<tr>
<td style="text-align: left;"># input with 5 to 7 triples</td>
<td style="text-align: right;">6,015</td>
<td style="text-align: right;">20,065</td>
</tr>
<tr>
<td style="text-align: left;"># text with 1 sentence</td>
<td style="text-align: right;">16,740</td>
<td style="text-align: right;">24,234</td>
</tr>
<tr>
<td style="text-align: left;"># text with 2 sentences</td>
<td style="text-align: right;">6,798</td>
<td style="text-align: right;">5,729</td>
</tr>
<tr>
<td style="text-align: left;"># text with $\geq 3$ sentences</td>
<td style="text-align: right;">1,760</td>
<td style="text-align: right;">879</td>
</tr>
<tr>
<td style="text-align: left;"># words/text (avg/min/max)</td>
<td style="text-align: right;">$22.69 / 4 / 80$</td>
<td style="text-align: right;">$18.37 / 1 / 76$</td>
</tr>
</tbody>
</table>
<p>Table 1: Some Statistics about the WebNLG Dataset</p>
<p>HCM) and Universitat Pompeu Fabra, Barcelona, Spain (UPF-FORGE). Each team submitted outputs from a single system except UTILBURG who submitted outputs from three different systems. As a result, there were nine systems in total: eight participating systems and our baseline (BASELINE) system. These can be grouped into three categories: pipeline systems, statistical machine translation (SMT) and neural machine translation (NMT) systems. Table 3 shows the system categorisations.</p>
<p>Pipeline Systems. Three submissions used a template or grammar-based pipeline framework with some NLG module: UTILBURG-PIPELINE, UIT-VNU-HCM and UPF-FORGE.</p>
<p>The first two systems, UTILBURG-PIPELINE and UIT-VNU-HCM, extracted rules or templates from the training data for surface realisation, whereas the third system, UPF-FORGE, used the FORGe grammar (Mille et al., 2017).</p>
<p>UTILBURG-PIPELINE extracted rules mapping a triple (or a triple set) to a text observed in the training data; both the triple and the associated text were delexicalised. Given a RDF triple set to generate</p>
<table>
<thead>
<tr>
<th style="text-align: center;">System ID</th>
<th style="text-align: center;">Institution</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">PIPELINE Systems</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">UTILBURG-SMT</td>
<td style="text-align: center;">Tilburg University</td>
</tr>
<tr>
<td style="text-align: center;">UIT-VNU-HCM</td>
<td style="text-align: center;">University of Information Technology</td>
</tr>
<tr>
<td style="text-align: center;">UPF-FORGE</td>
<td style="text-align: center;">Universitat Pompeu Fabra</td>
</tr>
<tr>
<td style="text-align: center;">SMT Systems</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">UTILBURG-SMT</td>
<td style="text-align: center;">Tilburg University</td>
</tr>
<tr>
<td style="text-align: center;">NMT Systems</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ADAPTCENTRE</td>
<td style="text-align: center;">ADAPT Centre, Ireland</td>
</tr>
<tr>
<td style="text-align: center;">UMELBOURNE</td>
<td style="text-align: center;">University of Melbourne</td>
</tr>
<tr>
<td style="text-align: center;">UTILBURG-NMT</td>
<td style="text-align: center;">Tilburg University</td>
</tr>
<tr>
<td style="text-align: center;">PKUWRITER</td>
<td style="text-align: center;">Peking University</td>
</tr>
<tr>
<td style="text-align: center;">BASELINE</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 2: Categorisation of participating systems.
from, UTILBURG-PIPELINE first ordered triples to maintain discourse order. Extracted rules were then applied to generate a delexicalised text. Missing entities were added using a referring expression generation module (Castro Ferreira et al., 2016). Finally, a 6 -gram language model trained on the Gigaword corpus was used to rank the system output.</p>
<p>UIT-VNU-HCM did not resort to delexicalisation in their rules. Instead of using the text to extract templates, it used the typed-dependency structure of the text to facilitate rule extraction from the training data. In addition, at run time, WordNet was used to estimate similarity between predicates in the test and train sets.</p>
<p>UPF-FORGE mostly focused on sentence planning with predicate-argument (PredArg) templates. For each of the DBPedia properties found in the training and evaluation data, they manually defined PredArg templates encoding various DBPediaspecific and linguistic features. Given a RDF triple set to generate from, PredArg templates were used to convert these triples to PredArg structures and to further aggregate them to form a PredArg graph structure. The FORGe generator took this linguistic PredArg structure as input and generated a text.</p>
<p>SMT Systems. UTilburg-SMT was the only system which used the statistical machine translation framework. It was trained on the WebNLG dataset using the Moses toolkit (Koehn et al., 2007). The dataset was pre-processed whereby each entity in the input and each corresponding referring expression in the output were delexicalised and annotated with the entity Wikipedia ID. The alignments from the training set were obtained using MGIZA and model weights were tuned using 60batch MIRA with BLEU as the evaluation metric. Similar to UTilburg-Pipeline, the system used a 6-gram language model trained on the Gigaword corpus using KenLM.</p>
<p>NMT Systems. Four systems (ADAPTCentre, UMElbourne, UTilburg-NMT and PKUWRIter) build upon the attention-based encoder-decoder architecture proposed in (Bahdanau et al., 2014). Most of them make use of existing NMT frameworks. There are however important differences among systems with respect to both the concrete architecture and the sequence representations they use.</p>
<p>ADAPTCentre makes use of the Nematus (Sennrich et al., 2017) system. They opt for subword representations rather than delexicalisation to deal with rare words and sparsity. They linearise the input sequence and insert tuple separation special tokens.</p>
<p>UMelbourne does a combined delexicalisation procedure and enrichment of the input sequence. Entities are delexicalised using an entity identifier (ENTITY-ID). When available, the DBPedia type of the entity is appended. An n-gram search is used to assure the most accurate target sequence delexicalisation. They use a standard encoder-decoder with attention model.</p>
<p>UTilburg-NMT is based on the Edinburgh Neural Machine Translation submission for the 2016 machine translation shared task (WMT 2016). The target sequences are the delexicalised texts (cf. UTilburg-PiPEline) and the input sequences are the linearisation of the delexicalised input set of triples. The REG module from their pipeline system is used to post-process the decoder outputs.</p>
<p>The PKUWRITER system relies upon two extra mechanisms, namely a ranking module and an extra Reinforcement Learning (RL) training objective. It uses an ensemble of attention-based encoderdecoder models based on the TensorFlow seq2seq API in addition to the baseline ( 7 models in total). They propose an output ranking module to choose the best verbalisation among those output by the generation models. The ranker is trained on supervised data generated automatically. Input triple sets are paired with verbalisations produced by each of the generation models. Then, each pair is associated with a quality score, i.e. the BLEU score of the verbalisation and the reference. Word and sentence level features are extracted to train the ranker. The generation models and ranker are trained on different data partitions. The RL objective encourages the generation of output texts which include subjects occurring in the input RDF triples. In addition, PKUWRITER uses a set of hand-crafted rules to handle input cases where the model fails.</p>
<h2>4 Evaluation Methodology</h2>
<p>The WebNLG challenge includes both an automatic and a human-based evaluation. Due to time constraints, only the results of the automatic evaluation are presented in this paper. The results of the humanbased evaluation will be provided on the WebNLG website ${ }^{2}$ in October 2017.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>4.1 Automatic Evaluation</h3>
<p>Three automatic metrics were used to evaluate the participating systems:</p>
<ul>
<li>BLEU-4 ${ }^{3}$ (Papineni et al., 2002). BLEU scores were computed using up to three references.</li>
<li>METEOR (v1.5) ${ }^{4}$ (Denkowski and Lavie, 2014);</li>
<li>$\mathrm{TER}^{5}$ (Snover et al., 2006).</li>
</ul>
<p>For statistical significance testing, we followed the bootstrapping algorithm described in (Koehn and Monz, 2006).</p>
<p>To assess the ability of the participating systems to generalise to out of domain data, the test dataset consists of two sets of roughly equal size: a test set containing inputs created for entities belonging to DBpedia categories that were seen in the training data (Astronaut, University, Monument, Building, ComicsCharacter, Food, Airport, SportsTeam, City, and WrittenWork), and a test set containing inputs extracted for entities belonging to 5 unseen categories (Athlete, Artist, MeanOfTransportation, CelestialBody, Politician). We call the first type of data seen categories, the second, unseen categories. Correspondingly, we report results for 3 datasets: the seen category dataset, the unseen category dataset and the total test set made of both the seen and the unseen category datasets.</p>
<p>Table 3 gives more detailed statistics about the number of properties, objects and subject entities occurring in each test set.</p>
<ul>
<li>$|$ Test $\mid$ is the number of distinct properties, subjects and objects in the test set;</li>
<li>$|$ Test $\cap T n D v \mid$ is the number of distinct properties, subjects and objects which are in the test set and were seen in the training or the development set;</li>
<li>$|$ Test $\backslash T n D v \mid$ is the number of distinct properties, subjects and objects which occur in the</li>
</ul>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>test set, but not in the training and development set.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Seen</th>
<th style="text-align: center;">Unseen</th>
<th style="text-align: center;">All</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Prop.</td>
<td style="text-align: center;">$\mid$ Test $\mid$</td>
<td style="text-align: center;">188</td>
<td style="text-align: center;">159</td>
<td style="text-align: center;">300</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mid$ Test $\cap$ TnDv $\mid$</td>
<td style="text-align: center;">188</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">192</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mid$ Test $\backslash T n D v \mid$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">108</td>
<td style="text-align: center;">108</td>
</tr>
<tr>
<td style="text-align: center;">Obj.</td>
<td style="text-align: center;">$\mid$ Test $\mid$</td>
<td style="text-align: center;">1033</td>
<td style="text-align: center;">898</td>
<td style="text-align: center;">1888</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mid$ Test $\cap$ TnDv $\mid$</td>
<td style="text-align: center;">1011</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">1025</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mid$ Test $\backslash T n D v \mid$</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">841</td>
<td style="text-align: center;">863</td>
</tr>
<tr>
<td style="text-align: center;">Subj.</td>
<td style="text-align: center;">$\mid$ Test $\mid$</td>
<td style="text-align: center;">343</td>
<td style="text-align: center;">238</td>
<td style="text-align: center;">575</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mid$ Test $\cap$ TnDv $\mid$</td>
<td style="text-align: center;">342</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">342</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mid$ Test $\backslash T n D v \mid$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">232</td>
<td style="text-align: center;">233</td>
</tr>
</tbody>
</table>
<p>Table 3: Test data statistics on properties, objects and subjects for seen, unseen and all datasets.</p>
<p>While in the seen test data (first column) almost all triple elements are present in the training and development sets, in the unseen test data (second column) the vast majority of subjects, objects, and, more importantly, properties (which need to be lexicalised) has not been seen in the training and development data.</p>
<p>Participants were requested to submit tokenised and lowercased texts. To ensure consistency between submissions, we pre-processed the submitted results one more time to double-check that those requirements were fullfilled. As teams used different strategies of tokenisation, we had to modify submissions using our own scripts. In particular, all punctuation signs were separated from alphanumeric sequences (e.g. a two-token group 65.6 feet was modified to a four-token 65.6 feet). Moreover, we converted both references and submission outputs to the ASCII character set.</p>
<h3>4.2 Baseline System</h3>
<p>We developed a baseline system using neural networks and delexicalisation. Before training, we preprocess the data by linearising triples, performing tokenisation and delexicalisation using exact matching.</p>
<p>While delexicalising, we make the following replacements:</p>
<ul>
<li>
<p>given a triple of the form ( $s p o$ ) where $s$ is of the category $C$ for which the triple set has been produced (e.g., Alan_Bean for the category Astronaut), we replace $s$ by $C$.</p>
</li>
<li>
<p>given a triple of the form $s p o$, we replace $o$ by p. E.g., (s country Indonesia) becomes (s country COUNTRY). The replacements were made using the exact match and as a result not all the entities were replaced.</p>
</li>
</ul>
<p>Examples 4 and 5 show a (data,text) pair before and after delexicalisation. Note that noodles was not substituted by the corresponding entity category in the target text (because there is no exact match with the Noodle object in the input). Table 4 shows the number of distinct tokens occurring in the original and delexicalised data.
(4) a. Set of triples: (INDONESIA LEADERNAME Jusuf.KALLA) (BAKSO INGREDIENT NOODLE) (BAKSO COUNTRY INDONESIA)
b. Text: Bakso is a food containing noodles; it is found in Indonesia where Jusuf Kalla is the leader.
(5) a. Source: (COUNTRY LEADERNAME LEADERNAME) (FOOD INGREDIENT INGREDIENT) (FOOD COUNTRY COUNTRY)
b. Target: FOOD is a food containing noodles; it is found in COUNTRY where LEADERNAME is the leader.</p>
<p>On this delexicalised data-to-text corpus, we trained a vanilla sequence-to-sequence model with attention mechanism using the OpenNMT toolkit (Klein et al., 2017) with default parameters for training and decoding. The network consists of a twolayered bidirectional encoder-decoder model with LSTM units. We use a batch size of 64 and a starting learning rate of 1.0. The size of the hidden layers is 500. The network was trained for 13 epochs with a stochastic gradient descent optimisation method and a dropout probability of 0.3 . We used the entire vocabulary for the baseline due to its rather small size.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Original</th>
<th style="text-align: center;">Delexicalised</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Source</td>
<td style="text-align: center;">2703</td>
<td style="text-align: center;">1300</td>
</tr>
<tr>
<td style="text-align: left;">Target</td>
<td style="text-align: center;">5374</td>
<td style="text-align: center;">5013</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: center;">8077</td>
<td style="text-align: center;">6313</td>
</tr>
</tbody>
</table>
<p>Table 4: Vocabulary size in tokens.
After training we relexicalised sentences with corresponding entities if of course their counterparts are
present in generated output. The performance of the baseline is shown in Tables 5, 6, 7 along with other teams' results.</p>
<h2>5 Results</h2>
<p>We briefly discuss the automatic scores distinguishing between results on the whole dataset, on data extracted from previously unseen categories and on data extracted from seen categories.</p>
<p>Global Scores. Table 5 shows the global results that is, results on the whole test set. Horizontal lines group together systems for which the difference in scores is not statistically significant. The names of the teams are coloured according to system type: neural-based systems are in red, pipeline systems in blue, and SMT systems in light grey.</p>
<p>Most systems (6 out of 8 ) outperform the baseline, four of them obtaining scores well above it. In terms of BLEU and TER scores, the first four systems include systems of each type (neural, SMT-based and pipelines).</p>
<p>While BLEU and METEOR yield almost identical rankings, METEOR does not, suggesting that the systems handle synonyms and morphological variation differently. In particular, the fact that UPFFORGE ranks first under the METEOR score suggests that it often generates text that differs from the references because of synonymic or morphological variation.</p>
<p>Scores on Seen Categories. For data extracted from DBPedia categories that were seen in the training data, machine learning based systems (neural and SMT) mostly outperform rule-based systems. In particular, in terms of BLEU and TER scores, the three pipeline systems are at the low end of the ranking. Again though, the METEOR scores show a much higher ranking (3rd rather than 6th) for the UPF-FORGE systems.</p>
<p>Scores on Unseen Categories. On unseen categories, the UPF-FORGE systems ranks first as the system could quickly be adapted to handle properties that had not been seen in the training data. The ranking of the other systems is more or less unchanged with the exception of the ADAPTCENTRE system. This neural system does not use delexicalisation and the subword approach that was adopted</p>
<table>
<thead>
<tr>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Melbourne 45.13</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Tilb-SMT 44.28</td>
</tr>
<tr>
<td style="text-align: center;">$3-4$</td>
<td style="text-align: center;">PKUWritER 39.88</td>
</tr>
<tr>
<td style="text-align: center;">$3-4$</td>
<td style="text-align: center;">UPF-FORGE 38.65</td>
</tr>
<tr>
<td style="text-align: center;">$5-6$</td>
<td style="text-align: center;">Tilb-PiPELine 35.29</td>
</tr>
<tr>
<td style="text-align: center;">$5-6$</td>
<td style="text-align: center;">Tilb-NMT 34.60</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">BASELINE 33.24</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">ADAPT 31.06</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">UIT-VNU 7.07</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">TER</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Melbourne 0.47</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Tilb-SMT 0.53</td>
</tr>
<tr>
<td style="text-align: center;">$3-4$</td>
<td style="text-align: center;">PKUWritER 0.55</td>
</tr>
<tr>
<td style="text-align: center;">$3-5$</td>
<td style="text-align: center;">UPF-FORGE 0.55</td>
</tr>
<tr>
<td style="text-align: center;">$4-5$</td>
<td style="text-align: center;">Tilb-PiPELine 0.56</td>
</tr>
<tr>
<td style="text-align: center;">$6-7$</td>
<td style="text-align: center;">Tilb-NMT 0.60</td>
</tr>
<tr>
<td style="text-align: center;">$6-7$</td>
<td style="text-align: center;">BASELINE 0.61</td>
</tr>
<tr>
<td style="text-align: center;">$8-9$</td>
<td style="text-align: center;">UIT-VNU 0.82</td>
</tr>
<tr>
<td style="text-align: center;">$8-9$</td>
<td style="text-align: center;">ADAPT 0.84</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">METEOR</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">UPF-FORGE 0.39</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Tilb-SMT 0.38</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Melbourne 0.37</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Tilb-NMT 0.34</td>
</tr>
<tr>
<td style="text-align: center;">$5-6$</td>
<td style="text-align: center;">ADAPT 0.31</td>
</tr>
<tr>
<td style="text-align: center;">$5-7$</td>
<td style="text-align: center;">PKUWritER 0.31</td>
</tr>
<tr>
<td style="text-align: center;">$6-7$</td>
<td style="text-align: center;">Tilb-PiPELine 0.30</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">BASELINE 0.23</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">UIT-VNU 0.09</td>
</tr>
</tbody>
</table>
<p>Table 5: Results for all categories. Lines between systems indicate a difference in scores which is statistically significant ( $p&lt;$ 0.05 ). A colour for a team name indicates a type of the system used (NMT, SMT, Pipeline).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">ADAPT</td>
</tr>
<tr>
<td style="text-align: center;">$2-3$</td>
<td style="text-align: center;">Melbourne 54.52</td>
</tr>
<tr>
<td style="text-align: center;">$2-4$</td>
<td style="text-align: center;">Tilb-SMT 54.29</td>
</tr>
<tr>
<td style="text-align: center;">$3-4$</td>
<td style="text-align: center;">BASELINE 52.39</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">PKUWritER 51.23</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Tilb-PiPELine 44.34</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">Tilb-NMT 43.28</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">UPF-FORGE 40.88</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">UIT-VNU 19.87</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">TER</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">ADAPT 0.37</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Melbourne 0.40</td>
</tr>
<tr>
<td style="text-align: center;">$3-4$</td>
<td style="text-align: center;">BASELINE 0.44</td>
</tr>
<tr>
<td style="text-align: center;">$3-4$</td>
<td style="text-align: center;">PKUWritER 0.45</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Tilb-SMT 0.47</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Tilb-PiPELine 0.48</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">Tilb-NMT 0.51</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">UPF-FORGE 0.55</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">UIT-VNU 0.78</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">METEOR</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">ADAPT 0.44</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Tilb-SMT 0.42</td>
</tr>
<tr>
<td style="text-align: center;">$3-4$</td>
<td style="text-align: center;">Melbourne 0.41</td>
</tr>
<tr>
<td style="text-align: center;">$3-4$</td>
<td style="text-align: center;">UPF-FORGE 0.40</td>
</tr>
<tr>
<td style="text-align: center;">$5-6$</td>
<td style="text-align: center;">Tilb-NMT 0.38</td>
</tr>
<tr>
<td style="text-align: center;">$5-8$</td>
<td style="text-align: center;">Tilb-PiPELine 0.38</td>
</tr>
<tr>
<td style="text-align: center;">$6-8$</td>
<td style="text-align: center;">PKUWritER 0.37</td>
</tr>
<tr>
<td style="text-align: center;">$6-8$</td>
<td style="text-align: center;">BASELINE 0.37</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">UIT-VNU 0.15</td>
</tr>
</tbody>
</table>
<p>Table 6: Results for seen categories.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">UPF-FORGE 35.70</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Melbourne 33.27</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Tilb-SMT 29.88</td>
</tr>
<tr>
<td style="text-align: center;">$4-5$</td>
<td style="text-align: center;">PKUWritER 25.36</td>
</tr>
<tr>
<td style="text-align: center;">$4-5$</td>
<td style="text-align: center;">Tilb-NMT 25.12</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Tilb-PiPELine 20.65</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">ADAPT 10.53</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">BASELINE 06.13</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">UIT-VNU 0.11</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">TER</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">UPF-FORGE 0.55</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Melbourne 0.55</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Tilb-SMT 0.61</td>
</tr>
<tr>
<td style="text-align: center;">$4-5$</td>
<td style="text-align: center;">Tilb-PiPELine 0.65</td>
</tr>
<tr>
<td style="text-align: center;">$4-5$</td>
<td style="text-align: center;">PKUWritER 0.67</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Tilb-NMT 0.72</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">BASELINE 0.80</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">UIT-VNU 0.87</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">ADAPT 1.4</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">METEOR</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">UPF-FORGE 0.37</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Tilb-SMT 0.33</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Melbourne 0.33</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Tilb-NMT 0.31</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">PKUWritER 0.24</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Tilb-PiPELine 0.21</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">ADAPT 0.19</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">BASELINE 0.07</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">UIT-VNU 0.03</td>
</tr>
</tbody>
</table>
<p>Table 7: Results for unseen categories.</p>
<p>S John Clancy is a labour politican who leads Birmingham, where architect John Madin, who designed 103 Colmore Row, was born.
$\mathbf{M}<em>{S} \quad{$ Birmingham|LeaderName|John_Clancy</em>(Labour_politician),
John_Madin|birthPlace|Birmingham,
103_Colmore_Row|architect|John_Madin $}$
$\mathbf{T}<em T__1="T_{1">{1} \quad$ Labour politician, John Clancy is the leader of Birmingham.
$\mathbf{M}</em> \quad{$ Birmingham|LeaderName|John_Clancy_(Labour_politician) $}$
$\mathbf{T}}<em T__2="T_{2">{2} \quad$ John Madin was born in Birmingham.
$\mathbf{M}</em> \quad{$ John_Madin|birthPlace|Birmingham $}$
$\mathbf{T}}<em T__3="T_{3">{3} \quad$ He was the architect of 103 Colmore Row.
$\mathbf{M}</em> \quad{$ 103_Colmore_Row|architect|John_Madin $}$
Figure 1: An example pair out of the Split-and-Rephrase Dataset. $\mathbf{S}$ is a single complex sentence with meaning $\mathbf{M}}<em 1="1">{S} . \mathbf{T}</em>}, \mathbf{T<em 1="1">{1}, \mathbf{T}</em>}$ form a text of three simple sentences whose joint meaning $\mathbf{M<em 1="1">{T</em>}} \cup \mathbf{M<em 2="2">{T</em>}} \cup \mathbf{M<em 3="3">{T</em>$.}}$ is the same as the meaning $\mathbf{M}_{S}$ of the corresponding single complex sentence $\mathbf{S</p>
<p>to handle unseen data does not seem to work well.</p>
<h2>6 Conclusion</h2>
<p>The WebNLG challenge was novel in that it was the first challenge to provide a benchmark on which to evaluate and compare microplanners. Despite a tight schedule (we released the training data in April for a submission in August), it generated a high level of interest among the NLG community: 62 groups from 18 countries ${ }^{6}$ downloaded the data, 6 groups submitted 8 systems and 3 groups developped a system but did not submit.</p>
<p>The training data for the WebNLG 2017 challenge is available on the WebNLG website ${ }^{7}$ and evaluation on the test data can be run by the organisers on demand. A larger dataset consisting of 40,049 (data, text) pairs, 15,095 distinct data input and 15 DBpedia categories is also available. Both datasets are under the creative common licence "CC Attribution-Noncommercial-Share Alike 4.0 International license". We hope that these resources will enable a long and fruitful strand of research on microplanning.</p>
<p>The usefulness of the WebNLG dataset reaches far beyond the WebNLG challenge. It can be used for instance to train a semantic parser which would convert a sentence into a set of RDF triples. It can also be used to derive new datasets for related tasks. Thus in (Narayan et al., 2017), we show how to derive from the WebNLG dataset, a dataset for sentence simplification which we call the Split-andRephrase dataset. In this dataset, each pair consists of (i) a single, complex sentence with its meaning representation in terms of RDF triples and (ii) a sequence of at least two sentences and their corresponding sets of RDF triples whereby these sets form a partition on the set of RDF triples associated with the input complex sentence. In other words, the Split-and-Rephrase dataset associates a complex sentence with a sequence of at least two sentences whose meaning is the same as that of the complex sentence. As explained in (Narayan et al., 2017), this dataset was created using the meaning represen-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>tations (sets of RDF triples) as pivot. The Split-and-Rephrase dataset consists of 1,100,166 pairs of the form $\left\langle\left(M_{C}, T_{C}\right),\left{\left(M_{1}, T_{1}\right) \ldots\left(M_{n}, T_{n}\right)\right}\right\rangle$ where $T_{C}$ is a complex sentence and $T_{1} \ldots T_{n}$ is a sequence of texts with semantics $M_{1}, \ldots M_{n}$ expressing the same content $M_{C}$ as $T_{C}$. Figure 1 shows an example pair. It was used to train four neural systems and the associated meaning representations were shown to improve performance.</p>
<p>In the future, we are planning to build a multilingual resource in which the English text present in the WebNLG dataset will be translated into French, Russian and Maltese. In this way, morphological variation can be explored which is an interesting avenue of research in particular for neural systems which have a limited ability to handle unseen input: how well will these systems be able to handle the generation of morphologically rich languages ?</p>
<p>The analysis of the participants results presented in this paper will be complemented in an arxiv report by the results of a human-based evaluation. Using human judgements obtained through crowdsourcing, this human evaluation will assess the system results on three criteria, namely fluency, grammaticality and appropriateness (does the text correctly verbalise the input data?). We will also provide a more in depth analysis of the participant results on data extracted from different categories and data of various length.</p>
<h2>Acknowledgments</h2>
<p>The research presented in this paper has been supported by the following grants and projects: "WebNLG", Project ANR-14-CE24-0033 of the French National Research Agency and "SUMMA", H2020 project No. 688139.</p>
<h2>References</h2>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. In Proceedings of ICLR-2015 (abs/1409.0473).
Eva Banik, Claire Gardent, and Eric Kow. 2013. The kbgen challenge. In the 14th European Workshop on Natural Language Generation (ENLG), pages 94-97.
Anja Belz and Albert Gatt. 2007. The attribute selection for gre challenge: Overview and evaluation results.</p>
<p>Proceedings of UCNLG+ MT: Language Generation and Machine Translation, pages 75-83.
Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt. 2008. The grec challenge: Overview and evaluation results.
Anja Belz, Eric Kow, and Jette Viethen. 2009. The grec named entity generation challenge 2009: overview and evaluation results. In Proceedings of the 2009 Workshop on Language Generation and Summarisation, pages 88-98. Association for Computational Linguistics.
Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt. 2010. Generating referring expressions in context: The grec task evaluation challenges. In Empirical methods in natural language generation, pages 294327. Springer.</p>
<p>Anja Belz, Michael White, Dominic Espinosa, Eric Kow, Deirdre Hogan, and Amanda Stent. 2011. The first surface realisation shared task: Overview and evaluation results. In Proceedings of the 13th European Workshop on Natural Language Generation, ENLG '11, pages 217-226, Stroudsburg, PA, USA. Association for Computational Linguistics.
Nadjet Bouayad-Agha, Gerard Casamayor, Leo Wanner, and Chris Mellish. 2013. Overview of the first content selection challenge from open semantic web data. In ENLG, pages 98-102.
J. B. Carroll. 1964. Language and thought. NJ: PrenticeHall. Englewood Cliffs.
Thiago Castro Ferreira, Emiel Krahmer, and Sander Wubben. 2016. Towards more variation in text generation: Developing and evaluating variation models for choice of referential form. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 568-577.
Michael Denkowski and Alon Lavie. 2014. Meteor universal: Language specific translation evaluation for any target language. In Proceedings of the EACL 2014 Workshop on Statistical Machine Translation.
Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. Creating training corpora for nlg micro-planners. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 179-188.
Albert Gatt, Anja Belz, and Eric Kow. 2008. The tuna challenge 2008: Overview and evaluation results. In Proceedings of the Fifth International Natural Language Generation Conference, pages 198-206. Association for Computational Linguistics.
Albert Gatt, Anja Belz, and Eric Kow. 2009. The tunareg challenge 2009: Overview and evaluation results. In Proceedings of the 12th European Workshop on</p>
<p>Natural Language Generation, pages 174-182. Association for Computational Linguistics.
G. Klein, Y. Kim, Y. Deng, J. Senellart, and A. M. Rush. 2017. OpenNMT: Open-Source Toolkit for Neural Machine Translation. ArXiv e-prints.
Philipp Koehn and Christof Monz. 2006. Manual and automatic evaluation of machine translation between european languages. In Proceedings of the Workshop on Statistical Machine Translation, StatMT '06, pages 102-121.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondřej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 177180.</p>
<p>Xiaofei Lu. 2008. Automatic measurement of syntactic complexity using the revised developmental level scale. In FLAIRS Conference, pages 153-158.
Simon Mille, Roberto Carlini, Alicia Burga, and Leo Wanner. 2017. FORGe at SemEval-2017 task 9: Deep sentence generation based on a sequence of graph transducers. In Proceedings of SemEval-2017, pages 917-920.
Shashi Narayan, Claire Gardent, Shay B. Cohen, and Anastasia Shimorina. 2017. Split and rephrase. In Proceedings of EMNLP.
Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311-318. Association for Computational Linguistics.
Laura Perez-Beltrachini and Claire Gardent. 2017. Analysing data-to-text generation benchmarks. In Proceedings of the tenth International Natural Language Generation Conference, INLG.
Laura Perez-Beltrachini, Rania Sayed, and Claire Gardent. 2016. Building rdf content for data-to-text generation. In COLING, pages 1493-1502.
Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexandra Birch-Mayne, Barry Haddow, Julian Hitschler, Marcin Junczys-Dowmunt, Samuel Laubli, Antonio Miceli Barone, Jozef Mokry, and Maria Nadejde. 2017. Nematus: A toolkit for neural machine translation. In Proceedings of the EACL 2017 Software Demonstrations, pages 65-68, 4.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation.</p>
<p>In Proceedings of association for machine translation in the Americas, volume 200.
Tsung-Hsien Wen, Milica Gašić, Nikola Mrkšić, Lina M. Rojas-Barahona, Pei-Hao Su, David Vandyke, and Steve Young. 2016. Multi-domain neural network language generation for spoken dialogue systems. In Proceedings of NAACL-HLT.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ Australia, Canada, China, Croatie, France, Germany, India, Iran, Ireland, Italy, Netherlands, Norway, Poland, Spain, Tunisia, UK, USA, Vietnam
${ }^{7}$ http://talcl.loria.fr/webnlg/stories/ challenge.html&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
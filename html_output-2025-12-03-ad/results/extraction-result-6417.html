<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6417 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6417</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6417</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-276937782</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.09512v1.pdf" target="_blank">Reinforcement Learning is all You Need</a></p>
                <p><strong>Paper Abstract:</strong> Inspired by the success of DeepSeek R1 in reasoning via reinforcement learning without human feedback, we train a 3B language model using the Countdown Game with pure reinforcement learning. Our model outperforms baselines on four of five benchmarks, demonstrating improved generalization beyond its training data. Notably, response length does not correlate with reasoning quality, and while"aha moments"emerge, they do not always yield correct answers. These findings highlight the potential of RL-only training for reasoning enhancement and suggest future work on refining reward structures to bridge emergent insights with accuracy.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6417.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6417.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2.5-3B-RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2.5-3B-Instruct fine-tuned with GRPO on Countdown Game</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3B instruction-tuned decoder-only transformer (Qwen2.5-3B-Instruct) further fine-tuned with Group Relative Policy Optimization (GRPO) on the Countdown Game using a rule-based format and answer reward; evaluated on multiple arithmetic and reasoning benchmarks showing improved arithmetic/word-problem performance vs the base checkpoint.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-3B-Instruct (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Fine-tuned on the Countdown Game (target-number puzzles using a drawn set of integers with +, -, *, / and each number used at most once) with a rule-based Format Reward and binary Answer Reward; training used GRPO via the TRL package.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MATH (subset), IFEval, BBH, MMLU-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic word problems, target-number arithmetic (Countdown), algebra/number theory problems from MATH, varied reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language word problems and Countdown prompts; training/evaluation used prompts requiring <think>...</think> chain-of-thought and <answer>...</answer> final-equation formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Grade-school to competition level (GSM8K = grade-school multi-step; MATH subset = competition problems; Countdown = variable target-number puzzles).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot evaluation with the R1 prompt instructing CoT inside <think> tags and final answer in <answer> (also compared to default harness prompt and CoT zero-shot baselines); few-shot evaluated and found to underperform this trained zero-shot setting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy variants reported per benchmark (GSM8K Flexible-Extract and Strict-Match; MATH math_verify; IFEval loose/strict accuracy; MMLU-Pro extract-match/group results).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K (Trained + R1 Prompt): 69.7% Flexible-Extract, 64.8% Strict-Match. Base model vs Trained on MATH (math_verify, zero-shot 5k subset): Base ~13.0%, Trained ~27.44%. IFEval Loose: Trained 64.8% vs Base 66.0%; IFEval Strict: Trained 60.1% vs Base 60.3%. MMLU-Pro group result: Base 16.25% -> Trained 22.42%.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Qualitative analyses only: observed emergence of 'aha moments' (linguistic cues like 'Wait', 'Oops' and internal self-correction attempts), verification-style trial-and-error reasoning, and a transition during training from brute-force enumeration toward shorter, more reasoning-like outputs. No mechanistic probes (e.g., activation/attention/logit-lens) were reported; observations are behavioral and format-based.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Frequent format violations (duplicated or multiple <think>/<answer> blocks), brute-force trial-and-error that is inefficient, arithmetic calculation mistakes, self-corrections that still produce incorrect final answers, occasional omission of explicit intermediate steps despite correct final answer (opaque internal shortcuts), degraded performance with few-shot prompting, evaluation harness mis-evaluating correct-but-unexpected formats.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Only a single model size (3B) reported; RL fine-tuning improved arithmetic performance relative to the base checkpoint on multiple benchmarks. GRPO training is reported to be faster per step but potentially less accurate than PPO (tradeoff of compute vs accuracy); no multi-size scaling curve provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reinforcement Learning is all You Need', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6417.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6417.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2.5-3B-Base</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2.5-3B-Instruct (base checkpoint before RL fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The pre-fine-tuned instruction-tuned Qwen2.5-3B checkpoint used as the baseline; evaluated with chain-of-thought and the R1 prompt to compare arithmetic and word-problem performance before RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-3B-Instruct (base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained and instruction-tuned as Qwen2.5 (details not re-specified in this paper); not trained on Countdown Game prior to RL fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MATH (subset), IFEval, MMLU-Pro, BBH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic word problems, math competition problems, instruction-following</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language word problems; evaluated with CoT prompts and R1 prompt variants.</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Grade-school (GSM8K) to higher (MATH subset).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>CoT zero-shot, and R1 prompt (zero-shot); few-shot experiments reported for comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Same metrics as trained model (Flexible-Extract / Strict-Match on GSM8K, math_verify on MATH, IFEval loose/strict accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K Base + CoT Zero-Shot: 66.3% Flexible-Extract, 29.3% Strict-Match. GSM8K Base + R1 Prompt: 60.9% Flexible-Extract, 55.0% Strict-Match. MATH math_verify (5k subset): ~13.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral errors illustrated by example (mis-parsing problem structure; e.g., incorrectly computing total hours in a word problem). Inconsistent formatting across responses led to evaluation harness undercounting correct answers. No mechanistic interpretability analyses were run.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Incorrect problem decomposition/assumptions (example: miscounting hours by misinterpreting 'half as much'), inconsistent or badly formatted CoT outputs causing automated evaluation failures, arithmetic calculation mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Serves as the comparison baseline; performance improved after RL fine-tuning, indicating that task-specific RL can enhance arithmetic performance for this checkpoint size.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reinforcement Learning is all You Need', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6417.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6417.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GRPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Group Relative Policy Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL policy optimization algorithm that omits an explicit learned value function and computes advantages by normalizing observed rewards within a generated group of responses, with clipping and a KL penalty to a reference policy for stability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GRPO (algorithm)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>reinforcement learning / policy optimization</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>policy optimization for language model fine-tuning with reward signals</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Training speed and downstream accuracy comparisons (versus PPO) and behavioral properties (length-hacking susceptibility).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported (citing prior work) ~60% reduction in per-step training time vs PPO; in this study GRPO used for 850-step fine-tuning; authors report GRPO underperforms PPO in accuracy and is more susceptible to length-hacking.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>GRPO computes advantage A_i = (r_i - mean(r))/std(r) across a group of G sampled responses, avoiding a learned value function; uses clipping and KL penalty to a reference policy. This design reduces compute but may give noisier advantage estimates and encourage reward-gaming behaviors (e.g., longer outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Length-hacking (models produce longer completions to game reward accumulation), potential suboptimal reward estimation due to group-normalized advantages, sensitivity to the sample/group size used in advantage calculation.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Enables faster per-step training making large-scale RL more computationally tractable, but reported trade-offs: speed vs final accuracy (PPO reportedly higher accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reinforcement Learning is all You Need', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6417.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6417.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rule-based Reward</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rule-based reward model (Format Reward + Answer Reward)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple rule-based reward scheme used during RL fine-tuning that enforces response structure with a Format Reward (requires a single <think> tag and an <answer> tag) and a binary Answer Reward (1 if final answer equals target, else 0).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Rule-based Format+Answer Reward</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>rule-based reward model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Operates on model outputs from Countdown Game prompts; uses regex extraction to enforce format and exact numeric match for correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Countdown Game (training), applied to subsequent evaluations indirectly</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>format compliance checking and binary correctness evaluation for arithmetic solutions</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Responses required to include reasoning inside <think>...</think> and final equation/result inside <answer>...</answer> tags.</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Paired with the R1 prompt instructing the model to show work in <think> tags and final answer in <answer> tags.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Binary reward (0/1) for answer correctness and format compliance; also used indirectly to shape downstream task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reward = 1 if <answer> content equals target number and format constraints satisfied; otherwise 0. No graded/partial-credit rewards were used.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Design is simple and enforces structural compliance, which improved format consistency and downstream STRICT-MATCH rates; however, it does not measure partial credit, intermediate-step correctness, or reasoning quality. Authors note that Process Reward Models (PRMs) could better capture partial progress.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Mis-judges correct answers presented in unexpected formats (false negatives), penalizes near-correct reasoning due to lack of partial credit, and may encourage format-focused optimization over genuine reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Scales easily computationally (binary checks), but authors recommend more nuanced reward models (PRMs) when precision of reasoning or partial credit matters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reinforcement Learning is all You Need', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6417.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6417.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>R1 Prompt / <think></td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>R1 prompt template that enforces <think> CoT and <answer> final-output tags</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting template used during evaluation and training that instructs the model to show chain-of-thought inside <think>...</think> tags and place the final equation/answer inside <answer>...</answer>, used both as an evaluation prompt and as an instruction during RL fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>R1 Prompt (prompting method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Applied at evaluation time to GSM8K, MATH and other benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>eliciting chain-of-thought and enforcing structured final answer formatting for arithmetic/word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompt instructing the model to 'show your work in <think> tags' and 'return final equation and answer in <answer> tags'; evaluation harness used variations of this prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>varied</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot chain-of-thought via explicit instruction to write CoT in tags and final answer in tags; compared against default harness prompt and CoT baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Impact on Strict-Match and Flexible-Extract accuracy for GSM8K and other benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Applying R1 Prompt to the Base Model increased GSM8K Strict-Match from 29.3% to 55.0% (Base + R1). Trained Model evaluated with R1 achieved GSM8K 69.7% Flexible-Extract and 64.8% Strict-Match.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Forcing a specific tag-based structure improved format consistency and therefore automated evaluation accuracy; prompt also interacted with RL fine-tuning to produce more consistent final-answer forms. However, some models produced the correct final answer without full visible reasoning in <think> tags.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Models may still violate the enforced format (multiple <think> segments, duplicated <answer> tags), causing evaluation failures; forcing tags can lead models to produce shallow or token-level optimizations to satisfy format without genuine process reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Prompting with explicit structure improves automated-evaluation scores at this model scale; trained models still showed degraded few-shot behavior despite strong zero-shot R1 performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reinforcement Learning is all You Need', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6417.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6417.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Countdown Game (dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Countdown Game (target-number arithmetic puzzle dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset of target-number puzzles where models are given a set of integers and asked to form an equation using +, -, *, / (each number used at most once) to reach a target number; used here for RL-only fine-tuning because correctness is easy to verify programmatically.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Structured numeric puzzles: random draws of integers and a target; designed to encourage numerical reasoning and allow automated correctness checking for reward assignment.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Countdown Game (training dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>target-number arithmetic puzzle solving (combinational arithmetic with +, -, *, /), encourages multi-step arithmetic reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>List of available numbers and a target; model must produce an equation and final numeric result (training prompted to include <think> and <answer> tags).</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>variable; puzzles can be simple or require multi-step composition of operations; used to teach multi-step arithmetic strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>R1-style prompt instructing CoT in <think> tags and final equation in <answer> tags; RL fine-tuning using rule-based rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Binary solved/not-solved reward during RL; also monitored behavioral changes (format compliance, emergence of strategies).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Model adopted required response structure after ~80 training steps; total RL fine-tuning run shown had 850 steps (batch size 2, group size 2 for GRPO). Observed behavioral transition from brute-force enumeration to more concise reasoning over training.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Training on Countdown induced verification-based, trial-and-error behavior early, later partial emergence of 'aha moments' (internal self-corrections). Model initially avoided multiplication/division and later used more structured strategies; no internal activation/attention probes were performed.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Early-stage rule/format violations (multiple <think> tags), inability to initially exploit multiplicative/divisive operations, persistent occasional format errors (duplicated tags) even after training.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Countdown is effective as a training signal for arithmetic strategy emergence at this scale/setting; authors note need for more sophisticated reward structures (PRMs) and experiment length/scale considerations (only 850 steps reported).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reinforcement Learning is all You Need', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Proximal policy optimization algorithms <em>(Rating: 1)</em></li>
                <li>The lessons of developing process reward models in mathematical reasoning <em>(Rating: 2)</em></li>
                <li>Deep reinforcement learning from human preferences <em>(Rating: 1)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6417",
    "paper_id": "paper-276937782",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "Qwen2.5-3B-RL",
            "name_full": "Qwen2.5-3B-Instruct fine-tuned with GRPO on Countdown Game",
            "brief_description": "A 3B instruction-tuned decoder-only transformer (Qwen2.5-3B-Instruct) further fine-tuned with Group Relative Policy Optimization (GRPO) on the Countdown Game using a rule-based format and answer reward; evaluated on multiple arithmetic and reasoning benchmarks showing improved arithmetic/word-problem performance vs the base checkpoint.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-3B-Instruct (fine-tuned)",
            "model_family": "decoder-only transformer",
            "model_size": "3B",
            "training_data_description": "Fine-tuned on the Countdown Game (target-number puzzles using a drawn set of integers with +, -, *, / and each number used at most once) with a rule-based Format Reward and binary Answer Reward; training used GRPO via the TRL package.",
            "benchmark_name": "GSM8K, MATH (subset), IFEval, BBH, MMLU-Pro",
            "task_type": "multi-step arithmetic word problems, target-number arithmetic (Countdown), algebra/number theory problems from MATH, varied reasoning tasks",
            "problem_format": "Natural-language word problems and Countdown prompts; training/evaluation used prompts requiring &lt;think&gt;...&lt;/think&gt; chain-of-thought and &lt;answer&gt;...&lt;/answer&gt; final-equation formatting.",
            "difficulty_level": "Grade-school to competition level (GSM8K = grade-school multi-step; MATH subset = competition problems; Countdown = variable target-number puzzles).",
            "prompting_method": "Zero-shot evaluation with the R1 prompt instructing CoT inside &lt;think&gt; tags and final answer in &lt;answer&gt; (also compared to default harness prompt and CoT zero-shot baselines); few-shot evaluated and found to underperform this trained zero-shot setting.",
            "performance_metric": "Accuracy variants reported per benchmark (GSM8K Flexible-Extract and Strict-Match; MATH math_verify; IFEval loose/strict accuracy; MMLU-Pro extract-match/group results).",
            "performance_value": "GSM8K (Trained + R1 Prompt): 69.7% Flexible-Extract, 64.8% Strict-Match. Base model vs Trained on MATH (math_verify, zero-shot 5k subset): Base ~13.0%, Trained ~27.44%. IFEval Loose: Trained 64.8% vs Base 66.0%; IFEval Strict: Trained 60.1% vs Base 60.3%. MMLU-Pro group result: Base 16.25% -&gt; Trained 22.42%.",
            "internal_analysis": "Qualitative analyses only: observed emergence of 'aha moments' (linguistic cues like 'Wait', 'Oops' and internal self-correction attempts), verification-style trial-and-error reasoning, and a transition during training from brute-force enumeration toward shorter, more reasoning-like outputs. No mechanistic probes (e.g., activation/attention/logit-lens) were reported; observations are behavioral and format-based.",
            "failure_modes": "Frequent format violations (duplicated or multiple &lt;think&gt;/&lt;answer&gt; blocks), brute-force trial-and-error that is inefficient, arithmetic calculation mistakes, self-corrections that still produce incorrect final answers, occasional omission of explicit intermediate steps despite correct final answer (opaque internal shortcuts), degraded performance with few-shot prompting, evaluation harness mis-evaluating correct-but-unexpected formats.",
            "scaling_trend": "Only a single model size (3B) reported; RL fine-tuning improved arithmetic performance relative to the base checkpoint on multiple benchmarks. GRPO training is reported to be faster per step but potentially less accurate than PPO (tradeoff of compute vs accuracy); no multi-size scaling curve provided.",
            "uuid": "e6417.0",
            "source_info": {
                "paper_title": "Reinforcement Learning is all You Need",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Qwen2.5-3B-Base",
            "name_full": "Qwen2.5-3B-Instruct (base checkpoint before RL fine-tuning)",
            "brief_description": "The pre-fine-tuned instruction-tuned Qwen2.5-3B checkpoint used as the baseline; evaluated with chain-of-thought and the R1 prompt to compare arithmetic and word-problem performance before RL.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-3B-Instruct (base)",
            "model_family": "decoder-only transformer",
            "model_size": "3B",
            "training_data_description": "Pretrained and instruction-tuned as Qwen2.5 (details not re-specified in this paper); not trained on Countdown Game prior to RL fine-tuning.",
            "benchmark_name": "GSM8K, MATH (subset), IFEval, MMLU-Pro, BBH",
            "task_type": "multi-step arithmetic word problems, math competition problems, instruction-following",
            "problem_format": "Natural-language word problems; evaluated with CoT prompts and R1 prompt variants.",
            "difficulty_level": "Grade-school (GSM8K) to higher (MATH subset).",
            "prompting_method": "CoT zero-shot, and R1 prompt (zero-shot); few-shot experiments reported for comparisons.",
            "performance_metric": "Same metrics as trained model (Flexible-Extract / Strict-Match on GSM8K, math_verify on MATH, IFEval loose/strict accuracy).",
            "performance_value": "GSM8K Base + CoT Zero-Shot: 66.3% Flexible-Extract, 29.3% Strict-Match. GSM8K Base + R1 Prompt: 60.9% Flexible-Extract, 55.0% Strict-Match. MATH math_verify (5k subset): ~13.0%.",
            "internal_analysis": "Behavioral errors illustrated by example (mis-parsing problem structure; e.g., incorrectly computing total hours in a word problem). Inconsistent formatting across responses led to evaluation harness undercounting correct answers. No mechanistic interpretability analyses were run.",
            "failure_modes": "Incorrect problem decomposition/assumptions (example: miscounting hours by misinterpreting 'half as much'), inconsistent or badly formatted CoT outputs causing automated evaluation failures, arithmetic calculation mistakes.",
            "scaling_trend": "Serves as the comparison baseline; performance improved after RL fine-tuning, indicating that task-specific RL can enhance arithmetic performance for this checkpoint size.",
            "uuid": "e6417.1",
            "source_info": {
                "paper_title": "Reinforcement Learning is all You Need",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "GRPO",
            "name_full": "Group Relative Policy Optimization",
            "brief_description": "An RL policy optimization algorithm that omits an explicit learned value function and computes advantages by normalizing observed rewards within a generated group of responses, with clipping and a KL penalty to a reference policy for stability.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GRPO (algorithm)",
            "model_family": "reinforcement learning / policy optimization",
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": null,
            "task_type": "policy optimization for language model fine-tuning with reward signals",
            "problem_format": null,
            "difficulty_level": null,
            "prompting_method": null,
            "performance_metric": "Training speed and downstream accuracy comparisons (versus PPO) and behavioral properties (length-hacking susceptibility).",
            "performance_value": "Reported (citing prior work) ~60% reduction in per-step training time vs PPO; in this study GRPO used for 850-step fine-tuning; authors report GRPO underperforms PPO in accuracy and is more susceptible to length-hacking.",
            "internal_analysis": "GRPO computes advantage A_i = (r_i - mean(r))/std(r) across a group of G sampled responses, avoiding a learned value function; uses clipping and KL penalty to a reference policy. This design reduces compute but may give noisier advantage estimates and encourage reward-gaming behaviors (e.g., longer outputs).",
            "failure_modes": "Length-hacking (models produce longer completions to game reward accumulation), potential suboptimal reward estimation due to group-normalized advantages, sensitivity to the sample/group size used in advantage calculation.",
            "scaling_trend": "Enables faster per-step training making large-scale RL more computationally tractable, but reported trade-offs: speed vs final accuracy (PPO reportedly higher accuracy).",
            "uuid": "e6417.2",
            "source_info": {
                "paper_title": "Reinforcement Learning is all You Need",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Rule-based Reward",
            "name_full": "Rule-based reward model (Format Reward + Answer Reward)",
            "brief_description": "A simple rule-based reward scheme used during RL fine-tuning that enforces response structure with a Format Reward (requires a single &lt;think&gt; tag and an &lt;answer&gt; tag) and a binary Answer Reward (1 if final answer equals target, else 0).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Rule-based Format+Answer Reward",
            "model_family": "rule-based reward model",
            "model_size": null,
            "training_data_description": "Operates on model outputs from Countdown Game prompts; uses regex extraction to enforce format and exact numeric match for correctness.",
            "benchmark_name": "Countdown Game (training), applied to subsequent evaluations indirectly",
            "task_type": "format compliance checking and binary correctness evaluation for arithmetic solutions",
            "problem_format": "Responses required to include reasoning inside &lt;think&gt;...&lt;/think&gt; and final equation/result inside &lt;answer&gt;...&lt;/answer&gt; tags.",
            "difficulty_level": null,
            "prompting_method": "Paired with the R1 prompt instructing the model to show work in &lt;think&gt; tags and final answer in &lt;answer&gt; tags.",
            "performance_metric": "Binary reward (0/1) for answer correctness and format compliance; also used indirectly to shape downstream task performance.",
            "performance_value": "Reward = 1 if &lt;answer&gt; content equals target number and format constraints satisfied; otherwise 0. No graded/partial-credit rewards were used.",
            "internal_analysis": "Design is simple and enforces structural compliance, which improved format consistency and downstream STRICT-MATCH rates; however, it does not measure partial credit, intermediate-step correctness, or reasoning quality. Authors note that Process Reward Models (PRMs) could better capture partial progress.",
            "failure_modes": "Mis-judges correct answers presented in unexpected formats (false negatives), penalizes near-correct reasoning due to lack of partial credit, and may encourage format-focused optimization over genuine reasoning.",
            "scaling_trend": "Scales easily computationally (binary checks), but authors recommend more nuanced reward models (PRMs) when precision of reasoning or partial credit matters.",
            "uuid": "e6417.3",
            "source_info": {
                "paper_title": "Reinforcement Learning is all You Need",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "R1 Prompt / &lt;think&gt;",
            "name_full": "R1 prompt template that enforces &lt;think&gt; CoT and &lt;answer&gt; final-output tags",
            "brief_description": "A prompting template used during evaluation and training that instructs the model to show chain-of-thought inside &lt;think&gt;...&lt;/think&gt; tags and place the final equation/answer inside &lt;answer&gt;...&lt;/answer&gt;, used both as an evaluation prompt and as an instruction during RL fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "R1 Prompt (prompting method)",
            "model_family": null,
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "Applied at evaluation time to GSM8K, MATH and other benchmarks",
            "task_type": "eliciting chain-of-thought and enforcing structured final answer formatting for arithmetic/word problems",
            "problem_format": "Natural-language prompt instructing the model to 'show your work in &lt;think&gt; tags' and 'return final equation and answer in &lt;answer&gt; tags'; evaluation harness used variations of this prompt.",
            "difficulty_level": "varied",
            "prompting_method": "Zero-shot chain-of-thought via explicit instruction to write CoT in tags and final answer in tags; compared against default harness prompt and CoT baselines.",
            "performance_metric": "Impact on Strict-Match and Flexible-Extract accuracy for GSM8K and other benchmarks.",
            "performance_value": "Applying R1 Prompt to the Base Model increased GSM8K Strict-Match from 29.3% to 55.0% (Base + R1). Trained Model evaluated with R1 achieved GSM8K 69.7% Flexible-Extract and 64.8% Strict-Match.",
            "internal_analysis": "Forcing a specific tag-based structure improved format consistency and therefore automated evaluation accuracy; prompt also interacted with RL fine-tuning to produce more consistent final-answer forms. However, some models produced the correct final answer without full visible reasoning in &lt;think&gt; tags.",
            "failure_modes": "Models may still violate the enforced format (multiple &lt;think&gt; segments, duplicated &lt;answer&gt; tags), causing evaluation failures; forcing tags can lead models to produce shallow or token-level optimizations to satisfy format without genuine process reliability.",
            "scaling_trend": "Prompting with explicit structure improves automated-evaluation scores at this model scale; trained models still showed degraded few-shot behavior despite strong zero-shot R1 performance.",
            "uuid": "e6417.4",
            "source_info": {
                "paper_title": "Reinforcement Learning is all You Need",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Countdown Game (dataset)",
            "name_full": "Countdown Game (target-number arithmetic puzzle dataset)",
            "brief_description": "A dataset of target-number puzzles where models are given a set of integers and asked to form an equation using +, -, *, / (each number used at most once) to reach a target number; used here for RL-only fine-tuning because correctness is easy to verify programmatically.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_family": null,
            "model_size": null,
            "training_data_description": "Structured numeric puzzles: random draws of integers and a target; designed to encourage numerical reasoning and allow automated correctness checking for reward assignment.",
            "benchmark_name": "Countdown Game (training dataset)",
            "task_type": "target-number arithmetic puzzle solving (combinational arithmetic with +, -, *, /), encourages multi-step arithmetic reasoning",
            "problem_format": "List of available numbers and a target; model must produce an equation and final numeric result (training prompted to include &lt;think&gt; and &lt;answer&gt; tags).",
            "difficulty_level": "variable; puzzles can be simple or require multi-step composition of operations; used to teach multi-step arithmetic strategy.",
            "prompting_method": "R1-style prompt instructing CoT in &lt;think&gt; tags and final equation in &lt;answer&gt; tags; RL fine-tuning using rule-based rewards.",
            "performance_metric": "Binary solved/not-solved reward during RL; also monitored behavioral changes (format compliance, emergence of strategies).",
            "performance_value": "Model adopted required response structure after ~80 training steps; total RL fine-tuning run shown had 850 steps (batch size 2, group size 2 for GRPO). Observed behavioral transition from brute-force enumeration to more concise reasoning over training.",
            "internal_analysis": "Training on Countdown induced verification-based, trial-and-error behavior early, later partial emergence of 'aha moments' (internal self-corrections). Model initially avoided multiplication/division and later used more structured strategies; no internal activation/attention probes were performed.",
            "failure_modes": "Early-stage rule/format violations (multiple &lt;think&gt; tags), inability to initially exploit multiplicative/divisive operations, persistent occasional format errors (duplicated tags) even after training.",
            "scaling_trend": "Countdown is effective as a training signal for arithmetic strategy emergence at this scale/setting; authors note need for more sophisticated reward structures (PRMs) and experiment length/scale considerations (only 850 steps reported).",
            "uuid": "e6417.5",
            "source_info": {
                "paper_title": "Reinforcement Learning is all You Need",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
            "rating": 2,
            "sanitized_title": "deepseekr1_incentivizing_reasoning_capability_in_llms_via_reinforcement_learning"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Proximal policy optimization algorithms",
            "rating": 1,
            "sanitized_title": "proximal_policy_optimization_algorithms"
        },
        {
            "paper_title": "The lessons of developing process reward models in mathematical reasoning",
            "rating": 2,
            "sanitized_title": "the_lessons_of_developing_process_reward_models_in_mathematical_reasoning"
        },
        {
            "paper_title": "Deep reinforcement learning from human preferences",
            "rating": 1,
            "sanitized_title": "deep_reinforcement_learning_from_human_preferences"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 1,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        }
    ],
    "cost": 0.017617999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>REINFORCEMENT LEARNING IS ALL YOU NEED</p>
<p>Yongsheng Lian yongsheng.lian@louisville.edu 
Mechanical Engineering Department
University of Louisville Louisville
40223KY</p>
<p>REINFORCEMENT LEARNING IS ALL YOU NEED
AFED9C9B251C1E6DAABBA3C786317AE2DeepSeek R1Reinforcement LearningRL-only TrainingAha MomentsSupervised Fine TurningLarge Language Models
Inspired by DeepSeek R1's success in reasoning via reinforcement learning without human feedback, we train a 3B language model using the Countdown Game with pure reinforcement learning.Our model outperforms baselines on four of five benchmarks, demonstrating improved generalization beyond its training data.Notably, response length does not correlate with reasoning quality, and while "aha moments" emerge, they do not always yield correct answers.These findings highlight the potential of RL-only training for reasoning enhancement and suggest future work on refining reward structures to bridge emergent insights with accuracy.</p>
<p>Introduction</p>
<p>Post-training plays a crucial role in refining language models, ensuring they exhibit good reasoning capabilities, alignment with ethical and social values, and adaptability to user-specific preferences.Unlike pre-training, which demands extensive computational resources and large-scale datasets, post-training is a more efficient process that leverages targeted fine-tuning techniques such as reinforcement learning from human feedback (RLHF) [1,2], instruction tuning [3], and Direct Preference Optimization (DPO) [4].These approaches enable models to generalize better across various tasks and mitigate biases.Furthermore, post-training requires significantly low computational overhead compared to pre-training [5].</p>
<p>Reasoning has long been regarded as a cornerstone in the development of large language models (LLMs).To effectively serve diverse purposes, LLMs must demonstrate a substantial level of reasoning proficiency.In the early stages of enhancing reasoning in language models, researchers focused on methods like prompt engineering, which involved carefully designing input prompts to guide the model's thought process and elicit more accurate and logical responses.One early breakthrough was the use of Chain-of-Thought (CoT) reasoning, which encouraged models to generate intermediate steps while solving complex tasks, allowing for improved reasoning in areas such as mathematics and logical problem-solving [6].</p>
<p>As the field advanced, OpenAI's O-series models introduced more sophisticated techniques, such as inference-time scaling, which expanded the CoT process to longer and more intricate reasoning chains.This approach resulted in substantial improvements in tasks requiring advanced reasoning [7].Other approaches, such as the Monte Carlo Tree Search (MCTS) [8,9] and process-based reward models (PBM) [10,11] have also been used to enhance reasoning capabilities.</p>
<p>In reinforcement learning, Supervised Fine-Tuning (SFT) is commonly employed to adapt pre-trained models to specific tasks by utilizing labeled data.This approach enables models to learn task-specific features and patterns, thereby enhancing their accuracy and relevance.However, the reliance on supervised data collection can be time-consuming and resource-intensive.</p>
<p>Reinforcement learning (RL) has shown significant potential in enhancing reasoning capabilities.One example is the AlphaGo where RL was used to teach the model to play the game of Go at a superhuman level.They trained deep neural networks by a combination of both supersived learning from human expert games and reinforcement learning from games of self-play [12].Another example is DeepSeek R1 [13,5].The model developed improved reasoning capabilities through rule-based reinforcement learning.They introduced a training pipeline that has two RL stages and two SFT stages.</p>
<p>The first SFT stage (Cold Start) begins by collecting thousands of long CoT examples as cold-start data.The DeepSeek-V3-Base model is then fine-tuned on this cold-start data.This initial SFT aims to provide the model with a foundation for reasoning.The first RL stage following the cold-start SFT is a large-scale reinforcement learning process.This stage focuses on enhancing the model's reasoning capabilities.During this RL, a language consistency reward is introduced to mitigate language mixing in the CoT.</p>
<p>The second SFT stage starts once the reasoning-oriented RL converges.The resulting checkpoint is used to collect new SFT data through rejection sampling.This new SFT data is more diverse than the initial cold-start data.It uses about 600k reasoning-related training samples and around 200k training samples unrelated to reasoning.The second SFT stage aims to enhance the model's capabilities in both reasoning and general-purpose tasks.</p>
<p>After the second SFT stage, the checkpoint undergoes an additional RL process.This final RL stage aims to further align the model with human preferences, improving its helpfulness and harmlessness while simultaneously refining its reasoning capabilities.This stage uses a combination of reward signals and diverse prompt distributions, taking into account prompts from all scenarios (reasoning and general data).While SFT is used in training, RL is the primary driver for advanced reasoning.</p>
<p>In this paper, we demonstrate that reinforcement learning can enhance the reasoning capabilities of language models.Our trained model outperforms the base model on four out of five benchmarks, showcasing the effectiveness of RL in improving generalization beyond the training data.We also replicate the phenomenon of the "aha moment" as reported in previous work [5], where the model exhibits moments of sudden insight.We find that the quality of reasoning decreases with longer responses in our experiments, highlighting a complex relationship between response length and reasoning quality.</p>
<p>Method</p>
<p>Training Dataset</p>
<p>To train our language models, we utilize the Countdown Game, a well-known numerical puzzle that challenges players to reach a given target number using a randomly drawn set of integers and basic arithmetic operations (+, , , ) [14].For instance, if the available numbers are 6, 7, 8, and 9, and the target number is 24, a valid solution would be 8  (6  (9  7)) = 24.This game serves as an ideal dataset for training models in numerical reasoning, as it provides a structured yet diverse set of problem-solving scenarios.Furthermore, the correctness of solutions is straightforward to verify, making it a reliable benchmark for evaluating model performance in arithmetic problem-solving.</p>
<p>Rule-Based Reward Modeling</p>
<p>To guide model behavior effectively, we employ a simple yet structured rule-based reward model that evaluates both the response format and the correctness of the generated solution.Specifically, we define two distinct reward mechanisms: the Format Reward and the Answer Reward.The prompts used for generating responses are adapted from [5] and are structured as follows: Format Reward</p>
<p> system: You are a helpful assistant.You first think about the reasoning process in the mind and then provide the user with the answer.</p>
<p> user: f"Using the numbers numbers, create an equation that equals target.You can use basic arithmetic operations (+, -, *, /) and each number can only be used once.Show your work in <think> </think> tags.And return the final equation and answer in <answer> </answer> tags.For example, <answer> (1 + 2) / 3 = 1 </answer>."</p>
<p> assistant: Let me solve this step by step.<think></p>
<p>The Format Reward ensures structural compliance by enforcing a well-formed response.Using regular expression-based extraction, we verify that the response begins with a <think> tag, ensuring that reasoning is explicitly articulated before the final answer.The <think> tag significantly reduces the difficulty for the base model to follow instructions [15].It also ensure the response does not contain nested <think> tags.The <answer>...</answer> section ensures that a final equation and result are clearly presented after the reasoning process.This reward mechanism serves as a fundamental safeguard, ensuring that the model adheres to the expected reasoning structure before evaluation of correctness.</p>
<p>Answer Reward The Answer Reward evaluates the correctness of the final result computed by the model.We use a simple binary answer reward function.If the answer within the <answer> </answer> tag matches the target number, the reward model receives a reward of 1; otherwise, the reward is 0. While this approach offers a straightforward correctness check, it does not consider partial progress or important intermediate steps.A more advanced reward structure such as Process Reward Models (PRMs) [10,16] would assign partial credit for logically sound reasoning, even if the final answer is incorrect.The PRMs can promote structured problem-solving and reduces the risk of penalizing models for minor calculation errors that do not invalidate the overall logical process.</p>
<p>By integrating both format validation and answer correctness, our reward model offers an effective mechanism for guiding the model to generate coherent, accurate, and logically consistent numerical solutions.</p>
<p>Reinforcement Learning Algorithm</p>
<p>The DeepSeek-R1 training used the Group Relative Policy Optimization (GRPO) method [5].It is an alternative to tranditional reinforcement learning methods such as the Proximal Policy Optimization (PPO) [17].The key distinction between GRPO and PPO lies in the handling of the value function.In PPO, the value function serves as a baseline for computing the advantage function, typically learned via a neural network.In contrast, GRPO eliminates the need for an explicit value function, reducing computational complexity while still maintaining a relative measure of reward effectiveness.</p>
<p>Instead of relying on a learned value function, GRPO generates a set of responses {o 1 , o 2 , . . ., o G } from the old policy   old and then optimizes the policy model   and uses their rewards to calculate each response's advantage as below:
J GRP O () = E G i=1 min   (o i )  old (o i ) A i , clip   (o i )  old (o i ) , 1  , 1 +  A i  D KL (    ref ) ,(1)
where the KL divergence term
D KL (  || ref ) =  ref (o i |q)   (o i |q)  log  ref (o i |q)   (o i |q)  1
acts as a regularization term to control divergence from a reference policy  ref , ensuring stability in training.The hyperparameters  and  regulate the clipping range and the penalty on KL divergence, respectively.</p>
<p>Rather than relying on a learned value function, GRPO computes the advantage function based purely on the observed rewards within a generated group:
A i = r i  mean({r 1 , r 2 ,    , r G }) std({r 1 , r 2 ,    , r G })
This formulation normalizes rewards within the group, effectively establishing a dynamic "score line", which serves as a baseline for determining whether a response is better or worse than the group average.By avoiding an explicit value function, GRPO significantly reduces computational overhead and simplifies policy optimization, making it more efficient than PPO.</p>
<p>Despite its computational efficiency, GRPO presents certain trade-offs when compared to PPO.Xie et al. [15] demonstrated that GRPO reduces per-step training time by 60% compared to PPO, making it particularly attractive for large-scale reinforcement learning applications.Hu [18] identified that GRPO is more susceptible to length hacking-a known issue in reinforcement learning where models generate longer responses to artificially increase reward accumulation.PPO's reliance on an explicit value function appears to mitigate this issue more effectively.</p>
<p>While GRPO accelerates training, it underperforms PPO in accuracy, as the lack of a value function may lead to suboptimal reward estimation, especially in complex tasks requiring precise numerical reasoning.</p>
<p>Benchmarks</p>
<p>To assess the impact of reinforcement learning (RL) training, we systematically compare the performance of our newly optimized model against the base model across six widely recognized benchmarks.These benchmarks have been extensively used to evaluate large language models (LLMs), with existing results available on HuggingFace [19].The selected benchmarks are: Grade School Math 8K (GSM8k) [20], Instruction-Following Eval (IFEval) [21], BIG-Bench Hard (BBH) [22], Mathematics Aptitude Test of Heuristics (MATH) [23], A More Robust and Challenging Multi-Task Language Understanding Benchmark (MMLU-Pro) [24].These benchmarks collectively cover a diverse range of linguistic and cognitive challenges, including instruction following, multi-step reasoning, mathematical problem-solving, expert-level Q&amp;A, and complex knowledge synthesis.Below, we provide a detailed overview of each benchmark and its significance.</p>
<p>Grade School Math 8K (GSM8K) GSM8K is a high-quality dataset designed to benchmark mathematical reasoning in language models.It consists of 8,500 carefully curated grade-school-level math word problems, requiring multi-step reasoning and arithmetic operations.GSM8K is widely used to evaluate a model's ability to perform complex problemsolving, as it emphasizes logical deduction rather than mere pattern recognition.Due to its structured, human-verified solutions, it has become a gold standard for assessing the reasoning capabilities of large language models, with many studies focusing on improving performance through techniques like chain-of-thought prompting and reinforcement learning.</p>
<p>Instruction-Following Eval (IFEval) IFEval is designed to assess an LLM's instruction-following capabilities through a structured set of verifiable instructions.The dataset consists of 541 prompts, each containing one or more atomic instructions that are objectively testable.These instructions span 25 categories, each with multiple syntactic and parametric variants, ensuring robustness against superficial prompt variations.The evaluation is deterministic: correctness is automatically verified using rule-based programs, providing an unambiguous measure of an LLM's ability to follow instructions precisely.</p>
<p>BIG-Bench Hard (BBH) BBH is a subset of the Beyond the Imitation Game Benchmark (BIG-Bench) [25], a comprehensive suite designed to assess LLMs across reasoning, mathematical problem-solving, commonsense knowledge, coding, and creative tasks.BBH specifically consists of 23 high-difficulty tasks that require advanced multi-step reasoning.These tasks are constructed to challenge models beyond surface-level memorization, making BBH an important test for evaluating the depth of reasoning capabilities in LLMs.</p>
<p>Mathematics Aptitude Test of Heuristics (MATH)</p>
<p>The MATH benchmark is a dataset of 12,500 problems sourced from leading U.S. mathematics competitions.Unlike standard high-school-level mathematical problems, MATH emphasizes advanced problem-solving techniques in algebra, calculus, geometry, and statistics.Many of these problems require mathematical heuristics rather than formulaic computation, making the benchmark a critical test for evaluating an LLM's ability to apply reasoning strategies beyond rote calculations.</p>
<p>A More Robust and Challenging Multi-Task Language Understanding Benchmark (MMLU-Pro) MMLU-Pro is an enhanced version of the Massive Multitask Language Understanding (MMLU) benchmark [26].Unlike the original MMLU, which is heavily knowledge-driven, MMLU-Pro shifts the focus toward complex reasoning by introducing: More challenging, reasoning-intensive questions, reducing reliance on factual recall.An expanded answer choice set (from 4 options to 10), making random guessing less effective.A refined dataset, removing trivial and noisy questions to enhance benchmark reliability.</p>
<p>Training Setup</p>
<p>We fine-tune the Qwen2.5-3B-Instructlanguage model [27], a 3-billion parameter instruction-tuned model.The training process is conducted on the Countdown Game dataset [28], which provides structured arithmetic challenges.</p>
<p>To implement reinforcement learning, we utilize HuggingFace's Transformer Reinforcement Learning (TRL) package [29], which includes a built-in implementation of Group Relative Policy Optimization (GRPO).The training parameters are shown in Table 1.In this example, the model incorrectly segments its reasoning into two separate <think>...</think> blocks instead of maintaining a single coherent reasoning section.Additionally, the math calculation is incorrect, possibly due to the incapacity of the 3B parameter base model.</p>
<p>Early Sign of Human-like Thinking</p>
<p>After 80 training steps, the model successfully adopts the correct response structure, encapsulating its reasoning process within the <think> and <answer> tags.However, instead of demonstrating logical, step-by-step reasoning, it follows a brute-force, trial-and-error approach, systematically testing different arithmetic combinations until it arrives at the correct answer.Notably, at this stage, the model only utilizes addition and subtraction, without incorporating multiplication or division.</p>
<p>This behavior indicates that rather than employing structured problem-solving strategies, the model relies on exhaustive enumeration of possible operations.While this method eventually produces the correct result, it is inefficient and lacks human-like problem-solving heuristics.Further training refinements are needed to encourage the model to develop structured reasoning instead of blindly iterating through possible solutions.</p>
<p>Despite its reliance on trial and error, the model does exhibit a verification-based thinking process by systematically evaluating its computations and comparing the results against the target value (43).This is evident in its responses such as "too high" and "too high again", which suggest an ability to assess and categorize outcomes.However, it lacks true reflection or strategic adaptation.Rather than analyzing incorrect attempts to refine its approach-such as recognizing numerical patterns or adjusting calculations based on previous errors-the model continues testing different combinations until it stumbles upon a valid equation.</p>
<p>Correct ) and then checks if the remaining numbers can be combined to reach 62.This is an efficient and direct approach; it does not waste time testing multiplication or division.The model then verifies the results to ensure the correctness before finalizing the equation.The logic is clear and each step follows from the last.The model also shows reflection when it evaluates different ways of manipulating the numbers.</p>
<p>The Aha Moment</p>
<p>In the R1 report [5], the term "Aha moment" primarily describes the model's sudden acquisition of complex reasoning behaviors.This phenomenon is often associated with emergent self-correction capabilities and is sometimes signaled by phrases like "Wait" or "Oops", which mimic human-like expressions of reflection.In the R1 report, the team suggested such linguistic cues indicate the model is engaging in real-time reassessment of its thought process, either revising an error or considering an alternative approach to solving the problem.</p>
<p>Our study indicates that responses containing such phrases often reflect an internal debugging process, where the model identifies inconsistencies in its reasoning and attempts to correct them.However, the Aha moment does not necessarily guarantee a correct final answer.Instead, it suggests that the model is exhibiting a structured cognitive process similar to human problem-solving through different strategies, detecting potential miscalculations, and refining its approach.</p>
<p>The following example highlights an Aha moment in action.Here, the model initially follows a straightforward mathematical strategy but soon realizes a flaw in its approach.It then self-corrects by reconsidering the numbers involved, demonstrating an essential aspect of higher-order reasoning.Even though the final answer remains incorrect, the underlying thought process exhibits a structured, step-by-step methodology that is characteristic of intelligent problem-solving.This instance highlights key characteristics of reasoning development in AI models.The model does not follow a rigid, predefined path but instead explores different possibilities, adjusting its approach as it detects inconsistencies.The phrase "But wait, we need 61, not 63" signifies an active recognition of a prior mistake, similar to how humans verbalize thought processes during problem-solving.While the model demonstrates a reasoning breakthrough, the final solution still contains an error, indicating that self-reflection alone does not guarantee correctness-it must be paired with a robust verification mechanism.Further analysis into the characteristics of these moments -when they occur, what triggers them, and why they sometimes fail -could provide deeper insights.</p>
<p>Response Length vs Reasoning Capability</p>
<p>Figure 2 shows the completion length, which fluctuates over time, dropping to its lowest point at 400 steps before increasing and decreasing again.This observation is based on a limited dataset from a simulation of only 850 steps, so the length may eventually rise further if the simulation is extended.Notably, this behavior contrasts with the findings in [5], where an increase in length was observed.A closer look at this discrepancy reveals that the model initially relies on a brute-force approach.During the early steps, it explores various combinations in an attempt to find the correct answer.However, as training progresses, the model gradually shifts from this brute-force method to a more reasoning-based approach 3.4, reflecting its growing ability to solve problems in a more structured, logical way.We also observed that, in some instances, the model provides the correct answer without fully revealing its reasoning process 3.5.This behavior raises important questions about the model's decision-making.One possible explanation for this could be that the model is capable of reaching conclusions through internal representations that do not always translate into explicit intermediate steps.In some cases, the model may identify patterns or shortcuts during training that allow it to generate the correct output without the need for detailed reasoning.While this might make the model more efficient in certain situations, it could also limit our understanding of how the model arrives at its answers.The lack of visible reasoning might pose challenges in ensuring the model's reliability, as users would have difficulty verifying the correctness of its decisions.Furthermore, the absence of a clear thought process makes it harder to debug or fine-tune the model, as it would be difficult to pinpoint where the reasoning might go wrong.</p>
<p>Aha Moment</p>
<p><think> First, I should consider how I can use each number to reach 61.With the given numbers 59, 42, and 44, I need to use each exactly once.Let's look for a way to subtract or add numbers to get the difference to 61.Probably multiplication or division won't get me much closer -it's more likely a combination of additions or subtractions will be helpful.</think> <answer>(59 -42) + 44</answer></p>
<p>Performance of the Trained Model</p>
<p>Our base model is initially trained on the Countdown Game, where we observe that it develops certain human-like reasoning patterns in its responses.To assess whether these acquired reasoning capabilities generalize beyond the training domain, we conduct a systematic evaluation on six diverse benchmarks spanning different problem domains.</p>
<p>For performance evaluation, we utilize the Language Model Evaluation Harness [30], a widely adopted framework for benchmarking language models across multiple tasks.We test both the base model and the trained model under standardized conditions:</p>
<p> Base Model Evaluation: We use two prompting strategies-(1) the <strong>default prompt</strong> from the evaluation harness and (2) the R1 prompt 3.6.</p>
<p> Trained Model Evaluation: We employ a prompt structure similar to that used in the R1 report [5], ensuring consistency in comparison.</p>
<p>All results are reported under a zero-shot setting unless explicitly stated otherwise, ensuring that performance is measured without any task-specific fine-tuning.This approach allows us to determine whether the model's reasoning skills, acquired from Countdown Game training, can enhance its performance across a broader range of challenges.</p>
<p>R1 Prompt</p>
<p>"You are a helpful assistant.You first think about the reasoning process in the mind and then provide the user with the answer.Problem: question Show your work in <think> </think> tags.Your response should end with The final answer is [answer] here [answer] is the response to the problem.Think step by step inside <think> tags.Let me solve this step by <think>"</p>
<p>GSM8K Table 2 presents the model performance on the GSM8K benchmark, evaluated using two metrics: Flexible-Extract and Strict-Match.Flexible-Extract allows some variation in response formatting while assessing correctness, whereas Strict-Match demands an exact match with the reference answer, making it a more rigorous metric.The Base Model with Chain-of-Thought (CoT) Zero-Shot achieves 66.3% on Flexible-Extract, but only 29.3% on Strict-Match.After checking the answers, we found that the output format does not consistently follow a specific pattern.As a result, the LLM Harness struggles to determine the correctness of the response, even though a human evaluator would consider the answer accurate.</p>
<p>Adding the R1 Prompt to the Base Model significantly boosts Strict-Match accuracy (55.0%), showing that the prompt improves both reasoning quality and consistency in answer formatting.The Trained Model with R1 Prompt outperforms all base model configurations, achieving 69.7% on Flexible-Extract and 64.8% on Strict-Match.This demonstrates that reinforcement learning (or fine-tuning) enhances both reasoning depth and answer consistency, leading to more structured and accurate responses.</p>
<p>We experimented with few-shot learning for the trained model; however, its performance was worse than in the zero-shot setting.This result aligns with the findings reported in [5], which also observed that few-shot learning underperformed relative to zero-shot performance.The reason is unknown.Further investigation into the model's behavior with different few-shot configurations may help clarify this issue.</p>
<p>We compare the answers from the base model and from the trained model to highlight the differences.The question is John runs 60 miles a week.He runs 3 days a week.He runs 3 hours the first day and half as much the other two days he runs.How fast does he run?</p>
<p>The model begins with an incorrect assumption about the total number of hours that John runs.It computes 3  6 = 18 hours, which is a misunderstanding of the structure of the problem.Next, the model incorrectly states that John runs 3 hours on the third day instead of correctly recognizing that he runs half of the first day's time (i.e., 1.5 hours).This introduces another error, leading to a total of 7.5 hours per week.</p>
<p>Base model answer to one GSM8K problem He runs 18 total hours because 3 x 6 = 18.He runs 3 hours the first day then 3 / 2 = 1. 5 hours the second day.He runs 3 hours the third day.He runs a total of 3 + 1. 5 + 3 = 7. 5 hours per week.So his speed is 60 / 7. 5 = 8 miles per hour.Therefore, the answer is 8.</p>
<p>The trained model begins by correctly calculating the running hours for each day.It uses the given information that John runs 3 hours on the first day and half that amount (1.5 hours) on the other two days.These values are accurately calculated.The model correctly adds the total weekly running hours: 3 + 1.5 + 1.5 = 6 hours.The trained model correctly interprets the question, using the information to break down the problem step by step.The Trained Model demonstrates a clear understanding of the problem and successfully breaks it down into manageable parts, ensuring accurate results.</p>
<p>Trained model answer to one GSM8K problem <think> First, let's determine how many hours John runs on each of the two days he runs half as much as the first day.We know that John runs a total of 3 hours on the first day.On the other two days, he runs half as much, so each of these days he runs 3 / 2 = 1.5 hours.</p>
<p>Next, we calculate the total running hours each day: -First day: 3 hours -Second day: 1.5 hours -Third day: 1.5 hours Summing these up gives the total weekly running hours: 3 + 1.5 + 1.5 = 6 hours Now we know John runs a total of 60 miles in a week and runs for a total of 6 hours.To find his average speed, we use the formula: Speed = Total Distance/Total Time Substituting the given values: Speed= 60 miles/6 hours Speed = 10 miles per hour </think>The final answer is 10.</p>
<p>Math Table 3 presents the performance on the Math benchmark, evaluated using two metrics: Extract Match and Math Verify.We use minerva math from the LLM Harness.We changed the setting to run a zero-shot evaluation.The evaluation used 5000 problems instead of the 12,500 ones.We compare the math verify result instead of the exact match because the exact match results are zero for both models.This indicates that neither model consistently produces answers that exactly match the reference solutions in the expected format.The lack of improvement even after training suggests that format inconsistencies or structural variations in responses remain a challenge.</p>
<p>The Math Verify metric, which assesses whether the final numerical answer is mathematically correct regardless of formatting, shows a clear improvement after training.The Base Model achieves 13%, while the Trained Model with R1 Prompt improves to 27%, representing a relative increase of more than 100%.This suggests that training enhances the model's reasoning ability, allowing it to solve more problems correctly, even if its output format does not strictly adhere to the reference answer format.</p>
<p>We compare answers from both models.The problem is What is the smallest whole number b such that 47 can be expressed in base b using only two digits?.The improved performance of the trained model is evident across multiple disciplines, particularly in Psychology (+68.5%),Biology (+60.5%), and Mathematics (+38.6%),indicating a broad enhancement in both factual recall and complex reasoning capabilities.This suggests that fine-tuning has strengthened the model's ability to integrate domain-specific knowledge with logical inference.The improvement in Psychology and Biology may stem from enhanced pattern recognition and causal reasoning, essential for understanding behavioral and biological systems.These results demonstrate that cross-disciplinary advancements in AI models are achievable through reinforcement learning.</p>
<p>IFEval The Base Model performed well with a Loose Accuracy of 66.0% and a Strict Accuracy of 60.3%, demonstrating a solid baseline performance without additional fine-tuning or prompt engineering.</p>
<p>For the trained model, the Loose Accuracy decreased slightly to 64.8%, and the Strict Accuracy showed a minor decrease to 60.1%.</p>
<p>Conclusion and Future Work</p>
<p>This work demonstrates the potential of pure RL in enhancing the reasoning capabilities of language models.Our experiments show that the RL-trained model outperforms the base model on four out of five benchmarks, indicating that RL can drive significant improvements in generalization and task performance.While we successfully replicated the "aha moment", we found that these moments did not directly contribute to higher accuracy.Additionally, we discovered reasoning responses became shorter as the model transition from brute-force approaches to a more human-like reasoning approaches.</p>
<p>During our training process, we observed several issues that impacted the model's performance and evaluation:</p>
<p> Response Format Violations: At the end of the training period, the model still occationally generates wrong formats like duplicated <answer></answer> blocks.</p>
<p> Model Evaluation Harness Limitations: The evaluation harness we used misjudged the quality of model responses when model responses deviated from the expected format.A more robust evaluation framework, potentially incorporating a stronger model, could better assess and validate the model's outputs.</p>
<p> Performance with Few-Shot Prompts: The trained model did not perform as well when using few-shot prompts, a result consistent with prior findings [5].</p>
<p> Reward Function Judgments: The format-reward and answer-reward functions occasionally misjudged the model responses, especially in cases where human evaluators would deem an answer correct, but the reward functions assessed it as incorrect.</p>
<p> GRPO: We need to examine the impact of sample size on the evaluation of the advantage function during training.The number of samples used may significantly affect the performance and stability of the model, and this aspect requires further exploration to optimize training efficacy.</p>
<p>Figure 1 :
1
Figure 1: Model performance on different benchmarks</p>
<p>Figure 2 :
2
Figure 2: Change of completion length with training steps</p>
<p>The base model has incorrect reasoning at Step 2 when it said We need 47 = ab + c where a  b  1 and 0  c &lt; b..It shold be a  b  1.The trained model recognize the if the number 47 can be represented using only two digits, then there should be a k so that b k  47 &gt; b k1 .The better way is to set k = 2 because it uses only two digits.</p>
<p>Table 1 :
1
Training hyperparameters for RL fine-tuning
HyperparameterValueTotal training steps850Batch size2Learning rate1.0  10 6GRPO reward calculationBased on two generated samples per stepKL regularization coefficient ( in Eq. 1)0.043 Results and Discussions</think><think>Considering we have only 76 and our sum so far is 31, let's try 36 + (49 -9 -9).</think><answer> (36 + 49) -9 -9 = 76 </answer>
3.1 Rule Violation in Early Training StageDuring the initial training phase, the model begins to structure its reasoning using <think>...</think> tags but fails to follow the correct format.Specifically, it introduces multiple <think>...</think> segments, which violates the expected format.RRule Violation in Early Training Stage <think>To get close to 76 with the given numbers and operations, I'll try combinations of the numbers 36, 9, and 49.Let's start with 49.I know that 49 -9 -9 = 31, which is close to 76.But still not quite there, so let's add some more numbers.</p>
<p>Response Structure and Verification-based Thinking <think> I need to create an equation that uses the numbers 73, 77, and 39 exactly once to equal 43.Let's try different combinations of operations.-First,let'sseeif adding or subtracting these numbers gives us 43.After 300 steps, the model is able to engage in an extensive step-by-step exploration of possible solutions.In the successful example shown below, we can see that it consistently evaluates expressions and adjusts calculations when results do not match the target value.It showcases trial-and-error thinking, similar to how a human might struggle and persist through different approaches.It also shows backtracking when an incorrect equation is identified.Human-like Thinking <think> Firstly, I need to create an equation using the numbers 39, 72, and 95 that is equal to 62. Let's first consider how we can manipulate the numbers.Notice that 95 is the largest and 39 is a relatively small number.One way to get the result close to 62 would be to subtract a large number and then use the remaining numbers to get the final result.Let's try using subtraction first.If I subtract 72 from 95, I get 23.Now I need to manipulate 39 and 23 to reach 62. Adding 39 and 23 gives me 62, which is exactly what we need!
-73 + 77 + 39 = 189 (too high)-73 + 77 -39 = 111 (still too high)-73 -77 + 39 = 35 (too low)-39 + 73 + 77 = 189 (too high again)-39 + 77 -73 = 43 (this looks promising)So, the equation works out to be 39 + 77 -73 = 43.</think><answer> (39 + 77) -73 </answer>3.3 Human-like Thinking</think> <answer> (95 -72) + 39 </answer>This example demonstrates the model's systematic exploration and pattern recognition. It correctly identifies thelargest number (95) and the smallest (39) and considers how they can be manipulated. It tests a reasonable operation(95-72=23</p>
<p>Table 2
2: Benchmark GSM8KModelFlexible-Extract Strict-MatchBase Model + CoT Llama66.3%29.3%Base Model + R1 Prompt60.9%55.0%Trained Model + R1 Prompt 69.7%64.8%</p>
<p>Table 3 :
3
Comparison of Model Performance on the MATH Benchmark
TasksBase Model (math_verify) Trained Model (math_verify)Math Algebra0.18370.4027Math Counting and Probability0.09700.2489Math Geometry0.04380.1983Math Intermediate Algebra0.05200.0997Math Number Theory0.10190.1926Math Prealgebra0.27670.5109Math Precalculus0.04030.0769Group Result0.13000.2744</p>
<p>Table 4 :
4
Comparison of Model Performance across Different Tasks in BBHMMLU Pro Table5presents the test results on the MMLU Pro benchmark, which evaluates a model's ability to generate correct answers across diverse professional-level knowledge domains.The Base Model achieves an Extract Match score of 16.3%, indicating its baseline ability to produce correct responses that match the reference format.This relatively low score suggests that the Base Model struggles with either formatting consistency or accurate recall of domain-specific knowledge.The Trained Model with R1 Prompt significantly improves to 22.4%, an absolute increase of 6.2% over the Base Model and a 9.1% improvement over the Base Model + R1 Prompt.
TasksBase Model Base Model + R1 Prompt Trained ModelGroup Result0.3750.3650.4406Boolean Expressions0.82800.78400.9040Causal Judgement0.44390.44920.4920Date Understanding0.35600.24800.4920Disambiguation QA0.42000.38000.4960Dyck Languages0.00800.02400.0040Formal Fallacies0.47200.34400.5040Geometric Shapes0.08000.05200.1280Hyperbaton0.55200.59600.5560Logical Deduction (Five Objects)0.34400.32400.3360Logical Deduction (Seven Objects)0.26800.31600.3440Logical Deduction (Three Objects)0.30800.23200.6200Movie Recommendation0.47600.35200.3760Multistep Arithmetic (Two)0.17600.20000.6000Navigate0.74800.76000.7200Object Counting0.05200.04000.1200Penguins in a Table0.67120.70550.8014Reasoning about Colored Objects0.50000.52400.6840Ruin Names0.35600.34800.2760Salient Translation Error Detection0.19200.21200.4040Snarks0.50000.55620.5955Sports Understanding0.55600.54000.4920Temporal Sequences0.30400.33600.2360Tracking Shuffled Objects (Five Objects)0.37200.33600.2680Tracking Shuffled Objects (Seven Objects)0.22000.23200.1840Tracking Shuffled Objects (Three Objects)0.37600.40000.6320Web of Lies0.65600.54000.7400Word Sorting0.10000.12400.1000</p>
<p>Table 5 :
5
Comparison of Base Model and Trained Model Performance on MMLU Pro Tasks
TasksBase Model Trained ModelBiology0.23290.3738Business0.24590.3016Chemistry0.05740.1087Computer Science0.20490.2537Economics0.25710.3128Engineering0.04850.0640Health0.14430.2347History0.15490.1890Law0.08270.0954Math0.23390.3242Other0.23480.2911Philosophy0.18040.2044Physics0.10390.1586Psychology0.19420.3271Group Result (MMLU Pro)0.16250.2242Table 6: Benchmark IFEvalModelLoose Accuracy Strict AccuracyBase Model66.0%60.3%Trained Model + R1 Prompt 64.8%60.1%
Base model answer to one MATH Number Theory problemTo determine the smallest whole number b such that the number 47 can be expressed in base b using only two digits, we need to follow these steps: 1. <strong>Understand the representation of a number in base b with two digits:</strong> -If a number N can be expressed with two digits in base b, it can be written as:where a and c are digits in base b.For a two-digit number in base b, the possible values for a (the leading digit) are b  1 or higher, and c (the trailing digit) ranges from 0 to b  1.We see that 6 2 = 36 &lt; 47 and 7 2 = 49 &gt; 47.Therefore, the smallest base b is 7.</think> The smallest whole number b such that 47 can be expressed in base b using only two digits is 7. So, the answer is 7.BBH Table4
Deep reinforcement learning from human preferences. Paul Christiano, Jan Leike, Tom B Brown, Miljan Martic, Shane Legg, Dario Amodei, 2023</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, ; , Ryan Lowe, Jan Leike,. 2022</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Dai, V Quoc, Le, 2022</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, Chelsea Finn, 2024</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2023</p>
<p>Learning to reason with llms. Openai, 2024</p>
<p>Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus Mcaleer, Ying Wen, Weinan Zhang, Jun Wang, arXiv:2309.17179Alphazero-like tree-search can guide large language model decoding and training. 2023arXiv preprint</p>
<p>Interpretable contrastive monte carlo tree search reasoning. Zitian Gao, Boye Niu, Xuzheng He, Haotian Xu, Hongzhang Liu, Aiwei Liu, Xuming Hu, Lijie Wen, arXiv:2410.017072024arXiv preprint</p>
<p>Vineet Hunter Lightman, Yura Kosaraju, Harri Burda, John Edwards ; Leike, Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. Bowen Baker, Teddy LeeJan. 2023</p>
<p>Peiyi Wang, Lei Li, Zhihong Shao, Damai Xu, Yifei Dai, Deli Li, Chen, Zhifang Wu, Sui, arXiv:2312.08935Mathshepherd: A label-free step-by-step verifier for llms in mathematical reasoning. 2023arXiv preprint</p>
<p>Mastering the game of go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den, Julian Driessche, Ioannis Schrittwieser, Veda Antonoglou, Marc Panneershelvam, Lanctot, nature. 52975872016</p>
<p>Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y K Li, Y Wu, Daya Guo, 2024</p>
<p>. Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, Alane Suhr, Tinyzero, 2025</p>
<p>Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, Chong Luo, 2025</p>
<p>The lessons of developing process reward models in mathematical reasoning. Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, Junyang Lin, 2025</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, Proximal policy optimization algorithms. 2017</p>
<p>Reinforce++: A simple and efficient approach for aligning large language models. Jian Hu, arXiv:2501.032622025arXiv preprint</p>
<p>Open llm leaderboard. 2025Hugging Face</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Instruction-following evaluation for large language models. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, Le Hou, 2023</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schrli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Ed H Quoc V Le, Denny Chi, Jason Zhou, Wei, arXiv:2210.092612022arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, 2021</p>
<p>Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, Wenhu Chen, 2024</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adri Gupta, Garriga-Alonso, arXiv:2206.046152022arXiv preprint</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, 2021</p>
<p>Qwen2.5: A party of foundation models. Qwen Team, September 2024</p>
<p>Countdown-tasks-3to4 dataset. Jiayi Pan, 2024</p>
<p>. Younes Leandro Von Werra, Lewis Belkada, Edward Tunstall, Tristan Beeching, Nathan Thrush, Shengyi Lambert, Kashif Huang, Quentin Rasul, Galloudec, Trl: Transformer reinforcement learning. 2020</p>
<p>A framework for few-shot language model evaluation. Leo Gao, Jonathan Tow, Stella Baber Abbasi, Sid Biderman, Anthony Black, Charles Dipofi, Laurence Foster, Jeffrey Golding, Alain Hsu, Haonan Le Noac'h, Kyle Li, Niklas Mcdonell, Chris Muennighoff, Jason Ociepa, Laria Phang, Hailey Reynolds, Aviya Schoelkopf, Lintang Skowron, Eric Sutawika, Anish Tang, Ben Thite, Kevin Wang, Andy Wang, Zou, 072024</p>            </div>
        </div>

    </div>
</body>
</html>
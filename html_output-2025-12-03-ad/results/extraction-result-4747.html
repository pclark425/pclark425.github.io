<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4747 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4747</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4747</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-265149412</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.07491v1.pdf" target="_blank">A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> While large language models exhibit remarkable performance in the Question Answering task, they are susceptible to hallucinations. Challenges arise when these models grapple with understanding multi-hop relations in complex questions or lack the necessary knowledge for a comprehensive response. To address this issue, we introduce the"Decompose-and-Query"framework (D&Q). This framework guides the model to think and utilize external knowledge similar to ReAct, while also restricting its thinking to reliable information, effectively mitigating the risk of hallucinations. Experiments confirm the effectiveness of D&Q: On our ChitChatQA dataset, D&Q does not lose to ChatGPT in 67% of cases; on the HotPotQA question-only setting, D&Q achieved an F1 score of 59.6%. Our code is available at https://github.com/alkaidpku/DQ-ToolQA.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4747.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4747.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>D&Q</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decompose-and-Query (D&Q)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A constrained multi-stage question decomposition framework that guides an LLM to iteratively decompose complex questions, invoke external retrievers against a curated reliable QA base, and backtrack when retrieval or reasoning paths fail to reduce hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA2-13B base model (approx. 13 billion parameters) used as the generative backbone and supervised-fine-tuned on annotated decomposition trajectories to learn tool invocation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Constrained Multi-Stage Question Decomposition (Decompose-and-Query)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Performs depth-first decomposition into subquestions, uses dedicated tools (QuestionRetriever, AnswerRetriever, Finish) to query a reliable QA base, stores intermediate states and supports backtracking to explore alternative solution trajectories when a path fails; diversity arises from exploring multiple decomposition/search paths rather than a single linear chain.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotPotQA; ChitChatQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>HotPotQA: an academic multi-hop question answering benchmark (question-only setting used); ChitChatQA: a dataset of real single-turn dialogue questions collected from KwaiTalk to evaluate decomposition and tool-invocation in conversational scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>HotPotQA (question-only): F1 = 59.6, EM = 39.0; ChitChatQA human evaluation (vs ChatGPT): not-losing in 62% of cases (win 24%, tie 38%, lose 38%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>IRCoT (Code-Davinci-002 175B) F1 = 60.7; Self-Ask (InstructGPT 175B) F1 = 55.2; ReAct (PaLM 540B) F1 = 44.7; CoT (PaLM 540B) F1 = 44.1; Standard PaLM F1 = 36.8.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>D&Q's constrained decomposition with explicit tool-invocation and backtracking substantially improves retrieval recall (from 52.3% to 68.8%) and yields competitive HotPotQA performance (F1 59.6) using a 13B model, showing that supervised fine-tuning to learn tool use plus backtracking lets smaller models approach or match larger-model baselines; backtracking explicitly improves retrieval compared to greedy tool-use methods.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>IRCoT (175B Code-Davinci-002) achieves a slightly higher F1 (60.7) than D&Q (59.6), indicating that some larger-model + retrieval+CoT pipelines can still outperform D&Q; no cases reported where D&Q's diverse strategy was clearly inferior to similar single-path strategies apart from this marginal numeric difference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4747.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4747.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (Reasoning + Acting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that interleaves chain-of-thought style reasoning with explicit action/tool calls (e.g., search, lookup) to acquire external evidence during reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing Reasoning and Acting in Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 540B (reported baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM 540B — a very large transformer model used in cited baseline results.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>ReAct (reasoning + acting interleaving)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Alternates natural-language reasoning steps and discrete actions (tool calls) so the model can consult external sources while reasoning; in practice it is executed greedily without systematic backtracking or breadth exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotPotQA (reported baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-hop QA benchmark; in the paper ReAct is listed as a baseline on question-only HotPotQA.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported baseline: EM = 24.9, F1 = 44.7 (PaLM 540B).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>D&Q (LLaMA2-13B) F1 = 59.6 (higher); CoT (PaLM 540B) F1 = 44.1; IRCoT Code-Davinci-002 F1 = 60.7.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper reports that ReAct's greedy tool-use and lack of explicit backtracking lead to weaker retrieval and less effective performance on multi-hop tasks compared to D&Q's backtracking-enabled approach.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No explicit case where ReAct outperforms D&Q is reported; the paper characterizes ReAct as effective for bringing in external info but limited in practice due to greediness and limited action space.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4747.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4747.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits step-by-step internal reasoning chains from language models to improve performance on multi-step reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 540B (reported baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM 540B — cited baseline model used with CoT prompting in referenced results.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Prompts the model to generate explicit stepwise reasoning traces (a single chain per sample) which can improve compositional reasoning; diversity is not inherently provided unless combined with sampling-based methods like self-consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotPotQA (reported baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-hop QA benchmark used to evaluate reasoning methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported baseline: EM = 30.0, F1 = 44.1 (PaLM 540B with CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>D&Q (LLaMA2-13B) F1 = 59.6 (not directly same-setting but comparable), Self-Consistency (diverse extension) discussed but not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT improves over standard prompting but can still fail when the single reasoning chain goes astray; the paper cites that incorrect single chains lead to wrong answers and motivates approaches that explore multiple reasoning paths or add tool-backed constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>The paper notes that incorrect reasoning paths from CoT often produce incorrect results and that CoT alone is insufficient on harder problems without diversity or external retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4747.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4747.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique that samples multiple chain-of-thought reasoning trajectories and aggregates their final answers to pick the most consistent answer, increasing robustness to single-chain errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Consistency Improves Chain of Thought Reasoning in Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Method-agnostic technique that can be applied to various LMs by sampling multiple reasoning traces; cited as prior work rather than run in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Consistency (ensemble of CoT samples)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Samples a diverse set of chain-of-thought outputs (via decoding randomness) and aggregates the resulting answers (e.g., majority vote) to improve robustness; achieves diversity through multiple sampled traces.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>General multi-step reasoning tasks (discussed in related work); not directly evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper cites self-consistency as an approach that increases diversity of reasoning paths but notes it raises decoding cost and still struggles on very challenging tasks; motivates tree/graph-style or retrieval-backed multi-path strategies like D&Q and ToT.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>The paper states self-consistency 'still struggles with challenging problem-solving' despite better robustness than single-chain CoT, but no quantitative counterexample is provided in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4747.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4747.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree of Thoughts (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deliberative search framework that expands multiple candidate thoughts into a tree structure, allowing lookahead and backtracking to improve long-range decision making and problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of Thoughts: Deliberate Problem Solving with Large Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Framework-level approach applicable across LMs; paper references it as a multi-path reasoning paradigm emphasizing lookahead and backtracking.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Tree-of-Thought (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>At each step, the model generates multiple candidate 'thoughts' (partial solution states), evaluates them, and can look ahead or backtrack in a tree search to find robust solution paths; achieves diversity by expanding branches rather than committing to a single chain.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>General complex reasoning tasks; discussed in related work but not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper cites ToT as an approach that extends CoT by enabling multi-path exploration and backtracking; D&Q shares the backtracking spirit and constrained/tool-backed exploration, suggesting conceptual similarities.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No quantitative comparisons in this paper; ToT is discussed as promising but not directly evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4747.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4747.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IRCoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Interleaving Retrieval with Chain-of-Thought (IRCoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that decomposes questions with chain-of-thought, retrieves relevant evidence, and feeds concatenated question+context to a more powerful model for final answer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Code-Davinci-002 (reported baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI Code-Davinci-002 (approx. 175B) used in cited IRCoT baseline results.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>IRCoT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Decomposes questions using chain-of-thought, retrieves relevant contexts for subquestions, and uses a strong LM conditioned on retrieved evidence for final answer; primarily follows one decomposition path per question.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotPotQA (reported baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-hop QA benchmark; IRCoT is cited as a high-performing retrieval+CoT baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported baseline: F1 = 60.7 (Code-Davinci-002 175B).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>D&Q (LLaMA2-13B) F1 = 59.6 (comparable despite smaller model size).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>IRCoT achieves slightly higher F1 than D&Q on HotPotQA, but D&Q achieves comparable performance with a much smaller model by learning constrained decomposition and tool invocation; suggests supervised tool-invocation and backtracking can close the gap between small and large models.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>IRCoT's numeric edge (60.7 vs 59.6) is presented as evidence that some large-model retrieval+CoT pipelines can outperform D&Q, although the authors emphasize model-size differences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4747.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4747.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Ask</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Ask</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decomposition approach where the model asks intermediate sub-questions to itself, queries external search to answer them, and aggregates sub-answer information to form the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (reported baseline, 175B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>InstructGPT (GPT-3 based instruction-tuned model, ~175B) referenced for reported Self-Ask baseline performance.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Ask (decompose-and-search)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>The model decomposes a complex question into sub-questions, uses a search engine to answer each sub-question independently, and then composes the final answer; typically follows a single decomposition trajectory per question.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotPotQA (reported baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-hop QA benchmark; Self-Ask reported as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported baseline: F1 = 55.2 (InstructGPT 175B).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>D&Q (LLaMA2-13B) F1 = 59.6 (higher).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Self-Ask improves multi-hop QA by explicit decomposition and search, but D&Q's constrained decomposition and backtracking lead to better retrieval and higher final performance in this paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No explicit negative examples beyond the numeric baseline where Self-Ask underperforms D&Q on HotPotQA.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-Consistency Improves Chain of Thought Reasoning in Language Models. <em>(Rating: 2)</em></li>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. <em>(Rating: 2)</em></li>
                <li>Tree of Thoughts: Deliberate Problem Solving with Large Language Models. <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing Reasoning and Acting in Language Models. <em>(Rating: 2)</em></li>
                <li>Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions. <em>(Rating: 2)</em></li>
                <li>Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4747",
    "paper_id": "paper-265149412",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "D&Q",
            "name_full": "Decompose-and-Query (D&Q)",
            "brief_description": "A constrained multi-stage question decomposition framework that guides an LLM to iteratively decompose complex questions, invoke external retrievers against a curated reliable QA base, and backtrack when retrieval or reasoning paths fail to reduce hallucinations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA2-13B",
            "model_description": "LLaMA2-13B base model (approx. 13 billion parameters) used as the generative backbone and supervised-fine-tuned on annotated decomposition trajectories to learn tool invocation.",
            "reasoning_method_name": "Constrained Multi-Stage Question Decomposition (Decompose-and-Query)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Performs depth-first decomposition into subquestions, uses dedicated tools (QuestionRetriever, AnswerRetriever, Finish) to query a reliable QA base, stores intermediate states and supports backtracking to explore alternative solution trajectories when a path fails; diversity arises from exploring multiple decomposition/search paths rather than a single linear chain.",
            "task_name": "HotPotQA; ChitChatQA",
            "task_description": "HotPotQA: an academic multi-hop question answering benchmark (question-only setting used); ChitChatQA: a dataset of real single-turn dialogue questions collected from KwaiTalk to evaluate decomposition and tool-invocation in conversational scenarios.",
            "performance": "HotPotQA (question-only): F1 = 59.6, EM = 39.0; ChitChatQA human evaluation (vs ChatGPT): not-losing in 62% of cases (win 24%, tie 38%, lose 38%).",
            "comparison_with_other_method": true,
            "performance_other_method": "IRCoT (Code-Davinci-002 175B) F1 = 60.7; Self-Ask (InstructGPT 175B) F1 = 55.2; ReAct (PaLM 540B) F1 = 44.7; CoT (PaLM 540B) F1 = 44.1; Standard PaLM F1 = 36.8.",
            "key_findings": "D&Q's constrained decomposition with explicit tool-invocation and backtracking substantially improves retrieval recall (from 52.3% to 68.8%) and yields competitive HotPotQA performance (F1 59.6) using a 13B model, showing that supervised fine-tuning to learn tool use plus backtracking lets smaller models approach or match larger-model baselines; backtracking explicitly improves retrieval compared to greedy tool-use methods.",
            "counter_examples_or_negative_results": "IRCoT (175B Code-Davinci-002) achieves a slightly higher F1 (60.7) than D&Q (59.6), indicating that some larger-model + retrieval+CoT pipelines can still outperform D&Q; no cases reported where D&Q's diverse strategy was clearly inferior to similar single-path strategies apart from this marginal numeric difference.",
            "uuid": "e4747.0",
            "source_info": {
                "paper_title": "A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct (Reasoning + Acting)",
            "brief_description": "A method that interleaves chain-of-thought style reasoning with explicit action/tool calls (e.g., search, lookup) to acquire external evidence during reasoning.",
            "citation_title": "ReAct: Synergizing Reasoning and Acting in Language Models.",
            "mention_or_use": "mention",
            "model_name": "PaLM 540B (reported baseline)",
            "model_description": "PaLM 540B — a very large transformer model used in cited baseline results.",
            "reasoning_method_name": "ReAct (reasoning + acting interleaving)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Alternates natural-language reasoning steps and discrete actions (tool calls) so the model can consult external sources while reasoning; in practice it is executed greedily without systematic backtracking or breadth exploration.",
            "task_name": "HotPotQA (reported baseline)",
            "task_description": "Multi-hop QA benchmark; in the paper ReAct is listed as a baseline on question-only HotPotQA.",
            "performance": "Reported baseline: EM = 24.9, F1 = 44.7 (PaLM 540B).",
            "comparison_with_other_method": true,
            "performance_other_method": "D&Q (LLaMA2-13B) F1 = 59.6 (higher); CoT (PaLM 540B) F1 = 44.1; IRCoT Code-Davinci-002 F1 = 60.7.",
            "key_findings": "The paper reports that ReAct's greedy tool-use and lack of explicit backtracking lead to weaker retrieval and less effective performance on multi-hop tasks compared to D&Q's backtracking-enabled approach.",
            "counter_examples_or_negative_results": "No explicit case where ReAct outperforms D&Q is reported; the paper characterizes ReAct as effective for bringing in external info but limited in practice due to greediness and limited action space.",
            "uuid": "e4747.1",
            "source_info": {
                "paper_title": "A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Chain-of-Thought",
            "name_full": "Chain-of-Thought (CoT) Prompting",
            "brief_description": "A prompting technique that elicits step-by-step internal reasoning chains from language models to improve performance on multi-step reasoning tasks.",
            "citation_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.",
            "mention_or_use": "mention",
            "model_name": "PaLM 540B (reported baseline)",
            "model_description": "PaLM 540B — cited baseline model used with CoT prompting in referenced results.",
            "reasoning_method_name": "Chain-of-Thought (CoT)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Prompts the model to generate explicit stepwise reasoning traces (a single chain per sample) which can improve compositional reasoning; diversity is not inherently provided unless combined with sampling-based methods like self-consistency.",
            "task_name": "HotPotQA (reported baseline)",
            "task_description": "Multi-hop QA benchmark used to evaluate reasoning methods.",
            "performance": "Reported baseline: EM = 30.0, F1 = 44.1 (PaLM 540B with CoT).",
            "comparison_with_other_method": true,
            "performance_other_method": "D&Q (LLaMA2-13B) F1 = 59.6 (not directly same-setting but comparable), Self-Consistency (diverse extension) discussed but not reported here.",
            "key_findings": "CoT improves over standard prompting but can still fail when the single reasoning chain goes astray; the paper cites that incorrect single chains lead to wrong answers and motivates approaches that explore multiple reasoning paths or add tool-backed constraints.",
            "counter_examples_or_negative_results": "The paper notes that incorrect reasoning paths from CoT often produce incorrect results and that CoT alone is insufficient on harder problems without diversity or external retrieval.",
            "uuid": "e4747.2",
            "source_info": {
                "paper_title": "A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency",
            "brief_description": "A technique that samples multiple chain-of-thought reasoning trajectories and aggregates their final answers to pick the most consistent answer, increasing robustness to single-chain errors.",
            "citation_title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": "Method-agnostic technique that can be applied to various LMs by sampling multiple reasoning traces; cited as prior work rather than run in this paper.",
            "reasoning_method_name": "Self-Consistency (ensemble of CoT samples)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Samples a diverse set of chain-of-thought outputs (via decoding randomness) and aggregates the resulting answers (e.g., majority vote) to improve robustness; achieves diversity through multiple sampled traces.",
            "task_name": null,
            "task_description": "General multi-step reasoning tasks (discussed in related work); not directly evaluated in this paper.",
            "performance": null,
            "comparison_with_other_method": true,
            "performance_other_method": null,
            "key_findings": "Paper cites self-consistency as an approach that increases diversity of reasoning paths but notes it raises decoding cost and still struggles on very challenging tasks; motivates tree/graph-style or retrieval-backed multi-path strategies like D&Q and ToT.",
            "counter_examples_or_negative_results": "The paper states self-consistency 'still struggles with challenging problem-solving' despite better robustness than single-chain CoT, but no quantitative counterexample is provided in this work.",
            "uuid": "e4747.3",
            "source_info": {
                "paper_title": "A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Tree-of-Thought",
            "name_full": "Tree of Thoughts (ToT)",
            "brief_description": "A deliberative search framework that expands multiple candidate thoughts into a tree structure, allowing lookahead and backtracking to improve long-range decision making and problem solving.",
            "citation_title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": "Framework-level approach applicable across LMs; paper references it as a multi-path reasoning paradigm emphasizing lookahead and backtracking.",
            "reasoning_method_name": "Tree-of-Thought (ToT)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "At each step, the model generates multiple candidate 'thoughts' (partial solution states), evaluates them, and can look ahead or backtrack in a tree search to find robust solution paths; achieves diversity by expanding branches rather than committing to a single chain.",
            "task_name": null,
            "task_description": "General complex reasoning tasks; discussed in related work but not evaluated in this paper.",
            "performance": null,
            "comparison_with_other_method": true,
            "performance_other_method": null,
            "key_findings": "The paper cites ToT as an approach that extends CoT by enabling multi-path exploration and backtracking; D&Q shares the backtracking spirit and constrained/tool-backed exploration, suggesting conceptual similarities.",
            "counter_examples_or_negative_results": "No quantitative comparisons in this paper; ToT is discussed as promising but not directly evaluated here.",
            "uuid": "e4747.4",
            "source_info": {
                "paper_title": "A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "IRCoT",
            "name_full": "Interleaving Retrieval with Chain-of-Thought (IRCoT)",
            "brief_description": "A method that decomposes questions with chain-of-thought, retrieves relevant evidence, and feeds concatenated question+context to a more powerful model for final answer generation.",
            "citation_title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions.",
            "mention_or_use": "mention",
            "model_name": "Code-Davinci-002 (reported baseline)",
            "model_description": "OpenAI Code-Davinci-002 (approx. 175B) used in cited IRCoT baseline results.",
            "reasoning_method_name": "IRCoT",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Decomposes questions using chain-of-thought, retrieves relevant contexts for subquestions, and uses a strong LM conditioned on retrieved evidence for final answer; primarily follows one decomposition path per question.",
            "task_name": "HotPotQA (reported baseline)",
            "task_description": "Multi-hop QA benchmark; IRCoT is cited as a high-performing retrieval+CoT baseline.",
            "performance": "Reported baseline: F1 = 60.7 (Code-Davinci-002 175B).",
            "comparison_with_other_method": true,
            "performance_other_method": "D&Q (LLaMA2-13B) F1 = 59.6 (comparable despite smaller model size).",
            "key_findings": "IRCoT achieves slightly higher F1 than D&Q on HotPotQA, but D&Q achieves comparable performance with a much smaller model by learning constrained decomposition and tool invocation; suggests supervised tool-invocation and backtracking can close the gap between small and large models.",
            "counter_examples_or_negative_results": "IRCoT's numeric edge (60.7 vs 59.6) is presented as evidence that some large-model retrieval+CoT pipelines can outperform D&Q, although the authors emphasize model-size differences.",
            "uuid": "e4747.5",
            "source_info": {
                "paper_title": "A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Self-Ask",
            "name_full": "Self-Ask",
            "brief_description": "A decomposition approach where the model asks intermediate sub-questions to itself, queries external search to answer them, and aggregates sub-answer information to form the final answer.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "InstructGPT (reported baseline, 175B)",
            "model_description": "InstructGPT (GPT-3 based instruction-tuned model, ~175B) referenced for reported Self-Ask baseline performance.",
            "reasoning_method_name": "Self-Ask (decompose-and-search)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "The model decomposes a complex question into sub-questions, uses a search engine to answer each sub-question independently, and then composes the final answer; typically follows a single decomposition trajectory per question.",
            "task_name": "HotPotQA (reported baseline)",
            "task_description": "Multi-hop QA benchmark; Self-Ask reported as baseline.",
            "performance": "Reported baseline: F1 = 55.2 (InstructGPT 175B).",
            "comparison_with_other_method": true,
            "performance_other_method": "D&Q (LLaMA2-13B) F1 = 59.6 (higher).",
            "key_findings": "Self-Ask improves multi-hop QA by explicit decomposition and search, but D&Q's constrained decomposition and backtracking lead to better retrieval and higher final performance in this paper's comparisons.",
            "counter_examples_or_negative_results": "No explicit negative examples beyond the numeric baseline where Self-Ask underperforms D&Q on HotPotQA.",
            "uuid": "e4747.6",
            "source_info": {
                "paper_title": "A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models.",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models.",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models.",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions.",
            "rating": 2,
            "sanitized_title": "interleaving_retrieval_with_chainofthought_reasoning_for_knowledgeintensive_multistep_questions"
        },
        {
            "paper_title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models.",
            "rating": 1,
            "sanitized_title": "leasttomost_prompting_enables_complex_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.01499375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models
13 Nov 2023</p>
<p>Hejing Cao caohejing@pku.edu.cn 
Peking University</p>
<p>Equal Contribution</p>
<p>Zhenwei An anzhenwei@pku.edu.cn 
Peking University</p>
<p>Equal Contribution</p>
<p>Jiazhan Feng fengjiazhan@pku.edu.cn 
Peking University</p>
<p>Kun Xu 
Kuaishou Technology</p>
<p>Liwei Chen 
Kuaishou Technology</p>
<p>Dongyan Zhao zhaody@pku.edu.cn 
Peking University</p>
<p>A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models
13 Nov 2023D396FB7D1232D5E04CC61958BBBC75CBarXiv:2311.07491v1[cs.CL]
While large language models exhibit remarkable performance in the Question Answering task, they are susceptible to hallucinations.Challenges arise when these models grapple with understanding multi-hop relations in complex questions or lack the necessary knowledge for a comprehensive response.To address this issue, we introduce the "Decompose-and-Query" framework (D&amp;Q).This framework guides the model to think and utilize external knowledge similar to ReAct, while also restricting its thinking to reliable information, effectively mitigating the risk of hallucinations.Experiments confirm the effectiveness of D&amp;Q: On our ChitChatQA dataset, D&amp;Q does not lose to ChatGPT in 67% of cases; on the Hot-PotQA question-only setting, D&amp;Q achieved an F1 score of 59.6%.Our code is available at https://github.com/alkaidpku/DQ-ToolQA .</p>
<p>Introduction</p>
<p>Large language models (LLMs), such as ChatGPT (OpenAI, 2022), GPT-4 (OpenAI, 2023), Bard (Google, 2023), have made significant inroads into people's work and daily life, substantially enhancing productivity.For LLMs, to produce highquality response for a complex question relies on two essential capabilities: 1) Understanding multihop reasoning in complex questions; 2) Employing external knowledge.</p>
<p>On the one hand, many questions in our daily lives manifest as multi-hop questions.Individuals often pose intricate questions or instructions within a single sentence, lacking necessary descriptions.This necessitates LLMs to proficiently decompose complex questions into multiple subquestions.One potential solution involves employing a sub-question decomposition approach, such as self-ask (Press et al., 2022), which decomposes the question into sub-questions, resolves them independently, and consolidates the answers.However, this approach relies on LLMs' ability to reasonably decompose questions and is typically more suitable for high-quality prompts, such as HotPotQA (Yang et al., 2018) and 2WikiMultiHopQA (Ho et al., 2020).Challenges may arise when dealing with low-quality or unclear prompts.</p>
<p>On the other hand, large language models are prone to hallucinations (Zhang et al., 2023b) and do not have access to up-to-date knowledge (Mallen et al., 2023).Integrating external tools (search engines, knowledge bases, APIs, and etc) with LLMs is a viable approach to address these challenges.Least-to-most (Zhou et al., 2022) decomposes complex question into sub-questions, ultimately leading to a comprehensive answer.However, answering these sub-questions may also bring hallucinations.ReAct (Yao et al., 2022) bridges the gap of reasoning and action, enabling thoughtful interactions with tools.It is noteworthy that the action space of ReAct is limited, and its effectiveness is relatively modest.Shao et al. (2023a) introduced Iter-RetGen, which iteratively follows a two-step process: retrieving relevant knowledge, and generating answer.The approach of Iter-RetGen bears similarities to Self-Refine (Madaan et al., 2023).However, Iter-RetGen lacks autonomous termination capabilities.</p>
<p>To this, we have proposed a constrained question decomposition strategy that transforms question decomposition into a "Decompose-and-Query" depthfirst search framework.Specifically, we construct a reliable question-answer base, which consists of high-quality question-answer pairs.The model is required to consult it to generate answers for search efficiency.In case of conflicts between the model's knowledge and the reliable QA base, the base's content takes precedence.</p>
<p>We annotated solution trajectories for fine-tuning the model and subsequent predictions.The supervised fine-tuning phase does not involve tool in-vocation; the model processes static dialogue text, learning strategies for invoking tools.During the prediction phase, the model calls external tools to query a reliable QA base.If the LLM realizes during the search process that it has pursued an incorrect path, it can backtrack to previous steps and initiate a new search.Experiments confirm the effectiveness of D&amp;Q: On our ChitChatQA dataset, D&amp;Q does not lose to ChatGPT in 67% of cases; on the HotPotQA question-only setting, D&amp;Q achieved an F1 score of 59.6%.</p>
<p>Our contributions are the following:</p>
<ol>
<li>
<p>We build and make public a dataset of real conversation scenarios: ChitChatQA.</p>
</li>
<li>
<p>We design a framework for LLMs using external tools to perform constrained problem decomposition.Although simple, it can theoretically be extended to any number of external tools.</p>
</li>
<li>
<p>We achieve an F1 score of 59.6% on the question-only setting on HotPotQA dataset.</p>
</li>
</ol>
<p>Related Works</p>
<p>Reasoning.The capacity for logical reasoning is a significant hallmark of intelligence in large language models.Wei et al. (2022) have proposed the Chain-of-Thought (CoT) as an effective means to enhance the reasoning abilities of large language models consistently and efficiently.However, incorrect reasoning paths often lead to incorrect reasoning results.To this, Wang et al. (2022) suggest self-consistency, which samples a diverse set of reasoning paths and selects the most consistent answer.Self-consistency, despite its higher decoding cost compared to CoT, still struggles with challenging problem-solving.Yao et al. (2023) introduce a novel approach by extending CoT into Tree-of-Thought (ToT).At each step, the model engages in self-evaluation and, when necessary, looks ahead or backtracks to enhance decision-making abilities.</p>
<p>Similarly, Graph of Thoughts (GoT) framework models information generated by large language models as a graph, allowing for the combination of various LLM thoughts to yield collaborative results, extract insights from the entire thought network, and employ feedback loops to enhance thinking (Besta et al., 2023).Cumulative reasoning (Zhang et al., 2023a) employs proposers, verifiers, and reporters to store intermediate results, facilitating the accumulation of the reasoning process.So far, the reasoning capabilities of large language models have remained understudied.</p>
<p>Augmented Language Models.Large language models are prone to generating hallucinations and do not have access to up-to-date knowledge.Large language models are likely to make factual errors with long-tail knowledge (Kandpal et al., 2022).As a consequence, more and more research is focusing on the use of external tools to enhance large language models.Retrieval-based language model is a classic augmented language model.The idea is to use discrete or dense retrievers to retrieve related context, and then merge the retrieved context into the input.It is possible to match certain capabilities of some of the largest LMs while having less parameters (Schick et al., 2023;Qin et al., 2023).Along with the research on CoT, many studies have integrated external tools into the chain of thought paradigm (Yao et al., 2022;Paranjape et al., 2023;Feng et al., 2023;Press et al., 2022;Shao et al., 2023b;Trivedi et al., 2022;Gou et al., 2023a,b).Synergizing reasoning and invocation tools in LLMs is an underexplored potential research domain.</p>
<p>Methodology</p>
<p>This section presents our methodology.We construct ChitChatQA and the reliable QA base, annotate problem-solving trajectories and train a model to invoke tools.</p>
<p>ChitChatQA dataset</p>
<p>We introduce a novel dataset, ChitChatQA, to assess the capabilities of large language models in handling complex and diverse questions.Specifically, we collected single-turn dialogue data from real scenarios in the KwaiTalk application.The dataset comprises authentic questions posed by users and responses generated by large language models.We denote the original question-answer data as S 0 = (q i , a i ) N 0 .We performed data cleaning on the collected question-answer dataset to eliminate noise.Specifically, we initially utilized a grammatical correction model to filter out questions with low scores, followed by an intent recognition model to screen for questions with ambiguous intent.During the data cleaning process, we applied a lower intent recognition threshold to retain data with unclear intents.The detailed procedure for data cleaning can be found in Algorithm 1. Algorithm 1 Reliable QA Database Creation Require: S 0 , ϵ 1 , ϵ 2 , GEC, Intent Ensure: S 1: S = new Set() 2: for q i in S 0 do 3:</p>
<p>if GEC(q i ) &gt; ϵ 1 and Intent(q i ) &gt; ϵ 2 then 4:</p>
<p>S.Add(q i ) 5:</p>
<p>end if 6: end for 7: return S</p>
<p>Reliable QA Base</p>
<p>We constructed a reliable question-answer base, which consists of high-quality question-answer pairs.Similar to Constitutional AI (Bai et al., 2022), all question-answer pairs are considered beyond doubt.The model is required to consult the database to generate answers.In case of conflicts between the model's knowledge and the reliable QA base, the base's content takes precedence.We believe that this makes the model less prone to hallucinations and more robust.</p>
<p>In dialogue system, users may inquire about similar questions, such as "please provide me with a Python code for Bubble Sort" or "tell me the names of the Eight Great Masters of the Tang and Song dynasties".These generic questions are frequent.Preliminary analysis shows that these questions exhibit high quality.Therefore, we selected the top 50,000 questions based on their frequency as benchmark questions for the reliable QA base.</p>
<p>Due to the fact that the pretraining data for large language models comprises noise, the outputs for the same question may vary.We categorized the questions into several types.Objective factual questions demand precise and unique responses.For these questions, we employ a majority voting approach to determine the response.Subjective opinion questions pertain to opinions or perspectives on certain matters, and for such questions, we use an opinion aggregation method to determine the response.Consequently, we designed an answer aggregation model to consolidate diverse viewpoints.</p>
<p>The answer aggregation model is essentially a large language model through supervised finetuning.We manually annotated 5,000 answer ag- For examples of answer aggregation, please refer to Appendix A.</p>
<p>Trajectory Annotation</p>
<p>To train the model to invoke external tools, we need gather a sufficient amount of data for supervised fine-tuning.Both ChatGPT and GPT-4 exhibit excellent performance in data annotation (Ubani et al., 2023).Based on preliminary experiments, the annotation capabilities of ChatGPT are significantly inferior to those of GPT-4.The annotation framework is illustrated in Figure 1.</p>
<p>Adaptive Constraint Question Decomposition</p>
<p>We employ imitation learning using data that involves multiple iterations of calling an external retriever for fine-tuning large language models.We use LLaMA2-13b as base model.The training is conducted in a multi-turn dialogue format, wherein, in each training round, only the output of that round's dialogue is retained, and the rest is masked.For more details, please refer to the Experimental Section.</p>
<p>Experiments</p>
<p>Our primary experiments consist of two parts.To evaluate question decomposition capability, experiments were conducted on the ChitChatQA dataset.</p>
<p>To assess the effectiveness of our approach in answering multi-hop questions, experiments were conducted using the academic dataset HotPotQA.</p>
<p>ChitChatQA</p>
<p>We utilized a set of 200 training instances and conducted supervised fine-tuning in a multi-turn dialogue format on LLaMA2-13b.The supervised fine-tuning phase does not involve tool invocation; the model processes static dialogue text, learning strategies for invoking tools.During the prediction phase, the model calls external tools to query the reliable QA base.We defined three tools:</p>
<p>[QuestionRetriever] for retrieving related questions from the QA base, [AnswerRetriever] for retrieve answer to a question, and [Finish] indicating task termination.ChitChatQA's data stems from real dialogue interactions with the Kuaishou dialogue model, with a test set comprising 100 instances.We employed the Kuaishou dialogue model's output as a baseline, noted as "KwaiDialog", and compared it with Chat-GPT's output.Model evaluation was conducted through human assessments labeled as win/tie/lose, as shown in  ReAct Utilizes Search and Lookup tools iteratively to provide external information for large language models (Yao et al., 2022).</p>
<p>Self-Ask Requires the model to ask itself and use the answers by the search engine to enhance the model (Press et al., 2022).</p>
<p>IRCoT Decomposes questions through Chain-of-Thought, retrieves results using a retriever, then concatenates the question and reference context to a more robust language model (GPT-3).</p>
<p>Implementation Details</p>
<p>We conducted experiments on the HotPotQA dataset.Similar to most baseline models, our configuration was question-only: providing only the question without accompanying references, allowing the model to autonomously invoke external tools for information retrieval.We defined three tools: [ArticleRetriever] for querying entries in Wikipedia, [PageRetriever] for retrieving pages corresponding to the entries, and [Finish] representing task termination.The specific implementations of these tools were achieved through calls to the MediaWiki * API.We utilized 200 training instances and performed supervised fine-tuning in a multi-turn dialogue format on LLaMA2-13b.</p>
<p>Results</p>
<p>Table 1 shows the D&amp;Q result of HotPotQA.We can draw the following conclusions: Backtracking enhances the model's retrieval capability.Although ReAct supports acquiring reference information through observation, this approach is, in practice, greedy.Specifically, ReAct does not explicitly backtrack but tends toward random walks.In contrast, D&amp;P stores intermediate states, allowing it to retreat to the previous state in case of retrieval failure, significantly enhancing retrieval capability.</p>
<p>Tool invocation can compensate for parameter limitations.In comparison to IRCoT's GPT-3 (175B), our generative model is LLaMA-13B.Through supervised fine-tuning, D&amp;Q can achieve results comparable to IRCoT.This indicates that smaller models, through supervised fine-tuning to learn tool invocation, can achieve performance comparable to larger models.</p>
<p>Ablation Study</p>
<p>Retrieval Ability</p>
<p>Method</p>
<p>Average Contexts Recall direct prompt 26.7 52.3 D&amp;Q 50.0 68.8</p>
<p>Table 3: Recall of retrieval.</p>
<p>We limit the retriever's calls to a maximum of 10, with each call returning up to 5 relevant entries.Therefore, the model can return a maximum * https://www.mediawiki.org/wiki/API:Search/ of 50 entries when generating responses.We calculate the recall by retrieving 50 entries using the initial question as a baseline.We tested the recall of D&amp;Q's retrieval ability and compared it with the baseline, resulting in an increase from 52.3% to 68.8%.This indicates that considering context contributes to more accurate tool invocation.</p>
<p>Conclusion</p>
<p>In conclusion, we propose the "Decompose-and-Query" framework (D&amp;Q) to address the susceptibility of large language models to hallucinations by guiding them to utilize external knowledge within a constrained and reliable context.Through experiments on the ChitChatQA dataset and the Hot-PotQA question-only setting, D&amp;Q demonstrated competitive performance against existing models.Our contributions include the introduction of the ChitChatQA dataset, a novel framework for constrained problem decomposition, and the achievement of a notable F1 score on HotPotQA.These findings underscore the potential of D&amp;Q in enhancing the robustness and performance of large language models in question-answering tasks.</p>
<p>Question: What is the Qixi Festival?Answer 1: The Qixi Festival, also known as the "Cowherd and Weaver Girl Festival" or "Chinese Valentine's Day," is a traditional Chinese festival.It originated from the ancient myth of the Cowherd and Weaver Girl, where the Weaver Girl, said to be the seventh daughter of the Queen Mother of the West, fell in love with the Cowherd.However, they separated by the Milky Way could only meet once a year on seventh day of the seventh lunar month.This festival is primarily associated with praying for love, reunion, and happiness.On Qixi Festival, people engage in various celebratory activities such as admiring lanterns, attending lantern festivals, paying respects to the Weaver Girl, and eating Qiao Guo (a type of pastry).Nowadays, Qixi Festival has also become a day for young people to express love and create a romantic atmosphere.Answer 2: The Qixi Festival, also known as the "Cowherd and Weaver Girl Festival" or "Chinese Valentine's Day," is a traditional Chinese festival.It originated from ancient star worship, with the legend of the Cowherd (Altair, the Cowherd Star) and the Weaver Girl (Vega, the Weaver Star) meeting in the Milky Way.This story symbolizes people's yearning for and wishes for beautiful love.Qixi Festival is typically celebrated on the seventh day of the seventh lunar month, and activities on this day include praying for love, couples' reunions, and gift-giving.One of the most famous activities is "Bai Qijie," a ritual to pray for wisdom and craftsmanship from the Weaver Girl.Additionally, there are traditional customs like flying kites, stargazing, and enjoying lanterns.Answer 3: The Qixi Festival, also known as the "Begging for Skills Festival" or "Daughter's Day," is a traditional Chinese festival rooted in ancient astronomical worship.Celebrated on the seventh day of the seventh lunar month, the day is believed to be when the Cowherd and Weaver Girl meet on the Magpie Bridge.On this day, people engage in various customs to pray for fulfilling love, such as praying to the Weaver Girl, begging for skills, paying respects to the Weaver Girl, and eating Qiao Guo.Simultaneously, Qixi Festival serves as China's Valentine's Day, where couples express love by exchanging gifts, having candlelit dinners, and more.Answer 4: The Qixi Festival is a traditional Chinese festival where people participate in various customs and activities, including folding peach branches, worshipping the Weaver Girl, rubbing rose of Sharon flowers, eating Qiao Guo, washing and arranging tools, dyeing fingernails, celebrating the birthday of the Cow, playing Qijie puzzles, threading needles for skill-begging, sowing seeds for birth, offering "Mo He Le," and more.Additionally, Qixi Festival is also considered China's Valentine's Day, with customs like begging for skills, worshiping immortals, and threading needles to pray for a happy marriage.Answer 5: The Qixi Festival is a traditional Chinese Valentine's Day, also known as "Daughter's Day" or "Qiaoqi Day," similar to Valentine's Day on February 14th.The seventh day of the seventh lunar month marks the meeting day of the Cowherd and Weaver Girl, a beautiful and eternal love story.Answer 6: The Qixi Festival is the most romantic traditional Chinese festival, primarily for young girls.The festival focuses on the Begging for Skills ceremony, hence also known as "Begging for Skills Festival," "Girl's Festival," or "Daughter's Festival."On this evening, women engage in needle threading to pray for wisdom and craftsmanship, worshipping the Weaver Girl with sincere and solemn ceremonies.The festival is characterized by displaying flowers, fruits, and needlework, with various activities like making small dolls.Answer 7: The Qixi Festival is a traditional Chinese festival where people perform various rituals and activities, such as needle threading for skills, spider-themed joyous activities, needle testing for skills, seed sowing for birth, offering "Mo He Le," worshiping the Weaver Girl, worshiping the Star of Marriage, sunning books, sunning clothes, celebrating the Cow's birthday, eating Qiao Guo, and more.Additionally, Qixi Festival features various small dolls displayed at home, with needle threading contests being one of the earliest popular activities, starting from the Han Dynasty.On Qixi Festival, women and girls gather under grape arbors, guessing what the Cowherd and Weaver Girl might say in secret while showcasing their needlework.The one who threads the needle the fastest is considered the most skillful.Answer 8: The Qixi Festival is a traditional Chinese folk festival, celebrating the birth of the Seventh Sister in traditional beliefs.It is a festival of both worshiping the Seventh Sister and celebrating love, centered around the folk tale of the Cowherd and Weaver Girl.Traditional customs of Qixi Festival include worshipping the Seventh Sister, making wishes, begging for skills, observing the Cowherd and Weaver Girl stars, praying for marriage, storing Qixi water, and more.On Qixi Festival, groups of women and girls gather, set up fruit offerings, bathe and fast, burn incense, and pray to the Weaver Girl, making wishes like marrying a good husband and having children.Answer 9: The Qixi Festival is a traditional Chinese Valentine's Day, a festival for couples.On this day, people pray for skillfulness and make wishes.Additionally, people light fireworks in the evening, eat sweets, change to new bedding, and display photos.Answer 10: The Qixi Festival is a traditional Chinese festival with many customary activities, including folding peach branches, worshipping the Weaver Girl, rubbing rose of Sharon flowers, eating Qiao Guo, washing and arranging tools, dyeing fingernails, celebrating the Cow's birthday, piecing together Qijie puzzles, threading needles for skill-begging, sowing seeds for birth, offering "Mo He Le," and more.Moreover, the Qixi Festival is China's Valentine's Day, and people engage in activities such as threading needles to pray for skillfulness, offering prayers for good fortune and longevity, worshipping the Weaver Girl, displaying flowers and needlework, and using beautifully crafted furniture and utensils.</p>
<p>Viewpoint: The Qixi Festival is a traditional Chinese festival, originating from the myth of the Cowherd and Weaver Girl, a day for praying for love, reunion, and happiness.Answer IDs: Answer 1, Answer 2, Answer 3 Viewpoint: The Qixi Festival is a traditional Chinese festival where people engage in various customs, such as worshiping the Weaver Girl and threading needles for skill-begging, praying for skillfulness, and achieving a happy marriage.Answer IDs: Answer 4, Answer 7, Answer 10 Viewpoint: The Qixi Festival is China's traditional Valentine's Day, a festival for couples to express love in various ways.Answer IDs: Answer 5, Answer 9 Viewpoint: The Qixi Festival is the most romantic traditional Chinese festival, primarily for young girls, with activities centered around begging for skills.Answer ID: Answer 6 Viewpoint: The Qixi Festival is a traditional Chinese folk festival, celebrating the birth of the Seventh Sister in traditional beliefs.It is a comprehensive festival with themes of praying for good fortune, begging for skills, and love, with women as the main participants.</p>
<p>Answer ID: Answer 8</p>
<p>Figure 1 :
1
Figure 1: Example of question decomposition trajectory.Q: Question; I: Iteration.Dashed line means rollback.We use Ques_Ret for QuestionRetriever and Ans_Ret for AnswerRetriever for brevity.</p>
<p>Figure 2 :
2
Figure 2: Model architecture of HotPotQA.</p>
<p>Table 1 .
1MethodGeneration Model Model Size EM F1StandardPaLM540B21.9 36.8CoT (Wei et al., 2023)PaLM540B30.0 44.1ReAct (Yao et al., 2022)PaLM540B24.9 44.7Self-Ask (Press et al., 2022) InstructGPT175B36.8 55.2IRCoT (Trivedi et al., 2022) Code-Davinci-002 175B-60.7D&amp;QLLaMA213B39.0 59.6</p>
<p>Table 1 :
1
Results on HotPotQA dataset.EM: Exact Match.
The results indicate a significant enhancement inthe response capability of the Base model with theD&amp;Q approach. Even after complex prompt fine-tuning and reinforcement learning for KwaiDialog,our method still achieves victory in 37% of testcases (not losing in 73%). Compared to ChatGPT,D&amp;Q maintains a competitive position, not losingin 62% of cases.wintieloseD&amp;Q vs KwaiDialog 37% 36% 27%D&amp;Q vs ChatGPT24% 38% 38%</p>
<p>Table 2 :
2
Human evaluation on ChitChatQA
4.2 HotPotQA4.2.1 Baselines</p>
<p>. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom HenighanNova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston; Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brownand Jared Kaplan. 2022. Constitutional AI: Harmlessness from AI Feedback</p>
<p>Graph of Thoughts: Solving Elaborate Problems with Large Language Models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, Torsten Hoefler, 2023</p>
<p>Knowledge Refinement via Interaction Between Search Engines and Large Language Models. Jiazhan Feng, Chongyang Tao, Xiubo Geng, Tao Shen, Can Xu, Guodong Long, Dongyan Zhao, Daxin Jiang, 2023Google. 2023. Google bard</p>
<p>CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen, 2023a</p>
<p>ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, 2023b</p>
<p>Constructing a multihop QA dataset for comprehensive evaluation of reasoning steps. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, Akiko Aizawa, 10.18653/v1/2020.coling-main.580Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsBarcelona, Spain (Online2020International Committee on Computational Linguistics</p>
<p>Large Language Models Struggle to Learn Long-Tail Knowledge. Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, Colin Raffel, 2022</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark, 2023</p>
<p>When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, Hannaneh Hajishirzi, 10.48550/arXiv.2212.10511ArXiv:2212.105112023</p>
<p>Chatgpt: Optimizing language models for dialogue. 2022OpenAI</p>
<p>ART: Automatic multistep reasoning and tool-use for large language models. Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, Marco Tulio, Ribeiro , 2023Ofir PressMuru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smithand Mike Lewis. 2022. Measuring and Narrowing the Compositionality Gap in Language Models</p>
<p>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, Maosong Sun, ToolLLM: Facilitating Large Language Models to Master 16000+ Realworld APIs. 2023</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language Models Can Teach Themselves to Use Tools. </p>
<p>Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy. Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, Weizhu Chen, 2023a</p>
<p>Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy. Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, Weizhu Chen, 2023b</p>
<p>Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal, 2022</p>
<p>Zeroshotdataaug: Generating and augmenting training data with chatgpt. Solomon Ubani, Suleyman Olcay Polat, Rodney Nielsen, 2023</p>
<p>Self-Consistency Improves Chain of Thought Reasoning in Language Models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, 2022</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2022</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2023</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D Manning, 10.18653/v1/D18-1259Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Tree of Thoughts: Deliberate Problem Solving with Large Language Models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, 2023</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, ReAct: Synergizing Reasoning and Acting in Language Models. 2022</p>
<p>Cumulative Reasoning with Large Language Models. Yifan Zhang, Jingqin Yang, Yang Yuan, Andrew Chi-Chih Yao, 2023a</p>
<p>Siren's song in the ai ocean: A survey on hallucination in large language models. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, arXiv:2309.012192023barXiv preprint</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. 2022. Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. </p>
<p>. Aggregation Answer, 问题：七夕节是干什么的？ 答案1：七夕节，又称为"牛郎织女节"或"中国情人节"，是中国的一个传统节日。它起源于古代的牛郎织女神话，相传织女是王母娘娘的第 七个女儿，与牛郎相爱，却被银河分隔，两人只能每年七夕这天才能相会一次。这个节日主要用于祈求姻缘、团圆和幸福。在七夕节这一 天，人们会举行各种庆祝活动，如赏花灯、游灯会、拜织女、吃巧果等。现在，七夕节也成为了年轻人表达爱意和浪漫氛围的一个节日。 答案2：七夕节，又称为"牛郎织女节"或"中国情人节"，是中国的一个传统节日。它起源于古代的星宿崇拜，传说牛郎(天河星)和织女 (织女星)在天河相会。这个故事表现了人们对美好爱情的向往和祈愿。七夕节通常在农历七月七日这一天庆祝，人们在这一天会举行各 种活动，如祈求姻缘、情侣相会、送礼物等。其中最著名的活动是"拜七姐"，即向织女祈求智慧和巧艺。此外，还有放风筝、观星、赏花 灯等传统习俗。 答案3：七夕节，又称乞巧节、女儿节，是中国传统的节日，起源于古代对天文星宿的崇拜。七夕节在农历七月初七，传说这一天是牛郎织 女鹊桥相会的日子。在这一天，许多人会通过各种方式祈求自己的爱情美满，如织女星的祈求、乞巧、拜织女、吃巧果等。同时，七夕节 也是中国的情人节，许多情侣会在这一天表达爱意，送礼物、吃烛光晚餐等。 答案4：七夕节是中国的传统节日，人们会进行许多习俗活动，如折桃枝、拜织女、揉木槿、吃巧果、洗梳具、染指甲、贺牛生日、拼七巧 板、穿针乞巧、种生求子、供奉磨喝乐等。此外，七夕节也是中国的情人节，人们会进行乞巧、拜仙、穿针等仪式，祈求心灵手巧、获得 美满姻缘。 答案5：七夕节是中国传统的情人节，也被称为"女儿节"或"乔奇日"，类似于中国情人节中的2月14日情人节。每年的七夕是牛郎织女相会 的日子，这是一个美丽而永恒的爱情故事。 答案6：七夕节是中国传统节日中最具浪漫色彩的一个节日，主要参与者是少女，节日活动以乞巧为主，故被称为"乞巧节"、"少女 节"、"女儿节"。在这一天晚上，妇女们穿针乞巧，祈祷福禄寿，礼拜七姐，仪式虔诚而隆重，陈列花果女红，各式家具、用具都精美小 巧、惹人喜爱。 答案7：七夕节是中国的传统节日，人们会举行各种仪式和活动，如穿针乞巧、喜蛛应巧、投针验巧、种生求子、供奉"磨喝乐"、拜织女、 拜魁星、晒书、晒衣、贺牛生日、吃巧果等等。此外，七夕节还有各种小玩偶摆在家里，穿针比赛是最早开始流行的，通过穿针的方式乞 巧，穿的最快者就是手最巧的。 答案8：七夕节是中国民间的传统节日，为传统意义上的七姐诞。七夕节既是拜七姐节日也是爱情节日，又是一个以"牛郎织女"民间传说为 载体，以祈福、乞巧、爱情为主题，以女性为主体的综合性节日。七夕节的传统习俗有拜七姐、祈福许愿、乞求巧艺、坐看牵牛织女星、 祈祷姻缘、储七夕水等。七夕节这天，少女、妇女们三五成群约好了一起举行仪式，摆好瓜果祭品，沐浴斋戒，焚香礼拜。然后大家一起 围坐在桌子前，一边吃瓜果点心，一边向织女默默许愿，如嫁个如意郎君、早生贵子之类。七夕节各项比赛中，穿针比赛其中最早开始流 行的，大约从汉代开始。在七夕节这天，各家各户的女人女孩都会有出家门，大家聚在葡萄架下，一边猜女郎织女相会会说什么轻言密 语，一边拿出自己的针线女红。通过穿针的方式乞巧，穿的最快者就是手最巧的。 答案9：七夕节是中国传统情人节，是情侣之间的节日。在七夕这一天，人们会祈求自己能够心灵手巧，同时也会许愿。此外，人们还会在 晚上燃放鞭炮，吃糖果，换新被褥，摆放照片等。 答案10：七夕节是中国的传统节日，人们会进行许多习俗活动，如折桃枝、拜织女、揉木槿、吃巧果、洗梳具、染指甲、贺牛生日、拼七 巧板、穿针乞巧、种生求子、供奉磨喝乐等。此外，七夕节也是中国的情人节，人们会穿针乞巧，祈祷福禄寿，礼拜织女，陈列花果、女 红，各式家具、用具都精美小巧、惹人喜爱。 观点：七夕节是中国的传统节日，起源于牛郎织女的神话故事，是祈求姻缘、团圆和幸福的日子。 答案id：答案1、答案2、答案3 观点：七夕节是中国的传统节日，人们会进行各种习俗活动，如拜织女、穿针乞巧等，祈求心灵手巧、获 Data, 得美满姻缘, </p>            </div>
        </div>

    </div>
</body>
</html>